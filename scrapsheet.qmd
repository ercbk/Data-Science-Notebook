---
title: "Scrapsheet"
---

## Other Places

-   words

## Grocery list

-   Snack
-   Frozen

## tidycensus 3

-   acs

    -   variables covering e.g. income, education, language, housing characteristics)

    -   Available as 1-year estimates (for geographies of population 65,000 and greater) and 5-year estimates (for geographies down to the block group)

-   Remove Water Boundaries (1:02)

    -   Get before and after pics; descriptions

    -   works specifically with census tigris

    -   Can take a couple minutes to run

    -   cb = FALSE avoid sliver poly

    -   erase_water

        -   area_threshold = 0.5 - water areas in 50th percentile in terms of size are removed

-   Map types

    -   tmap in ch 6 of walker's book

    -   rates or percentages - choropleths

    -   counts - bubble maps or circle maps - need to convert data to centroids

        -   dot density for hetergeneity (1:45)

            -   get descriptions of shuffling, aes_legend

        -   basemap (1:47)

            ``` r
            dot_density_with_basemap <- ggplot() + 
              annotation_map_tile(type = "cartolight", zoom = 9) + 
              geom_sf(data = san_diego_race_dots, aes(color = variable), size = 0.01) + 
              scale_color_brewer(palette = "Set1") + 
              guides(color = guide_legend(override.aes = list(size = 3))) + 
              theme_void() + 
              labs(color = "Race / ethnicity",
                   caption = "2018-2022 ACS | 1 dot = approximately 200 people")
            ```

            -   get pros and code

-   mapview

    -   m1 + m2 creates layers

    -   m1 \| m2 creates swipe map (need leaflet.extras2)

    -   {leafsync} side by side maps zooms and syncs cursor `sync(m1, m2)`

-   rdeck

    -   visualize large amounts of data

    -   migration flows

        -   tidycensus::get_flows

        -   only for \> 2020 5-yr ACS

    -   map type also good for mapping commuting patterns

-   Automated Mapping

    -   Memory intensive

    -   https://walker-data.com/posts/iterative-mapping/ for more advanced Metro exampleThank

        -   Shows how to export too

    -   geographic patterns in remote work for 100 largest counties by population in US. (2:17)

        -   important for office space real estate

        -   Maps per County

        -   Generate list of 100 largest counties

            ``` r
            library(tidycensus)
            library(tidyverse)

            top100counties <- get_acs(
              geography = "county",
              variables = "B01003_001",
              year = 2022,
              survey = "acs1"
            ) %>%
              slice_max(estimate, n = 100)
            ```

            -   MOE is NA which means this is true value

                -   Plus ACS more recent

        -   pull remote work data at county level for those counties

            -   Need to get tract data for remote work data

                ``` r

                wfh_tract_list <- top100counties %>%
                 split(~NAME) %>% # splits into a list with each element per county
                 map(function(county) {
                   state_fips <- str_sub(county$GEOID, 1, 2) # extract first 2 chars (state)
                   county_fips <- str_sub(county$GEOID, 3, 5) # extract next 3 chars (county)

                   get_acs(
                     geography = "tract",
                     variables = "DP03_0024P",
                     state = state_fips,
                     county = county_fips,
                     year = 2022,
                     geometry = TRUE
                   )
                 })
                ```

                -   need census key since hitting api 100s of times

        -   Make 100 Maps

            ``` r
            library(mapview)

            wfh_maps <- map(wfh_tract_list, function(county) {
              mapview(
                county, 
                zcol = "estimate",
                layer.name = "% working from home"
              ) 
            })
            ```

-   Small Area Time Series Analysis (2:40)

    -   Where has remote work increased the most in Salt Lake City, Utah

        -   5yr acs represent overlapping samples

        -   For 2018-2022

            -   Compare 2008-2012 to

        -   Comparison Profile only at county level

            ``` r
            utah_wfh_compare <- get_acs(
              geography = "county",
              variables = c(
                work_from_home17 = "CP03_2017_024",
                work_from_home22 = "CP03_2022_024"
              ),
              state = "UT",
              year = 2022
            )
            ```

        -   Census Tract (neighborhood-leve)

            -   Issue: geographies change

                -   get more details

            -   Aerial Interpolation (see book for more details)

                -   Check for incongruent boundaries

                    ``` r
                    library(sf)

                    wfh_17 <- get_acs(geography = "tract", variables = "B08006_017", year = 2017,
                                      state = "UT", county = "Salt Lake", geometry = TRUE) %>%
                      st_transform(6620)

                    wfh_22 <- get_acs(geography = "tract", variables = "B08006_017", year = 2022,
                                      state = "UT", county = "Salt Lake", geometry = TRUE) %>%
                      st_transform(6620)
                    ```

                    -   get details on crs projection

                    -   get details on how he found incongruent boundaries

                -   Use st_interpolate_aw

                    ``` r
                    library(sf)

                    wfh_22_to_17 <- wfh_22 %>%
                      select(estimate) %>%
                      st_interpolate_aw(to = wfh_17, extensive = TRUE)
                    ```

                    -   rolls backwards

                    -   Uses area weighting (get details

                -   Population weighted roll forward method

                    ``` r
                    library(tigris)
                    options(tigris_use_cache = TRUE)

                    salt_lake_blocks <- blocks(
                      "UT", 
                      "Salt Lake", 
                      year = 2020
                    )

                    wfh_17_to_22 <- interpolate_pw(
                      from = wfh_17,
                      to = wfh_22,
                      to_id = "GEOID",
                      weights = salt_lake_blocks,
                      weight_column = "POP20",
                      crs = 6620,
                      extensive = TRUE
                    )

                    # calculate change over time
                    wfh_shift <- wfh_17_to_22 %>%
                      select(GEOID, estimate17 = estimate) %>%
                      left_join(
                        select(st_drop_geometry(wfh_22), 
                               GEOID, estimate22 = estimate), by = "GEOID"
                      ) %>%
                      mutate(
                        shift = estimate22 - estimate17,
                        pct_shift = 100 * (shift / estimate17)
                      )

                    mapview(wfh_shift, zcol = "shift")
                    ```

## lab 91

-   clvtools for prob type, h2o::automl for ML
-   agg, cohort, prob, ml, fcast
-   group by, summarize, pad_by_time, ungroup
-   lag - use horizon and use 2\*horizon
-   rolling - 2,3,6 (uses lag parameters and 2; lags were 3 and 6 months with horizon = 3)
-   splits: timetk::time_series_split, cumulative = TRUE says use all previous data(?)

## Rhino

-   rhinoverse.dev
-   opiniated project structure, development toolbox, guides you towards best practices
-   `rhino::init()` or RStudio New Project wizard
-   github discussions for questions
-   Can use other UI packages and not just those in rhinoverse
-   Project structure
    -   config.yml for different environments (e.g. dev, prod)
    -   main.R with server and ui
    -   view - modules that rely on reactivity
    -   static - imgs
    -   styles - sass files (css stuff)
    -   dependencies - explicit list of packages
    -   cypress - unit tests of functions
-   `options(shiny.qutoreload = TRUE)` - once you save, the app changes automatically
-   addins
    -   formatting, lintr
    -   create rhino module
    -   build sass - automatically shows changes in app when changing and saving sass file
    -   build javascript - same as build sass but for react components
-   Uses {box} for function imports from packages and has a box linter
-   dependency management
    -   `pkg_install`/`remove` - install packages from everywhere and not just cran. Updates dependency.R and renv.lock
-   Add react components with {shiny.react}

## Signature Transform

-   Todo
    -   Continue reading
    -   Look at the separate papers from with the applied data examples are taken from
    -   Go back to original Amazon paper and see if signature parts and its appendix make more sense.
    -   Look at Discussion section in Signatory github and ask questions
-   Misc
    -   $e$ is a monomial (pg 13)
    -   $\lambda$ is a real number (pg 13)
    -   $\otimes$ is defined as the the *joining* (i.e. concantenating) of multi-indexes of monomials: $e_{i_1} \cdots e_{i_k} \otimes e_{j_1} \cdots e_{j_m} = e_{i_1} \cdots e_{i_k} e_{j_1} \cdots e_{j_m}$ (pg 13)
        -   Chen's Identity: $S(X*Y)_{a,c} = S(X)_{a,b} \otimes S(Y)_{b,c}$ where $X*Y$ is the concantenation of two paths (pg 14)
            -   So the signature of a concantenated path is equal to the circle-product of the signatures of the component paths.
        -   $\otimes n$ is the n^th^ power with respect to the circle product, $\otimes$ (pg15)\
-   Workflow
    -   Create a continuous path $X_i$ from each time-series $\{Y_i\}$ (row-wise)
    -   If needed, make use of the lead-lag transform to account for the variability in data
        -   Cumalative sum transform is another
    -   Compute the truncated signature $S(X_i)|_L$ of the path $X_i$ up to level $L$
        -   Either a Full or Log signature
    -   Standardize each signature column
    -   Use the terms of signature $\{S^I_i\}$ as features
-   Issue
    -   Degeneracy in the terms of the signature causes this representation not to be unique and introducing a problem of colinearity of the signature terms.
        -   Solution: LASSO, ridge or elastic net regularization
        -   Paper uses a 2-step lasso where signature features are selected by LASSO. Then those selected features are used in a second regression with other predictors.
-   Signature
    -   $S^{(1)}_{a,b} = X_b - X_a$
    -   $S^{(1,1)}_{a,b} = \frac{(X_b - X_a)^2}{2!}$
    -   $S^{(1,1,1)}_{a,b} = \frac{(X_b - X_a)^3}{3!}$
-   Cumulative + Lead Lag Signature Truncated to Level 2
    -   Signature
        -   $S(\tilde X)|_{L=2} = (1, S^{(1)}, S^{(2)}, S^{(1,1)}, S^{(1,2)}, S^{(2,1)}, S^{(2,2)})$
        -   $S^{(1)} = S^{(2)} = \sum_i^N X_i$
        -   $S^{(1,1)} = S^{(2,2)} = \frac{1}{2} \left(\sum_i^N X_i \right)^2$
        -   $S^{(1,2)} = \frac{1}{2} \left[\left(\sum_i^N X_i\right)^2 + \sum_i^N X_i^2 \right]$
        -   $S^{(1,2)} = \frac{1}{2} \left[\left(\sum_i^N X_i\right)^2 - \sum_i^N X_i^2 \right]$
    -   Moments
        -   Mean(X): $\frac{1}{N}S^{(1)}$
        -   Var(X): $-\frac{N+1}{N^2}S^{(1,2)} + \frac{N-1}{N^2}S^{(2,1)}$
-   Lead Lag Signature Truncated to Level 2
    -   $S(\tilde X)|_{L=2} = (1, S^{(1)}, S^{(2)}, S^{(1,1)}, S^{(1,2)}, S^{(2,1)}, S^{(2,2)})$
    -   $S^{(1)} = S^{(2)} = \sum_i^{N-1} (X_{i+1} - X_i)$
    -   $S^{(1,1)} = S^{(2,2)} = \frac{1}{2} \left(\sum_i^N (X_{i+1} - X_i) \right)^2$
    -   $S^{(1,2)} = \frac{1}{2} \left[\left(\sum_i^N (X_{i+1} - X_i)\right)^2 + \sum_i^N (X_{i+1} - X_i) \right]$
    -   $S^{(2,1)} = \frac{1}{2} \left[\left(\sum_i^N (X_{i+1} - X_i)\right)^2 - \sum_i^N (X_{i+1} - X_i) \right]$
-   Log Signature Truncated to Level 2\
    $$
    \begin{aligned}
    &\log S(X) = (\Delta X, \Delta X, \frac{1}{2}\text{QV}(X))\\
    &\begin{aligned}
    \text{where} \quad &\Delta X = X_N - X_1 \\
    &\text{QV}(X)) = \sum_{i=1}^{N-1} (X_{i+1} - X_i)^2
    \end{aligned}
    \end{aligned}
    $$
    -   [Example]{.ribbon-highlight}: eq 2.17, Calculation for Quadratic Variation (QV)

        ``` r
        x <- c(1, 4, 2, 6)
        x_lead <- dplyr::lead(x1)

        QV <- function(x1, x2){
          x1_a <- x1[-length(x1)]
          x2_a <- x2[-length(x2)]
          sq_diff <- function(j1, j2) {
            (j2 - j1)^2
          }
          purrr::map2_dbl(x1_a, x2_a, sq_diff) |> sum()
        }

        QV(x, x_lead)
        #> [1] 29
        ```
-   Questions
    -   $\otimes n$ makes no sense (pg 15) — just literally
    -   Log transform makes no sense (pg 15, 16) in terms of applying it to data
    -   Formula never provided lag-lead dimensions (pg 20)
        -   What's a lead-lag embedding and how are these things connected.
    -   How can it be -14.5 in 2.17 with QV is always positive? (pg 26)
    -   eq 2.18, Standard Signature, $S(X^2) = (1, 5, 5, 12.5, −2, 27, 12.5)$ --- How is this calculated? (pg 26)
        -   S^(1,2)^ and S^(2,1)^ don't match lead-lag algorithm. NO WHERE in the paper is the formula for these terms explicitly given. Think it might have to do with shuffle product maybe.
    -   What is the last column in the feature matrix (pg 31)? Paragraph makes it sound like it's the mean.
    -   In eq 2.34, what the fuck is going on in the 3rd dimension? It's supposed to be an indicator variable (i.e. 0 or 1)
    -   Are there guidelines on when to use cum/lead-lag transforms and full/log signatures and Level 1,2, or 3?
        -   In order to capture the quadratic variation of the price, the path is extended by means of a lead-lag transform ()
    -   Annoying phrases
        -   One can easily rewrite (pg 26) - then just spits out an answer with no previous example of how it was obtained.
-   Full Signature
    -   The terms of the signature are iterated integrals of a path, while the path is normally constructed by an interpolation of data points. One can compute such iterated integrals using several computational algorithms (cubature methods) which are generally straightforward to implement. (sect 2.3, pg 34)
    -   Signature approach is to convert data into paths and then compute the iterated integrals of the resulting paths (sect 2.4.1, pg 35)

## Reproducibility

-   Notes from <https://www.brodrigues.co/blog/2023-07-13-nix_for_r_part1/>
-   To ensure that a project is reproducible you need to deal with at least four things:
    -   Make sure that the required/correct version of R (or any other language) is installed
    -   Make sure that the required versions of packages are installed
    -   Make sure that system dependencies are installed (for example, you'd need a working Java installation to install the {rJava} R package on Linux)
    -   Make sure that you can install all of this for the hardware you have on hand.
-   Consensus seems to be a mixture of Docker to deal with system dependencies,`{renv}`for the packages (or`{groundhog}`, or a fixed CRAN snapshot like those [Posit provides](https://packagemanager.posit.co/__docs__/user/get-repo-url/#ui-frozen-urls)) and the [R installation manager](https://github.com/r-lib/rig) to install the correct version of R (unless you use a Docker image as base that already ships the required version by default). As for the last point, the only way out is to be able to compile the software for the target architecture.
-   Nix
    -   a package manager for Linux distributions, macOS and apparently it even works on Windows if you enable WSL2.

    -   huge package repository, over 80K packages

    -   possible to install software in (relatively) isolated environments

## Text Tiling

-   Previous articles

    -   5 sentence chunks - Instead of creating chunks large enough to fit into a context window (langchain default), I propose that the chunk size should be the number of sentences it generally takes to express a discrete idea. This is because we will later embed this chunk of text, essentially distilling its semantic meaning into a vector. I currently use 5 sentences (but you can experiment with other numbers). I tend to have a 1-sentence overlap between chunks, just to ensure continuity so that each chunk has some contextual information about the previous chunk. ([2-stage summarizing method](https://towardsdatascience.com/summarize-podcast-transcripts-and-long-texts-better-with-nlp-and-ai-e04c89d3b2cb))
    -   Chunk markdown documents by section using header tags (h1, h2, etc.) ([company doc searchable db](https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736#8ed1))
    -   Chunk size = Context window size (langchain default)

-   [nltk.tokenize.texttiling](https://www.nltk.org/_modules/nltk/tokenize/texttiling.html) - text tiling method from {{nltk}}

-   Created a test document that was the amalgam 4 different articles that can be used to test tiling method undefined

## Propensity Score Models?

-   bayesian vid (currently at 23:14)
    -   I don't remember which video this is
-   in observational analysis its imagined that sample is drawn from a joint distribution of all the variables
-   in causal inf, imagine intervening to change Z, treatment, independent of X, confounders.
-   Frequentist method: G-Estimation
    -   Models

        -   propensity score models, $b(x;\gamma)$
            -   Equivalent to $\text{Pr} [Z = 1 |x]$

            -   Estimating Equation for $\gamma$

                $$
                \sum \limits_{i=1}^n x_i^T (z_i - b(x_i; \gamma)) = 0
                $$
        -   treatment free mean model, $\mu_0(x; \beta)$
            -   Used for doubly robust estimation
        -   treatment effect (or blip) model $\tau z$, which can be extended to $z\mu_1(x;\tau)$

    -   Propensity Score Regression\

        $$
        Y = Z \tau + b(X; \hat{\gamma}) \phi + \epsilon
        $$

        -   $\phi$ is the estimated coefficient for the propensity score model
-   Bayesian
    -   There are other ways but this procedure is recommended
    -   Perform full Bayesian estimation of $\gamma$ , plug that (best) estimate into the propensity score model, $b(x_i; \hat{\gamma})$ , and then perform Bayesian analysis of $\tau$ (i.e. propensity score regression)
        -   The propensity score model part of the formula is basically a hack and not mimmicking any part of the dgp therefore for bayesians, the regression model is a misspecification.

## Copulas

-   Todo
    -   <https://twiecki.io/blog/2018/05/03/copulas/> - explainer, good starting point
    -   <https://copulae.readthedocs.io/en/latest/explainers/introduction.html> - another beginner explainer
    -   <https://www.r-bloggers.com/2015/10/modelling-dependence-with-copulas-in-r/> - practical example using returns of two stocks
-   Packages
    -   {[copula](https://cran.r-project.org/web/packages/copula/index.html)} - Multivariate Dependence with Copulas — lots of vignettes
    -   {{[copulae](https://github.com/DanielBok/copulae)}} - Multivariate data modelling with Copulas in Python
-   I think copulas are used for bias correction in post-processing separate forecasts of variables that are related. See [paper](https://www.annualreviews.org/doi/10.1146/annurev-statistics-062713-085831#_i59) (section 4.2 and 4.3)
    -   stocks of the same sector
    -   ensemble forecasts
        -   weather - meteorologists will forecast a variable (e.g. temp) many times but each time the model uses a different set of atmosphereic conditions. These forecasts are put into a regression (i.e. the ensemble) to create the final forecast. But that forecast is biased because the forecasts are related to each other. Post-processing with copula corrects this.
-   Ensemble Copula Coupling (ECC) applies the empirical copula of the original ensemble to samples from the postprocessed predictive distributions. ([paper](https://arxiv.org/pdf/1302.7149.pdf))
    1.  Generate a raw ensemble, consisting of multiple runs of the computer model that differ in the inputs or model parameters in suitable ways.
    2.  Apply statistical postprocessing techniques, such as Bayesian model averaging or nonhomogeneous regression, to correct for systematic errors in the raw ensemble, to obtain calibrated and sharp predictive distributions for each univariate output variable individually.
    3.  Draw a sample from each postprocessed predictive distribution.
    4.  Rearrange the sampled values in the rank order structure of the raw ensemble to obtain the ECC postprocessed ensemble
-   Depending on the use of Quantiles, Random draws or Transformations at the sampling stage, we distinguish the ECC-Q, ECC-R and ECC-T variants
-   ECC is based on empirical copulas aimed at restoring the dependence structure of the forecast and is derived from the rank order of the members in the raw ensemble forecast, under a perfect model assumption, with exchangeable ensemble members. For Schaake shuffle (SSH), on the other hand, the dependence structure is derived from historical observations instead. (Overview of subject - [paper](https://journals.ametsoc.org/view/journals/bams/102/3/BAMS-D-19-0308.1.xml))
-   Packages
    -   <https://cran.r-project.org/web/packages/ensembleBMA/index.html>
    -   <https://cran.r-project.org/web/packages/ensemblepp/index.html>
        -   Data for book, Statistical Postprocessing of Ensemble Forecasts
    -   <https://cran.r-project.org/web/packages/ensembleMOS/index.html>\
