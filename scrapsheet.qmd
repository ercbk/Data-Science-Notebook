---
title: "Scrapsheet"
---

-   text tiling
    -   Previous articles
        -   5 sentence chunks - Instead of creating chunks large enough to fit into a context window (langchain default), I propose that the chunk size should be the number of sentences it generally takes to express a discrete idea. This is because we will later embed this chunk of text, essentially distilling its semantic meaning into a vector. I currently use 5 sentences (but you can experiment with other numbers). I tend to have a 1-sentence overlap between chunks, just to ensure continuity so that each chunk has some contextual information about the previous chunk. ([2-stage summarizing method](https://towardsdatascience.com/summarize-podcast-transcripts-and-long-texts-better-with-nlp-and-ai-e04c89d3b2cb))
        -   Chunk markdown documents by section using header tags (h1, h2, etc.) ([company doc searchable db](https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736#8ed1))
        -   Chunk size = Context window size (langchain default)
    -   [nltk.tokenize.texttiling](https://www.nltk.org/_modules/nltk/tokenize/texttiling.html) - text tiling method from {{nltk}}
    -   Created a test document that was the amalgam 4 different articles that can be used to test tiling method undefined
-   bayesian vid (currently at 23:14)
    -   in observational analysis its imagined that sample is drawn from a joint distribution of all the variables
    -   in causal inf, imagine intervening to change Z, treatment, independent of X, confounders.
    -   Frequentist method: G-Estimation
        -   Models
            -   propensity score models, $b(x;\gamma)$
                -   Equivalent to $\text{Pr} [Z = 1 |x]$

                -   Estimating Equation for $\gamma$

                    $$
                    \sum \limits_{i=1}^n x_i^T (z_i - b(x_i; \gamma)) = 0
                    $$
            -   treatment free mean model, $\mu_0(x; \beta)$
                -   Used for doubly robust estimation
            -   treatment effect (or blip) model $\tau z$, which can be extended to $z\mu_1(x;\tau)$
        -   Propensity Score Regression\
            \$\$\
            Y = Z \\tau + b(X; \\hat{\\gamma}) \\phi + \\epsilon\
            \$\$
            -   $\phi$ is the estimated coefficient for the propensity score model
    -   Bayesian
        -   There are other ways but this procedure is recommended
        -   Perform full Bayesian estimation of $\gamma$ , plug that (best) estimate into the propensity score model, $b(x_i; \hat{\gamma})$ , and then perform Bayesian analysis of $\tau$ (i.e. propensity score regression)
            -   The propensity score model part of the formula is basically a hack and not mimmicking any part of the dgp therefore for bayesians, the regression model is a misspecification.\
