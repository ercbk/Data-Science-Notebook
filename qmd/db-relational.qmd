# Relational  {#sec-db-rel}

## Misc  {#sec-db-rel-misc}

-   When you start to have multiple datasets or when you want to make use of several columns in one table and other columns in another table you should consider going the local database route.
-   Use db "normalization" (that thing adam was doing?) to figure out a schema
-   Optimized for a mix of read and write queries that insert/select a small number of rows at a time and can handle up to 1TB of data reasonably well.
-   The main difference between a "relational database" and a "data warehouse" is that the former is created and optimised to "record" data, whilst the latter is created and built to "react to analytics".
-   Types
    -   Embedded aka In-Process (see [Databases, Engineering \>\> Terms](db-engineering.qmd#sec-db-eng-terms){style="color: green"}): DuckDB (analytics) and SQLite (transactional)
    -   Server-based: postgres, mysql, SQL Server
        -   Mix of transactional and analytical
        -   Distributed SQL (database replicants across regions or hybrid (on-prem + cloud)
            -   mysql, postgres available for both in AWS Aurora (See below)
            -   postgres available using [yugabytedb](https://www.yugabyte.com/)
            -   SQL Server on Azure SQL Database
            -   Cloud Spanner on GCP
-   SQLite vs MySQL as transactional dbs ([article](https://towardsdatascience.com/mysql-vs-sqlite-ba40997d88c5))
    -   SQLite:
        -   embedded, size \~600KB
        -   limited data types
        -   being self-contained, other clients on a network would not have access to the database (no multi-users) unlike with MySQL
        -   no built-in authentication that is supported
        -   multiple processes are able to access the database at the same time, but making changes at the same time is not something supported
        -   Use Cases
            -   Data being confined in the files of the device is not a problem
            -   Network access to the db is not needed
            -   Applications that will minimally access the database and not require heavy calculations
    -   MySQL:
        -   opposites of the sqlite stuff
        -   Size \~600MB
        -   supports replication and scalability
        -   Security is a large; built-in features to keep unwanted people from easily accessing data
        -   Use cases
            -   transactions are more frequent like on web or desktop applications
            -   if network capabilities are a must
            -   multi-user access and therefore security and authentication
            -   large amounts of data
-   Wrapper for db connections (e.g. `con_depA <- connect_databaseA(username = ..., password = ...)` )
    
    ```r         
    # ... other stuff including code for "connect_odbc" function
    
    # connection attempt loop
    while(try < retries) {
        con <- connect_odbc(source_db = "<database name>"
                            username = username,
                            password = password)
        if(class(con) == "NetexxaSQL") {
            try <- retries + 1
        } else if (!"NetezzaSQL" %in% class(con) & try < retries {
            warning("<database name> connection failed. Retrying...")
            try <- try + 1
            Sys.sleep(retry_wait)
        } else {
            try <- try + 1
            warning("<database name> connection failed")
        }
    }
    ```

    -   Guessing "NetezzaSQL" is some kind of error code for a failed connection to the db

-   MySQL

    -   Installation [docs](https://dev.mysql.com/doc/mysql-getting-started/en/)
    -   Basic [intro](https://towardsdatascience.com/an-introduction-to-sql-4c9eb27995df)
    -   See SQL notebook

-   [Cloud SQL](https://cloud.google.com/sql/docs/introduction) - Google service to provide hosting services for relational dbs (see [Google, BigQuery \>\> Misc](google-bigquery.qmd#sec-goog-bigq-misc){style="color: green"}). Can use postgres, mysql, etc. on their machines.

    -   [Cloud SQL Insights](https://cloud.google.com/blog/products/databases/get-ahead-of-database-performance-issues-with-cloud-sql-insights) - good query optimization tool

-   [AWS RDS](https://aws.amazon.com/rds/) for db instances (see [Database, postgres \>\> AWS RDS](db-postgres.qmd#sec-db-pstgr-rds){style="color: green"})

    -   Available: Amazon Aurora, MySQL, MariaDB, postgres, Oracle, Microsoft SQL Server
    -   RDS (Relational Database Service)
        -   Benefits over hosting db on EC2: AWS handles scaling, availability, backups, and software and operating system updates
    -   **S3** is like googledrive or dropbox
        -   Con: only contains data *about* the files, not what's inside them, i.e. no querying
        -   Ideal use cases
            -   backup for logs,
            -   raw sensor data for your IoT application,
            -   text files from user interviews
            -   images
            -   trained machine learning models (with the database simply storing the path to the object)

-   [AWS Aurora](http://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Aurora.html) - MySQL- and PostgreSQL-compatible enterprise-class database

    -   starting at \<\$1/day.
    -   supports up to 64TB of auto-scaling storage capacity, 6-way replication across three availability zones, and 15 low-latency read replicas.
    -   [Create MySQL and Postgres instances using AWS Cloudformation](https://towardsdatascience.com/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a)

-   AWS DynamoDB - for creating and querying NoSQL databases.

-   Relational databases do not keep all data together but split it into multiple smaller tables. That separation into sub-tables has several advantages:

    -   all information is stored only once, avoiding repetition and conserving memory
    -   all information is updated only once and in one place, improving consistency and avoiding errors that may result from updating the same value in multiple locations
    -   all information is organized by topic and segmented into smaller tables that are easier to handle

-   Separation of data, thus, helps with data quality, and explains the continuing popularity of relational databases in production-level data management.

-   dplyr

    -   `compute` stores results in a remote temporary table
    -   `collect` retrieves data into a local tibble.
    -   `collapse` doesn't force computation, but instead forces generation of the SQL query.
        -   sometimes needed to work around bugs in dplyr's SQL generation.

-   [{dm}]{style="color: #990000"}

    -   Can join multiple tables from a db, but keeps the meta info such as table names, primary and foreign keys, size of original tables etc.

-   SQLite

    -   [{RSQLite}]{style="color: #990000"}

-   Apache Avro

    -   row storage file format unlike parquet
    -   a single Avro file contains a JSON-like schema for data types and the data itself in binary format
    -   4x *slower* reading than csv but 1.5x *faster* writing than csv
    -   1.7x *smaller* file size than csv

-   Benchmarks

    -   Example
        -   Data
            -   \~54,000,000 rows and 6 columns
            -   10 .rds files with gz compression is 220MB total,
                -   If they were .csv, 1.5 GB
            -   SQLite file is 3 GB
            -   DuckDB file is 2.5 GB
            -   Arrow creates a structure of directories, 477 MB total
        -   Operation: read, filter, group_by, summarize
        -   Results
            
            ```         
            ##  format          median_time mem_alloc
            ##  <chr>              <bch:tm> <bch:byt>
            ## 1 R (RDS)              1.34m    4.08GB
            ## 2 SQL (SQLite)          5.48s    6.17MB
            ## 3 SQL (DuckDB)          1.76s  104.66KB
            ## 4 Arrow (Parquet)      1.36s  453.89MB
            ```

-   Tradional relational db solutions balloon up the file size
    -   SQLite 2x, DuckDB 1.66x (using csv size)
-   Indexing on SQLite and DuckDB and partitioning with Arrow help with speed

## Transitioning from Spreadsheet to DB  {#sec-db-rel-transspr}

-   Misc
    -   When you start to have multiple datasets or when you want to make use of several columns in one table and other columns in another table you should consider going the local database route.
    -   Use db "normalization" to figure out a schema
    -   Also see
        -   [Databases, Engineering >> Schema](db-engineering.qmd#sec-db-eng-schema){style="color:green"}
        -   [Databases, Warehouses >> Design a Warehouse](db-warehouses.qmd#sec-db-ware-dsgn){style="color: green"}
-   DB advantages over spreadsheets:
    -   Efficient analysis: Relational databases allow information to be retrieved quicker to then be analyzed with SQL (Structured Query Language), to then run queries.
        -   Once spreadsheets get large, they can lag or freeze when opening, editing, or performing simple analyses in them.
    -   Centralized data management: Since relational databases often require a certain type or format of data to be input into each column of a table, it's less likely that you'll end up with duplicate or inconsistent data.
    -   Scalability: If your business is experiencing high growth, this means that the database will expand, and a relational database can accommodate an increased volume of data.
-   Start documenting the spreadsheets
    -   file names, file paths
    -   Understand where values are coming from
        -   source (e.g. department, store, sensor), owner
    -   How rows of data are being generated
        -   who/what is inputting the data
    -   How does each spreadsheet/notebooks/set of spreadsheets fit in the company's business model
        -   How are they being used and by whom
    -   Map the spreadsheets relationships to one another
        -   See [Databases, Warehouses >> Design a Warehouse](db-warehouses.qmd#sec-db-ware-dsgn){style="color: green"}

## Normalization  {#sec-db-rel-norm}

-   Organizing according to data attributes to reduce or eliminate data redundancy (i.e. having the same data in multiple places).
    -   It gives you a set of rules to be able to start categorizing your data and forming a layout
-   By establishing structure in a database, you are able to help establish a couple of important things: data integrity and scalability.
    -   Integrity ensures that data is entered correctly and accurately.
    -   Scalability ensures you have organized the data in a way that it is more computationally efficient when you start to run SQL queries.
-   Misc
    -   Notes from [When Spreadsheets Aren't Good Enough: A Lesson in Relational Databases](https://towardsdatascience.com/when-spreadsheets-arent-good-enough-a-lesson-in-relational-databases-2e5b0b847f5a)
        -   Gives an example of normalizing a dataset through a MySQL analysis
    -   Packages
        -   [{{]{style="color: goldenrod"}[autonormalize](https://github.com/FeatureLabs/autonormalize){style="color: goldenrod"}[}}]{style="color: goldenrod"} - analyzes transaction df and creates relational tables - python library for automated dataset normalization
-   Forms
    -   Databases are often considered as "normalized" if they meet the third normal form
    -   See [A Complete Guide to Database Normalization in SQL](https://towardsdatascience.com/a-complete-guide-to-database-normalization-in-sql-6b16544deb0) for details on the other 4 forms.
        -   Also gives an example of normalizing a dataset through a posgresSQL analysis
    -   First normal form (1NF)
        -   Every value in each column of a table must be reduced to its most simple value, also known as atomic.
            -   An atomic value is one where there are no sets of values within a column. (i.e. 1 value per cell)
        -   There are no repeating columns or rows within the database.
        -   Each table should have a primary key which can be defined as a non-null, unique value that identifies each row insertion. Second normal form (2NF)
        -   Conforms to first normal form rules.
        -   Adjust columns so that each table only contains data relating to the primary key.
        -   Foreign keys are used to establish relationships between tables. Third normal form (3NF)
        -   Conforms to both first and second normal form rules.
        -   Necessary to shift or remove columns (attributes) that are transitively dependent, which means they rely on other columns that aren't foreign or primary keys.
