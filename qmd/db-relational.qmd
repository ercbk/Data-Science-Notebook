# Relational {#sec-db-rel .unnumbered}

## Misc {#sec-db-rel-misc .unnumbered}

-   Packages

    -   [{dplyr}]{style="color: #990000"}
        -   `compute` stores results in a remote temporary table
        -   `collect` retrieves data into a local tibble.
        -   `collapse` doesn't force computation, but instead forces generation of the SQL query.
            -   sometimes needed to work around bugs in dplyr's SQL generation.
    -   [{dm}]{style="color: #990000"}
        -   Can join multiple tables from a db, but keeps the meta info such as table names, primary and foreign keys, size of original tables etc.

-   Relational databases do not keep all data together but split it into multiple smaller tables. That separation into sub-tables has several advantages:

    -   All information is stored only once, avoiding repetition and conserving memory
    -   All information is updated only once and in one place, improving consistency and avoiding errors that may result from updating the same value in multiple locations
    -   All information is organized by topic and segmented into smaller tables that are easier to handle

-   Optimized for a mix of read and write queries that insert/select a small number of rows at a time and can handle up to 1TB of data reasonably well.

-   The main difference between a "relational database" and a "data warehouse" is that the former is created and optimized to "record" data, whilst the latter is created and built to "react to analytics".

-   Types

    -   Embedded aka In-Process (see [Databases, Engineering \>\> Terms](db-engineering.qmd#sec-db-eng-terms){style="color: green"}): DuckDB (analytics) and SQLite (transactional)
    -   Server-based: postgres, mysql, SQL Server
        -   Mix of transactional and analytical
        -   Distributed SQL (database replicants across regions or hybrid (on-prem + cloud)
            -   mysql, postgres available for both in AWS Aurora (See below)
            -   postgres available using [yugabytedb](https://www.yugabyte.com/)
            -   SQL Server on Azure SQL Database
            -   Cloud Spanner on GCP

-   Wrapper for db connections (e.g. `con_depA <- connect_databaseA(username = ..., password = ...)` )

    ``` r
    # ... other stuff including code for "connect_odbc" function

    # connection attempt loop
    while(try < retries) {
        con <- connect_odbc(source_db = "<database name>"
                            username = username,
                            password = password)
        if(class(con) == "NetexxaSQL") {
            try <- retries + 1
        } else if (!"NetezzaSQL" %in% class(con) & try < retries {
            warning("<database name> connection failed. Retrying...")
            try <- try + 1
            Sys.sleep(retry_wait)
        } else {
            try <- try + 1
            warning("<database name> connection failed")
        }
    }
    ```

    -   Guessing "NetezzaSQL" is some kind of error code for a failed connection to the db

-   Benchmarks

    -   Example
        -   Data

            -   \~54,000,000 rows and 6 columns
            -   10 .rds files with gz compression is 220MB total,
                -   If they were .csv, 1.5 GB
            -   SQLite file is 3 GB
            -   DuckDB file is 2.5 GB
            -   Arrow creates a structure of directories, 477 MB total

        -   Operation: read, filter, group_by, summarize

        -   Results

            ```         
            ##  format          median_time mem_alloc
            ##  <chr>              <bch:tm> <bch:byt>
            ## 1 R (RDS)              1.34m    4.08GB
            ## 2 SQL (SQLite)          5.48s    6.17MB
            ## 3 SQL (DuckDB)          1.76s  104.66KB
            ## 4 Arrow (Parquet)      1.36s  453.89MB
            ```
    -   Tradional relational db solutions balloon up the file size
        -   SQLite 2x, DuckDB 1.66x (using csv size)

## Brands {#sec-db-rel-brands .unnumbered}

-   SQLite vs MySQL as transactional dbs ([article](https://towardsdatascience.com/mysql-vs-sqlite-ba40997d88c5))

    -   SQLite:
        -   embedded, size \~600KB
        -   limited data types
        -   being self-contained, other clients on a network would not have access to the database (no multi-users) unlike with MySQL
        -   no built-in authentication that is supported
        -   multiple processes are able to access the database at the same time, but making changes at the same time is not something supported
        -   Use Cases
            -   Data being confined in the files of the device is not a problem
            -   Network access to the db is not needed
            -   Applications that will minimally access the database and not require heavy calculations
    -   MySQL:
        -   opposites of the sqlite stuff
        -   Size \~600MB
        -   supports replication and scalability
        -   Security is a large; built-in features to keep unwanted people from easily accessing data
        -   Use cases
            -   transactions are more frequent like on web or desktop applications
            -   if network capabilities are a must
            -   multi-user access and therefore security and authentication
            -   large amounts of data

-   MySQL

    -   Installation [docs](https://dev.mysql.com/doc/mysql-getting-started/en/)
    -   Basic [intro](https://towardsdatascience.com/an-introduction-to-sql-4c9eb27995df)
    -   See SQL notebook

-   [Cloud SQL](https://cloud.google.com/sql/docs/introduction) - Google service to provide hosting services for relational dbs (see [Google, BigQuery \>\> Misc](google-bigquery.qmd#sec-goog-bigq-misc){style="color: green"}). Can use postgres, mysql, etc. on their machines.

    -   [Cloud SQL Insights](https://cloud.google.com/blog/products/databases/get-ahead-of-database-performance-issues-with-cloud-sql-insights) - good query optimization tool

-   [AWS RDS](https://aws.amazon.com/rds/) for db instances (see [Database, postgres \>\> AWS RDS](db-postgres.qmd#sec-db-pstgr-rds){style="color: green"})

    -   Available: Amazon Aurora, MySQL, MariaDB, postgres, Oracle, Microsoft SQL Server
    -   RDS (Relational Database Service)
        -   Benefits over hosting db on EC2: AWS handles scaling, availability, backups, and software and operating system updates
    -   **S3** is like googledrive or dropbox
        -   Con: only contains data *about* the files, not what's inside them, i.e. no querying
        -   Ideal use cases
            -   backup for logs,
            -   raw sensor data for your IoT application,
            -   text files from user interviews
            -   images
            -   trained machine learning models (with the database simply storing the path to the object)
        -   Alternative: [**Minio**](https://min.io/)
            -   Open-Source alternative to AWS S3 storage.
            -   Given that S3 often stores customer PII (either inadvertently via screenshots or actual structured JSON files), Minio is a great alternative to companies mindful of who has access to user data.
                -   Of course, [AWS claims that AWS personnel](https://aws.amazon.com/compliance/privacy-features/#:~:text=We%20prohibit%2C%20and%20our%20systems,or%20to%20comply%20with%20law.) doesn’t have direct access to customer data, but by being closed-source, that statement is just a function of trust.

-   [AWS Aurora](http://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Aurora.html) - MySQL- and PostgreSQL-compatible enterprise-class database

    -   starting at \<\$1/day.
    -   supports up to 64TB of auto-scaling storage capacity, 6-way replication across three availability zones, and 15 low-latency read replicas.
    -   [Create MySQL and Postgres instances using AWS Cloudformation](https://towardsdatascience.com/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a)

-   AWS DynamoDB - for creating and querying NoSQL databases.

-   SQLite

    -   [{RSQLite}]{style="color: #990000"}

-   Apache Avro

    -   row storage file format unlike parquet
    -   a single Avro file contains a JSON-like schema for data types and the data itself in binary format
    -   4x *slower* reading than csv but 1.5x *faster* writing than csv
    -   1.7x *smaller* file size than csv
