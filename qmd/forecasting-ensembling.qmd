# Forecasting, Ensembling

Misc

* Statistical ensemble nearly as good as a DL ensemble and was much faster and cheaper. ([Thread](https://twitter.com/MergenthalerMax/status/1598397618572259328))
	* 4 models used in the ensemble
		* AutoARIMA and Exponential Smoothing ([{forecast}]{style='color: #990000'})
		* Complex Exponential Smoothing ([{smooth}]{style='color: #990000'})
		* Dynamically Optimized Theta method ([{{StatsForecast}}]{style='color: goldenrod'})



Diagnostics

* residual testing
	* Hyndman suggests taking average degrees of freedom (dof) of the models included in the ensemble to use for residual tests (?)



Averaging (equal weighting)

* CDC ensemble: https://github.com/reichlab/covid19-forecast-hub#ensemble-model
	* each model has a forecast distribution for each horizon
		* "23 quantiles to be submitted for each of the one through four week ahead values for forecasts of deaths, and a full set of 7 quantiles for the one through four week ahead values for forecasts of cases (see technical README for details), and that the 10th quantile of the predictive distribution for a 1 week ahead forecast of cumulative deaths is not below the most recently observed data."
	* used the median prediction across all eligible models at each quantile level



Adaptive Ensembling

* Amaal Saadallah
	* Video http://whyr.pl//foundation/2021/amal/
	* Github https://github.com/AmalSd/DEMSC
	* She also has paper that uses Reinforcement Learning with actor-critic approach to compute weights, EADRL
	* ![](./_resources/Forecasting,_Ensembling.resources/Screenshot (142).png)
	* 
* Steps
	1. Drift Detection
		* Drift from stationarity detected which triggers a re-selection of base models and ensemble
		* Calculating drift
			1. Select a testing window with most recent data
			2. Calculate predictions using each base model from the ensemble for this testing window
			3. Calculate scaled root correlation (SRC) between each model's predictions and the testing window observations ![](./_resources/Forecasting,_Ensembling.resources/ql_f9aef97973d3e7b87e239df209efb48e_l3.png)
				* Ŷ is the predictions for model, Mi, and testing window W,t.
				* Y is testing window observations for testing window W,t
				* corr is the pearson correlation
			4. Hoeffding Bound![](./_resources/Forecasting,_Ensembling.resources/ql_2215d3fd4df6738abcecdfa38f1fe7a1_l3.png)
		
		* R is the range of the random variable
		* W is the number of observations of the testing window used to calculate the SRC
		* δ is a user-defined hyperparameter
		
		* 
		
	2. Prune base models
		* CV
		* remove the awful ones
	3. Cluster pruned set of models based on predictions
		* By selecting a model from each cluster, you get a diversified set of models for the ensemble
			* By reducing covariance between models in your ensemble, it reduces the variance in the ensemble's residuals (my notes)
		* The representative model from each cluster is selected for the ensemble
		* Suggests using an Improper Maximum Likelihood Estimator Clustering (IMLEC) method
			* see paper \[Coretto and Hennig, 2016\]
			* Says Euclidean distance is bad. This method uses covariance to form clusters which fits with trying to get a set of time series with maximum variance for the ensemble
			* otrimle PKG
				* hyperparameters automatically tuned; outliers removed
				* Seems to be a robust gaussian mixture clustering algorithm
			* Since this is a GMM you could have models belonging to more than 1 cluster I assume.
				* If 1 model is chosen as representative for more than 1 cluster than probably wouldn't matter too much. Maybe less redundancy.
				* if 1 model is in more than 1 cluster but not chosen as representative in both, it might be interesting to know that. Maybe means there's some redundancy in your ensemble but not too much. Might want to score an ensemble with only 1 of cluster representatives and see if it's better.
		* She has a DTW option but seems to prefer this IMLEC method
			* showed some RMSE stats where the dtw method had a 50 RMSE and IMLEC had a 43
			* used partitioning for a clustering method but no tuning was involved
		* 
	4. Combine models predictions
		* sliding window average, averaging, stacking, metalearner

