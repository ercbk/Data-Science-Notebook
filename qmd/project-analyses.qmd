# Analyses {#sec-proj-anal .unnumbered}

## General {#sec-proj-analy-gen .unnumbered}

-   [General Questions]{.underline}
    -   "What variables are relevant to the problem I'm trying to solve?"
    -   "What are the key components of this data set?"
    -   "Can this data be categorized?"
    -   "Is this analysis result out of the ordinary?"
    -   "What are the key relationships?"
    -   "Is this the best way this company could be carrying out this task?"
    -   "What will happen under new conditions?"
    -   "What factors are best used to determine or predict this eventuality?"
-   [Break down the problem into parts and focus on those during EDA]{.underline}
    -   Also see [Decison Intelligence \>\> Mental Models](decision-intelligence.qmd#sec-decint-mentmod){style="color: green"} for details on methods to break down components
    -   [Example]{.ribbon-highlight}: Why are sales down?
        -   How are sales calculated?
            -   e.g. Total Sales = \# of Orders \* Average Order Value
        -   Breakdown \# of orders and average order value
            -   number of orders = number of walk-ins \* % conversion
                -   Has walk-ins or conversion declined?
            -   Average Order Value
                -   Bin avg order value by quantiles, plot and facet or group by binned groups. Is one group more responsible for the decline than others?
        -   Is there regional or store or brand variability? (grouping variables)
-   [Drill down into each component until the data doesn't allow you to go any farther.]{.underline}
-   [Segment data by groups]{.underline}
    -   Color or facet by cat vars
    -   Pay attention to counts of each category (may need to collapse categories)
    -   Common segments in product analytics
        -   Free vs Paid users
        -   Device Type (desktop web vs mobile web vs native app)
        -   Traffic Source (people coming from search engines, paid marketing, people directly typing in your company's URL into their browser, etc.)
        -   Day of the Week.

## TROPICS framework {#sec-proj-anal-tropf .unnumbered}

-   [Misc]{.underline}
    -   For analyzing changes in key performance metrics
    -   From <https://towardsdatascience.com/answering-the-data-science-metric-change-interview-question-the-ultimate-guide-5e18d62d0dc6>
    -   Components: Time, Region, Other Internal Products, Platform, Industry and Competitors, Cannibalization, Segmentation
-   [Time]{.underline}
    -   What to explore
        -   How has our performance been **trending** over the last few weeks (or months)?
            -   Example: *If we saw a 10% increase in the last week, was the percentage change in the weeks before also 10%? In which case the 10% may actually be pretty normal? Or was the change lower? Higher?*
        -   Is this change [**seasonal**](https://www.sequoiacap.com/article/metrics-seasonal-factors#:~:text=Seasonality%20is%20often%20the%20root,behavioral%20change%20factor%20to%20explore.&text=You%20might%20then%20hypothesize%20that,or%20negative%E2%80%94of%20the%20product.)? Do we see the same spike around this time each year?
            -   Example: *Does WhatsApp see a [spike in messages](https://www.flurry.com/blog/messaging-apps-spike-during-virtual-holiday-christmas-new-year/) sent during the holiday season?*
        -   Was the change **sudden or gradual**? Did we see a sudden spike or drop overnight? Or has the metric gradually been moving in this direction over time?
            -   Example: *If product usage jumps by 50% overnight could there be a bug in our logging systems?*
        -   Are there **specific times** during the day or week where this change is more pronounced?
    -   Solution examples
        -   If the change is seasonal then there may not necessarily be anything you need to 'solve' for. But, you can leverage this to your advantage.
            -   Example: *Amazon sales may jump up on Black Friday so they would want to make sure they have the proper infrastructure in place so the [site doesn't crash](https://www.theverge.com/2018/7/16/17577654/amazon-prime-day-website-down-deals-service-disruption). They may also see if there are certain types of products that are popular purchases and increase their inventory accordingly.*
        -   If there is a sudden decline, there may be **a bug in the logging** or a **new feature or update** recently launched that's creating problems that you may need to roll back.
        -   If there's a gradual decline, it may indicate a **change in user behavior**.
            -   Example: *If the time spent listening to music is declining because people prefer to listen to podcasts then [Spotify may want to focus more of their content inventory on podcasts.](https://techcrunch.com/2019/01/10/spotifys-increased-focus-on-podcasts-in-2019-includes-selling-its-own-ads/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAIKqPepZqERJ58OeY27wAkiohH_MUbrRZIUzwNaqErpdLFt9sV_a07V0dfOR1ftRdoOqC68hZWcC8fMQacip0FkeKYwvSohInKqI44RewmFC582isa0WTvpOXA72LBFOF2a0TTX7ZYiFX4tTO_foSUaBWJ7-Up4D3llAUjwQIpPM)*
-   [Region]{.underline}
    -   What to explore
        -   Is this change concentrated in a **specific region** or do we see a similar change across the board?
    -   Solution examples
        -   There may be **newly enforced regulations** in countries that are affecting your product metrics. You would need to do further research to assess the impacts of these regulations and potential workarounds.
            -   Example: [*Uber was temporarily banned in London in 2019*](https://www.nytimes.com/2019/11/25/business/uber-london.html) *for repeated safety failures* which resulted in a series of lawsuits and court cases.
        -   **Popular local events** may also be potential explanations. While these may not be areas to 'solve' for they can be opportunities to take advantage of.
            -   Example: *Coachella season means a jump in the number of Airbnb bookings in Southern California that are capitalized on by surge pricing.*
-   [Other Internal Products]{.underline}
    -   What to explore
        -   Is this change **specific to one product or is it company-wide**? How does this metric vary across our other product offerings?
            -   Example: *If the Fundraising feature on Facebook is seeing increased usage, is the swipe up to donate feature on Instagram (which Facebook owns) also seeing a similar uptick?*
        -   Are there **other metrics** that have also changed in addition to the one in question?
            -   Example: *If the time spent on Uber is going down, is the number of cancellations by drivers also declining (implying people are spending less time on the app because they're having a more reliable experience)?*
    -   Solution examples
        -   If there is a metric change across our other features and products, it's likely a larger problem we should address with multiple teams and may need a Public Relations consultant.
            -   Example: [*Elon + Twitter.*](https://www.cnbc.com/2021/01/29/elon-musks-tweets-are-moving-markets.html#:~:text=He%20infamously%20tweeted%20last%20year,cryptocurrencies%20for%20a%20while%20now.)
-   [Platform]{.underline}
    -   What to explore
        -   Mobile vs Desktop?
        -   Mac vs Windows?
        -   Android vs iOS?
    -   Solution examples
        -   If there was a positive change in our metric on a specific platform (e.g. iOS) and coincides with an (iOS) update we released, we would want to do a retrospective to determine **what about that update was favorable so we can double down on it.** Alternatively, if the metric change was negative, we may want to **reconsider and even roll back the update.**
        -   If the change was due to a change in the platform experience **(e.g. app store placement, ratings)** we may want to seek advice from our marketing team since this is a top of the funnel problem
        -   If users are showing astrong preference for a specific platform, we want to make sure that the experience of the preferred platform is up to par. We also need to make sure our platform-specific monetization strategies are switching to follow the trend.
            -   Example: [*Facebook's ad model was initially tied to the desktop app only and had to be expanded as mobile became the platform of preference.*](https://blog.hubspot.com/marketing/history-facebook-adtips-slideshare)
-   [Industry & Competitors]{.underline}
    -   What to explore
        -   When our decline began, **was there a new competitor or category that emerged?**
            -   Example: [*Did the number of users listening to Apple podcasts go down when Clubhouse came on to the scene?*](https://www.vulture.com/2021/02/does-clubhouse-mean-bad-things-for-podcasting.html)
        -   Have competitors changed their offering lately?
        -   Is the category as a whole declining?
    -   Solution examples
        -   If the category is shifting as a whole, we should begin looking at larger-scale changes to the app.
            -   Example: *What [Kodak](https://www.forbes.com/sites/chunkamui/2012/01/18/how-kodak-failed/?sh=ad644166f27a) should have done.*
        -   If there's a new competitor taking our market share, we can begin with reactivation campaigns on churned users. We may also want to conduct user research to understand the gap between our offering and those of our competitors
-   [Cannibalization]{.underline}
    -   What to explore
        -   Are **other products or features** in our offering experiencing **growth** in the face of our **decline** or vice versa?
        -   Have we **released a new feature that is drawing users away from our old features?** If so, can we fully attribute the release of the new feature with the decline in the metric of our feature in question?
            -   Example: *When Facebook released reactions, did the number of comments on a post go down because people found it easier to press a react button instead of writing a comment?*
    -   Solution examples
        -   Cannibalization may not necessarily be a bad thing. We need to determine whether this shift in user interest across our features is favorable by determining whether the new features align better with the goals of the business.
        -   Cannibalization may also be an indication of but it is indicative of a change in user behavior. In which case we may want to consider if perhaps our core metrics need to change as user behaviors change.
            -   Example: *If users care more about watching Instagram stories than engaging with the Instagram feed we may want to optimize for retention (because the ephemeral nature of stories is more likely to motivate users to keep coming back to the platform) instead of time spent on the app.*
        -   We can also look at ways to bridge the two features together to create a more unified platform.
-   [Segmentation]{.underline}
    -   What to explore
        -   How does this metric **vary by user type**:
            -   Age, sex, education
            -   Power users versus casual users \* New users versus existing users
        -   How does this metric vary by **different attributes of the product:**
            -   Example: *If the time spent watching YouTube videos is going down, is it across longer videos or shorter clips? Is it only for DIY videos or interview tutorial content? Is the same number of people that started watching a video the same but a large chunk of them stop watching it halfway through?*
    -   Solution examples
        -   If the metric varies between new and existing users then maybe there is a overcrowding effect.
            -   Example: *Reddit forums could hit a critical mass where new users feel lost and less likely to engage than existing users resulting in a drop in engagements per user*
        -   If users are dropping off at certain parts of the funnel then maybe the experience at that funnel step is broken.
            -   Example: *While the same number of people are starting carts on Amazon there may be a drop in purchases if the payment verification system isn't working.*

## Actionable Analyses {#sec-proj-anal-actanl .unnumbered}

-   Notes from: [Driving Product Impact With Actionable Analyses](https://towardsdatascience.com/driving-product-impact-with-actionable-analyses-d72430684908)
-   Actionable insights do not only provide a specific data point that might be interesting, but lay out a clear narrative how this insight is connected to the problem at hand, what the ramifications are, as well as possible options and next steps to take with the associated benefits/risks of (not) acting upon these.
-   [Not Actionable]{.underline}: *Users under the age of 25 hardly use audiobooks.*
    -   Is this good, bad? Should they be listening to audiobooks and is there anything we should do about it?
-   [Actionable]{.underline}: *Users under the age of 25 hardly use audiobooks because they never explore the feature in the app. However users who listen to audiobooks have a 20% higher retention rate.*
    -   This information tells us that audiobooks represent a potential opportunity to increase retention amongst younger users, however there seems to be more work to be done to encourage users exploring this feature.
-   Steps
    -   [Problem Statement]{.underline}: High-level business problem to solve (e.g. Increasing Retention, Conversion Rate, Average Order Value)
        -   Can also be in regards to a metric that's believed to be highly associated with a North Star metric like a Primary metric (See [KPIs](https://ercbk.github.io/Domain-Knowledge-Notebook/qmd/kpis.html){style="color: green"})
    -   [Opportunity Areas]{.underline}: Areas or problems with a strong connection to the problem at hand
        -   Investigate behaviors of users with the behavior that you're interested in (i.e. high or low values of the desired metric).

        -   Discovering the characteristics of these users can help to figure out ways to encourage other users to act similarily or gain insight into the type of users you want to attract.
    -   [Levers]{.underline}: Different ways to work on the opportunity areas
        -   A lever should be data-based and able to be validated on whether working to increase or decrease the lever will lead to a positive solution to the problem statement.
        -   There are typically multiple levers for a given opportunity area
            -   These should be ordered in terms of priority, and priority should be given to the lever that is believed to result in the greatest impact on the opportunity area that will result in the greatest impact on the solution to the problem statement.
    -   [Experiments]{.underline} \[Optional\]: Concrete implementation of a specific lever that can help prove/disprove our hypotheses.
        -   Optional but always helpful to convey recommendations and suggestions with concrete ideas for what the team could or should be building.
-   [Example]{.ribbon-highlight}\
    ![](_resources/Project-Analyses.resources/act-ex-breakdown-1.webp){width="632"}
    -   Problem Statement: *How can we increase daily listening time for premium users in the Spotify app?*
        -   Hypothesis: Daily Listening Time is strongly connected to retention for premium users and hensce to monthly revenue.
    -   Opportunity Areas:
        -   *Users who use auto-generated playlists have a x% higher daily listening time*
        -   *Users who subscribed to at least 3 podcasts have a x% higher listening time per day than those who did not subscribe to any*.
        -   *Users who listen to audiobooks have a x% higher daily listening time.*
    -   Levers:
        -   Opportunity Area: Increase the percentage of users under 25 using audiobooks from x% to y%.
        -   Questions:
            -   Do users not see the feature?
            -   Do users see the feature but don't engage with the feature?
            -   Do users engage with the feature but drop off after a short amount of time?
        -   Finding: Users under 25 engage less with the Home Screen, the only screen where Audiobooks are promoted, and hence don't see this feature in the App. This is likely leading low usage and engagement.
        -   Lever: *Increase prominence of Audiobooks within the app*
        -   Prioritzation Table for Report\
            ![](_resources/Project-Analyses.resources/act-ex-prior-1.webp){width="532"}
    -   Experiments:\
        ![](_resources/Project-Analyses.resources/act-ex-ab-1.jpg){width="232"}
        -   "We predict that adding a banner promoting Audiobooks when the App opens \[Experiment Change\] will increase younger users' daily listening time \[Problem\] because more younger users will see and listen to Audiobooks \[Lever\]. We will know this is true when we see an increase in young users using Audiobooks \[Lever\], followed by an increase in the daily listening time for younger users \[Validation Metrics\]."
        -   If there is no significant increase in audiobook usage, then there many other ways to increase the visibility of a feature which can be the hypotheses of further experiments.
        -   If , however, there is a significant increase in users using Audiobooks (lever) but no effect on daily listening time (main problem), then the lever is invalidated and we can move on to the next one.
