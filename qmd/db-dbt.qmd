# dbt {#sec-db-dbt .unnumbered}

## Overview {#sec-db-dbt-over .unnumbered}

-   A command-line tool that transforms data in your warehouse and brings software engineering principles to the world of SQL.
-   Features
    -   Seamless integration across other tools in your data stack
    -   Developer-friendly experience for data transformation
    -   Modular design that promotes code reusability and maintainability
    -   Comprehensive testing framework to ensure robust data quality
-   Built for data modeling
    -   **Models** are like sql queries
-   Modularizes SQL code and makes it reusable across "models"\
    ![](./_resources/DB,_dbt.resources/1_uf-Jo4ybZT0qdsCHNZavGg%20(2).png){.lightbox width="332"}
    -   Running the [orders]{.var-text} model also runs the [base_orders]{.var-text} model and [base_payments]{.var-text} model (i.e. dependencies for [orders]{.var-text})
        -   Base models are in the staging layer and output views which means they'll have the most up-to-date data for models in the intermediate and mart layers (final output).
    -   [base_orders]{.var-text} and [base_payments]{.var-text} are independent in that they can also be used in other models
    -   Creates more dependable code because you’re using the same logic in all your models
-   You can schedule running sets of models by tagging them (e.g. #daily, #weekly)
-   Version Control
    -   **Snapshots** provide a mechanism for versioning datasets
    -   Within every yaml file is an option to include the version
-   Package add-ons that allow you to interact with spark, snowflake, duckdb, redshift, etc.
-   Documentation for every step of the way
    -   .yml files can be used to generate a website (localhost:8080) around all of your dbt documentation.\
        ![](./_resources/DB,_dbt.resources/1-xn3o7SBakX5qcFWLks4DCQ.png){.lightbox width="432"}

        ``` bash
        dbt docs generate
        dbt docs serve
        ```

## Misc {#sec-db-dbt-misc .unnumbered}

-   [List](https://docs.getdbt.com/docs/available-adapters) of available DB Adaptors

    -   Runs on Python, so adaptors are installed via pip

-   Notes from

    -   [Anatomy of a dbt project](https://towardsdatascience.com/anatomy-of-a-dbt-project-50e810abc695)
    -   [What is dbt?](https://towardsdatascience.com/what-is-dbt-a0d91109f7d0)

-   Resources

    -   Overview of all DBT Products [Docs](https://docs.getdbt.com/docs/introduction)
    -   DBT-Core Quickstart [Docs](https://docs.getdbt.com/guides/manual-install?step=1)
        -   Shows you how to set up a project locally, connect to a BigQuery warehouse, build and document models
    -   [Create a Local dbt Project](https://towardsdatascience.com/create-local-dbt-project-e12c31bd3992)
        -   Uses docker containers to set up a local dbt project and a local postgres db to play around with

-   Development Workflow

    -   Mock out a design in a spreadsheet so you know you’re aligned with stakeholders on output goals.
    -   Write the SQL to generate that final output, and identify what tables are involved.
    -   Write SQL code for basic transformations (staging/base models) in the staging layer which directly source the tables
    -   Write models in the intermediate layer and/or mart layer using the more complex SQL code which source the models in the staging layer
    -   The final output (materializations) will be the output of the models in the mart layer
    -   Finally, with a functioning model running in dbt:
        -   Start refactoring and optimizing the mart
        -   Add tests and data validation

-   Model Running Workflow

    ``` bash
    dbt deps
    dbt seed
    dbt snapshot
    dbt run
    dbt run-operation {{ macro_name }}
    dbt test
    ```

-   Common Errors ([source](https://github.com/stretcharm/dbt/blob/main/dbt_cheat_sheet.md#common-dbt-errors-and-resolutions))

    | Issue | Example | Resolution |
    |------------------------|------------------------|------------------------|
    | Duplicate Model Names | Two models both called "customers" | Rename one model or reorganize project |
    | Missing Dependencies | `{{ ref('sales') }}` for non-existent model | Ensure referenced model exists |
    | Missing Sources | `{{ source('raw'.'sales') }}` | List sources in sources.yml |
    | Incorrect Database Setup | Wrong schema or database in config | Update dbt_project.yml or profile settings |
    | Syntax or SQL Errors | Forgotten comma or unbalanced parentheses | Check SQL syntax |
    | Missing Configs | No materialization specified | Add config block |
    | Missing Package Dependency | Unrecognized macro | Include in packages.yml, run `dbt deps` |
    | Missing Macro | Undefined macro | Define in macros/ directory |
    | Circular References | Model A references Model B, which references Model A | Remove or refactor references |

-   Style Guide Components

    -   Naming conventions (the case to use and tense of the column names)
    -   SQL best practices (commenting code, CTEs, subqueries, etc.)
    -   Documentation standards for your models
    -   Data types of date, timestamp, and currency columns
    -   Timezone standards for all dates

-   [Castor](https://www.castordoc.com/) - tool that takes your project and autofills much of the documentation\
    ![](./_resources/DB,_dbt.resources/image.3.png){.lightbox width="432"}

    -   Has a free tier
    -   Very helpful if you have the same column name in multiple datasets, you don’t have to keep defining it
    -   Tribal Knowledge
        -   When a dataset is discussed in a team slack channel, Castor pulls the comments and adds them to the documentation of the dataset

-   [Lightdash](https://www.lightdash.com/) - BI tool for dbt projects - free tier for self hosting

-   DBT-Cloud

    -   [Thread](https://bsky.app/profile/bijilsubhash.bsky.social/post/3lerh2sp4sp2z) from experienced DBT-Core user
        -   Likes the extra features if you have the money. Nice for teams with a mix of technical and not-so technical people — enhances collaboration.

## Set-Up {#sec-db-dbt-setup .unnumbered}

-   [Docs](https://docs.getdbt.com/docs/core/installation-overview)
-   Within a python virtual environment
    -   Create: `python3 -m venv dbt-venv`

    -   Activate: `source dbt-venv/bin/activate`

        -   Should be able to see a `(dbt-venv)` prefix in every line on the terminal

    -   Install dbt-core: `pip install dbt-core`

        -   Specific version: `pip install dbt-core==1.3.0`
        -   Confirm installation by checking version: `dbt --version`

    -   Install plugins

        ``` bash
        pip install dbt-bigquery
        pip install dbt-spark
        # etc...
        ```
-   Basic set-up: [Article](https://towardsdatascience.com/getting-hands-on-with-dbt-data-build-tool-a157d4151bbc)
    -   Example uses postgres adaptor
-   Connections
    -   [Docs](https://docs.getdbt.com/docs/core/connect-data-platform/about-core-connections)
    -   [Example]{.ribbon-highlight}: DBT-Core Quickstart Guide (See [Misc](db-dbt-qmd#sec-db-dbt-misc){style="color: green"} \>\> Resources)
        -   Connection to BigQuery in `profiles.yml` (See [Project Files](db-dbt.qmd#sec-db-dbt-projfil){style="color: green"})

            ``` yaml
            jaffle_shop: # this needs to match the profile in your dbt_project.yml file
                target: dev
                outputs:
                    dev:
                        type: bigquery
                        method: service-account
                        keyfile: /Users/BBaggins/.dbt/dbt-tutorial-project-331118.json # replace this with the full path to your keyfile
                        project: grand-highway-265418 # Replace this with your project id
                        dataset: dbt_bbagins # Replace this with dbt_your_name, e.g. dbt_bilbo
                        threads: 1
                        timeout_seconds: 300
                        location: US
                        priority: interactive
            ```

        -   Check connection

            ``` bash
            dbt debug
            #> Connection test: OK connection ok
            ```
-   Environments
    -   dev - Your local development environment; configured in a `profiles.yml` file on your computer.
        -   While making changes with t[arget: dev]{.arg-text}, your objects will be built in your development target without affecting production queries made by your end users
    -   prod - The production deployment of your dbt project, like in dbt Cloud, Airflow, etc.
        -   Once you are confident in your changes and set [target: dev]{.arg-text}, you can deploy the code to production. Running models in this enviroment will affect end user queries.
    -   See [continuous integration](https://docs.getdbt.com/docs/cloud/git/connect-github) for details on using Github, Gitlab with dbt
-   Schemas
    -   Use different schemas within one database to separate your environments
    -   If you have multiple dbt users writing code, it often makes sense for each user to have their own development environment.
        -   Set your dev target schema to be `dbt_<username>`. User credentials should also differ across targets so that each dbt user is using their own data warehouse user.
    -   Custom Schemas ([Docs](https://docs.getdbt.com/docs/build/custom-schemas))
        -   Allows you to build models across multiple schemas and group similar models together
        -   Avoids dbt users creating models in the same schema and overwriting each other's work
        -   Use Cases
            -   Group models based on the business unit using the model, creating schemas such as core, marketing, finance and support.
            -   Hide intermediate models in a staging (database) schema, and only present models that should be queried by an end user in an analytics (database) schema (aka production).
        -   By default, dbt generates the schema name for a model by appending the custom schema to the target schema.
            -   Ensures that objects created in your data warehouse don't collide with one another.
            -   To create a custom macro that overrides this default, see docs
        -   Set-Up
            -   Inside `dbt_project.yml`

                ``` yaml
                # models in `models/marketing/ will be built in the "*_marketing" schema
                models:
                  my_project:
                    marketing:
                      +schema: marketing
                ```

            -   Inside Model

                ``` sql
                {{ config(schema='marketing') }}

                select ...
                ```

## Commands {#sec-db-dbt-comm .unnumbered}

-   Notes from [dbt cheatsheet](https://github.com/stretcharm/dbt/blob/main/dbt_cheat_sheet.md)

-   [Reference](https://docs.getdbt.com/reference/dbt-commands)

-   Core

    -   [dbt run](https://docs.getdbt.com/reference/commands/run) - Runs the models you defined in your project
    -   [dbt build](https://docs.getdbt.com/reference/commands/build) - Builds and tests your selected resources such as models, seeds, snapshots, and tests
    -   [dbt test](https://docs.getdbt.com/reference/commands/test) - Executes the tests you defined for your project

-   Basic Operations

    ``` powershell
    pip install dbt-core dbt-duckdb            # Install the dbt you have installed and adapter(s)
    pip install --upgrade dbt-core dbt-duckdb  # Upgrade the version of dbt you have installed
    dbt --version                                  # Get the version of dbt
    dbt init project_name                         # Setup a new project called project_name
    dbt debug                                     # Check your dbt setup
    pip install sqlfluff sqlfluff-templater-dbt   # Install SQL linter
    dbt deps                                      # Install or update any dept packages in packages.yml
    dbt clean                                     # Remove packages and target folder. Run dbt deps after
    dbt clean                                    # A utility function that deletes all folders specified in the clean-targets list
    dbt docs generate                           # Create the documentation
    dbt serve --port 8081                       # Open the documentation page on port 8081
    dbt run-operation my_macro                  # Run a macro manually. Useful for execution sql that does not build a table
    dbt show --select my_dbt_model             # Show the first 5 rows from the model
    dbt show --inline "select * from {{ ref('my_dbt_model') }}" limit 10  # Show the first 5 rows from the sql query
    dbt source freshness --select source:my_source  # Show if the source table meets the freshness settings
    dbt parse                                   # Quick check of the code integrity without connecting to the database
    dbt list --select tag:staging              # List the models that are in the selection
    dbt test --store-failures                  # Store the failure records for tests in a table
    ```

-   Running or Building

    ``` powershell
    dbt build                                     # Run/seed/test/snapshot all your dbt models
    dbt run                                       # Test all your dbt model tests
    dbt test                                      # Test all your dbt model tests
    dbt run --select my_dbt_model                 # Runs a specific model .sql is optional
    dbt run -s my_dbt_model another_dbt_model.sql # Runs the models .sql is optional. -s is the same as --select
    dbt compile -s my_dbt_model another_dbt_model.sql # Compile the models .sql is optional. -s is the same as --select
    dbt run --select models\risk360\staging       # Runs all models in a specific directory
    dbt run --select tag:nightly                  # Run models with the "nightly" tag
    dbt run --vars 'run_date : "2021123"'         # Set a variable at run time
    dbt run --select tag:staging                  # Run everything tagged as staging
    dbt run --select +my_dbt_model                # Run everything up to and including my_dbt_model
    dbt run --select my_dbt_model+                # Run everything from my_dbt_model and onwards
    dbt run --select my_dbt_model+1               # Run my_dbt_model and one level of onward dependencies
    dbt run --select +my_dbt_model+               # Run everything that is needed for my_dbt_model and everything that uses it
    dbt run -s my_dbt_model                       # Runs a specific model .sql is optional  -s and --select are the same
    dbt run -s my_dbt_model --target dev          # Runs a specific model against target dev
    dbt run -s my_dbt_model -t dev                # Runs a specific model against target dev -t and --target are the same
    dbt run --select my_dbt_model --dry_run       # Compiles against the DB my_dbt_model but without running the code
    dbt run --select my_dbt_model --dry_run --empty # Compiles against the DB my_dbt_model but without running the code and avoids using any source data
    dbt parse                                     # Check your project without connecting to the database
    dbt source freshness                          # Run the freshness tests on your sources
    dbt seed                                      # Run all your dbt seeds
    dbt snapshot                                  # Run all your dbt snapshots
    dbt snapshot --select my_snapshot             # Run selected dbt snapshots
    dbt retry                                     # Rerun only the failed or skipped items from the last run. Use after fixing an issue
    dbt run --select my_dbt_model --full-refresh  # Refresh incremental models fully
    dbt run --select tag:staging --exclude my_dbt_model # Run everything tagged as staging excluding my_dbt_model
    dbt run --select config.materialized:view     # select only view materializations
    ```

## Project Files {#sec-db-dbt-projfil .unnumbered}

-   [Project Templates]{.underline}

    -   [Style Guide](https://github.com/dbt-labs/corp/blob/master/dbt_style_guide.md)
        -   More detailed: [link](https://discourse.getdbt.com/t/how-we-structure-our-dbt-projects/355)
    -   [Example]{.ribbon-highlight} [Starter Project](https://github.com/dbt-labs/dbt-init/tree/master/starter-project)\
        ![](./_resources/DB,_dbt.resources/1_bgUI3LTLdY19EFRwSyZh1A%20(2).png){.lightbox width="115"}

-   `profiles.yml`

    ![](_resources/DB,_dbt.resources/projfiles-profile-ex1-1.png){.lightbox width="232"}

    -   Not included in project directory
    -   Only have to worry about this file if you set up dbt locally.
    -   [Docs](https://docs.getdbt.com/docs/core/connect-data-platform/connection-profiles)
    -   Created by `dbt init` in `~/.dbt/`
    -   Contents
        -   database connection, database credentials that dbt will use to connect to the data warehouse
        -   If you work on multiple projects locally, the different project names (configured in the `dbt_project.yml` file) will allow you to set up various profiles for other projects.
        -   Where you set the target (i.e. branch) (e.g. dev or prod)
    -   When you use dbt from the CLI, it looks in [dbt_project.yml]{.arg-text} for a name in [profile: 'name']{.arg-text}. Then, it looks for that name in [profiles.yml]{.arg-text} in order to find all the information it needs to connect to the database/warehouse and do work.
        -   You'll have one profile name/info for each warehouse that you use
    -   Target
        -   [dev]{.arg-text} target: An analyst using dbt locally will have this set as the default.
        -   [prod]{.arg-text} target: Creates the objects in your production schema.
            -   However, since it's often desirable to perform production runs on a schedule, it is recommended that you deploy your dbt project to a separate machine other than your local machine.
            -   Most dbt users only have a [dev]{.arg-text} target in their profile on their local machine.
        -   Components
            -   [type]{.arg-text}: The type of data warehouse you are connecting to
            -   Warehouse credentials: Get these from your database administrator if you don’t already have them. Remember that user credentials are very sensitive information that should not be shared.
            -   [schema]{.arg-text}: The default schema that dbt will build objects in.
            -   [threads]{.arg-text}: The number of threads the dbt project will run on.

-   `dbt_project.yml`

    -   [Docs](https://docs.getdbt.com/reference/dbt_project.yml)
    -   Main configuration file for your project
    -   Basic Set-Up
        -   Fill in your project name and profile name\
            ![](./_resources/DB,_dbt.resources/image.2.png){.lightbox width="432"}
        -   Add variables and models
    -   All available configurations that can be included in this file\
        ![](_resources/DB,_dbt.resources/proj-proj-configs-1.png){.lightbox width="154"}

-   `packages.yml`

    -   [Docs](https://docs.getdbt.com/docs/build/packages)
    -   List of external dbt packages you want to use in your project
    -   After adding the packages to packages.yml, run `dbt deps` to install the packages.
    -   Packages get installed in the `dbt_packages` directory — by default this directory is ignored by git, to avoid duplicating the source code for the package.
    -   When you update a version or revision in your `packages.yml` file, it isn't automatically updated in your dbt project. You should run `dbt deps` to update the package. You may also need to run a full refresh of the models in this package.
    -   [Examples]{.ribbon-highlight}
        -   Specific Version

            ``` yaml
            packages:
                - package: dbt-labs/dbt_utils
                  version: 0.7.3
            ```

        -   Range of Versions

            ``` yaml
            packages:
              - package: calogica/dbt_expectations
                version: [">=0.7.0", "<0.8.0"]
            ```

-   `dependencies.yml`

    -   [Docs](https://docs.getdbt.com/docs/collaborate/govern/project-dependencies#when-to-use-package-dependencies)
    -   Designed for the [dbt Mesh](#0) and [cross-project reference](#0) workflow
    -   Can contain both types of dependencies: "package" and "project" dependencies
    -   If your dbt project doesn't require the use of Jinja within the package specifications, you can simply rename your existing `packages.yml` to `dependencies.yml`.
    -   However, if your project's package specifications use Jinja, particularly for scenarios like adding an environment variable or a [Git token method](https://docs.getdbt.com/docs/build/packages#git-token-method) in a private Git package specification, you should continue using the `packages.yml` file name.

## Directories {#sec-db-dbt-dir .unnumbered}

-   [Example]{.ribbon-highlight}: [Source](https://docs.getdbt.com/best-practices/how-we-structure/1-guide-overview#guide-structure-overview)\
    ![](_resources/DB,_dbt.resources/dir-ex-1.png){.lightbox width="272"}

-   [Models]{.underline}

    -   Sources (i.e. data sources) are defined in `src_<source>.yml` files in your models directory
        -   .yml files contain definitions and tests ([Docs](https://docs.getdbt.com/reference/source-configs))
        -   .doc files contain source documentation
    -   Models (i.e. sql queries) are defined `stg_<source>.yml`
        -   .yml files contain definitions and tests ([Docs](https://docs.getdbt.com/reference/model-configs))
            -   [Example]{.ribbon-highlight}:

                ``` yaml
                models:
                  - name: your_model_name
                    config:
                      materialized: incremental
                      unique_key: primary_key_name
                ```
        -   .doc files contain source documentation
        -   The actual models are the .sql files
    -   [Example]{.ribbon-highlight}\
        ![](./_resources/DB,_dbt.resources/Screenshot%20(533).png){.lightbox width="432"}
        -   Staging:
            -   Different data sources will have separate folders underneath staging (e.g. stripe).
        -   Marts:
            -   Use cases or departments have different folders underneath marts (e.g. core or marketing)

-   [Macros]{.underline}

    -   [Docs](https://docs.getdbt.com/reference/macro-properties)
    -   Similar to functions in excel
    -   Define custom functions in the macros folder or override default macros and macros from a package
    -   See bkmks for tutorials on writing custom macros with jinja
    -   [{]{style="color: #990000"}[dbtplyr](https://github.com/emilyriederer/dbtplyr){style="color: #990000"}[}]{style="color: #990000"} macros
        -   dplyr tidy selectors, across, etc.

-   [Seeds]{.underline}

    -   [Docs](https://docs.getdbt.com/reference/seed-properties)
    -   Seeds are csv files that you add to your dbt project to be uploaded to your data warehouse.
        -   Uploaded into your data warehouse using the `dbt seed` command
    -   Best suited to static data which changes infrequently.
        -   Use Cases:
            -   A list of unique codes or employee ids that you may need in your analysis but is not present in your current data.
            -   A list of mappings of country codes to country names
            -   A list of test emails to exclude from analysis
    -   Referenced in downstream models the same way as referencing models — by using the `ref` function

-   [Snapshots]{.underline}

    -   Captures of the state of a table at a particular time

    -   [Docs](https://docs.getdbt.com/docs/building-a-dbt-project/snapshots), More [Docs](https://docs.getdbt.com/reference/snapshot-properties)

    -   build a slowly changing dimension (SCD) table for sources that do not support change data capture (CDC)

    -   [Example]{.ribbon-highlight}

        -   Every time the status of an order change, your system overrides it with the new information. In this case, there we cannot know what historical statuses that an order had.
        -   Daily snapshots of this table builds a history and allows you to track order statuses\
            ![](./_resources/DB,_dbt.resources/1-UenQCtO0h2WLAWW5lK67DQ.png){width="382"}

    -   [Example]{.ribbon-highlight}: ([source](https://github.com/stretcharm/dbt/blob/main/dbt_cheat_sheet.md#snapshots))

        ``` sql
        {% snapshot user_snapshots %}
        {{ config(
            target_schema='snapshots',
            unique_key='id',
            strategy='timestamp',
            updated_at='updated_at'
        ) }}

        SELECT
            id,
            name,
            email,
            updated_at
        FROM {{ source('raw_data', 'users') }}
        {% endsnapshot %}
        ```

## Components {#sec-db-dbt-comp .unnumbered}

### Materializations {#sec-db-dbt-comp-mats .unnumbered}

-   [Misc]{.underline}
    -   The manner in which the data is represented, i.e. the outputs of the models
    -   [Docs](https://docs.getdbt.com/best-practices/materializations/1-guide-overview)
    -   Start with models as views, when they take too long to query, make them tables, when the tables take too long to build, make them incremental.
-   [View]{.underline}
    -   Default materialization when there is no specification in the configuration

    -   Just like Standard Views, i.e. a saved SQL query (See [SQL \>\> Views](sql.qmd#sec-sql-views){style="color: green"})

    -   Always reflects the most up-to-date version of the input data

    -   Cost nothing to build since they're just queries, but have to be processed every time they’re ran.

        -   Slower to return results than a table of the same data. 
        -   Can cost more *over time*, especially if they contain intensive transformations and are queried often.

    -   Best for

        -   Building a model that stitches lots of other models together (Don't want to worry about freshness of ever table involved).
        -   Small datasets with minimally intensive logic that you want near realtime access to

    -   [Example]{.ribbon-highlight}

        ``` sql
        {{ config(
            materialized='view'
        ) }}

        SELECT col1, col2
        FROM {{ ref('my_model') }}
        ```
-   [Table]{.underline}
    -   Stores the literal rows and columns on disk (i.e. materialized view)

    -   Ideal for models that get queried regularly

    -   Reflects the most recent run on the source data that was available

    -   Best for sources for BI products and other frontend apps (e.g. like a mart that services a popular dashboard)

    -   [Example]{.ribbon-highlight}

        ``` sql
        {{ config(
            materialized='table'
        ) }}

        SELECT col1, col2
        FROM {{ ref('my_model') }}
        ```
-   [Incremental]{.underline}
    -   [Docs](https://docs.getdbt.com/best-practices/materializations/4-incremental-models)

    -   Builds a table in pieces over time — only adding and updating new or changed records which makes them efficient.

    -   Builds more quickly than a regular table of the same logic.

    -   Typically used on very large datasets, so building the initial table on the full dataset is time consuming and equivalent to the table materialization.

    -   Adds complexity and requires deeper consideration of layering and timing.

        -   See "Late arriving facts" section of the Docs

    -   Can drift from source data over time since you're not processing all of the source data. Extra effort is required to capture changes to historical data.

        -   See "Long-term considerations" section of the Docs

    -   Best for — same as tables, but for larger datasets.

        -   Can break down when processing "petabytes of data while supporting 100+ developers simultaneously working across 2,500+ models." See [Discord article](https://discord.com/blog/overclocking-dbt-discords-custom-solution-in-processing-petabytes-of-data)

            -   "devs frequently needed to modify the same tables, but with dbt's default behavior, they would constantly overwrite each other's test tables. Having 100+ developers in the same “kitchen”, of course, created confusion and a suboptimal experience for fellow devs who couldn't reliably test their changes."
            -   "using dbt's `is_incremental()` macro became a major bottleneck. Using this macro in BigQuery is suboptimal since it requires a full table scan to determine the latest timestamp from dbt's previous run, which can be costly and slow at scale. Our developers were regularly waiting over 20 minutes for compilations, severely limiting productivity and creating a frustrating development experience."

    -   Requirements

        -   A *filter* to select just the new or updated records
        -   A *conditional block* that wraps the filter and only applies it when we want it
        -   A *configuration* that tells dbt to build incrementally and helps apply the conditional filter when needed

    -   What happens underneath inside the data warehouse

        -   When there are records that already exist within the table but have a new field value:
            -   Option 1: `delete + insert` deletes these records and replaces them with an updated version of that record.
            -   Option 2: `merge` combines the insert, update, and delete statements into one. Instead of deleting changed records, it will replace the old field values with the new ones.
            -   Option 3: `insert + overwrite` applies to the same records as the other strategies, but overwrites all fields of an existing record rather than just the ones that have changed

    -   [Example]{.ribbon-highlight}

        ``` sql
        {{ config(
            materialized='incremental',
            incremental_strategy='merge',
            unique_key=['col1','col2']
        ) }}

        SELECT col1, col2, col3, updated_at
        FROM {{ ref('my_model') }}
        {% if is_incremental() %}
        WHERE updated_at > (SELECT MAX(updated_at) FROM {{ this }})
        {% endif %}
        ```
-   [Ephemeral]{.underline}
    -   Creates a CTE
    -   More difficult to troubleshoot, as they’re interpolated into the models that `ref` them, rather than existing on their own in a way that you can view the output of.
    -   Use Cases
        -   Very light-weight transformations that are early on in your DAG
        -   Should only used in one or two downstream models
        -   For a materialization that does not need to be queried directly
            -   Macros called using `dbt run-operation` cannot `ref()` ephemeral materializations
    -   Doesn't support model contracts
-   [Set-Up]{.underline}
    -   In practice, you want to set materializations at the folder level, and use individual model configs to override those as needed.

    -   In `dbt_project.yml`

        -   A [+]{.arg-text} is used to indicate a materialization

        -   [Example]{.ribbon-highlight}: models

            ``` yaml
            models:
              jaffle_shop:
                marketing:
                  +materialized: view
                  paid_ads:
                    google:
                      +materialized: table
            ```

            -   All models in the [marketing]{.var-text} and [paid_ads]{.var-text} folders are *views* while models in the [google]{.var-text} folder are *tables*

    -   In Model files

        -   [Example]{.ribbon-highlight}: Table (or View)

            ``` sql
            {{
                config(
                    materialized='table'
                )
            }}

            select ...
            ```

        -   [Example]{.ribbon-highlight}: Incremental

            ``` sqlpostgresql
            {{
                config(
                    materialized='incremental',
                    unique_key='order_id' # <1>
                )
            }}

            select * from orders

            {% if is_incremental() %} # <2>

            where
              updated_at > (select max(updated_at) from {{ this }}) # <3>

            {% endif %}
            ```

            1.  [unique_key]{.arg-text} specifies the field that allows dbt to find a record from the previous run (old data) with the same unique id as the new data in order to *update* that record instead of adding it as a separate row.
            2.  An if-else conditional that uses `is_incremental` (macro) to check if the systematic conditions necessary for an incremental model are met (See below)
            3.  The *conditional block* containing the *filter* that finds the latest [updated_at]{.var-text} date, so dbt knows whether the source data has new records that need to be *added* or not.

            -   `{{ this }}` means *filter* from "this" materialization
            -   `is_incremental` (macro) checks:
                -   [materialized ='incremental']{.arg-text}
                -   There is an existing table for this model in the warehouse to add to/update
                -   `--full-refresh` flag was not passed ([docs](https://docs.getdbt.com/reference/resource-configs/full_refresh))
                    -   Overrides the incremental materialization and builds the table from scratch again
                    -   Useful if the table drifts from the source table over time.

### Variables {#sec-db-dbt-comp-var .unnumbered}

-   Defined in the `project.yml` and used in models

    -   [Example]{.ribbon-highlight}: Assigning States to Regions

        ``` yaml
        vars:
          state_lookup:
            Northeast:
              - CT
              - ME
            Midwest:
              - IL
              - IN
        ```

-   Using the variables in a model

    ``` sql
    {# Option 1 #}
    SELECT state,
          CASE {% for k, v in var("state_lookup").items() %}
                WHEN state in ({% for t in v %}'{{ t }}'{% if not loop.last %}, {% endif %}{% endfor %}) THEN {{ k }}{% endfor %}
                ELSE NULL END AS region
      FROM {{ ref('my_table') }}

    {# Option 2 #}
    SELECT state,
          CASE {% for k, v in var("state_lookup").items() %}
                WHEN state in ({{ t|csl }}) THEN {{ k }}{% endfor %}
                ELSE NULL END AS region
      FROM {{ ref('my_table') }}
    ```

    -   Variables are accessed using `var`
    -   This is a complicated example, see [docs](https://docs.getdbt.com/reference/dbt-jinja-functions/var) for something simpler
    -   `{% ... %}` are used to encapsulate for-loops and if-then conditions, see [docs](https://docs.getdbt.com/docs/building-a-dbt-project/jinja-macros)
        -   `{# ... #}` is for comments
    -   Option 2 uses a csl filter (comma-separated-list)

### Models {#sec-db-dbt-comp-mod .unnumbered}

#### Misc {#sec-db-dbt-comp-mod-misc .unnumbered}

-   Tagging
    -   Allows you to run groups of models
        -   [Example]{.ribbon-highlight}:`dbt run --models tag:daily`

#### YAML Options {#sec-db-dbt-comp-mod-yaml .unnumbered}

-   [Contracts]{.underline}
    -   Enforces constraints on the materialization output from a model (emphemeral not supported)
        -   Data Tests differ from contracts since they are a more flexible mechanism for validating the content of your model *after* it's built (contracts execute at build time)
        -   Contracts probably requires less compute (cost) in your data platform
    -   [Docs](https://docs.getdbt.com/reference/resource-configs/contract)
    -   Incremental materializations requires setting: [on_schema_change: append_new_columns]{.arg-text}
    -   Set-Up
        -   When [enforced: true]{.arg-text}, [name]{.arg-text} and [data_type]{.arg-text} are required to be set for every column

        -   Set [alias_types: false]{.arg-text} to opt-out of letting dbt try to convert a specified type to one that conforms to a specific platform

            -   e.g. Specify string in your contract while on Postgres/Redshift, and dbt will convert it to text.

        -   If a varchar size or numeric scale is not specified, then dbt relies on default values

        -   [Example]{.ribbon-highlight}

            ``` yaml
            models:
              - name: dim_customers
                config:
                  materialized: table
                  contract:
                    enforced: true
                columns:
                  - name: customer_id
                    data_type: int
                    constraints:
                      - type: not_null
                  - name: customer_name
                    data_type: string
                  - name: non_integer
                    data_type: numeric(38,3)
            ```

            -   [numeric(38,3)]{.arg-text} says [non-integer]{.var-text} is allowed to have 38 (precision) total digits (whole + decimal) and 3 decimal places (scale).
-   [Constraints]{.underline}
    -   When specified, the platform (e.g. postgres, BigQuery, etc.) will perform additional validation on data as it is being populated in a new table or inserted into a preexisting table. If the validation fails, the table creation or update fails (or there's a warning), the operation is rolled back, and you will see a clear error message.

    -   Only table and incremental materializations supported

    -   Can be set at the column level (recommended) or model level

    -   Keys and Values

        -   [type]{.arg-text} (required): one of [not_null]{.arg-text}, [unique]{.arg-text}, [primary_key]{.arg-text}, [foreign_key]{.arg-text}, [check]{.arg-text}, [custom]{.arg-text}
            -   postgres supports and enforces all types, but Redshift, Snowflake, BigQuery, Databricks, Spark do not. See [docs](https://docs.getdbt.com/reference/resource-properties/constraints#platform-specific-support), [docs](https://docs.getdbt.com/docs/collaborate/govern/model-contracts#platform-constraint-support) for details.
        -   [expression]{.arg-text}: Free text input to qualify the constraint.
            -   Required for certain constraint types and optional for others.
        -   [name]{.arg-text} (optional): Human-friendly name for this constraint. Supported by some data platforms.
        -   [columns]{.arg-text} (model-level only): List of column names to apply the constraint over

    -   [Example]{.ribbon-highlight}

        ``` yaml
        models:
          - name: <model_name>

            # required
            config:
              contract:
                enforced: true

            # model-level constraints
            constraints:
              - type: primary_key
                columns: [FIRST_COLUMN, SECOND_COLUMN, ...]
              - type: FOREIGN_KEY # multi_column
                columns: [FIRST_COLUMN, SECOND_COLUMN, ...]
                expression: "OTHER_MODEL_SCHEMA.OTHER_MODEL_NAME (OTHER_MODEL_FIRST_COLUMN, OTHER_MODEL_SECOND_COLUMN, ...)"
              - type: check
                columns: [FIRST_COLUMN, SECOND_COLUMN, ...]
                expression: "FIRST_COLUMN != SECOND_COLUMN"
                name: HUMAN_FRIENDLY_NAME
              - type: ...

            columns:
              - name: FIRST_COLUMN
                data_type: DATA_TYPE

                # column-level constraints
                constraints:
                  - type: not_null
                  - type: unique
                  - type: foreign_key
                    expression: OTHER_MODEL_SCHEMA.OTHER_MODEL_NAME (OTHER_MODEL_COLUMN)
                  - type: ...
        ```

#### Best Practices {#sec-db-dbt-comp-mod-bp .unnumbered}

-   Modularity where possible

    -   Same as the functional mindset: "if there's any code that's continually repeated, then it should be a function(i.e. its own separate model in dbt)."

-   Readability

    -   Comment
    -   Use CTEs instead of subqueries
    -   Use descriptive names
        -   [Example]{.ribbon-highlight}: if you are joining the tables "users" and "addresses" in a CTE, you would want to name it "users_joined_addresses" instead of "user_addresses"

-   [Example]{.ribbon-highlight}: Comments, CTE, Descriptive Naming

    ``` sql
    WITH
    Active_users AS (
      SELECT
        Name AS user_name,
        Email AS user_email,
        Phone AS user_phone,
        Subscription_id
      FROM users
      --- status of 1 means a subscription is active
      WHERE subscription_status = 1
    ),
    Active_users_joined_subscriptions AS (
      SELECT
        Active_users.user_name,
        active_users.user_email,
        Subscriptions.subscription_id,
        subscriptions.start_date ,
        subscriptions.subscription_length
      FROM active_users
      LEFT JOIN subscriptions
        ON active_users.subscription_id = subscriptions.subscription_id
    )
    SELECT * FROM Active_users_joined_subscriptions
    ```

#### Layers {#sec-db-dbt-comp-mod-lay .unnumbered}

-   [Staging]{.underline}

    -   Subdirectories based on the data sources (*not* business entities)
    -   Naming Convention
        -   Syntax: `stg_[source]__[entity]s.sql`
            -   The double underscore between source system and entity helps visually distinguish the separate parts in the case of a source name having multiple words.
    -   Contains all the individual components of your project that the other layers will use in order to craft more complex data models.
    -   Once you get a feel for these models, use the codegen package to automatically generate them. (See [Packages](db-dbt.qmd#sec-db-dbt-pkgs){style="color: green"})
    -   Data Sources
        -   Only layer where the `source` macro is used
        -   Each model bears a one-to-one relationship with the source data table it represents (i.e. 1 staging model per source)
    -   Transformations
        -   Only transformations that are needed for every downstream model should be applied.
        -   Typical Transformations: recasting, column/view renaming, basic computations (e.g. cents to dollars), binning continuous fields, collapsing categoricals fields.
            -   Unions may also be applied if you have two sources with the same fields
                -   e.g. two tables that represent the same store except that perhaps they are in different regions or selling on two different platforms, and your project only wants to use aggregated calculations from both.
        -   Aggregations and joins should be avoided. But, if *joins* are necessary, then create a subdirectory called [base]{.var-text} within the source directory.
            1.  Within that subdirectory, create `base_` models that perform the previously mentioned "Typical Transformations."
            2.  Then, at the source directory level, create staging, `stg_`, models that join those base models that are in the subdirectory. (See [docs](https://docs.getdbt.com/best-practices/how-we-structure/2-staging#staging-other-considerations) for examples)
        -   [Base Models]{.underline}
            -   Basic transformations (e.g. cleaning up the names of the columns, casting to different data types)
            -   Other models use these models as data sources
                -   Prevents errors like accidentally casting your dates to two different types of timestamps, or giving the same column two different names.
                    -   Two different timestamp castings can cause all of the dates to be improperly joined downstream, turning the model into a huge disaster
            -   Usually occuring in staging
            -   Read directly from a source, which is typically a schema in your data warehouse
                -   Source object: `{{ source('campaigns', 'channel') }}`
                    -   [campaigns]{.var-text} is the name of the source in the .yml file
                    -   [channel]{.var-text} is the name of a table from that source
    -   Materialized as views.
        -   Allows any intermediate or mart models referencing the staging layer to get access to fresh data and at the same time it saves us space and reduces costs.

        -   In `dbt_project.yml`

            ``` yaml
            models:
              jaffle_shop:
                staging:
                  +materialized: view
            ```

            -   View is the default materialization, but still good to set manually just for clarification purposes.

-   [Intermediate]{.underline}

    -   Uses views from the staging layer to build more complex models
    -   Subdirectories based on business entities (e.g. marketing, finance, etc.)
        -   If you have fewer than 10 models and aren’t having problems developing and using them, then feel free to forego subdirectories completely.
    -   Naming Convention
        -   Syntax: `int_[entity]s_[verb]s.sql`
        -   Verb Examples: pivoted, aggregated_to_user, joined, fanned_out_by_quantity, funnel_created, etc.)
        -   No double underscores
            -   If for some reason you had to use a source macro in this layer, then you could use double underscores between entity(s) and verb(s)
        -   [Example]{.ribbon-highlight}: `int_payments_pivoted_to_orders`
    -   Other models should reference the models within this stage as Common Table Expressions although there may be cases where it makes sense to materialize them as Views
        -   Macros called via `run-operation` cannot reference ephemeral materizations (i.e. CTEs)
        -   Recommended to start with ephemeral materializations unless this doesn’t work for the specific use case
    -   Whenever you decide to materialize them as Views, it may be easier to to do so in a custom schema, that is a schema outside of the main schema defined in your dbt profile.
    -   If the same intermediate model is referenced by more than one model then it means your design has probably gone wrong.
        -   Usually indicates that you should consider turning your intermediate model into a macro. reference the base models rather than from a source
        -   Reference object
            -   `{{ ref('base_campaign_types') }}`
            -   [base_campaign_types]{.var-text} is a base model

-   [Marts]{.underline}

    -   Where everything comes together in a way that business-defined entities and processes are constructed and made readily available to end users via dashboards or applications.
    -   Since this layer contains models that are being accessed by end users it means that performance matters. Therefore, it makes sense to materialize them as tables.
        -   If a table takes too much time to be created (or perhaps it costs too much), then you may also need to consider configuring it as an incremental model.
    -   A mart model should be relatively simple and therefore, too many joins should be avoided
    -   [Example]{.ribbon-highlight}
        -   A monthly recurring revenue (MRR) model that classifies revenue per customer per month as new revenue, upgrades, downgrades, and churn, to understand how a business is performing over time.
            -   It may be useful to note whether the revenue was collected via Stripe or Braintree, but they are not fundamentally separate models.

-   [Utilities]{.underline}

    -   Directory for any general purpose models that are generated from macros or based on seeds that provide tools to help modeling, rather than data to model itself.
    -   The most common use case is a [date spine](https://github.com/dbt-labs/dbt-utils#date_spine-source) generated with the dbt utils package.
        -   Also see [Packages](db-dbt.qmd#sec-db-dbt-pkgs){style="color: green"}, [Databases, Engineering \>\> Terms](db-engineering.qmd#sec-db-eng-terms){style="color: green"} \>\> Date Spine

## Optimizations {#sec-db-dbt-opt .unnumbered}

-   Runs parallelized
    -   Models that have dependencies aren’t run until their upstream models are completed but models that don’t depend on one another are run at the same time.
    -   The [thread]{.arg-text} parameter in your `dbt_project.yml` specifies how many models are permitted to run in parallel
-   Incremental Materializations
    -   See [Components \>\> Materializations](db-dbt.qmd#sec-db-dbt-comp-mats){style="color: green"}
    -   Saves money by not rebuilding the entire table from scratch when records only need to be updated or added to a table (plus transformations to the new records).
    -   Useful for big data tables and expensive transformations

## Packages {#sec-db-dbt-pkgs .unnumbered}

-   Available Packages: [link](https://hub.getdbt.com/)
-   See [Project Files](db-dbt.qmd#sec-db-dbt-projfil){style="color: green"} \>\> packages.yml
-   Packages
    -   [dbt-athena](https://github.com/Tomme/dbt-athena) - Adaptor for AWS Athena
    -   [audit-helper](https://github.com/dbt-labs/dbt-audit-helper/tree/0.4.0/)
        -   Compares columns, queries; useful if refactoring code or migrating db
    -   [codegen](https://github.com/dbt-labs/dbt-codegen)
        -   Generate base model, barebones model and source .ymls
    -   [dbtplyr](https://github.com/emilyriederer/dbtplyr) - Macros
        -   dplyr tidy selectors, across, etc.
    -   [dbt-duckdb](https://github.com/duckdb/dbt-duckdb) ([Overview](https://duckdb.org/2025/04/04/dbt-duckdb.html)) - Adapter for duckdb
    -   [elementary](https://docs.elementary-data.com/introduction) - Includes schema change alerts and anomaly detection tests. It is great for tracking trends in your data to see if anything falters from the usual
    -   [dbt-expectations](https://hub.getdbt.com/calogica/dbt_expectations/latest/)
        -   Data validation based on great expectations py lib
    -   [external-tables](https://github.com/dbt-labs/dbt-external-tables)
        -   Create or replace or refresh external tables
        -   Guessing this means any data source (e.g. s3, spark, google, another db like snowflake, etc.) that isn't the primary db connected to the dbt project
    -   [logging](https://github.com/dbt-labs/dbt-event-logging)
        -   Provides out-of-the-box functionality to log events for all dbt invocations, including run start, run end, model start, and model end.
        -   Can slow down runs substantially
    -   [profiler](https://github.com/data-mie/dbt-profiler)
        -   Implements dbt macros for profiling database relations and creating doc blocks and table schemas (schema.yml) containing said profiles
    -   [re_data](https://docs.getre.io/latest/docs/introduction/whatis/)
        -   Dashboard for monitoring, macros, models
    -   [dbt-score](https://dbt-score.picnic.tech/)
        -   Linting of models and sources which have metadata associated with them: documentation, tests, types, etc.
    -   [dbt_set_similarity](https://github.com/Matts52/dbt-set-similarity)
        -   Calculates similarity metrics between fields in your database
        -   [Docs](https://matthewsenick.com/dbt-set-similarity/#!/overview)
        -   [Measuring Cross-Product Adoption Using dbt_set_similarity](https://towardsdatascience.com/measuring-cross-product-adoption-using-dbt-set-similarity-fdf7c1f88bc2)
        -   Fields must be arrays.
    -   [dbt-spark](https://docs.getdbt.com/docs/core/connect-data-platform/spark-setup) - If you're using databricks, it's recommended to use dbt-databricks
        -   [Migrating to dbt-databricks](https://docs.getdbt.com/guides/migrate-from-spark-to-databricks)
    -   [spark-utils](https://github.com/dbt-labs/spark-utils)
        -   Enables use of (most of) the {dbt-utils} macros on spark
    -   [dbt-utils](https://github.com/dbt-labs/dbt-utils)
        -   Ton of stuff for tests, queries, etc.

## Data Validation and Unit Tests {#sec-db-dbt-dvaut .unnumbered}

![](./_resources/DB,_dbt.resources/image.png){.lightbox width="532"}

### Misc {#sec-db-dbt-dvaut-misc .unnumbered}

-   Test both your raw data (sources) and transformed data (models). It’s important to test both to help you pinpoint where issues originate, helping in the debugging process.
    -   [Source Tests]{.underline}: Confirms that you are transforming quality data, allowing you to be confident in the input of your data models. When you are confident in the input, data quality issues can then be attributed to transformation code.
    -   [Model Tests]{.underline}: They come after source testing. When adding a test to a model, you are setting expectations on how the code you’ve written should transform your data.
        -   If the data produced by a model passed a test when it was first written, but now fails, its most likely due to something in the data changing that your code can no longer handle.
-   Built-in support for CI/CD pipelines to test your "models" and stage them before committing to production
-   Run test - `dbt test`
-   See also
    -   [Docs](https://docs.getdbt.com/docs/building-a-dbt-project/tests)
    -   [Packages](db-dbt.qmd#sec-db-dbt-pkgs){style="color: green"}
    -   [How to do Unit Testing in dbt](https://towardsdatascience.com/how-to-do-unit-testing-in-dbt-cb5fb660fbd8)
-   Most of the tests are defined in a models-type .yml file in the models directory
-   Can be applied to a model or a column
-   Test that the primary key column is unique and not null
-   Differences from [Contracts](db-dbt.qmd#sec-db-dbt-comp-mod-yaml){style="color: green"}
    -   Tests are more configurable, such as with [custom severity thresholds](https://docs.getdbt.com/reference/resource-configs/severity).
    -   Tests are asier to debug after finding failures, because you can query the already-built model, or [store the failing records in the data warehouse](https://docs.getdbt.com/reference/resource-configs/store_failures).

### Macros {#sec-db-dbt-dvaut-mac .unnumbered}

-   Custom (aka Singular) tests should be located in a tests folder.
    -   dbt will evaluate the SQL statement.

    -   The test will pass if no row is returned and failed if at least one or more rows are returned.

    -   Useful for testing for some obscurity in the data

    -   [Example]{.ribbon-highlight}: Check for duplicate rows when joining two tables

        ``` sql
        select
        a.id
        from {{ ref(‘table_a’) }} a
        left join {{ ref(‘table_b’) }} b
        on a.b_id = b.id
        group by a.id
        having count(b.id)>1
        ```

        -   i.e. If I join table a with table b, there should only be one record for each unique id in table a
        -   Process
            -   Join the tables on their common field
            -   Group them by the id that should be distinct
            -   Count the number of duplicates created from the join.
                -   This tells me that something is wrong with the data.
                -   Add a having clause to filter out the non-dups
-   [Example]{.ribbon-highlight}: ([source](https://github.com/stretcharm/dbt/blob/main/dbt_cheat_sheet.md#macros))
    -   Definition

        ```         
        {% macro calculate_revenue(total_sales, cost_of_goods_sold) %}
        {{ total_sales }} - {{ cost_of_goods_sold }}
        {% endmacro %}
        ```

    -   Usage

        ``` sql
        SELECT
            order_id,
            {{ calculate_revenue('total_sales', 'cogs') }} AS revenue
        FROM {{ ref('orders') }}
        ```

### Mock Data {#sec-db-dbt-dvaut-mock .unnumbered}

![](./_resources/DB,_dbt.resources/image.1.png){.lightbox width="175"}

-   Data used for unit testing SQL code
-   To ensure completeness, it's best if analysts or business stakeholders are the ones provide test cases or test data
-   Store in the "data" folder (typically .csv files)
    -   each CSV file represents one source table
    -   should be stored in a separate schema (e.g. unit_testing) from production data
    -   dbt seed (see below, Other \>\> seeds) command is used to load mock data into the data warehouse

### Tests {#sec-db-dbt-dvaut-test .unnumbered}

-   Misc
    -   Notes from [How to Test Your Data Models with dbt](https://learnanalyticsengineering.substack.com/p/how-to-test-your-data-models-with)
    -   Packages
        -   dbt-expectations, dbt-utils, elementary
    -   Besides Freshness, Uniqueness, and Not NULL, only add more tests if there's an actionable next step when they fail.
    -   Considerations
        -   Which source fields are used in downstream data models? Are there qualities you should be testing to ensure the transformations downstream turn out as expected?
            -   e.g. testing the datatype of an amount field that’s used in a SUM calculation downstream
        -   Are there [id]{.var-text} fields used in JOINs downstream? Would adding a [relationships test](https://docs.getdbt.com/reference/resource-properties/data-tests#relationships) prevent joining records that don’t exist?
        -   Are CASE statements used in your model code? Should there be an [accepted_values test](https://docs.getdbt.com/reference/resource-properties/data-tests#accepted_values) at the source to ensure all possible values are captured?
-   Model Tests YAML
    -   [Example]{.ribbon-highlight}: ([source](https://github.com/stretcharm/dbt/blob/main/dbt_cheat_sheet.md#generic-tests))

        ``` yaml
        version: 2

        models:
          - name: orders
            columns:
              - name: order_id
                data_tests:
                  - unique
                  - not_null
              - name: status
                data_tests:
                  - accepted_values:
                      values: ['placed', 'shipped', 'completed', 'returned']
              - name: customer_id
                data_tests:
                  - relationships:
                      to: ref('customers')
                      field: id
        ```
-   User-Defined Test
    -   [Example]{.ribbon-highlight}: Put into `data_tests` directory ([source](https://github.com/stretcharm/dbt/blob/main/dbt_cheat_sheet.md#singular-tests))

        ``` sql
        -- Refunds have a negative amount, so the total amount should always be >= 0.
        -- Therefore return records where total_amount < 0 to make the test fail.
        select
            order_id,
            sum(amount) as total_amount
        from {{ ref('fct_payments') }}
        group by 1
        having total_amount < 0
        ```
-   [Freshness]{.underline} ([docs](https://docs.getdbt.com/reference/resource-properties/freshness)) - Used to define the acceptable amount of time between the most recent record, and now, for a table
    -   The built-in test only available for sources, but dbt_expectations has one for models

    -   Freshness tests on sources should match the frequency of your ingestion syncs

        -   See [Production, Tools \>\> Ingestion](production-tools.qmd#sec-prod-tools-dating){style="color: green"} \>\> Concepts \>\> Sync Methods
        -   e.g. If you sync your data every hour because you expect new records in a source every hour, your freshness tests should reflect this.

    -   Should only be configured for *critical* data sources. If you aren’t using a data source despite having it documented, freshness tests may only contribute to test bloat.

    -   Identify the timestamp field that represents when your data was created or ingested.

        -   Examples
            -   A [created_at]{.var-text} field directly from the source itself
            -   An autogenerated field by your ingestion tool
                -   e.g. Airbyte’s [\_airbyte_extracted_at]{.var-text} field which is automatically created in each data source

    -   [Example]{.ribbon-highlight}

        ``` yaml
        sources:
          - name: sales_pipeline
            freshness:
               warn_after:
                  count: 2
                  period: hour
               error_after:
                  count: 2
                  period: hour
            loaded_at_field: _airbyte_extracted_at
        ```

    -   [Example]{.ribbon-highlight}: dbt_expectations

        ``` yaml
        models:
          - name: sales_mrr
              - name: mrr_computed_at 
                tests:
                   - dbt_expectations.expect_row_values_to_have_recent_data:
                       datepart: week
                       interval: 1
        ```
-   [Uniqueness]{.underline} - Tests that the value in a field is unique across all records.
    -   Important to apply to the primary key of every source and model to ensure this field accurately represents a unique record

    -   [Example]{.ribbon-highlight}

        ``` yaml
        sources:
          - name: sales_pipeline
            columns:
              - name: opportunity_id
                tests:
                  - unique
        ```
-   [Not NULL]{.underline} - Should be applied to every field where you expect there to be a value, particularly on the primary key.
    -   [Example]{.ribbon-highlight}

        ``` yaml
        sources:
          - name: sales_pipeline
            columns:
              - name: opportunity_id
                tests:
                  - unique
                  - not_null
        ```
-   [Recency]{.underline} - Asserts that a timestamp column in the reference model contains data that is at least as recent as the defined date interval.
    -   Can be useful for incremental models

    -   [Example]{.ribbon-highlight}: [dbt-utils](https://github.com/dbt-labs/dbt-utils?tab=readme-ov-file#recency-source)

        ``` yaml
        version: 2

        models:
          - name: model_name
            tests:
              - dbt_utils.recency:
                  datepart: day
                  field: created_at
                  interval: 1
        ```

### Examples {#sec-db-dbt-dvaut-ex .unnumbered}

-   [Example]{.ribbon-highlight}: `project.yml`\
    ![](./_resources/DB,_dbt.resources/1-65vBHGQB1CXpJMrtRwsY3A.png){.lightbox width="532"}
-   [Example]{.ribbon-highlight}: Unit Test
    -   Add test to `dbt_project.yml`

        ``` yaml
        seeds:
          unit_testing:
            revenue:
              schema: unit_testing
              +tags:
                - unit_testing
        ```

        -   Every file in the unit_testing/revenue folder will be loaded into unit_testing
        -   Executing `dbt build -s +tag:unit_testing` will run all the seeds/models/tests/snapshots with tag unit_testing and their upstreams

    -   Create macro that switches the source data in the model being tested from production data (i.e. using `{{ source() }}` ) to mock data (i.e. using `ref` ) when a unit test is being run

        ``` sql
        {% macro select_table(source_table, test_table) %}
              {% if var('unit_testing', false) == true %}

                    {{ return(test_table) }}
              {% else %}
                    {{ return(source_table) }}
              {% endif %}
        {% endmacro %}
        ```

        -   Article calls this file "select_table.sql"
        -   2 inputs: "source_table" (production data) and "test_table" (mock data)
        -   macro returns the appropriate table based on the variable in the dbt command
            -   If the command doesn’t provide unit_testing variable or the value is false , then it returns source_table , otherwise it returns test_table.

    -   Add macro code chunk to model

        ``` sql
        {{ config
            (
                materialized='table',
                tags=['revenue']
            )
        }}
        {% set import_transaction = select_table(source('user_xiaoxu','transaction'), ref('revenue_transaction')) %}
        {% set import_vat = select_table(source('user_xiaoxu','vat'), ref('revenue_vat')) %}
        SELECT
            date
            , city_name
            , SUM(amount_net_booking) AS amount_net_booking
            , SUM(amount_net_booking * (1 - 1/(1 + vat_rate)))  AS amount_vat
        FROM {{ import_transaction }}
        LEFT JOIN {{ import_vat }} USING (city_name)
        GROUP BY 1,2
        ```

        -   Inside the `{%...%}` , the macro "select_table" is called to set the local variables, "import_transaction" and "import_vat" which are later used in the model query
        -   Model file is named "revenue2.sql"

    -   Run model and test using mock data: `dbt build -s +tag:unit_testing --vars 'unit_testing: true'`

        -   Run model with production data (aka source data): `dbt build -s +tag:revenue --exclude tag:unit_testing`

    -   Compare output

        ``` yaml
        version: 2
        models:
          - name: revenue
            meta:
              owner: "@xiaoxu"
            tests:
              - dbt_utils.equality:
                  compare_model: ref('revenue_expected')
                  tags: ['unit_testing']
        ```

        -   Model properties file that's named `revenue.yml` in the models directory
        -   By including [tags: \['unit_testing'\]]{.arg-text} we can insure that we don't run this test in production (see build code above with `--exclude tag:unit_testing`

    -   Macro for comparing numeric output

        ``` {.sql .jinja}
        {% test advanced_equality(model, compare_model, round_columns=None) %}
        {% set compare_columns = adapter.get_columns_in_relation(model) | map(attribute='quoted') %}
        {% set compare_cols_csv = compare_columns | join(', ') %}
        {% if round_columns %}
            {% set round_columns_enriched = [] %}
            {% for col in round_columns %}
                {% do round_columns_enriched.append('round('+col+')') %}
            {% endfor %}
            {% set selected_columns = '* except(' + round_columns|join(', ') + "), " + round_columns_enriched|join(', ') %}
        {% else %}
            {% set round_columns_csv = None %}
            {% set selected_columns = '*' %}
        {% endif %}
        with a as (
            select {{compare_cols_csv}} from {{ model }}
        ),
        b as (
            select {{compare_cols_csv}} from {{ compare_model }}
        ),
        a_minus_b as (
            select {{ selected_columns }} from a
            {{ dbt_utils.except() }}
            select {{ selected_columns }} from b
        ),
        b_minus_a as (
            select {{ selected_columns }} from b
            {{ dbt_utils.except() }}
            select {{ selected_columns }} from a
        ),
        unioned as (
            select 'in_actual_not_in_expected' as which_diff, a_minus_b.* from a_minus_b
            union all
            select 'in_expected_not_in_actual' as which_diff, b_minus_a.* from b_minus_a
        )
        select * from unioned
        {% endtest %}
        ```

        -   File called "advanced_equality.sql"
