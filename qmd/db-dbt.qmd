# DB, dbt

TOC

* Misc
* Set-up
* Description
* Optimizations
* Components
* Packages
* Data Validation and Unit Tests



Misc

* [List](https://docs.getdbt.com/docs/available-adapters) of available DB Adaptors
	* Runs on Python, so adaptors are installed via pip
	Notes from
	* [Anatomy of a dbt project](https://towardsdatascience.com/anatomy-of-a-dbt-project-50e810abc695)
	* [What is dbt?](https://towardsdatascience.com/what-is-dbt-a0d91109f7d0)
		[Docs](https://docs.getdbt.com/docs/introduction)
		
	Resources
	* [Create a Local dbt Project](https://towardsdatascience.com/create-local-dbt-project-e12c31bd3992)
		* Uses docker containers to set up a local dbt project and a local postgres db to play around with
	Typical Workflow
	

```
dbt deps
dbt seed
dbt snapshot
dbt run
dbt run-operation {{ macro_name }}
dbt test
```

* Style Guide Components
	* Naming conventions (the case to use and tense of the column names)
	* SQL best practices (commenting code, CTEs, subqueries, etc.)
	* Documentation standards for your models
	* Data types of date, timestamp, and currency columns
	* Timezone standards for all dates
* [Castor](https://www.castordoc.com/) - tool that takes your project and autofills much of the documentation![](./_resources/DB,_dbt.resources/image.3.png)
	* Has a free tier
	* Very helpful if you have the same column name in multiple datasets, you don’t have to keep defining it
	* Tribal Knowledge
		* When a dataset is discussed in a team slack channel, Castor pulls the comments and adds them to the documentation of the dataset
* [Lightdash](https://www.lightdash.com/) - BI tool for dbt projects - free tier for self hosting
	
	



Set-up

* Basic set-up: [Article](https://towardsdatascience.com/getting-hands-on-with-dbt-data-build-tool-a157d4151bbc)
	* Example uses postgres adaptor
* Within a python virtual environment
	* Create: `python3 -m vevn dbt-venv`
	* Activate: `source dbt-venv/bin/activate`
		* Should be able to see a `(dbt-venv)` prefix in every line on the terminal
	* Install dbt-core: `pip install dbt-core`
		* Specific version: `pip install dbt-core==1.3.0`
		* Confirm installation by checking version: `dbt --version` 
	* Install plugins

```
pip install dbt-bigquery
pip install dbt-spark
# etc...
```

Description

	built for data modeling
		**models** are like sql queries
		
	modularizes SQL code and makes it reusable across "models"
	![](./_resources/DB,_dbt.resources/1_uf-Jo4ybZT0qdsCHNZavGg (2).png)
	* Running the orders "model" also runs the base\_orders model and base\_payments model (i.e. dependencies for orders)
		* Not sure this exactly right. Seems like doing this would result in wasting time rerunning the same dependencies multiple times
	* base\_orders and base\_payments are independent in that they can also be used in other models
	* creates more dependable code because you’re using the same logic in all your models
	* Makes runs faster since you aren’t wasting time and resources running the same blocks of code over and over again
* You can schedule running sets of models by tagging them (e.g. #daily, #weekly)
* Version Control
	* _snapshots_ provide mechanism for versioning datasets
	* Within every yaml file is an option to include the version
* package add-ons that allow you to interact with spark, snowflake, duckdb, redshift, etc.
* Documentation for every step of the way
	* .yml files can be used to generate a website (localhost:8080) around all of your dbt documentation.![](./_resources/DB,_dbt.resources/1-xn3o7SBakX5qcFWLks4DCQ.png)

```
dbt docs generate
dbt docs serve
```

	Current understanding
		Data is brought in from warehouses via base models and basic transformations are performed (models >> staging directory)
		
		Then the data is transformed to the desired state via intermediate models and calculations performed (models >> marts directory)
		
		Then the final product is stored in the data directory
		



Optimizations

* using an M1, the new Apple laptops with Apple’s own CPUs improved speed by 3x
* upgrading from dbt 0.15.0 -> 0.20.0 resulted in another 3x speed increase
* moving dbt out of a container resulted in a 2x speed increase for those using a Intel CPU MacBook Pro
* Runs parallelized
	* Models that have dependencies aren’t run until their upstream models are completed but models that don’t depend on one another are run at the same time.
	* `thread`  parameter in your dbt\_project.yml specifies how many models are permitted to run in parallel
* BigQuery
	* 



Components

	Project Templates
		[Style Guide](https://github.com/dbt-labs/corp/blob/master/dbt_style_guide.md)
			More detailed: [link](https://discourse.getdbt.com/t/how-we-structure-our-dbt-projects/355)
			
		Example - [Starter Project](https://github.com/dbt-labs/dbt-init/tree/master/starter-project)
		![](./_resources/DB,_dbt.resources/1_bgUI3LTLdY19EFRwSyZh1A (2).png)
	* 
* profiles.yml
	* Not included in project directory
	* Only have to worry about this file if you set up dbt locally.
	* [doc](https://docs.getdbt.com/dbt-cli/configure-your-profile)
	* Created by `dbt init` in ~/.dbt/
	* Contents
		* database connection, database credentials that dbt will use to connect to the data warehouse
		* If you work on multiple projects locally, the different project names (configured in the dbt\_project.yml file) will allow you to set up various profiles for other projects.
		* ... something about "targets" but not sure what this is or how it's used
* dbt\_project.yml
	* [doc](https://docs.getdbt.com/reference/dbt_project.yml)
	* main configuration file for your project
	* Fields where you need to change default values to your project name:
		* project name, profile name![](./_resources/DB,_dbt.resources/image.2.png)
		* variables
			* Defined in the project yaml and used in models
				* Accessed using `var`
			* [Example]{.ribbon-highlight}: Assigning States to Regions
				* Define variables in dbt\_project.yml

```
vars:
  state_lookup:
    Northeast:
      - CT
      - ME
    Midwest:
      - IL
      - IN
```

* Using the variables in a model

```
{# Option 1 #}
SELECT state,
      CASE {% for k, v in var("state_lookup").items() %}
            WHEN state in ({% for t in v %}'{{ t }}'{% if not loop.last %}, {% endif %}{% endfor %}) THEN {{ k }}{% endfor %}
            ELSE NULL END AS region
  FROM {{ ref('my_table') }}

{# Option 2 #}
SELECT state,
      CASE {% for k, v in var("state_lookup").items() %}
            WHEN state in ({{ t|csl }}) THEN {{ k }}{% endfor %}
            ELSE NULL END AS region
  FROM {{ ref('my_table') }}
```

* This is a complicated example, see [docs](https://docs.getdbt.com/reference/dbt-jinja-functions/var) for something simpler
* `{% ... %}` are used to encapsulate for-loops and if-then conditions, see [docs](https://docs.getdbt.com/docs/building-a-dbt-project/jinja-macros)
	* `{# ... #}`  is for comments
* Option 2 uses a csl filter (comma-separated-list)

* models section
	* my\_new\_project

* models
		Misc
		* key components of a well-written data model
			* modularity where possible
				* Same as the functional mindset: "if there's any code that's continually repeated, then it should be a function(i.e. its own separate model in dbt)."
			* readability
				* Comment
				* Use CTEs instead of subqueries
				* Use descriptive names
					* Example: if you are joining the tables "users" and "addresses" in a CTE, you would want to name it "users\_joined\_addresses" instead of "user\_addresses"
				* Example: Comments, CTE, Descriptive Naming

```
WITH
Active_users AS (
  SELECT
    Name AS user_name,
    Email AS user_email,
    Phone AS user_phone,
    Subscription_id
  FROM users
  --- status of 1 means a subscription is active
  WHERE subscription_status = 1
),
Active_users_joined_subscriptions AS (
  SELECT
    Active_users.user_name,
    active_users.user_email,
    Subscriptions.subscription_id,
    subscriptions.start_date ,
    subscriptions.subscription_length
  FROM active_users
  LEFT JOIN subscriptions
    ON active_users.subscription_id = subscriptions.subscription_id
)
SELECT * FROM Active_users_joined_subscriptions
```

	Model categories: staging, marts, base/intermediate
	* Staging
		* Contains all the individual components of your project that the other layers will use in order to craft more complex data models.
		* Each model bears a one-to-one relationship with the source data table it represents (i.e. 1 staging model per source)
		* Typical Transformations: recasting, column renaming, basic computations (such as KBs to MBs or GBs), categorization (e.g. using CASE WHEN statements).
			* Aggregations and joins should also be avoided
		* Usually materialized as views.
			* Allows any intermediate or mart models referencing the staging layer to get access to fresh data and at the same time it saves us space and reduces costs.
		* Example
			* Both the Stripe and Braintree payments are recast into a consistent shape, with consistent column names.
	* Marts
		* Where everything comes together in a way that business-defined entities and processes are constructed and made readily available to end users via dashboards or applications.
		* Since this layer contains models that are being accessed by end users it means that performance matters. Therefore, it makes sense to materialize them as tables.
			* If a table takes too much time to be created (or perhaps it costs too much), then you may also need to consider configuring it as an incremental model.
		* A mart model should be relatively simple and therefore, too many joins should be avoided
		* Example
			* A monthly recurring revenue (MRR) model that classifies revenue per customer per month as new revenue, upgrades, downgrades, and churn, to understand how a business is performing over time.
				* It may be useful to note whether the revenue was collected via Stripe or Braintree, but they are not fundamentally separate models.
		Base/Intermediate
			Base
				basic transformations (e.g. cleaning up the names of the columns, casting to different data types)
				
				Other models use these models as data sources
					prevents errors like accidentally casting your dates to two different types of timestamps, or giving the same column two different names.
						two different timestamp castings can cause all of the dates to be improperly joined downstream, turning the model into a huge disaster
						
				Usually occuring in staging
				
				read directly from a source, which is typically a schema in your data warehouse
					Source object
						`{{ source('campaigns', 'channel'){style='color: goldenrod'}[}}]{style='color: goldenrod'}`
						
					* campaigns is the name of the source in the .yml file
					* channel is the name of a table from that source
			Intermediate
				Brings together the atomic building blocks that reside on staging layer such that more complex and meaningful models are constructed
				
				Usually occuring in marts
				
				Additional transformations that particular marts-models require
				* Created to isolate complex operations
					Typically used for joins between multiple base models
					
			* Should not be directly exposed to end users via dashboards or applications
			* Other models should reference them as Common Table Expressions although there may be cases where it makes sense to materialize them as Views
				* Macros called via `run-operation` cannot reference ephemeral objects such as CTEs
				* Recommended to start with ephemeral objects unless this doesn’t work for the specific use case
				* Whenever you decide to materialize them as Views, it may be easier to to do so in a [custom schema](https://docs.getdbt.com/docs/build/custom-schemas), that is a schema outside of the main schema defined in your dbt profile.
			* If the same intermediate model is referenced by more than one model then it means your design has probably gone wrong.
				* Usually indicates that you should consider turning your intermediate model into a macro.
				reference the base models rather than from a source
				* Reference object
					* `{{ ref('base_campaign_types'){style='color: goldenrod'}[}}]{style='color: goldenrod'}`
					* base\_campaign\_types is a base model
* Tagging
	* Allows you to run groups of models
		* Example `dbt run --models tag:daily`

	Directories
		models
			Sources (i.e. data sources) are defined in **src**\_<source> .yml files in your models directory
			* .yml files contain definitions and tests
			* .doc files contain source documentation
		* Models (i.e. sql queries) are defined **stg**\_<source>\_yml
			* .yml files contain definitions and tests
			* .doc files contain source documentation
			* The actual models are the .sql files
			[Example]{.ribbon-highlight}
			![](./_resources/DB,_dbt.resources/Screenshot (533).png)
			* staging:
				* Different data sources will have separate folders underneath staging (e.g. stripe).
			* marts:
				* Use cases or departments have different folders underneath marts (e.g. core or marketing)
		data
			contains all manual data that will be loaded to the database by dbt.
			
			To load the .csv files in this folder to the database, you will have to run the `dbt seed` command.
			
			For github or other repos, do not put large files or files with sensitive information here
				acceptable use cases: yearly budget, status mappings, category mappings, etc
				
	* snapshots
		* captures of the state of a table at a particular time
		* [doc](https://docs.getdbt.com/docs/building-a-dbt-project/snapshots)
		* build a slowly changing dimension (SCD) table for sources that do not support change data capture (CDC)
		* Example
			* Every time the status of an order change, your system overrides it with the new information. In this case, there we cannot know what historical statuses that an order had.
			* Daily snapshots of this table builds a history and allows you to track order statuses![](./_resources/DB,_dbt.resources/1-UenQCtO0h2WLAWW5lK67DQ.png)
* Macros
	* similar to functions in excel
	* define custom functions in the macros folder or override default macros and macros from a package
	* See bkmks for tutorials on writing custom macros with jinja
	* [{]{style='color: #990000'}[dbtplyr](https://github.com/emilyriederer/dbtplyr){style='color: #990000'}[}]{style='color: #990000'} macros
		* dplyr tidy selectors, across, etc.
* Seeds
	* Seeds are csv files that you add to your dbt project to be uploaded to your data warehouse.
		* Uploaded into your data warehouse using the `dbt seed` command
	* Best suited to static data which changes infrequently.
		* Examples of use cases:
			* A list of unique codes or employee ids that you may need in your analysis but is not present in your current data.
			* A list of mappings of country codes to country names
			* A list of test emails to exclude from analysis
	* Referenced in downstream models the same way as referencing models — by using the `ref` function



Packages

* packages.yml
	* list of external dbt packages you want to use in your project
* available packages: [link](https://hub.getdbt.com/)
* format

```
        packages:
            - package: dbt-labs/dbt_utils
              version: 0.7.3
```

* Install packages - `dbt deps`
* some of the packages (there are a lot of packages and I didn't get through them all)
	* [{]{style='color: #990000'}[audit-helper](https://github.com/dbt-labs/dbt-audit-helper/tree/0.4.0/){style='color: #990000'}[}]{style='color: #990000'}
		* compares columns, queries; useful if refactoring code or migrating db
	* [{]{style='color: #990000'}[codegen](https://github.com/dbt-labs/dbt-codegen){style='color: #990000'}[}]{style='color: #990000'}
		* generate base model, barebones model and source .ymls
	* [{dbt-athena}](https://github.com/Tomme/dbt-athena) - adaptor for AWS Athena
	* [{]{style='color: #990000'}[dbt-expectations](https://hub.getdbt.com/calogica/dbt_expectations/latest/){style='color: #990000'}[}]{style='color: #990000'}
		* data validation based on great expectations py lib
	* [{]{style='color: #990000'}[dbt-utils](https://github.com/dbt-labs/dbt-utils){style='color: #990000'}[}]{style='color: #990000'}
		* ton of stuff for tests, queries, etc.
	* [{]{style='color: #990000'}[dbtplyr](https://github.com/emilyriederer/dbtplyr){style='color: #990000'}[}]{style='color: #990000'} macros
		* dplyr tidy selectors, across, etc.
	* [{]{style='color: #990000'}[dbt-duckdb](https://github.com/jwills/dbt-duckdb){style='color: #990000'}[}]{style='color: #990000'} - adapter for duckdb
	* [{]{style='color: #990000'}[external-tables](https://github.com/dbt-labs/dbt-external-tables){style='color: #990000'}[}]{style='color: #990000'}
		* create/replace/refresh external tables
		* Guessing this means any data source (e.g. s3, spark, google, another db like snowflake, etc.) that isn't the primary db connected to the dbt project
	* [{]{style='color: #990000'}[logging](https://github.com/dbt-labs/dbt-event-logging){style='color: #990000'}[}]{style='color: #990000'}
		* provides out-of-the-box functionality to log events for all dbt invocations, including run start, run end, model start, and model end.
		* can slow down runs substantially
	* [{]{style='color: #990000'}[re_data](https://docs.getre.io/latest/docs/introduction/whatis/){style='color: #990000'}[}]{style='color: #990000'}
		* dashboard for monitoring, macros, models
	* [{]{style='color: #990000'}[profiler](https://github.com/data-mie/dbt-profiler){style='color: #990000'}[}]{style='color: #990000'}
		* implements dbt macros for profiling database relations and creating doc blocks and table schemas (schema.yml) containing said profiles
	* [{]{style='color: #990000'}[spark-utils](https://github.com/dbt-labs/spark-utils){style='color: #990000'}[}]{style='color: #990000'}
		* enables use of (most of) the {dbt-utils} macros on spark



Data Validation and Unit Tests
![](./_resources/DB,_dbt.resources/image.png)

* Built-in support for CI/CD pipelines to test your "models" and stage them before committing to production
	* Misc
		* See also
			* [Docs](https://docs.getdbt.com/docs/building-a-dbt-project/tests)
			* packages (see above)
			* [How to do Unit Testing in dbt](https://towardsdatascience.com/how-to-do-unit-testing-in-dbt-cb5fb660fbd8)
* Most of the tests are defined in a models-type .yml file in the models directory
* Uses pre-made or custom macros
	* custom (aka singular) tests should be located in a tests folder.
		* dbt will evaluate the SQL statement.
		* The test will pass if no row is returned and failed if at least one or more rows are returned.
		* Useful for testing for some obscurity in the data
		* [Example]{.ribbon-highlight}: Check for duplicate rows when joining two tables

```
select 
a.id 
from {{ ref(‘table_a’) }} a 
left join {{ ref(‘table_b’) }} b 
on a.b_id = b.id 
group by a.id 
having count(b.id)>1
```

* i.e. If I join table a with table b, there should only be one record for each unique id in table a
* Process
	* join the tables on their common field
	* group them by the id that should be distinct
	* count the number of duplicates created from the join.
		* This tells me that something is wrong with the data.
		* add a having clause to filter out the non-dups

* Can be applied to a model or a column
* Run test - `dbt test`
* Mock data![](./_resources/DB,_dbt.resources/image.1.png)
	* Data used for unit testing SQL code
	* To ensure completeness, it's best if analysts or business stakeholders are the ones provide test cases or test data
	* Store in the "data" folder (typically .csv files)
		* each CSV file represents one source table
		* should be stored in a separate schema (e.g. unit\_testing) from production data
		* dbt seed (see below, Other >> seeds) command is used to load mock data into the data warehouse
* Tests
	* freshness ([docs](https://docs.getdbt.com/reference/resource-properties/freshness)) - used to define the acceptable amount of time between the most recent record, and now, for a table
		* [Example]{.ribbon-highlight}

```
sources:
  - name: users
    freshness:
      warn_after:
        count: 3
        period: day
      error_after:
        count: 5
        period: day
```

* [Example]{.ribbon-highlight}![](./_resources/DB,_dbt.resources/1-65vBHGQB1CXpJMrtRwsY3A.png)
* [Example]{.ribbon-highlight}: Unit Test![](./_resources/DB,_dbt.resources/image.1.png)
	
	* Fig shows the mock data (.csv) files
	
	* Add test to dbt\_project.yml

```
seeds:
  unit_testing:
    revenue:
      schema: unit_testing
      +tags:
        - unit_testing
```

* Every file in the unit\_testing/revenue folder will be loaded into unit\_testing
* Executing `dbt build -s +tag:unit_testing` will run all the seeds/models/tests/snapshots with tag unit\_testing and their upstreams

* Create macro that switches the source data in the model being tested from production data (i.e. using `{{ source() }}` ) to mock data (i.e. using `ref` ) when a unit test is being run

```
{% macro select_table(source_table, test_table) %}
      {% if var('unit_testing', false) == true %}

            {{ return(test_table) }}
      {% else %}
            {{ return(source_table) }}
      {% endif %}
{% endmacro %}
```

* Article calls this file "select\_table.sql"
* 2 inputs: "source\_table" (production data) and "test\_table" (mock data)
* macro returns the appropriate table based on the variable in the dbt command
	* If the command doesn’t provide unit\_testing variable or the value is false , then it returns source\_table , otherwise it returns test\_table.

* Add macro code chunk to model

```
{{ config
    (
        materialized='table',
        tags=['revenue']
    )
}}
{% set import_transaction = select_table(source('user_xiaoxu','transaction'), ref('revenue_transaction')) %}
{% set import_vat = select_table(source('user_xiaoxu','vat'), ref('revenue_vat')) %}
SELECT
    date
    , city_name
    , SUM(amount_net_booking) AS amount_net_booking
    , SUM(amount_net_booking * (1 - 1/(1 + vat_rate)))  AS amount_vat
FROM {{ import_transaction }}
LEFT JOIN {{ import_vat }} USING (city_name)
GROUP BY 1,2
```

* Inside the `{%...%}` , the macro "select\_table" is called to set the local variables, "import\_transaction" and "import\_vat" which are later used in the model query
* Model file is named "revenue2.sql"

* Run model and test using mock data: `dbt build -s +tag:unit_testing --vars 'unit_testing: true'`
	* Run model with production data (aka source data): `dbt build -s +tag:revenue --exclude tag:unit_testing`
* Compare output

```
version: 2
models:
  - name: revenue
    meta:
      owner: "@xiaoxu"
    tests:
      - dbt_utils.equality:
          compare_model: ref('revenue_expected')
          tags: ['unit_testing']
```

* model properties file that's named "revenue.yml" in the models directory
* By including `tags: ['unit_testing']` we can insure that we don't run this test in production (see build code above with `--exclude tag:unit_testing`

* Macro for comparing numeric output

```
{% test advanced_equality(model, compare_model, round_columns=None) %}
{% set compare_columns = adapter.get_columns_in_relation(model) | map(attribute='quoted') %}
{% set compare_cols_csv = compare_columns | join(', ') %}
{% if round_columns %}
    {% set round_columns_enriched = [] %}
    {% for col in round_columns %}
        {% do round_columns_enriched.append('round('+col+')') %}
    {% endfor %}
    {% set selected_columns = '* except(' + round_columns|join(', ') + "), " + round_columns_enriched|join(', ') %}
{% else %}
    {% set round_columns_csv = None %}
    {% set selected_columns = '*' %}
{% endif %}
with a as (
    select {{compare_cols_csv}} from {{ model }}
),
b as (
    select {{compare_cols_csv}} from {{ compare_model }}
),
a_minus_b as (
    select {{ selected_columns }} from a
    {{ dbt_utils.except() }}
    select {{ selected_columns }} from b
),
b_minus_a as (
    select {{ selected_columns }} from b
    {{ dbt_utils.except() }}
    select {{ selected_columns }} from a
),
unioned as (
    select 'in_actual_not_in_expected' as which_diff, a_minus_b.* from a_minus_b
    union all
    select 'in_expected_not_in_actual' as which_diff, b_minus_a.* from b_minus_a
)
select * from unioned
{% endtest %}
```

* File called "advanced\_equality.sql"













