# dbt {#sec-db-dbt .unnumbered}

## Misc {#sec-db-dbt-misc .unnumbered}

-   [List](https://docs.getdbt.com/docs/available-adapters) of available DB Adaptors

    -   Runs on Python, so adaptors are installed via pip

-   Notes from

    -   [Anatomy of a dbt project](https://towardsdatascience.com/anatomy-of-a-dbt-project-50e810abc695)
    -   [What is dbt?](https://towardsdatascience.com/what-is-dbt-a0d91109f7d0)
    -   [Docs](https://docs.getdbt.com/docs/introduction)

-   Resources

    -   [Create a Local dbt Project](https://towardsdatascience.com/create-local-dbt-project-e12c31bd3992)
        -   Uses docker containers to set up a local dbt project and a local postgres db to play around with

-   Architecture\
    ![](_resources/DB,_dbt.resources/architecture-1.png){.lightbox width="332"}

-   Typical Workflow

    ``` bash
    dbt deps
    dbt seed
    dbt snapshot
    dbt run
    dbt run-operation {{ macro_name }}
    dbt test
    ```

-   Style Guide Components

    -   Naming conventions (the case to use and tense of the column names)
    -   SQL best practices (commenting code, CTEs, subqueries, etc.)
    -   Documentation standards for your models
    -   Data types of date, timestamp, and currency columns
    -   Timezone standards for all dates

-   [Castor](https://www.castordoc.com/) - tool that takes your project and autofills much of the documentation\
    ![](./_resources/DB,_dbt.resources/image.3.png){.lightbox width="432"}

    -   Has a free tier
    -   Very helpful if you have the same column name in multiple datasets, you don’t have to keep defining it
    -   Tribal Knowledge
        -   When a dataset is discussed in a team slack channel, Castor pulls the comments and adds them to the documentation of the dataset

-   [Lightdash](https://www.lightdash.com/) - BI tool for dbt projects - free tier for self hosting

## Set-Up {#sec-db-dbt-setup .unnumbered}

-   Basic set-up: [Article](https://towardsdatascience.com/getting-hands-on-with-dbt-data-build-tool-a157d4151bbc)
    -   Example uses postgres adaptor
-   Within a python virtual environment
    -   Create: `python3 -m vevn dbt-venv`

    -   Activate: `source dbt-venv/bin/activate`

        -   Should be able to see a `(dbt-venv)` prefix in every line on the terminal

    -   Install dbt-core: `pip install dbt-core`

        -   Specific version: `pip install dbt-core==1.3.0`
        -   Confirm installation by checking version: `dbt --version`

    -   Install plugins

        ``` bash
        pip install dbt-bigquery
        pip install dbt-spark
        # etc...
        ```

## Description {#sec-db-dbt-desc .unnumbered}

-   Built for data modeling
    -   **Models** are like sql queries
-   Current understanding
    -   Data is brought in from warehouses via base models and basic transformations are performed (models \>\> staging directory)
    -   Then the data is transformed to the desired state via intermediate models and calculations performed (models \>\> marts directory)
    -   Then the final product is stored in the data directory
-   Modularizes SQL code and makes it reusable across "models"\
    ![](./_resources/DB,_dbt.resources/1_uf-Jo4ybZT0qdsCHNZavGg%20(2).png){.lightbox width="332"}
    -   Running the [orders]{.var-text} model also runs the [base_orders]{.var-text} model and [base_payments]{.var-text} model (i.e. dependencies for [orders]{.var-text})
        -   Not sure this exactly right. Seems like doing this would result in wasting time rerunning the same dependencies multiple times
    -   [base_orders]{.var-text} and [base_payments]{.var-text} are independent in that they can also be used in other models
    -   Creates more dependable code because you’re using the same logic in all your models
    -   Makes runs faster since you aren’t wasting time and resources running the same blocks of code over and over again
-   You can schedule running sets of models by tagging them (e.g. #daily, #weekly)
-   Version Control
    -   **Snapshots** provide a mechanism for versioning datasets
    -   Within every yaml file is an option to include the version
-   Package add-ons that allow you to interact with spark, snowflake, duckdb, redshift, etc.
-   Documentation for every step of the way
    -   .yml files can be used to generate a website (localhost:8080) around all of your dbt documentation.\
        ![](./_resources/DB,_dbt.resources/1-xn3o7SBakX5qcFWLks4DCQ.png){.lightbox width="432"}

        ``` bash
        dbt docs generate
        dbt docs serve
        ```

## Optimizations {#sec-db-dbt-opt .unnumbered}

-   Runs parallelized
    -   Models that have dependencies aren’t run until their upstream models are completed but models that don’t depend on one another are run at the same time.
    -   The [thread]{.arg-text} parameter in your `dbt_project.yml` specifies how many models are permitted to run in parallel
-   Incremental Model
    -   See [Components \>\> Materializations](db-dbt.qmd#sec-db-dbt-comp-mats){style="color: green"}
    -   Saves money by not rebuilding the entire table from scratch when records only need to be updated or added to a table (plus transformations to the new records).
    -   Useful for big data tables and expensive transformations

## Project Files {#sec-db-dbt-projfil .unnumbered}

-   [Project Templates]{.underline}

    -   [Style Guide](https://github.com/dbt-labs/corp/blob/master/dbt_style_guide.md)
        -   More detailed: [link](https://discourse.getdbt.com/t/how-we-structure-our-dbt-projects/355)
    -   [Example]{.ribbon-highlight} [Starter Project](https://github.com/dbt-labs/dbt-init/tree/master/starter-project)\
        ![](./_resources/DB,_dbt.resources/1_bgUI3LTLdY19EFRwSyZh1A%20(2).png){.lightbox width="115"}

-   `profiles.yml`

    -   Not included in project directory
    -   Only have to worry about this file if you set up dbt locally.
    -   [doc](https://docs.getdbt.com/dbt-cli/configure-your-profile)
    -   Created by `dbt init` in `~/.dbt/`
    -   Contents
        -   database connection, database credentials that dbt will use to connect to the data warehouse
        -   If you work on multiple projects locally, the different project names (configured in the `dbt_project.yml` file) will allow you to set up various profiles for other projects.
        -   ... something about "targets" but not sure what this is or how it's used

-   `dbt_project.yml`

    -   [Docs](https://docs.getdbt.com/reference/dbt_project.yml)
    -   Main configuration file for your project
    -   Fill in your project name and profile name\
        ![](./_resources/DB,_dbt.resources/image.2.png){.lightbox width="432"}
    -   Add variables and models

-   `packages.yml`

    -   List of external dbt packages you want to use in your project

    -   Format

        ``` yaml
        packages:
            - package: dbt-labs/dbt_utils
              version: 0.7.3
        ```

## Components {#sec-db-dbt-comp .unnumbered}

### Materializations {#sec-db-dbt-comp-mats .unnumbered}

-   The manner in which the data is represented
-   [Docs](https://docs.getdbt.com/best-practices/materializations/1-guide-overview)
-   Start with models as views, when they take too long to query, make them tables, when the tables take too long to build, make them incremental.
-   View
    -   Default materialization when there is no specification in the configuration
    -   Just like Standard Views, i.e. a saved SQL query (See [SQL \>\> Views](sql.qmd#sec-sql-views){style="color: green"})
    -   Always reflects the most up-to-date version of the input data
    -   Cost nothing to build since they're just queries, but have to be processed every time they’re ran.
        -   Slower to return results than a table of the same data. 
        -   Can cost more *over time*, especially if they contain intensive transformations and are queried often.
    -   Best for
        -   Building a model that stitches lots of other models together (Don't want to worry about freshness of ever table involved).
        -   Small datasets with minimally intensive logic that you want near realtime access to
-   Table
    -   Stores the literal rows and columns on disk (i.e. materialized view)
    -   Ideal for models that get queried regularly
    -   Reflects the most recent run on the source data that was available
    -   Best for sources for BI products and other frontend apps (e.g. like a mart that services a popular dashboard)
-   Incremental Model
    -   [Docs](https://docs.getdbt.com/best-practices/materializations/4-incremental-models)
    -   Builds a table in pieces over time — only adding and updating new or changed records which makes them efficient.
    -   Builds more quickly than a regular table of the same logic.
    -   Typically used on very large datasets, so building the initial table on the full dataset is time consuming and equivalent to the table materialization.
    -   Adds complexity and requires deeper consideration of layering and timing.
        -   See "Late arriving facts" section of the Docs
    -   Can drift from source data over time since you're not processing all of the source data. Extra effort is required to capture changes to historical data.
        -   See "Long-term considerations" section of the Docs
    -   Best for — same as tables, but for larger datasets.
    -   Requirements
        -   A *filter* to select just the new or updated records
        -   A *conditional block* that wraps the filter and only applies it when we want it
        -   A *configuration* that tells dbt to build incrementally and helps apply the conditional filter when needed
-   Set-Up
    -   In practice, you want to set materializations at the folder level, and use individual model configs to override those as needed.

    -   In `dbt_project.yml`

        -   A [+]{.arg-text} is used to indicate a materialization

        -   [Example]{.ribbon-highlight}: models

            ``` yaml
            models:
              jaffle_shop:
                marketing:
                  +materialized: view
                  paid_ads:
                    google:
                      +materialized: table
            ```

            -   All models in the [marketing]{.var-text} and [paid_ads]{.var-text} folders are *views* while models in the [google]{.var-text} folder are *tables*

    -   In Model files

        -   [Example]{.ribbon-highlight}: Table (or View)

            ``` sql
            {{
                config(
                    materialized='table'
                )
            }}

            select ...
            ```

        -   [Example]{.ribbon-highlight}: Incremental

            ``` sqlpostgresql
            {{
                config(
                    materialized='incremental',
                    unique_key='order_id' # <1>
                )
            }}

            select * from orders

            {% if is_incremental() %} # <2>

            where
              updated_at > (select max(updated_at) from {{ this }}) # <3>

            {% endif %}
            ```

            1.  [unique_key]{.arg-text} specifies the field that allows dbt to find a record from the previous run (old data) with the same unique id as the new data in order to *update* that record instead of adding it as a separate row.
            2.  An if-else conditional that uses `is_incremental` (macro) to check if the systematic conditions necessary for an incremental model are met (See below)
            3.  The *conditional block* containing the *filter* that finds the latest [updated_at]{.var-text} date, so dbt knows whether the source data has new records that need to be *added* or not.

            -   `{{ this }}` means *filter* from "this" materialization
            -   `is_incremental` (macro) checks:
                -   [materialized ='incremental']{.arg-text}
                -   There is an existing table for this model in the warehouse to add to/update
                -   `--full-refresh` flag was not passed ([docs](https://docs.getdbt.com/reference/resource-configs/full_refresh))
                    -   Overrides the incremental materialization and builds the table from scratch again
                    -   Useful if the table drifts from the source table over time.

### Variables {#sec-db-dbt-comp-var .unnumbered}

-   Defined in the `project.yml` and used in models

    -   [Example]{.ribbon-highlight}: Assigning States to Regions

        ``` yaml
        vars:
          state_lookup:
            Northeast:
              - CT
              - ME
            Midwest:
              - IL
              - IN
        ```

-   Using the variables in a model

    ``` sql
    {# Option 1 #}
    SELECT state,
          CASE {% for k, v in var("state_lookup").items() %}
                WHEN state in ({% for t in v %}'{{ t }}'{% if not loop.last %}, {% endif %}{% endfor %}) THEN {{ k }}{% endfor %}
                ELSE NULL END AS region
      FROM {{ ref('my_table') }}

    {# Option 2 #}
    SELECT state,
          CASE {% for k, v in var("state_lookup").items() %}
                WHEN state in ({{ t|csl }}) THEN {{ k }}{% endfor %}
                ELSE NULL END AS region
      FROM {{ ref('my_table') }}
    ```

    -   Variables are accessed using `var`
    -   This is a complicated example, see [docs](https://docs.getdbt.com/reference/dbt-jinja-functions/var) for something simpler
    -   `{% ... %}` are used to encapsulate for-loops and if-then conditions, see [docs](https://docs.getdbt.com/docs/building-a-dbt-project/jinja-macros)
        -   `{# ... #}` is for comments
    -   Option 2 uses a csl filter (comma-separated-list)

### Models {#sec-db-dbt-comp-mod .unnumbered}

#### Misc {#sec-db-dbt-comp-mod-misc .unnumbered}

-   Naming Conventions
    -   A prefix for the layer the model exists in, important grouping information, and specific information about the entity or transformation in the model
    -   `stg_[source]__[entity]s.sql` - The double underscore between source system and entity helps visually distinguish the separate parts in the case of a source name having multiple words.
-   Tagging
    -   Allows you to run groups of models
        -   [Example]{.ribbon-highlight}`dbt run --models tag:daily`

#### Best Practices {#sec-db-dbt-comp-mod-bp .unnumbered}

-   Modularity where possible

    -   Same as the functional mindset: "if there's any code that's continually repeated, then it should be a function(i.e. its own separate model in dbt)."

-   Readability

    -   Comment
    -   Use CTEs instead of subqueries
    -   Use descriptive names
        -   [Example]{.ribbon-highlight}: if you are joining the tables "users" and "addresses" in a CTE, you would want to name it "users_joined_addresses" instead of "user_addresses"

-   [Example]{.ribbon-highlight}: Comments, CTE, Descriptive Naming

    ``` sql
    WITH
    Active_users AS (
      SELECT
        Name AS user_name,
        Email AS user_email,
        Phone AS user_phone,
        Subscription_id
      FROM users
      --- status of 1 means a subscription is active
      WHERE subscription_status = 1
    ),
    Active_users_joined_subscriptions AS (
      SELECT
        Active_users.user_name,
        active_users.user_email,
        Subscriptions.subscription_id,
        subscriptions.start_date ,
        subscriptions.subscription_length
      FROM active_users
      LEFT JOIN subscriptions
        ON active_users.subscription_id = subscriptions.subscription_id
    )
    SELECT * FROM Active_users_joined_subscriptions
    ```

#### Layers {#sec-db-dbt-comp-mod-lay .unnumbered}

##### Outer Layers {#sec-db-dbt-comp-mod-lay-outer .unnumbered}

-   [Staging]{.underline}

    -   Subdirectories based on the data sources (*not* business entities)
    -   Contains all the individual components of your project that the other layers will use in order to craft more complex data models.
    -   Each model bears a one-to-one relationship with the source data table it represents (i.e. 1 staging model per source)
    -   Typical Transformations: recasting, column renaming, basic computations (such as KBs to MBs or GBs), categorization (e.g. using CASE WHEN statements).
        -   Aggregations and joins should also be avoided
    -   Usually materialized as views.
        -   Allows any intermediate or mart models referencing the staging layer to get access to fresh data and at the same time it saves us space and reduces costs.

        -   In `dbt_project.yml`

            ``` yaml
            models:
              jaffle_shop:
                staging:
                  +materialized: view
            ```

            -   View is the default materialization, but still good to set manually just for clarification purposes.
    -   [Example]{.ribbon-highlight}
        -   Both the Stripe and Braintree payments are recast into a consistent shape, with consistent column names.

-   [Marts]{.underline}

    -   Where everything comes together in a way that business-defined entities and processes are constructed and made readily available to end users via dashboards or applications.
    -   Since this layer contains models that are being accessed by end users it means that performance matters. Therefore, it makes sense to materialize them as tables.
        -   If a table takes too much time to be created (or perhaps it costs too much), then you may also need to consider configuring it as an incremental model.
    -   A mart model should be relatively simple and therefore, too many joins should be avoided
    -   [Example]{.ribbon-highlight}
        -   A monthly recurring revenue (MRR) model that classifies revenue per customer per month as new revenue, upgrades, downgrades, and churn, to understand how a business is performing over time.
            -   It may be useful to note whether the revenue was collected via Stripe or Braintree, but they are not fundamentally separate models.

##### Inner Layers {#sec-db-dbt-comp-mod-lay-inner .unnumbered}

-   [Base]{.underline}
    -   Basic transformations (e.g. cleaning up the names of the columns, casting to different data types)
    -   Other models use these models as data sources
        -   Prevents errors like accidentally casting your dates to two different types of timestamps, or giving the same column two different names.
            -   Two different timestamp castings can cause all of the dates to be improperly joined downstream, turning the model into a huge disaster
    -   Usually occuring in staging
    -   Read directly from a source, which is typically a schema in your data warehouse
        -   Source object: `{{ source('campaigns', 'channel') }}`
            -   [campaigns]{.var-text} is the name of the source in the .yml file
            -   [channel]{.var-text} is the name of a table from that source
-   [Intermediate]{.underline}
    -   Brings together the atomic building blocks that reside in staging layer such that more complex and meaningful models are constructed
    -   Usually occuring in marts
    -   Additional transformations that particular marts-models require
        -   Created to isolate complex operations
            -   Typically used for joins between multiple base models
    -   Should not be directly exposed to end users via dashboards or applications
    -   Other models should reference them as Common Table Expressions although there may be cases where it makes sense to materialize them as Views
        -   Macros called via `run-operation` cannot reference ephemeral objects such as CTEs
        -   Recommended to start with ephemeral objects unless this doesn’t work for the specific use case
    -   Whenever you decide to materialize them as Views, it may be easier to to do so in a [custom schema](https://docs.getdbt.com/docs/build/custom-schemas), that is a schema outside of the main schema defined in your dbt profile.
    -   If the same intermediate model is referenced by more than one model then it means your design has probably gone wrong.
        -   Usually indicates that you should consider turning your intermediate model into a macro. reference the base models rather than from a source
        -   Reference object
            -   `{{ ref('base_campaign_types') }}`
            -   [base_campaign_types]{.var-text} is a base model

### Macros {#sec-db-dbt-comp-mac .unnumbered}

-   Similar to functions in excel
-   Define custom functions in the macros folder or override default macros and macros from a package
-   See bkmks for tutorials on writing custom macros with jinja
-   [{]{style="color: #990000"}[dbtplyr](https://github.com/emilyriederer/dbtplyr){style="color: #990000"}[}]{style="color: #990000"} macros
    -   dplyr tidy selectors, across, etc.

### Seeds {#sec-db-dbt-comp-seed .unnumbered}

-   Seeds are csv files that you add to your dbt project to be uploaded to your data warehouse.
    -   Uploaded into your data warehouse using the `dbt seed` command
-   Best suited to static data which changes infrequently.
    -   Use Cases:
        -   A list of unique codes or employee ids that you may need in your analysis but is not present in your current data.
        -   A list of mappings of country codes to country names
        -   A list of test emails to exclude from analysis
-   Referenced in downstream models the same way as referencing models — by using the `ref` function

## Directories {#sec-db-dbt-dir .unnumbered}

-   [Models]{.underline}

    -   Sources (i.e. data sources) are defined in `src_<source>.yml` files in your models directory
        -   .yml files contain definitions and tests
        -   .doc files contain source documentation
    -   Models (i.e. sql queries) are defined `stg_<source>.yml`
        -   .yml files contain definitions and tests
        -   .doc files contain source documentation
        -   The actual models are the .sql files
    -   [Example]{.ribbon-highlight}\
        ![](./_resources/DB,_dbt.resources/Screenshot%20(533).png){.lightbox width="432"}
        -   Staging:
            -   Different data sources will have separate folders underneath staging (e.g. stripe).
        -   Marts:
            -   Use cases or departments have different folders underneath marts (e.g. core or marketing)

-   [Data]{.underline}

    -   Contains all manual data that will be loaded to the database by dbt.
    -   To load the .csv files in this folder to the database, you will have to run the `dbt seed` command.
    -   For github or other repos, do not put large files or files with sensitive information here
        -   Acceptable use cases: yearly budget, status mappings, category mappings, etc

-   [Snapshots]{.underline}

    -   Captures of the state of a table at a particular time
    -   [Docs](https://docs.getdbt.com/docs/building-a-dbt-project/snapshots)
    -   build a slowly changing dimension (SCD) table for sources that do not support change data capture (CDC)
    -   [Example]{.ribbon-highlight}
        -   Every time the status of an order change, your system overrides it with the new information. In this case, there we cannot know what historical statuses that an order had.
        -   Daily snapshots of this table builds a history and allows you to track order statuses\
            ![](./_resources/DB,_dbt.resources/1-UenQCtO0h2WLAWW5lK67DQ.png){width="382"}

## Packages {#sec-db-dbt-pkgs .unnumbered}

-   Available Packages: [link](https://hub.getdbt.com/)
-   Install Packages - `dbt deps`
-   Packages
    -   [audit-helper](https://github.com/dbt-labs/dbt-audit-helper/tree/0.4.0/)
        -   Compares columns, queries; useful if refactoring code or migrating db
    -   [codegen](https://github.com/dbt-labs/dbt-codegen)
        -   Generate base model, barebones model and source .ymls
    -   [dbt-athena](https://github.com/Tomme/dbt-athena) - Adaptor for AWS Athena
    -   [dbt-expectations](https://hub.getdbt.com/calogica/dbt_expectations/latest/)
        -   Data validation based on great expectations py lib
    -   [dbt-utils](https://github.com/dbt-labs/dbt-utils)
        -   Ton of stuff for tests, queries, etc.
    -   [dbtplyr](https://github.com/emilyriederer/dbtplyr) - Macros
        -   dplyr tidy selectors, across, etc.
    -   [dbt-duckdb](https://github.com/jwills/dbt-duckdb) - Adapter for duckdb
    -   [external-tables](https://github.com/dbt-labs/dbt-external-tables)
        -   Create or replace or refresh external tables
        -   Guessing this means any data source (e.g. s3, spark, google, another db like snowflake, etc.) that isn't the primary db connected to the dbt project
    -   [logging](https://github.com/dbt-labs/dbt-event-logging)
        -   Provides out-of-the-box functionality to log events for all dbt invocations, including run start, run end, model start, and model end.
        -   Can slow down runs substantially
    -   [re_data](https://docs.getre.io/latest/docs/introduction/whatis/)
        -   Dashboard for monitoring, macros, models
    -   [profiler](https://github.com/data-mie/dbt-profiler)
        -   Implements dbt macros for profiling database relations and creating doc blocks and table schemas (schema.yml) containing said profiles
    -   [spark-utils](https://github.com/dbt-labs/spark-utils)
        -   Enables use of (most of) the {dbt-utils} macros on spark

## Data Validation and Unit Tests {#sec-db-dbt-dvaut .unnumbered}

![](./_resources/DB,_dbt.resources/image.png){.lightbox width="532"}

-   Misc
    -   Built-in support for CI/CD pipelines to test your "models" and stage them before committing to production
    -   Run test - `dbt test`
    -   See also
        -   [Docs](https://docs.getdbt.com/docs/building-a-dbt-project/tests)
        -   [Packages](db-dbt.qmd#sec-db-dbt-pkgs){style="color: green"}
        -   [How to do Unit Testing in dbt](https://towardsdatascience.com/how-to-do-unit-testing-in-dbt-cb5fb660fbd8)
    -   Most of the tests are defined in a models-type .yml file in the models directory
    -   Can be applied to a model or a column
-   Macros: Pre-Made or Custom
    -   Custom (aka Singular) tests should be located in a tests folder.
        -   dbt will evaluate the SQL statement.

        -   The test will pass if no row is returned and failed if at least one or more rows are returned.

        -   Useful for testing for some obscurity in the data

        -   [Example]{.ribbon-highlight}: Check for duplicate rows when joining two tables

            ``` sql
            select
            a.id
            from {{ ref(‘table_a’) }} a
            left join {{ ref(‘table_b’) }} b
            on a.b_id = b.id
            group by a.id
            having count(b.id)>1
            ```

            -   i.e. If I join table a with table b, there should only be one record for each unique id in table a
            -   Process
                -   Join the tables on their common field
                -   Group them by the id that should be distinct
                -   Count the number of duplicates created from the join.
                    -   This tells me that something is wrong with the data.
                    -   Add a having clause to filter out the non-dups
-   Mock data\
    ![](./_resources/DB,_dbt.resources/image.1.png){.lightbox width="175"}
    -   Data used for unit testing SQL code
    -   To ensure completeness, it's best if analysts or business stakeholders are the ones provide test cases or test data
    -   Store in the "data" folder (typically .csv files)
        -   each CSV file represents one source table
        -   should be stored in a separate schema (e.g. unit_testing) from production data
        -   dbt seed (see below, Other \>\> seeds) command is used to load mock data into the data warehouse
-   Tests
    -   Freshness ([docs](https://docs.getdbt.com/reference/resource-properties/freshness)) - Used to define the acceptable amount of time between the most recent record, and now, for a table
        -   [Example]{.ribbon-highlight}

            ``` yaml
            sources:
              - name: users
                freshness:
                  warn_after:
                    count: 3
                    period: day
                  error_after:
                    count: 5
                    period: day
            ```
-   [Example]{.ribbon-highlight}: `project.yml`\
    ![](./_resources/DB,_dbt.resources/1-65vBHGQB1CXpJMrtRwsY3A.png){.lightbox width="532"}
-   [Example]{.ribbon-highlight}: Unit Test
    -   Add test to `dbt_project.yml`

        ``` yaml
        seeds:
          unit_testing:
            revenue:
              schema: unit_testing
              +tags:
                - unit_testing
        ```

        -   Every file in the unit_testing/revenue folder will be loaded into unit_testing
        -   Executing `dbt build -s +tag:unit_testing` will run all the seeds/models/tests/snapshots with tag unit_testing and their upstreams

    -   Create macro that switches the source data in the model being tested from production data (i.e. using `{{ source() }}` ) to mock data (i.e. using `ref` ) when a unit test is being run

        ``` sql
        {% macro select_table(source_table, test_table) %}
              {% if var('unit_testing', false) == true %}

                    {{ return(test_table) }}
              {% else %}
                    {{ return(source_table) }}
              {% endif %}
        {% endmacro %}
        ```

        -   Article calls this file "select_table.sql"
        -   2 inputs: "source_table" (production data) and "test_table" (mock data)
        -   macro returns the appropriate table based on the variable in the dbt command
            -   If the command doesn’t provide unit_testing variable or the value is false , then it returns source_table , otherwise it returns test_table.

    -   Add macro code chunk to model

        ``` sql
        {{ config
            (
                materialized='table',
                tags=['revenue']
            )
        }}
        {% set import_transaction = select_table(source('user_xiaoxu','transaction'), ref('revenue_transaction')) %}
        {% set import_vat = select_table(source('user_xiaoxu','vat'), ref('revenue_vat')) %}
        SELECT
            date
            , city_name
            , SUM(amount_net_booking) AS amount_net_booking
            , SUM(amount_net_booking * (1 - 1/(1 + vat_rate)))  AS amount_vat
        FROM {{ import_transaction }}
        LEFT JOIN {{ import_vat }} USING (city_name)
        GROUP BY 1,2
        ```

        -   Inside the `{%...%}` , the macro "select_table" is called to set the local variables, "import_transaction" and "import_vat" which are later used in the model query
        -   Model file is named "revenue2.sql"

    -   Run model and test using mock data: `dbt build -s +tag:unit_testing --vars 'unit_testing: true'`

        -   Run model with production data (aka source data): `dbt build -s +tag:revenue --exclude tag:unit_testing`

    -   Compare output

        ``` yaml
        version: 2
        models:
          - name: revenue
            meta:
              owner: "@xiaoxu"
            tests:
              - dbt_utils.equality:
                  compare_model: ref('revenue_expected')
                  tags: ['unit_testing']
        ```

        -   Model properties file that's named `revenue.yml` in the models directory
        -   By including [tags: \['unit_testing'\]]{.arg-text} we can insure that we don't run this test in production (see build code above with `--exclude tag:unit_testing`

    -   Macro for comparing numeric output

        ``` {.sql .jinja}
        {% test advanced_equality(model, compare_model, round_columns=None) %}
        {% set compare_columns = adapter.get_columns_in_relation(model) | map(attribute='quoted') %}
        {% set compare_cols_csv = compare_columns | join(', ') %}
        {% if round_columns %}
            {% set round_columns_enriched = [] %}
            {% for col in round_columns %}
                {% do round_columns_enriched.append('round('+col+')') %}
            {% endfor %}
            {% set selected_columns = '* except(' + round_columns|join(', ') + "), " + round_columns_enriched|join(', ') %}
        {% else %}
            {% set round_columns_csv = None %}
            {% set selected_columns = '*' %}
        {% endif %}
        with a as (
            select {{compare_cols_csv}} from {{ model }}
        ),
        b as (
            select {{compare_cols_csv}} from {{ compare_model }}
        ),
        a_minus_b as (
            select {{ selected_columns }} from a
            {{ dbt_utils.except() }}
            select {{ selected_columns }} from b
        ),
        b_minus_a as (
            select {{ selected_columns }} from b
            {{ dbt_utils.except() }}
            select {{ selected_columns }} from a
        ),
        unioned as (
            select 'in_actual_not_in_expected' as which_diff, a_minus_b.* from a_minus_b
            union all
            select 'in_expected_not_in_actual' as which_diff, b_minus_a.* from b_minus_a
        )
        select * from unioned
        {% endtest %}
        ```

        -   File called "advanced_equality.sql"
