# DL {#sec-diag-dl .unnumbered}

## Misc {#sec-diag-dl-misc .unnumbered}

-   Packages
    -   [{{]{style="color: goldenrod"}[weightwatcher](https://weightwatcher.ai/){style="color: goldenrod"}[}}]{style="color: goldenrod"} - Diagnostic tool for analyzing Deep Neural Networks (DNN), without needing access to training or even test data. It is based on theoretical research into Why Deep Learning Works, using the new Theory of Heavy-Tailed Self-Regularization (HT-SR)
    -   [{]{style="color: #990000"}[innsight](https://cran.r-project.org/web/packages/innsight/index.html){style="color: #990000"}[}]{style="color: #990000"} ([Vignette](https://www.jstatsoft.org/article/view/v111i08)) - Interpretation methods for analyzing the behavior and individual predictions of modern neural networks in a three-step procedure: Converting the model, running the interpretation method, and visualizing the results
-   Notes from
    -   Raschka ([Thread](https://twitter.com/rasbt/status/1565798671781961728))
    -   [Learning Curves: A Picture Says More Than a Thousand Words](https://pub.towardsai.net/learning-curves-a-picture-says-more-than-a-thousand-words-45dc24835069)

## General {#sec-diag-dl-gen .unnumbered}

-   Look at failure cases
    -   Always useful to check what cases the model gets wrong.
    -   Analysis of these cases might detect things like mislabeled data
-   Plot at a confusion matrix
    -   [Example]{.ribbon-highlight}: PyTorch digit classifier

        ``` python
        import matplotlib

        from mlxtend.plotting import plot_confusion_matrix
        from torchmetrics import ConfusionMatrix

        cmat = ConfusionMatrix(num_classes=len(class_dict))

        for x, y in dm.test_dataloader():

          with torch.inference.mode():
            pred = lightning_model(x)
          cmat(pred, y)

        cmat_tensor = cmat.compute()
        cmat = cmat_tensor.numpy()

        fig, ax = plot_confusion_matrix(
          conf_mat=cmat,
          class_names=class_dict.values(),
          norm_colormap=matplotlib.colors.LogNorm()
        )
        plt.xticks(rotation=45, ha="right", rotation_mode="anchor")

        plt.savefig('cm.pdf')
        plt.show()
        ```

        ![](./_resources/Diagnostics,_DL.resources/raschka-confmat-1.png){.lightbox width="432"}
-   New Architecture
    -   Check that you can overfit 1000 data points, by using the same training and validation.
        -   PyTorch Lightning has this flag
        -   The loss should be near zero (because the network should be able to memorize it); if not, there's a bug in your code.
-   Run [{{weightwatcher}}}}]{style="color: goldenrod"} and check that the layers have converged individually to a good alpha, and exhibit no rank collapse or correlation traps.

## Learning Curves {#sec-diag-dl-lc .unnumbered}

-   Make sure training loss converged\
    ![](./_resources/Diagnostics,_DL.resources/image.png){.lightbox width="732"}

    -   Want to see a plateau in the loss (y-axis)
    -   Left: bad - Increase complexity; Increase training iterations
    -   Right: better

-   Check for overfitting\
    ![](./_resources/Diagnostics,_DL.resources/image.1.png){.lightbox width="732"}

    -   Don't want the gap between training and validation accuracy to be too large
    -   Left: bad - Reduce complexity; Regularization
        -   This lack fo generalization could also indicate that the training set is too small or the training and validation set come from different distributions
    -   Right: better

-   Don't want a U-shape in the validation loss\
    ![](_resources/Diagnostics,_DL.resources/cv-early-stopping-1.jpg){.lightbox width="332"}

    -   Use early stopping to avoid overfitting (i.e. when th validation starts turning upwards)

-   Issues with the Validation Set

    ::: {layout-ncol="2"}
    ![](_resources/Diagnostics,_DL.resources/cv-unstable-val-loss-1.jpg){.lightbox width="382"}

    ![](_resources/Diagnostics,_DL.resources/cv-val-loss-smaller-1.jpg){.lightbox width="382"}
    :::

    -   Validation set could be too small or easier to predict

-   Unstable Training Loss\
    ![](_resources/Diagnostics,_DL.resources/cv-unstable-training-1.jpg){.lightbox width="332"}

    -   Typically indicates a wrong choice of hyperparameters.
    -   Solutions can be to decrease the learning rate or increase the batch size.

-   Compare accuracy to a zero-rule baseline

    -   Check that the validation accuracy is substantially better than a baseline based on always predicting the majority class (aka zero-rule classifier)\
        ![](./_resources/Diagnostics,_DL.resources/image.2.png){.lightbox width="732"}
        -   Top chunk of code is just to determine which class is the majority class, which is class 1 with 1135 observations (aka examples)
        -   Bottom chunk calculates the accuracy if a model just choose to classify each observation as class 1
