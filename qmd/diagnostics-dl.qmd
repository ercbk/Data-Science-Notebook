# Diagnostics, DL

TOC




Misc

* Raschka ([thread](https://twitter.com/rasbt/status/1565798671781961728))
	* General
		* Make sure training loss converged
			* Want to see a plateau in the loss (y-axis)![](./_resources/Diagnostics,_DL.resources/image.png)
				* Left: bad; Right: better
		* Check for overfitting
			* Don't want the gap between training and validation accuracy to be too large![](./_resources/Diagnostics,_DL.resources/image.1.png)
				* Left: bad; Right: better
		* Compare accuracy to a zero-rule baseline
			* Check that the validation accuracy is substantially better than a baseline based on always predicting the majority class (aka zero-rule classifier)![](./_resources/Diagnostics,_DL.resources/image.2.png)
				* Top chunk of code is just to determine which class is the majority class, which is class 1 with 1135 observations (aka examples)
				* Bottom chunk calculates the accuracy if a model just choose to classify each observation as class 1
		* Look at failure cases
			* Always useful to check what cases the model gets wrong.
			* Analysis of these cases might detect things like mislabeled data
		* Plot at a confusion matrix
			* Example: PyTorch digit classifier![](./_resources/Diagnostics,_DL.resources/image.3.png)
	* New Architecture
		* Check is you can overfit 1000 data points, by using the same training and validation.
			* PyTorch Lightning has this flag
			* The loss should be near zero (because the network should be able to memorize it); if not, there's a bug in your code.
	* Run weightwatcher and check that the layers have converged individually to a good alpha, and exhibit no rank collapse or correlation traps.























