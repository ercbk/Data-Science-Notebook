# Lakes {#sec-db-lakes .unnumbered}

## Misc {#sec-db-lakes-misc .unnumbered}

-   Data is stored in structured format or in its raw native format without any transformation at any scale.
    -   Handling both types allows all data to be centralized which means it can be better organized and more easily accessed.
-   Optimal for fit for bulk data types such as server logs, clickstreams, social media, or sensor data.
-   Ideal use cases
    -   Backup for logs
    -   Raw sensor data for your IoT application,
    -   Text files from user interviews
    -   Images
    -   Trained machine learning models (with the database simply storing the path to the object)
-   Tools
    -   [Rclone](https://rclone.org/) - A command-line program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors' web storage interfaces. [Over 70 cloud storage products](https://rclone.org/#providers) support rclone including S3 object stores
    -   [{]{style="color: #990000"}[pins](https://pins.rstudio.com/){style="color: #990000"}[}]{style="color: #990000"}, [{{]{style="color: goldenrod"}[pins](https://rstudio.github.io/pins-python/){style="color: goldenrod"}[}}]{style="color: goldenrod"}
        -   Posit's Pins [Docs](https://docs.posit.co/connect/user/pins/)
        -   Articles
            -   [Automating data updates with R: Tracking changes in a GitHub-hosted dataset](https://medium.com/@oyogoc/automating-data-updates-with-r-c99e34980663)
                -   Pulls sha from dataset repo and compares to the old one to determine if data needs update. Logs the sha and any error messages with [{logger}]{style="color: #990000"}. If there's new data, uses [{pins}]{style="color: #990000"} to download dataset and version it.
                -   There's not much to this script, but I think [{git2r}]{style="color: #990000"} might've been nicer that using [{httr}]{style="color: #990000"} and [{jsonlite}]{style="color: #990000"} to manually pull from the github API
        -   Convenient storage method
            -   Can be used for caching
        -   Can be automatically versioned, making it straightforward to track changes, re-run analyses on historical data, and undo mistakes
        -   Needs to be manually refreshed
            -   i.e. Update data model, etc. and run script that rights it to the board.
        -   Use when:
            -   Object is less than a 1 Gb
                -   Use [{butcher}]{style="color: #990000"} for large model objects
                    -   Some model objects store training data
        -   Benefits
            -   Just need the pins board name and name of pinned object
                -   Think the set-up is supposed to be easy
            -   Easy to share; don't need to understand databases
        -   Boards
            -   Folders to share on a networked drive or with services like DropBox

            -   Posit Connect, Amazon S3, Google Cloud Storage, Azure storage, [Databricks](https://posit.co/blog/pins-in-databricks/) and Microsoft 365 (OneDrive and SharePoint)

            -   [Example]{.ribbon-highlight}: Pull data, clean and write to board

                ``` r
                board <-
                  pins::board_connect(
                    auth = "manual",
                    server = Sys.getenv("CONNECT_SERVER"),
                    key = Sys.getenv("CONNECT_API_KEY")
                  )

                # code to pull and clean data

                pins::pin_write(board = board,
                                x = clean_data,
                                name = "isabella.velasquez/shiny-calendar-pin")
                ```
    -   [obstore](https://github.com/developmentseed/obstore): The simplest, highest-throughput Python interface to S3, GCS & Azure Storage, powered by Rust.
        -   Features
            -   Easy to install with no Python dependencies.
            -   Sync and async API.
            -   Streaming downloads with configurable chunking.
            -   Automatically supports multipart uploads under the hood for large file objects.
            -   The underlying Rust library is production quality and used in large scale production systems, such as the Rust package registry crates.io.
            -   Simple API with static type checking.
            -   Helpers for constructing from environment variables and `boto3.Session` objects
        -   Supported object storage providers include:
            -   Amazon S3 and S3-compliant APIs like Cloudflare R2
            -   Google Cloud Storage
            -   Azure Blob Gen1 and Gen2 accounts (including ADLS Gen2)
            -   Local filesystem
            -   In-memory storage
-   Lower storage costs due to their more open-source nature and undefined structure
-   On-Prem set-ups have to manage hardward and environments
    -   If you wanted to separate stuff like test data from production data, you also probably had to set up new hardware.
    -   If you had data in one physical environment that had to be used for analytical purposes in another physical environment, you probably had to copy that data over to the new replica environment.
        -   Have to keep a tie to the source environment to ensure that the stuff in the replica environment is still up-to-date, and your operational source data most likely isn’t in one single environment. It’s likely that you have tens — if not hundreds — of those operational sources where you gather data.
    -   Where on-prem set-ups focus on isolating data with physical infrastructure, cloud computing shifts to focus on isolating data using security policies.
-   Object Storage Systems
    -   Cloud data lakes provide organizations with additional opportunities to simplify data management by being accessible everywhere to all applications as needed
    -   Organized as collections of files within directory structures, often with multiple files in one directory representing a single table.
        -   Pros: highly accessible and flexible
        -   Metadata Catalogs are used to answer these questions:
            -   What is the schema of a dataset, including columns and data types
            -   Which files comprise the dataset and how are they organized (e.g., partitions)
            -   How different applications coordinate changes to the dataset, including both changes to the definition of the dataset and changes to data
        -   Hive Metastore (HMS) and AWS Glue Data Catalog are two popular catalog options
            -   Contain the schema, table structure and data location for datasets within data lake storage
    -   Issues:
        -   Does not coordinate data changes or schema evolution between applications in a transactionally consistent manner.
            -   Creates the necessity for data staging areas and this extra layer makes project pipelines brittle

## Brands {#sec-db-lakes-brands .unnumbered}

-   Hadoop
    -   Traditional format for data lakes
-   Amazon S3
    -   **Add a hash to your bucket names and explicitly specify your bucket region!!**
        -   Some dude casually named his bucket and was charged \>\$1K in a day, because some software was unintentionally hitting his bucket. Even if they don't have access, just *trying* to access it creates charges. ([link](https://medium.com/@maciej.pocwierz/how-an-empty-s3-bucket-can-make-your-aws-bill-explode-934a383cb8b1))
        -   Evidently AWS is coming up with a solution ([link](https://arstechnica.com/information-technology/2024/04/aws-s3-storage-bucket-with-unlucky-name-nearly-cost-developer-1300/))
    -   Try to stay \<1000 entries per level of hierarchy when designing the partitioning format. Otherwise there is paging and things get expensive.
    -   AWS Athena (\$5/TB scanned)
        -   [AWS Athena](https://aws.amazon.com/athena/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc) is serverless and intended for ad-hoc SQL queries against data on AWS S3
-   Microsoft Azure Data Lake Storage (ADLS)
-   [**Minio**](https://min.io/)
    -   Open-Source alternative to AWS S3 storage.
    -   Given that S3 often stores customer PII (either inadvertently via screenshots or actual structured JSON files), Minio is a great alternative to companies mindful of who has access to user data.
        -   Of course, [AWS claims that AWS personnel](https://aws.amazon.com/compliance/privacy-features/#:~:text=We%20prohibit%2C%20and%20our%20systems,or%20to%20comply%20with%20law.) doesn’t have direct access to customer data, but by being closed-source, that statement is just a function of trust.
-   [Databricks Delta Lake](https://www.databricks.com/product/delta-lake-on-databricks)
-   [Google Cloud Storage](https://cloud.google.com/storage/?hl=en)
    -   5 GB of US regional storage free per month, not charged against your credits.
-   [Apache Hudi](https://hudi.apache.org/) - A transactional data lake platform that brings database and data warehouse capabilities to the data lake. Hudi reimagines slow old-school batch data processing with a powerful new incremental processing framework for low latency minute-level analytics.

## Apache Iceberg {#sec-db-lakes-iceb .unnumbered}

-   Open source table format that addresses the performance and usability challenges of using Apache Hive tables in large and demanding data lake environments.
    -   Other currently popular open table formats are Hudi and Delta Lake.
-   Interfaces
    -   DuckDB can query Iceberg tables in S3 with an [extension](https://duckdb.org/docs/extensions/iceberg.html), [docs](https://duckdb.org/docs/guides/import/s3_iceberg_import.html)
    -   Athena can [create](https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg.html) Iceberg Tables
    -   Google Cloud Storage has something called [BigLake](https://cloud.google.com/biglake?hl=en) that can [create](https://cloud.google.com/blog/products/data-analytics/announcing-apache-iceberg-support-for-biglake) Iceberg tables
-   Features
    -   Transactional consistency between multiple applications where files can be added, removed or modified atomically, with full read isolation and multiple concurrent writes
    -   Full schema evolution to track changes to a table over time
    -   Time travel to query historical data and verify changes between updates
    -   Partition layout and evolution enabling updates to partition schemes as queries and data volumes change without relying on hidden partitions or physical directories
    -   Rollback to prior versions to quickly correct issues and return tables to a known good state
    -   Advanced planning and filtering capabilities for high performance on large data volumes
    -   The full history is maintained within the Iceberg table format and without storage system dependencies
-   Components
    -   Iceberg Catalog - Used to map table names to locations and must be able to support atomic operations to update referenced pointers if needed.
    -   Metadata Layer (with metadata files, manifest lists, and manifest files) - Stores instead all the enriching information about the constituent files for every different snapshot/transaction
        -   e.g. table schema, configurations for the partitioning, etc.
    -   Data Layer - Associated with the raw data files
-   Supports common industry-standard file formats, including Parquet, ORC and Avro
-   Supported by major data lake engines including Dremio, Spark, Hive and Presto
-   Queries on tables that do *not* use or save file-level metadata (e.g., Hive) typically involve costly list and scan operations
-   Any application that can deal with parquet files can use Iceberg tables and its API in order to query more efficiently
-   Comparison\
    ![](./_resources/DB,_Engineering.resources/Screenshot%20(487).png){.lightbox width="532"}

## Lakehouse {#sec-db-lakes-lkhs .unnumbered}

![](_resources/DB-Lakes.resources/lakehouse-comp-1.webp){.lightbox}

-   The key idea behind a Lakehouse is to be able to take the best of a Data Lake and a Data Warehouse.
    -   Data Lakes can in fact provide a lot of flexibility (e.g. handle structured and unstructured data) and low storage cost.
    -   Data Warehouses can provide really good query performance and ACID guarantees.
-   "Cloudflare R2 with Iceberg or Delta Lake and polars (automated with GitHub actions) is a free data lakehouse."
