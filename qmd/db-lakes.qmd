# Lakes {#sec-db-lakes .unnumbered}

## Misc {#sec-db-lakes-misc .unnumbered}

-   Data is stored in structured format or in its raw native format without any transformation at any scale.
    -   Handling both types allows all data to be centralized which means it can be better organized and more easily accessed.
-   Optimal for fit for bulk data types such as server logs, clickstreams, social media, or sensor data.
-   Ideal use cases
    -   Backup for logs
    -   Raw sensor data for your IoT application,
    -   Text files from user interviews
    -   Images
    -   Trained machine learning models (with the database simply storing the path to the object)
-   Lower storage costs due to their more open-source nature and undefined structure
-   On-Prem set-ups have to manage hardward and environments
    -   If you wanted to separate stuff like test data from production data, you also probably had to set up new hardware.
    -   If you had data in one physical environment that had to be used for analytical purposes in another physical environment, you probably had to copy that data over to the new replica environment.
        -   Have to keep a tie to the source environment to ensure that the stuff in the replica environment is still up-to-date, and your operational source data most likely isn’t in one single environment. It’s likely that you have tens — if not hundreds — of those operational sources where you gather data.
    -   Where on-prem set-ups focus on isolating data with physical infrastructure, cloud computing shifts to focus on isolating data using security policies.
-   Object Storage Systems
    -   Cloud data lakes provide organizations with additional opportunities to simplify data management by being accessible everywhere to all applications as needed
    -   Organized as collections of files within directory structures, often with multiple files in one directory representing a single table.
        -   Pros: highly accessible and flexible
        -   Metadata Catalogs are used to answer these questions:
            -   What is the schema of a dataset, including columns and data types
            -   Which files comprise the dataset and how are they organized (e.g., partitions)
            -   How different applications coordinate changes to the dataset, including both changes to the definition of the dataset and changes to data
        -   Hive Metastore (HMS) and AWS Glue Data Catalog are two popular catalog options
            -   Contain the schema, table structure and data location for datasets within data lake storage
    -   Issues:
        -   Does not coordinate data changes or schema evolution between applications in a transactionally consistent manner.
            -   Creates the necessity for data staging areas and this extra layer makes project pipelines brittle

## Brands {#sec-db-lakes-brands .unnumbered}

-   Hadoop
    -   Traditional format for data lakes
-   Amazon S3
    -   Try to stay \<1000 entries per level of hierarchy when designing the partitioning format. Otherwise there is paging and things get expensive.
    -   AWS Athena (\$5/TB scanned)
        -   [AWS Athena](https://aws.amazon.com/athena/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc) is serverless and intended for ad-hoc SQL queries against data on AWS S3
-   Microsoft Azure Data Lake Storage (ADLS)
-   [**Minio**](https://min.io/)
    -   Open-Source alternative to AWS S3 storage.
    -   Given that S3 often stores customer PII (either inadvertently via screenshots or actual structured JSON files), Minio is a great alternative to companies mindful of who has access to user data.
        -   Of course, [AWS claims that AWS personnel](https://aws.amazon.com/compliance/privacy-features/#:~:text=We%20prohibit%2C%20and%20our%20systems,or%20to%20comply%20with%20law.) doesn’t have direct access to customer data, but by being closed-source, that statement is just a function of trust.
-   Databricks Delta Lake
-   [Google Cloud Storage](https://cloud.google.com/storage/?hl=en)
    -   5 GB of US regional storage free per month, not charged against your credits.

## Apache Iceberg {#sec-db-lakes-iceb .unnumbered}

-   Open source table format that addresses the performance and usability challenges of using Apache Hive tables in large and demanding data lake environments.
-   Interfaces
    -   DuckDB can query Iceberg tables in S3 with an [extension](https://duckdb.org/docs/extensions/iceberg.html), [docs](https://duckdb.org/docs/guides/import/s3_iceberg_import.html)
    -   Athena can [create](https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg.html) Iceberg Tables
    -   Google Cloud Storage has something called [BigLake](https://cloud.google.com/biglake?hl=en) that can [create](https://cloud.google.com/blog/products/data-analytics/announcing-apache-iceberg-support-for-biglake) Iceberg tables
-   Features
    -   Transactional consistency between multiple applications where files can be added, removed or modified atomically, with full read isolation and multiple concurrent writes
    -   Full schema evolution to track changes to a table over time
    -   Time travel to query historical data and verify changes between updates
    -   Partition layout and evolution enabling updates to partition schemes as queries and data volumes change without relying on hidden partitions or physical directories
    -   Rollback to prior versions to quickly correct issues and return tables to a known good state
    -   Advanced planning and filtering capabilities for high performance on large data volumes
    -   The full history is maintained within the Iceberg table format and without storage system dependencies
-   Supports common industry-standard file formats, including Parquet, ORC and Avro
-   Supported by major data lake engines including Dremio, Spark, Hive and Presto
-   Queries on tables that do *not* use or save file-level metadata (e.g., Hive) typically involve costly list and scan operations
-   Any application that can deal with parquet files can use Iceberg tables and its API in order to query more efficiently
-   Comparison\
    ![](./_resources/DB,_Engineering.resources/Screenshot%20(487).png){.lightbox width="532"}
