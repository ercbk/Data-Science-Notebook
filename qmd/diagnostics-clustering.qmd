# Diagnostics, Clustering

TOC

* Misc
* Spherical/Centroid based
	* Within-Cluster SS (WSS)
	* Silhouette Coefficient
	* Calinski-Harabasz Index (aka Variance Ratio Criterion)
	* Davies-Bouldin Index
* Feature Reduction
	* Bayesian Information Criteria (BIC)




Misc

* Also see Notebook, pg 57
* For K-Means, elbow method (i.e. WSS) is awful. Recommended: Calinski-Harabasz Index and BIC then Silhouette Coefficient or Davies-Bouldin Index
* {{sklearn.metrics.cluster}}![](./_resources/Diagnostics,_Clustering.resources/image.6.png)




Spherical/Centroid based

* **Within-Cluster sum of squares (WSS) (aka Inertia)**
	* typically used in the elbow method for kmeans for choosing the number of clusters
* **Silhouette Coefficient**
	* computationally expensive
	* Formula![](./_resources/Diagnostics,_Clustering.resources/image.2.png)
		* Intra-cluster distance![](./_resources/Diagnostics,_Clustering.resources/image.png)
			* the mean distance between i and all the other data points within C.
			* |CI| is the number of points belonging to cluster i, and d(i , j) is the distance between data points i and j in the cluster CI
		* Inter-cluster distance![](./_resources/Diagnostics,_Clustering.resources/image.1.png)
			* the mean distance between i to all the points of its nearest neighbor cluster
	* range: -1 to 1
	* Good: 1
	* Bad: -1
		* Says inter-cluster distances are not comparable to the intra-cluster distances
* **Calinski-Harabasz Index** (aka **Variance Ratio Criterion**)
	* NOT to be used to the density based methods, such as mean-shift clustering, DBSCAN, OPTICS, etc.
		* Clusters in density based methods are unlikely to be spherical and therefore centroids-based distances will not be that informative to tell the quality of the clustering algorithm
		* much faster than Silhouette score calculation
	* Ratio of the squared inter-cluster distance sum and the squared intra-cluster distance sum for all clusters
	* Formula![](./_resources/Diagnostics,_Clustering.resources/image.3.png)
		* nk is the size of the kth cluster
		* ck is the feature vector of the centroid of the kth cluster
		* c is the feature vector of the global centroid of the entire dataset
		* di is the feature vector of data point i
		* N is the total number of data points
	* Range: no upper bound
	* Higher is better; means the clusters are separated from each other
* **Davies-Bouldin Index**
	* NOT to be used to the density based methods, such as mean-shift clustering, DBSCAN, OPTICS, etc.
		* Clusters in density based methods are unlikely to be spherical and therefore centroids-based distances will not be that informative to tell the quality of the clustering algorithm
		* much faster than Silhouette score calculation
	* Formula![](./_resources/Diagnostics,_Clustering.resources/image.4.png)
		
		* An averaged similarity score across all clusters with its nearest neighbor cluster
		
		* Di is the ith cluster’s worst (largest) similarity score across all other clusters![](./_resources/Diagnostics,_Clustering.resources/image.5.png)
			* Smaller similarity score indicates a better cluster separation
	* similarity score, that measures how similar two clusters are to each other
	* Lower is better; means the clusters are separated from each other



Feature Reduction

* Bayesian Information Criteria (BIC)
	* Higher is better
	* typically used for GMM
	* Not on the same scale as WSS
	* Py Code

```
def bic_score(X, labels):
  """
  BIC score for the goodness of fit of clusters.
  This Python function is directly translated from the GoLang code made by the author of the paper. 
  The original code is available here: https://github.com/bobhancock/goxmeans/blob/a78e909e374c6f97ddd04a239658c7c5b7365e5c/km.go#L778
  """

  n_points = len(labels)
  n_clusters = len(set(labels))
  n_dimensions = X.shape[1]
  n_parameters = (n_clusters - 1) + (n_dimensions * n_clusters) + 1
  loglikelihood = 0
  for label_name in set(labels):
    X_cluster = X[labels == label_name]
    n_points_cluster = len(X_cluster)
    centroid = np.mean(X_cluster, axis=0)
    variance = np.sum((X_cluster - centroid) ** 2) / (len(X_cluster) - 1)
    loglikelihood += \
      n_points_cluster * np.log(n_points_cluster) \
      - n_points_cluster * np.log(n_points) \
      - n_points_cluster * n_dimensions / 2 * np.log(2 * math.pi * variance) \
      - (n_points_cluster - 1) / 2

  bic = loglikelihood - (n_parameters / 2) * np.log(n_points)

  return bic
```


