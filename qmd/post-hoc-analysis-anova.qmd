# Post-Hoc Analysis, ANOVA

TOC

* Misc
* Mathematics
* One-way
* ANCOVA



Misc

* Analysis of covariance is classical terminology from linear models but we often use the term also for nonlinear models
* Assumptions
	* Each group category has a normal distribution.
	* Each group category is independent of each other and identically distributed (iid)
	* Group categories have of _similar_ variance (i.e. homoskedastic variance)
		* If this is violated
			* and the ratio of the largest variance to the smallest variance is less than 4, then proceed with one-way ANOVA (robust to small differences)
			* and the ratio of the largest variance to the smallest variance is greater than 4, we may instead choose to perform a Kruskal-Wallis test. This is considered the non-parametric equivalent to the one-way ANOVA. ([example](https://www.statology.org/kruskal-wallis-test-r/))
		* EDA

```
data %>%
  group_by(program) %>%
  summarize(var=var(weight_loss))
# A tibble: 3 x 2
  program  var   
1 A      0.819
2 B      1.53 
3 C      2.46
```

* Perform a statisical test to see if these variables are statistically significant (See [Post-Hoc Analysis, Difference-in-Means](Post-Hoc Analysis, general) >> EDA >> Tests for equal variances)

* Eta Squared
	* metric to describe the effect size of a variable
	* Range: \[0, 1\]; values closer to 1 indicating that a specific variable in the model can explain a greater fraction of the variation
	* `lsr::etaSquared(anova_model)` (use first column of output)
	* Guidelines
			0.01: Effect size is small.
			
			0.06: Effect size is medium.
			
			Large effect size if the number is 0.14 or above
			



Mathematics

* asides:
	* these look like the  variance formula except for not dividing by the sample size to get the "average" squared distance
	* SSA formula - the second summation just translates to multiplying by ni, the group category sample size, since there is no j in that formula

SST\=SSA+SSE∑_i_\=1_a_∑_j_\=_i__n__i_(_x__i__j_−_μ_)2\=∑_i_\=1_a_∑_j_\=1_n__i_(_x__i_¯−_μ_)2+∑_i_\=1_a_∑_j_\=1_n__i_(_x__i__j_−_x__i_¯)2

* Calculate SSA and SSE
	* SST = sum of squares total
	* SSA = sum of squares between categories, treatments, or factors
		* "A" stands for attributes (i.e. categories)
	* SSE = sum of squares of errors; randomness within categories, treatments, or factors
	* Xij is the jth observation of the ith category
	* Xi(bar) is the sample mean of category i
	* μ is the overall sample mean
	* ni is the group category sample size
	* a is the number of group categories
* Calculate MSA and MSE

MSE\=_S__S__E__N_−_a_ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​MSA\=_S__S__A__a_−1

* Where N is the total sample size

* Calculate the F statistic and find the p-value (need a table to look it up)
	F = MSA/MSE
	* If our F statistic is less than the critical value F statistic for a α=0.05 than we cannot reject the null hypothesis (no statistical difference between categories)

Discussion

* If there is a group category that has more variance than the others attribute error (SSA) we should then pick that up when we compare it to the random error (SSE)
	* if a group is further away from the overall mean then it will increase SSA and thus influence the overall variance but might not always increase random error



One-Way

* Measures if there's a difference in means between _any_ group category
* Example (1 control, 2 Test groups)

```
data <- data.frame(Group = rep(c("control", "Test1", "Test2"), each = 10),
value = c(rnorm(10), rnorm(10),rnorm(10)))
data$Group<-as.factor(data$Group)
head(data)
  Group      value
1 control  0.1932123
2 control -0.4346821
3 control  0.9132671
4 control  1.7933881
5 control  0.9966051
6 control  1.1074905
```

* Fit model

```
model <- aov(value ~ Group, data = data)
summary(model)
            Df    Sum Sq  Mean Sq  F value  Pr(>F) 
Group        2    4.407    2.2036    3.71  0.0377 *
Residuals  27    16.035    0.5939

# or
lm_mod <- lm(value ~ Group, data = data)
anova(lm_mod)
```

* pval < 0.05 says at least 1 group category has a statistically significant different mean from another category

* Post-hoc Dunnet's Test (also see Post-Hoc Analysis [note](Post-Hoc Analysis, general))
	* measures if there is any difference between treatments and the control

```
DescTools::DunnettTest(x=data$value, g=data$Group)

Dunnett's test for comparing several treatments with a control : 
    95% family-wise confidence level
$control
                    diff    lwr.ci      upr.ci  pval   
Test1-control -0.8742469 -1.678514 -0.06998022 0.0320 * 
Test2-control -0.7335283 -1.537795  0.07073836 0.0768 .
```

* The mean score of the test1 group was significantly higher than the control group. The mean score of the test2 group was not significantly higher than the control group.

* Tukey's HSD (also see Post-Hoc Analysis [note](Post-Hoc Analysis, general))
	* measure difference in means between all categories and each other

```
stats::TukeyHSD(model, conf.level=.95)
```


ANCOVA

* Analysis of covariance is used to measure the main effect and interaction effects of categorical variables on a continuous dependent variable while controlling the effects of selected other continuous variables which co-vary with the dependent variable.
* Misc
	* Analysis of covariance is classical terminology from linear models but we often use the term also for nonlinear models (Harrell)
	* See also
		* [Harrell - Biostatistics for Biomedical Research Ch. 13](http://hbiostat.org/bbr/ancova.html)
* Assumptions
	* Independent observations (i.e. random assignment, avoid is having known relationships among participants in the study)
	* Linearity: the relation between the covariate(s) and the dependent variable must be linear.
	* Normality: the dependent variable must be normally distributed within each subpopulation. (only needed for small samples of n < 20 or so)
	* Homogeneity of regression slopes: the b-coefficient(s) for the covariate(s) must be equal among all subpopulations. (regression lines for these individual groups are assumed to be parallel)
		* Failure to meet this assumption implies that there is an interaction between the covariate and the treatment.
		* This assumption can be checked with an F test on the interaction of the independent variable(s) with the covariate(s).
			* If the F test is significant (i.e., significant interaction) then this assumption has been violated and the covariate should not be used as is.
			* A possible solution is converting the continuous scale of the covariate to a categorical (discrete) variable and making it a subsequent independent variable, and then use a factorial ANOVA to analyze the data.
	* The covariate (adjustment variable) and the treatment are independent

```
model <- aov(grade ~ technique, data = data)
summary(model)

            Df Sum Sq Mean Sq F value Pr(>F)
technique    2    9.8    4.92    0.14  0.869
Residuals  87 3047.7  35.03
```

* H0: variables are independent

* Homogeneity of variance: variance of the dependent variable must be equal over all subpopulations (only needed for sharply unequal sample sizes)

```
# response ~ treatment
leveneTest(exam ~ technique, data = data)

      Df F value    Pr(>F)   
group  2  13.752 6.464e-06 ***
      87
# alt test
fligner.test(size ~ location, my.dataframe)
```

* H0: Homogeneous variance
* This one fails

* Fit

```
    ancova_model <- aov(exam ~ technique + grade, data = data)
    car::Anova(ancova_model, type="III")

                Sum Sq Df F value    Pr(>F)   
    (Intercept) 3492.4  1 57.1325 4.096e-11 ***
    technique  1085.8  2  8.8814 0.0003116 ***
    grade          4.0  1  0.0657 0.7982685   
    Residuals  5257.0 86
```

* When adjusting for current grade (covariate), study technique (treatment) has a significant effect on the final exam score (response).

* Does the effect differ by treatment (also see [Mixed Effects, Post-Hoc](Post-Hoc Analysis, Mixed Effects))

```
    postHocs <- multicomp::glht(ancova_model, linfct = mcp(technique = "Tukey"))
    summary(postHocs)

                Estimate Std. Error t value Pr(>|t|)   
    B - A == 0  -5.279      2.021  -2.613  0.0284 * 
    C - A == 0    3.138      2.022  1.552  0.2719   
    C - B == 0    8.418      2.019  4.170  <0.001 ***
```

* A, B, and C are the study techniques (treatment)
* Significant differences between B and A and a pretty large difference between B and C.

* [Example]{.ribbon-highlight}: RCT![](./_resources/Post-Hoc_Analysis,_ANOVA.resources/Screenshot (904).png)

```
w2 <- glm(
  data = dw,
  family = gaussian,
  post ~ 1 + tx + pre)
```

* Specification
	* post, pre are the post-treatment and pre-treatment measurement of the outcome variable
	* tx is the treatment indicator variable
	* β0: population mean for the outcome variable in the control group
	* β1: parameter is the population level difference in pre/post change in the treatment group, compared to the control group.
		* Also a causal estimate for the average treatment effect (ATE) in the population, τ
	* Because "pre" is added as a covariate, both β0 and β1 are conditional on the outcome variable, as collected at baseline before random assignment.





















