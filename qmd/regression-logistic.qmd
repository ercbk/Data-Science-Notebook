# Logistic {#sec-reg-log .unnumbered}

## Misc {#sec-reg-log-misc .unnumbered}

-   Also see
    -   [Classification](classification.qmd#sec-classif){style="color: green"}
    -   [Regression, Regularized \>\> Misc](regression-regularized.qmd#sec-reg-reg-misc){style="color: green"}, [Firth's Estimator](regression-regularized.qmd#sec-reg-reg-firth){style="color: green"}
-   Packages
    -   [{]{style="color: #990000"}[glmnet](https://glmnet.stanford.edu/){style="color: #990000"}[}]{style="color: #990000"}
    -   [{]{style="color: #990000"}[brglm2](https://cran.r-project.org/web/packages/brglm2/index.html){style="color: #990000"}[}]{style="color: #990000"}
        -   Estimation and inference from generalized linear models using various methods for bias reduction
        -   Can be used in models with Separation (See [Diagnostics, GLM \>\> Separation](diagnostics-glm.qmd#sec-diag-glm-sep){style="color: green"})
        -   Able to fit multinomial and logistic regression models
        -   Reduction of estimation bias is achieved by solving either:
            -   The mean-bias reducing adjusted score equations in [Firth (1993)](https://doi.org/10.1093/biomet/80.1.27) and [Kosmidis & Firth (2009)](https://doi.org/10.1093/biomet/asp055)
            -   The median-bias reducing adjusted score equations in [Kenne et al (2017)](https://doi.org/10.1093/biomet/asx046)
            -   The direct subtraction of an estimate of the bias of the maximum likelihood estimator from the maximum likelihood estimates as prescribed in [Cordeiro and McCullagh (1991)](https://www.jstor.org/stable/2345592).
    -   [{]{style="color: #990000"}[LogisticCopula](https://cran.r-project.org/web/packages/LogisticCopula/){style="color: #990000"}[}]{style="color: #990000"} ([Paper](https://arxiv.org/abs/2410.08803)) - A Copula Based Extension of Logistic Regression that aims to account for non-linear main effects and complex interactions, while keeping the model inherently explainable.
        -   Uses a generative specification of the model, consisting of a combination of certain margins on natural exponential form, combined with vine copulas.
            -   The model is constructed by starting from a Naive Bayes model resulting in linear log odds, and then adding vine copula terms which introduce additional non-linear effects and interaction terms in the form of between - covariate dependence, conditional on the response.
            -   The resulting model for the log odds ratio becomes a sum of the logistic regression model with linear main effects and some non-linear terms that involve two covariates or more.
        -   Performs best when non-linearities and complex interactions are present, even when n is not large compared to p.
-   `glm` needs the outcome to be numeric 0/1 (not factor)
-   Regularized Logistic Regression is most necessary when the number of candidate predictors is large in relationship to the effective sample size 3np(1−p) where p is the proportion of Y=1 [Harrell](https://discourse.datamethods.org/t/penalized-likelihood-vs-variance-decomposition-approaches/6790)
-   Sample size requirements
    -   Logistic Regression: (Harrell, [link](https://hbiostat.org/rmsc/lrm.html#sec-lrm-n))
        -   [These are conservative estimates]{.underline}. Sample size estimates assume an event probability of 0.50.
        -   For just estimating the intercept and a margin of error for predicted probabilities of 0.1
            -   With no covariates (i.e. population is homogeneous), n = 96
            -   With 1 categorical covariate, n = 96 for each level of the covariate
                -   e.g. For gender, you need 96 males and 96 females
        -   For just estimating the intercept and a margin of error for predicted probabilities of 0.05
            -   With no covariates (i.e. population is homogeneous), n = 384
            -   If true probabilities of event (and non-event) are known to be extreme, i.e. $p \notin [0.2, 0.8]$, n = 246
        -   For estimating predicted probabilities with 1 continuous predictor
            -   For a margin of error of 0.1, n = 150
            -   For a margin of error of 0.07, n = 300
    -   [Papers](https://discourse.datamethods.org/t/reference-collection-to-push-back-against-common-statistical-myths/1787#sample-size-number-of-variables-for-regression-models-6) that study sample size for logistic regression
-   "Stable" AUC requirements for 0/1 outcome:
    -   paper: [Modern modelling techniques are data hungry: a simulation study for predicting dichotomous endpoints \| BMC Medical Research Methodology \| Full Text](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-137)
    -   Logistic Regression: 20 to 50 events per predictor variable
    -   Random Forest and SVM: greater than 200 to 500 events per predictor variable
-   [**Non-Collapsibility**]{style="color: #009499"}: The conditional odds ratio (OR) or hazard ratio (HR) is different from the marginal (unadjusted) ratio even in the complete absence of confounding.
-   Don't use percents to report probabilities ([Harrell](https://www.fharrell.com/post/percent/))
    -   Examples (Good)
        -   The chance of stroke went from 0.02 to 0.03
        -   The chance of stroke increased by 0.01 (or the absolute chance of stroke increased by 0.01)
        -   The chance of stroke increased by a factor of 1.5
        -   If 0.02 corresponded to treatment A and 0.03 corresponded to treatment B: treatment A multiplied the risk of stroke by 2/3 in comparison to treatment B.
        -   Treatment A modified the risk of stroke by a factor of 2/3
        -   The treatment A: treatment B risk ratio is 2/3 or 0.667.
-   RCTs (notes from [Harrell](https://www.fharrell.com/post/marg/))
    -   outcome \~ treatment
        -   These simple models are for homogeneous patient populations, i.e.,  patients with no known risk factors
        -   When heterogeneity (patients have strong risk factors) is present, patients come from a mixture of distributions and this causes the treatment effect to shrink towards 0 in logistic and cox-ph models. (see Ch. 13 for in Harrell's biostats [book](https://hbiostat.org/doc/bbr.pdf) details
            -   In a linear model, this heterogeneity (i.e. risk factors) that's unaccounted for gets absorbed into the error term (residual variance ↑, power ↓), but logistic/cox-ph models don't have residuals so the treatment effect shrinks as a result.
    -   outcome \~ treatment + risk_factor_vars
        -   Adjusting for risk factors *stops* a loss of power but never increases power like it does for linear models.
    -   In Cox and logistic models there are no residual terms, and unaccounted outcome heterogeneity has nowhere to go. So it goes into the regression coefficients that are in the model, attenuating them towards zero. Failure to adjust for easily accountable outcome heterogeneity in nonlinear models causes a loss of power as a result.
    -   Modeling is a question of approximating the effects of baseline variables that explain outcome heterogeneity. The better the model the more complete the conditioning and the more accurate the patient-specific effects that are estimated from the model. Omitted covariates or under-fitting strong nonlinear relationships results in effectively conditioning on only part of what one would like to know. This partial conditioning still results in useful estimates, and the estimated treatment effect will be somewhere between a fully correctly adjusted effect and a non-covariate-adjusted effect.

## Interpretation {#sec-reg-log-interp .unnumbered}

-   Misc

    -   Also see [Visualization](regression-logistic.fqmd#sec-reg-log-viz){style="color: green"} \>\> Log Odds Ratios vs Odds Ratios
    -   A logit link, which means that the effects (parameter magnitudes or CIs) can't easily be translated to the probability scale without picking a baseline value (unlike identity-link models or log-link models such as Poisson regression). Consider also reporting the marginal effects as they are more interpretable. See [Marginal Effects](regression-logistic.qmd#sec-reg-log-me){style="color: green"}
    -   "Divide by 4" shortcut (Gelman)
        -   Valid for coefficients and standard errors
        -   "1 unit increase in X results in a β/4 increase in probability that Y = 1"
        -   Always round down
        -   Better appproximations when the probability is close to 0.50

-   Definitions\
    ![](./_resources/Regression,_Logistic.resources/Screenshot%20(834).png){.lightbox width="360"}

    -   [Example]{.ribbon-highlight}: Event probability is 0.20 ([source](https://www.montana.edu/rotella/documents/502/Prob_odds_log-odds.pdf))

        ``` r
        # odds
        o <- 0.2/(1 - 0.2)

        # log odds
        lo <- log(o)
        lo <- qlogis(0.2)

        # reconstruct probability
        p <- plogis(lo)
        p <- o / (1 + o)
        ```

-   Effect is non-linear in *p*\
    ![](./_resources/Regression,_Logistic.resources/Screenshot%20(1015).png){.lightbox width="480"}

-   Probability prediction:\
    $$
    p(x) = \text{logit}^{-1}(a + bx) = \frac{1}{1 + \exp(-a - bx)}
    $$

-   **Odds Ratio** - Describes the percent *change in odds* of the outcome based on a one unit increase in the input variable.

    -   A ratio of odds. The ratio of something that's happening to something that's not happening

    -   Associated with terms like better chances, greater odds

    -   OR significance is being different from 1 and log odds ratios (logits) significance is being different from 0

    -   logit(p) = 0, p = 0.5, OR = 1

    -   logit(p) = 6, p = always (close to 1)

    -   logit(p) = -6, p = never (close to 0)

    -   Guidelines

        -   OR \> 1 means increased occurrence of an event (more likely)
        -   OR \< 1 means decreased occurrence of an event (less likely)
        -   OR = 1 ↔ log OR = 0 ⇒ No difference in odds
        -   OR \< 0.5 or OR \> 2 ⇒ Moderate effect
        -   OR \> 4 is pretty strong and unlikely to be explained by unmeasured variables

    -   Code

        ``` r
        prostate_model %>% 
              broom::tidy(exp = TRUE)
        ##  term        estimate std.error statistic  p.value
        ##  <chr>          <dbl>    <dbl>    <dbl>    <dbl>
        ## 1 (Intercept)  0.0318    0.435    -7.93 2.15e-15
        ## 2 fam_hx        0.407      0.478    -1.88 6.05e- 2
        ## 3 b_gs          3.27      0.219      5.40 6.70e- 8
        ```

        -   Exponentiates coefficients to get ORs
        -   Interpretation: the odds ratio estimate of 0.407 means that for someone with a positive family history (fam_hx) of prostate cancer, the odds of their having a recurrence are 59.3% ((1-0.407) x 100) lower than someone without a family history. Similarly, for each unit increase in the baseline Gleason score (b_gs), the odds of recurrence increase by 227% ((3.27-1) x 100).

-   **Risk Ratio**

    -   A ratio of probabilities. A ratio of something that's happening to everything that could happen

    -   Assoicated with terms like: increased risk, higher likelihood

    -   P-Values between odds ratios and risk ratios will not be the same

        -   So while you can convert ORs to RRs, the p-value of the OR doesn't transfer

    -   Log-Binomials which calculate risk ratios as effects (and p-values) often fail to converge for rare events (which is why most medical research uses ORs and Logistic Regression)

    -   ORs and RRs can be close in value for *rare* events.

        -   Rule of Thumb: When the prevalence of an event is less than 10%, the odds ratio closely approximates the risk ratio.

    -   See Odds Ratio \>\> Guidelines for interpretation

        -   Even though RRs have the same guidelines as ORs, ORs typically have a larger magnitude for a given RR. (e.g. RR = 2 $\rightarrow$ OR = 3.5, RR = 0.5 $\rightarrow$ OR = 0.2)

    -   Formula\
        $$
        \begin{align}
        \text{RR} &= \frac{\text{Risk of Category A}}{\text{Risk of Category A}} \\
                  &= \frac{\frac{\text{# of category A that experienced event}}{\text{total # of category A}}}{\frac{\text{# of category B that experienced event}}{\text{total # of category B}}}
        \end{align}
        $$

    -   Relationship between odds and probabilities\
        $$
        \begin{align}
        P &= \frac{\text{odds}}{1 + \text{odds}} \\
        Odds &= \frac{P}{1-P}
        \end{align}
        $$

    -   Relationship between Risk Ratios and Odds Ratios\
        $$
        \begin{align}
        \text{RR} &= \frac{\text{OR}}{1 - I_0 \cdot (1 - \text{OR})} \\
        \text{OR} &= \frac{\text{RR} \cdot (1-I_0)}{1-I_0 \cdot \text{RR}}
        \end{align}
        $$

        -   $I_0$ is the baseline risk in the control group (i.e. reference category; the denominator in the RR calculation)

            -   [Example]{.ribbon-highlight}:\
                $$
                \text{RR} = \frac{\text{Risk of Females}}{\text{Risk of Males}} = \frac{7/10}{2/10} = 3.5
                $$

                -   $I_0 = 2/10  = 0.20$

-   Summary

    ``` r
    ## glm(formula = recurrence ~ fam_hx + b_gs, family = binomial, 
    ##    data = .)
    ## 
    ## Deviance Residuals: 
    ##    Min      1Q  Median      3Q      Max 
    ## -1.2216  -0.5089  -0.4446  -0.2879  2.5315 
    ## 
    ## Coefficients:
    ##            Estimate Std. Error z value            Pr(>|z|)
    ## (Intercept)  -3.4485    0.4347  -7.932 0.00000000000000215
    ## fam_hx      -0.8983    0.4785  -1.877              0.0605
    ## b_gs          1.1839    0.2193  5.399 0.00000006698872947
    ##               
    ## (Intercept) ***
    ## fam_hx      . 
    ## b_gs        ***
    ## ---
    ## Signif. codes: 
    ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##    Null deviance: 281.88  on 313  degrees of freedom
    ## Residual deviance: 246.81  on 311  degrees of freedom
    ##  (2 observations deleted due to missingness)
    ## AIC: 252.81
    ## 
    ## Number of Fisher Scoring iterations: 5
    ```

    -   The **Deviance Residuals** should have a Median near zero, and be roughly symmetric around zero. If the median is close to zero, the model is not biased in one direction (the outcome is not over- nor under-estimated).
    -   The **Coefficients** estimate how much a change of one unit in each predictor will affect the outcome (in logit units).
        -   The family history predictor (fam_hx) is not significant, but trends toward an association with a decreased odds of recurrence, while the baseline Gleason score (b_gs) is significant and is associated with an 18% increased log-odds of recurrence for each extra point in the Gleason score.
    -   **Null Deviance and Residual Deviance**. The null deviance is measured for the null model, with only an intercept. The residual deviance is measured for your model with predictors. [Your residual deviance should be lower than the null deviance.]{.underline}
        -   You can even measure whether your model is significantly better than the null model by calculating the difference between the Null Deviance and the Residual Deviance. This difference \[281.9 - 246.8 = 35.1\] has a chi-square distribution. You can look up the value for chi-square with 2 degrees (because you had 2 predictors) of freedom.
        -   Or you can calculate this in R with `pchisq(q = 35.1, df=2, lower.tail = TRUE)` which gives you a p value of 1.
    -   The **degrees of freedom** are related to the number of observations, and how many predictors you have used. If you look at the mean value in the prostate dataset for recurrence, it is 0.1708861, which means that 17% of the participants experienced a recurrence of prostate cancer. If you are calculating the mean of 315 of the 316 observations, and you know the overall mean of all 315, you (mathematically) *know* the value of the last observation - recurrence or not - it has no degrees of freedom. So for 316 observations, you have n-1 or 315, degrees of freedom. For each predictor in your model you 'use up' one degree of freedom. The degrees of freedom affect the significance of the test statistic (T, or chi-squared, or F statistic).
    -   **Observations deleted due to missingness** - the logistic model will only work on *complete cases*, so if one of your predictors or the outcome is frequently missing, your effective dataset size will shrink rapidly. You want to know if this is an issue, as this might change which predictors you use (avoid frequently missing ones), or lead you to consider imputation of missing values.

-   [Predicted Risk]{.underline} - predicted probabilities from a logistic regression model that's used to predict the risk of an event given a set of variables.

    ``` r
    str(salespeople)
    ## 'data.frame':    350 obs. of  4 variables:
    ##  $ promoted    : int  0 0 1 0 1 1 0 0 0 0 ...
    ##  $ sales        : int  594 446 674 525 657 918 318 364 342 387 ...
    ##  $ customer_rate: num  3.94 4.06 3.83 3.62 4.4 4.54 3.09 4.89 3.74 3 ...
    ##  $ performance  : int  2 3 4 2 3 2 3 1 3 3 ...

    model <- glm(formula = promoted ~ ., data = salespeople, family = "binomial")
    exp(model$coefficients) %>% 
      round(2)
    ##  (Intercept)        sales customer_rate  performance2  performance3 
    ##          0.00          1.04          0.33          1.30          1.98 
    ##  performance4 
    ##          2.08
    ```

    -   Interpretation:
        1.  For two salespeople with the same customer rating and the same performance, each additional thousand dollars in sales increases the odds of promotion by 4%.
            -   Sales in thousands of dollars
        2.  For two salespeople with the same sales and performance, each additional point in customer rating *decreases* the odds of promotion by 67%
        3.  Increasing performance4 by 1 unit and holding the rest of the variables constant increases the odds of getting a promotion by 108%.
            -   I think this is being treated as a factor variable, and therefore the estimate is relative to the reference level (performance1).
            -   Should be: "Performance4 increases the odds of getting a promotion by 108% relative to having a performance1"
        4.  For two salespeople of the same sales and customer rating, performance rating has no significant effect on the odds of promotion.
            -   None of the levels of performance were statistically significant

## Assumptions {#sec-reg-log-ass .unnumbered}

-   `performance::model_check(mod)` - provides a facet panel of diagnostic charts with subtitles to help interprete each chart.
    -   Looks more geared towards a regression model than a logistic model.
-   Linear relationship between the *logit* of the binary outcome and each *continuous* explanatory variable
    -   `car::boxTidwell`
        -   May not work with factor response so might need to as.numeric(response) and use 1,2 values instead of 0,1
        -   p \< 0.05 indicates non-linear relationship which is what you want
    -   Can also look a scatter plot of logit(response) vs numeric predictor
-   No outliers
    -   Cook's Distance
        -   Different opinions regarding what cut-off values to use. One standard threshold is 4/N (where N = number of observations), meaning that observations with Cook's Distance \> 4/N are deemed as influential
    -   Standardized Residuals
        -   Absolute standardized residual values greater than 3 represent possible extreme outliers
-   Absence of Multicollinearity
    -   VIF
    -   Correlation Heatmap
-   iid
    -   Deviance residuals (y-axis) vs row index (x-axis) should show points randomly around zero (y-axis)
        -   Jesper's [response](https://stats.stackexchange.com/a/455665) shows calculation and R code
        -   I think the dHARMA pkg also handles this
-   Sufficiently large sample size
    -   Rules of thumb
        -   10 observations with the least frequent outcome for each independent variable
        -   \> 500 observations total
    -   I'm sure Harrell has thoughts on this somewhere

## Diagnostics {#sec-reg-log-diag .unnumbered}

-   Misc
    -   \*\* The formulas for the deviances for a logistic regression model are slightly different from other GLMs since the deviance for the saturated logistic regression model is 0 \*\*
    -   Also see
        -   [Diagnostics, Classification \>\> Misc](diagnostics-classification.qmd#sec-diag-class-misc){style="color: green"} \>\> Workflow
        -   [Diagnostics, GLM](diagnostics-glm.qmd#sec-diag-glm){style="color: green"}
-   Brier Score (See [Diagnostics, Classification \>\> Scores](diagnostics-classification.qmd#sec-diag-class-scores){style="color: green"})
-   Residual Deviance (G2)
    -   -2 \* LogLikelihood(proposed_mod)))
-   Null Deviance
    -   -2 \* LogLikelihood(null_mod)))
    -   i.e. deviance for the intercept-only model
-   McFadden's Pseudo R^2^ = (LL(null_mod) - LL(proposed_mod)) / LL(null_mod))
    -   See [What are Pseudo-R Squareds?](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/) for formulas to various alternative R2s for logistic regression

    -   The p-value for this R^2^ is the same as the p-value for:

        -   2 \* (LL(proposed_mod) - LL(null_mod))
        -   Null Deviance - Residual Deviance
            -   For the dof, use proposed_dof - null_dof
                -   dof for the null model is 1

    -   [Example]{.ribbon-highlight}: Getting the p-value

        ``` r
        m1 <- glm(outcome ~ treat)
        m2 <- glm(outcome ~ 1)
        (ll_diff <- logLik(m1) - logLik(m2))
        ## 'log Lik.' 3.724533 (df=3)
        1 - pchisq(2*ll_diff, 3)
        ```
-   Compare nested models
    -   Models

        ``` r
        model1 <- glm(TenYearCHD ~ ageCent + currentSmoker + totChol, 
                      data = heart_data, family = binomial)
        model2 <- glm(TenYearCHD ~ ageCent + currentSmoker + totChol + 
                        as.factor(education), 
                      data = heart_data, family = binomial)
        ```

        -   Add Education or not?

    -   Extract Deviances

        ``` r
        # Deviances
        (dev_model1 <- glance(model1)$deviance)
        ## [1] 2894.989
        (dev_model2 <- glance(model2)$deviance)
        ## [1] 2887.206
        ```

    -   Calculate difference and test significance

        ``` r
        # Drop-in-deviance test statistic
        (test_stat <- dev_model1 - dev_model2)
        ## [1] 7.783615

        # p-value
        1 - pchisq(test_stat, 3)  # 3 = number of new model terms in model2 (i.e. 3(?) levels of education)
        ## [1] 0.05070196

        # Using anova
        anova(model1, model2, test = "Chisq")
        ## Analysis of Deviance Table
        ## 
        ## Model 1: TenYearCHD ~ ageCent + currentSmoker + totChol
        ## Model 2: TenYearCHD ~ ageCent + currentSmoker + totChol + as.factor(education)
        ##  Resid. Df Resid. Dev Df Deviance Pr(>Chi) 
        ## 1      3654    2895.0                       
        ## 2      3651    2887.2  3  7.7836  0.0507 .
        ```
-   \|β\| \> 10
    -   Extreme
    -   Implies probabilities close to 0 or 1 which is suspect
    -   Consider removal of the variable or outlier(s) influencing the model
-   Intercepts ≈ -17
    -   Indicate a need for a simpler model (see bkmk - Troubleshooting glmmTMB)
-   If residuals are heteroskedastic, see [{]{style="color: #990000"}[glmx](https://cran.r-project.org/web/packages/glmx/index.html){style="color: #990000"}[}]{style="color: #990000"}
    -   neg.bin, hurdle, logistic - Extended techniques for generalized linear models (GLMs), especially for binary responses, including parametric links and heteroskedastic latent variables
-   Binned Residuals
    -   It is not useful to plot the raw residuals, so examine binned residual plots

    -   Misc

        -   {arm} will mask some {tidyverse} functions, so don't load whole package

    -   Look for :

        -   Patterns
        -   Nonlinear trend may be indication that squared term or log transformation of predictor variable required
        -   If bins have average residuals with large magnitude
        -   Look at averages of other predictor variables across bins
        -   Interaction may be required if large magnitude residuals correspond to certain combinations of predictor variables

    -   Process

        -   Extract raw residuals
            -   Include `type.residuals = "response"` in the `broom::augment` function to get the raw residuals
        -   Order observations either by the values of the predicted probabilities (or by numeric predictor variable)
        -   Use the ordered data to create g bins of approximately equal size.
            -   Default value: g = sqrt(n)
        -   Calculate average residual value in each bin
        -   Plot average residuals vs. average predicted probability (or average predictor value)

    -   [Example]{.ribbon-highlight}: vs Predicted Values

        ``` r
        arm::binnedplot(x = risk_m_aug$.fitted, y = risk_m_aug$.resid,
                        xlab = "Predicted Probabilities",
                        main = "Binned Residual vs. Predicted Values",
                        col.int = FALSE)
        ```

        -   [Example]{.ribbon-highlight}: vs Predictor

            ``` r
            arm::binnedplot(x = risk_m_aug$ageCent,
                            y = risk_m_aug$.resid,
                            col.int = FALSE,
                            xlab = "Age (Mean-Centered)",
                            main = "Binned Residual vs. Age")
            ```

    -   Check that residuals have mean zero: `mean(resid(mod))`

    -   Check that residuals for each level of categorical have mean zero

        ``` r
        risk_m_aug %>%
          group_by(currentSmoker) %>%
          summarize(mean_resid = mean(.resid))
        ```

## Marginal Effects {#sec-reg-log-me .unnumbered}

-   Misc
    -   Notes from
        -   R \>\> Documents \>\> Regression \>\> glm-marginal-effects.pdf
    -   Also see
        -   [Regression, Interactions \>\> Logistic Regression](regression-interactions.qmd#sec-reg-inter-logreg){style="color: green"}
        -   [Post-Hoc Analysis, emmeans \>\> Logistic Regression](post-hoc-analysis-emmeans.qmd#sec-phocanal-emmeans-logreg){style="color: green"}
    -   Marginal Effects and Elasticities are similar except elasticities are percent change.
        -   e.g. a percentage change in a regressor results in this much of a percentage change in the response level probability
-   In general
    -   A "marginal effect" is a measure of the association between an infinitely small change in a regressor and a change in the response variable
        -   "infinitely small" because we're using partial derivatives
        -   Example: If I change the cost (regressor) of taking the bus, how does that change the *probability* (not odds or log odds) of taking a bus to work instead of a car (response)
            -   Allows you to ask counterfactuals.
    -   In OLS regression with no interactions or higher-order term, the estimated slope coefficients *are* marginal effects, but for glms, the coefficients are not marginal effects at least not on the scale of the response variable
    -   Marginal effects are partial derivatives of the regression equation with respect to each variable in the model for each unit in the data. (also see notebook, Regression \>\> logistic section)
-   Logistic Regression
    -   The partial derivative gives the slope of a tangent line at point on a nonlinear curve (e.g. logit) which is the linear change in probability at a single point on the nonlinear curve\
        $$
        \frac {\partial P}{\partial X_{k}} = \beta_{k} \times P \times (1 - P)
        $$
        -   Where P is the predicted probability and β is the model coefficient for the kth predictor
-   3 types of marginal effects
    -   [Marginal effect at the Means]{.underline}
        -   The marginal effect that is "typical" of the sample
        -   Each model coefficient is multiplied times their respective independent variables mean and summed along with the intercept.
            -   This sum is transformed from a logit to a probability, P, is used in the partial derivative equation to calculate the marginal effect
        -   Interpretation
            -   continuous: an infinitely small change in <predictor> while all other predictors are held at their means results in a <marginal effect> change in <dependent variable>
                -   e.g. an infinitely small change in this predictor for a hypothetical average person results in this amount of change in the dependent variable
        -   "at the *medians*" can be easily calculated using `marginaleffects::typical`  (this is outdated) without any columns specified and then used to calculate marginal effects
            -   to get "at the means" you'd have to supply each column with its mean value
    -   [Marginal effect at Representative ("Typical") Values]{.underline}
        -   The marginal effect that is "typical" of a group represented in the sample
        -   A *real* "average" person doesn't usually have the mean/median values for predictor values (e.g  mean age of 54.68 years), so you might want to find the marginal effect for a "typical" person of a certain demographic or group you're interested in by specifying values for predictors
        -   Interpretation
            -   continuous: an infinitely small change in <predictor> for a person with <representative values for other predictors> results in a <marginal effect> change in <dependent variable>
        -   Calculate using `marginaleffects::typical`  (this is outdated) to specify column values (this is outdated)
    -   [Average Marginal Effect (AME)]{.underline}
        -   The marginal effect of a case chosen at random from the sample
            -   Considered the best summary of an independent variable
        -   Calculate marginal effect for each observation (distribution of marginal effects) and then take the average
            -   Multiply the model coefficient times each value of an independent variable, repeat for each predictor, sum with intercept, use predicted probabilities to calculate marginal effect, and average all marginal effects across all obseravation for a predictor to get the AME
        -   Interpretation
            -   continuous: Holding all covariates constant, an infinitely small change in <predictor> results in a <marginal effect> change in <dependent variable> on average.
        -   Use `marginaleffects::marginaleffects` +`summary` or `tidy`
-   [{marginaleffects}]{style="color: #990000"}
    -   [Example]{.ribbon-highlight}: Palmer penguins data and logistic regression model
        -   Create marginal effects object

            ``` r
            mfx <- marginaleffects(mod)
            head(mfx)
            #>  rowid              term        dydx  std.error large_penguin bill_length_mm
            #> 1    1    bill_length_mm 0.017622745 0.007837288            0          39.1
            #> 2    1 flipper_length_mm 0.006763748 0.001561740            0          39.1
            #> 3    2    bill_length_mm 0.035846649 0.011917159            0          39.5
            #> 4    2 flipper_length_mm 0.013758244 0.002880123            0          39.5
            #> 5    3    bill_length_mm 0.084433436 0.021119186            0          40.3
            #> 6    3 flipper_length_mm 0.032406447 0.008159318            0          40.3
            #>  flipper_length_mm species
            #> 1              181  Adelie
            #> 2              181  Adelie
            #> 3              186  Adelie
            #> 4              186  Adelie
            #> 5              195  Adelie
            #> 6              195  Adelie
            ```

        -   Average Marginal Effect (AME)

            ``` r
            summary(mfx)
            #> Average marginal effects 
            #>      type              Term          Contrast    Effect Std. Error  z value
            #> 1 response    bill_length_mm              <NA>  0.02757    0.00849  3.24819
            #> 2 response flipper_length_mm              <NA>  0.01058    0.00332  3.18766
            #> 3 response          species Chinstrap / Adelie  0.00547    0.00574 -4.96164
            #> 4 response          species    Gentoo / Adelie  2.19156    2.75319  0.62456
            #> 5 response          species Gentoo / Chinstrap 400.60647  522.34202  4.59627
            #>    Pr(>|z|)      2.5 %    97.5 %
            #> 1  0.0011614    0.01093    0.04421
            #> 2  0.0014343    0.00408    0.01709
            #> 3 2.0906e-06  -0.00578    0.01673
            #> 4  0.8066373  -3.20459    7.58770
            #> 5 1.2828e-05 -623.16509 1424.37802
            ```

            -   Interpretation:
                -   Holding all covariates constant, for an infinitely small increase in bill length, the probability of being a large penguin increases on average by 2.757%
                -   Species contrasts are from [{emmeans}]{style="color: #990000"}(also see [Post-Hoc Analysis, emmeans](Post-Hoc%20Analysis,%20emmeans))
                    -   Contrasts from get_contrasts.R on package github using `emmeans::contrast(emmeans_obj, method = "revpairwise")`
                    -   odds ratios
                    -   You can get response means per category on the probability scale using `emmeans::emmeans(mod, "species", type = "response")`
    -   Other features available for marginaleffects object
        -   `tidy`
            -   coef stats with AME, similar to summary, just a different a object class I think
        -   `glance`
            -   model GOF stats
        -   `typical`
            -   generate artificial predictor values and get marginal effects for them
            -   median (or mode depending on variable type) values used for columns that aren't provided
        -   `counterfactual`
            -   use observed data for all but a one or a few columns. Provide values for those column(s) (e.g. flipper_length_mm = c(160, 180))
            -   genereates a new larger dataset where each observation has each of the provided values
        -   Viz
            -   [{modelsummary}]{style="color: #990000"} tables
            -   `plot` (error bar plot for AME) and `plot_cme` (line plot for interaction term)
                -   outputs ggplot objects
-   Categorical Variables
    -   The 3 types of marginal effects can be modified for categoricals
    -   Steps ("at the means", binary)
        1.  Calculate the predicted probability when the variable = 1 and the other predictors are equal to their means.
        2.  Calculate the predicted probability when the variable = 0 and the other predictors are equal to their means.
        3.  The difference in predicted probabilities is the marginal effect for a change from the "base level" (aka reference category)
    -   This can extended to categorical variables with multiple categories by calculating the pairwise differences between each category and the reference category (contrasts)

## Log Binomial {#sec-reg-log-logbin .unnumbered}

-   Assumes a binomial distribution for a binary outcome but, unlike logistic regression, uses a log link function which automatically gives more interpretable risk ratios as effects.

### Misc {#sec-reg-log-logbin-misc .unnumbered}

-   Also see
    -   [Experiments, Analysis \>\> Risk](experiments-analysis.qmd#sec-exp-anal-risk){style="color: green"}
    -   [Starting Values](regression-logistic.qmd#sec-reg-log-stval){style="color: green"}
-   Notes from
    -   Introduction to Regression Methods for Public Health Using R, [Ch. 6.21](https://www.bookdown.org/rwnahhas/RMPH/blr-log-binomial.html#blr-log-binomial)
        -   Brief introduction with a basic [{logbin}]{style="color: #990000"} example.
    -   [Video: Beyond Logistic Regression: Master Risk Ratios with Log-Binomial by yuzaR](https://www.youtube.com/watch?v=6-mfTmKTYKM&t=177s&pp=0gcJCccJAYcqIYzv)
-   Packages
    -   `glm(..., family = binomial(link = "log"))`
        -   Requires you to pick starting values
        -   Employs step-halving to bring estimates back into the parameter space if the deviance becomes infinite or any fitted values are outside (0, 1) at any iteration. (logbin vignette)
            -   If a full Fisher scoring step of IRLS (glm fitting method) will lead to either an infinite deviance or predicted values that are invalid for the model being fitted, then the increment in parameter estimates is repeatedly halved until the updated estimates no longer exhibit these features. (glm2 vignette)
    -   [{]{style="color: #990000"}[glm2](https://cran.r-project.org/web/packages/glm2/index.html){style="color: #990000"}[}]{style="color: #990000"} ([Vignette](https://journal.r-project.org/articles/RJ-2011-012/)) - Fits models with a modified default fitting method that provides greater stability for models that may fail to converge using glm
        -   Sometimes the method (step-halving) that glm uses when it fails to converge doesn't get activated. This package uses that method at the start.
        -   Could still require starting values.
    -   [{]{style="color: #990000"}[logbin](https://cran.r-project.org/web/packages/logbin/index.html){style="color: #990000"}[}]{style="color: #990000"} ([Vignette](https://www.jstatsoft.org/article/view/v086i09)) - Methods for fitting log-link GLMs and GAMs to binomial data, including EM-type algorithms with more stable convergence properties than standard methods.
        -   Don't have to mess with starting values
        -   Has a variety of methods to aid in convergence and restricting the parameter space so as to keep the predicted probabilities between 0 and 1
            -   Default: [method = "cem"]{.arg-text} which is a combinatorial EM algorithm
            -   CEM can be computationally intensive for larger datasets or more complex specifications (e.g. splines).
            -   Recommended to try using the parameter expanded EM algorithm ([method = "em"]{.arg-text}) and SQUAREM acceleration scheme ([acclerate = "squarem"]{.arg-text}) to speed up convergence .
        -   Other methods and acceleration schemes are available
        -   Fits B-splines via `logbin.smooth`
        -   Due to the way in which the covariate space is defined in the CEM algorithm, models that include terms that are functionally dependent on one another — such as interactions and polynomials — may give unexpected results. (i.e. **interactions and `poly` is not supported**)
            -   2-way interactions between factors can be included by calculating a new factor term that has levels corresponding to all possible combinations of the factor levels
            -   e.g. `heart$AgeSev <- 10 * heart$AgeGroup + heart$Severity`
            -   In this example [AgeGroup]{.var-text} and [Severity]{.var-text} are each 3-level categorical variables coded as numerics
            -   Not sure why they multiplied the [AgeGroup]{.var-text} variable by 10.
            -   Don't think marginal effects are possible using this method. If indeed not, then this is not a satisfactory solution. Use glm or glm2.
        -   Categorical covariates should always be entered directly as factors rather than dummy variables.
-   Papers
    -   [Particle swarm optimization with Applications to Maximum Likelihood Estimation and Penalized Negative Binomial Regression](https://arxiv.org/abs/2405.12386)
        -   Uses Partical Swarm optimization via [{]{style="color: goldenrod"}[pyswarm](https://pyswarms.readthedocs.io/en/latest/){style="color: goldenrod"}[}]{style="color: goldenrod"} to fit various mixture models and also Log-Binomial
        -   Successfully converges where other optimization methods fail. May also be more efficient in some cases.
-   With logistic regression, the left-hand side is the log of the odds, whereas in log-binomial regression it is the log of the probability ($p$).
-   Exponentiating a regression coefficient in log-binomial regression results in a Risk Ratio (RR) or Prevalence Ratio (PR).
-   A disadvantage of (standard) log-binomial regression is that the left-hand side $\ln(p)$ is constrained to be positive while the right-hand side can be anything from $-\infty$ to $\infty$, and this leads to:
    -   Convergence issues for extreme data (i.e. rare events which have low prevalence)
    -   Predicted probabilities \> 1 which is nonsensical

### Examples {#sec-reg-log-logbin-ex .unnumbered}

-   [Example]{.ribbon-highlight}: Simple Model using a Polynomial and a Spline

    ::: panel-tabset
    ## Data and Models

    ``` r
    pacman::p_load(
      dplyr,
      ggplot2,
      logbin,
      emmeans,
      marginaleffects,
      gtsummary,
      sjPlot
    )

    theme_set(theme_notebook())

    d <- carData::TitanicSurvival |> filter(!is.na(age))

    m_poly <- 
      glm(survived ~ poly(age, 3),
          data = d,
          family = binomial(link = "log"))

    m_spl_bin <- 
      logbin.smooth(
        survived ~ B(age, knot.range = 0:3),
        data = d
      )
    ```

    -   Both are similar fitting models. [{logbin}]{style="color: #990000"} can't fit a polynomial, but it can fit a B-spline.
    -   The polynomial model converges just fine, so no need for starting values or [{glm2}]{style="color: #990000"}

    ## emmeans

    ``` r
    m_poly <- 
      glm(survived ~ poly(age, 3),
          data = d,
          family = binomial(link = "log"))

    emmeans(
      m_poly,
      pairwise ~ age,
      type = "response",
      at = list(age = c(quantile(d$age, 
                                 probs = c(0.05, 0.50, 0.95)))),
      weights = "prop",
      infer = TRUE
    )
    # $emmeans
    # age  prob     SE  df asymp.LCL asymp.UCL null z.ratio p.value
    # 5   0.558 0.0399 Inf     0.485     0.642    1  -8.154  <.0001
    # 28  0.378 0.0181 Inf     0.344     0.415    1 -20.350  <.0001
    # 57  0.412 0.0410 Inf     0.339     0.501    1  -8.914  <.0001
    # 
    # Confidence level used: 0.95 
    # Intervals are back-transformed from the log scale 
    # Tests are performed on the log scale 
    # 
    # $contrasts
    # contrast      ratio    SE  df asymp.LCL asymp.UCL null z.ratio p.value
    # age5 / age28  1.476 0.124 Inf     1.212       1.8    1   4.632  <.0001
    # age5 / age57  1.354 0.165 Inf     1.018       1.8    1   2.489  0.0343
    # age28 / age57 0.917 0.104 Inf     0.703       1.2    1  -0.757  0.7293
    # 
    # Confidence level used: 0.95 
    # Conf-level adjustment: tukey method for comparing a family of 3 estimates 
    # Intervals are back-transformed from the log scale 
    # P value adjustment: tukey method for comparing a family of 3 estimates 
    # Tests are performed on the log scale 
    ```

    -   [{emmeans}]{style="color: #990000"} doesn't support a logbin.smooth model
    -   emmeans are the (equally weighted) marginal means and the contrasts are the risk ratios

    ## marginaleffects

    ``` r
    avg_comparisons(m_poly) # <1>
    #>  Estimate Std. Error     z Pr(>|z|)   S    2.5 %    97.5 %
    #>   -0.0028    0.00105 -2.66  0.00783 7.0 -0.00486 -0.000736
    #> 
    #> Term: age
    #> Type:  response 
    #> Comparison: +1

    avg_predictions(m_poly, variables = list(age = c(5, 28, 57))) # <2>
    #> age Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %
    #>   5    0.558     0.0399 14.0   <0.001 144.9 0.480  0.636
    #>  28    0.378     0.0181 20.9   <0.001 320.3 0.343  0.413
    #>  57    0.412     0.0410 10.1   <0.001  76.6 0.332  0.492

    slopes(m_poly, variables = "age", newdata = datagrid(age = c(5, 28, 57))) # <3>
    #> age Estimate Std. Error      z Pr(>|z|)    S    2.5 %   97.5 %
    #>   5 -0.02563    0.00670 -3.824   <0.001 12.9 -0.03877 -0.01250
    #>  28  0.00120    0.00175  0.682    0.495  1.0 -0.00224  0.00463
    #>  57 -0.00472    0.00454 -1.039    0.299  1.7 -0.01362  0.00418
    #> 
    #> Term: age
    #> Type:  response 
    #> Comparison: dY/dX

    # 1 risk ratio
    avg_comparisons(m_poly, 
                    comparison = "ratioavg",
                    variables = list(age = c(5, 28))
    )
    #> Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %
    #>    0.678     0.0569 11.9   <0.001 106.1 0.566  0.789
    #> 
    #> Term: age
    #> Type:  response 
    #> Comparison: mean(28) / mean(5)

    # all risk ratios
    avg_predictions(m_poly, 
                    variables = list(age = c(5, 28, 57)), 
                    hypothesis = ratio ~ revpairwise)
    #> Hypothesis Estimate Std. Error     z Pr(>|z|)     S 2.5 % 97.5 %
    #> (5) / (28)     1.476      0.124 11.90   <0.001 106.1 1.233   1.72
    #> (5) / (57)     1.354      0.165  8.21   <0.001  52.1 1.031   1.68
    #> (28) / (57)    0.917      0.104  8.79   <0.001  59.2 0.713   1.12
    ```

    1.  The average effect of increasing the [age]{.var-text} by 1 unit on the predicted probability of [survival]{.var-text}.
    2.  These are the marginal means. The median age, 28, has a survival probability of 37.8% on average.
    3.  These are the simple slopes. At age = 5, the slope is negative which indicates that an increase in age is associated with an decrease in the probability of survival. This changes by age 28 when a slightly positive slope indicates increasing age increases the probability of survival. For ages 28 and 57, the slopes are near zero which indicates a weak assoication with survival. The high p-values suggest this association could be null.

    ## Visualize

    ``` r
    plot_predictions(m_poly, condition = "age")
    # plot_comparisons(m_poly, 
    #                  comparison = "ratioavg",
    #                  variables = list(age = c(5, 28)),
    #                  condition = "age"
    # )
    ```

    -   Thoughts
    :::

## Probit {#sec-reg-log-probit .unnumbered}

-   "For some reason, econometricians have never really taken on the benefits of the generalized linear modelling framework. So you are more likely to see an econometrician use a probit model than a logistic regression, for example. Probit models tended to go out of fashion in statistics after the GLM revolution prompted by [Nelder and Wedderburn (1972)](http://dx.doi.org/10.2307%2F2344614)." ([Hyndman](https://robjhyndman.com/hyndsight/statistics-vs-econometrics/))
-   Very similar to Logistic\
    ![](./_resources/Regression,_Logistic.resources/probit_logit.svg){width="382"}
-   Where probit and logistic curves differ\
    ![](./_resources/Regression,_Logistic.resources/probit_minus_logit.svg){width="382"}
-   link function: Φ-1(p) where Φ is the standard normal CDF:\
    ![](./_resources/Regression,_Logistic.resources/probit_link2.svg)
-   The probability prediction, p:\
    ![](./_resources/Regression,_Logistic.resources/probit_regression.svg)

## Starting Values {#sec-reg-log-stval .unnumbered}

-   GLM models that use non-standard link functions (e.g. Binomial w/log link, Poisson w/identitly link) sometimes require starting values in order to converge
-   `glm` has a [start]{.arg-text} argument that allows you to pass a vector of starting values
    -   The values should be in the same order as the coefficients in the model (i.e. intercept is first, etc.)
-   [Number of Starting Values]{.underline}
    -   In a typical model, it should equal the number of coefficients + the intercept
    -   You can fit the model and then count the coefficients: `names(coef(mod))`
    -   Example: `survived ~ passengerClass * age_cats`
        -   Where [survived]{.var-text} is a binary variable, [passengerClass]{.var-text} is a factor variable with 3 levels, [age_cats]{.var-text} is a factor variable with 3 levels.
        -   Number of Starting Values (Total of 9)
            -   Intercept: 1 parameter
            -   passengerClass: 2 parameters (3 levels - 1 reference level)
            -   age_cats: 2 parameters (3 levels - 1 reference level)
            -   Interaction (passengerClass:age_cats): 4 parameters (2 × 2 from the non-reference levels)
-   [Options]{.underline}
    -   Guidelines
        -   Values should be reasonable for your given link function
            -   e.g. With a log link, the values should be reasonable on the log scale
        -   Avoid extreme values that could cause numerical issues
    -   Use zeros
        -   e.g. `start = rep(0, 9)`
    -   Use estimates from the standard link model
        -   [Example]{.ribbon-highlight}:

            ``` r
            # Fit logit model first
            logit_model <- 
              glm(survived ~ passengerClass * age_cats, 
                  data = d, 
                  family = binomial("logit"))
            start_values <- coef(logit_model)

            # Then use these as starting values for log model
            log_model <- 
              glm(survived ~ passengerClass * age_cats, 
                  data = d,
                  family = binomial("log"), 
                  start = start_values)
            ```
    -   Data-informed values
        -   [Example]{.ribbon-highlight}

            ``` r
            # Use overall survival rate
            intercept_start <- log(mean(d$survived))

            # Use small values for other parameters
            start_values <- c(intercept_start, rep(0, 8))
            ```
    -   Slight perturbation around 0
        -   e.g. `start = rnorm(9, mean = 0, sd = 0.1)`

## Visualization {#sec-reg-log-viz .unnumbered}

-   Misc

    -   Notes from
        -   Visualizing logistic regression results for non-technical audiences ([github](https://github.com/keikcaw/visualizing-logistic-regression), [vignette](https://keikcaw.github.io/visualizing-logistic-regression/Intro.html), [video](https://www.youtube.com/watch?v=svHT7H1ZykA))
    -   Tables\
        ![](./_resources/Regression,_Logistic.resources/Screenshot%20(1035).png){.lightbox width="461"}
        -   Appropriate for technical audiences who are familiar with logistic regression
        -   Can be taxing on the eyes making it difficult to absorb insights from your model

-   Log Odds Ratio vs Odds Ratio\
    ![](./_resources/Regression,_Logistic.resources/Screenshot%20(1033).png)

    -   Using log odds ratio is recommended for visualizations for puposes of comparing effect magnitudes
        -   Each coefficient represents an additive change on the log odds scale; when we exponentiate to get odds, each coefficient represents a multiplicative change
        -   Odds ratios makes the chart asymmetric and squishes some bars (e.g. Pet: Fish)
        -   Odds ratios can be misleading in comparing negative vs postive variable effects
            -   e.g. Prior GPA looks like it has much bigger effect than Pet: Fish when using odds ratio
        -   There's a danger that the percent change in odds might be misinterpreted as the absolute probability of the outcome (or the change in its probability)
            -   e.g. A 300% change in the odds ratio is a tripling of the odds ratio, not an 300% increase in probability. (see example below)
    -   Although numerically, changes in odds ratios may be a bit easier to describe for your audience than changes in log odds.
        -   Example: tripling the odds ratio is like going from 3-to-1 odds to 9-to-1 odds, or from 1-to-3 odds to an even chance.)

-   Error bar chart (caterpillar chart)

    -   Change in log odds\
        ![](./_resources/Regression,_Logistic.resources/image.png){.lightbox width="428"}
        -   These are untransformed parameter estimates (See [Misc](regression-logistic.qmd#sec-reg-log-misc){style="color: green"} \>\> Tables for estimate values)
    -   Change in probability from a baseline
        -   Since the effect is non-linear (See [Interpetation](regression-logistic.qmd#sec-reg-log-interp){style="color: green"} \>\> Effect is non-linear in p), a baseline is needed in order to properly interpret the changes in probability due to the increases of 1 unit of a variable
        -   Process:
            -   Choose an appropriate baseline
            -   Compute the marginal effect of a predictor given that baseline
        -   Intercept as a baseline\
            ![](./_resources/Regression,_Logistic.resources/image.2.png){.lightbox width="428"}![](./_resources/Regression,_Logistic.resources/image.1.png){.lightbox width="428"}
            -   The x-axis is the problem. Values shown depict changes from the baseline
            -   Both charts have the same values, but the chart on the left more clearly indicates the meaning while the chart on the right includes the CIs
            -   Estimates transformations
                -   Intercept: `invlogit(intercept)`
                -   Other parameters: `invlogit(<param> + intercept)`
                    -   Including CI values
            -   Vertical line is the output of the inverse logit of the intercept
                -   Representing the probability of passing for a student with average prior GPA, average height, and the baseline value of each categorical variable -- not a Mac user, doesn't wear glasses, has no pet, favorite color is blue, and didn't go to tutoring.

-   Outcome vs Predictor with Logistic Curve\
    ![](./_resources/Regression,_Logistic.resources/image.3.png){.lightbox width="468"}

    ``` r
    # Plot of data with a logistic curve fit
      ggplot(data, aes(x = z_homr, y = as.numeric(alive) - 1)) +
        geom_jitter(height = 0.1, size =1, alpha = 0.5) +
        geom_smooth(method = "glm",
                    method.args = list(family = "binomial")) +
        theme_minimal() +
        scale_y_continuous(breaks = c(0, 1), labels = c("Alive", "Dead")) +
        ylab("") +
        xlab("HOMR Linear Predictor")
    ```

    -   Dead/Alive is the outcome and HOMR is the predictor

-   Predictor by outcome (count)\
    ![](./_resources/Regression,_Logistic.resources/image.4.png){.lightbox width="468"}

    ``` r
    g1 <- ggplot(data, aes(x = z_homr, fill = alive)) +
        geom_histogram() +
        theme_minimal() +
        xlab("HOMR Linear Predictor") +
        ylab("Number of Participants") +
        scale_fill_brewer("Alive", palette = "Paired")
    ```

-   Predictor by outcome (proportion)\
    ![](./_resources/Regression,_Logistic.resources/image.5.png){.lightbox width="468"}

    ``` r
    g2 <- ggplot(data, aes(x = z_homr, fill = alive)) +
        geom_histogram(position = "fill") +
        theme_minimal() +
        xlab("HOMR Linear Predictor") +
        ylab("Proportion") +
        scale_fill_brewer("Alive", palette = "Paired")
    ```

-   Predictor vs Outcome (beeswarm)\
    ![](./_resources/Regression,_Logistic.resources/image.6.png){.lightbox width="468"}

    ``` r
    ggplot(data, aes(y = z_homr, x = alive, fill = alive, color = alive)) +
        geom_beeswarm() +
        geom_boxplot(alpha = 0, color = "black") +
        theme_minimal() +
        ylab("HOMR Linear Predictor") +
        xlab("Alive at 1 year") +
        scale_fill_brewer(guide = FALSE, palette = "Paired") +
        scale_color_brewer(guide = FALSE, palette = "Paired")
    ```
