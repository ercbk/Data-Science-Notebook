# Time Series {#sec-eda-ts .unnumbered}

## Misc {#sec-eda-ts-misc .unnumbered}

-   Resources
    -   [{]{style="color: #990000"}[astsa](https://github.com/nickpoison/astsa){style="color: #990000"}[}]{style="color: #990000"} - Package, code, and docs for the books
        -   Time Series Analysis and Its Applications: With R Examples (graduate-level)
            -   See R \>\> Documents \>\> Time Series
        -   Time Series: A Data Analysis Approach using R (introductory)
            -   See R \>\> Documents \>\> Time Series
-   Packages
    -   [{]{style="color: #990000"}[mists](https://pkg.earo.me/mists/){style="color: #990000"}[}]{style="color: #990000"} - A suite of 1d, 2d, and visual tools for exploring and polishing missing values residing in temporal data.

        -   Performs a "smart" removal ("polishing") of observations when a time series dataset has NAs scattered throughout. It tries to remove bad chunks of NAs while still retaining as much information as possible by minimizing a loss function.

    -   [{]{style="color: #990000"}[timetk](https://business-science.github.io/timetk/){style="color: #990000"}[}]{style="color: #990000"} - Various diagnostic, preprocessing, and engineering functions. Also available in Python

    -   [{]{style="color: goldenrod"}[diaquiri](https://ropensci.github.io/daiquiri/){style="color: goldenrod"}[}]{style="color: goldenrod"}

        ::: {layout-ncol="2"}
        ![](./_resources/EDA,_Time_Series.resources/image.1.png){.lightbox group="diaq-pkg-1" width="332"}

        ![](./_resources/EDA,_Time_Series.resources/image.png){.lightbox group="diaq-pkg-1" width="332"}
        :::

        -   Aggregated values are automatically created for each data field (column) depending on its contents (e.g. min/max/mean values for numeric data, no. of distinct values for categorical data)
        -   Overviews for missing values, non-conformant values, and duplicated rows.

    -   [{]{style="color: #990000"}[actfts](https://sergiofinances.github.io/actfts/){style="color: #990000"}[}]{style="color: #990000"} - A flexible approach to time series analysis by focusing on Autocorrelation (ACF), Partial Autocorrelation (PACF), and stationarity tests, generating interactive plots for dynamic data visualization.

    -   [{]{style="color: #990000"}[gglinedensity](https://hrryt.github.io/gglinedensity/){style="color: #990000"}[}]{style="color: #990000"} - Provides a “derived density visualisation (that) allows users both to see the aggregate trends of multiple (time) series and to identify anomalous extrema."

    -   [{]{style="color: #990000"}[brolgar](https://brolgar.njtierney.com/){style="color: #990000"}[}]{style="color: #990000"}

        -   Efficiently explore raw longitudinal data
        -   Calculate features (summaries) for individuals
        -   Evaluate diagnostics of statistical models

    -   [{]{style="color: #990000"}[ggtime](https://pkg.mitchelloharawild.com/ggtime){style="color: #990000"}[}]{style="color: #990000"} - Various EDA ggplots for tsibble objects

## Basic Characteristics {#sec-eda-ts-basic .unnumbered}

-   [Plot series]{.underline}

    ``` r
    ts_tbl |> 
        timetk::plot_time_series(date, 
                                 value, 
                                 .smooth = T, 
                                 .smooth_span = 0.3, 
                                 .interactive = F)
    ```

    -   Includes a LOESS smoothing line. Control the curviness of LOESS using [smooth_span]{.arg-text}

-   [Missingness]{.underline}

    -   NAs affect the number of lags to be calculated for a variable
        -   e.g. exports only recorded quarterly but stock price has a monthly close price you want to predict. So if you're forecasting monthly oil price then creating a lagged variable for exports is difficult
            -   Bizsci (lab 29), minimum_lag = length_of_sequence_of_tail_NAs +1 
                -   tail(ts) are the most recent values
    -   Even if you don't want a lag for a predictor var and it has NAs, you need to `recipe::step_lag(var, lag = #_of_tail_NAs)`. So var has no NAs.
    -   Consider seasonality of series when determining imputation method

-   [Outcome Variable Distribution]{.underline}

    -   For low volume data, a right-skewed distribution might be needed instead of a Gaussian.

        -   Thin tails - Use a Gamma distribution
        -   Heavier tails - Use a Log Normal or the Inverse Gaussian

    -   [Example]{.ribbon-highligh}\
        ![](_resources/EDA,_Time_Series.resources/gghistogram-1.png){.lightbox width="432"}

        ``` r
        gghistogram(lynx, add.kde=TRUE)
        ```

        -   [add.normal]{.arg-text} adds a gaussian density

-   [Zeros]{.underline}

    -   See notebook for tests on the number of zeros in Poisson section
    -   Might be tests in the intermittent forecasting packages, so see bkmks
    -   If so, see [Logistics, Demand Forecasting \>\> Intermittent Demand](https://ercbk.github.io/Domain-Knowledge-Notebook/qmd/logistics-demand-planning.html#sec-log-demfcast-intdem){style="color: green"} for modeling approaches
    -   Are there gaps in the time series (e.g. missing a whole day/multiple days, days of shockingly low volume)?

-   [Is data recorded at irregular intervals]{.underline}. If so:

    -   [{BINCOR}]{style="color: goldenrod"} handles cross-correlation between 2 series with irregular intervals and series with regular but different intervals
    -   I think common forecasting algorithms require regularly spaced data, so look towards ML, Multilevel Model for Change (MLMC), or Latent Growth Models
    -   May also try binning points in the series (like BINCOR does) or smoothing them to get a regular interval

## Seasonality {#sec-eda-ts-seas .unnumbered}

-   [STL Decomposition]{.underline}
    -   Also see [Forecasting, Decomposition](forecasting-decomposition.qmd#sec-decomp){style="color: green"}

    -   STL uses LOESS for the seasonal component which handles fluctuations in that component better than the classical decomposition approach.

    -   If we are interested in short- or long-term movements of the time series, we do not care about the various seasonalities. We want to know the trend component of the time series and if it has reached a turning point

    -   Is there strong seasonality or trend?

    -   If there's a pattern in the random/remainder component, this could indicate that there are other strong influences present. This component could give you ideas of the type of features you should create.

    -   [Example]{.ribbon-highlight}: [{timetk}]{style="color: #990000"}\
        ![](_resources/EDA,_Time_Series.resources/timetk-stl-decomp-1.jpg){.lightbox width="487"}

        ``` r
        ts_tbl %>%
            plot_stl_diagnostics(
                date, value,
                .feature_set = c("observed", "season", "trend", "remainder"),
                .trend       = 180,
                .frequency   = 30,
                .interactive = F
            )
        ```

        -   ["seasadj"]{.arg-text} also available for [.feature_set]{.arg-text}
        -   For [.frequency]{.arg-text}, ["auto"]{.arg-text}, a time-based definition (e.g. ["2 weeks"]{.arg-text}), or a numeric number of observations per frequency (e.g. [10]{.arg-text}).
            -   ["auto"]{.arg-text} or `tk_get_frequency()` will tell you the frequency of your series.

    -   [Daily Seasonal Adjustment (DSA)]{.underline}

        -   Used to remove seasonal and calendar effects from time series data with daily observations. This allows you to analyze the underlying trends and patterns in the data without being masked by predictable fluctuations like weekdays, weekends, holidays, or seasonal changes.
        -   In a EDA context, the deseasonalized series can be used to locate the time series trend (daily data is very noisy), identify turning points, and compare components with other series.
        -   Packages: [{dsa}]{style="color: #990000"}
        -   Notes from [Seasonal Adjustment of Daily Time Series](https://towardsdatascience.com/seasonal-adjustment-of-daily-time-series-1bd2aa9b096d)
            -   Example shows it outperforming Hyndman's STR procedure in extracting seasonal components more completely.
        -   Daily data can have multiple seasonalities present
            -   Raw Time Series\
                ![](./_resources/EDA,_Time_Series.resources/1-D074QuDWqHGSttFx6tKgjg.jpeg){.lightbox width="632"}
            -   DSA Processed Time Series\
                ![](_resources/EDA,_Time_Series.resources/dsa-example-1.webp){.lightbox width="632"}
        -   Combines the seasonal trend decomposition procedure using Loess (STL) with a regression model with ARIMA errors
        -   Procedure
            -   STL adjusts intra-weekly periodic patterns.
            -   RegARIMA estimates calendar effects, cross-seasonal effects, and outliers.
            -   STL adjusts intra-monthly periodic effects.
            -   STL adjusts intra-annual effects
-   [Seasonality Tests]{.underline} (weekly, monthly, and yearly)
    -   [{]{style="color: #990000"}[seastests](https://cran.r-project.org/web/packages/seastests/index.html){style="color: #990000"}[}]{style="color: #990000"} QS and Friedman (see bkmk in Time Series \>\> eda for example)
    -   QS test's null hypothesis is no positive autocorrelation in seasonal lags in the time series
    -   Friedman test's null hypothesis is no significant differences between the values' period-specific means present in the time series
    -   For QS and Friedman, pval \> 0.05 indicates NO seasonality present
-   [Additive or Multiplicative Structure]{.underline}
    -   Is the variance (mostly) constant (Additive) or not constant (Multiplicative) over time?
    -   Does the amplitude of the seasonal or cyclical component increase over time?\
        ![](./_resources/EDA,_Time_Series.resources/Screenshot%20(248).png){.lightbox width="432"}
        -   The amplitude of the seasonal component increases over time so this series has a multiplicative structure
        -   Also multiplicative, if there's a changing seasonal amplitude for different times of the year
    -   [{]{style="color: #990000"}[nlts::add.test](https://cran.r-project.org/web/packages/nlts/index.html){style="color: #990000"}[}]{style="color: #990000"} - Lagrange multiplier test for additivity in a time series
    -   If you have a multiplicative structure and zeros in your data (i.e. intermittent data), then they must handled in some way.
        -   See [Logistics, Demand Planning \>\> Intermittent Demand](https://ercbk.github.io/Domain-Knowledge-Notebook/qmd/logistics-demand-planning.html#sec-log-demfcast-intdem){style="color: green"} and [Forecasting, Statistical \>\> Count Forecasting](forecasting-statistical.qmd#sec-fcast-stat-count){style="color: green"}

## Groups {#sec-eda-ts-group .unnumbered}

-   [Quantile values per frequency unit (by group and total)]{.underline}
    -   [{timetk::plot_time_series_boxplot}]{style="color: #990000"}

    -   Average closing price for each month, each day of the month, each day of the week

    -   When are dips and peaks?

    -   Which groups are similar

    -   What are the potential reasons behind these dips and peaks?

    -   [Example]{.ribbon-highlight}: Daily Power Consumption\
        ![](./_resources/EDA,_Time_Series.resources/1-mDCO3WKk5gqfK-k-pf-p0g.jpeg)

        -   Median, the lower quartile, and the upper quartile for Saturdays and Sundays are below the remaining weekdays when inspecting daily power consumption
        -   Some outliers are present during the week, which could indicate lower power consumption due to moving holidays
            -   Moving holidays are holidays which occur each year, but where the exact timing shifts (e.g. Easter)

    -   [Example]{.ribbon-highlight}: {timetk} Calendar Effects\
        ![](_resources/EDA,_Time_Series.resources/timetk-calendar-effects.jpg){.lightbox width="486"}

        ``` r
        ts_tbl %>%
            plot_seasonal_diagnostics(date, value, .interactive = F)
        ```

    -   [Example]{.ribbon-highlight}: Monthly Precipitation ([source](https://www.johaniefournier.com/blog/cdq_precipitation_eda/#weather-anomalies))\
        ![](_resources/EDA,_Time_Series.resources/group-precip-month-1.png){.lightbox width="532"}

        ``` r
        ggplot(precipitation_dt, 
               aes(x = month, 
                   y = mean*1000, 
                   group = month)) +
          geom_boxplot(fill="#DBBDC3")+
          geom_dotplot(binaxis = "y", 
                       stackdir = "center", 
                       dotsize = 0.5, 
                       alpha=0.3)+
          labs(title="Cummulative Monthly Precipitations (mm) for Centre-du-Québec 1993-2023",
               x="Month",
               y="Precipitation (mm)")+
          theme(plot.title = element_text(hjust = 0, face = "bold", size=12),
                axis.title.x = element_text(hjust = 0, face = "bold", size=8),
                axis.title.y = element_text(hjust = 1, face = "bold", size=8))+
          scale_x_continuous(breaks=seq(1,12,1), limits=c(0.5, 12.5))
        ```

    -   [Example]{.ribbon-highlight}: Monthly Power Consumption![](./_resources/EDA,_Time_Series.resources/1-ZKjg0i7fLRIWrys8phtCiw.jpeg)

        -   Median, the lower quartile, and the upper quartile of power consumption are lower during the spring and summer than autumn and winter

    -   [Example]{.ribbon-highlight}: Demand per Month and per Category![](./_resources/EDA,_Time_Series.resources/image.2.png)

    -   Interactive Spaghetti Plot ([source](https://ramikrispin.github.io/r-ladies-rome-workshop/04_seasonal_analysis.html))

        ::: {layout-ncol="2"}
        ![](_resources/EDA,_Time_Series.resources/seas-spag-1.png){.lightbox width="332"}

        ![](_resources/EDA,_Time_Series.resources/seas-spag-2.png){.lightbox width="332"}
        :::

        ``` r
        years <- unique(ts2$year)

        colors <- colorRampPalette(RColorBrewer::brewer.pal(9, "YlGnBu"))(length(years))

        p <- plot_ly()

        for (i in seq_along(years)) {
          year_data <- ts2 |> filter(year == years[i])

          p <- p |>
            add_trace(
              data = year_data,
              x = ~month_label,
              y = ~y,
              type = 'scatter',
              mode = 'lines+markers',
              name = as.character(years[i]),
              line = list(color = colors[i]),
              marker = list(color = colors[i])
            )
        }

        p
        ```
-   [Cumulative Counts per Group]{.underline}\
    ![](_resources/EDA,_Time_Series.resources/cum-count-group-1.webp){.lightbox width="532"}
    -   Similar pattern indicating seasonality
    -   2011 seems like an outlier
-   [Variance of value by group]{.underline}
    -   Example: how sales vary between store types over a year
        -   Important to standardize the value by group
            -   `df %>% group_by(group) %>% mutate(sales = scale(sales))`
        -   Which groups vary wildly and which are more stable
-   [Rates by group]{.underline}
    -   Example: sales(\$) per customer
        -   `df %>% group_by(group, month) %>% mutate(sales_per_cust = sum(sales)/sum(customers)`

## Statistical Features {#sec-eda-ts-feats .unnumbered}

-   Outliers
    -   Also see

        -   "PCA the Features" below
        -   [Anomaly Detection](anomaly-detection.qmd#sec-anomdet){style="color: green"}

    -   [Example]{.ribbon-highlight}: Using the Remainder to detect outliers ([source](https://ramikrispin.github.io/r-ladies-rome-workshop/02_time_series_decomposition.html#the-irregular-component-1))\
        ![](_resources/EDA,_Time_Series.resources/stat-feat-out-stl-1.png){.lightbox width="480"}

        ``` r
        sdv <- sd(ts_stl_d$remainder)

        ts_stl_d <- 
          ts_stl_d |>
          dplyr::mutate(sd3 = ifelse(remainder >= 3 * sdv | remainder <= -3 * sdv, y, NA ),
                        sd2 = ifelse(remainder >= 2 * sdv & remainder < 3 * sdv |  remainder <= -2 * sdv &  remainder >  -3 * sdv, y, NA))


        table(!is.na(ts_stl_d$sd2))
        #> FALSE  TRUE 
        #>   270    22 

        table(!is.na(ts_stl_d$sd3))
        #> FALSE  TRUE 
        #>   286     6 
        ```

        -   Krispin manually made this STL plot with plotly, so see source for the code
        -   He used 2 and 3 times the standard deviation of the remainder as thresholds for spotting outlier points.
        -   Point are orange (2x) and red (3x) in the observed series. Dashed lines in the remainder indicate the thresholds.
        -   These points require further investiigation as they might be difficult to forecast with standard modeling techniques. Features may be built so that the model can account for them.

    -   [Example]{.ribbon-highlight}: [{timetk}]{style="#990000"}\
        ![](_resources/EDA,_Time_Series.resources/timetk_anomaly-1.png){.lightbox width="632"}

        ``` r
        walmart_sales_weekly |>  
            group_by(id) |> 
            plot_anomaly_diagnostics(Date, Weekly_Sales,
                                     .message = FALSE,
                                     .facet_ncol = 3,
                                     .ribbon_alpha = 0.25,
                                     .interactive = FALSE)
        ```
-   [Statistical Features vs Outcome]{.underline}
    -   Misc

        -   Also see [Feature Engineering, Time Series](feature-engineering-time-series.qmd#sec-feat-eng-ts){style="color: green"}

        -   Resources

            -   [Feature-based time series analysis](https://robjhyndman.com/hyndsight/fbtsa/) (Hyndman Blog Post)

            -   Krispin [notebook](https://raw.githack.com/RamiKrispin/ts-cluster-analysis-r/main/02_features_engineering.html) that details using [{tsfeatures}]{style="color: #990000"} to extract features from a group series. Then, he PCAs and Clusters the features, and uses a shiny dashboard to eda them.

        -   Determining seasonal strength, trend strength, spectral entropy for series might indicate a particular group of models is likely better than others at forecasting these series.

        -   Determining outlier series may indicate further exploration is required. A different set of variables may be more predictive for these series than the others.

        -   PCA and clustering of statistical features may indicate distinct groups of series that can be grouped together into a global model.

    -   First Autocorrelation Coefficient vs Categorical vs Binary Outcome

        ![](_resources/EDA,_Time_Series.resources/autocor-vs-cat-vs-cat.png){.lightbox width="632"}

        -   There does seem to be some variance. An interaction between autocorrelation and the cateogorical variable might be predictive of a heart murmur event.
-   [Trend Strength and Seasonal Strength plots]{.underline}
    -   [{tsfeatures}]{style="color: #990000"} has seasonality strength metric
    -   Seems like this can tease apart the Spectral Entropy value into its constituents.
        -   e.g. Maybe a series has a medium positive spectral entropy score because of really strong seasonal strength but only middling trend strength.
    -   Cox-Stuart test - Tests whether there's an increasing or decreasing trend
        -   [{]{style="color: #990000"}[tsutils::trendtest](https://cran.r-project.org/web/packages/tsutils/index.html){style="color: #990000"}[}]{style="color: #990000"} - Test a time series for trend by either fitting exponential smoothing models and comparing then using the AICc, or by using the non-parametric Cox-Stuart test.
-   [Shannon Spectral Entropy]{.underline}
    -   `feasts::feat_spectral`  will compute the (Shannon) spectral entropy of a time series, which is a measure of how easy the series is to forecast.
    -   A series which has strong trend and seasonality (and so is easy to forecast) will have entropy close to 0.
    -   A series that is very noisy (and so is difficult to forecast) will have entropy close to 1.
-   [PCA the Features]{.underline}
    -   Identify characteristics for high dimensional data (From [fpp3](https://otexts.com/fpp3/exploring-australian-tourism-data.html))\
        ![](_resources/EDA,_Time_Series.resources/pca-1.png){.lightbox width="432"}

        ``` r
        library(broom)
        library(feasts)

        tourism_features <- tourism |>
          features(Trips, feature_set(pkgs = "feasts"))

        pcs <- tourism_features |>
          select(-State, -Region, -Purpose) |>
          prcomp(scale = TRUE) |>
          augment(tourism_features)
        pcs |>
          ggplot(aes(x = .fittedPC1, y = .fittedPC2, col = Purpose)) +
          geom_point() +
          theme(aspect.ratio = 1)
        ```

        -   Holiday series behave quite differently from the rest of the series. Almost all of the holiday series appear in the top half of the plot, while almost all of the remaining series appear in the bottom half of the plot.
        -   Clearly, the second principal component is distinguishing between holidays and other types of travel.
        -   The four points where PC1 \> 10 stand out as outliers.

    -   Visualize Outliers\
        ![](_resources/EDA,_Time_Series.resources/pcaoutliers-1.png){.lightbox width="532"}

        ``` r
        outliers <- pcs |>
          filter(.fittedPC1 > 10) |>
          select(Region, State, Purpose, .fittedPC1, .fittedPC2)
        outliers
        #> # A tibble: 4 × 5
        #>   Region                 State             Purpose  .fittedPC1 .fittedPC2
        #>   <chr>                  <chr>             <chr>         <dbl>      <dbl>
        #> 1 Australia's North West Western Australia Business       13.4    -11.3  
        #> 2 Australia's South West Western Australia Holiday        10.9      0.880
        #> 3 Melbourne              Victoria          Holiday        12.3    -10.4  
        #> 4 South Coast            New South Wales   Holiday        11.9      9.42
        outliers |>
          left_join(tourism, 
                    by = c("State", "Region", "Purpose"), 
                    multiple = "all") |>
          mutate(Series = glue("{State}", "{Region}", "{Purpose}", 
                               .sep = "\n\n")) |>
          ggplot(aes(x = Quarter, y = Trips)) +
          geom_line() +
          facet_grid(Series ~ ., 
                     scales = "free") +
          labs(title = "Outlying time series in PC space")
        ```

    -   Why might these series be identified as unusual?

        -   Holiday visits to the south coast of NSW is highly seasonal but has almost no trend, whereas most holiday destinations in Australia show some trend over time.
        -   Melbourne is an unusual holiday destination because it has almost no seasonality, whereas most holiday destinations in Australia have highly seasonal tourism.
        -   The north western corner of Western Australia is unusual because it shows an increase in business tourism in the last few years of data, but little or no seasonality.
        -   The south western corner of Western Australia is unusual because it shows both an increase in holiday tourism in the last few years of data and a high level of seasonality.
-   Clustering the Features
    -   Boxplots: Feature vs Cluster\
        ![](_resources/EDA,_Time_Series.resources/stat-feat-cluster-box-1.png){.lightbox}
        -   This is from the Krispin notebook in Resources
        -   Features: Trend, Entropy, Linearity, and ACF 1

## Association {#sec-eda-ts-assoc .unnumbered}

-   See [Association, Time Series](association-time-series.qmd#sec-assoc-ts){style="color: green"}
-   [Lag Scatter Plots]{.underline}
    -   Lag scatterplots between target series and lags of the target series (i.e. y~t~ vs y~t+h~)\
        ![](_resources/EDA,_Time_Series.resources/astsa-lag1-1.png){.lightbox width="532"}

        ``` r
        astsa::lag1.plot(y, 12) # lags 1-12 of y
        astsa::lag1.plot(soi, 12, col=astsa.col(4, .3), pch=20, cex=2) # prettified
        ```

        -   Autocorrelation values in upper right corner
            -   Autocorrelations/Cross-Correlation values only valid if relationships are linear but maybe still useful in determining a positive or negative relationship
        -   LOESS smoothing line added
        -   Nonlinear patterns can indicate that behavior between the two variables is different for high values and low values

    -   Lag scatterplots between target series and lags of the predictor Series (i.e. y~t~ vs x~t+h~)\
        ![](_resources/EDA,_Time_Series.resources/astsa-lag2-1.png){.lightbox width="532"}

        ``` r
        astsa::lag2.plot(y, x, 8) # y vs lags 0-8 of x
        astsa::lag2.plot(soi, rec, 8, cex=1.1, pch=19, col=5, bgl='transparent', lwl=2, gg=T, box.col=gray(1))  #prettified
        ```

        -   If either series has autocorrelation, then it should be prewhitened before being inputted into the function.
            -   See [Association, Time Series \>\> CCF](association-time-series.qmd#cross-correlation-function-ccf){style="color: green"}
        -   Cross-Correlation (CCF) values in upper right corner
            -   Autocorrelations/Cross-Correlation values only valid if relationships are linear but maybe still useful in determining a positive or negative relationship
        -   Nonlinear patterns can indicate that behavior between the two variables is different for high values and low values

    -   Manual Code -[{plotly}]{style="color: #990000"}\
        ![](_resources/EDA,_Time_Series.resources/ass-lag-plot-man-1.png){.lightbox width="632"}

        <Details>

        <Summary>Code</Summary>

        ``` r
        library(plotly)

        plot_lag <- function(ts, var, lag){

          d <- ts |> 
          dplyr::mutate(lag = dplyr::lag(x= !!rlang::sym(var), n = lag))

          # Create the regression formula
          formula <- as.formula(paste(var, "~ lag" ))

          # Fit the linear model
          model <- lm(formula, data = d)

          # Extract model coefficients
          intercept <- coef(model)[1]
          slope <- coef(model)[2]

          # Format regression formula text
          reg_formula <- paste0("y = ", round(intercept, 2),
                                ifelse(slope < 0, " - ", " + "),
                                abs(round(slope, 2)), paste("*lag", lag, sep = ""))

          # Get adjusted R-squared
          adj_r2 <- summary(model)$adj.r.squared
          adj_r2_label <- paste0("Adjusted R² = ", round(adj_r2, 3))

          # Add predicted values to data
          d$predicted <- predict(model, newdata = d)

          # Create plot
          p <- plot_ly(d, x = ~ lag, y = ~get(var), type = 'scatter', mode = 'markers',
                       name = 'Actual') %>%
            add_lines(x = ~ lag, y = ~predicted, name = 'Regression Fitted Line',
                      line = list(color = "red", dash = "dash")) %>%
            layout(title = paste(var, "vs Lag", lag, sep = " "),
                   xaxis = list(title = paste("Lag", lag, sep = " ")),
                   yaxis = list(title = var),
                   annotations = list(
                     list(x = 0.05, y = 0.95, xref = "paper", yref = "paper",
                          text = reg_formula,
                          showarrow = FALSE,
                          font = list(size = 12)),
                     list(x = 0.05, y = 0.88, xref = "paper", yref = "paper",
                          text = adj_r2_label,
                          showarrow = FALSE,
                          font = list(size = 12))
                   ))

          return(p)
        }
        ```

        </Details>

        -   Also displays adjusted R^2^ and the linear regression equation
-   [Partial Autocorrelation]{.underline}
    -   Removes the correlation between $y_{t-k}$ and lags before it ($y_{t-(k-1)}, \ldots, y_{t-1}$) to get a more accurate correlation between y~t~ and y~t-k~. Sort of like a partial correlation but for a univariate time series.

    -   Can be interpreted as the amount correlation between y~t-k~ and y~t~ thats not explained by the previous lags.

    -   Example: `forecast::Pacf(plot = TRUE)` or `forecast::ggPacf`\
        ![](_resources/EDA,_Time_Series.resources/pacf-1.png){.lightbox width="432"}

    -   [Example]{.ribbon-highlight}: {timetk} plots ACF and PACF charts\
        ![](_resources/EDA,_Time_Series.resources/timetk-acf-pacf-1.png){.lightbox width="632"}

        ``` r
        m4_hourly %>%
            group_by(id) %>%
            plot_acf_diagnostics(
                date, value,               # ACF & PACF
                .lags = "7 days",          # 7-Days of hourly lags
                .interactive = FALSE
            )
        ```

    -   [{]{style="color: #990000"}[feasts::feat_pacf](https://feasts.tidyverts.org/reference/feat_pacf.html){style="color: #990000"}[}]{style="color: #990000"}

        -   Contains several features involving partial autocorrelations including:
            -   The sum of squares of the first five partial autocorrelations for the original series
            -   The first-differenced series and the second-differenced series.
            -   For seasonal data, it also includes the partial autocorrelation at the first seasonal lag.

## Stationarity {#sec-eda-ts-station .unnumbered}

-   CCF and most statistical and ML models need or prefer stationary time series.
-   Packages
    -   [{]{style="color: #990000"}[nonstat](https://cran.r-project.org/web/packages/nonstat/index.html){style="color: #990000"}[}]{style="color: #990000"} - Provides a nonvisual procedure for screening time series for nonstationarity
        -   Method combines two diagnostics: one for detecting trends (based on the split R-hat statistic from Bayesian convergence diagnostics) and one for detecting changes in variance (a novel extension inspired by Levene's test).
-   [ACF]{.underline}
    -   For a stationary series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly.

    -   For a non-stationary series:

        -   The value of r1 (correlation between y~t~ and y~t-1~) is often large and positive.
        -   A steady, slow decline towards 0 indicates trend is present
            -   Is the series a trend-stationary or unit root process?
                -   Test all series of interest with ADF and KPSS tests (See [Forecasting, Statistical \>\> Preprocessing](forecasting-statistical.qmd#sec-fcast-stat-preproc){style="color: green"} \>\> Detrend or Difference
        -   A scalloped pattern indicates seasonality is present

    -   95% CIs are $\pm \frac{1.96}{\sqrt{T}}$ where $T$ is the length of the time series.

    -   [Example]{.ribbon-highlight}: `forecast::Acf(plot = TRUE)` or `forecast::ggAcf`

        ![](./_resources/EDA,_Time_Series.resources/acfstationary-1.png){.lightbox width="632"}

        -   The ACF of the differenced Google stock price (right fig) looks just like that of a white noise series. There are no autocorrelations lying outside the 95% limits, and the Ljung-Box 
        -   Q∗ statistic (Ljung-Box) has a p-value of 0.355 (for h = 10) which implies the ts is stationary. This suggests that the daily change in the Google stock price is essentially a random amount which is uncorrelated with that of previous days.
-   [Ljeung-Box]{.underline}
    -   `stats::Box.test` or `feasts::ljung_box`
        -   [x]{.arg-text}: numeric or univariate ts
        -   [lag]{.arg-text}: Recommended 10 for non-seasonal, 2m (e.g. m = 12 for monthly series, m = 4 for quarterly), maximum is T/5 where T is the length of the series.
        -   [type]{.arg-text}: "Lj"
    -   Interpretation: small Q\* or p-value \> 0.05 means the time series is stationary.

## Nonlinear {#sec-eda-ts-nonlin .unnumbered}

-   [Misc]{.underline}
    -   Packages
        -   [{]{style="color: #990000"}[tseriesEntropy](https://cran.r-project.org/web/packages/tseriesEntropy/index.html){style="color: #990000"}[}]{style="color: #990000"} - Tests for serial and cross dependence and nonlinearity based on Bhattacharya-Hellinger-Matusita distance.
            -   `Trho.test.AR.p` - Entropy Tests For Nonlinearity In Time Series - Parallel Version
            -   `surrogate.SA` - Generates surrogate series through Simulated Annealing. Each surrogate series is a constrained random permutation having the same autocorrelation function (up to nlag lags) of the original series x.
            -   `surrogate.ARs` - Generates surrogate series by means of the smoothed sieve bootstrap.
-   [Lag Plots]{.underline}\
    ![](_resources/Forecasting,_Nonlinear.resources/eda-lagplot-ex-1.webp){.lightbox width="432"}
    -   U-pattern shown in nonlinear
    -   See [Association](eda-time-series.qmd#sec-eda-ts-assoc){style="color: green"} for code
-   [Average Mutual Information]{.underline}
    -   [Example]{.ribbon-highlight}: [link](https://towardsdatascience.com/nonlinear-time-series-an-intuitive-introduction-7390aae8b446)
        -   ACF\
            ![](_resources/Forecasting,_Nonlinear.resources/eda-mutinf-ex1-1.webp){.lightbox width="332"}
            -   Assuming this ts has been differenced and/or detrended and there is still autocorrelation at lags 6 and 8. So, attempts at stationarity have failed
        -   Average Mutual Information\
            ![](_resources/Forecasting,_Nonlinear.resources/eda-mutinf-ex1-2.webp){.lightbox width="332"}
            -   If the time series is linear, the AMI should decay exponentially or follow a power-law decay as the time lag increases, whereas if the time series is nonlinear, the AMI may decay more slowly or exhibit specific patterns such as oscillations or plateaus, indicating the presence of nonlinear structures or long-range correlations.
                -   Oscillations are present in this time series.
            -   Compare with a surrogate model data. If the AMI decay pattern of the original time series deviates significantly from that of the surrogate data, it suggests the presence of nonlinearity.
            -   [{]{style="color: #990000"}[nonlinearTseries::mutualInformation](https://cran.r-project.org/web/packages/nonlinearTseries/nonlinearTseries.pdf){style="color: #990000"}[}]{style="color: #990000"}\
-   [Surrogate Testing]{.underline}
    -   [Example]{.ribbon-highlight}: From [{nonlinearTseries}]{style="color: #990000"} vignette\
        ![](_resources/Forecasting,_Nonlinear.resources/eda-surrogate-ex1-1.png){.lightbox width="332"}

        ``` r
        st <- 
          surrogateTest(
            lor.x,
            significance = 0.05,
            one.sided = F,
            FUN = timeAsymmetry, 
            do.plot=T)
        ## Null Hypothesis: Data comes from a linear stochastic process
        ## Reject Null hypothesis:
        ##      Original data's stat is significant larger than surrogates' stats
        ```

        -   `timeAsymmetry` is a function that's included in the package. It measures the asymmetry of a time series under time reversal. If linear, it should be symmetric.
-   [Compare Oberservational vs Surrogate Data]{.underline}
    -   Chaotic nature of the time series is obvious (e.g. frequent, unexplainable shocks that can't be explained by noise)
    -   Create an artificial data set using a gaussian dgp and compare it to the observed data set
        -   For details see Nonlinear Time Series Analysis (pg 6 and Ch.4 sect 7.1)
        -   Takes the range of values, mean, and variance from the observed distribution and generates data
        -   Then data is filtered so that the power spectum is the same
        -   "Phase Portraits" are used to compare the datasets.
