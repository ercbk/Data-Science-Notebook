# EDA, Time Series

TOC


Misc

* Packages
	* [{{]{style='color: goldenrod'}[diaquiri](https://ropensci.github.io/daiquiri/){style='color: goldenrod'}[}}]{style='color: goldenrod'}![](./_resources/EDA,_Time_Series.resources/image.1.png)![](./_resources/EDA,_Time_Series.resources/image.png)
		* aggregated values are automatically created for each data field (column) depending on its contents (e.g. min/max/mean values for numeric data, no. of distinct values for categorical data)
		* overviews for missing values, non-conformant values, and duplicated rows.




* Basic Steps
	* Check seasonality at all periods at and above data frequency
	* Investigate spikes (i.e. special days or weeks)
		Check lags of outcome variable
		
		Check for NAs in outcome variable
		
		Are there a significant amount of zeros
		
* Look for NAs
	* NAs affect the number of lags to be calculated for a variable
		* e.g. exports only recorded quarterly but stock price has a monthly close price you want to predict. So if you're forecasting monthly oil price then creating a lagged variable for exports is difficult
			* Bizsci (lab 29), minimum\_lag = length\_of\_sequence\_of\_tail\_NAs +1 
				* tail(ts) are the most recent values
	* Even if you don't want a lag for a predictor var and it has NAs, you need to recipe::step\_lag(var, lag = #\_of\_tail\_NAs). So var has no NAs.
	* Consider seasonality of series when determining imputation method
* Check the shape of the distribution of the outcome variable
	* For low volume data, a right-skewed distribution might be needed instead of a Gaussian.
		* Thin tails - Use a Gamma distribution
		* Heavier tails - Use a Log Normal or the Inverse Gaussian
* Are there a significant amount of zeros
	* See notebook for tests on the number of zeros in Poisson section
	* Also might be tests in the intermittent forecasting packages, so see bkmks
	* If so, see [Logistics](Logistics) >> Demand Forecasting >> Intermittent Data for modeling approaches
	Timestamp Columns
	* Are there gaps in the time series (e.g. missing a whole day/multiple days, days of shockingly low volume)?
* Seasonality Tests (weekly, monthly, and yearly)
	* [{seastests}]{style='color: #990000'} QS and Friedman (see bkmk in Time Series >> eda for example)
	* QS test’s null hypothesis is no positive autocorrelation in seasonal lags in the time series
	* Friedman test’s null hypothesis is no significant differences between the values’ period-specific means present in the time series
	* For QS and Friedman, pval > 0.05 indicates NO seasonality present
* Look at quantile values per frequency unit (by group and total)
	* {timetk::plot\_time\_series\_boxplot}
	* average closing price for each month, each day of the month, each day of the week
	* when are dips and peaks?
	* Which groups are similar?
	* What are the potential reasons behind these dips and peaks?
	* Example (daily power consumption)![](./_resources/EDA,_Time_Series.resources/1-mDCO3WKk5gqfK-k-pf-p0g.jpeg)
		* median, the lower quartile, and the upper quartile for Saturdays and Sundays are below the remaining weekdays when inspecting daily power consumption
		* some outliers are present during the week, which could indicate lower power consumption due to moving holidays
			* Moving holidays are holidays which occur each year, but where the exact timing shifts (e.g. Easter)
	* Example (monthly power consumption)![](./_resources/EDA,_Time_Series.resources/1-ZKjg0i7fLRIWrys8phtCiw.jpeg)
		* median, the lower quartile, and the upper quartile of power consumption are lower during the spring and summer than autumn and winter
	* Example: Demand per Month and per Category![](./_resources/EDA,_Time_Series.resources/image.2.png)
* Variance of value by group
	* Example: how sales vary between store types over a year
	* important to standardize the value by group
		* `df %>% group_by(group) %>% mutate(sales = scale(sales))`
	* Which groups vary wildly and which are more stable
* rates by group
	* Example: sales($) per customer
		* `df %>% group_by(group, month) %>% mutate(sales_per_cust = sum(sales)/sum(customers)`
* stl decomposition
	* If we are interested in short- or long-term movements of the time series, we do not care about the various seasonalities. We want to know the trend component of the time series and if it has reached a turning point
	* is there strong seasonality or trend?
	* If there's a pattern in the random/remainder component, this could indicate that there are other strong influences present.
	* Daily Seasonal Adjustment (DSA)
		* Daily data can have multiple seasonalities present![](./_resources/EDA,_Time_Series.resources/1-D074QuDWqHGSttFx6tKgjg.jpeg)
		* Combines the seasonal trend decomposition procedure using Loess (STL) with a regression model with ARIMA errors
		* [{dsa}]{style='color: #990000'}, [example](https://towardsdatascience.com/seasonal-adjustment-of-daily-time-series-1bd2aa9b096d)
			* example shows it outperforming Hyndman's STR procedure
		* Procedure
			* STL adjusts intra-weekly periodic patterns.
			* RegARIMA estimates calendar effects, cross-seasonal effects, and outliers.
			* STL adjusts intra-monthly periodic effects.
			* STL adjusts intra-annual effects
* Does the series have an additive or multiplicative structure
	* Does the amplitude of the seasonal or cyclical component increase over time?![](./_resources/EDA,_Time_Series.resources/Screenshot (248).png)
		* The amplitude of the seasonal component increases over time so this series has a multiplicative structure
			* Also if there's a changing seasonal amplitude for different times of the year
	* \*\* If you have a multiplicative structure and zeros in your data (i.e. intermittent data), then they must handled in some way. \*\*
		* See [Logistics](Logistics) >> Demand Forecasting >> Intermittent Data
* lag (scatter) plot
	* cross-correlation
		* Steps
			1. standardize all variables
			2. look at faceted plots for outcome ts vs lags of a predictor ts
			3. chose promising predictors and lags
			4. run pearson correlation heatmap
		* Issues/questions
			* should differences be done before doing this?
			* are there better correlation metrics?
* acf, pcf plots
	* can also be done with heat maps which maybe more useful for presentation than eda
* trend strength and seasonal strength plots
* PCA (Dynamic Factor modeling takes into account the time dimension — see )
* subseries plots for multi-categorical series
	* number of trips (measurement), region, state, purpose (keys)



EDA

* Is data recorded at irregular intervals. If so:
	* [{{BINCOR}}]{style='color: goldenrod'}handles cross-correlation between 2 series with irregular intervals and series with regular but different intervals
	* I think common forecasting algorithms require regularly spaced data, so look towards ML, Multilevel Model for Change (MLMC), or Latent Growth Models
	* May also try binning points in the series (like BINCOR does) or smoothing them to get a regular interval
* **Seasonality**
	* facet scatterplots by month;  with x = year, y = value
	* tsfeatures has seasonality strength metric
* **Steps:**
	* Get a sense of whether the relationships are linear or nonlinear (should difference series first I think)
		* Lag scatterplots within the target series (i.e. yt vs yt+h)
		* Lag scatterplots between target series and lags of predictor series (i.e. yt vs xt+h)

```
astsa::lag1.plot(y, 12) # lags 1-12 of y
astsa::lag2.plot(y, x, 8) # y vs lags 0-8 of x
```

* autocorrelation/cross-correlation values in upper right corner
	* autocorrelations/cross-correlation values only valid if relationships are linear but maybe still useful in determining a positive or negative relationship
* loess smoothing line added
* see ccf section below for interpreting lag.2 plot (e.g. lags and leads)
* nonlinear patterns can indicate that behavior between the two variables is different for high values and low values

* Check ACF for all series
	* if relationships were linear, confirm strongest relationships seen in scatterplot with spikes in ACF
* Is the series a trend-stationary or unit root process?
	* test all series of interest with ADF and KPSS tests
* Difference or detrend (according to results of ADF and KPSS tests) all series of interest
	* should all series be required to have the same transform?
* Apply CCF to transformed series with linear and/or nonlinear correlation with function (interpretations of lag scatterplots to give a clue) to determine predictive lags.

* Is the time series **additive or multiplicative**, is the variation (mostly) constant or not constant over time?
* **Shannon Spectral Entropy**
	* `feasts::feat_spectral`  will compute the (Shannon) spectral entropy of a time series, which is a measure of how easy the series is to forecast.
	* A series which has strong trend and seasonality (and so is easy to forecast) will have entropy close to 0.
	* A series that is very noisy (and so is difficult to forecast) will have entropy close to 1.
* **ACF** (autocorrelation function)
	* For a stationary time series,
		* For stationary data, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly.
		* for non-stationary data, the value of r1 is often large and positive.
		* ![](./_resources/EDA,_Time_Series.resources/acfstationary-1.png)
			* The ACF of the differenced Google stock price (right fig) looks just like that of a white noise series. There are no autocorrelations lying outside the 95% limits, and the Ljung-Box  Q∗ statistic has a p-value of 0.355 (for  h = 10) which implies the ts is stationary. This suggests that the daily change in the Google stock price is essentially a random amount which is uncorrelated with that of previous days.
* **CCF ** (cross-correlation function)
	* The correlation between two stationary series at different lags
	* the set of sample correlations between xt+h and yt for _h_ = 0, ±1, ±2, ±3, and so on. A negative value for h is a correlation between the _x_\-variable at a time before t and the _y_\-variable at time t.
		* For instance, consider h = −2. The CCF value would give the correlation between xt−2 and yt.
			* i.e. The CCF would the correlation between lag(x, 2) and y
	* When calculating correlations between lags of a variable and the variable itself (ACF) or another variable (CCF) you can't, for example in a CCF, simply take corr(xt-k, yt) for the correlation of x at lag k and y and repeat for all the lags of x.  Since the corr formula/function requires the mean of the series, you would be using a different mean for xk for each calculation of the correlation of each pair. The assumption is that the series are (second-order?) stationary and therefore have a constant mean (i.e. each series has a single mean (and variance)), so the mean(s) of (each) original series should be used in the calculations of the correlations.
		* source: Modern Applied Statistics with S, Venables and Ripley
		* Compared one against the other in COVID-19 CFR project and they actually produce similar patterns but different values of the CCF. The corr method produce inflated CCF values.
	* we expect about 1.75 "false alarms" out of the 35 sample cross-correlations even after prewhitening (Cryer and Chan)
	* Terms
		* In a cross-correlation in which the direction of influence between two time-series is hypothesized or known,
			* the influential time-series is called the “input” time-series
			* the affected time-series is called the “output” time-series.
		* When one or more xt+h , with h _negative_, are predictors of yt, it is sometimes said that _x leads y._
			* i.e. When lagged values of x predict y, the lagged values of x "happen" _before_ the values of y, therefore, they lead (in front of) y.
		* When one or more xt+h, with h _positive_, are predictors of yt, it is sometimes said that _x lags y_.
			* i.e. When the leading values of x predict y, the leading values of x "happen" _after_ the values  of y, therefore, they lag (behind) y.
	* Avoid spurious correlations
		* Make sure there's a theoretical reason for the two series to be related
		* \*\*Autocorrelation of at least one series should be removed (transformed to white noise)
			* With autocorrelation present:
				* the variance of cross-correlation coefficient is high and therefore spurious correlations are likely
				* Significance calculations are no longer valid since the CCF distribution will not be normal and the  variance is no longer  1/n
		* spurious correlations will still be possible even with stationary series (even more so with non-stationary series)
	* Steps
		1. Test for stationarity
		2. find number of differences to make the series stationary
			* Not sure if each series should be differenced the same number of times
				* Why would you test a seasonal series and non-seasonal series for an association.
				* the requirement is stationarity, so maybe try using the highest difference needed between both series
					* In dynamic regression, Hyndman says difference all variables if one needs differencing, so proabably applicable here.
			* Some examples also used log transformation , but when I did, it produced nonsensical CCF values. (covid cfr project). So, beware.
		3. seasonal difference --> difference
		4. (optional) lag scatter plots if the differenced series and look for patterns to get an idea of the strength of the linear correlation
			* if there's a nonlinear pattern, might be difficult or might not to use a nonlinear or nonparametric correlation function. See 3rd bullet under CCF header above for discussion of stationarity assumption. May not be necessary with another correlation type.
		5. prewhiten both series (cryer, chan method)
		6. apply correlation function
	* Prewhitening
		* Example from COVID-19 CFR project

```
# numbers of differences
ind_cases_diff <- forecast::ndiffs(ind_cases_ts)
ind_deaths_diff <- forecast::ndiffs(ind_deaths_ts)
# seasonal
ind_cases_sdiff <- forecast::nsdiffs(ind_cases_ts)
ind_deaths_sdiff <- forecast::nsdiffs(ind_deaths_ts)
# no seasonal diffs needed
ind_cases_proc <- diff(ind_cases_ts, ind_cases_diff)
ind_deaths_proc <- diff(ind_deaths_ts, ind_deaths_diff)

# fit AR model with processed input series
ind_cases_ar <- ind_cases_proc %>%
  ts_tsibble() %>%
  model(AR(value ~ order(p = 1:30), ic = "aicc"))
# pull AR coefs
ind_ar <- coef(ind_cases_ar) %>% 
  filter(stringr::str_detect(term, "ar")) %>% 
  pull(estimate)
# linearly filter both input and output series using coefs
ind_cases_fil <- stats::filter(ind_cases_proc, filter = c(1, -ind_ar),
                              method = 'convolution', sides = 1)
ind_deaths_fil <- stats::filter(ind_deaths_proc, filter = c(1, -ind_ar),
                                method = 'convolution', sides = 1)
# spike at -20 with corr = 0.26; nonsensical lags at -4 and -15, -68
ggCcf(ind_cases_fil, ind_deaths_fil)
```


