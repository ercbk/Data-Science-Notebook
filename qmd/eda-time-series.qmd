# EDA, Time Series

TOC

Misc

-   Packages
    -   [{{]{style="color: goldenrod"}[diaquiri](https://ropensci.github.io/daiquiri/){style="color: goldenrod"}[}}]{style="color: goldenrod"}![](./_resources/EDA,_Time_Series.resources/image.1.png)![](./_resources/EDA,_Time_Series.resources/image.png)
        -   aggregated values are automatically created for each data field (column) depending on its contents (e.g. min/max/mean values for numeric data, no. of distinct values for categorical data)
        -   overviews for missing values, non-conformant values, and duplicated rows.
-   Basic Steps
    -   Check seasonality at all periods at and above data frequency

    -   Investigate spikes (i.e. special days or weeks) Check lags of outcome variable

        Check for NAs in outcome variable

        Are there a significant amount of zeros
-   Look for NAs
    -   NAs affect the number of lags to be calculated for a variable
        -   e.g. exports only recorded quarterly but stock price has a monthly close price you want to predict. So if you're forecasting monthly oil price then creating a lagged variable for exports is difficult
            -   Bizsci (lab 29), minimum_lag = length_of_sequence_of_tail_NAs +1 
                -   tail(ts) are the most recent values
    -   Even if you don't want a lag for a predictor var and it has NAs, you need to recipe::step_lag(var, lag = #\_of_tail_NAs). So var has no NAs.
    -   Consider seasonality of series when determining imputation method
-   Check the shape of the distribution of the outcome variable
    -   For low volume data, a right-skewed distribution might be needed instead of a Gaussian.
        -   Thin tails - Use a Gamma distribution
        -   Heavier tails - Use a Log Normal or the Inverse Gaussian
-   Are there a significant amount of zeros
    -   See notebook for tests on the number of zeros in Poisson section
    -   Also might be tests in the intermittent forecasting packages, so see bkmks
    -   If so, see [Logistics](Logistics) \>\> Demand Forecasting \>\> Intermittent Data for modeling approaches Timestamp Columns
    -   Are there gaps in the time series (e.g. missing a whole day/multiple days, days of shockingly low volume)?
-   Seasonality Tests (weekly, monthly, and yearly)
    -   [{seastests}]{style="color: #990000"} QS and Friedman (see bkmk in Time Series \>\> eda for example)
    -   QS test's null hypothesis is no positive autocorrelation in seasonal lags in the time series
    -   Friedman test's null hypothesis is no significant differences between the values' period-specific means present in the time series
    -   For QS and Friedman, pval \> 0.05 indicates NO seasonality present
-   Look at quantile values per frequency unit (by group and total)
    -   {timetk::plot_time_series_boxplot}
    -   average closing price for each month, each day of the month, each day of the week
    -   when are dips and peaks?
    -   Which groups are similar?
    -   What are the potential reasons behind these dips and peaks?
    -   Example (daily power consumption)![](./_resources/EDA,_Time_Series.resources/1-mDCO3WKk5gqfK-k-pf-p0g.jpeg)
        -   median, the lower quartile, and the upper quartile for Saturdays and Sundays are below the remaining weekdays when inspecting daily power consumption
        -   some outliers are present during the week, which could indicate lower power consumption due to moving holidays
            -   Moving holidays are holidays which occur each year, but where the exact timing shifts (e.g. Easter)
    -   Example (monthly power consumption)![](./_resources/EDA,_Time_Series.resources/1-ZKjg0i7fLRIWrys8phtCiw.jpeg)
        -   median, the lower quartile, and the upper quartile of power consumption are lower during the spring and summer than autumn and winter
    -   Example: Demand per Month and per Category![](./_resources/EDA,_Time_Series.resources/image.2.png)
-   Variance of value by group
    -   Example: how sales vary between store types over a year
    -   important to standardize the value by group
        -   `df %>% group_by(group) %>% mutate(sales = scale(sales))`
    -   Which groups vary wildly and which are more stable
-   rates by group
    -   Example: sales(\$) per customer
        -   `df %>% group_by(group, month) %>% mutate(sales_per_cust = sum(sales)/sum(customers)`
-   stl decomposition
    -   If we are interested in short- or long-term movements of the time series, we do not care about the various seasonalities. We want to know the trend component of the time series and if it has reached a turning point
    -   is there strong seasonality or trend?
    -   If there's a pattern in the random/remainder component, this could indicate that there are other strong influences present.
    -   Daily Seasonal Adjustment (DSA)
        -   Daily data can have multiple seasonalities present![](./_resources/EDA,_Time_Series.resources/1-D074QuDWqHGSttFx6tKgjg.jpeg)
        -   Combines the seasonal trend decomposition procedure using Loess (STL) with a regression model with ARIMA errors
        -   [{dsa}]{style="color: #990000"}, [example](https://towardsdatascience.com/seasonal-adjustment-of-daily-time-series-1bd2aa9b096d)
            -   example shows it outperforming Hyndman's STR procedure
        -   Procedure
            -   STL adjusts intra-weekly periodic patterns.
            -   RegARIMA estimates calendar effects, cross-seasonal effects, and outliers.
            -   STL adjusts intra-monthly periodic effects.
            -   STL adjusts intra-annual effects
-   Does the series have an additive or multiplicative structure
    -   Does the amplitude of the seasonal or cyclical component increase over time?![](./_resources/EDA,_Time_Series.resources/Screenshot%20(248).png)
        -   The amplitude of the seasonal component increases over time so this series has a multiplicative structure
            -   Also if there's a changing seasonal amplitude for different times of the year
    -   \*\* If you have a multiplicative structure and zeros in your data (i.e. intermittent data), then they must handled in some way. \*\*
        -   See [Logistics](Logistics) \>\> Demand Forecasting \>\> Intermittent Data
-   lag (scatter) plot
    -   cross-correlation
        -   Steps
            1.  standardize all variables
            2.  look at faceted plots for outcome ts vs lags of a predictor ts
            3.  chose promising predictors and lags
            4.  run pearson correlation heatmap
        -   Issues/questions
            -   should differences be done before doing this?
            -   are there better correlation metrics?
-   acf, pcf plots
    -   can also be done with heat maps which maybe more useful for presentation than eda
-   trend strength and seasonal strength plots
-   PCA (Dynamic Factor modeling takes into account the time dimension --- see )
-   subseries plots for multi-categorical series
    -   number of trips (measurement), region, state, purpose (keys)

EDA

-   Is data recorded at irregular intervals. If so:
    -   [{{BINCOR}}]{style="color: goldenrod"}handles cross-correlation between 2 series with irregular intervals and series with regular but different intervals
    -   I think common forecasting algorithms require regularly spaced data, so look towards ML, Multilevel Model for Change (MLMC), or Latent Growth Models
    -   May also try binning points in the series (like BINCOR does) or smoothing them to get a regular interval
-   **Seasonality**
    -   facet scatterplots by month;  with x = year, y = value
    -   tsfeatures has seasonality strength metric
-   **Steps:**
    -   Get a sense of whether the relationships are linear or nonlinear (should difference series first I think)
        -   Lag scatterplots within the target series (i.e. yt vs yt+h)
        -   Lag scatterplots between target series and lags of predictor series (i.e. yt vs xt+h)

```         
astsa::lag1.plot(y, 12) # lags 1-12 of y
astsa::lag2.plot(y, x, 8) # y vs lags 0-8 of x
```

-   autocorrelation/cross-correlation values in upper right corner

    -   autocorrelations/cross-correlation values only valid if relationships are linear but maybe still useful in determining a positive or negative relationship

-   loess smoothing line added

-   see ccf section below for interpreting lag.2 plot (e.g. lags and leads)

-   nonlinear patterns can indicate that behavior between the two variables is different for high values and low values

-   Check ACF for all series

    -   if relationships were linear, confirm strongest relationships seen in scatterplot with spikes in ACF

-   Is the series a trend-stationary or unit root process?

    -   test all series of interest with ADF and KPSS tests

-   Difference or detrend (according to results of ADF and KPSS tests) all series of interest

    -   should all series be required to have the same transform?

-   Apply CCF to transformed series with linear and/or nonlinear correlation with function (interpretations of lag scatterplots to give a clue) to determine predictive lags.

-   Is the time series **additive or multiplicative**, is the variation (mostly) constant or not constant over time?

-   **Shannon Spectral Entropy**

    -   `feasts::feat_spectral`  will compute the (Shannon) spectral entropy of a time series, which is a measure of how easy the series is to forecast.
    -   A series which has strong trend and seasonality (and so is easy to forecast) will have entropy close to 0.
    -   A series that is very noisy (and so is difficult to forecast) will have entropy close to 1.

-   **ACF** (autocorrelation function)

    -   For a stationary time series,
        -   For stationary data, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly.
        -   for non-stationary data, the value of r1 is often large and positive.
        -   ![](./_resources/EDA,_Time_Series.resources/acfstationary-1.png)
            -   The ACF of the differenced Google stock price (right fig) looks just like that of a white noise series. There are no autocorrelations lying outside the 95% limits, and the Ljung-Box  Q∗ statistic has a p-value of 0.355 (for  h = 10) which implies the ts is stationary. This suggests that the daily change in the Google stock price is essentially a random amount which is uncorrelated with that of previous days.
