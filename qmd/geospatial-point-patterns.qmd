# Point Patterns {#sec-geo-ptpat .unnumbered}

## Misc {#sec-geo-ptpat-misc .unnumbered}

-   Notes from
    -   [Ch.9 - Ch.18](https://paezha.github.io/spatial-analysis-r/point-pattern-analysis-i.html), An Introduction to Spatial Data Analysis and Statistics: A Course in R
    -   [Ch.11](https://r-spatial.org/book/11-PointPattern.html), Spatial Data Science With Applications in R
-   Packages
    -   [{]{style="color: #990000"}[spatstat](https://cran.r-project.org/web/packages/spatstat/index.html){style="color: #990000"}[}]{style="color: #990000"} - Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests
        -   CRAN page has some vignettes, but the manual is a kind of cheatsheet that lists names of functions that perform certain spatial operations
        -   Loading [{spatstat}]{style="color: #990000"} loads Spatstat-verse:
            -   [{]{style="color: #990000"}[spatstat.data](https://cran.r-project.org/web/packages/spatstat.data/index.html){style="color: #990000"}[}]{style="color: #990000"} - Datasets
            -   [{]{style="color: #990000"}[spatstat.univar](https://cran.r-project.org/web//packages//spatstat.univar/index.html){style="color: #990000"}[}]{style="color: #990000"} - Estimation of one-dimensional probability distributions including kernel density estimation, weighted empirical cumulative distribution functions, Kaplan-Meier and reduced-sample estimators for right-censored data, heat kernels, kernel properties, quantiles and integration.
            -   [{]{style="color: #990000"}[spatstat.geom](https://cran.r-project.org/web/packages/spatstat.geom/index.html){style="color: #990000"}[}]{style="color: #990000"} - Defines spatial data types and supports geometrical operations on them. Data types include point patterns, windows (domains), pixel images, line segment patterns, tessellations and hyperframes
            -   [{]{style="color: #990000"}[spatstat.random](https://cran.r-project.org/web/packages/spatstat.random/index.html){style="color: #990000"}[}]{style="color: #990000"} - Generates random spatial patterns of points
            -   [{]{style="color: #990000"}[spatstat.explore](https://cran.r-project.org/web/packages/spatstat.explore/index.html){style="color: #990000"}[}]{style="color: #990000"} - Exploratory data analysis and nonparametric analysis of spatial data, mainly spatial point patterns
            -   [{]{style="color: #990000"}[spatstat.model](https://cran.r-project.org/web/packages/spatstat.model/index.html){style="color: #990000"}[}]{style="color: #990000"} - Parametric statistical modelling and inference for spatial data, mainly spatial point patterns
            -   [{]{style="color: #990000"}[spatstat.linnet](https://cran.r-project.org/web/packages/spatstat.linnet/index.html){style="color: #990000"}[}]{style="color: #990000"} - Defines types of spatial data on a linear network and provides functionality for geometrical operations, data analysis and modelling of data on a linear network
-   Resources
    -   Spatial Point Patterns: Methodology and Applications with R (R \>\> Documents \>\> Geospatial)
    -   [An Introduction to Spatial Data Analysis and Statistics: A Course in R](https://paezha.github.io/spatial-analysis-r/)
-   Other stuff
    -   Marked Point Patterns - Points with labels
        -   Sort of a spatial factor analysis
        -   Correlation: Measures the correlation between marks of points separated by a given distance $r$.
            -   [Example]{style="color: green"}: A dataset of spruce tree locations that also has their heights. The spatial correlation will tell us if trees of similar heights near each other or further away.
            -   `spatstat.explore::markcorr`
                -   Under mark independence, the mark correlation function should be approximately constant (e.g., close to 1 for standardized marks).
                -   If $\text{markcorr}(r) \gt 1$ , it suggests *positive correlation* between marks at distance r (e.g., tall trees tend to be near other tall trees).
                -   If $\text{markcorr} \lt 1$, it suggests *negative correlation* between marks at distance r (e.g., tall trees tend to be near short trees).
            -   (`spatstat.explore::markvario`)
                -   A spatial variogram measures the variance between marks of points separated by a given distance $r$
                -   If the estimated variance is greater than the theoretical variance (a constant), $\hat \gamma (r) \gt \gamma (r)$, then marked points at that distance have greater variance and are more dissimilar to each other. (implies negative spatial correlation)
                -   If $\hat \gamma (r) \approx \gamma (r)$, then marked points are independent at that distance. (implies spatial independence)
                -   If $\hat \gamma (r) \lt \gamma (r)$, then marked points have smaller variance and are more similar. (implies positive spatial correlation)
        -   Interactions: Analyze the spatial interaction between *two* different types of points (e.g. one factor variable w/two levels)
            -   `spatstat.explore::Lcross`
                -   Requires a multitype point pattern (a marked point pattern whose marks are a factor).
                    -   e.g. Do trees of species A tend to be close to trees of species B? (species would be a factor variable with levels: A and B)
                -   $\hat L (r) \gt r$ : Clustering between category 1 and category 2 at distance $r$.
                -   $\hat L (r) \approx r$ : Independence between category 1 and category 2 at distance $r$.
                -   $\hat L (r) \lt r$ : Inhibition (repulsion) between category 1 and category 2 at distance $r$.
                -   If you want to see if the individual categories have random (i.e. poisson) patterns, use `Kall <- alltypes(rescale(amacrine), Lcross)`. In the matrix plot, the diagonal with have the components (i.e. category 1 vs category 1 and category 2 vs category 2).
            -   There is no `Lmulti` but there is a `Kmulti` which is the same as `Kcross` except instead of the variable needing to be a factor, it can be any form of dichotomization, e.g. diameter \>= 15 and diameter \< 15 or collapsing multiple categories into 2 categories.

## Terms {#sec-geo-ptpat-terms .unnumbered}

-   [**Intensity**]{style="color: #009499"} - The *expected* number of events per unit area — conventionally denoted by $\lambda$. In most cases the process is not know, so its intensity cannot be directly measured.
-   [**Density**]{style="color: #009499"} - Empirical estimate of **Intensity** — $\hat \lambda = n / a$ , where $a$ is the area of the region
-   [**Homogeneity**]{style="color: #009499"} - The points have no preference for any spatial location;
-   [**Inhomogeneous**]{style="color: #009499"} - The average density of points is spatially varying (i.e. density is a function of location)
-   [**Quadrats**]{style="color: #009499"} - Cells of a gridded area representing subregions. Useful for analyzing how density varies across a region
    -   Rules of Thumb for choosing the number of quadrats
        -   Each quadrat should have a minimum of two events
        -   Formula based on the area (A) and number of events (N)\
            $$
            Q = \frac{2A}{N}
            $$
-   [**Regularity**]{style="color: #009499"} (or [**Dispersion**]{style="color: #009499"}) - The state at which points tend to be located at similar distances from each other.
-   [**Stationary**]{style="color: #009499"} - Intensity is constant

## Basics {#sec-geo-ptpat-bas .unnumbered}

-   [Example 1]{.ribbon-highlight}: Quadrats\
    ![](_resources/Geospatial-Point-Patterns.resources/basics-geom-bin2d-1.png){.lightbox width="382"}

    ``` r
    pacman::p_load(
      dplyr,
      ggplot2,
      spatstat
    )
    data("PointPatterns", package = "isdas")
    summary(PointPatterns)
    ##        x                y                 Pattern  
    ##  Min.   :0.0169   Min.   :0.005306   Pattern 1:60  
    ##  1st Qu.:0.2731   1st Qu.:0.289020   Pattern 2:60  
    ##  Median :0.4854   Median :0.550000   Pattern 3:60  
    ##  Mean   :0.5074   Mean   :0.538733   Pattern 4:60  
    ##  3rd Qu.:0.7616   3rd Qu.:0.797850                 
    ##  Max.   :0.9990   Max.   :0.999808

    ggplot() +
      geom_bin2d(data = filter(PointPatterns, 
                               Pattern == "Pattern 1"),
                 aes(x = x, 
                     y = y),
                 binwidth = c(0.25, 
                              0.25)) +
      geom_point(data = filter(PointPatterns, 
                               Pattern == "Pattern 1"), 
                 aes(x = x, 
                     y = y)) +
      scale_fill_distiller(palette = "RdBu") +
      coord_fixed()
    ```

    -   `geom_bin2d` is called to plot a map of counts of events in the space defined by the bins.
    -   [PointPatterns]{.var-text} contains [x]{.var-text}, [y]{.var-text} coordinates that range from 0 to 1 and a categorical variable Pattern indicating each of the four difffernt density patterns

-   [Example 2]{.ribbon-highlight}: Create a ppp object

    ``` r
    # define a window
    wnd <- owin(c(0,1), c(0,1)) 
    ppp1 <- as.ppp(PointPatterns, wnd)
    summary(ppp1)
    ## Marked planar point pattern:  240 points
    ## Average intensity 240 points per square unit
    ## 
    ## Coordinates are given to 16 decimal places
    ## 
    ## Multitype:
    ##           frequency proportion intensity
    ## Pattern 1        60       0.25        60
    ## Pattern 2        60       0.25        60
    ## Pattern 3        60       0.25        60
    ## Pattern 4        60       0.25        60
    ## 
    ## Window: rectangle = [0, 1] x [0, 1] units
    ## Window area = 1 square unit

    # plot a specific category of point
    plot(split.ppp(ppp1)$`Pattern 3`)
    ```

    -   The window defined in `owin` should define a region for analysis that is consistent with the pattern of interest
    -   `ppp` (plannar point pattern) is the fundamental spatstat object
    -   [frequency]{.arg-text} is the number of points in that region (e.g. [Pattern]{.var-text})
    -   [proportion]{.arg-text} is the proportion of points in that region to the overall dataset
    -   [intensity]{.arg-text} it the number of points divided by the area (1 x 1 = 1)

-   [Example 3]{.ribbon-highlight}: Get point counts for each quadrat by region/subregion

    ``` r
    quadratcount(split(ppp1),
                 nx = 4,
                 ny = 4)

    ## List of spatial objects
    ## 
    ## Pattern 1:
    ##             x
    ## y            [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1]
    ##   [0.75,1]          3          5          1        6
    ##   [0.5,0.75)        2          3          4        6
    ##   [0.25,0.5)        5          4          2        3
    ##   [0,0.25)          2          4          4        6
    ## 
    ## Pattern 2:
    ##             x
    ## y            [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1]
    ##   [0.75,1]         14          2          2        6
    ##   [0.5,0.75)        0          0          4        6
    ##   [0.25,0.5)        6          3          1        2
    ##   [0,0.25)          4          6          2        2
    ## 
    ## Pattern 3:
    ##             x
    ## y            [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1]
    ##   [0.75,1]          2         11          5        7
    ##   [0.5,0.75)        1          1          6        4
    ##   [0.25,0.5)        1         10          3        2
    ##   [0,0.25)          2          1          2        2
    ## 
    ## Pattern 4:
    ##             x
    ## y            [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1]
    ##   [0.75,1]          4          5          6        3
    ##   [0.5,0.75)        3          3          4        2
    ##   [0.25,0.5)        3          3          4        2
    ##   [0,0.25)          5          4          6        3
    ```

    -   [nx]{.arg-text} and [ny]{.arg-text} specify how many quadrats (i.e. cells) you want per row and per column respectively
    -   `split` divides the dataset by the region variable or event type

-   [Example 4]{.ribbon-highlight}: Quadrat Count for Toronto Fast Food\
    ![](_resources/Geospatial-Point-Patterns.resources/ff-qct-1.png){.lightbox width="332"}

    ``` r
    library(spatstat)
    library(sf)

    data("Fast_Food", package = "isdas")
    data("Toronto", package = "isdas")

    head(Fast_Food)
    #> Simple feature collection with 6 features and 1 field
    #> Geometry type: POINT
    #> Dimension:     XY
    #> Bounding box:  xmin: 620173 ymin: 4840698 xmax: 638544.7 ymax: 4853394
    #> Projected CRS: NAD83 / UTM zone 17N
    #>     Class                 geometry
    #> 1 Chicken POINT (635575.8 4853394)
    #> 2 Chicken POINT (636724.5 4842644)
    #> 3 Chicken POINT (622524.7 4840698)
    #> 4 Chicken POINT (638544.7 4846541)
    #> 5 Chicken POINT (627850.5 4843178)
    #> 6 Chicken   POINT (620173 4841782)
    head(toronto)
    #> Simple feature collection with 1 feature and 0 fields
    #> Geometry type: MULTIPOLYGON
    #> Dimension:     XY
    #> Bounding box:  xmin: 609550.5 ymin: 4826375 xmax: 651611.8 ymax: 4857439
    #> Projected CRS: NAD83 / UTM zone 17N
    #>                         geometry
    #> 1 MULTIPOLYGON (((609550.5 48...

    ppp_ff <- as.ppp(Fast_Food, as.owin(Toronto)) # <1>
    #> Marked planar point pattern: 614 points
    #> Multitype, with levels = Chicken, Hamburger, Pizza, Sub  # <2>
    #> window: polygonal boundary
    #> enclosing rectangle: [609550.5, 651611.8] x [4826375, 4857439] units

    qct_ff <- quadratcount(ppp_ff, nx = 3, ny = 3) # <3>
    table(qct_ff) # <4>
    #>   0   6  44  48  60  64  85 144 163 
    #>   1   1   1   1   1   1   1   1   1 

    plot(qct_ff)
    ```

    1.  To automatically create a window object using the boundaries from a sf object, [{sf}]{style="color: #990000"} needs to be loaded.
    2.  The categories in the [Class]{.var-text} variable are captured as levels
    3.  3 x 3 seems to be a good starting grid for regions such as a city if your data isn't too sparse
    4.  There is 1 quadrat with 0 points. Given that were using a 3 x3 grid, it's very small and probably located underneath the quadrat with 44 points.

-   [Example 5]{.ribbon-highlight}: Approximate a window using coordinates

    ``` r
    data(bear_df, package = "isdas")
    summary(bear_df)
    ##        x                y                  marks    
    ##  Min.   :515743   Min.   :6812138   Day Time  :502  
    ##  1st Qu.:518994   1st Qu.:6813396   Night Time:498  
    ##  Median :519526   Median :6816724                   
    ##  Mean   :519321   Mean   :6816474                   
    ##  3rd Qu.:519982   3rd Qu.:6818111                   
    ##  Max.   :522999   Max.   :6821440

    W <- 
      owin(xrange = c(515000, 523500), 
           yrange = c(6812000, 6822000))
    bear.ppp <- as.ppp(bear_df, W = W)

    summary(bear.ppp)
    ## Marked planar point pattern:  1000 points
    ## Average intensity 1.176471e-05 points per square unit
    ## 
    ## Coordinates are given to 10 decimal places
    ## 
    ## Multitype:
    ##            frequency proportion    intensity
    ## Day Time         502      0.502 5.905882e-06
    ## Night Time       498      0.498 5.858824e-06
    ## 
    ## Window: rectangle = [515000, 523500] x [6812000, 6822000] units
    ##                     (8500 x 10000 units)
    ## Window area = 8.5e+07 square units
    ```

    -   Uses the minimum and maximum values of each coordinate

## Kernel Density {#sec-geo-ptpat-kdens .unnumbered}

-   Kernel density is a smooth estimate of the underlying intensity of the process, and the degree of smoothing is controlled by the bandwidth

    -   A map of the kernel density is better able to capture the variations in density across the region.

-   Process

    -   Each quadrat is treated as independent of the others in the window.
    -   There isn't a grid of quadrats but in essence, one that slides around the study area.
    -   it gives greater weight to events that are close to the center of the window, and less weight to events that are more distant from the center of the window
    -   The kernel function visits each point on a fine grid and obtains an estimate of the density by summing the weights of all events.
    -   The shape of the Gaussian kernel depends on the standard deviation, which controls how “big” the window is, or alternatively, how quickly the function decays via decreasing weights. We will call the standard deviation the kernel bandwidth of the function.

-   [{spatstat.explore::density.ppp}]{style="color: #990000"} - Kernel smoothed intensity function from a point pattern

    -   [kernel]{.arg-text}: "gaussian", "epanechnikov", "quartic" or "disc"
    -   [weights]{.arg-text}: Optional weights to be attached to the points
    -   [diggle]{.arg-text}: Logical. If TRUE, use the Jones-Diggle improved edge correction, which is more accurate but slower to compute than the default correction.

-   [Example]{.ribbon-highlight}: Fast Food in Toronto\
    ![](_resources/Geospatial-Point-Patterns.resources/dens-ff-1.png){.lightbox width="632"}

    ``` r
    library(spatstat)
    library(sf)

    # sf dfs
    data("Fast_Food", package = "isdas")
    data("Toronto", package = "isdas")

    # create a ppp obj
    ppp_ff <- as.ppp(Fast_Food, as.owin(Toronto))
    # calculate densities for each type of fast food
    kernel_density <- density(split(ppp_ff), sigma = bw.diggle)

    par(mfrow = c(2, 2), mar = c(0, 0, 1.1, 2))
    purrr::pwalk(
      list(
        kernel_density,
        split.ppp(ppp_ff), # add pts
        names(kernel_density)
      ),
      \(x1, x2, x3) {
        plot(x1, main = x3)
        plot(x2, add = TRUE)
      }
    )
    ```

    -   `bw.diggle` calculates the bandwidth using cross-validation. It's part of one of the group of sub-packages, [{spatstat.explore}]{style="color: #990000"}, that automatically gets loaded. There do seem to be other options, but I chose this based on the example in the Pebesma-Bivand [book](https://r-spatial.org/book/11-PointPattern.html).
    -   If you just want to plot the densities without the points, it's `plot(kernel_density)`
    -   See `spatstat.geom::plot.im` for beaucoup styling options for the density plot. I didn't add it here, but when there aren't any other labels, I kind of liked the [addcontour = TRUE]{.arg-text} as an extra density cue especially for the darker colors.
    -   Note that density color scales have different ranges in the legends

## Tests {#sec-geo-ptpat-tests .unnumbered}

-   [Misc]{.underline}

    -   tl;dr:
        -   Run the J Function if you want to analyze the overall pattern of the study area
        -   Run the L Function if you want to analyze patterns at all scales of the study area
            -   Think... the point pattern of 1st order neighbors area, the pattern of 1st and 2nd order neighbors area, etc.
            -   Use [correction = "best"]{.arg-text}
        -   Use a Simulation Envelope for the functions to include uncertainty in the analysis
    -   Assumptions
        -   An appropriate window selection is used
            -   If at all possible, the region should be selected in such a way that it is consistent with the underlying process. This is not always possible, either because the underlying process is not known, or because of limitations in data collection capabilities.
        -   All events within the window have been identified
            -   No solutions were given for *sampled* events, so the technique is discouraged.
    -   Edge Effects
        -   When a window is chosen, if events lie outside the window, this can bias the test results. An event at the edge of window may not be close to another event inside the window but may be close to one just outside the window.
        -   [Example]{style="color: green"}: Analysis of pizza restaurants in Toronto may be limited to the city limits. This does not mean that establishments do not exist beyond those boundaries. When the extent of the process exceeds the window used in the analysis, the point pattern is observed only partially, and it is possible that the omitted information regarding the location of events beyond the boundary may introduce some bias.
        -   A bias correction is recommended (especially for F and G but not for J. See K-Function for its correction methods) by using weighting events via [correction]{.arg-text}
            -   ["rs"]{.arg-text} (reduced-sample): Restricts the analysis to points or locations that are far enough from the boundary so that the entire region of interest (e.g., a circle of radius r) lies within the study area (Available for G and F)
                -   You're essentially removing data at the edges, so there will be a reduction in power
            -   ["km"]{.arg-text} (Kaplan-Mier): Treats the boundary as a "censoring" mechanism, where distances beyond the boundary are treated as censored observations. (Available for G and F)
            -   ["Hanisch"]{.arg-text}: Weights observations based on their proximity to the circle boundary drawn for each value of $r$. Points near the boundary receive lower weights because their neighborhoods are more likely to be truncated. (Available for G)
            -   ["cs"]{.arg-text}(Chiu-Stoyan): Same idea as the Hanisch method but taylored for the F-Function.
            -   ["best"]{.arg-text}: Chooses the best option based on the data and window geometry
            -   ["all"]{.arg-text} : Performs all the corrections, `plot` will plot each corrected empirical curve so you see their effects

-   [Quadrat-based Chi-Square Test]{.underline}

    -   A Pearson $\chi^2$ independence test that compares the empirical distribution of events by quadrats to the distribution of events as expected under the hypothesis that the underlying process is random.\
        $$
            \begin{align}
            &\chi^2 = \sum_i^Q r_i^2\\
            &\text{where} \;\; r_i = \frac{\text{observed}_i - \text{expected}_i}{\sqrt{\text{expected}_i}}
            \end{align}
            $$

    -   $r_i$ is the Pearson residual and $Q$ is the number of quadrats

    -   Issues

        -   Test results is affected by the chosen quadrat grid.
        -   Count-based so size of the quadrat matters. With irregular shaped quadrats (e.g. within a city boundary), it might be difficult to create a grid with roughly homogeneous counts.
        -   The test is *not* sensitive to the relative position of the events within the quadrats. So, there could be extreme clustering happening within the quadrats and the test might not reject the Null.

    -   [Example]{.ribbon-highlight}: `quadrat.test` using a ppp object\
        ![](_resources/Geospatial-Point-Patterns.resources/quadtest-fastfood-ex-1.png){.lightbox width="432"}

        ``` r
        q_test <- 
          quadrat.test(ppp_ff, 
                       nx = 3, 
                       ny = 3)
        q_test
        ## Warning: Some expected counts are small; chi^2 approximation may be inaccurate
        ##  Chi-squared test of CSR using quadrat counts
        ## 
        ## data:  Fast_Food.ppp
        ## X2 = 213.74, df = 8, p-value < 2.2e-16
        ## alternative hypothesis: two.sided
        ## 
        ## Quadrats: 9 tiles (irregular windows)
        ```

        -   All expected counts (assumes a uniform Poisson point process) should be greater than 5 which is why there's a warning.
            -   The docs of this function sound very much like the stuff in my Discrete Analysis notebook, so see that for further details.
            -   Options seem to be to use [method = "MonteCarlo"]{.arg-text} which relaxes the expected count \> 5 condition or using a smaller grid. My DA notebook also suggests Exact Tests if that's available for this sort of thing.
        -   p-value \< 0.05 suggests that this is *not* a CSR (completely spatially random — aka uniform Poisson point process) pattern
            -   If [lambda]{.arg-text} is provided then the Null is a Poisson point process with that $\lambda$ (i.e. intensity or probably also Poisson mean)
        -   Can also use `split.ppp(ppp_ff)` and have each point category tested.
        -   This function has methods for point patterns (class "ppp"), split point patterns (class "splitppp"), point process models (class "ppm" or "slrm"\`) and quadrat count tables (class "quadratcount").
        -   Plotting the object will display the quadrats, annotated by their observed and expected counts and the Pearson residuals.
        -   Pearson residuals can be extracted with `residuals`. They evidently aren't standardized, but if standardized, they can be treated as z-scores with values \> 1.96 indicating which quadrats are causing the rejection of the Null. (See [Regression, Diagnostics \>\> Residuals](diagnostics-regression.qmd#sec-diag-reg-res){style="color: green"} \>\> Standardized Residuals)

-   [G-Function]{.underline}

    -   A cumulative distribution function of distances that tells you the proportion of events that have a nearest neighbor at a distance less than some value. (i.e. ecdf)

        -   e.g. At a distance of 1.5 km, 32% of events/locations have a nearest neighbor at that distance or less.

    -   Edge correction needs to be applied via [correction]{.arg-text} if being used by itself (i.e. not as part of the J-Function)

    -   Empirical G-Function\
        $$
        \hat G(r) = \frac{\mathbb{I}(d_{i} \le r), \forall i}{n}
        $$

        -   Where $d_{i}$ is the distance from event/location, $i$, to its nearest neighbor and $r$ is a distance value

    -   Theoretical G-Function\
        $$
        G(r) = 1 - e^{-\lambda \pi r^2}
        $$

        -   Represents the Null point generating process from a Poisson distribution

    -   Guidelines

        -   $\hat G(r) \gt G(r)$ says events are *closer* together than expected from a random process, i.e. *Clustered*
        -   $\hat G(r) \approx G(r)$ says event/location pattern resembles a *random* process
        -   $\hat G(r) \lt G(r)$ says events are *further away* from each other than expected from a random process, i.e. *Dispersed* or *Regular*

    -   [Example]{.ribbon-highlight}: [{spatstat.explore::Gest}]{style="color: #990000"}\
        ![](_resources/Geospatial-Point-Patterns.resources/tests-gfunc-ex-1.png){.lightbox width="382"}

        ``` r
        # Use split to calculate the G-function only for "Pattern 1"
        g_pattern1 <- 
          Gest(split(pp0.ppp)$"Pattern 1", 
               correction = "none")

        plot(g_pattern1)
        lines(x = c(0.04, 0.04), 
              y = c(-0.1, 0.5), 
              lty = "dotted")
        lines(x = c(-0.1, 0.04), 
              y = c(0.5, 0.5), 
              lty = "dotted")
        lines(x = c(-0.1, 0.04), 
              y = c(0.16, 0.16), 
              lty = "dotted", 
              col = "red")
        ```

        -   See [Basics](geospatial-point-patterns.qmd#sec-geo-ptpat-bas){style="color: green"} \>\> Example 1 for what [Pattern 1]{.var-text} looks like
        -   $\hat G(r) \gt G(r)$ indicates clustering
        -   The line shows about 50% of events have a nearest neighbor at a distance of less than approximately 0.04
        -   [correction]{.arg-text}: Optional. The edge correction(s) to be used to estimate $G(r)$. A vector of character strings selected from ["none"]{.arg-text}, ["rs"]{.arg-text}, ["km"]{.arg-text}, ["Hanisch"]{.arg-text} and ["best"]{.arg-text}. Alternatively [correction = "all"]{.arg-text} selects all options.

-   [F-Function]{.underline}

    -   Same as the G-Functions except the distance isn't from event-to-event but from point-to-event.

    -   The point is an arbitrary location on a map and not necessarily a location of an event

    -   Same empirical and theoretical formulas — again, only the $d_i$ measurement is different.

    -   Edge correction needs to be applied via [correction]{.arg-text} if being used by itself (i.e. not as part of the J-Function)

    -   Guidelines (Opposite of $G$)

        -   $\hat F(r) \gt F(r)$ says *empty spaces are closer to events* than expected from a random process, i.e. *regular* or *dispersed*
        -   $\hat F(r) \approx F(r)$ says event/location pattern resembles a *random* process
        -   $\hat F(r) \lt F(r)$ says *empty spaces are further from events* than expected from a random process, i.e. *clustered*

    -   [Example]{.ribbon-hightlight}: [{spatstat.explore::Fest}]{style="color: #990000"}

        ::: {layout-ncol="2"}
        ![](_resources/Geospatial-Point-Patterns.resources/test-ffunc-ex-1.png){.lightbox width="282"}

        ![](_resources/Geospatial-Point-Patterns.resources/test-ffunc-ex-2.png){.lightbox width="510"}
        :::

        ``` r
        data("pp2_df", package = "isdas")
        W <- owin(c(0, 1), c(0, 1))
        pp2.ppp <- as.ppp(pp2_df, W = W)
        plot(pp2.ppp)

        f_pattern2 <- Fest(pp2.ppp, correction = "none")
        plot(f_pattern2)
        lines(x = c(0, 0.097), 
              y = c(0.4, 0.4), 
              col = "blue", 
              lty = "dotted")
        lines(x = c(0.045, 0.045), 
              y = c(0.0, 0.4), 
              col = "blue", 
              lty = "dotted")
        lines(x = c(0.097, 0.097), 
              y = c(0.0, 0.4), 
              col = "blue", 
              lty = "dotted")
        ```

        -   $\hat F \lt F$ indicates clustering
        -   Under the theoretical function 40% of points have a nearest event that is at a distance of approximately 0.045 or less, under the empirical function, the events are generally more distant from the points

-   [J-Function]{.underline}

    -   A ratio of the G and F functions

    -   The uncorrected estimate of $J$ is approximately unbiased (if the process is close to Poisson); it is insensitive to edge effects, and should be used when edge effects are severe.

    -   Formula

        $$
        J(r) = \frac{1-G(r)}{1-F(r)}
        $$

    -   [{spatstat.explore::Jest}]{style="color: #990000"}

        -   Values for G and F are also returned by the function

    -   Guidelines

        -   $J(r) \gt 1$ indiates *dispersion* or *regularity*
        -   $J(r) \approx 1$ indicates a random process
        -   $J(r) \lt 1$ indicates *clustering*

-   [K-Function]{.underline}

    -   AKA Ripley's K-Function

    -   Instead of only using distances to first order neighbors, it takes into account multiple orders of neighbors.

    -   Different types of patterns can be present at different scales ([pp3.ppp]{.var-text} plot)\
        ![](_resources/Geospatial-Point-Patterns.resources/test-kfunc-1.png){.lightbox width="232"}

        -   Overall there's clustering but the clusters are regularly spaced.
        -   At the cluster-level, events look to possibly have a random pattern.
        -   F and G will indicate clustering but not recognize the regular spacing of the clusters

    -   Process: At each event, neighbors are counted at some radius, r. Then r is increased and neighbors are counted again. This continues until all events have been counted as a neighbor. (See [Geospatial, Spatial Weights \>\> Diagnostics \>\> Connectedness](geospatial-spatial-weights.qmd#sec-geo-swgt-diag){style="color: green"} for details on higher order neighbors)

    -   Empirical Formula\
        $$
        \hat K(r) = \frac{1}{\hat \lambda (n-1)} \sum_i \sum_{j \ne i} \mathbb{I}(d_{ij} \le r)e_{ij}
        $$

        -   $\hat \lambda$ is the estimated intensity (i.e. density)
        -   The rest is summing all instances where the distances ($d_{ij}$) from event, $i$, to all the other events, $j$ is less than $r$.
        -   $e_{ij}$ is the edge correction method

    -   Theoretical Formula\
        $$
        K(r) = \pi r^2
        $$

    -   Guidelines

        -   $\hat K(r) \gt K(r)$ says events are *closer* together than expected from a random process, i.e. *Clustered*
        -   $\hat K(r) \approx K(r)$ says event/location pattern resembles a *random* process
        -   $\hat K(r) \lt K(r)$ says events are *further away* from each other than expected from a random process, i.e. *Dispersed* or *Regular*

    -   Edge [correction]{.arg-text} methods should be applied

        -   ["border"]{.arg-text} (aka “reduced sample” estimator): The least efficient (statistically) and the fastest to compute. It can be computed for a window of arbitrary shape.
            -   See Misc \>\> Edge Effects \>\> "rs" (reduced-sample)
        -   ["border.motif"]{.arg-text}: A weighted version of the ["border"]{.arg-text} method
            -   Similar to Misc \>\> Edge Effects \>\> "hanisch"
        -   ["isotropic"]{.arg-text}/["Ripley"]{.arg-text}: The weight is the circumference of the circle specified by $r$ divided by the circle's proportion of the whole window (without overlapping the boundary)\
            $$
            e(u,r) = \frac{2 \pi r}{\text{length}(c(u,r) \cap W)}
            $$
            -   "The denominator is the length of the overlap between this circle and the window $W$."
                -   $u$ is the point and $c(u,r$) is the "circle." It says "length" so maybe it's the arclength of circle that intersects the boundary.
            -   Implemented for rectangular and polygonal windows (not for binary masks).
            -   See [{spatstat.explore::edge.Ripley}]{style="color: #990000"}
        -   ["translate"]{.arg-text}/["translation"]{.arg-text}: Implemented for all window geometries, but slow for complex windows.\
            $$
            e(u,r) = \frac{\text{area}(W)}{\text{area}(W \cap (W + y - x))}
            $$
            -   $W + y − x$ is the result of shifting the window $W$ by the vector $y − x$. The denominator is the area of the overlap between this shifted window and the original window.
            -   See [{spatstat.explore::edge.Trans}]{style="color: #990000"}
        -   ["rigid"]{.arg-text}: There's no documentation for this method that I can find. Looking at the code, it looks similar to ["translate"]{.arg-text} since its seems to have a shifting window feature.
            -   Implemented for all window geometries, but slow for complex windows.
        -   ["periodic"]{.arg-text}: (aka toroidal) is only defined for rectangular windows.
        -   ["none"]{.arg-text}: This uncorrected estimate is biased and should not be used for data analysis, *unless* you have an extremely large point pattern (more than 100,000 points).
        -   ["best"]{.arg-text}: Selects Ripley's ["isotropic"]{.arg-text} correction for a rectangular or polygonal window, and the ["translation"]{.arg-text} correction for masks.
        -   ["good"]{.arg-text}: Selects the best edge correction that can be computed in a reasonable time. This is the same as ["best"]{.arg-text} for datasets with fewer than 3000 points; otherwise the selected edge correction is ["border"]{.arg-text}, unless there are more than 100,000 points, when it is ["none"]{.arg-text}.

    -   [Example]{.ribbon-highlight}: [{spatstat.explore::Kest}]{style="color: #990000"}\
        ![](_resources/Geospatial-Point-Patterns.resources/test-kfunc-ex-1.png){.lightbox width="432"}

        ``` r
        data("pp3_df", package = "isdas")
        W <- owin(c(0, 1), c(0, 1))
        pp3.ppp <- as.ppp(pp3_df, W = W)

        k_pattern3 <- Kest(pp3.ppp, correction = "none")
        plot(k_pattern3)
        ```

        -   Plot of [pp3.ppp]{.var-text} shown earlier
        -   $\hat K(r) \gt K(r)$ at a smaller scale which indicates clustering,
        -   But also, $\hat K(r) \lt K(r)$ at a larger scale which indicates regularity.

-   [L-Function]{.underline}

    -   Transformation for the K-Function

    -   K-Function Issues

        -   Non-Linearity: For a Poisson process (complete spatial randomness), $K=\pi r^2$, which is a quadratic function. This makes it harder to visually assess deviations from CSR.
        -   Scale Dependency: The K-function grows with r, making it difficult to compare patterns at different scales.

    -   L-Function Advantages

        -   Linear: Easier to visually assess
        -   Scale Independent: Can compare point patterns at different scales

    -   Guidelines

        -   $L(r) \gt r$ : The point pattern exhibits *clustering* at distance $r$.
        -   $L(r) \approx r$ : The point pattern is *random* at distance$r$.
        -   $L(r) \lt r$ : The point pattern exhibits *regularity* at distance $r$.

    -   [Example]{.ribbon-highlight}: [{spatstat.explore::Lest}]{style="color: #990000"}

        ::: {layout-ncol="2"}
        ![](_resources/Geospatial-Point-Patterns.resources/tests-lfunc-ex1-1.png){.lightbox width="232"}

        ![](_resources/Geospatial-Point-Patterns.resources/tests-lfunc-ex1-2.png){.lightbox width="350"}
        :::

        ``` r
        plot(cells)
        L <- Lest(cells)
        plot(L)
        ```

        -   Basic example from the docs
        -   At small to intermediate distance, there is regularity, but at further distances, the pattern is random.
            -   i.e. The distance between points is roughly the same, but the overall pattern is random
        -   Border (Reduced Sample), Translation, and Isotropic edge corrections are shown

-   [Simulation Envelopes]{.underline}

    -   Simulation makes it possible to calculate the variance of the expected value under the null hypothesis, i.e. an uncertainty measurement. While it doesn't give you a p-value, you can obtain a acceptance (of the Null) region.

    -   Process: Plug the average intensity of the dataset, $\hat \lambda$, (from the [ppp]{.arg-text} object) into random number generating function (`rpoispp` which is something like `rpois`) to get the *Null Landscape*. Then, use those generated values as inputs to `Gest`, `Fest`, or `Kest` to get a curve at multiple $r$ values. Repeat many times to get your Null distribution. If your empirical $\hat G$, $\hat F$, or $\hat K$ function falls within the ribbon of simulated Null values (aka **Simulation Envelope**), then the pattern is random.

    -   [Example]{.ribbon-highlight}: [{spatstat.explore::envelope}]{style="color: #990000"}\
        ![](_resources/Geospatial-Point-Patterns.resources/test-env-ex1-1.png){.lightbox width="482"}

        ``` r
        env_pp3 <- 
          envelope(Y = pp3.ppp, 
                   fun = Kest, 
                   nsim = 99, # default
                   funargs = list(correction = "none"))
        ```

        -   See previous example for details on the [pp3.ppp]{.var-text} pattern
        -   $\hat K$ only crosses the envelope to indicate a different pattern at a larger scale. Otherwise, it's completely outside the envelope, so the Null is reliably rejected.
        -   Given how uncentered this envelope is on the theoretical value, it would probably be a good idea to do more than 99 simulations.
