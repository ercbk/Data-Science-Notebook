# Point Patterns {#sec-geo-ptpat .unnumbered}

## Misc {#sec-geo-ptpat-misc .unnumbered}

-   Notes from
    -   [Ch.9 - Ch.18](https://paezha.github.io/spatial-analysis-r/point-pattern-analysis-i.html), An Introduction to Spatial Data Analysis and Statistics: A Course in R
    -   [Ch.11](https://r-spatial.org/book/11-PointPattern.html), Spatial Data Science With Applications in R
    -   [{spatstat}]{style="color: #990000"} book
-   Packages
    -   [{]{style="color: #990000"}[spatstat](https://cran.r-project.org/web/packages/spatstat/index.html){style="color: #990000"}[}]{style="color: #990000"} - Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests
        -   CRAN page has some vignettes, but the manual is a kind of cheatsheet that lists names of functions that perform certain spatial operations
        -   Loading [{spatstat}]{style="color: #990000"} loads Spatstat-verse:
            -   [{]{style="color: #990000"}[spatstat.data](https://cran.r-project.org/web/packages/spatstat.data/index.html){style="color: #990000"}[}]{style="color: #990000"} - Datasets
            -   [{]{style="color: #990000"}[spatstat.univar](https://cran.r-project.org/web//packages//spatstat.univar/index.html){style="color: #990000"}[}]{style="color: #990000"} - Estimation of one-dimensional probability distributions including kernel density estimation, weighted empirical cumulative distribution functions, Kaplan-Meier and reduced-sample estimators for right-censored data, heat kernels, kernel properties, quantiles and integration.
            -   [{]{style="color: #990000"}[spatstat.geom](https://cran.r-project.org/web/packages/spatstat.geom/index.html){style="color: #990000"}[}]{style="color: #990000"} - Defines spatial data types and supports geometrical operations on them. Data types include point patterns, windows (domains), pixel images, line segment patterns, tessellations and hyperframes
            -   [{]{style="color: #990000"}[spatstat.random](https://cran.r-project.org/web/packages/spatstat.random/index.html){style="color: #990000"}[}]{style="color: #990000"} - Generates random spatial patterns of points
            -   [{]{style="color: #990000"}[spatstat.explore](https://cran.r-project.org/web/packages/spatstat.explore/index.html){style="color: #990000"}[}]{style="color: #990000"} - Exploratory data analysis and nonparametric analysis of spatial data, mainly spatial point patterns
            -   [{]{style="color: #990000"}[spatstat.model](https://cran.r-project.org/web/packages/spatstat.model/index.html){style="color: #990000"}[}]{style="color: #990000"} - Parametric statistical modelling and inference for spatial data, mainly spatial point patterns
            -   [{]{style="color: #990000"}[spatstat.linnet](https://cran.r-project.org/web/packages/spatstat.linnet/index.html){style="color: #990000"}[}]{style="color: #990000"} - Defines types of spatial data on a linear network and provides functionality for geometrical operations, data analysis and modelling of data on a linear network
-   Resources
    -   [{spatstat}]{style="color: #990000"} book: Spatial Point Patterns: Methodology and Applications with R (R \>\> Documents \>\> Geospatial)
    -   [An Introduction to Spatial Data Analysis and Statistics: A Course in R](https://paezha.github.io/spatial-analysis-r/)

## Terms {#sec-geo-ptpat-terms .unnumbered}

-   [**Correlation-Stationary**]{style="color: #009499"} - The pair correlation between points u and v depends only on their relative position. ([{spatstat}]{style="color: #990000"} book, p. 243)
    -   This would be true if the process is stationary, but is also true for an inhomogeneous Poisson process and for many other processes.
-   [**Intensity**]{style="color: #009499"} - The *expected* number of events per unit area — conventionally denoted by $\lambda$. In most cases the process is not know, so its intensity cannot be directly measured. ([{spatstat}]{style="color: #990000"} book, Ch.6 p. 157)
    -   [Example 1]{.ribbon-highlight}: Average Intensity

        ``` r
        X <- rescale(swedishpines)
        lam <- intensity(X)
        # standard error
        sdX <- sqrt(lam/area(Window(X))))
        #> 0.08777
        ```

        -   `rescale` is used to convert the coordinates to a standard unit of length, e.g. unit like 0.1 meters to the base unit meters.
        -   The standard error is in points per square meter in this case
            -   Assumes homogeneous intensity

    -   [Example 2]{.ribbon-highlight}: Per Quadrat

        ``` r
        swp <- rescale(swedishpines)
        Q3 <- quadratcount(swp, nx = 3, ny = 3)
        intensity(Q3)
        #>              x
        #> y             [0,3.2) [3.2,6.4) [6.4,9.6]
        #>   [6.67,10]   0.75000   0.56250   0.65625
        #>   [3.33,6.67) 0.75000   1.03125   0.84375
        #>   [0,3.33)    0.46875   0.56250   1.03125

        L3 <- as.numeric(intensity(Q3))
        # standard error of the mean
        sem <- sqrt(var(L3)/(length(L3)-1))
        #> 0.07118

        # Using tesselized quadrats
        H <- hextess(swp, s = 1)
        hQ <- quadratcount(swp, tess = H)
        intensity(hQ, image = TRUE)
        ```

        -   Standard Error of the Mean assumes that the counts in each quadrat are independent (from other quadrat counts)
        -   `hextess` produces hex-shaped quadrats. [s]{.arg-text} is the side length of the hexagons (e.g. 1 meter). [image = TRUE]{.arg-text} allows `plot` to create a sort of hex-shaped celled heatmap of the intensities.
            -   Since the side-length is required, these have to be fixed-size tesselations which is supported by the figure in the book

    -   [Example 3]{.ribbon-highlight}: Relative Intensity

        -   A spatial case-control dataset gives the spatial locations of a set of disease cases, and of a separate set of controls (notionally a sample from the population at risk of the disease).
        -   If the disease risk per head of population does not depend on spatial location, then we would expect the spatially varying intensities of these two point processes to be proportional

    -   Weighted Intensity

        -   [Example 4]{.ribbon-highlight}

            -   When there are multiple disease cases at the same residence, it is then appropriate to weight each residential location by the number of cases.
            -   Intensity would be the average total number of cases per unit area, not the number of affected residences per unit area.

        -   [Example 5]{.ribbon-highlight}: [{spatstat}]{style="color: #990000"} book, p. 174

            ``` r
            vols <- 
              with(marks(finpines),
                   (pi/12) * height * (diameter/100)^2)
            Dvol <- 
              density(finpines, 
                      weights = vols, 
                      sigma = bw.ppl)
            intensity(finpines, weights = vols)
            #> [1] 0.001274
            ```

            -   Volume-weighted intensity is the average standing volume of wood per unit area of forest.
-   [**Isotropic**]{style="color: #009499"} - Same as **Stationarity** but instead of talking about a shifting window, statistical properties are unaffected by rotations, i.e. orientation.
-   [**Density**]{style="color: #009499"} - Empirical estimate of **Intensity** — $\hat \lambda = n / a$ , where $a$ is the area of the region
-   [**Homogeneity**]{style="color: #009499"} - In a general sense, this means the points have no preference for any spatial location (e.g. constant intensity for all windows, quadrats, etc.)
-   [**Inhomogeneous**]{style="color: #009499"} - The average density of points is spatially varying (i.e. density is a function of location)
-   [**Interaction**]{style="color: #009499"} - Stochastic dependence between points (e.g. positive and negative correlation, clustering, inhibition or repelling)
-   [**Quadrats**]{style="color: #009499"} - Cells of a gridded area representing subregions. Useful for analyzing how density varies across a region
    -   Rules of Thumb for choosing the number of quadrats
        -   Each quadrat should have a minimum of two events
        -   Formula based on the area (A) and number of events (N)\
            $$
            Q = \frac{2A}{N}
            $$
-   [**Regularity**]{style="color: #009499"} (or [**Dispersion**]{style="color: #009499"}) - The state at which points tend to be located at similar distances from each other.
-   [**Stationary**]{style="color: #009499"} - The size and coordinates of the window doesn't affect the point process' statistical properties. This implies that ntensity is constant (homogeneous).
-   [**Tesselation**]{style="color: #009499"} - A division of space into non-overlapping regions. These regions are called ‘tiles’ and their shapes are arbitrary. Often used to represent administrative or political divisions, such as the subdivision of a country into states or provinces. ([{spatstat}]{style="color: #990000"} book p.120)
    -   Also see

        -   [Terms](geospatial-point-patterns.qmd#sec-geo-ptpat-terms){style="color: green"} \>\> Intensity \>\> Example 2
        -   p\. 689 in the [{spatstat}]{style="color: #990000"} book, a permutation test procedure

    -   Types

        ::: {layout-ncol="3"}
        ![retangular](_resources/Geospatial-Point-Patterns.resources/term-tess-type-1.png){.lightbox group="tess-type-1" width="232"}

        ![tile list](_resources/Geospatial-Point-Patterns.resources/term-tess-type-2.png){.lightbox group="tess-type-1" width="232"}

        ![pixellated](_resources/Geospatial-Point-Patterns.resources/term-tess-type-3.png){.lightbox group="tess-type-1" width="232"}
        :::

        ``` r
        # rectangular
        quadrats(W, nx, ny) # nx and ny are number of vert and horiz quadrats
        tess(xgrid = xg, ygrid = yg) # xg, yg are vecs of vert and horiz grid lines 

        # tile list
        tess(tiles = z) # z is list of windows (class "owin")

        # pixellated
        tess(image=Z) # e Z is a pixel image with factor values
        ```

        -   `as.tess` can also be used to convert other types of data to a tessellation

    -   Algorithms

        ::: {layout-ncol="2"}
        ![Dirichlet](_resources/Geospatial-Point-Patterns.resources/term-tess-algo-1.png){.lightbox group="tess-algo-1" width="232"}

        ![Delaunay](_resources/Geospatial-Point-Patterns.resources/term-tess-algo-2.png){.lightbox group="tess-algo-1" width="232"}
        :::

        ``` r
        dirichlet(ppp_obj)
        delaunay(ppp_obj)
        ```

        -   *Dirichlet*: The tile associated with a given point of the pattern is the region of space which is closer to that point than to any other point of the pattern.
        -   *Delaunay*: A network of straight lines connecting the points. Two points of the pattern are joined if their Dirichlet tiles share a common edge. The resulting network defines a set of non-overlapping triangles, which cover the convex hull of the point pattern rather than the entire window containing the point pattern.

    -   Operations

        ``` r
        plot(tess_obj) # plot.tess
        B <- A[c(1,3,5)] # subset tiles where A is a tesselation
        A_w <- A[W] # by window
        ls_A <- tiles(A) # list of tiles
        tile.areas(A) # areas
        sapply(ls_A, diameter) # diameters
        split(ppp_obj, A) # list of tiles with points
        ```

## Basics {#sec-geo-ptpat-bas .unnumbered}

-   [Example 1]{.ribbon-highlight}: Quadrats\
    ![](_resources/Geospatial-Point-Patterns.resources/basics-geom-bin2d-1.png){.lightbox width="382"}

    ``` r
    pacman::p_load(
      dplyr,
      ggplot2,
      spatstat
    )
    data("PointPatterns", package = "isdas")
    summary(PointPatterns)
    ##        x                y                 Pattern  
    ##  Min.   :0.0169   Min.   :0.005306   Pattern 1:60  
    ##  1st Qu.:0.2731   1st Qu.:0.289020   Pattern 2:60  
    ##  Median :0.4854   Median :0.550000   Pattern 3:60  
    ##  Mean   :0.5074   Mean   :0.538733   Pattern 4:60  
    ##  3rd Qu.:0.7616   3rd Qu.:0.797850                 
    ##  Max.   :0.9990   Max.   :0.999808

    ggplot() +
      geom_bin2d(data = filter(PointPatterns, 
                               Pattern == "Pattern 1"),
                 aes(x = x, 
                     y = y),
                 binwidth = c(0.25, 
                              0.25)) +
      geom_point(data = filter(PointPatterns, 
                               Pattern == "Pattern 1"), 
                 aes(x = x, 
                     y = y)) +
      scale_fill_distiller(palette = "RdBu") +
      coord_fixed()
    ```

    -   `geom_bin2d` is called to plot a map of counts of events in the space defined by the bins.
    -   [PointPatterns]{.var-text} contains [x]{.var-text}, [y]{.var-text} coordinates that range from 0 to 1 and a categorical variable Pattern indicating each of the four difffernt density patterns

-   [Example 2]{.ribbon-highlight}: Create a ppp object

    ``` r
    # define a window
    wnd <- owin(c(0,1), c(0,1)) 
    ppp1 <- as.ppp(PointPatterns, wnd)
    summary(ppp1)
    ## Marked planar point pattern:  240 points
    ## Average intensity 240 points per square unit
    ## 
    ## Coordinates are given to 16 decimal places
    ## 
    ## Multitype:
    ##           frequency proportion intensity
    ## Pattern 1        60       0.25        60
    ## Pattern 2        60       0.25        60
    ## Pattern 3        60       0.25        60
    ## Pattern 4        60       0.25        60
    ## 
    ## Window: rectangle = [0, 1] x [0, 1] units
    ## Window area = 1 square unit

    # plot a specific category of point
    plot(split.ppp(ppp1)$`Pattern 3`)
    ```

    -   The window defined in `owin` should define a region for analysis that is consistent with the pattern of interest
    -   `ppp` (plannar point pattern) is the fundamental spatstat object
    -   [frequency]{.arg-text} is the number of points in that region (e.g. [Pattern]{.var-text})
    -   [proportion]{.arg-text} is the proportion of points in that region to the overall dataset
    -   [intensity]{.arg-text} it the number of points divided by the area (1 x 1 = 1)

-   [Example 3]{.ribbon-highlight}: Get point counts for each quadrat by region/subregion

    ``` r
    quadratcount(split(ppp1),
                 nx = 4,
                 ny = 4)

    ## List of spatial objects
    ## 
    ## Pattern 1:
    ##             x
    ## y            [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1]
    ##   [0.75,1]          3          5          1        6
    ##   [0.5,0.75)        2          3          4        6
    ##   [0.25,0.5)        5          4          2        3
    ##   [0,0.25)          2          4          4        6
    ## 
    ## Pattern 2:
    ##             x
    ## y            [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1]
    ##   [0.75,1]         14          2          2        6
    ##   [0.5,0.75)        0          0          4        6
    ##   [0.25,0.5)        6          3          1        2
    ##   [0,0.25)          4          6          2        2
    ## 
    ## Pattern 3:
    ##             x
    ## y            [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1]
    ##   [0.75,1]          2         11          5        7
    ##   [0.5,0.75)        1          1          6        4
    ##   [0.25,0.5)        1         10          3        2
    ##   [0,0.25)          2          1          2        2
    ## 
    ## Pattern 4:
    ##             x
    ## y            [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1]
    ##   [0.75,1]          4          5          6        3
    ##   [0.5,0.75)        3          3          4        2
    ##   [0.25,0.5)        3          3          4        2
    ##   [0,0.25)          5          4          6        3
    ```

    -   [nx]{.arg-text} and [ny]{.arg-text} specify how many quadrats (i.e. cells) you want per row and per column respectively
    -   `split` divides the dataset by the region variable or event type

-   [Example 4]{.ribbon-highlight}: Quadrat Count for Toronto Fast Food\
    ![](_resources/Geospatial-Point-Patterns.resources/ff-qct-1.png){.lightbox width="332"}

    ``` r
    library(spatstat)
    library(sf)

    data("Fast_Food", package = "isdas")
    data("Toronto", package = "isdas")

    head(Fast_Food)
    #> Simple feature collection with 6 features and 1 field
    #> Geometry type: POINT
    #> Dimension:     XY
    #> Bounding box:  xmin: 620173 ymin: 4840698 xmax: 638544.7 ymax: 4853394
    #> Projected CRS: NAD83 / UTM zone 17N
    #>     Class                 geometry
    #> 1 Chicken POINT (635575.8 4853394)
    #> 2 Chicken POINT (636724.5 4842644)
    #> 3 Chicken POINT (622524.7 4840698)
    #> 4 Chicken POINT (638544.7 4846541)
    #> 5 Chicken POINT (627850.5 4843178)
    #> 6 Chicken   POINT (620173 4841782)
    head(toronto)
    #> Simple feature collection with 1 feature and 0 fields
    #> Geometry type: MULTIPOLYGON
    #> Dimension:     XY
    #> Bounding box:  xmin: 609550.5 ymin: 4826375 xmax: 651611.8 ymax: 4857439
    #> Projected CRS: NAD83 / UTM zone 17N
    #>                         geometry
    #> 1 MULTIPOLYGON (((609550.5 48...

    ppp_ff <- as.ppp(Fast_Food, as.owin(Toronto)) # <1>
    #> Marked planar point pattern: 614 points
    #> Multitype, with levels = Chicken, Hamburger, Pizza, Sub  # <2>
    #> window: polygonal boundary
    #> enclosing rectangle: [609550.5, 651611.8] x [4826375, 4857439] units

    qct_ff <- quadratcount(ppp_ff, nx = 3, ny = 3) # <3>
    table(qct_ff) # <4>
    #>   0   6  44  48  60  64  85 144 163 
    #>   1   1   1   1   1   1   1   1   1 

    plot(qct_ff)
    ```

    1.  To automatically create a window object using the boundaries from a sf object, [{sf}]{style="color: #990000"} needs to be loaded.
    2.  The categories in the [Class]{.var-text} variable are captured as levels
    3.  3 x 3 seems to be a good starting grid for regions such as a city if your data isn't too sparse
    4.  There is 1 quadrat with 0 points. Given that were using a 3 x3 grid, it's very small and probably located underneath the quadrat with 44 points.

-   [Example 5]{.ribbon-highlight}: Approximate a window using coordinates

    ``` r
    data(bear_df, package = "isdas")
    summary(bear_df)
    ##        x                y                  marks    
    ##  Min.   :515743   Min.   :6812138   Day Time  :502  
    ##  1st Qu.:518994   1st Qu.:6813396   Night Time:498  
    ##  Median :519526   Median :6816724                   
    ##  Mean   :519321   Mean   :6816474                   
    ##  3rd Qu.:519982   3rd Qu.:6818111                   
    ##  Max.   :522999   Max.   :6821440

    W <- 
      owin(xrange = c(515000, 523500), 
           yrange = c(6812000, 6822000))
    bear.ppp <- as.ppp(bear_df, W = W)

    summary(bear.ppp)
    ## Marked planar point pattern:  1000 points
    ## Average intensity 1.176471e-05 points per square unit
    ## 
    ## Coordinates are given to 10 decimal places
    ## 
    ## Multitype:
    ##            frequency proportion    intensity
    ## Day Time         502      0.502 5.905882e-06
    ## Night Time       498      0.498 5.858824e-06
    ## 
    ## Window: rectangle = [515000, 523500] x [6812000, 6822000] units
    ##                     (8500 x 10000 units)
    ## Window area = 8.5e+07 square units
    ```

    -   Uses the minimum and maximum values of each coordinate

-   Various Object Manipulations

    ``` r
    library(sf)

    # change the window of a ppp obj
    new_ppp <- ppp_obj[W] # W is a window object of class "owin"

    # SpatialPoints class
    as.ppp(sp_pts)

    # "SpatialPolygons" class; e.g. US State boundaries
    # 1
    as.owin(sp_poly_obj) # combines all states into one obj i.e. US country map

    # 2 Create a separate window for each state
    regions <- slot(sp_poly_obj, "polygons")
    regions <- 
      lapply(regions,
             function(x) { SpatialPolygons(list(sp_poly_obj)) })
    windows <- solapply(regions, as.owin) # list of owin objs
    te <- tess(tiles = windows)

    # "SpatialPolygonsDataFrame"
    cp <- as(columbus, "SpatialPolygons")
    cregions <- slot(cp, "polygons")
    cregions <- lapply(cregions, function(x){ SpatialPolygons(list(x)) })
    cwindows <- solapply(cregions, as.owin)
    ch <- hyperframe(window=cwindows)
    ch <- cbind.hyperframe(ch, columbus@data)
    ```

    -   See [{spatstat}]{style="color: #990000"} book p. 76 for other GIS object manipulations
    -   [{maptools}]{style="color: #990000"} is no more and many of it's functionalities were transferred to [{sf}]{style="color: #990000"}, so just loading that package makes most of this stuff pretty painless.
    -   For an object of class "SpatialPolygonsDataFrame" (geometry + variables), the result is a hyperframe containing a column of "owin" objects followed by the columns of auxiliary data

## Kernel Density {#sec-geo-ptpat-kdens .unnumbered}

-   Kernel density is a smooth estimate of the underlying intensity of the process, and the degree of smoothing is controlled by the bandwidth

    -   A map of the kernel density is better able to capture the variations in density across the region.

-   Misc

    -   The choice of bandwidth involves a tradeoff between bias and variance: as bandwidth increases, typically the bias increases and variance decreases.
    -   Edge effects in uncorrected estimates will show the intensity decreasing at the window boundaries
    -   Densities estimates at each point can be used as weights in a model: `density(swp, sigma = 1, at = "points")`
    -   Standard Errors: `dse <- density(swp, 1, se=TRUE)$SE`
        -   Although the standard error provides an indication of accuracy, and is justified by asymptotic theory, confidence intervals based on the standard error are notoriously unreliable
        -   Should have a `plot` method
    -   A fixed smoothing bandwidth (like in `density`) is unsatisfactory if the true intensity varies *greatly* across the spatial domain, because it is likely to cause oversmoothing in the high-intensity areas and undersmoothing in the low intensity areas. (e.g. bad for seismology)
        -   Solution: **Adaptive Smoothing**
            -   Dirichlet-Voronoï estimator (f[= 1]{.arg-text}): `adaptive.density(swp, f=1)`
            -   [{]{style="color: #990000"}[sparr](https://tilmandavies.github.io/sparr/){style="color: #990000"}[}]{style="color: #990000"} - Provides a suite of adaptive kernel spatial smoothing techniques and related tools
        -   Solution: **Nearest-Neighbor**
            -   Example: 10th Nearest Neighbor Estimate: `nndensity(swp, k=10)`

-   Process

    -   Each quadrat is treated as independent of the others in the window.
    -   There isn't a grid of quadrats but in essence, one that slides around the study area.
    -   it gives greater weight to events that are close to the center of the window, and less weight to events that are more distant from the center of the window
    -   The kernel function visits each point on a fine grid and obtains an estimate of the density by summing the weights of all events.
    -   The shape of the Gaussian kernel depends on the standard deviation, which controls how “big” the window is, or alternatively, how quickly the function decays via decreasing weights. We will call the standard deviation the kernel bandwidth of the function.

-   [{spatstat.explore::density.ppp}]{style="color: #990000"} - Kernel smoothed intensity function from a point pattern

    -   [kernel]{.arg-text}: "gaussian", "epanechnikov", "quartic" or "disc"
    -   [weights]{.arg-text}: Optional weights to be attached to the points
    -   [diggle]{.arg-text}: Logical. If TRUE, use the Jones-Diggle improved edge correction, which is more accurate but slower to compute than the default correction.
    -   sigma: Bandwidth
        -   bw.ppl assumes an inhomogeneous Poisson process. Uses a likelihood cross-validation method
        -   bw.diggle assumes a Cox process, which is more clustered (positively correlated) than a Poisson process. Uses a mean square error cross-validation method
        -   bw.frac for a fast bandwidth selection rule based on the window geometry

-   [Example]{.ribbon-highlight}: Fast Food in Toronto\
    ![](_resources/Geospatial-Point-Patterns.resources/dens-ff-1.png){.lightbox width="632"}

    ``` r
    library(spatstat)
    library(sf)

    # sf dfs
    data("Fast_Food", package = "isdas")
    data("Toronto", package = "isdas")

    # create a ppp obj
    ppp_ff <- as.ppp(Fast_Food, as.owin(Toronto))
    # calculate densities for each type of fast food
    kernel_density <- density(split(ppp_ff), sigma = bw.diggle)

    par(mfrow = c(2, 2), mar = c(0, 0, 1.1, 2))
    purrr::pwalk(
      list(
        kernel_density,
        split.ppp(ppp_ff), # add pts
        names(kernel_density)
      ),
      \(x1, x2, x3) {
        plot(x1, main = x3)
        plot(x2, add = TRUE)
      }
    )
    ```

    -   `bw.diggle` calculates the bandwidth using cross-validation. It's part of one of the group of sub-packages, [{spatstat.explore}]{style="color: #990000"}, that automatically gets loaded. There do seem to be other options, but I chose this based on the example in the Pebesma-Bivand [book](https://r-spatial.org/book/11-PointPattern.html).
    -   If you just want to plot the densities without the points, it's `plot(kernel_density)`
    -   See `spatstat.geom::plot.im` for beaucoup styling options for the density plot. I didn't add it here, but when there aren't any other labels, I kind of liked the [addcontour = TRUE]{.arg-text} as an extra density cue especially for the darker colors.
    -   Note that density color scales have different ranges in the legends

## CSR Tests {#sec-geo-ptpat-csrt .unnumbered}

-   Completely Spatially Random (CSR)

-   These simple numerical indexes of spatial pattern are useful when a large number of different point patterns need to be compared, or to monitor changes in spatial clustering over time. Their main weakness is that they compress all the spatial information into a single number, conflating information from different spatial scales and different spatial locations.

-   10 realizations (i.e. potential patterns) of the *same* random point process\
    ![](_resources/Geospatial-Point-Patterns.resources/csr-10-rand-1.png){.lightbox}

-   [Quadrat-based Chi-Square Test]{.underline}

    -   A Pearson $\chi^2$ independence test that compares the empirical distribution of events by quadrats to the distribution of events as expected under the hypothesis that the underlying process is random. (i.e. tests homogeneity assuming independence)\
        $$
            \begin{align}
            &\chi^2 = \sum_i^Q r_i^2\\
            &\text{where} \;\; r_i = \frac{\text{observed}_i - \text{expected}_i}{\sqrt{\text{expected}_i}}
            \end{align}
            $$

        -   $r_i$ is the Pearson residual and $Q$ is the number of quadrats

    -   Hypotheses

        -   H~0~: The point process is homogeneous Poisson process
        -   H~a~: It's not

    -   Reasons for Rejection of the Null

        -   Intensity is not homogeneous
        -   Events/points are not independent

    -   Issues

        -   Test results is affected by the chosen quadrat grid.
        -   Count-based so size of the quadrat matters. With irregular shaped quadrats (e.g. within a city boundary), it might be difficult to create a grid with roughly homogeneous counts.
        -   The test is *not* sensitive to the relative position of the events within the quadrats. So, there could be extreme clustering happening within the quadrats and the test might not reject the Null.

    -   It's recommended to compute the variance-to-mean ratio or $\chi^2$ statistic for different sizes of quadrats, and to plot the statistic against quadrat size.

    -   [{spatstat.explore::quadrat.test}]{style="color: #990000"}

        -   If [lambda]{.arg-text} is provided then the Null is a Poisson point process with that $\lambda$ (i.e. intensity or probably also Poisson mean)
        -   Multiples from multiple study areas can be pooled: `pool(test1, test2, test3)`
        -   Can also use `split.ppp(ppp_ff)` and have each point category tested.
        -   This function has methods for point patterns (class "ppp"), split point patterns (class "splitppp"), point process models (class "ppm" or "slrm"\`) and quadrat count tables (class "quadratcount").
        -   Plotting the object will display the quadrats, annotated by their observed and expected counts and the Pearson residuals.
        -   Pearson residuals can be extracted with `residuals`. They evidently aren't standardized, but if standardized, they can be treated as z-scores with values \> 1.96 indicating which quadrats are causing the rejection of the Null. (See [Regression, Diagnostics \>\> Residuals](diagnostics-regression.qmd#sec-diag-reg-res){style="color: green"} \>\> Standardized Residuals)

    -   [Example]{.ribbon-highlight}: `quadrat.test` using a ppp object\
        ![](_resources/Geospatial-Point-Patterns.resources/quadtest-fastfood-ex-1.png){.lightbox width="432"}

        ``` r
        library(spatstat)
        library(sf)

        # sf dfs
        data("Fast_Food", package = "isdas")
        data("Toronto", package = "isdas")

        # create a ppp obj
        ppp_ff <- as.ppp(Fast_Food, as.owin(Toronto))

        q_test <- 
          quadrat.test(ppp_ff, 
                       nx = 3, 
                       ny = 3)
        q_test
        ## Warning: Some expected counts are small; chi^2 approximation may be inaccurate
        ##  Chi-squared test of CSR using quadrat counts
        ## 
        ## data:  Fast_Food.ppp
        ## X2 = 213.74, df = 8, p-value < 2.2e-16
        ## alternative hypothesis: two.sided
        ## 
        ## Quadrats: 9 tiles (irregular windows)
        ```

        -   All expected counts (assumes a uniform Poisson point process) should be greater than 5 which is why there's a warning.
            -   The docs of this function sound very much like the stuff in my Discrete Analysis notebook, so see that for further details.
            -   Options seem to be to use [method = "MonteCarlo"]{.arg-text} which relaxes the expected count \> 5 condition or using a smaller grid.
        -   p-value \< 0.05 suggests that this is *not* a CSR (completely spatially random — aka uniform Poisson point process) pattern

-   [Cressie-Read Power Divergence Test]{.underline}

    -   $\chi^2$-based family of tests
    -   Hypotheses (Same as previous section)
        -   H~0~: The point process is homogeneous Poisson process
        -   H~a~: It's not
    -   Test Statistic\
        $$
        2nI = \frac{2}{\text{CR}(\text{CR} + 1)} \sum_i \left[\left(\frac{X_i}{E_i}\right)^{CR}-1\right]
        $$
        -   $X_i$ is the ith observed count
        -   $E_i$ is the ith expected count
        -   $\text{CR}$ options
            -   $\text{CR} = -2$ results in the Neyman modified statistc (NM^2^)
            -   $\text{CR} = -1$ results in the modified likelihood ratio test statistic (GM^2^)
            -   $\text{CR} = -\frac{1}{2}$ results in the Freeman-Tukey statistic (T^2^)
                -   More robust to small expected frequencies
            -   $\text{CR} = 0$ results in the likelihood ratio test statistic (G^2^)
                -   More sensitive to small observed frequencies
            -   $\text{CR} = \frac{2}{3}$ is the recommnedation by Cressie-Read
                -   A compromise between Pearson's Chi-Square and the Likelihood Ratio
            -   $\text{CR} = 1$ results in the Pearson $\chi^2$ statistic (previous section)
                -   Balanced sensitivity to small counts
    -   [{spatstat.explore::quadrat.test}]{style="color: #990000"} has a [CR]{.arg-text} argument

-   [Clark-Evans Test]{.underline}

    -   Assumes the distribution of a distance-based index is Normal and applies NHST to the pattern's estimated index.

    -   Clark-Evans Index, $R$\
        $$
        R =\frac{\sqrt{\bar \lambda}}{m} \sum_i^m d_i
        $$

        -   $d_i$ is a nearest-neighbor distance for event $i$ out of $m$ events
        -   $\bar \lambda = n \;/\; |W|$ is the average intestity for the entire point pattern

    -   Guidelines

        -   $R \gt 1$ suggests regularity
        -   $R =1$ is consistent with a CSR
        -   $R \lt 1$ suggests clustering

    -   Assumes point process is stationary (i.e. constant intensity)

        -   An inhomogeneous point pattern will typically give R \< 1, and can produce spurious significance

    -   Test requires edge effects correction otherwise R will be positively biased

    -   [Example]{.ribbon-highlight}: [{spatstat}]{style="color: #990000"} book, p. 259\
        ![](_resources/Geospatial-Point-Patterns.resources/csr-ce-ex1-1.png){.lightbox width="332"}

        ``` r
        data("redwood")
        plot(redwood)
        summary(redwood)
        #> Planar point pattern:  62 points
        #> Average intensity 62 points per square unit
        #> 
        #> Coordinates are given to 3 decimal places
        #> i.e. rounded to the nearest multiple of 0.001 units
        #> 
        #> Window: rectangle = [0, 1] x [-1, 0] units
        #>                     (1 x 1 units)
        #> Window area = 1 square unit

        clarkevans.test(redwood, 
                        correction = "donnelly",
                        alternative = "clustered")

        #>  Clark-Evans test
        #>  Donnelly correction
        #>  Z-test
        #> 
        #> data:  redwood
        #> R = 0.58499, p-value = 1.906e-11
        #> alternative hypothesis: clustered (R < 1)
        ```

-   [Hopskell-Skellam Test]{.underline}

    -   Assumes the distribution of a distance-based index is F and applies NHST to the pattern's estimated index.

    -   Hopskell-Skellam Index, $A$\
        $$
        A = \frac{\sum_i^m d_i^2}{\sum_i^m e_i^2}
        $$

        -   $d_i$ is the distance the event $i$'s nearest neighbor from $m$ randomly sampled events
        -   $e_i$ is the distance of point $i$'s (event or empty space) nearest neighbor from $m$ randomly sampled points

    -   Guidelines

        -   $A \gt 1$ is consistent with regularity
        -   $A = 1$ is consistent with CSR
        -   $A \lt 1$ is consistent with clustering

    -   The Hopkins-Skellam index is much less sensitive than the Clark-Evans index to problems such as edge effect bias and spatial inhomogeneity

    -   [Example]{.ribbon-highlight}: [{spatstat}]{style="color: #990000"} book, p. 259

        ``` r
        hopskel.test(redwood, alternative="clustered")
        #>  Hopkins-Skellam test of CSR
        #>  using F distribution
        #> 
        #> data:  redwood
        #> A = 0.18176, p-value < 2.2e-16
        #> alternative hypothesis: clustered (A < 1)
        ```

        -   See the Clark-Evans Test example for details in the [redwood]{.var-text} dataset (ppp object)

## Dependence Functions {#sec-geo-ptpat-depfun .unnumbered}

### Misc {#sec-geo-ptpat-depfun-misc .unnumbered}

-   tl;dr:
    -   Run the J Function if you want to analyze the overall pattern of the study area
    -   Run the L Function if you want to analyze patterns at all scales of the study area
        -   Think... the point pattern of 1st order neighbors area, the pattern of 1st and 2nd order neighbors area, etc.
        -   Use [correction = "best"]{.arg-text}
    -   You should probably use both J and L as they technically measure different things but the interpretations of the results have similar implications. i.e. come at the analysis from multiple angles
    -   Use a Simulation Envelope and Loh bootstrap or Block Variance method for the functions to include uncertainty in the analysis
-   Basic Assumptions
    -   An appropriate window selection is used
        -   If at all possible, the region should be selected in such a way that it is consistent with the underlying process. This is not always possible, either because the underlying process is not known, or because of limitations in data collection capabilities.
    -   All events within the window have been identified
        -   No solutions were given for *sampled* events, so the technique is discouraged.
-   From the [{spatstat}]{style="color: #990000"} book
    -   When interpreting the dependence functions: "the careful scientist will not say that this ‘indicates’ clustering, but that it is ‘consistent with’ clustering, or that it indicates ‘positive association’ between points"
    -   "Spatial clustering does not imply that the points are organised into identifiable ‘clusters’; merely that they are closer together than would be expected for a completely random pattern."
    -   Think he's saying that "clustering" implies a cause and further investigation is required in order to call a "spatial inhomogeneity" a cluster.
-   Edge Effects for Spacing Functions
    -   When a window is chosen, if events lie outside the window, this can bias the test results. An event at the edge of window may not be close to another event inside the window but may be close to one just outside the window.
    -   [Example]{style="color: green"}: Analysis of pizza restaurants in Toronto may be limited to the city limits. This does not mean that establishments do not exist beyond those boundaries. When the extent of the process exceeds the window used in the analysis, the point pattern is observed only partially, and it is possible that the omitted information regarding the location of events beyond the boundary may introduce some bias.
    -   A bias correction is recommended (especially for F and G but not for J) by using weighting events via [correction]{.arg-text}
        -   ["rs"]{.arg-text} (reduced-sample): Restricts the analysis to points or locations that are far enough from the boundary so that the entire region of interest (e.g., a circle of radius r) lies within the study area (Available for G and F)
            -   You're essentially removing data at the edges, so there will be a reduction in power
        -   ["km"]{.arg-text} (Kaplan-Mier): Treats the boundary as a "censoring" mechanism, where distances beyond the boundary are treated as censored observations. (Available for G and F)
        -   ["Hanisch"]{.arg-text}: Weights observations based on their proximity to the circle boundary drawn for each value of $r$. Points near the boundary receive lower weights because their neighborhoods are more likely to be truncated. (Available for G)
        -   ["cs"]{.arg-text}(Chiu-Stoyan): Same idea as the Hanisch method but taylored for the F-Function.
        -   ["best"]{.arg-text}: Chooses the best option based on the data and window geometry
        -   ["all"]{.arg-text} : Performs all the corrections, `plot` will plot each corrected empirical curve so you see their effects

### Spacing Functions {#sec-geo-ptpat-depfun-spfun .unnumbered}

-   Functions that measure the ‘nearest-neighbour’ distance from each data point to the nearest other data point

-   [G-Function]{.underline}

    -   A cumulative distribution function of distances that tells you the proportion of events that have a nearest neighbor at a distance less than some value. (i.e. ecdf)

        -   e.g. At a distance of 1.5 km, 32% of events/locations have a nearest neighbor at that distance or less.

    -   Edge correction needs to be applied via [correction]{.arg-text} if being used by itself (i.e. not as part of the J-Function)

    -   Empirical G-Function\
        $$
        \hat G(r) = \frac{\mathbb{I}(d_{i} \le r), \forall i}{n}
        $$

        -   Where $d_{i}$ is the distance from event/location, $i$, to its nearest neighbor and $r$ is a distance value

    -   Theoretical G-Function\
        $$
        G(r) = 1 - e^{-\lambda \pi r^2}
        $$

        -   Represents the Null point generating process from a Poisson distribution

    -   Guidelines

        -   $\hat G(r) \gt G(r)$ says events are *closer* together than expected from a random process, i.e. *Clustered*
        -   $\hat G(r) \approx G(r)$ says event/location pattern resembles a *random* process
        -   $\hat G(r) \lt G(r)$ says events are *further away* from each other than expected from a random process, i.e. *Dispersed* or *Regular*

    -   [Example]{.ribbon-highlight}: [{spatstat.explore::Gest}]{style="color: #990000"}\
        ![](_resources/Geospatial-Point-Patterns.resources/tests-gfunc-ex-1.png){.lightbox width="382"}

        ``` r
        # Use split to calculate the G-function only for "Pattern 1"
        g_pattern1 <- 
          Gest(split(pp0.ppp)$"Pattern 1", 
               correction = "none")

        plot(g_pattern1)
        lines(x = c(0.04, 0.04), 
              y = c(-0.1, 0.5), 
              lty = "dotted")
        lines(x = c(-0.1, 0.04), 
              y = c(0.5, 0.5), 
              lty = "dotted")
        lines(x = c(-0.1, 0.04), 
              y = c(0.16, 0.16), 
              lty = "dotted", 
              col = "red")
        ```

        -   See [Basics](geospatial-point-patterns.qmd#sec-geo-ptpat-bas){style="color: green"} \>\> Example 1 for what [Pattern 1]{.var-text} looks like
        -   $\hat G(r) \gt G(r)$ indicates clustering
        -   The line shows about 50% of events have a nearest neighbor at a distance of less than approximately 0.04
        -   [correction]{.arg-text}: Optional. The edge correction(s) to be used to estimate $G(r)$. A vector of character strings selected from ["none"]{.arg-text}, ["rs"]{.arg-text}, ["km"]{.arg-text}, ["Hanisch"]{.arg-text} and ["best"]{.arg-text}. Alternatively [correction = "all"]{.arg-text} selects all options.

-   [F-Function]{.underline}

    -   Same as the G-Functions except the distance isn't from event-to-event but from point-to-event.

    -   The point is an arbitrary location on a map and not necessarily a location of an event

    -   Same empirical and theoretical formulas — again, only the $d_i$ measurement is different.

    -   Edge correction needs to be applied via [correction]{.arg-text} if being used by itself (i.e. not as part of the J-Function)

    -   Guidelines (Opposite of $G$)

        -   $\hat F(r) \gt F(r)$ says *empty spaces are closer to events* than expected from a random process, i.e. *regular* or *dispersed*
        -   $\hat F(r) \approx F(r)$ says event/location pattern resembles a *random* process
        -   $\hat F(r) \lt F(r)$ says *empty spaces are further from events* than expected from a random process, i.e. *clustered*

    -   [Example]{.ribbon-highlight}: [{spatstat.explore::Fest}]{style="color: #990000"}

        ::: {layout-ncol="2"}
        ![](_resources/Geospatial-Point-Patterns.resources/test-ffunc-ex-1.png){.lightbox width="282"}

        ![](_resources/Geospatial-Point-Patterns.resources/test-ffunc-ex-2.png){.lightbox width="510"}
        :::

        ``` r
        data("pp2_df", package = "isdas")
        W <- owin(c(0, 1), c(0, 1))
        pp2.ppp <- as.ppp(pp2_df, W = W)
        plot(pp2.ppp)

        f_pattern2 <- Fest(pp2.ppp, correction = "none")
        plot(f_pattern2)
        lines(x = c(0, 0.097), 
              y = c(0.4, 0.4), 
              col = "blue", 
              lty = "dotted")
        lines(x = c(0.045, 0.045), 
              y = c(0.0, 0.4), 
              col = "blue", 
              lty = "dotted")
        lines(x = c(0.097, 0.097), 
              y = c(0.0, 0.4), 
              col = "blue", 
              lty = "dotted")
        ```

        -   $\hat F \lt F$ indicates clustering
        -   Under the theoretical function 40% of points have a nearest event that is at a distance of approximately 0.045 or less, under the empirical function, the events are generally more distant from the points

-   [J-Function]{.underline}

    -   A ratio of the G and F functions

    -   The uncorrected estimate of $J$ is approximately unbiased (if the process is close to Poisson); it is insensitive to edge effects, and should be used when edge effects are severe.

    -   Formula

        $$
        J(r) = \frac{1-G(r)}{1-F(r)}
        $$

    -   [{spatstat.explore::Jest}]{style="color: #990000"}

        -   Values for G and F are also returned by the function

    -   Guidelines

        -   $J(r) \gt 1$ indiates *dispersion* or *regularity*
        -   $J(r) \approx 1$ indicates a random process
        -   $J(r) \lt 1$ indicates *clustering*

### Spatial Correlation {#sec-geo-ptpat-depfun-spcor .unnumbered}

-   [K-Function]{.underline}

    -   AKA Ripley's K-Function

    -   \*Assumes homogeneity of intensity\*

    -   Instead of only using distances to first order neighbors, it takes into account multiple orders of neighbors.

    -   The interaction between to points doesn't necessarily happen at $r$. From [{spatstat}]{style="color: #990000"} book, p. 212:

        -   "For point processes which have a well-defined scale of interaction, it is not always true that the greatest deviation in the K-function occurs when r is equal to the scale of interaction. The K-function reflects correlation between pairs of points, not direct dependence. Dependence between points at one scale can give rise to correlation between points at another scale."
        -   "$K(r)$ accumulates contributions from all distances less than or equal to $r$. If a pattern of emergent seedlings yields $K(r) > \pi r^2$ for the distance $r = 10$ metres, this does not necessarily indicate that seedlings are clustered at distances of 10 metres. A plausible explanation is that seedlings are organised in clusters at a much smaller spatial scale, and the cumulative effect is still evident at 10 metres."
        -   "The K-function is optimal for detecting interpoint interaction that occurs equally at all distances up to a certain maximum distance $r$"

    -   Different types of patterns can be present at different scales ([pp3.ppp]{.var-text} plot)\
        ![](_resources/Geospatial-Point-Patterns.resources/test-kfunc-1.png){.lightbox width="232"}

        -   Overall there's clustering but the clusters are regularly spaced.
        -   At the cluster-level, events look to possibly have a random pattern.
        -   F and G will indicate clustering but not recognize the regular spacing of the clusters

    -   Process: At each event, neighbors are counted at some radius, r. Then r is increased and neighbors are counted again. This continues until all events have been counted as a neighbor. (See [Geospatial, Spatial Weights \>\> Diagnostics \>\> Connectedness](geospatial-spatial-weights.qmd#sec-geo-swgt-diag){style="color: green"} for details on higher order neighbors)

    -   Empirical Formula\
        $$
        \hat K(r) = \frac{1}{\hat \lambda (n-1)} \sum_i \sum_{j \ne i} \mathbb{I}(d_{ij} \le r)e_{ij}
        $$

        -   $\hat \lambda$ is the estimated intensity (i.e. density)
        -   The rest is summing all instances where the distances ($d_{ij}$) from event, $i$, to all the other events, $j$ is less than $r$.
        -   $e_{ij}$ is the edge correction method

    -   Theoretical Formula\
        $$
        K(r) = \pi r^2
        $$

    -   Guidelines

        -   $\hat K(r) \gt K(r)$ says events are *closer* together than expected from a random process, i.e. *Clustered*
        -   $\hat K(r) \approx K(r)$ says event/location pattern resembles a *random* process
            -   Even with this result, it's possible that the processe is not a Poisson process (so that there is dependence amongst the points) ([{spatstat}]{style="color: #990000"} book, p. 211)
        -   $\hat K(r) \lt K(r)$ says events are *further away* from each other than expected from a random process, i.e. *Inhibition (repulsion)*

    -   Edge [correction]{.arg-text} methods should be applied

        -   Details about these corrections can be found in the [{spatstat}]{style="color: #990000"} book, pp. 212-220
        -   ["border"]{.arg-text} (aka “reduced sample” estimator): The least efficient (statistically) and the fastest to compute. It can be computed for a window of arbitrary shape.
            -   See Misc \>\> Edge Effects \>\> "rs" (reduced-sample)
        -   ["border.motif"]{.arg-text}: A weighted version of the ["border"]{.arg-text} method
            -   Similar to Misc \>\> Edge Effects \>\> "hanisch"
        -   ["isotropic"]{.arg-text}/["Ripley"]{.arg-text}: The weight is the circumference of the circle specified by $r$ divided by the circle's proportion of the whole window (without overlapping the boundary)\
            $$
            e(u,r) = \frac{2 \pi r}{\text{length}(c(u,r) \cap W)}
            $$
            -   "The denominator is the length of the overlap between this circle and the window $W$."
                -   $u$ is the point and $c(u,r$) is the "circle." It says "length" so maybe it's the arclength of circle that intersects the boundary.
            -   Implemented for rectangular and polygonal windows (not for binary masks).
            -   See [{spatstat.explore::edge.Ripley}]{style="color: #990000"}
        -   ["translate"]{.arg-text}/["translation"]{.arg-text}: Implemented for all window geometries, but slow for complex windows.\
            $$
            e(u,r) = \frac{\text{area}(W)}{\text{area}(W \cap (W + y - x))}
            $$
            -   $W + y − x$ is the result of shifting the window $W$ by the vector $y − x$. The denominator is the area of the overlap between this shifted window and the original window.
            -   See [{spatstat.explore::edge.Trans}]{style="color: #990000"}
        -   ["rigid"]{.arg-text}: There's no documentation for this method that I can find. Looking at the code, it looks similar to ["translate"]{.arg-text} since its seems to have a shifting window feature.
            -   Implemented for all window geometries, but slow for complex windows.
        -   ["periodic"]{.arg-text}: (aka toroidal) is only defined for rectangular windows.
        -   ["none"]{.arg-text}: This uncorrected estimate is biased and should not be used for data analysis, *unless* you have an extremely large point pattern (more than 100,000 points).
        -   ["best"]{.arg-text}: Selects Ripley's ["isotropic"]{.arg-text} correction for a rectangular or polygonal window, and the ["translation"]{.arg-text} correction for masks.
        -   ["good"]{.arg-text}: Selects the best edge correction that can be computed in a reasonable time. This is the same as ["best"]{.arg-text} for datasets with fewer than 3000 points; otherwise the selected edge correction is ["border"]{.arg-text}, unless there are more than 100,000 points, when it is ["none"]{.arg-text}.

    -   [Example]{.ribbon-highlight}: [{spatstat.explore::Kest}]{style="color: #990000"}\
        ![](_resources/Geospatial-Point-Patterns.resources/test-kfunc-ex-1.png){.lightbox width="432"}

        ``` r
        data("pp3_df", package = "isdas")
        W <- owin(c(0, 1), c(0, 1))
        pp3.ppp <- as.ppp(pp3_df, W = W)

        k_pattern3 <- Kest(pp3.ppp, correction = "none")
        plot(k_pattern3)
        ```

        -   Plot of [pp3.ppp]{.var-text} shown earlier
        -   $\hat K(r) \gt K(r)$ at a smaller scale which indicates clustering,
        -   But also, $\hat K(r) \lt K(r)$ at a larger scale which indicates regularity.

-   [L-Function]{.underline}

    -   Transformation for the K-Function\
        $$
        L(r) = \sqrt \frac{K(r)}{\pi}
        $$

    -   K-Function Issues

        -   Non-Linearity: For a Poisson process (complete spatial randomness), $K=\pi r^2$, which is a quadratic function. This makes it harder to visually assess deviations from CSR.
        -   Scale Dependency: The K-function grows with r, making it difficult to compare patterns at different scales.

    -   L-Function Advantages

        -   Linear: Easier to visually assess
        -   Scale Independent: Can compare point patterns at different scales

    -   Guidelines

        -   $L(r) \gt r$ : The point pattern exhibits *clustering* at distance $r$.
        -   $L(r) \approx r$ : The point pattern is *random* at distance$r$.
        -   $L(r) \lt r$ : The point pattern exhibits *inhibition (repulsion)* at distance $r$.

    -   [Example]{.ribbon-highlight}: [{spatstat.explore::Lest}]{style="color: #990000"}

        ::: {layout-ncol="2"}
        ![](_resources/Geospatial-Point-Patterns.resources/tests-lfunc-ex1-1.png){.lightbox group="lfunc-ex1-1" width="232"}

        ![](_resources/Geospatial-Point-Patterns.resources/tests-lfunc-ex1-2.png){.lightbox group="lfunc-ex1-1" width="350"}
        :::

        ``` r
        plot(cells)
        L <- Lest(cells)
        plot(L)
        ```

        -   Basic example from the docs
        -   At small to intermediate distance, there is regularity, but at further distances, the pattern is random.
            -   i.e. The distance between points is roughly the same, but the overall pattern is random
        -   Border (Reduced Sample), Translation, and Isotropic edge corrections are shown

### Uncertainty {#sec-geo-ptpat-depfun-uncert .unnumbered}

-   [Simulation Envelopes]{.underline}

    -   Simulation makes it possible to calculate the variance of the expected value under the null hypothesis, i.e. an uncertainty measurement. While it doesn't give you a p-value, you can obtain a acceptance (of the Null) region.

    -   Available for any summary statistic or dependence function

    -   Process: Plug the average intensity of the dataset, $\hat \lambda$, (from the [ppp]{.arg-text} object) into random number generating function (`rpoispp` which is something like `rpois`) to get the *Null Landscape*. Then, use those generated values as inputs to `Gest`, `Fest`, or `Kest` to get a curve at multiple $r$ values. Repeat many times to get your Null distribution. If your empirical $\hat G$, $\hat F$, or $\hat K$ function falls within the ribbon of simulated Null values (aka **Simulation Envelope**), then the pattern is random.

    -   [Example]{.ribbon-highlight}: [{spatstat.explore::envelope}]{style="color: #990000"}\
        ![](_resources/Geospatial-Point-Patterns.resources/test-env-ex1-1.png){.lightbox width="482"}

        ``` r
        env_pp3 <- 
          envelope(Y = pp3.ppp, 
                   fun = Kest, 
                   nsim = 99, # default
                   funargs = list(correction = "none"))
        ```

        -   See previous example for details on the [pp3.ppp]{.var-text} pattern
        -   $\hat K$ only crosses the envelope to indicate a different pattern at a larger scale. Otherwise, it's completely outside the envelope, so the Null is reliably rejected.
        -   Given how uncentered this envelope is on the theoretical value, it would probably be a good idea to do more than 99 simulations.

-   [Loh's Bootstrap]{.underline}

    -   [{spatstat.explore::lohboot}]{style="color: #990000"}
    -   Gives a 95% CI for a correlation function
    -   Available functions: "pcf", "Kest", "Lest", "pcfinhom", "Kinhom", "Linhom", "Kcross", "Lcross", "Kdot", "Ldot", "Kcross.inhom", "Lcross.inhom"
    -   When the intensity of the point process is unknown, the bootstrap error estimate is larger than it should be. When the K function is used, an adjustment procedure has been proposed in Loh (2010) that is used if [Vcorrection=TRUE]{.arg-text}
    -   Beware of CIs that look too narrow
        -   This may occur because the point pattern has a hard core (the true pair correlation function is zero for certain values of distance) or because of an optical illusion when the function is steeply sloping (remember the width of the confidence bands should be measured vertically).
        -   Maybe try Block Variance method

-   [Block Variance]{.underline}

    -   [{spatstat.explore::varblock}]{style="color: #990000"}
    -   Gives a 95% CI for a any summary statistic or dependence function
    -   Process: The estimate from block B is computed by finding the subset of X consisting of points that fall inside B, and applying fun to these points, by calling fun(X\[B\]).

## Other Stuff {#sec-geo-ptpat-ostuff .unnumbered}

-   Making some general notes on things that I'm not digging deeper into at this time, but that I still want to include while it's still fresh in my mind.
-   Misc

### Marked Point Patterns {.unnumbered}

-   Points with labels
-   Basically analysis using 1 variable (label) + coordinates where the variable has information about the event at the location of the coordinates.
-   Subset marks: e.g. `diameter <- marks(finpines)$diameter`
-   [Correlation]{.underline}: Measures the correlation between marks of points separated by a given distance $r$.
    -   [Example]{style="color: green"}: A dataset of spruce tree locations that also has their heights. The spatial correlation will tell us if trees of similar heights near each other or further away.
    -   `spatstat.explore::markcorr`
        -   Under mark independence, the mark correlation function should be approximately constant (e.g., close to 1 for standardized marks).
        -   If $\text{markcorr}(r) \gt 1$ , it suggests *positive correlation* between marks at distance r (e.g., tall trees tend to be near other tall trees).
        -   If $\text{markcorr} \lt 1$, it suggests *negative correlation* between marks at distance r (e.g., tall trees tend to be near short trees).
    -   (`spatstat.explore::markvario`)
        -   A spatial variogram measures the variance between marks of points separated by a given distance $r$
        -   If the estimated variance is greater than the theoretical variance (a constant), $\hat \gamma (r) \gt \gamma (r)$, then marked points at that distance have greater variance and are more dissimilar to each other. (implies negative spatial correlation)
        -   If $\hat \gamma (r) \approx \gamma (r)$, then marked points are independent at that distance. (implies spatial independence)
        -   If $\hat \gamma (r) \lt \gamma (r)$, then marked points have smaller variance and are more similar. (implies positive spatial correlation)
-   [Interactions]{.underline}: Analyze the spatial interaction between *two* different types of points (e.g. one factor variable w/two levels)
    -   `spatstat.explore::Lcross`
        -   Requires a multitype point pattern (a marked point pattern whose marks are a factor).
            -   e.g. Do trees of species A tend to be close to trees of species B? (species would be a factor variable with levels: A and B)
        -   $\hat L (r) \gt r$ : Clustering between category 1 and category 2 at distance $r$.
        -   $\hat L (r) \approx r$ : Independence between category 1 and category 2 at distance $r$.
        -   $\hat L (r) \lt r$ : Inhibition (repulsion) between category 1 and category 2 at distance $r$.
        -   If you want to see if the individual categories have random (i.e. poisson) patterns, use `Kall <- alltypes(rescale(amacrine), Lcross)`. In the matrix plot, the diagonal with have the components (i.e. category 1 vs category 1 and category 2 vs category 2).
    -   There is no `Lmulti` but there is a `Kmulti` which is the same as `Kcross` except instead of the variable needing to be a factor, it can be any form of dichotomization, e.g. diameter \>= 15 and diameter \< 15 or collapsing multiple categories into 2 categories.
    -   I-Function
        -   Summarizes the association between all types in a multi-type point process
        -   Formula\
            $$
            I(r) = \sum_{i = 1}^m  p_i J_{ii}(r) - J_{\boldsymbol{\cdot}\boldsymbol{\cdot}} (r)
            $$
            -   $p_i$ is the proportion of type $i$ events
            -   $J_{ii}$ is the $J$ function for only type $i$ events
            -   $J_{\boldsymbol{\cdot}\boldsymbol{\cdot}}$ is the standard J function that doesn't take types into account
        -   Assumes stationarity
        -   Guidelines
            -   $I(r) \gt 1$ suggests a positive association between all types
            -   $I(r) = 0$ if all types (i.e. categories) are independent of each other
            -   $I(r) \lt 1$ suggests a negative association between all types
-   [Dot Functions]{.underline}
    -   Analyzes the spatial dependence between 1 category of a variable and the rest of its categories.
    -   Requires the variable to be a factor, but seems to accept factor variables with more than 2 levels
    -   Example of Guidelines (e.g. `Gdot`)
        -   $\hat G_{i \boldsymbol\cdot} \gt G_{i \boldsymbol\cdot}$ says that category $i$ events are *closer* to any type of category event than expected from a random process, i.e. *Clustered*
        -   $\hat G_{i \boldsymbol\cdot} \approx G_{i \boldsymbol\cdot}$ says that category $i$ events are spatially independent of category event types
        -   $\hat G_{i \boldsymbol\cdot} \lt G_{i \boldsymbol\cdot}$ says that category $i$ events are *further* from any type of category event than expected from a random process, i.e. *regularity (dispersion)*

### Inhomogeneity {.unnumbered}

-   [EDA]{.underline}
    -   Split the pattern into equal areas and calculate the intensity
        -   [Example]{.ribbon-highlight}: Tesselation by Variable ([{spatstat}]{style="color: #990000"} book p. 178)

            ::: panel-tabset
            ## Data

            ![](_resources/Geospatial-Point-Patterns.resources/inhom-eda-ex1-ppp-1.png){.lightbox width="532"}

            ``` r
            library(spatstat)
            data(bei)

            plot(bei)
            ```

            -   [bei]{.var-text} is tropical rainforest data with tree locations
            -   [bei.extra]{.var-text} has elevvation and slope gradation variables

            ## Tesselation by Elevation

            ![](_resources/Geospatial-Point-Patterns.resources/inhom-eda-ex1-text-1.png){.lightbox width="682"}

            ``` r
            tess_bei <- 
              quantess(M = bei,
                       Z = bei.extra$elev,
                       n = 4)
            textureplot(tess_bei)
            ```

            -   Elevation is binned into four quantiles and four tiles are created with each associated to an elevation bin.

            ## Intensity By Elevation

            ![](_resources/Geospatial-Point-Patterns.resources/inhom-ex1-1.png){.lightbox width="382"}

            ``` r
            qb <- quadratcount(bei, tess=V)
            qb
            tile
            #>   1   2    3   4
            #> 714 883 1344 663


            b5 <- seq(0, 5 * ceiling(max(elev)/5), by=5)
            Zcut5 <- cut(elev, breaks=b5, include.lowest=TRUE)
            Q5 <- quadratcount(bei, tess=tess(image=Zcut5))
            lam5 <- intensity(Q5)
            barplot(lam5)
            ```

            -   Since the four regions have equal area, the counts should be approximately equal if there is a uniform density of trees — which they are not.
            -   The range of elevations is divided into intervals of equal width and the average intensity for each interval is estimated.
            -   Intensity peaks at high elevations
            :::
    -   On p. 689 in the [{spatstat}]{style="color: #990000"} book, a permutation test procedure is performed to test whether a point pattern is correlation-stationary (inhomogeneous) or locally scaled. Testing for whether the pattern is homogeneous or locally scaled and inhomogeneous, etc. could also have been done as well I think.
-   [Inhomogeneous Functions]{.underline}
    -   Instead of assuming the process is stationary, the assumption is that the process is corrrelation-stationary
    -   The functions require an estimate of the intensity function $\lambda(u)$ :
        -   User-Supplied: A vector of intensity values at the data points, a pixel image, a function, or a fitted point process model (ppm).
        -   Otherwise, $\lambda(u)$ will be estimated from the data by a leave-one-out kernel smoother. Manually this would be `density(swp, sigma = bw.diggle, at="points")`.
