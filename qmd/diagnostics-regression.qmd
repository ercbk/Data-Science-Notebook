---
fig-cap-location: top
---

# Regression {#sec-diag-reg .unnumbered}

## Misc {#sec-diag-reg-misc .unnumbered}

-   Packages
    -   [{]{style="color: #990000"}[performance](https://easystats.github.io/performance/){style="color: #990000"}[}]{style="color: #990000"} - Utilities for computing indices of model quality and goodness of fit.
        -   Metrics such as R-squared (R^2^), root mean squared error (RMSE), intraclass correlation coefficient (ICC), etc.
        -   Checks for overdispersion, zero-inflation, convergence, or singularity.
    -   [{]{style="color: #990000"}[olsrr](https://olsrr.rsquaredacademy.com/){style="color: #990000"}[}]{style="color: #990000"} - Various functions for residuals, heteroskedacity, GOF, etc.

## Terms {#sec-diag-reg-term .unnumbered}

-   [**Collinearity**]{style="color: #009499"} - Umbrella term encompassing any linear association between variables.
-   [**Hat Matrix**]{style="color: #009499"} (AKA [**Projection Matrix**]{style="color: #009499"} or [**Influence Matrix**]{style="color: #009499"}) - The matrix that maps the data, $y$, to the model predictions, $\hat y$ (i.e. puts "hats" on the predictions). In $y = A \hat y$, $A$ is the hat matrix. ([source](https://calgary.converged.yt/articles/ncv.html#fn2))
-   [**Multicollinearity**]{style="color: #009499"} - Specific type of collinearity where the linear relationship is particularly strong and involves multiple variables.

## GOF {#sec-diag-reg-gof .unnumbered}

### [{performance}]{style="color: #990000"} {#sec-diag-reg-gof-perfpkg .unnumbered}

-   Handles all kinds of models including mixed models, bayesian models, econometric models
-   `performance::check_model`\
    ![](./_resources/Diagnostics,_Regression.resources/performance_check_model_plot.jpg){.lightbox width="532"}
    -   Can take a tidymodel as input.
-   `performance::model_performance`
    -   Scores model using AIC, AIC~c~, BIC, R^2^, R^2^~adj~, RMSE, Sigma
        -   "Sigma" is the standard deviation of the residuals (aka Residual standard error, see below)
-   `performance::compare_performance`
    -   Outputs table with scores for each model

### `summary` {#sec-diag-reg-gof-sum .unnumbered}

-   See Matrix Algebra for Educational Scientists [Ch. 29.5](https://zief0002.github.io/matrix-algebra/standard-errors-and-variance-estimates.html#inference-model-level) and [Ch. 29.6](https://zief0002.github.io/matrix-algebra/standard-errors-and-variance-estimates.html#inference-coefficient-level) for details on how these values are calculated

-   **Standard Errors**: An estimate of how much estimates would 'bounce around' from sample to sample, if we were to repeat this process over and over and over again.

    -   More specifically, it is an estimate of the standard deviation of the sampling distribution of the estimate.

-   **t-score**: Ratio of parameter estimate and its SE

    -   Used for hypothesis testing, specifically to test whether the parameter estimate is 'significantly' different from 0.

-   **p-value**: The probability of finding an estimated value that is as far or further from 0, if the null hypothesis were true.

    -   Note that if the null hypothesis is not true, it is not clear that this value is telling us anything meaningful at all.

-   **F-statistic**: This "simultaneous" test checks to see if the model as a whole predicts the response variable better than that of the intercept only (a.k.a. mean) model.

    -   i.e. Whether or not all the coefficient estimates should *jointly* be considered unable to be differentiated from 0
    -   Assumes homoskedastic variance
    -   If this statistic's p-value \< 0.05, then this suggests that at least some of the parameter estimates are not equal to 0
    -   Many authors recommend ignoring the P values for individual regression coefficients if the overall F ratio is not statistically significant. This is because of the multiple testing problem. In other words, your p-value and f-value should both be statistically significant in order to correctly interpret the results.
    -   Unless you only have an intercept model, you have multiple tests (e.g. variables + intercept p-values). There is no protection from the problem of multiple comparisons without this.
        -   Bear in mind that because p-values are random variables--whether something is significant would vary from experiment to experiment, if the experiment were re-run--it is possible for these to be inconsistent with each other.

-   **Residual standard error** - Variation around the regression line.

    $$
    \text{Residual Standard Error} = \sqrt{\frac{\sum_{i=1}^n (Y_i-\hat Y_i)^2}{dof}}
    $$

    -   $\text{dof} = n − k^*$ is the model's degrees of freedom
        -   $k^*$ = The numbers of parameters you're estimating including the intercept
    -   Aside from model comparison, can also be compared to the sd of the observed outcome variable

### Kolmogorov--Smirnov test (KS) {#sec-diag-reg-gof-ks .unnumbered}

-   Guessing this can be used for GOF to compare predictions to observed
-   Misc
    -   See [Distributions \>\> Tests](distributions.qmd#sec-distr-tests){style="color: green"} for more details
    -   Vectors may need to be standardized (e.g. normality test) first *unless* comparing two samples
-   Packages
    -   [{KSgeneral}]{style="color: #990000"} has tests to use for contiuous, mixed, and discrete distributions; written in C++
    -   [{stats}]{style="color: #990000"} and [{dgof}]{style="color: #990000"} also have functions, `ks.test`
    -   All functions take a numeric vector and a base R density function (e.g. `pnorm`, `pexp`, etc.) as args
    -   {KSgeneral} docs don't say you can supply your own comparison sample (2nd arg) only the density function but with stats and dgof, you can.
        -   Although they have function to compute the CDFs, so if you need speed, it might be possible to use their functions and do it man

### g-index {#sec-diag-reg-gof-gind .unnumbered}

-   From Harrell's rms ([doc](https://search.r-project.org/CRAN/refmans/rms/html/gIndex.html))
    -   Note: Harrell often recommends using Gini's mean difference as a robust substitute for the s.d.
    -   For Gini's mean difference, see [Feature Engineering, General \>\> Continuous \>\> Transformations \>\> Standardization](feature-engineering-general.html#sec-feat-eng-gen-cont-tran-stand){style="color: green"}
-   I think the g-index for a model is the total of all the partial g-indexes
    -   Each independent variable would have a partial g-index
    -   He also supplies 3 different ways of combining the partial indexes I think
    -   Harrell has a pretty thorough example in the function doc that might shed light
-   Partial g-Index
    -   [Example]{.ribbon-highlight}: A regression model having independent variables, `age + sex + age*sex`, with corresponding regression coefficients $\beta_1$, $\beta_2$, $\beta_3$
        -   g-index~age~ = Gini's mean difference (age \* ($\beta_1$ + $\beta_3$\*w))
            -   Where w is an indicator set to one for observations with sex *not* equal to the reference value.
            -   When there are nonlinear terms associated with a predictor, these terms will also be combined.

### Information Criteria {#sec-diag-reg-gof-infc .unnumbered}

-   AIC vs BIC ([paper](https://t.co/6Lfs0ilrKF))\
    ![](./_resources/Diagnostics,_Probabilistic.resources/image.png){.lightbox width="632"}
    -   **Lower is Better** (which is the model that minimizes the information loss)
        -   e.g. -754.21 is better than -544.32
-   Assumptions
    -   Likelihoods of the models being compared is calculated the same way
        -   e.g. ARIMA and ETS models can't be compared with AIC or BIC
    -   The number of observations that the models were trained on must be the same
        -   A model that uses more lags of a variable than another model will be fit with a different number of observations.
-   AIC\
    $$
    \begin{align}
    \text{AIC} &= 2k - 2\ln (\hat L) \\
    &= T\log \left(\frac{\text{SSE}}{T}\right) + 2(K+2)
    \end{align}
    $$
    -   $\hat L$ is the maximized likelihood
    -   $T$ is the number of observations and $K$ is the number of parameters
    -   Penalizes parameters by 2 points per parameter
    -   Ideal AIC scenario
        -   Numerous hypotheses are considered
        -   You have a conviction that all of them are to differing degrees wrong
-   AIC~c~\
    $$
    \text{AIC}_c = \text{AIC} + \frac{2(K+2)(K+3)}{T-K-3}
    $$
    -   Small sample size corrected
-   BIC\
    $$
    \text{BIC} = T\log \left(\frac{\text{SSE}}{T}\right) + (K+2)\log(T)
    $$
    -   Penalizes parameters by ln(sample size) points per parameter and ln(20) = 2.996
    -   Almost always a stronger penalty in practice
    -   Ideal BIC scenario
        -   Only a few potential hypotheses are considered
        -   One of the hypotheses is (essentially) correct

## Residuals {#sec-diag-reg-res .unnumbered}

-   [Misc]{.underline}

    -   Packages
        -   [{]{style="color: #990000"}[gglm](https://github.com/graysonwhite/gglm/){style="color: #990000"}[}]{style="color: #990000"} - Diagnostic plots for residuals
        -   [{]{style="color: #990000"}[nullabor](https://dicook.github.io/nullabor/){style="color: #990000"}[}]{style="color: #990000"} ([article](https://mansthulin.se/posts/nullabor0315/)) - Generates a Null distribution from your residuals. Samples from that distribution to produce a panel of nulll diagnostics plots for comparison against your diagnostic plot. (See example below)
            -   If your residual plot is indistinguishable from the panel of plots generated by the Null distribution, then there's no evidence of whichever lack-of-fit issue you're trying to diagnose.
            -   If the residual plot is distinguishable, then there is a lack-of-fit issue
        -   [{]{style="color: #990000"}[asympDiag](https://cran.r-project.org/web/packages/asympDiag/index.html){style="color: #990000"}[}]{style="color: #990000"} - Generates an envelope plot that compares observed residuals to those expected under the model, helping to identify potential model misspecifications
        -   [{]{style="color: #990000"}[skedastic](https://cran.r-project.org/web/packages/skedastic/index.html){style="color: #990000"}[}]{style="color: #990000"} - Implements numerous methods for testing for, modelling, and correcting for heteroskedasticity in the classical linear regression model
    -   `influence.measures(mod)` wll calculate DFBETAS for each model variable, DFFITS, covariance ratios, Cook's distances and the diagonal elements of the hat matrix. Cases which are influential with respect to any of these measures are marked with an asterisk.
    -   Scenario: Check of model fit reveals heavy tailed residuals. ([Thread](https://bsky.app/profile/vilgothuhn.bsky.social/post/3lyswtszchc2x))
        -   The residuals are pretty symmetric, but not just a few outliers.
        -   Therefore, the concern is mis-estimating standard errors more than mis-estimating the coefficient. (i.e. a violation of the normality of residuals)
        -   Potential Solutions:
            -   Use a heavy-tailed error distribution in your original model - Fit the same regression but assume t-distributed errors instead of normal errors. This directly addresses the standard error problem.
                -   [{brms}]{style="color: #990000"}, [{glmmTMB}]{style="color: #990000"} have a Student-t distribution family
                -   [{mgcv}]{style="color: #990000"} with [family = "scat"]{.arg-text} for a scaled Student-t distribution
                    -   The location and scale parameter are estimated along with the dof.
            -   Bootstrap standard errors - Use bootstrap resampling to get empirically-based standard errors that don't rely on distributional assumptions.
            -   Sandwich/robust standard errors - These provide consistent standard error estimates even under misspecified error distributions.

-   [Standardized Residuals]{.underline}:

    $$
    R_i = \frac{r_i}{SD(r)}
    $$

    -   Where r is the residuals
    -   Normalize the residuals to have unit variance using an overall measure of the error variance
    -   Follows a Chi-Square distribution
    -   Any $|R|$ \> 2 or 3 is an indication that the point may be an outlier
    -   In R: `rstandard(mod)`
        -   `rstandard(mod, type = "predictive")` provides leave-one-out cross validation residuals, and the “PRESS” statistic (PREdictive Sum of Squares) which is `sum(rstandard(model, type="pred")^2)`
    -   It may be a good idea to run the regression twice --- with and without the outliers to see how much they have an effect on the results.
    -   Inflation of the MSE due to outliers will affect the width of CIs and PIs but not predicted values, hypothesis test results, or effect point estimates

-   [Studentized Residuals]{.underline}

    $$
    t_i = r_i \left(\frac{n-k-2}{n-k-1-r_i^2}\right)^{\frac{1}{2}}
    $$

    -   Where $r_i$ is the i^th^ standardized residual, $n$ = the number of observations, and $k$ = the number of predictors.
    -   Normalize the residuals to have unit variance using an leave-one-out measure of the error variance
    -   Some outliers won't be flagged as outlierrs because they drag the regression line towards them. These residuals are for detecting them.
        -   Therefore, [generally better at detecting outliers than standardized residuals.]{.underline}
    -   The studentized residuals, $t$, follow a student t distribution with dof = n--k--2
        -   Rule of thumb is any $|t_i|$ \> 3 is considered an outlier but you can check the residual against the critical value to be sure.
    -   In R, `rstudent(mod)`
    -   It may be a good idea to run the regression twice --- with and without the outliers to see how much they have an effect on the results.
    -   Inflation of the MSE due to outliers will affect the width of CIs and PIs but not predicted values, hypothesis test results, or effect point estimates
    -   A Q-Q plot shows the quantiles of the studentized residuals versus fitted values

-   [Check for Normality]{.underline}

    -   Residuals vs Fitted scatter
        -   Looking for data to centered around 0
        -   Helpful to have a horizontal and vertical line at the zero markers on the X & Y axis.
    -   Residuals historgram
        -   Look for symmetry
        -   Helpful to have a gaussian overlay

-   [Check for heteroskedacity or Non-Linear Patterns]{.underline}

    -   Residuals vs Fitted scatter
        -   Heteroskedastic\
            ![](./_resources/Diagnostics,_Regression.resources/Screenshot%20(689).png){.lightbox width="432"}\
            ![](./_resources/Diagnostics,_Regression.resources/1lK2Ho8Lu_gpUXFR5bh3GPw.png){.lightbox width="432"}

            -   Subtler but present (reverse megaphone shape)\
                ![](./_resources/Diagnostics,_Regression.resources/Screenshot%20(691).png)

            -   Solutions:

                -   Log transformations of a predictior, outcome or both
                -   Heteroskedastic robust standard errors (See [Econometrics, General \>\> Standard Errors](econometrics-general.qmd#sec-econ-gen-se){style="color: green"})
                -   Generalized Least Squares (see [Regression, Other \>\> Misc](regression-other.html#sec-reg-other-misc){style="color: green"})
                -   Weighted Least Squares (see [Regression, Other \>\> Weighted Least Squares](regression-other.html#sec-reg-other-wls){style="color: green"})
                    -   Also see Feasible Generalized Least Squares (FGLS) in the same note
                    -   Example: [Real Estate \>\> Appraisal Methods \>\> CMA \>\> Market Price](https://ercbk.github.io/Domain-Knowledge-Notebook/qmd/real-estate.html#sec-rlest-apprais-ml-cmaprc-marprc){style="color: green"} \>\> Case-Shiller method
                -   Scale model (`greybox::sm` ) models the variance of the residuals or `greybox::alm` will call `sm` and fit a model with estimated residual variance
                    -   See [article](https://forecasting.svetunkov.ru/en/2022/01/23/introducing-scale-model-in-greybox/) for an example
                    -   Can be used with other distributions besides gaussian
                    -   The unknown factor or function to describe the residual variance is a problem w/WLS
                -   [{gamlss}]{style="color: #990000"} also models location, scale, and shape
                    -   Also can be used with other distributions besides gaussian

        -   Nonlinear\
            ![](./_resources/Diagnostics,_Regression.resources/1-yP8OulKEBavWukcnJLixCw.png){.lightbox width="332"}![](./_resources/Diagnostics,_Regression.resources/1-jkwegbRDZKqjnT8w9yWE6A.png){.lightbox width="332"}

        -   Breusch Pagan test (`lmtest::bptest` or `car::ncvtest` )

            -   H~0~: No heteroskedacity present
            -   `bptest` takes data + formula or lm model; ncvtest only takes a lm model

-   Check for Autocorrelation

    -   Run Durbin-Watson, Breusch-Godfrey tests: `forecast::checkresiduals(fit)`  to look for autocorrelation between residuals.
        -   Range: 1.5 to 2.5
        -   Close to 2 which means you're in the clear.

-   Check for potential variable transformations

    -   Residual vs Predictor
        -   Run every predictor in the model and every predictor that wasn't used.
        -   Should look random.
        -   Nonlinear patterns suggest non linear model should be used that variable (square, splines, gam, etc.). Linear patterns in predictors that weren't use suggest they should be used.
    -   `car::residualPlots` - Plots predictors vs residuals and performs curvature test
        -   p \< 0.05 --\> Curvature present and need a quadratic version of the variable
        -   Or `car::crPlots(model)` for just the partial residual plots
    -   [{ggeffects}]{style="color: #990000"}
        -   [Introduction: Adding Partial Residuals to Marginal Effects Plots](https://strengejacke.github.io/ggeffects/articles/introduction_partial_residuals.html)
            -   Shows how to detect not-so obvious non-linear relationships and potential interactions through visualizing partial residuals
    -   Harrell's [{rms}]{style="color: #990000"}
        -   Logistic regression example ([link](https://hbiostat.org/rmsc/lrm.html#assessment-of-model-fit))
            -   End of section 10.5; listen to audio

-   [Example]{.ribbon-highlight}: [{nullafor}]{style="color: #990000"} Non-linearity ([source](https://mansthulin.se/posts/nullabor0315/))

    ::: panel-tabset
    ## Assess Null Plots

    ![](_resources/Diagnostics,_Regression.resources/res-ex1-nullabor-panel-1.png){.lightbox width="332"}

    ``` r
    library(nullabor); library(ggplot)

    mtcars$cyl <- factor(mtcars$cyl)
    m <- lm(mpg ~ hp + wt + cyl, data = mtcars)

    m_data <- data.frame(Fitted = predict(m),
                         Residuals = residuals(m))

    lineup_residuals(m, type = 1)
    #> decrypt("Pime gvGv DY FbKDGDbY UR")

    decrypt("Pime gvGv DY FbKDGDbY UR")
    #> [1] "True data in position  5"
    ```

    -   type = 1 says create residuals vs fitted plots
        -   2 = normal Q-Q, 3 = scale-location, 4 = residuals vs leverage
    -   Styling (color, opacity) also possible within the function, but since it's a ggplot, I'm sure a theme could be applied.
    -   Plot 5 sort of sticks out with its slant and extremely whippy tail.
    -   Other plots have whippy tails and some waviness but not like plot 5
    -   `decrypt` shows us that plot 5 is our plot. This provides evidence of nonlinearity.

    ## Create Diagnostic Plot

    ![](_resources/Diagnostics,_Regression.resources/res-ex1-nullabor-linearity-1.png){.lightbox group="res-nullafor-1" width="332"}

    ``` r
    ggplot(m_data, aes(Fitted, Residuals)) +
        geom_point(size = 2) +
        geom_abline(aes(intercept = 0, slope = 0),
                    colour = "red",
                    linetype = "dashed") +
        geom_smooth(se = FALSE)
    ```

    -   Now that we have evidence of non-linearity, we can take a closer look at the shape to figure out our next steps (e.g. transformation)
    :::

## plot(fit) {#sec-diag-eg-plot .unnumbered}

-   For each fitted model object look at residual plots searching for non-linearity, heteroskedacity, normality, and outliers.
-   The correlation matrix (see bkmk in stats) for correlation and VIF score (See below) for collinearity among predictors.
    -   Also see [Regression, Linear \>\> Linear Algebra](regression-linear.qmd#sec-reg-lin-linalg){style="color: green"} for the correlation matrix and link to code
    -   Collinearity/Multicollinearity Issues:
        -   Model can't distinguish the individual effects of each predictor on the response variable. It captures the shared variance with other correlated predictors. Consequently, small changes in the data can lead to significant fluctuations in the estimated coefficients, potentially leading to misleading interpretations.
        -   Inflation of the variance of the coefficient estimates results in wider confidence intervals which makes it harder to detect significant effects.
-   If non-linearity is present in a variable run poly function to determine which polynomial produces the least cv error
-   Is heteroscedasticity is present use a square root or log transform on the variable. Not sure if this is valid with multiple regression. If one variable is transformed the others might have to also be transformed in order to maintain interpretability. \*try sandwich estimator in regtools, car, or sandwich pkgs\*
-   Outliers can be investigated further with domain knowledge or other statistical methods
-   [Example]{.ribbon-highlight}: Diamonds dataset from {ggplot2}
    -   [price \~ carat + cut]{.arg-text}

        ![](_resources/Diagnostics,_Regression.resources/plot-ex1-bad.png){.lightbox width="632"}

        -   Bad fit
        -   Residuals vs Fitted: Clear structure in the residuals, not white noise. They curve up at the left (so some non-linearity going on), plus they fan out to the right (heteroskedasticity)
        -   Scale-Location: The absolute scale of the residuals definitely increases as the expected value increases --- a definite indicator of heteroskedasticity
        -   Normal Q-Q: Strongly indicates the residuals aren't normal, but has fat tails (e.g. when they theoretically would be about 3 on the standardised scale, they are about 5 - much higher)

    -   [log(price) \~ log(carat) + cut]{.arg-text}

        ![](_resources/Diagnostics,_Regression.resources/plot-ex1-good.png){.lightbox width="632"}

        -   Good fit
        -   Residuals vs Fitted: Curved shape and the fanning has gone and we're left with something looking much more like white noise
        -   Scale-Location: Looks like solid homoskedasticity
        -   Normal Q-Q: A lot more "normal" (i.e. straight line) and apart from a few outliers the values of the standardized residuals are what you'd expect them to be for a normal distribution

## Other Diagnostics {#sec-diag-reg-othdiag .unnumbered}

-   Misc

    -   [Tutorial](https://www.nicholas-ollberding.com/post/an-introduction-to-the-harrell-verse-predictive-modeling-using-the-hmisc-and-rms-packages/) for modeling with Harrell's [{Hmisc}]{style="color: #990000"}
    -   {kernelshap} and {fastshap} can handle complex lms, glms ([article](https://www.r-bloggers.com/2022/12/interpret-complex-linear-models-with-shap-within-seconds/))
        -   Also see [Diagnostics, Model Agnostic \>\> SHAP](diagnostics-model-agnostic.qmd#sec-diag-modagn-shap){style="color: green"}

-   Check for **influential observations** - outliers that are in the extreme x-direction

-   Check VIF score for collinearity 

-   For prediction, if coefficients vary significantly across the test folds their robustness is not guaranteed (see coefficient boxplot below), and they should probably be interpreted with caution.\
    ![](./_resources/Diagnostics,_Regression.resources/sphx_glr_plot_linear_model_coefficient_interpretat.png){.lightbox width="532"}

    -   Boxplots show the variance of the coefficient across the folds of a repeated 5-fold cv.
    -   The "Coefficient importance" in the example is just the coefficient value of the standardized variable in a ridge regression
    -   Note outliers beyond the whiskers for Age and Experience
        -   In this case, the variance is caused by the fact that experience and age are strongly collinear.
    -   Variability in coefficients can also be explained by collinearity between predictors\
        ![](./_resources/Diagnostics,_Regression.resources/sphx_glr_plot_linear_model_coefficient_interpretat.1.png){.lightbox width="432"}
    -   Perform sensitivity analysis by removing one of the collinear predictors and re-running the CV. Check if the variance of the variable that was kept has stabilized (e.g. fewer outliers past the whiskers of a boxplot).

-   [`step_lincomb`](https://recipes.tidymodels.org/reference/step_lincomb.html) - Finds exact linear combinations between two or more variables and recommends which column(s) should be removed to resolve the issue. These linear combinations can create multicollinearity in your model.

    ``` r
    example_data <- tibble(
      a = c(1, 2, 3, 4),
      b = c(6, 5, 4, 3),
      c = c(7, 7, 7, 7)
    )

    recipe(~ ., data = example_data) |>
      step_lincomb(all_numeric_predictors()) |>
      prep() |>
      bake(new_data = NULL)

    # A tibble: 4 × 2
          a     b
      <dbl> <dbl>
    1     1     6
    2     2     5
    3     3     4
    4     4     3
    ```

## ML Prediction {#sec-diag-reg-mlpred .unnumbered}

-   Misc

    -   Get test predictions from tidymodels workflow fit obj

        ``` r
        preds_tbl <- wflw_fit_obj %>%
            predict(testing(splits)) %>%
            bind_cols(testing(splits), .)
        ```

-   Calibration

    -   [{]{style="color: #990000"}[tailor](https://tailor.tidymodels.org/){style="color: #990000"}[}]{style="color: #990000"} - Postprocessing of predictions for tidymodels;
        -   Creates an object class for performing calibration that can be set used with workflow objects, tuning, etc. Diagnostics of calibration seems to remain in [{probably}]{style="color: #990000"}.

        -   For numeric outcomes: calibration, truncate prediction range

        -   Truncating ranges involves limiting the output of a model to a specific range of values, typically to avoid extreme or unrealistic predictions. This technique can help improve the practical applicability of a model's outputs by constraining them within reasonable bounds based on domain knowledge or physical limitations.

        -   See [introduction](https://www.tidyverse.org/blog/2024/10/postprocessing-preview/) to the package for a calibration example

            ::: {layout-ncol="2"}
            ![Correlated Errors](_resources/Diagnostics,_Regression.resources/ml-tailor-corr-errors-1.png){.lightbox group="tail-ex-1" width="332" height="208"}

            ![Calibrated Predictions](_resources/Diagnostics,_Regression.resources/ml-tailor-calib-1.png){.lightbox group="tail-ex-1" width="332"}
            :::

-   GOF

    -   RMSE of model vs naive

        ``` r
        preds_tbl %>%
            yardstick::rmse(outcome_var, .pred)
        preds_tbl %>%
            mutate(.naive = mean(outcome_var)) %>%
            yardstick::rmse(outcome_var, .naive
        ```

-   R^2^

    ``` r
    preds_tbl %>%
        yardstick::rsq(outcome_var, .pred)
    ```

    -   Squared correlation between truth and estimate to guarantee a value between 0 and 1

-   Observed vs Predicted plot

    ``` r
    preds_tbl %>%
        ggplot(aes(outcome_var, .pred)) +
        # geom_jitter(alpha = 0.5, size = 5, width = 0.1, height = 0) + # if your outcome var is discrete
        geom_smooth()
    ```

-   Feature Importance

    -   [Example]{.ribbon-highlight}: tidymodel xgboost

        ``` r
        xgb_feature_imp_tbl <- workflow_obj_xgb %>%
            extract_fit_parsnip() %>%
            pluck("fit") %>%
            xgboost::xgb.importance(model = .) %>%
            as_tibble() %>%
            slice(1:20)

        xgb_feature_imp_tbl %>%
            ggplot(aes(Gain, fct_rev(as_factor(Feature)))) %>%
            geom_point()
        ```
