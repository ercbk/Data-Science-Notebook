# Other {#sec-reg-other .unnumbered}

## Misc {#sec-reg-other-misc .unnumbered}

-   Packages
    -   [{]{style="color: #990000"}[brtobit](https://www.zeileis.org/news/biasreduction/){style="color: #990000"}[}]{style="color: #990000"}
        -   Estimation and inference from generalized linear models using various methods for bias reduction
        -   Can be used in models with Separation (See [Diagnostics, GLM \>\> Separation](diagnostics-glm.qmd#sec-diag-glm-sep){style="color: green"})
        -   Reduction of estimation bias is achieved by solving either:
            -   The mean-bias reducing adjusted score equations in [Firth (1993)](https://doi.org/10.1093/biomet/80.1.27) and [Kosmidis & Firth (2009)](https://doi.org/10.1093/biomet/asp055)
            -   The median-bias reducing adjusted score equations in [Kenne et al (2017)](https://doi.org/10.1093/biomet/asx046)
            -   The direct subtraction of an estimate of the bias of the maximum likelihood estimator from the maximum likelihood estimates as prescribed in [Cordeiro and McCullagh (1991)](https://www.jstor.org/stable/2345592).
-   Harrell: It is not appropriate to compute a mean or run parametric regression on "% change" unless you first compute `log((%_change/100) + 1)`  to undo damage done by % change
-   Reaction time data can be modelled using several families of skewed distributions ([Lindeløv, 2019](https://lindeloev.github.io/shiny-rt/))

## Other {#sec-reg-other-other .unnumbered}

-   Rates between 0 and 1
    -   Outcome without 0s and 1s
        -   Beta Regression
    -   Outcome with 0s and 1s
        -   Ordered Beta Regression
            -   [paper](https://t.co/F6p0cK0TwB), [{ordbetareg}]{style="color: #990000"}, [{]{style="color: #990000"}[glmmTMB](https://twitter.com/bolkerb/status/1578818008474333184){style="color: #990000"}[}]{style="color: #990000"}
            -   Heiss [overview](https://stats.andrewheiss.com/compassionate-clam/notebook/ordbeta.html) of the model
            -   Heiss [example](https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/#zero-inflated-beta-regression-bayesian-style) using a 0,1 Inf Ordered Beta Regression
        -   Zero-inflated or Zero-1-inflated beta regression (ZOIB) (see [{brms}]{style="color: #990000"})
        -   If the zeros and ones are censored, use tobit.
-   Outcome variable is greater than 0
    -   Gamma Regression - Can handle some dispersion with a log link
    -   Can model multiplicative dgp
    -   If zero-inflated, use Tweedie Regression
-   Bounded Outcome Variable
    -   Ordered Beta Regression
        -   [paper](https://t.co/F6p0cK0TwB), [{ordbetareg}]{style="color: #990000"}, [{]{style="color: #990000"}[glmmTMB](https://twitter.com/bolkerb/status/1578818008474333184){style="color: #990000"}[}]{style="color: #990000"}
        -   Heiss [overview](https://stats.andrewheiss.com/compassionate-clam/notebook/ordbeta.html) of the model
        -   Heiss [example](https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/#zero-inflated-beta-regression-bayesian-style) using a 0,1 Inf Ordered Beta Regression to model an outcome with values between 1 and 32
-   Tweedie Regression - Where the frequency of events follows a Poisson distrbution and the amount associated with each event follows an Exponential distribution
    -   e.g. Insurance claims, Operational loss (banking)
    -   Tweedie distribution is a Gamma distribution with a spike at zero.
-   Generalized Least Squares
    -   Packages
        -   [{nlme::gls}]{style="color: #990000"}
    -   Math - [A Deep-Dive into Generalized Least Squares Estimation](https://towardsdatascience.com/a-deep-dive-into-generalized-least-squares-estimation-8bf5319edd7d)
    -   Also See Weighted Least Squares and Weighted Least Squares \>\> Feasible Generalized Least Squares

## Censored and Truncated Data {#sec-reg-other-censtrunc .unnumbered}

-   [**Censored Data**]{style="color: #009499"} - Arise if exact values are only reported in a restricted range. Data may fall outside this range but those data are reported at the range limits (i.e. at the minimum or maximum of the range)

    -   i.e. Instances outside the range are recorded in the data but the true values of those instances are not.
    -   It can be done purposely in order to better the model fit. From a [paper](https://arxiv.org/pdf/2209.04660v1) that models precipitation, "fitting is improved by censoring the data, so in practice we remove values $Y (x) < 0.5 \text{(mm)}$. Other thresholds ($\le 0.2$ and $\le 0.1\text{mm}$) have been tested but $Y (x) < 0.5 \text{(mm)}$ gives the best results, in particular for the upper tails."
    -   [Tobit Regression]{.underline} - regression models with a Gaussian response variable left-censored at zero, assumes constant (homoskedastic variance)
        -   Log-Likelihood function

            $$
            \ln \mathcal{L}(\beta, \sigma^2) = 
            \sum_{i=1}^N \left[d_i\left(\frac{1}{2}\ln 2\pi - \frac{1}{2}\ln \sigma^2 - \frac{1}{2\sigma^2} (y_i - \boldsymbol{x}_i'\beta)^2\right) + (1-d_i) \ln \left(1-\Phi(\frac{\boldsymbol{x}_i' \beta}{\sigma}\right)\right]
            $$

            -   $d_i = 0$ if $y_i = 0$, and $1$ otherwise
                -   1st term OLS likelihood
                -   2nd term accounts for the probability that observation i is censored.

        -   Marginal Effect\
            $$
            \frac{\partial \mathbb{E}(y|x)}{\partial x} = \beta_1 \left[\Phi \left(\frac{\beta_0 + \beta_1 x}{\sigma}\right) \right]
            $$

            -   $\beta_1$ is multiplied by the CDF ($\Phi$) of the Normal distribution
                -   $\beta_1$ is weighted by the probability of y occurring at $x$
                -   [Example]{.ribbon-highlight}: [work_completed \~ hourly rate]{.arg-text}
                    -   $\beta_1$ is weighted by the probability that an individual is [willing to work]{.var-text} at the present [hourly rate]{.var-text}, as represented by the CDF.
            -   $\sigma$ is the standard deviation of the model's residuals

        -   [Example]{.ribbon-highlight}: Right-Censored at 800

            ``` r
            VGAM::vglm(resp ~ pred1 + pred2, 
                       family = tobit(Upper = 800), 
                       data = dat)
            ```
    -   [2-Part Models]{.underline} (e.g. [Hurdle Models]{.underline}) - A binary (e.g. Probit) regression model fits the exceedance probability of the lower limit and a truncated regression model fits the value given the lower limit is exceeded.

-   [**Truncated Data**]{style="color: #009499"} - Arise if exact values are only reported in a restricted range. If data outside this range are omitted completely, we call it truncated

    -   i.e. Instances outside the range are NOT recorded. No evidence of these instances are in the data.
    -   Truncated Regression - Also assumes constant (homoskedastic variance)
    -   A poisson model will try to predict zeros even if zeros are impossible. Therefore, you need a zero-truncated model.
    -   The truncated normal model is different from a glm, because $\mu$ and $\sigma$ are not orthogonal and have to be estimated simultaneously. Misspecification of one parameter will lead to inconsistent estimation of the other. That's why for these models, not only is $\mu$ often specified as a function of regressors but also $\sigma$, often in the framework of GAMLSS (generalized additive models of location, scale, and shape).
        -   Expectation: $E[y|x] = \mu + \sigma + \frac {\phi(\mu / \sigma)}{\Phi(\mu / \sigma)}$
            -   Where $\phi(\cdot)$ and $\Phi(\cdot)$ are the probability density and cumulative distribution function of the standard normal distribution, respectively. This intrinsically depends on both $\mu$ and $\sigma$.

-   [**Heteroskadastic Variance**]{style="color: #009499"} - The variance of an underlying normal distribution does depend on covariates

    -   [{crch}]{style="color: #990000"}

-   [Examples]{.ribbon-highlight}

    -   [Insurance]{.underline}: There is a claim on a policy that has a payout limit of $u$ and a deductible of $d$,
        -   Any loss amount that is greater than $u$ will be reported to the insurance company as a loss of $u - d$ because that is the amount the insurance company has to pay.
        -   Insurance loss data is left-truncated because the insurance company doesn't know if there are values below the deductible $d$ because policyholders won't make a claim.
            -   "Truncated" because the values (claims) are below $d$, so the instances aren't recorded in the data.
            -   "Left" because the values are below $d$ and not above
        -   Insurance loss data is also right-censored if the loss is greater than $u$ because $u$ is the most the insurance company will pay. Thus, it only knows that your claim is greater than $u$, not the exact claim amount.
            -   "Censored" because the values (claims) that are exactly $u$ (policy limit) imply that claim is greater than $u$, so the instances are recorded but the true values are unknown.
            -   "Right" because the values are above $u$ and not below
    -   [Measuring Wind Speed]{.underline}: The instrument needs a minimum wind speed, $m$, to start working.
        -   If wind speeds below this minimum are recorded as the minimum value, $m$, the data is censored.
            -   i.e. Some other instrument detects a wind instance occurred and that instance is recorded as $m$ even though the true speed of the wind of that instance is unknown.
        -   If wind speeds below this minimum are NOT recorded at all, the data is truncated.
            -   i.e. Any wind instances (detected or not) are not recorded. No evidence in the data that these instances will have ever occurred.

## Fractional Regression {#sec-reg-other-fracreg .unnumbered}

-   Outcome with values between 0 and 1
-   \*\* Use a fractional logit (aka quasi-binomial) only for big data situations \*\*
    -   The fractional logit model is not a statistical distribution, leading it to produce biased results.
    -   See Kubinec [article](https://www.robertkubinec.com/post/frac_logit/)
        -   Recommends ordered beta regression, continuous bernoulli and provides examples
    -   In a big data situation, it respects the bounds of proporitional/fractional outcomes, and is significantly easier to fit than the other alternatives.
    -   Having a large dataset means that inefficiency or an incorrect form for the uncertainty of fractional logit estimates is unlikely to affect decision-making or inference.
-   Beta: values lie between zero and one
    -   See [{betareg}]{style="color: #990000"}, [{DirichletReg}]{style="color: #990000"}, [{mgcv}]{style="color: #990000"}, [{brms}]{style="color: #990000"}
    -   [{ordbetareg}]{style="color: #990000"}
-   Zero/One-Inflated Beta: larger percentage of the observations are at the boundaries (i.e. high amounts of 0s and 1s
    -   See [{brms}]{style="color: #990000"}, [{VGAM}]{style="color: #990000"}, [{gamlss}]{style="color: #990000"}
-   Logistic, Quasi-Binomial, or GAM w/robust std.errors: outcome, y, is $0 \le y \le 1$ (i.e. 0s and 1s included)
    -   [Example]{.ribbon-highlight}

        ``` r
        library(lmtest)
        library(sandwich)
        # logistic w/robust std.errors
        model_glm = glm(
          prate ~ mrate + ltotemp + age + sole,
          data = d,
          family = binomial
        )
        se_glm_robust <- coeftest(model_glm, vcov = vcovHC(model_glm, type="HC"))
        # quasi-binomial w/robust std.errors
        model_quasi = glm(
          prate ~ mrate + ltotemp + age + sole,
          data = d,
          family = quasibinomial
        )
        se_glm_robust_quasi = coeftest(model_quasi, vcov = vcovHC(model_quasi, type="HC"))

        # Can also use a GAM to get the same results
        # Useful for more complicated model specifications
        model_gam_re = gam(
          prate ~ mrate + ltotemp + age + sole + s(id, bs = 're'),
          data = d,
          family = binomial,
          method = 'REML'
        )
        ```

## Zero-Inflated and Zero-Truncated {#sec-reg-other-zeroinftrunc .unnumbered}

-   [Continuous]{.underline}
    -   Some economists will use $\ln(1 + Y)$ or $\mbox{arcsinh}(Y)$ to model a skewed, continous $Y$ with $0$s. In this case, the treatment effects (ATE) can't be interpreted as percents. The effect sizes will depend on the scale of $Y$. (see [Thread](https://twitter.com/jiafengkevinc/status/1602403378809647104))
    -   Solutions
        -   Normalize $Y$ by a pretreatment baseline

            $$
            Y' = \frac{Y}{Y_{\text{pre}}}
            $$

            -   Where $Y_{\text{pre}}$ is the measured $Y$ prior to treatment
            -   In regression, average treatment effect (ATE) would then be\
                $$
                \theta_{\bar Y} = \mathbb{E} \left[\frac{Y(1)}{Y_{\text{pre}}} - \frac{Y(0)}{Y_{\text{pre}}} \right]
                $$
                -   Where $Y(1)$ is the value of $Y$ for treated subjects
                -   Interpretation (e.g outcome = earnings): average treatment effect on *earnings* expressed as a percentage of pre-treatment *earnings*

        -   Normalizing $Y$ by the expected outcome given observable covariates

            $$
            Y' = \frac{Y}{\mathbb{E}[Y(0) \:|\: X]}
            $$

            -   $Y(0)$ are the observed outcome values for the control group
            -   The "$|X$" is kind of confusing but I don't think want the fitted values from a model where the outcome is the $Y(0)$ values. I think they'd use a $\hat Y$ somewhere.
                -   So I think $\mathbb{E}[Y(0) | X]$ just the mean of the $Y(0)$ values
                -   Interpretation of this transformed variable (e.g. outcome = earnings)
                    -   an individual's *earnings* as a percentage of the average control group's *earnings* for people with the same observable characteristics X.
            -   Average Treatment Effect (ATE) Interpretation (e.g. outcome = earnings, $X$ = pretreatment earnings, education)
                -   The average change in *earnings* as a percentage of the control group's *earnings* for people with the same *education* and *previous earnings*.
-   [ML 2-step Hurdle]{.underline}
    -   Steps
        -   Transform Target variable to 0/1 where 1 is any count that isn't a 0.
        -   Use a classifier to predict 0s (according to some probability threshold)
            -   Remember that models is predicting the probability of being a 1
        -   Filter rows that aren't predicted to be 0, and predict counts using a regressor model
            -   round-up or round-down predictions based on which results in lower error?
-   [Statistical]{.underline}
    -   Options: Poission, Neg.Binomial, Zero-Inf Poisson/Neg.Binomial, Poisson/Neg.Binomial Hurdle
-   [Zero-Inf Poisson/Neg.Binomial]{.underline}\
    ![](./_resources/Regression,_Other.resources/1-Q9dtw-96AUhEFD920nV3-w.png){.lightbox width="403"}
    -   Uses a second underlying process that determines whether a count is zero or non-zero. Once a count is determined to be non-zero, the regular Poisson process takes over to determine its actual non-zero value based on the Poisson process's PMF.
    -   ϕi is the predicted probability from a logistic regression that yi is a 0. This vector of values is then plugged into both probability mass functions.

## Multi-modal {#sec-reg-other-multmod .unnumbered}

-   Also see [EDA \>\> Interactions \>\> Categorical Outcome](eda-general.qmd#sec-eda-gen-inter-cat){style="color: green"}
-   [{upsetr}]{style="color: #990000"} - Might be useful to examine bimodal structure and determine cutpoints based on categorical predictor values and not just outcome values
-   [{]{style="color: #990000"}[gghdr](https://sayani07.github.io/gghdr/){style="color: #990000"}[}]{style="color: #990000"} - Visualization of Highest Density Regions in ggplot2
-   Test for more than one mode: `multimode::modetest(dat)`
    -   Performs Ameijeiras-Alonso excess mass test/dip statistic
    -   H~a~: More than 1 mode
-   Find location of modes: `multimode::locmodes(dat, mod0 = 2, display = TRUE)`\
    ![](./_resources/Regression,_Other.resources/1-7pU7wdm3l50-oXcFbvthig.png){.lightbox width="332"}
    -   Anti-Mode location might be a good place for a cutpoint
-   Ideas
    -   Quantile Regression
    -   Mixture Model
    -   Establish cutpoints and model each modal distribution separately
    -   ML

## Weighted Least Squares (WLS) {#sec-reg-other-wls .unnumbered}

-   OLS with Weighted Pbservations

-   Commonly used to overcome binomial or "megaphone-shaped" types of heteroskedacity of OLS residuals\

    ::: {layout-ncol="2"}
    ![](./_resources/Regression,_Other.resources/1-lJYcKOFR88JFo76QPSdvPQ.png){.lightbox group="wls-res" width="232"}

    ![](./_resources/Regression,_Other.resources/1-lK2Ho8Lu_gpUXFR5bh3GPw.png){.lightbox group="wls-res" width="232"}
    :::

-   [Misc]{.underline}

    -   Also see
        -   [Other](regression-other.qmd#sec-reg-other-other) \>\> Generalized Least Squares
        -   [Real Estate \>\> Appraisal Methods \>\> CMA \>\> Market Price](https://ercbk.github.io/Domain-Knowledge-Notebook/qmd/real-estate.html#sec-rlest-apprais-cmaprc-marprc){style="color: green"} \>\> Case-Shiller Method for an example
    -   Resources
        -   R \>\> Documents \>\> Econometrics \>\> applied-econometrics-in-r-zeileis-kleiber \>\> pg 76
    -   **Feasible Generalized Least Squares (FGLS)** seems to have advantages over WLS Allows you to find the "form of the skedastic function to use and... estimate it from the data" \* See Zeileis applied econometrics book or another econometrics book for details

-   The residual error to be minimized becomes:\
    $$
    \begin{align}
    J(\hat \beta) &= \mathbb{E} [(f(x) - y)^2]\\
    & = \sum_i^n (w_i f(x_i) - y_i)^2\\
    & = \sum_i^n (w_ix_i^T \beta - y_i)^2
    \end{align}
    $$

    -   Where $w_i$ is the weight assigned to observation $i$

-   $\hat \beta$ becomes $(X^T WX)^{-1}XY$

    -   Where $W$ is a diagonal matrix containing the weights for each observation

-   For "megaphone-shaped" and binomial types of heteroskedacity, it's common to set the weights to equal to each observation's squared residual error\
    $$
    w_i = (f(x_i)-y_i)^2
    $$

## Generalized Estimating Equations (GEE) {#sec-reg-other-gee .unnumbered}

-   Models that are used when individual observations are correlated within groups. Often used when repeated measures (panel data) for an individual are collected over time.
    -   You make a good guess on the *within-subject covariance structure.* The model averages over all subjects, and instead of assuming that data were generated from a certain distribution, it uses moment assumptions to iteratively choose the best β to describe the relationship between covariates and response.
    -   [A semiparametric method]{.underline}: while we impose some structure on the data generating process (linearity), we do not fully specify its distribution. Estimating β is purely an exercise in optimization.
    -   Limitations
        -   Likelihood-based methods are not available for usual statistical inference. GEE is a quasi-likelihood method.
        -   Unclear on how to perform model selection, as GEE is just an estimating procedure. There is no goodness-of-fit measure readily available.
        -   No subject-specific estimates; if that is the goal of your study, use a different method. (see below)
    -   Other option is Generalized Linear Mixed Model (GLMM), but GLMMs require some parametric assumptions (See [Mixed Effects, General](mixed-effects-general.qmd#sec-me-gen){style="color: green"})
        -   \*Note that the interpretations of the resulting estimates are different for GLMM and GEE\*
    -   Scenarios
        1.  You are a doctor. You want to know how much a statin drug will lower your patient's odds of getting a heart attack.
            -   GLMM answers this question
        2.  You are a state health official. You want to know how the number of people who die of heart attacks would change if everyone in the at-risk population took the stain drug.
            -   GEE answers this question. GEE estimates [population-averaged model parameters and their standard errors]{.underline}
-   [Misc]{.underline}
    -   Notes from
        -   [Generalized Estimating Equations (GEE)](https://rlbarter.github.io/Practical-Statistics/2017/05/10/generalized-estimating-equations-gee/)
    -   Packages
        -   [{]{style="color: #990000"}[gee](https://cran.r-project.org/web/packages/gee/index.html){style="color: #990000"}[}]{style="color: #990000"}: traditional implementations (only has a manual)
        -   [{]{style="color: #990000"}[geepack](https://cran.r-project.org/web/packages/geepack/index.html){style="color: #990000"}[}]{style="color: #990000"}: traditional implementations (1 vignette)
            -   Can also handle clustered categorical responses
        -   [{]{style="color: #990000"}[multgee](https://cran.r-project.org/web/packages/multgee/index.html){style="color: #990000"}[}]{style="color: #990000"}: GEE solver for correlated nominal or ordinal multinomial responses using a local odds ratios parameterization
    -   The traditional GEE implementation has severe computation challenges and may not be possible when the cluster sizes (large numbers of individuals per cluster) get too large (e.g. \>1000)
        -   Use One-Step Generalized Estimating Equations method ([article](https://www.rdatagen.net/post/2023-03-21-implementing-a-1-step-gee-with-large-cluster-sizes-in-r/) with code)
            -   Operates under the assumption of exchangeable correlation (see below)
            -   Characteristics
                -   Matches the asymptotic efficiency of the fully iterated GEE;
                -   Uses a simpler formula to estimate the \[intra-cluster correlation\] ICC that avoids summing over all pairs;
                -   Completely avoids matrix multiplications and inversions for computational efficiency
-   [Assumptions]{.underline} (similar to the assumptions for GLMs)
    -   The responses $Y_1, Y_2, \ldots , Y_n$ are correlated or clustered
    -   There is a linear relationship between the covariates and a transformation of the response, described by the link function, g.
    -   Within-cluster covariance has some structure ("working covariance")
    -   Individuals in *different* clusters are uncorrelated
-   [Covariance Structure]{.underline}
    -   Need to pick one of these working covariance structures in order to fit the GEE
    -   Types
        -   Independence: observations over time are independent)
        -   Exchangeable (aka Compound Symmetry): all observations over time have the same correlation
            -   Correlation across individuals is constant within a cluster
            -   Intra-Cluster Correlation (ICC) is the measure of this correlation.
        -   AR(1): correlation decreases as a power of how many timepoints apart two observations are
            -   Reasable if measurements taken closer together (i.e. probably more highly correlated)
        -   Unstructured: correlation between all timepoints may be different)
    -   If the wrong covariance structure is chosen, β will be estimated consistently, even if the working covariance structure is wrong. However, the standard errors computed from this will be wrong.
        -   To fix this, use Huber-White "sandwich estimator" (HC standard errors) for robustness. (See [Econometrics, General \>\> Standard Errors](econometrics-general.html#sec-econ-gen-se){style="color: green"})
            -   The idea behind the sandwich variance estimator is to use the empirical residuals to approximate the underlying covariance.
            -   Problematic if:
                -   The number of independent subjects is much smaller than the number of repeated measures
                -   The design is unbalanced -- the number of repeated measures differs across individuals
-   [Example]{.ribbon-highlight}: geepack
    -   Description: How does Vitamin E and copper level in the feeds affect the weights of pigs?

    -   Data

        ``` r
        library("geepack")
        data(dietox)
        dietox$Cu <- as.factor(dietox$Cu)
        dietox$Evit <- as.factor(dietox$Evit)
        head(dietox)
        ##     Weight      Feed Time  Pig Evit Cu Litter
        ## 1 26.50000        NA    1 4601    1  1      1
        ## 2 27.59999  5.200005    2 4601    1  1      1
        ## 3 36.50000 17.600000    3 4601    1  1      1
        ## 4 40.29999 28.500000    4 4601    1  1      1
        ## 5 49.09998 45.200001    5 4601    1  1      1
        ## 6 55.39999 56.900002    6 4601    1  1      1
        ```

        -   [Weight]{.var-text} of slaughter pigs measured weekly for 12 weeks
        -   Starting weight (i.e. the weight at week ([Time]{.var-text}) 1 and [Feed]{.var-text} = NA)
        -   Cumulated Feed Intake ([Feed]{.var-text})
        -   [Evit]{.var-text} is an indicator of Vitamin E treatment
        -   [Cu]{.var-text} is an indicator of Copper treatment

    -   Model: Independence Working Covariance Structure

        ``` r
        mf <- formula(Weight ~ Time + Evit + Cu)
        geeInd <- geeglm(mf, id=Pig, data=dietox, family=gaussian, corstr="ind")
        summary(geeInd)

        ##  Coefficients:
        ##            Estimate  Std.err    Wald Pr(>|W|)   
        ## (Intercept) 15.07283  1.42190  112.371  <2e-16 ***
        ## Time        6.94829  0.07979 7582.549  <2e-16 ***
        ## Evit2        2.08126  1.84178    1.277    0.258   
        ## Evit3      -1.11327  1.84830    0.363    0.547   
        ## Cu2        -0.78865  1.53486    0.264    0.607   
        ## Cu3          1.77672  1.82134    0.952    0.329   
        ## ---
        ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
        ## 
        ## Estimated Scale Parameters:
        ##            Estimate Std.err
        ## (Intercept)    48.28  9.309
        ## 
        ## Correlation: Structure = independenceNumber of clusters:  72  Maximum cluster size: 12
        ```

        -   [corstr="ind"]{.arg-text} is the argument for the covariance structure - See article for examples of the other structures and how they affect estimates

    -   ANOVA

        ``` r
        anova(geeInd)

        ## Analysis of 'Wald statistic' Table
        ## Model: gaussian, link: identity
        ## Response: Weight
        ## Terms added sequentially (first to last)
        ## 
        ##      Df  X2 P(>|Chi|)   
        ## Time  1 7507    <2e-16 ***
        ## Evit  2    4      0.15   
        ## Cu    2    2      0.41   
        ## ---
        ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
        ```
