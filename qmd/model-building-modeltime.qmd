# modeltime {#sec-modbld-mdltm .unnumbered}

## Misc {#sec-modbld-mdltm-misc .unnumbered}

-   Also see [Forecasting, Hierarchical/Grouped \>\> Modeltime](forecasting-hierarchical_grouped.qmd#sec-fcast-group-mdltm){style="color: green"}

-   Plotting

    ``` r
    plot_times_series(data, date_var, value_var)
    ```

-   Combining multiple models into a table

    ``` r
    model_tbl <- modeltime_table(
    Â  Â  model_arima,
    Â  Â  model_prophet,
    Â  Â  model_glmnet
    )
    ```

    -   Allows you to apply functions across multiple models

-   Accessing a fitted model object to see coefficient values, hyperparameter values, etc.

    ``` r
    model_arima <- arima_reg() %>%
    Â  Â  set_engine("auto_arima") %>%
    Â  Â  fit(value_var ~ date_var, training(splits_obj))
    model_arima
    ```

-   The fit obj shows coefficients, AICc, etc.

## Steps {#sec-modbld-mdltm-steps .unnumbered}

1.  Set-Up
2.  Recipe
3.  Specify Model(s)
4.  Fit Models
5.  Forecast on Test/Assessment Dataset
6.  Assess Performance
7.  Choose Model
8.  Forecast and Assess Performance on Holdout Dataset
9.  Refit on Whole Training Set
10. Forecast the Future

## Set-up {#sec-modbld-mdltm-setup .unnumbered}

-   Some packages

    -   tidyverse - cleaning
    -   tidymodels - model building
    -   timetk - loss metrics
    -   lubridate - ts cleaning functions
    -   stacks - ensembling

-   Parallelize/Cluster

    -   For all methods, you must also set `allow_par = TRUE`Â  in the control arg of the function

    -   Parallel

        ``` r
        parallel_start(6, .method = "parallel") # uses 6 vcpus
        parallel_stop()
        ```

    -   Spark

        ``` r
        sc <- spark_connect(master = "local")
        parallel_start(sc, .method = "spark")
        parallel_stop()
        spark_disconnect_all()
        ```

        -   For Databricks, replace "local" with "databricks"

-   Splits

    ``` r
    splits <- time_series_split(
    Â  Â  data,
    Â  Â  assess = "3 months",
    Â  Â  cumulative = TRUE
    )
    ```

    -   The above test set will be 3 months long
        -   dataset is daily so assess arg doesn't haven't to on the same scale as the dataset

-   Visualize the split

    ``` r
    splits %>%
    Â  Â  # extract cv plan from split obj
    Â  Â  tk_time_series_cv_plan() %>%
    Â  Â  plot_time_series(date_var, value_var)
    ```

## Model Specification, Fit {#sec-modbld-mdltm-modspec .unnumbered}

-   **Window mean/median forecast**

    ``` r
    median_fit <- window_reg(id = "id", window_size = 6) %>%
    Â  Â  set_engine("window_function", window_function = median) %>%
    Â  Â  fit(value ~ ., data = training(splits))
    ```

    -   Basically just calculates the median value for the window size that's specified (not rolling)

        -   The forecast is just this value repeated for the length of the forecast horizon

    -   Notes

        -   Used mostly as a baseline forecast, but with tuning, it supposedly performs pretty well for some series

    -   Args

        -   window_function - can be mean or whatever
        -   id - used for global modeling

    -   Tuning

        ``` r
        grid_tbl <- tibble(
        Â  Â  window_size = 1:12
        ) %>%
        Â  Â  create_model_grid(
        Â  Â  Â  Â  f_model_spec = "window_reg",
        Â  Â  Â  Â  id = "id",
        Â  Â  Â  Â  engine_name = "window_function",
        Â  Â  Â  Â  engine_params = list(
        Â  Â  Â  Â  Â  Â  window_function = ~ median(.)
        Â  Â  Â  Â  )
        Â  Â  )
        ```

        -   Slightly different than the normal procedure where you have to specify the window function like a lambda function

-   **Seasonal Naive**

    ``` r
    naive_fit <- naive_reg(seasonal_period = 12, id = "id") %>%
    Â  Â  set_engine("snaive") %>%
    Â  Â  fit(value ~ ., data = training(splits))
    ```

    -   Seasonal version of a naive forecast which just repeats the last value of the series for the length of the forecast horizon.

-   **auto_arima**

    ``` r
    model_arima <- arima_reg() %>%
    Â  Â  set_engine("auto_arima") %>%
    Â  Â  fit(value_var ~ date_var, training(splits_obj))
    ```

-   **prophet**

    ``` r
    model_prophet <- prophet_reg(seasonality_yearly = TRUE) %>%
    Â  Â  set_engine("prophet") %>%
    Â  Â  fit(value_var ~ date_var, training(split_obj))
    ```

    -   Seems to work better for highly seasonal or cyclical data otherwise it's ðŸ’©

-   **glmnet**

    ``` r
    model_glmnet <- linear_reg(penalty = 0.01) %>%
    Â  Â  set_engine("glmnet") %>%
    Â  Â  # date_var is daily in this example
    Â  Â  fit(value_var ~ wday(date_var, label = TRUE) +
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  month(date_var, label = TRUE) +
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # trend feature
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  as.numeric(date_var),
    Â  Â  Â  Â  training(splits_obj)
    Â  Â  )
    ```

-   **gluonts deepar**

    ``` r
    fit_deepar <- deep_ar(
    Â  Â  id = "id",
    Â  Â  freq = "M",
    Â  Â  prediction_length = 6,
    Â  Â  lookback_length = 6*3,
    Â  Â  epochs = 10
    ) %>%
    Â  Â  # set_engine("gluonts_deepar") %>%
    Â  Â  set_engine("torch") %>%
    Â  Â  fit(value ~ date + id, data = training(splits))
    ```

    -   Description
        -   Developed by Amazon; LSTM RNN
        -   Requires more data than arima or prophet
            -   Uses weighted resampling techniques to help with lower n, larger p data
        -   Handles seasonality with minimal tuning
        -   Outputs probabilitistic forecasts
            -   in the form of Monte Carlo samples
            -   can be used to develop quantile forecasts
        -   Supports non-normal likelihood functions
            -   e.g. discrete target variables
    -   Notes
        -   "gluonts_deepar" is the older style engine that uses reticulate to use the python gluonts library. "torch" uses the torch pkg which is written in R and is faster.
        -   Standardizing predictors is recommended
        -   impute missing values
            -   Recommended the imputing algorithm uses predictor variables to calculate values
        -   learning rate and encoder/decoder length are subject-specific and should be tuned manually (not sure how these are subject specific)
        -   To ensure the model is not fitting based on the index of our dependent variable in the TS, the authors suggest training the model on "empty" data prior our start period. Just use a bunch of zeros as our dependent variable.
            -   Not sure if this can be implemented in modeltime
    -   args
        -   "id" is for fitting a global model with panel data
        -   freq - A pandas timeseries frequency such as "5min" for 5-minutes or "D" for daily. Refer to [Pandas Offset Aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).
        -   prediction_length - forecast horizon
        -   lookback_length - Number of steps to unroll the RNN for before computing predictions (default: NULL, in which case context_length = prediction_length)
            -   i.e. like the number of lags to use as predictors in a ML or AR model
        -   epochs - default 5, the number of backpropagations before stopping (I think)
        -   and many more...

-   **gluonts_gp_forecaster**

    ``` r
    fit_gp_forecaster <- gp_forecaster(
    Â  Â  id = "id",
    Â  Â  freq = "M",
    Â  Â  prediction_length = 6,
    Â  Â  epochs = 30
    ) %>%
    Â  Â  set_engine("gluonts_gp_forecasster") %>%
    Â  Â  fit(value ~ date + id, data = training(splits))
    ```

    -   Gaussian Process (GP) Forecaster model
    -   Notes
    -   Args
        -   "id" is for fitting a global model with panel data
        -   freq - A pandas timeseries frequency such as "5min" for 5-minutes or "D" for daily. Refer to [Pandas Offset Aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).
        -   prediction_length - forecast horizon
        -   epochs - default 5, the number of backpropagations before stopping (I think)
        -   and many more...

-   **gluonts_deepstate**

    ``` r
    fit_deepar <- deep_state(
    Â  Â  id = "id",
    Â  Â  freq = "M",
    Â  Â  prediction_length = 6,
    Â  Â  lookback_length = 6*3,
    Â  Â  epochs = 20
    ) %>%
    Â  Â  set_engine("gluonts_deepstate") %>%
    Â  Â  fit(value ~ date + id, data = training(splits))
    ```

    -   deep learning state-space model
    -   Args
        -   "id" is for fitting a global model with panel data
        -   freq - A pandas timeseries frequency such as "5min" for 5-minutes or "D" for daily. Refer to [Pandas Offset Aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).
        -   prediction_length - forecast horizon
        -   lookback_length - Number of steps to unroll the RNN for before computing predictions (default: NULL, in which case context_length = prediction_length)
            -   i.e. like the number of lags to use as predictors in a ML or AR model
        -   epochs - default 5, the number of backpropagations before stopping (I think)
        -   and many more...

## Tune {#sec-modbld-mdltm-tune .unnumbered}

-   `create_model_grid`

    ``` r
    grid_tbl <- tibble(
    Â  Â  learn_rate = c(0.001, 0.01, 0.1)
    ) %>%
    Â  Â  create_model_grid(
    Â  Â  Â  Â  f_model_spec = boost_tree,
    Â  Â  Â  Â  engine_name = "xgboost",
    Â  Â  Â  Â  mode = "regression
    Â  Â  )
    ```

    -   Alternative

        ``` r
        grid_tbl <- dials::grid_regular(
        Â  Â  learn_rate(),
        Â  Â  levels = 3
        )

        grid_tbl %>%
        Â  Â  create_model_grid(
        Â  Â  Â  Â  f_model_spec = boost_tree,
        Â  Â  Â  Â  engine_nameÂ  = "xgboost",
        Â  Â  Â  Â  # Static boost_tree() args
        Â  Â  Â  Â  mode = "regression",
        Â  Â  Â  Â  # Static set_engine() args
        Â  Â  Â  Â  engine_params = list(
        Â  Â  Â  Â  Â  Â  max_depth = 5
        Â  Â  Â  Â  )
        Â  Â  )
        ```

    -   Creates a list of model specifications

        -   I think you could create more than 1 of these objects in order to tune multiple algorithms.Â  Then `append` the `$.models` together and feed it to `workflow_set(models= )` (see below)

        -   Example of another way

            ``` r
            models <- 
              union(snaive_grid_tbl %>% 
                      select(.models), 
                    window_grid_median_tbl %>%
                      select(.models))
            ```

            -   Combination of seasonal naive and window-median models

    -   Args

        -   f_model_spec - the parsnip model function
        -   engine_name - the parsnip model engine
        -   mode - always regression for forecasting

-   `workflow_set`

    ``` r
    wfset <- workflow_set() %<%
    Â  Â  preproc = list(
    Â  Â  Â  Â  recipe_spec),
    Â  Â  models = grid_tbl$.models,
    Â  Â  cross = TRUE
    )
    ```

    -   Instantiates workflfow
    -   Args
        -   cross = TRUE says fit each recipe to each model

-   `modeltime_fit_workflowset`

    ``` r
    # parallel_start(6)

    wkflw_fit <- wfset %>%
    Â  Â  modeltime_fit_workflowset(
    Â  Â  Â  Â  data = training(splits),
    Â  Â  Â  Â  control = control_fit_workflowset(
    Â  Â  Â  Â  Â  Â  verbose = TRUE,
    Â  Â  Â  Â  Â  Â  allow_par = TRUE
    Â  Â  Â  Â  )
    Â  Â  )

    # parallel_stop()
    ```

    -   Fits all the models with different hyperparameter combinations
    -   `parallel_start/stop` is a wrapper around `parallel::doParallel` stuff
        -   Think I'd rather use doFuture
    -   Args
        -   allow_par - turns on parallel processing

## Assessing Performance {#sec-modbld-mdltm-perf .unnumbered}

-   `modeltime_calibration`: Calculate Residuals

    ``` r
    calib_tbl <- model_table %>%
    Â  Â  modeltime_calibration(testing(split_obj), id = "id")
    ```

    -   adds residuals to model table
    -   "id" allows you to assess accuracy for each group member

-   `modeltime_accuracy` :

    ``` r
    calib_tbl %>%
    Â  Â  modeltime_accuracy(acc_by_id = TRUE) #%>%
    Â  Â  # select best model by rmse for each group
    Â  Â  # slice_min(rmse)
    ```

    -   uses the residual data to calculate mae, mape, mase, smape, rmse, rsq
    -   `acc_by_id = TRUE` assess accuracy by group id (for global models)

-   `table_modeltime_accuracy`

    ``` r
    calib_tbl %>%
    Â  Â  group_by(id)
    Â  Â  table_modeltime_accuracy()
    ```

    -   html table of results (might be interactive)

## Forecast {#sec-modbld-mdltm-fcast .unnumbered}

-   `modeltime_forecast` : makes predictions, calculates prediction intervals

    -   On the test set:

    ``` r
    # Using test set
    forcast_obj <- calibration_table %>%
    Â  Â  modeltimes_forecast(
    Â  Â  Â  Â  new_data = testing(splits_obj),
    Â  Â  Â  Â  actual_data = whole_training_dataset,
    Â  Â  Â  Â  conf_by_id = TRUE
    Â  Â  )
    ```

    -   `conf_by_id = TRUE` is for global models to get individual PIs for each group
        -   must have included an id arg in the model function

-   `modeltime_refit` : forecast the future

    ``` r
    future_forecast_tbl <- calibration_tbl %>%
    Â  Â  modeltime_refit(whole_training_set) %>%
    Â  Â  modeltime_forecast(
    Â  Â  Â  Â  h = "3 months",
    Â  Â  Â  Â  actual_data = whole_training_set
    Â  Â  )
    ```

    -   In example, dataset is daily, so horizon time scale doesn't have to match the dataset's scale.

-   `plot_modeltime_forecast`

    ``` r
    plot_modeltime_forecast(forecast_obj)
    ```
