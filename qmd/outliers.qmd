# Outliers {#sec-outliers .unnumbered}

## Misc {#sec-outliers-eda .unnumbered}

-   Also see [Anomaly Detection](anomaly-detection.qmd#sec-anomdet){style="color: green"} for ML methods
-   Resources
    -   Need to examine this article more closely, [Taking Outlier Treatment to the Next Level](https://www.r-bloggers.com/2021/07/taking-outlier-treatment-to-the-next-level/)
        -   Discusses detailed approach to diagnosing outliers , eda, diagnostics, robust regression, winsorizing, nonlinear approaches for nonrandom outliers.
    -   For Time Series, see bkmks, pkgs in time series \>\> cleaning/processing \>\> outliers

## EDA {.unnumbered}

-   IQR
    -   Observations above $q_{0.75} + (1.5 \times \text{IQR})$ are considered outliers
    -   Observations below $q_{0.25} - (1.5 \times \text{IQR})$ are considered outliers
    -   Where $q_{0.25}$ and $q_{0.75}$ correspond to first and third quartile respectively, and IQR is the difference between the third and first quartile
-   Hampel Filter
    -   Observations above $\text{median} + (3 \times \text{MAD})$ are considered outliers
    -   Observations below $\text{median} - (3 \times \text{MAD})$ are considered outliers
    -   Use `mad(vec, constant = 1)`  for the MAD

## Tests {#sec-outliers-tests .unnumbered}

-   \*\* All tests assume data is from a Normal distribution \*\*
-   See the EDA section for ways to find potential outliers to test
-   Grubbs's Test
    -   Test either a maximum or minimum point

        -   If you suspect multiple points, you have remove the max/min points above/below the suspect point. Then test the subsetted data. Repeat as necessary

    -   H~0~: There is no outlier in the data.

    -   H~a~: There is an outlier in the data.

    -   Test statistics

        $$
        G = \frac {\bar{Y} - Y_{\text{min}}}{s}G = \frac {Y_{\text{max}} - \bar{Y}}{s}
        $$

        -   Statistics for whether the minimum or maximum sample value is an outlier

    -   The maximum value is outlier if

        $$
        G > \frac {N-1}{\sqrt{N}} \sqrt{\frac {t^2_{\alpha/(2N),N-2}}{N-2+t^2_{\alpha/(2N),N-2}}}
        $$

        -   "\<" for minimum
        -   t is denotes the critical value of the t distribution with (N-2) degrees of freedom and a significance level of α/(2N).
        -   For testing either the maximum or minimum value, use a significance level of level of *α*/*N*

    -   Requirements

        -   Normally distributed
        -   More than 7 observations

    -   `outliers::grubbs.test(x, type = 10, opposite = FALSE, two.sided = FALSE)`

        -   x: a numeric vector of data values

        -   type=10: check if the maximum value is an outlier, 11 = check if both the minimum and maximum values are outliers, 20 = check if one tail has two outliers.

        -   opposite:

            -   FALSE (default): check value at maximum distance from mean
            -   TRUE: check value at minimum distance from the mean

        -   two-sided: If this test is to be treated as two-sided, this logical value indicates that.

    -   see bkmk for examples
-   Dixon's Test
    -   Test either a maximum or minimum point
        -   If you suspect multiple points, you have remove the max/min points above/below the suspect point. Then test the subsetted data. Repeat as necessary.
    -   Most useful for small sample size (usually n≤25)
    -   H~0~: There is no outlier in the data.
    -   H~a~: There is an outlier in the data.
    -   `outliers::dixon.test`
        -   Will only accept a vector between 3 and 30 observations
        -   "opposite=TRUE" to test the maximum value
-   Rosner's Test (aka generalized (extreme Studentized deviate) ESD test) Tests multiple points
    -   Avoids the problem of masking, where an outlier that is close in value to another outlier can go undetected.
    -   Most appropriate when n≥20
    -   H~0~: There are no outliers in the data set
    -   H~a~: There are up to k outliers in the data set
    -   `res <- EnvStats::rosnerTest(x,k)`
        -   x: numeric vector
        -   k: upper limit of suspected outliers
        -   alpha: 0.05 default
        -   The results of the test, `res` , is a list that contains a number of objects
    -   `res$all.stats` shows all the calculated statistics used in the outlier determination and the results
        -   "Value" shows the data point values being evaluated
        -   "Outlier" is True/False on whether the point is determined to be an outlier by the test
        -   Rs are the test statistics
        -   λs are the critical values

## Methods {#sec-outliers-meth .unnumbered}

-   Removal
    -   An option if there's sound reasoning (e.g. data entry error, etc.)
-   Winsorization
    -   A typical strategy is to set all outliers (values beyond a certain threshold) to a specified percentile of the data
    -   [Example]{.ribbon-highlight}: A 90% winsorization would see all data below the 5th percentile set to the 5th percentile, and data above the 95th percentile set to the 95th percentile.
    -   Packages
        -   ([{]{style="color: #990000"}[DescTools::Winsorize](https://andrisignorell.github.io/DescTools/reference/Winsorize.html){style="color: #990000"}[}]{style="color: #990000"})
        -   ([{]{style="color: #990000"}[datawizard::winsorize](https://easystats.github.io/datawizard/reference/winsorize.html){style="color: #990000"}[}]{style="color: #990000"})

## Models {#sec-outliers-mod .unnumbered}

-   Bayes has different distributions for increasing uncertainty
-   Isolation Forests - See [Anomaly Detection \>\> Isolation Forests](anomaly-detection.qmd#sec-anomdet-isofor){style="color: green"}
-   Support Vector Regression (SVR) - See [Algorithms, ML \>\> Support Vector Machines](algorithms-ml.qmd#sec-alg-ml-svm){style="color: green"} \>\> Regression
-   Extreme Value Theory approaches
    -   fat tail stuff (need to finish those videos)
-   Robust Regression (see bkmks \>\> Regression \>\> Other \>\> Robust Regression)
    -   [{robustbase}]{style="color: #990000"}
    -   [CRAN Task View](http://cran.nexr.com/web/views/Robust.html)
-   Huber Regression
    -   See [Loss Functions \>\> Huber Loss](loss-functions.qmd#sec-lossfun-hubloss){style="color: green"}
    -   See bkmks, Regression \>\> Generalized \>\> Huber
-   Theil-Sen estimator
    -   [{mblm}]{style="color: #990000"}
