# Outliers {#sec-outliers .unnumbered}

## Misc {#sec-outliers-eda .unnumbered}

-   Also see
    -   [Anomaly Detection](anomaly-detection.qmd#sec-anomdet){style="color: green"} for ML methods
    -   [EDA, General \>\> Outliers](eda-general.qmd#sec-eda-gen-out){style="color: green"}
    -   [Mathematics, Statistics \>\> Multivariate \>\> Depth](mathematics-statistics.qmd#sec-math-statc-multiv-depth){style="color: green"}
        -   Outlier detection for multivariate data
-   Packages
    -   CRAN [Task View](https://github.com/pridiltal/ctv-AnomalyDetection)
    -   [{]{style="color: #990000"}[ShapleyOutlier](https://cran.r-project.org/web/packages/ShapleyOutlier/index.html){style="color: #990000"}[}]{style="color: #990000"} - Multivariate Outlier Explanations using Shapley Values and Mahalanobis Distances
    -   [{]{style="color: #990000"}[robustmatrix](https://cran.r-project.org/web/packages/robustmatrix/index.html){style="color: #990000"}[}]{style="color: #990000"} ([vignette](https://arxiv.org/html/2403.03975v1)) - Robust covariance estimation for matrix-valued data and data with Kronecker-covariance structure using the Matrix Minimum Covariance Determinant (MMCD) estimators and outlier explanation using Shapley values.
        -   Examples of matrix data would be image resolution and repeated measueres (e.g different time points, different spatial locations, different experimental conditions, etc)
-   Resources
    -   Need to examine this article more closely, [Taking Outlier Treatment to the Next Level](https://www.r-bloggers.com/2021/07/taking-outlier-treatment-to-the-next-level/)
        -   Discusses detailed approach to diagnosing outliers , eda, diagnostics, robust regression, winsorizing, nonlinear approaches for nonrandom outliers.
    -   For Time Series, see bkmks, pkgs in time series \>\> cleaning/processing \>\> outliers
-   For small sample sizes with outliers
    -   "With n=36? Just do the whole analysis with and without, and see if your results are roughly pointing in the same direction both ways. With a sample size that small, "rough estimate of direction, justification for bigger study" is the only result you can really achieve anyway." ([Thread](https://bsky.app/profile/mmparker.bsky.social/post/3lcnmgrfd5c2d))

## Tests {#sec-outliers-tests .unnumbered}

-   \*\* All tests assume data is from a Normal distribution \*\*
-   See the EDA section for ways to find potential outliers to test
-   Grubbs's Test
    -   Test either a maximum or minimum point

        -   If you suspect multiple points, you have remove the max/min points above/below the suspect point. Then test the subsetted data. Repeat as necessary

    -   H~0~: There is no outlier in the data.

    -   H~a~: There is an outlier in the data.

    -   Test statistics

        $$
        G = \frac {\bar{Y} - Y_{\text{min}}}{s}G = \frac {Y_{\text{max}} - \bar{Y}}{s}
        $$

        -   Statistics for whether the minimum or maximum sample value is an outlier

    -   The maximum value is outlier if

        $$
        G > \frac {N-1}{\sqrt{N}} \sqrt{\frac {t^2_{\alpha/(2N),N-2}}{N-2+t^2_{\alpha/(2N),N-2}}}
        $$

        -   "\<" for minimum
        -   t is denotes the critical value of the t distribution with (N-2) degrees of freedom and a significance level of α/(2N).
        -   For testing either the maximum or minimum value, use a significance level of level of *α*/*N*

    -   Requirements

        -   Normally distributed
        -   More than 7 observations

    -   `outliers::grubbs.test(x, type = 10, opposite = FALSE, two.sided = FALSE)`

        -   x: a numeric vector of data values

        -   type=10: check if the maximum value is an outlier, 11 = check if both the minimum and maximum values are outliers, 20 = check if one tail has two outliers.

        -   opposite:

            -   FALSE (default): check value at maximum distance from mean
            -   TRUE: check value at minimum distance from the mean

        -   two-sided: If this test is to be treated as two-sided, this logical value indicates that.

    -   see bkmk for examples
-   Dixon's Test
    -   Test either a maximum or minimum point
        -   If you suspect multiple points, you have remove the max/min points above/below the suspect point. Then test the subsetted data. Repeat as necessary.
    -   Most useful for small sample size (usually n≤25)
    -   H~0~: There is no outlier in the data.
    -   H~a~: There is an outlier in the data.
    -   `outliers::dixon.test`
        -   Will only accept a vector between 3 and 30 observations
        -   "opposite=TRUE" to test the maximum value
-   Rosner's Test (aka generalized (extreme Studentized deviate) ESD test) Tests multiple points
    -   Avoids the problem of masking, where an outlier that is close in value to another outlier can go undetected.
    -   Most appropriate when n≥20
    -   H~0~: There are no outliers in the data set
    -   H~a~: There are up to k outliers in the data set
    -   `res <- EnvStats::rosnerTest(x,k)`
        -   x: numeric vector
        -   k: upper limit of suspected outliers
        -   alpha: 0.05 default
        -   The results of the test, `res` , is a list that contains a number of objects
    -   `res$all.stats` shows all the calculated statistics used in the outlier determination and the results
        -   "Value" shows the data point values being evaluated
        -   "Outlier" is True/False on whether the point is determined to be an outlier by the test
        -   Rs are the test statistics
        -   λs are the critical values

## Preprocessing {#sec-outliers-preproc .unnumbered}

-   Removal
    -   An option if there's sound reasoning (e.g. data entry error, etc.)
-   Winsorization
    -   A typical strategy is to set all outliers (values beyond a certain threshold) to a specified percentile of the data
    -   [Example]{.ribbon-highlight}: A 90% winsorization would see all data below the 5th percentile set to the 5th percentile, and data above the 95th percentile set to the 95th percentile.
    -   Packages
        -   ([{]{style="color: #990000"}[DescTools::Winsorize](https://andrisignorell.github.io/DescTools/reference/Winsorize.html){style="color: #990000"}[}]{style="color: #990000"})
        -   ([{]{style="color: #990000"}[datawizard::winsorize](https://easystats.github.io/datawizard/reference/winsorize.html){style="color: #990000"}[}]{style="color: #990000"})
-   Binning
    -   See [Feature-Engineering, General \>\> Continuous \>\> Binning](feature-engineering-general.qmd#sec-feat-eng-gen-cont-bin){style="color: green"}
    -   Depending on the modeling algorithm, binning can help with minimizing the influence of outliers and skewness. Beware of information loss due to too few bins. Some algorithms also don't perform well with variables with too few bins.

## Statistics {#sec-outliers-stat .unnumbered}

-   Packages

    -   [{]{style="color: #990000"}[WRS2](https://cran.r-project.org/web/packages/WRS2/index.html){style="color: #990000"}[}]{style="color: #990000"} - Robust t-tests (independent and dependent samples), robust ANOVA (including between-within subject designs), quantile ANOVA, robust correlation, robust mediation, and nonparametric ANCOVA models based on robust location measures

-   [Winsorization and Trimming]{.underline}

    -   For a skewed distribution, a Winsorized Mean (percentage of points replaced) often has *less bias* than a Trimmed Mean ([{]{style="color: #990000"}[DescTools::Winsorize](https://andrisignorell.github.io/DescTools/reference/Winsorize.html){style="color: #990000"}[}]{style="color: #990000"})

    -   For a symmetric distribution, a Trimmed Mean (percentage of points removed) often has *less variance* than a Winsorized Mean. (`mean(num_vec, trim = 0.10)`

    -   [{]{style="color: #990000"}[DescTools::YuenTTest](https://andrisignorell.github.io/DescTools/reference/YuenTTest.html){style="color: #990000"}[}]{style="color: #990000"} - Yuen t-Test For Trimmed Means

    -   Winsorized t-test

        ``` r
        mu_w <- WRS2::winmean(x, tr = 0.2)
        se_w <- WRS2::winse(x, tr = 0.2)

        t_stat <- (mean_w - mu0) / se_w
        dof <- n - 1
        p_value <- 2 * pt(-abs(t_stat), df)
        ```

-   [M-Estimators]{.underline}

    -   Applies a flexible, continuous loss function to down-weight or ignore outliers adaptively.

    -   Used in robust regression models, e.g. [{MASS::rlm}]{style="color: #990000"} which has Huber, Tukey Bi-Square, and Hampel.

        -   Note that the [k]{.arg-text} argument is the $c$ value in the Huber estimator equation

    -   Offer flexibility, particularly when:

        -   You need continuous control over outlier influence.
        -   Data exhibit mild deviations from normality.
        -   You’re dealing with regression or multivariate settings.
        -   Efficiency under normality is important.

    -   Recommendations

        -   Gaussian with mild outlers: Huber M-estimator (w/large c)
        -   Heavy Tails (e.g Cauchy, Student's t): Huber M-estimator (w/small c), or Tukey's bi-weight estimator
        -   Fat Tails: Huber M-estimator or Cauchy M-estimator
        -   Skewed (e.g. log-normal): Huber M-estimator (w/large c)

    -   Huber's M Estimator\
        $$
        \rho(u) = 
        \left\{ \begin{array}{lcl}
        \frac{1}{2}u^2 & \mbox{if}\;|u| \le c \\
        2c|u|-c^2 & \mbox{if}\;|u| \gt c
        \end{array}\right.
        $$

        -   A compromise between the mean and median.
        -   Uses a squared error for small residuals and switches to absolute deviations for large residuals
        -   $u$ is the residual difference between $y$ (your data) and $\hat u$ (your guess)
        -   $c$ is a tuning constant (also called the threshold or cutoff), which determines the point where the loss function transitions from quadratic to linear. A common choice for $c$ is 1.345, based on the asymptotic properties of the estimator under normal errors.
        -   [{]{style="color: #990000"}[DescTools::HuberM](https://andrisignorell.github.io/DescTools/reference/HuberM.html){style="color: #990000"}[}]{style="color: #990000"} - (Generalized) Huber M-estimator of location with MAD scale, being sensible also when the scale is zero where `huber` returns an error.
        -   Process (Iterative optimization methods like Newton-Raphson or gradient descent used)
            -   Initialize $\hat u$, e.g. set at the median
            -   Compute the residual vector, $u$
            -   For each residual $u_i$, apply the Huber loss function $\rho(u_i)$ based on whether $|u_i|$ is smaller or greater than the tuning constant $c$.
            -   Update $\hat u$
            -   Iterate until convergence

    -   Others

        -   Tukey's Biweight (or Bisquare) Estimator: Uses a loss function that completely disregards extreme outliers
        -   Andrews' Sine Estimator: Weighs residuals based on a sine function
        -   Least Absolute Deviations (LAD): Minimizes the sum of absolute deviations

-   [Hodges–Lehmann Estimator]{.underline}

    -   Packages: [{]{style="color: #990000"}[DescTools::HodgesLehmann](https://andrisignorell.github.io/DescTools/reference/HodgesLehmann.html){style="color: #990000"}[}]{style="color: #990000"}
    -   A robust and nonparametric estimator of a population's location parameter.
    -   Preferred for small data, symmetric distributions, and extreme outliers.
    -   For *symmetric* populations about one median, such as the Gaussian or normal distribution or the Student t-distribution, the Hodges–Lehmann estimator is a consistent and median-unbiased estimate of the population median.
        -   Has a **Breakdown Point** of 0.29, which means that the statistic remains bounded even if nearly 30 percent of the data have been contaminated.
            -   **Sample Median** is more robust with breakdown point of 0.50 for symmetric distributions, but is less efficient (i.e. needs more data).
    -   For *non-symmetric* populations, the Hodges–Lehmann estimator estimates the "pseudo–median", which is closely related to the population median (relatively small difference).
        -   The psuedo-median is defined for heavy-tailed distributions that lack a finite mean.
    -   For two-samples, it's the median of the difference between a sample from x and a sample from y.
    -   One-Variable Procedure
        -   Find all possible two-element subsets of the vector.
        -   Calculate the mean of each two-element subset.
        -   Calculate the median of all the subset means.
    -   Two-Variable Procedure
        -   Find all possible two-element subsets between the two vectors (i.e. cartesian product)
        -   Calculate difference between subsets
        -   Calculate median of differences

## Models {#sec-outliers-mod .unnumbered}

-   Bayes has different distributions for increasing uncertainty
-   Isolation Forests - See [Anomaly Detection \>\> Isolation Forests](anomaly-detection.qmd#sec-anomdet-isofor){style="color: green"}
-   Support Vector Regression (SVR) - See [Algorithms, ML \>\> Support Vector Machines](algorithms-ml.qmd#sec-alg-ml-svm){style="color: green"} \>\> Regression
-   Extreme Value Theory approaches
    -   fat tail stuff (need to finish those videos)
-   Robust Regression (see bkmks \>\> Regression \>\> Other \>\> Robust Regression)
    -   [{MASS:rlm}]{style="color: #990000"}
    -   [{robustbase}]{style="color: #990000"}
    -   [CRAN Task View](http://cran.nexr.com/web/views/Robust.html)
-   Huber Regression
    -   See [Loss Functions \>\> Huber Loss](loss-functions.qmd#sec-lossfun-hubloss){style="color: green"}
    -   See bkmks, Regression \>\> Generalized \>\> Huber
-   Theil-Sen estimator
    -   [{mblm}]{style="color: #990000"}
