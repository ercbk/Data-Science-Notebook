# General {#sec-llm-gen .unnumbered}

## Misc

::: {.callout-tip collapse="true"}
## Packages

-   Simple API Wrappers
    -   [{]{style="color: #990000"}[tidychatmodels](https://tidychatmodels.albert-rapp.de/){style="color: #990000"}[}]{style="color: #990000"} - Communicates with different chatbot vendors like openAI, mistral.ai, etc. using the same interface.
    -   [{]{style="color: #990000"}[gemini.R](https://jhk0530.github.io/gemini.R/){style="color: #990000"}[}]{style="color: #990000"} - Wrapper around Google Gemini API
    -   [{]{style="color: #990000"}[rollama](https://jbgruber.github.io/rollama/){style="color: #990000"}[}]{style="color: #990000"} - Wrapper around the Ollama API
    -   [{]{style="color: #990000"}[chatAI4R](https://cran.r-project.org/web/packages/chatAI4R/index.html){style="color: #990000"}[}]{style="color: #990000"} - Wrapper around OpenAI API
    -   [{]{style="color: #990000"}[TheOpenAIR](https://openair-lib.org/){style="color: #990000"}[}]{style="color: #990000"} - Wrapper around OpenAI models
-   Code Assistants
    -   [{]{style="color: #990000"}[chattr](https://mlverse.github.io/chattr/){style="color: #990000"}[}]{style="color: #990000"} - Code assistant for RStudio
    -   [{]{style="color: #990000"}[gander](https://simonpcouch.github.io/gander/){style="color: #990000"}[}]{style="color: #990000"} - A higher-performance and lower-friction chat experience for data scientists in RStudio and Positron–sort of like completions with Copilot, but it knows how to talk to the objects in your R environment.
        -   Brings [{ellmer}]{style="color: #990000"} chats into your project sessions, automatically incorporating relevant context and streaming their responses directly into your documents.
-   [{]{style="color: #990000"}[ellmer](https://ellmer.tidyverse.org/){style="color: #990000"}[}]{style="color: #990000"} - Supports a wide variety of LLM providers and implements a rich set of features including streaming outputs, tool/function calling, structured data extraction, and more.
-   [{]{style="color: #990000"}[ollamar](https://hauselin.github.io/ollama-r/){style="color: #990000"}[}]{style="color: #990000"} - R version of [{{]{style="color: goldenrod"}[ollama](https://github.com/ollama/ollama-python){style="color: goldenrod"}[}}]{style="color: goldenrod"} python and [{{{]{style="color: #CE3375"}[ollama](https://github.com/ollama/ollama-js){style="color: #CE3375"}[}}}]{style="color: #CE3375"} JS libraries
    -   Makes it easy to work with data structures (e.g., conversational/chat histories) that are standard for different LLMs (such as those provided by OpenAI and Anthropic).
    -   Lets you specify different output formats (e.g., dataframes, text/vector, lists) that best suit your need, allowing easy integration with other libraries/tools and parallelization via the [{httr2}]{style="color: #990000"} library.
-   [{]{style="color: #990000"}[mall](https://mlverse.github.io/mall/){style="color: #990000"}[}]{style="color: #990000"} ([Intro](https://blogs.rstudio.com/ai/posts/2024-10-30-mall/))- Text analysis by using rows of a dataframe along with a pre-determined (depending on the function), one-shot prompt. The prompt + row gets sent to an Ollama LLM for the prediction
    -   Also available in Python
    -   Features
        -   Sentiment analysis
        -   Text summarizing
        -   Classify text
        -   Extract one, or several, specific pieces information from the text
        -   Translate text
        -   Verify that something it true about the text (binary)
        -   Custom prompt
-   [{]{style="color: #990000"}[batchLLM](https://github.com/dylanpieper/batchLLM){style="color: #990000"}[}]{style="color: #990000"} - Process prompts through multiple LLMs at the same time.
    -   Uses data frames and column rows as LLM input and a new column with the text completions as the output.
    -   Supports OpenAI, Claude & Gemini.
-   [{]{style="color: #990000"}[llmR](https://github.com/bakaburg1/llmR){style="color: #990000"}[}]{style="color: #990000"} - Interface to OpenAI’s GPT models, Azure’s language models, Google’s Gemini models, or custom local servers
    -   Unified API: Setup and easily switch between different LLM providers and models using a consistent set of functions.
    -   Prompt Processing: Convert chat messages into a standard format suitable for LLMs.
    -   Output Processing: Can request JSON output from the LLMs and tries to sanitize the response if the parsing fails.
    -   Error Handling: Automatically handle errors and retry requests when rate limits are exceeded. If a response is cut due to token limits, the package will ask the LLM to complete the response.
    -   Custom Providers: Interrogate custom endpoints (local and online) and allow implementation of ad-hoc LLM connection functions.
    -   Mock Calls: Allows simulation of LLM interactions for testing purposes.
    -   Logging: Option to log the LLM response details for performance and cost monitoring
-   [{]{style="color: #990000"}[aigenflow](https://github.com/mharu997/aigenflow){style="color: #990000"}[}]{style="color: #990000"} - Enables you to create intelligent agents and orchestrate workflows with just a few lines of code, making advanced AI capabilities accessible to developers, data scientists, and researchers across diverse fields.
-   [{]{style="color: #990000"}[samesies](https://dylanpieper.github.io/samesies/){style="color: #990000"}[}]{style="color: #990000"} - A reliability tool for comparing the similarity of texts, factors, or numbers across two or more lists. The motivating use case is to evaluate the reliability of Large Language Model (LLM) responses across models, providers, or prompts
-   [{]{style="color: #990000"}[hellmer](https://dylanpieper.github.io/hellmer/){style="color: #990000"}[}]{style="color: #990000"} - Enables sequential or parallel batch processing for chat models from ellmer.
-   [{]{style="color: #990000"}[shinychat](https://posit-dev.github.io/shinychat/){style="color: #990000"}[}]{style="color: #990000"} - Shiny ui component for LLM apps
    -   [Example]{.ribbon-highlight}: Basic ([source](https://www.infoworld.com/article/3848270/genai-tools-for-r-new-tools-to-make-r-programming-easier.html))

        ``` r
        library(shiny)
        library(shinychat)

        ui <- bslib::page_fluid(
          chat_ui("chat")
        )

        server <- function(input, output, session) {
          chat <- ellmer::chat_ollama(system_prompt = "You are a helpful assistant", model = "phi4")

          observeEvent(input$chat_user_input, {
            stream <- chat$stream_async(input$chat_user_input)
            chat_append("chat", stream)
          })
        }

        shinyApp(ui, server)
        ```

        -   [More robust version](https://gist.github.com/smach/f38c61ec9a8ad4649f43cd8a134db687)
:::

-   Resources

    -   [Awesome Generative AI Data Scientist](https://github.com/business-science/awesome-generative-ai-data-scientist) - Curated list of LLM resources

-   What chatGPT is:

    -   "What would a response to this question sound like" machine![](./_resources/NLP,_LLMs.resources/image.png) Researchers build (train) large language models like GPT-3 and GPT-4 by using a process called "unsupervised learning," which means the data they use to train the model isn't specially annotated or labeled. During this process, the model is fed a large body of text (millions of books, websites, articles, poems, transcripts, and other sources) and repeatedly tries to predict the next word in every sequence of words. If the model's prediction is close to the actual next word, the neural network updates its parameters to reinforce the patterns that led to that prediction.

        Conversely, if the prediction is incorrect, the model adjusts its parameters to improve its performance and tries again. This process of trial and error, though a technique called "backpropagation," allows the model to learn from its mistakes and gradually improve its predictions during the training process. As a result, GPT learns statistical associations between words and related concepts in the data set.

        In the current wave of GPT models, this core training (now often called "pre-training") happens only once. After that, people can use the trained neural network in "**inference mode**," which lets users feed an input into the trained network and get a result. During inference, the input sequence for the GPT model is always provided by a human, and it's called a "prompt." The prompt determines the model's output, and altering the prompt even slightly can dramatically change what the model produces.Iterative prompting is limited by the size of the model's "context window" since each prompt is appended onto the previous prompt. ![](./_resources/NLP,_LLMs.resources/image.1.png) ChatGPT is different from vanilla GPT-3 because it has also been trained on transcripts of conversations written by humans. "We trained an initial model using **supervised fine-tuning**: human AI trainers provided conversations in which they played both sides---the user and an AI assistant,"

        ChatGPT has also been tuned more heavily than GPT-3 using a technique called "**reinforcement learning from human feedback**," or RLHF, where human raters ranked ChatGPT's responses in order of preference, then fed that information back into the model. This has allowed the ChatGPT to produce coherent responses with fewer confabulations than the base model. The prevalence of accurate content in the data set, recognition of factual information in the results by humans, or reinforcement learning guidance from humans that emphasizes certain factual responses.

        Two major types of falsehoods that LLMs like ChatGPT might produce. The first comes from inaccurate source material in its training data set, such as common misconceptions (e.g., "eating turkey makes you drowsy"). The second arises from making inferences about specific situations that are absent from its training material (data set); this falls under the aforementioned "hallucination" label.

        Whether the GPT model makes a wild guess or not is based on a property that AI researchers call "**temperature**," which is often characterized as a "creativity" setting. If the creativity is set high, the model will guess wildly; if it's set low, it will spit out data deterministically based on its data set. If creativity is set low, "\[It\] answers 'I don't know' all the time or only reads what is there in the Search results (also sometimes incorrect). What is missing is the tone of voice: it shouldn't sound so confident in those situations."

        In some ways, ChatGPT is a mirror: It gives you back what you feed it. If you feed it falsehoods, it will tend to agree with you and "think" along those lines. That's why it's important to start fresh with a new prompt when changing subjects or experiencing unwanted responses.

        "One of the most actively researched approaches for increasing factuality in LLMs is **retrieval augmentation**---providing external documents to the model to use as sources and supporting context," said Goodside. With that technique, he explained, researchers hope to teach models to use external search engines like Google, "citing reliable sources in their answers as a human researcher might, and rely less on the unreliable factual knowledge learned during model training." Bing Chat and Google Bard do this already by roping in searches from the web, and soon, a browser-enabled version of ChatGPT will as well. Additionally, ChatGPT plugins aim to supplement GPT-4's training data with information it retrieves from external sources, such as the web and purpose-built databases.

        Other things that might help with hallucination include, "a more sophisticated data curation and the linking of the training data with **'trust' scores**, using a method not unlike PageRank... It would also be possible to fine-tune the model to hedge when it is less confident in the response." (arstechnica [article](https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/))

-   OpenAI [models](https://platform.openai.com/docs/models/models)

-   Benchmarks

    -   Sites

        -   [LLM Evals](https://kschaul.com/llm-evals/) (Shaul)

    -   Extracting data from a .pdf or .jpg of a table ([source](https://kschaul.com/llm-evals/evals/extract-fema-incidents/))

        -   gemini-2.5-pro-preview-03-25 scored 100% accuracy
        -   Claude 3.5 and 3.7 sonnet only got 1 or the two requests correct

-   Use Cases

    -   Understanding code (Can reduce cognative load)([article](https://www.caitlinhudon.com/posts/programming-beyond-cognitive-limitations-with-ai))
        -   During code reviews or onboarding new programmers
        -   under-commented code
    -   Generating the code scaffold for a problem where you aren't sure where or how to start solving it.
    -   LLMs don't require removing stopwords during preprocessing of documents

-   Generate "Impossibility" List ([source](https://fosstodon.org/@smach@masto.machlis.com/112733952485781181))

    -   “I suggest that people and organizations keep an ‘impossibility list’ - things that their experiments have shown that AI can definitely not do today but which it can almost do. . . . When AI models are updated, test them on your impossibility list to see if they can now do these impossible tasks.” - Ethan Mollick, Gradually, then Suddenly: Upon the Threshold”

-   Cost

    -   For lower usage in the 1000's of requests per day range ChatGPT works out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests per day, open-sourced models deployed in AWS work out cheaper. ([article](https://towardsdatascience.com/llm-economics-chatgpt-vs-open-source-dfc29f69fec1), April 24th, 2023.)

-   Methods for giving chatGPT data

    -   Think you can upload a file
    -   Through prompt
        -   See bizsci video
            -   paste actual data
            -   paste column names and types (glimse() with no values)
        -   Generate a string for each row of data that contains the column name and value
            -   Example
                -   "The \<column name\> is \<cell value\>. The \<column name\> is \<cell value\>. ..."
                -   "The fico_score is 578.0. The load_amount is 6000.0. The annual income is 57643.54."

-   Evolution of LLMs\
    ![](./_resources/NLP,_LLMs.resources/image.2.png){.lightbox width="732"}

## Model Context Protocol (MCP) {#sec-llm-gen-mcp .unnumbered}

-   Allows LLMS to more easily integrate with external tools
    -   It establishes a standard interface for APIs to communicate to LLMs. This way all types of APIs and their individual patterns can be integrated into an LLM app without too much hassle.
-   Notes from [MCP (Model Context Protocol): Simply explained in 5 minutes](https://read.highgrowthengineer.com/p/mcps-simply-explained?utm_source=multiple-personal-recommendations-email&utm_medium=email&triedRedirect=true)
-   Resources
    -   [Docs](https://modelcontextprotocol.io/introduction)
    -   [Model Context Protocol servers](https://github.com/modelcontextprotocol/servers?tab=readme-ov-file) - Links to official mcp servers and community-based servers
-   Servers
    -   [R Econometrics MCP Server](https://github.com/gojiplus/rmcp/)
-   Protocol\
    ![](_resources/LLM-General.resources/mcp-protocol-1.png){.lightbox width="532"}
    -   **MCP Client:** Various clients and apps you use, like Cursor, know how to talk using the “MCP Protocol.”
    -   **MCP Servers:** Providers like Sentry, Slack, JIRA, Gmail, etc. set up **adapters** around their APIs that follow the MCP Protocol.
        -   They convert a message like “Get me the recent messages in the #alerts channel” to a request that can be sent to the Slack API
-   Server File(s) Components
    -   Defines a set of tools for the MCP Server to implement
    -   Creates the server to listen for incoming requests
    -   Defines a \`switch\` case that receives the tool call and calls the underlying external API, like the Slack API.
    -   Wire up transport between Client and Server
        -   Allows the MCP client and server to communicate more simply than standard HTTP requests. Instead, they can read from standard IO when communicating.
