# DL, General

TOC

* Misc
* Terms
* Activation Functions
* Regularization
* Reinforcement Learning



Misc

* Guide for suitable baseline models: [link](https://ludwigstumpp.com/ml-baselines)
* DL model cost [calculator](https://aipaca.ai/) ([github](https://github.com/aipaca-mlops/ML-training-cost-calculator)) ([article](https://towardsdatascience.com/how-to-estimate-the-time-and-cost-to-train-a-machine-learning-model-eb6c8d433ff7))
* Use Adam and AdamW optimizers
* Keras also provides out-of-the-box [preprocessing layers](https://keras.io/guides/preprocessing_layers/). This way, when the model is saved, the preprocessing steps will automatically be part of the model.
	* i.e. the same preprocessing steps applied in training are applied in production
	* But preprocessing steps will be wastefully repeated on each iteration through the training dataset. The more expensive the computation, the more this adds up.
* DL architectures![](./_resources/DL,_General.resources/DL-architectures.jpg)



Terms

* **Fully-Connected Layers** (aka **Linear Layers**) - connect every input neuron to every output neuron. Each neuron applies a linear transformation to the input vector through a weights matrix. As a result, all possible connections layer-to-layer are present, meaning every input of the input vector influences every output of the output vector. Three parameters define a fully-connected layer: batch size, number of inputs, and number of outputs. (see [article](https://builtin.com/machine-learning/fully-connected-layer))![](./_resources/DL,_General.resources/image.10.png)
* **Modalities**
	* unimodal models: text-only, image-only, etc.
	* multimodal models: text, image, continuous sensor data, etc.



Activation functions
![](./_resources/DL,_General.resources/image.2.png)![](./_resources/DL,_General.resources/image.1.png)![](./_resources/DL,_General.resources/image.png)

* **Activation Function**: after the node calculates the weighted sum of the input, the activation function transforms the output which is fed to the next layer.
* Misc
	* The choice of activation function has a large impact on the capability and performance of the neural network, and
	* Different activation functions may be used in different layers of the model.
		* Commonly the same activation function is used for the hidden layers and a different one for the outer layer that makes the prediction (e.g. softmax)
	* Most activation functions add non-linearity to the neural network
	* Both the sigmoid and Tanh functions can make the model more susceptible to problems during training, via the so-called vanishing gradients problem.
	* Architectures
		* Hidden layers
			* Multilayer Perceptron (MLP): ReLU activation function.
			* Convolutional Neural Network (CNN): ReLU activation function.
			* Recurrent Neural Network (RNN): Tanh and/or Sigmoid activation function
		* Outer layer
			* Regression: One node, linear activation
			* Binary Classification: One node, sigmoid activation.
			* Multiclass Classification: One node per class, softmax activation.
			* Multilabel Classification: One node per class, sigmoid activation.
* Base Types
	* **ReLU**: if the input value (x) is negative, then a value 0.0 is returned, otherwise, the value is returned
	* **Sigmoid**: Logistic function; output is 0 to 1
		* A perceptron is called the logistic regression model if the activation function is sigmoid.
		* Good practice to use a “Xavier Normal” or “Xavier Uniform” weight initialization (aka Glorot initialization)) and scale input data to the range 0-1 (e.g. the range of the activation function) prior to training.
	* **tanh**: takes any real value as input and outputs values in the range -1 to 1
		* The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0.
		* Good practice to use a “_Xavier Normal_” or “_Xavier Uniform_” weight initialization (aka Glorot initialization) and scale input data to the range -1 to 1 (e.g. the range of the activation function) prior to training.
* Comprehensive Survey and Performance Analysis of Activation Functions in Deep Learning, [paper](https://arxiv.org/abs/2109.14545)![](./_resources/DL,_General.resources/FMzd5nsWUAEhqvI.png)



Regularization

* Misc
	* Other methods
		* Data Augmentation
			* More data reduces variance
			* Computer vision: Gain data by flipping, zooming, and translating the original images
			* Digits Recognition: Gain data by mposing distortion on the images
		* Early Stopping
			* Stopping the training phase before a defined number of iterations
			* For an overfitting model, if we plot the cost function on both the training set and the validation set as a function of the iterations.
				* The training error always keeps decreasing but the validation error might start to increase after a certain number of iterations.
				* When the validation error stops decreasing, that is exactly the time to stop the training process.
				* By stopping the training process earlier, we force the model to be simpler, thus reducing overfitting.
* L1 and L2 regularization
	* Shrinks model weights
	* Regularization process
		* The weights of some hidden units become closer (or equal) to 0. As a consequence, their effect is weakened and the resulting network is simpler because it’s closer to a smaller network. As stated in the introduction, a simpler network is less prone to overfitting.
		* For smaller weights, also the input z of the activation function of a hidden neuron becomes smaller. For values close to 0, many activation functions behave linearly.
	* L1
		* Cost Function, J![](./_resources/DL,_General.resources/image.4.png)
		* L is the loss function
		* m is the number of training "examples"
		* w and b are the weight and bias terms in the output of the node
		* Regularization term![](./_resources/DL,_General.resources/image.5.png)
			* The norm of the weights, w, is![](./_resources/DL,_General.resources/image.3.png)
			* L is the number of layers
			* λ is the regularization factor
	* L2
		* Cost Function, J![](./_resources/DL,_General.resources/image.7.png)
			* See L1 for definitions of terms
		* Regularization term![](./_resources/DL,_General.resources/image.6.png)
			* The norm of the weights, w, is![](./_resources/DL,_General.resources/image.8.png)
				* n^{\[l\]} rows and n^{\[l-1\]} columns (?)
* Dropout![](./_resources/DL,_General.resources/image.9.png)
	* Randomly remove some nodes in the network
		* Performed separately for each training example.
		* Therefore, each training example might be trained on a different network.
	* Regularization Process
		* Has the effect of temporarily transforming the network into a smaller one, and we know that smaller networks are less complex and less prone to overfitting.
		* Because some of its inputs may be temporarily shut down due to dropout, the unit can’t always rely on them during the training phase. As a consequence, the hidden unit is encouraged to spread its weights across its inputs. Spreading the weights has the effect of decreasing the squared norm of the weight matrix, resulting in a sort of L2 regularization.
	* Process
		* Set a probability, p, for each node of the network.
			* Typically, the keeping probability is set separately for each layer of the neural network
			* For layers with a large weight matrix, a smaller keeping probability because, at each step, we want to conserve proportionally fewer weights with respect to smaller layers
		* During the training phase, each node has a p probability to be turned off.



Reinforcement Learning

* Involves training a smart agent that can learn to perform a goal through trial & error in an environment and at the end of the training we have an agent that can perform the goal in real life independently
* A type of machine learning problem where, rather than making a single decision, you have to make multiple sequential decisions as part of a strategy
* RL does not require any explicit labels to be provided unlike in supervised learning techniques
* Use Cases
	* Email
		* What's the most optimal time for each user as to when they'll want to read it.
		* How many is too many emails per user?
	* Dynamic paywall metering
		* Help make the decision about a tradeoff between making revenue through serving ads by allowing users to read articles for free and making revenue through subscriptions by blocking free access with a digital paywall (after a certain number of free articles), inducing the user to subscribe
* Deep Q Network (DQN)
	* a combination of the principles of deep learning and Q-learning
		* Q-learning is an algorithm of a class of RL solutions called tabular solutions which aims to learn the q-values for each state.
			* Q-value of a state is the cumulative (discounted) reward from all the states that the agent could go in the future.
			* An elegant solution for problems that have a finite state spaces such as frozen lake problem.
	* For larger state spaces, this Q-learning gets unwieldy and needs to adopt an approximate way of estimating state value and this class of solution is called ‘approximate methods’.
		* DQN is the most popular algorithm among the approximate methods.
	* In DQN, the deep learning network serves as a function approximation that estimates the value for a given state/action
	* The solution design, the algorithm and the setup would be the same for all the use cases, but the configuration of the MDP (Markov Decision Process)— the states spaces, rewards, actions to be taken would vary for each use case.
		* Example
				_States — The NL opens/clicks pattern for the last (1/2) months._
				
				_Action — 1–24 hours of the day. This could further be reduced to 12 action values with each action representing a 2 hour period when the email could be send._
				
				_Reward — +2 for a NL click, +1 for a NL open, 0 otherwise_
				



















