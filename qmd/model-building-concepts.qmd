# Concepts {#sec-modbld-conc .unnumbered}

## Misc {#sec-modbld-misc .unnumbered}

-   Packages

    -   [{]{style="color: #990000"}[multiverse](https://mucollective.github.io/multiverse/){style="color: #990000"}[}]{style="color: #990000"} - makes it easy to specify and execute all combinations of reasonable analyses of a dataset\
        ![](./_resources/Model_Building,_Concepts.resources/image.5.png)\
        ![](./_resources/Model_Building,_Concepts.resources/image.4.png)
        -   [Paper](https://osf.io/yfbwm/), [Summary](https://statmodeling.stat.columbia.edu/2023/02/28/multiverse-r-package/) of it's usage
        -   Lots of vignettes

-   Regression Workflow ([Paper](https://www.frontiersin.org/articles/10.3389/fevo.2023.1065273/full))

    ![](_resources/Model_Building,_Concepts.resources/regression-workflow-1.jpg){.lightbox width="632"}

-   Basic approach to algorithm choice\
    ![](./_resources/Model_Building,_Concepts.resources/image.3.png){.lightbox width="551"}

-   Model is performing well on the training set but much worse on the validation/test set\
    ![](./_resources/Model_Building,_Concepts.resources/image.png){.lightbox width="476"}

    -   Andrew Ng calls the validation set the "Dev Set" ðŸ™„
    -   Test: Random sample the training set and use that as your validation set. Score your model on this new validation set
        -   "Train-Dev" is the sampled validation set
        -   Possibilities
            -   Variance: The data distribution of the training set is the same as the validation/test sets\
                ![](./_resources/Model_Building,_Concepts.resources/image.1.png){.lightbox width="465" height="45"}
                -   The model has been overfit to the training data
            -   Data Mismatch: The data distribution of the training set is NOT the same as the validation/test sets\
                ![](./_resources/Model_Building,_Concepts.resources/image.2.png){.lightbox width="465" height="33"}
                -   Unlucky and the split was bad
                    -   Something maybe is wrong with the splitting function
                -   Split ratio needs adjusting. Validation set isn't getting enough data to be representative.

-   Model is performing well on the validation/test set but not in the real world

    -   Investigate the validation/test set and figure out why it's not reflecting real world data. Then, apply corrections to the dataset.
        -   e.g. distributions of your validation/tests sets should look like the real world data.
    -   Change the metric
        -   Consider weighting cases that your model is performing extremely poorly on.

-   Splits

    -   [Harrell](https://discourse.datamethods.org/t/rms-describing-resampling-validating-and-simplifying-the-model/4790/39?u=erc_bk): "not appropriate to split data into training and test sets unless n\>20,000 because of the luck (or bad luck) of the split."
    -   If your dataset is over 1M rows, then having a test set of 200K might be overkill (e.g. ratio of 60/20/20).
        -   Might be better to use a ratio of 98/1/1 for big data projects and 60/20/20 for smaller data projects
    -   link
        -   Shows that simple data splitting does not give valid confidence intervals (even asymptotically) when one refits the model on the whole dataset. Thus, if one wants valid confidence intervals for prediction error, we can only recommend either data splitting without refitting the model (which is viable when one has ample data), or nested CV.
