# Concepts {#sec-modbld-conc .unnumbered}

-   Packages

    -   [{]{style="color: #990000"}[multiverse](https://mucollective.github.io/multiverse/){style="color: #990000"}[}]{style="color: #990000"} - Makes it easy to specify and execute all combinations of reasonable analyses of a dataset\
        ![](./_resources/Model_Building,_Concepts.resources/image.5.png)\
        ![](./_resources/Model_Building,_Concepts.resources/image.4.png)
        -   [Paper](https://osf.io/yfbwm/), [Summary](https://statmodeling.stat.columbia.edu/2023/02/28/multiverse-r-package/) of it's usage

-   Regression Workflow ([Paper](https://www.frontiersin.org/articles/10.3389/fevo.2023.1065273/full))

    ![](_resources/Model_Building,_Concepts.resources/regression-workflow-1.jpg){.lightbox width="632"}

-   ML Workflow\
    ![](./_resources/Model_Building,_Concepts.resources/ml-workflow-1.png){.lightbox width="632"}

-   Sources of Data Leakage ([article](https://towardsdatascience.com/common-causes-of-data-leakage-and-how-to-spot-them-17113406f9f8))

    -   Feature Leakage - Often happens with features that are directly related to the target and exist as a result of the target event. Normally, feature leakage happens because the feature value is updated in a point in time after the target event.
        -   [Examples]{.ribbon-highlight}:
            -   You are trying to predict loan [default]{.var-text} of a certain customer and one of your features is the number of [outbound calls]{.var-text} (i.e. calls from the bank) that the customer had in past 30 days. What you donâ€™t know is that in this fictional bank the customer receives [outbound calls]{.var-text} only ***after*** they've entered into a [default]{.var-text} scenario.
            -   You are trying to predict a certain [disease]{.var-text} of a patient and are using the number of times the patient went through a specific diagnostic [test]{.var-text}. However, you later find that this [test]{.var-text} is only prescribed to people ***after*** it's been determined that they have a high likelihood of having the [disease]{.var-text}.
    -   Data Splits (See [Cross-Validation \>\> K-Fold](cross-validation.qmdf#sec-cv-kfold){style="color: green"} \>\> Misc)
        -   Distributional statistics used in transformations of the training set should be used in transformations of the validation and test sets. eg. scaling/standardization/normalization.
        -   Imputation should happen after the train/test split.
        -   Perform outlier analysis (i.e. if removing rows) only on the training set.
        -   Subsampling for class imbalance should only be on the training set. (See [Classification \>\> Class Imbalance](classification.qmd#sec-class-imbal){style="color: green"} \>\> CV)
    -   Duplicate Rows (See [EDA, General \>\> Preprocessing](eda-general.qmd#sec-eda-gen-bascln){style="color: green"})
    -   Checks
        -   Fit an extremely shallow decision tree and check if one of the features has an enormous difference, in terms of importance, compared to others
        -   If there is a time stamp, you might be able to determine if certain feature values (indicator or counts) occur very close to the event time (e.g. churn, loan default).

-   Make ML model pipelines reusable and reproducible\
    ![](_resources/Model_Building,_Concepts.resources/modular-version-consist-1.webp){.lightbox width="432"}

    -   Notes from [7 Tips to Future-Proof Machine Learning Projects](https://towardsdatascience.com/7-tips-to-future-proof-machine-learning-projects-582397875edc)
    -   [Modularization]{.underline} - Useful for debugging and iteration
        -   Don't used declarative programming. Create functions/classes for preprocessing, training, tuning, etc., and keep in separate files. You'll call these functions in the main script
            -   Helper function

                ``` python
                ## file preprocessing.py ##
                def data_preparation(data):
                    data = data.drop(['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am'], axis=1)
                    numeric_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']
                    data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())
                    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)
                    return data
                ```

            -   Main script

                ``` python
                from preprocessing import data_preparation 
                train_preprocessed = data_preparation(train_data)
                inference_preprocessed = data_preparation(inference_data)
                ```
        -   Keep parameters in a separate config file
            -   Config file

                ``` python
                ## parameters.py ##
                DROP_COLS = ['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am']
                NUM_COLS = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']
                ```

            -   Proprocessing script

                ``` python
                ## preprocessing.py ##
                from parameters import DROP_COLS, NUM_COLS
                def data_preparation(data):
                    data = data.drop(DROP_COLS, axis=1)
                    data[NUM_COLS] = data[NUM_COLS].fillna(data[NUM_COLS].mean())
                    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)
                    return data
                ```
    -   [Versioning Code, Data, and Models]{.underline} - Useful for investigating drift
        -   See tools like DVC, MLFlow, Weights and Biases, etc. for model and data versioning
            -   Important to save data snapshots throughout the project lifecycle, for example: raw data, processed data, train data, validation data, test data and inference data.
        -   Github and dbt for code versioning
    -   [Consistent Structures]{.underline} - Consistency in project structures and naming can reduce human error, improve communication, and just make things easier to find.
        -   Naming examples:

            -   `<model-name>-<parameters>-<model-version>`

            -   `<model-name>-<data-version>-<use-case>`

        -   [Example]{.ribbon-highlight}: Reduced project template based on [{{]{style="color: goldenrod"}[cookiecutter](https://drivendata.github.io/cookiecutter-data-science/){style="color: goldenrod"}[}}]{style="color: goldenrod"}

            ```         
            â”œâ”€â”€ data
            â”‚   â”œâ”€â”€ output      <- The output data from the model. 
            â”‚   â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
            â”‚   â””â”€â”€ raw            <- The original, immutable data dump.
            â”‚
            â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
            â”‚
            â”œâ”€â”€ notebooks          <- Jupyter notebooks. 
            â”‚
            â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
            â”‚   â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
            â”‚
            â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
            â”‚                         generated with `pip freeze > requirements.txt`
            â”‚
            â”œâ”€â”€ code              <- Source code for use in this project.
                â”œâ”€â”€ __init__.py    <- Makes src a Python module
                â”‚
                â”œâ”€â”€ data           <- Scripts to generate and process data
                â”‚   â”œâ”€â”€ data_preparation.py
                â”‚   â””â”€â”€ data_preprocessing.py
                â”‚
                â”œâ”€â”€ models         <- Scripts to train models and then use trained models to make
                â”‚   â”‚                 predictions
                â”‚   â”œâ”€â”€ inference_model.py
                â”‚   â””â”€â”€ train_model.py
                â”‚
                â””â”€â”€ analysis  <- Scripts to create exploratory and results oriented visualizations
                    â””â”€â”€ analysis.py
            ```

-   [Sparse Data Representation]{.underline}

    -   Also see
        -   [Model Building, tidymodels \>\> Misc](model-building-tidymodels.qmd#sec-modlbld-tidymod-misc){style="color: green"} \>\> Utilizing a sparse matrix
        -   [Mathematics, Linear Algebra \>\> Misc](mathematics-linear-algebra.qmd#sec-math-linalg-misc){style="color: green"} \>\> [{sparsevctrs}]{style="color: #990000"}
    -   Estimates, model performance metrics, etc. are unaffected
    -   Advantages
        -   Speed is gained from any specialized model algorithms built for sparse data.
        -   The amount of memory this object requires decreases dramatically.
            -   e.g. A dataset as a sparse matrix (transformed via {Matrix}) that's been saved to a rds file takes up around 1MB compressed, and around 12MB once loaded into R. If it were turned into a dense matrix, it would take up 3GB of memory. ([source](https://www.tidymodels.org/learn/work/sparse-matrix/#example-data))
    -   [Example]{.ribbon-highlight}: `c(0, 0, 1, 0, 3, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)`
        -   It could be represented sparsely using the 3 values `positions = c(1, 3, 7)`, `values = c(3, 5, 8)`, and `length = 20`.

-   Model is performing well on the training set but much worse on the validation/test set\
    ![](./_resources/Model_Building,_Concepts.resources/image.png){.lightbox width="476"}

    -   Andrew Ng calls the validation set the "Dev Set" ðŸ™„
    -   Test: Random sample the training set and use that as your validation set. Score your model on this new validation set
        -   "Train-Dev" is the sampled validation set
        -   Possibilities
            -   Variance: The data distribution of the training set is the same as the validation/test sets\
                ![](./_resources/Model_Building,_Concepts.resources/image.1.png){.lightbox width="465" height="45"}
                -   The model has been overfit to the training data
            -   Data Mismatch: The data distribution of the training set is NOT the same as the validation/test sets\
                ![](./_resources/Model_Building,_Concepts.resources/image.2.png){.lightbox width="465" height="33"}
                -   Unlucky and the split was bad
                    -   Something maybe is wrong with the splitting function
                -   Split ratio needs adjusting. Validation set isn't getting enough data to be representative.

-   Model is performing well on the validation/test set but not in the real world

    -   Investigate the validation/test set and figure out why it's not reflecting real world data. Then, apply corrections to the dataset.
        -   e.g. distributions of your validation/tests sets should look like the real world data.
    -   Change the metric
        -   Consider weighting cases that your model is performing extremely poorly on.

-   Splits

    -   [Harrell](https://discourse.datamethods.org/t/rms-describing-resampling-validating-and-simplifying-the-model/4790/39?u=erc_bk): "not appropriate to split data into training and test sets unless n\>20,000 because of the luck (or bad luck) of the split."
    -   If your dataset is over 1M rows, then having a test set of 200K might be overkill (e.g. ratio of 60/20/20).
        -   Might be better to use a ratio of 98/1/1 for big data projects and 60/20/20 for smaller data projects
    -   link
        -   Shows that simple data splitting does not give valid confidence intervals (even asymptotically) when one refits the model on the whole dataset. Thus, if one wants valid confidence intervals for prediction error, we can only recommend either data splitting without refitting the model (which is viable when one has ample data), or nested CV.

-   Seeds

    -   Notes from [Fine-grained control of RNG seeds in R](https://blog.djnavarro.net/posts/2023-12-27_seedcatcher/)

    -   Functions can change the state of the RNG

        ``` r
        set.seed(1)
        state <- .Random.seed # <1>

        sample(10)
        #> [1]  9  4  7  1  2  5  3 10  6  8

        sample(10) # <2>
        #> [1]  3  1  5  8  2  6 10  9  4  7

        .Random.seed <- state
        sample(10) # <3>
        #> [1]  9  4  7  1  2  5  3 10  6  8
        ```

        1.  Store the state of the RNG after setting it with `set.seed`
        2.  After calling `sample` the second time, we see it outputs a different permutation. This is because each time `sample` is called it changes the state of the RNG.
        3.  After setting the RNG to it's original state, `sample` outputs the orignial permutation

    -   To keep the same seed for a function or chunk of code, wrap it in `withr::with_seed`

        ``` r
        set.seed(1)
        sample(10)
        #> [1]  9  4  7  1  2  5  3 10  6  8
        sample(10)
        #> [1]  3  1  5  8  2  6 10  9  4  7

        withr::with_seed(1,
                         sample(10)
        )
        #> [1]  9  4  7  1  2  5  3 10  6  8

        withr::with_seed(1,
                         {
                           sample(10)
                           sample(10)
                         }
        )
        #> [1]  3  1  5  8  2  6 10  9  4  7
        ```

        -   If you wrap a chunk of code, be aware if you call more than one function that uses the RNG, you're back in the original situation. `with_seed` does *not* freeze the RNG state for each function called in the chunk.

-   Online Learning

    -   Models that operate on data streams
    -   Packages
        -   [{{]{style="color: goldenrod"}[river](https://riverml.xyz/latest/){style="color: goldenrod"}[}}]{style="color: goldenrod"} - Online machine learning
        -   [{{]{style="color: goldenrod"}[deep-river](https://github.com/online-ml/deep-river){style="color: goldenrod"}[}}]{style="color: goldenrod"} ([JOSS](https://joss.theoj.org/papers/10.21105/joss.07226)) - Online deep learning
    -   Reactive Data Streams - Streams that you don't have control of.
        -   e.g. A user visits your website. That's out of your control. You have no influence on the event. It just happens and you have to react to it.
    -   Proactive Data Streams - Streams that you have control of.
        -   e.g. Reading the data from a file. You decide at which speed you want to read the data, in what order, etc.
    -   The challenge for traditional model training is to ensure the models that you train offline on proactive datasets will perform correctly in production on reactive data streams.
    -   An online model is trained one sample at a time. It's a stateful, dynamic object that keeps learning and doesn't have to revisit past data.
    -   Online model evaluation involves learning and inference in the same order as what would happen in production. If you know the order in which your data arrives, then you can process it the exact same order. This allows you to replay a production scenario and evaluate your model.
    -   Online models adapt to concept drift in a seamless manner. As opposed to batch models which have to be retrained from scratch.
