# Experiments, Design

TOC

* Misc
* General
	* Compliance
	* Effects
	* Types
* Factorial Designs
* Observational
* Quasi-ExperimentaL
* Randomized Complete Block Design (RCBD)



Misc

* Check randomization procedure by testing for pairwise associations between the treatment variable and the adjustment variables. If independence is rejected (pval < 0.05), then randomization failed. (also see [Experiments, A/B Testing](Experiments, A_B Testing) >> Terms >> A/A Testing)
	* treatment vs numeric - 2 sample t-tests
	* treatment vs character - chisq test

****

General
![](./_resources/Experiments,_Design.resources/image.png)

* Preference of experiment in measuring causality where RCT is the most desirable
* To adequately guide decision making by all stakeholders, report estimates of both the intention-to-treat effect and the per-protocol effect, as well as methods and key conditions underlying the estimation procedures.
* **Compliance -** refers to how observations respond to treatment.
	* Full compliance with treatment means that all units to whom a program (i.e. treatment) has been offered actually enroll, and none of the control (aka comparison) units receive the program
	* Exclusion of some randomized subjects can be justified (such as patients who were deemed ineligible after randomization or certain patients who never started treatment) when analyzing the results of an experiment.
	* Types
		* Always-Takers: A person who receives the treatment regardless of the assignment (or instrument in IV).
		* Never-Takers: A person who never receives the treatment regardless of the assignment (or instrument in IV).
		* Compliers: A person who receives the treatment only when assigned to the treatment group (or when the instrument compels them to).
		* Defiers: A person who receives the treatment only when they are NOT assigned to the treament group (or the instrument does not compel them to). They always do the opposite of what the assignment mechanism instructs them to do (assholes). Usually a reasonable assumption to make that there are none in your (quasi-) experiment, but it can only be made based on intuition 
	* [Example]{.ribbon-highlight}:

receiving additional years of Education is the treatment; whether an educational Reform was implemented in their region is the instrument
![](./_resources/Experiments,_Design.resources/screenshot-121.png)

* A person who decides to stay in school regardless of whether there's a government policy to do so would be an Always-Taker (EDUCATION=1, regardless of REFORM )

* **Effects**
	* Comparison![](./_resources/Experiments,_Design.resources/Screenshot (1048).png)
		* ATE is for the whole population (he should've had the arrow coming out of the word, population, or the edge of the circle)
		* Circle is split into half: treatment (upper left, yellow), control/comparison (bottom right, pink)
			* ATT is for treated (treated compliers, always takers, treated defiers)
			* ATU is for control/comparison (non-treated compliers, never takers, non-treated defiers)
		* LATE is for treated compliers
		* CATE is for a subset of the population (e.g. men)
	* **Heterogeneous Treatment Effect (HTE)** - Also called differential treatment effect, includes difference of means, odds ratios, and Hazard ratios for time-to-event outcome vars
		* Ascertaining subpopulations for which a treatment is most beneficial (or harmful) is an important goal of many clinical trials.
		* Outcome heterogeneity is due to wide distributions of baseline prognostic factors. When strong risk factors exist, there is hetergeneity in the outcome variable.
			* Solution: add baseline predictors to your model that account for these strong risk factors.
		* Heterogeneity of Treatment Effects  - The degree to which different treatments have differential causal effects on each unit.
		* Examples
			* The effect of attending college on earnings differs across students
			* The effect of a state-wide smoking ban on smoking rates varies across states
	* **Intention-to-Treat** **(ITT)** - estimates the difference in outcomes between the units assigned to the treatment group and the units assigned to the control (aka **comparison group** in quasi-experiments) group, irrespective of whether the units assigned to the treatment group actually receive the treatment.
			An intention-to-treat analysis is not feasible if trial participants are lost to follow-up
			
		* Potential solution:  weighted average of the outcomes of participants and non-participants in the treatment group compared with the average outcome of the control group
		* [Example]{.ribbon-highlight}: Doctor tells everyone in a treatment group to go home and exercise for an hour per day and tell the control group nothing.
			* After a month, if you just compare the difference in mean blood pressures between the two groups, you get the intention to treat estimator
			* Doesn't tell you the causal effect of exercise on blood pressure, but the causal effect of telling people to exercise on blood pressure.
				* This estimate would be smaller than the treatment effect of exercise per se, as only a (small!) fraction of people in the treatment group would completely follow the treatment
	* **Modified Intention-to-Treat (mITT)** \- No ineligible users. This applies to cases where we detect the ineligibility after assignment, but the eligibility criteria are based on factors that could have been known before the experiment. Hence, it should be safe to exclude the ineligible users after the fact
		* e.g. bots and existing users should increase the observed effect size, but not change the preferred variant.
	* **Modified Intention-to-Treat No Crossovers (mITTnc)**. If we have a mechanism to detect some crossovers, excluding them and comparing the results to the intention-to-treat analysis may uncover implementation bugs.
		
		* **Crossovers** are users that experience both the treatment and control exposures or (unintentionally) more than one treatment
		
		* It’s worth noting that crossovers shouldn’t occur in cases where we can uniquely identify users at all stages of the experiment – it is a problem that is more likely to occur when dealing with anonymous users.
		* As such, and given the inability to detect all crossovers, A/B experiments should be avoided when users are highly motivated to cross over.
			* Example: displaying different price levels based on anonymous and transient identifiers like cookies is often a bad idea.
	* **Average Treatement Effect (ATE)** -  expected causal effect of the treatment across all individuals in the population
		* OLS estimate, Yi = β0 + β1Xi + ui
			* β1 = ATE
				      = E\[Y |X = 1\] − E\[Y |X = 0\]
				      = E\[β1,i \] = Average effect of a unit change in X
				
		* **Conditional Average Treatment Effect (CATE)** - ATE for a subgroup
			* Coefficient for an interaction (e.g. explanatory\*treatment)
				* Also see [Generalized Additive Models (GAM)](Generalized Additive Models (GAM)) >> Interactions
		* **Average Treatment Effect on the Treated** **(ATT)** \- expected causal effect of the treatment for individuals in the treatment group
			ATT = E\[δ | D = 1\] = E\[Y1 − Y0 | D = 1\]
			 = E\[Y1 | D = 1\] − E\[Y0 | D = 1\]
			
			* where δ: individual-level causal effect of the treatment and D is the treatment
			
			* In the ideal scenario of a randomized control trial (RCT) (commonly violated in observational studies), ATE equals ATT because we assume that:
				* the baseline of the treatment group equals the baseline of the control group (layman terms: people in the treatment group would do as _bad_ as the control group if they were not treated) and
				* the treatment effect on the treated group equals the treatment effect on the control group (layman terms: people in the control group would do as _good_ as the treatment group if they were treated).
			* ATT should be used instead of ATE when there's extreme imbalance between covariate criteria of treated vs control/comparison groups (e.g. quasi-experiment)![](./_resources/Experiments,_Design.resources/Screenshot (1043).png)
				* Also see [Econometrics, General](Econometrics, General) >> Propensity Scoring
				* Overlap plot or balance plot from [video](https://www.youtube.com/watch?v=Z4p38TYdmZs&list=PLlzRFZmxVl9Qg2_k3gvzSI9qE0sP93-tZ&index=20)
					* [{cobalt}]{style='color: #990000'} may provide a way generate these
					* y-axis: count, x-axis: covariate, color: treatment
				* The range of x covered by blue (treatment) is much smaller than the range of x covered by red (control), therefore ATT might be a better choice of estimated effect
	* **Local Average Treatment Effect** **(LATE)** - applies when there is noncompliance in the treatment group, comparison group, or both simultaneously.
		
		* If there is noncompliance in both the treatment and comparison group, then the LATE estimate is valid only for those in the treatment group (who enrolled in the program; i.e. treated) and (who would have not enrolled had they been assigned to the control/comparison group).
		* "who would have not enrolled had they been assigned to the comparison group" is a weird counterfactual
		* "Local" indicates that LATE is the average effect for the group known as compliers
		* Treatment and Instrument are binary variables
			* IV models still valid for treatments and instruments with more than 2 levels, but effect calculation is more complicated
		* Calculation (always-takers and defiers are assumed not to exist)
			* LATE =
				(avg potential outcome of compliers who _do_ receive treatment) -
				(avg potential outcome of compliers who _don't_ receive treatment)
				
			* The (avg potential outcome of compliers who _don't_ receive treatment) has to be solved for.
				* Given that we know the proportions and outcomes for the compliers and never-takers in our treatment group, you can solve a simple equation for this quantity.
				* See [video](https://www.youtube.com/watch?v=rvdndvUZUFk) for details
			* Think this is the primary estimate of an IV model as well (see [Econometrics, General](Econometrics, General) >> Instrumental Variables)
		
		* **Treatment-on-the-treated** **(ToT)** is simply a LATE in the more specific case when there is noncompliance only in the treatment group. Estimates the difference in outcomes between the units that _actually_ receive the treatment and the comparison group (Seems similar to ATT)
	* **Per-Protocol Effect** **(PPE)** - the effect of receiving the assigned treatment strategies throughout the follow-up as specified in the study protocol
		* i.e. the effect that would have been observed if all patients had adhered to the protocol of the RCT
		* Alternative to the intention-to-treat effect that is not affected by the study-specific adherence to treatment
		* Valid estimation of the per-protocol effect in the presence of imperfect adherence generally requires untestable assumptions
		* Approaches below are generally _invalid_ to estimate the per-protocol effect. (G-estimation and instrumental variable methods can sometimes be used to estimate some form of per-protocol effects even in the presence of unmeasured confounders)
			* (biased) Approaches:
				* As-Treated: Compare the outcomes of those who took treatment (A=1) and didn't take the the treatment (A=0) regardless of their assignment
					* Pr\[Y=1|A=1\] − Pr\[Y=1|A=0\]
				* Per-Protocol: Compare the outcomes of those who took treatment (A=1) among those assigned to Treatment (Z=1) to those who didn't take the treatment (A=0) among those assigned to Control (Z=0)
					* Pr\[Y=1|A=1, Z=1\] − Pr\[Y=1|A=0, Z=0\].)
* **Types**
	* RCT
		* Cannot return valid causal estimates of the treatment effect at the participant level, but it can return a valid causal estimate of the average treatment effect (ATE), in the population
			* Approaches for estimating  the ATE
				* Change-Score model (see [Experiments, RCT](Experiments, RCT) >> Change Score Model)
				* ANCOVA model (see [ANOVA](ANOVA note) \>> ANCOVA)
		* Typical procedure
			* recruit participants from the target population,
			* measure the outcome variable during a pre-treatment assessment,
			* randomly assign participants into
				* a control condition or
				* an experimental treatment condition,
			* treat the participants in the treatment condition, and
			* measure the outcome variable again at the conclusion of treatment.
		* Reasons for not running a RCT
			1. It’s just not technically feasible to have individual-level randomization of users as we would in a classical A/B test
				* e.g. randomizing which individuals see a billboard ad is not possible
			2. We can randomize but expect interference between users assigned to different experiences, either through word-of-mouth, mass media, or even our own ranking systems; in short, the [stable unit treatment value assumption](https://en.wikipedia.org/wiki/Rubin_causal_model#Stable_unit_treatment_value_assumption_(SUTVA)) (SUTVA) would be violated, biasing the results
	* Quasi-Experiemental
		* Due to the lack of a random assignment, the treatment and control groups are not equivalent before the intervention. So, any differences from these two groups could be caused by the pre-existing differences.
		* Example
			* Randomly choose some cities within which to show billboards and other cities to leave without.
			* We can look for changes in the test regions at-specific-times as compared to the control regions at-specific-times.
			* Since random changes happen all the time, we need to look historically to figure out what kinds of changes are normal so we can identify the impact of our test.
			* Because groups of individuals are assigned based on location rather than assigning each individual at random, and without the individual randomization there is a much larger chance for imbalance due to skewness and heterogeneous differences.
		* Types
			* Difference-in-Differences, Regression Discontinuity Design, Synthetic Control Method, Interrupted Time Series
	* Observational
		* Types
			* Matching, Propensity Score Matching, Propensity Score Stratification, Inverse Probability of Treatment Weighting, and Covariate Adjustment



Factorial Designs (aka Multifactorial Designs)

* Two or more independent variables that are qualitatively different
	* Each has two or more levels
* Notation
	* Described in terms of number of IVs and number of levels of each IV
	* Example 2 X 2 X 3
		* 3 IVs
			* 2 with 2 levels and 1 with 3 levels
		* Results in 12 conditions
* Flavors
	* Between-subjects: different subjects participating in each cell of the matrix
	* Within-subjects: the same subjects participating in each cell of the matrix
	* Mixed: a combination where one (or more) factor(s) is manipulated between subjects and another factor(s) is manipulated within subjects![](./_resources/Experiments,_Design.resources/Screenshot (501).png)
	* Combined/Expericorr![](./_resources/Experiments,_Design.resources/Screenshot (502).png)
		
		* In this example both depressed and non-depressed categories are between-subjects & non-experimental
			* I think experimental/non-experimental terminology is the same as manipulated/measured
		* Believe the no
		
		* An experimental design that includes one or more manipulated independent variables and one or more preexisting participant variables that are measured rather than manupulated
		* Sometimes participant continuous variables are dicotomized to keep a strict factorial design but this may bias the results by missing effects that are actually present or obtaining effects that are statistical artifacts. (Should just use multivariable regression instead)
			* Median-split procedure – participants who score below the median on the participant variable are classified as low, and participants scoring above the median are classified as high
			* Extreme groups procedure – use only participants who score very high or low on the participant variable (such as lowest and highest 25%)
		* Use cases
			* Determine whether effects of the independent variable generalize only to participants with particular characteristics
			* Examine how personal characteristics relate to behavior under different experimental conditions
			* Reduce error variance by accounting for individual differences among participants



Observational

* Matching and Propensity Score Matching
* Propensity Score Stratification
* Inverse Probability of Treatment Weighting
* Covariate Adjustment



Quasi-Experimental

* Typical Preconditions
	* The treated group looks like the control group (similarity for comparability);
	* A sufficiently large number of observations within each group (a large n)
* Randomizing at the lowest level possible
	Notes from: [Key Challenges with Quasi Experiments at Netflix](https://netflixtechblog.com/key-challenges-with-quasi-experiments-at-netflix-89b4f234b852)
	* Description: RCTs require you to randomize similar units (e.g. individual people) into treatment and control groups. If this isn't possible at the individual level, then randomizing at the lowest level possible is the closest, next best thing.
	* Example
		* Netflix: Measure the impact of TV or billboard advertising on member engagement. It is impossible to have identical treatment and control groups at the member level as we cannot hold back individuals from such forms of advertising. Randomize our member base at the smallest possible level. For instance, TV advertising can be bought at TV media market level only in most countries. This usually involves groups of cities in closer geographic proximity.
	* Problems
		* small sample sizes
			* e.g. If randomizing by geographical units, there are probably not too many of these
		* high variation and uneven distributions in treatment and control groups due to heterogeneity across units
			* e.g. London with its high population is randomly assigned to the treatment cell, and people in London love sci-fi much more than other cities. London's love for sci-fi would result in an overestimated effect.
	* Solutions
		* repeated randomizations (aka re-randomization)
			* keep randomizing until we find a randomization that gives us the maximum desired level of balance on key variables across treatment cells
			* Some problems still remain
				1. Can only simultaneously balance on a limited number of observed variables, and it is very difficult to find identical geographic units on all dimensions
				2. Can still face noisy results with large confidence intervals due to small sample size
		* Implement designs involving _multiple interventions_ in each treatment cell over an extended period of time whenever possible (i.e. instead of a typical experiment with single intervention period).
			* This can help us gather enough evidence to run a well-powered experiment even with a very small sample size. Large amounts of data per treatment cell increases the power of the experiment.
		* Use a Bayesian Dynamic Linear Model (DLM) to estimate the treatment effect
			* uses a multivariate structure to analyze more than a single point-in-time intervention in a single region.
			* dlm PKG (see bkmks)



Randomized Complete Block Design (RCBD)

* notes from https://www.r-bloggers.com/2020/12/accounting-for-the-experimental-design-in-linear-nonlinear-regression-analyses/
* The defining feature is that each block sees each treatment exactly once
* Running a linear regression analysis without taking into account the correlation within blocks
	* Any block-to-block variability goes into the residual error term, which is, therefore, inflated.
	* Taking the mea
* Advantages
	* Generally more precise than the completely randomized design (CRD).
	* No restriction on the number of treatments or replicates.
	* Some treatments may be replicated more times than others.
	* Missing plots are easily estimated.
* Disadvantages
	* Error degrees of freedom is smaller than that for the CRD (problem with a small number of treatments).
	* Large variation between experimental units within a block may result in a large error term
	* If there are missing data, a RCBD experiment may be less efficient than a CRD
* Steps
	1. Choose the number of blocks (minimum 2) – e.g. 4
		* The number of blocks is the number of "replications"
	2. Choose treatments (assign numbers or letters for each) – e.g. 6 trt – A,B, C, D, E, F
		* Treatments are assigned at random within blocks of adjacent subjects, each treatment once per block.
		* Any treatment can be adjacent to any other treatment, but not to the same treatment within the block
	3. Randomize the treatments and blocks
		* Example

```
Obs block trt
1 2 B
2 2 C
3 2 A
4 2 D
5 2 E
6 2 F
7 1 B
8 1 C
9 1 E
10 1 A
11 1 F
12 1 D
13 3 D
14 3 A
15 3 C
16 3 F
17 3 B
18 3 E
19 4 A
20 4 F
21 4 B
22 4 C
23 4 D
24 4 E
```

* Fitting a linear model (eda: check scatterplot of outcome vs treatment)
	* _do not_ model with block as a fixed effect

```
mod.reg <- lm(yield ~ block + density, data=dataset)
```

* assumes that the blocks produce an effect only on the intercept of the regression line, while the slope is unaffected

* _do_ model with block as a random effect (i.e. block effect may produce random fluctuations for both model parameters, intercept and slope)

```
modMix.1 <- lme(yield ~ density, random = ~ density|block, data=dataset)
# or equivalently
modMix.1 <- lme(yield ~ density, random = list(block = pdSymm(~density)), data=dataset)
## Linear mixed-effects model fit by REML
##  Data: dataset 
##        AIC      BIC    logLik
##  340.9166 355.0569 -164.4583
## 
## Random effects:
##  Formula: ~density | block
##  Structure: General positive-definite, Log-Cholesky parametrization
##            StdDev    Corr 
## (Intercept) 3.16871858 (Intr)
## density    0.02255249 0.09 
## Residual    1.38891957       
## 
## Fixed effects: yield ~ density 
##                Value Std.Error DF  t-value p-value
## (Intercept) 31.78987 1.0370844 69  30.65311      0
## density    -0.26744 0.0096629 69 -27.67704      0
##  Correlation: 
##        (Intr)
## density -0.078
## 
## Standardized Within-Group Residuals:
##        Min        Q1        Med        Q3        Max 
## -1.9923722 -0.5657555 -0.1997103  0.4961675  2.6699060 
## 
## Number of Observations: 80
## Number of Groups: 10
```

* If there is NOT a strong correlation between the slope (e.g. listed above as corr = 0.09 for density) and intercept (i.e. correlated random effects) in the Random Effects section of summary(modMix.1), try modeling with the random effects as independent

```
modMix.2 <- lme(yield ~ density, random = list(block = pdDiag(~density)), data=dataset)
```

* ‘pdDiag’ specifies a var-covar diagonal matrix, where covariances (off-diagonal terms) are constrained to 0
* check if the change made a significant difference (i.e. pval < 0.05)

```
anova(modMix.1, modMix.2)
```

* Other options include: either random intercept or random slope

```
# Model with only random intercept
modMix.3 <- lme(yield ~ density, random = list(block = ~1), data=dataset)
# Alternative notation
# random = ~ 1|block

# Model with only random slope
modMix.4 <- lme(yield ~ density, random = list(block = ~ density - 1), data=dataset)
# Alternative notation
# random = ~density - 1 | block
```

* Fitting a nonlinear model

```
library(aomisc)
datasetG <- groupedData(yieldLoss ~ 1|block, dataset)
nlin.mix <- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, 
                        fixed = list(i ~ 1, A ~ 1),
            random = i + A ~ 1|block)
# or equivalently
nlin.mix2 <- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, 
                              fixed = list(i ~ 1, A ~ 1),
                  random = pdSymm(list(i ~ 1, A ~ 1)))

## Nonlinear mixed-effects model fit by maximum likelihood
##  Model: yieldLoss ~ NLS.YL(density, i, A) 
##  Data: datasetG 
##        AIC      BIC    logLik
##  474.8225 491.5475 -231.4113
## 
## Random effects:
##  Formula: list(i ~ 1, A ~ 1)
##  Level: block
##  Structure: General positive-definite
##          StdDev    Corr 
## i        0.1112839 i   
## A        4.0466971 0.194
## Residual 1.4142009     
## 
## Fixed effects: list(i ~ 1, A ~ 1) 
##      Value Std.Error  DF  t-value p-value
## i  1.23242  0.038225 104 32.24107      0
## A 68.52068  1.945173 104 35.22600      0
##  Correlation: 
##  i     
## A -0.409
## 
## Standardized Within-Group Residuals:
##        Min        Q1        Med        Q3        Max 
## -2.4414051 -0.7049356 -0.1805322  0.3385275  2.8787362 
## 
## Number of Observations: 120
## Number of Groups: 15
```

* Exclude correlation between random effects (0.194 above) if not substantial for a simpler model

```
nlin.mix3 <- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, 
                              fixed = list(i ~ 1, A ~ 1),
                  random = pdDiag(list(i ~ 1, A ~ 1)))
```


* 




