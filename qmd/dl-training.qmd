# Training {#sec-dl-train .unnumbered}

## Misc {#sec-dl-train-misc .unnumbered}

-   Packages
    -   [{]{style="color: goldenrod"}[autokeras](https://autokeras.com/){style="color: goldenrod"}[}]{style="color: goldenrod"} - Leverages Neural Architecture Search (NAS) techniques behind the scenes. It uses a trial-and-error approach powered by Keras Tuner under the hood to test different configurations. Once a good candidate is found, it trains it to convergence and evaluates.
    -   [{]{style="color: goldenrod"}[keras-tuner](https://keras.io/keras_tuner/getting_started/){style="color: goldenrod"}[}]{style="color: goldenrod"} - Focused on hyperparameter optimization. You define the search space (e.g., number of layers, number of units, learning rates), and it uses optimization algorithms (random search, Bayesian optimization, Hyperband) to find the best configuration.
-   Resources
    -   [Automating Deep Learning: A Gentle Introduction to AutoKeras and Keras Tuner](https://towardsdatascience.com/automating-deep-learning-a-gentle-introduction-to-autokeras-and-keras-tuner/)
        -   Basic intro to [{autokeras}]{style="color: goldenrod"} and [{keras-tuner}]{style="color: goldenrod"}
-   Use early stopping and set the number of iterations arbitrarily high. This removes the chance of terminating learning too early.
-   Symplectic Optimization
    -   From[ What to Do When Your Credit Risk Model Works Today, but Breaks Six Months Later](https://towardsdatascience.com/your-credit-risk-model-works-today-it-breaks-in-six-months/) ([Code](https://github.com/Javihaus/Hamiltonian-Neural-Network-Optimization)) (See IEEE DSA2025 for paper)
    -   Claims to forestall model drift (AUC) for months later than gradient descent optimizers (e.g. ADAM)
    -   Use as alternative to gradient descent optimizers when
        -   Ranking matters more than classification accuracy
        -   Distribution shift is gradual and predictable (economic cycles, not black swans)
        -   Temporal stability is critical (financial risk, medical prognosis over time)
        -   Retraining is expensive (regulatory validation, approval overhead)
        -   You can afford 2–3x training time for production stability
        -   You have \<10K features (works well up to \~10K dimensions)
    -   Don’t use when:
        -   Distribution shift is abrupt/unpredictable (market crashes, regime changes)

        -   You need interpretability for compliance (this doesn’t help with explainability)

        -   You’re in ultra-high dimensions (\>10K features, cost becomes prohibitive)

        -   Real-time training constraints (2–3x slower than Adam)
-   Strategies\
    ![](./_resources/DL,_Training.resources/image.1.png){.lightbox width="632"}
    -   Includes Active Learning, Transfer Learning, Few-Shot Learning, Weakly-Supervised Learning, Self-Supervised Learning, Semi-Supervised Learning
-   Common Variations of Gradient Descent
    -   [**Batch Gradient Descent**](https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a): Use the entire training dataset to compute the gradient of the loss function. This is the most robust but not computationally feasible for large datasets.
    -   [**Stochastic Gradient Descent**](https://realpython.com/gradient-descent-algorithm-python/#:~:text=Stochastic%20gradient%20descent%20is%20an,used%20in%20machine%20learning%20applications.): Use a single data point to compute the gradient of the loss function. This method is the quickest but the estimate can be noisy and the convergence path slow.
    -   [**Mini-Batch Gradient Descent**](https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a): Use a subset of the training dataset to compute the gradient of the loss function. The size of the batches varies and depends on the size of the dataset. This is the best of both worlds of both batch and stochastic gradient descent.
        -   Use the largest batch sizes possible that can fit inside your computer's GPU RAM as they parallelize the computation
-   GPU methods\
    ![](./_resources/DL,_Training.resources/image.2.png){.lightbox width="632"}
-   Data Storage
    -   Data samples are iteratively loaded from a storage location and fed into the DL model
        -   The speed of each training step and, by extension, the overall time to model convergence, is directly impacted by the speed at which the data samples can be loaded from storage
    -   Which metrics are important depends on the application
        -   Time to first sample might be important in a video application
        -   Average sequential read time and Total processing time would be important for DL models
    -   Factors
        -   *Data Location*: The location of the data and its distance from the training machine can impact the latency of the data loading.
        -   *Bandwidth*: The bandwidth on the channel of communication between the storage location and the training machine will determine the maximum speed at which data can be pulled.
        -   *Sample Size*: The size of each individual data sample will impact the number of overall bytes that need to be transported.
        -   *Compression*: Note that while compressing your data will reduce the size of the sample data, it will also add a decompression step on the training instance.
        -   *Data Format*: The format in which the samples are stored can have a direct impact on the overhead of data loading.
        -   *File Sizes*: The sizes of the files that make up the dataset can impact the number of pulls from the data storage. The sizes can be controlled by the number of samples that are grouped together into files.
        -   *Software Stack*: Software utilities exhibit different performance behaviors when pulling data from storage. Among other factors, these behaviors are determined by how efficient the system resources are utilized.
    -   Metrics
        -   **Time to First Sample** --- How much time does it take until the first sample in the file is read?
        -   **Average Sequential Read Time** --- What is the average read time per sample when iterating sequentially over all of the samples?
        -   **Total Processing Time** --- What is the total processing time of the entire data file?
        -   **Average Random Read Time** --- What is the average read time when reading samples at arbitrary offsets?
    -   Reading from S3 ([article](https://towardsdatascience.com/streaming-big-data-files-from-cloud-storage-634e54818e75))\
        ![](./_resources/DL,_Training.resources/image.png){.lightbox width="632"}
