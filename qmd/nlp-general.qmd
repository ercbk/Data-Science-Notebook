# NLP, General

TOC

* Misc
* Terms
* Hearst Patterns
* Use Cases



Misc

	Also see
	* [Feature Engineering, Text](Feature Engineering, Text)
	* [EDA, Text](EDA, Text)
	* [Diagnostics, NLP](Diagnostics, NLP)
	Use cases for discovering hyper/hyponym relationships
	* Taxonomy prediction: identifying broader categories for the terms, building taxonomy relations (like WikiData GraphAPI)
	* Information extraction (IE): automated retrieval of the specific information from text is highly reliable on relation to searched entities.
	* Dataset creation: advanced models need examples to be learned to identify the relationships between entities.
* Baseline Models
	* Regularized Logistic Regression + Bag-of-Words (BoW) (recommended by Raschka)
		* Example: py, kaggle [notebook](https://www.kaggle.com/code/bisman/logistic-regression-bow-and-tfidf/notebook) (sentiment analysis)
			* Uses Compressed Sparse Row (CSR) type of sparse matrix
			* Uses a time series cv folds and scores by precision
				* Precision because "Amazon would be more concerned about the products with negative reviews rather than positive reviews"
			* Says random search > grid search with regularized regression
			* Also fits a model with Tf-idf instead of BoW
		* Preprocess
			* Tokenize
			* Remove stopwords and punctuation
			* Stem
			* unigrams and bigrams
			* Sparse token count matrix
			* Normalize the matrix
		* Model with regularized logistic regression
* GPT-4
	* Accepts prompts of 25,000 words (GPT-3 accepted 1500-2000 words)
	* allegedly around 1T parameters (GPT-3 had 175B parameters)
	* Some use cases: translation, q/a, text summaries, writing/getting news, creative writing
	* Multi-modal training data (i.e. text and audio, pictures, etc.)
	* Still hallucinates



Terms

* **Flood Words** - words that are too common in the domain (i.e. noise)
* **Hypernym** - a word with a broad meaning constituting a category into which words with more specific meanings fall
	* Example: A device can use multiple storage units such as a hard drive or CD
		* hyponym of storage units: hard drive, cd
		* hypernym of hard drive/cd: storage units
* **Hyponym** -  opposite of Hypernym; a word of more specific meaning than a general term applicable to it.
* **Named Entity Recognition (NER)** - a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.![](./_resources/NLP,_General.resources/image.1.png)
	* Example: An automated NER system will identify the incoming customer request (e.g. installation, maintenance, complaint, and troubleshoot of a particular product) and send it to the respective support desk
	* aka **named entity identification**, **entity chunking**, and **entity extraction**
	* Other use cases: filtering resumés, diagnose patients based on symptoms in healthcare data
* **sequence to sequence** (aka **string transduction**) -  problems where the input and output is text
	* e.g. Text summarization, Text simplification, Question answering, Chatbots, Machine translation
* **Spam Words** - Words that don't belong in the domain (i.e. noise)



Hearst Patterns
![](./_resources/NLP,_General.resources/image.png)

* a set of test patterns that can be employed to extract Hypernyms and Hyponyms from text.
* In the table, X is the hypernym and Y is the hyponym
* "rhyper" stands for reverse-hyper
* Usually, you don’t want to extract all possible hyponyms relations, but only entities in the specific domain
* Packages
		[{{SpaCy}}]{style='color: goldenrod'} - [example](https://towardsdatascience.com/implementing-hearst-patterns-with-spacy-216e585f61f8)
		



Use Cases

* Creating labels for text that can later be used for supervised learning tasks like classification
* Create metadata for columns in datasets (i.e. data dictionary)
	* [Predicting Metadata for Humanitarian Datasets Using GPT-3](https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d)
		* Prompt: Column name; sample of data from that column.
		* Completion: metadata which is a tag that includes column attributes.
		* Potential Improvements
			* Trying other models ('ada' used here) to see if this improves performance (though it will cost more)
			* Model hyperparameter tuning. The log probability cutoff will likely be very important
			* More prompt engineering to perhaps include column list on the table might provide better context, we well as overlying columns on two-row header tables.
			* More preprocessing. Not much was done for this article, blindly taking tables extracted from CSV files, so the data is can be a bit messy
	* [Using GPT-3.5-Turbo and GPT-4 for Predicting Humanitarian Data Categories](https://towardsdatascience.com/using-gpt-3-5-turbo-and-gpt-4-to-apply-text-defined-data-quality-checks-on-humanitarian-datasets-6f02219c693c)
		* GPT-4 resulted in 96% accuracy when predicting category and 89% accuracy when predicting both category and sub-category.
			* GPT-3.5-turbo for the same prompts, with 96% accuracy versus 66% for category.
		* Limitations exist due to the maximum number of tokens allowed in prompts affecting the amount of data that can be included in data excerpts, as well as performance and cost challenges — especially if you’re a small non-profit! — at this early stage of commercial generative AI.
		* Likely related to being an early preview, GPT-4 model performance was very slow, taking 20 seconds per prompt to complete
* Generalizations from OSINT ([article](https://towardsdatascience.com/how-large-language-models-changed-my-entire-osint-workflow-35960099e258))
	* Note: most of these use a combination of the others to improve their overall performance — a document clustering model might use topic extraction and NER to improve the quality of their clusters.
	* Recommendation Engines: Bespoke recommendation engines can be fine-tuned to recommend related documentation that may not be within the user’s immediate sphere of interest, but still relevant.
		* Enables the discovery of “unknown unknowns”.
	* Topic extraction and Document clustering: generate topics from multiple texts and detect similarities between documents publishing by dozens, sometimes hundred information feeds. 
		* You don’t have the time to read every single document to get a higher view of the main problematics evolving within your multiple information feeds
	* Named (and unnamed) Entity Extraction and Disambiguation (NER / NED) (see Terms): identifying and categorizing named entities
		* The extraction part involves locating and tagging entities, while the disambiguation part involves determining the correct identity or meaning of an entity, especially when it can have multiple interpretations or references in a text.
		* Allow you to build entire NLP logics to keep tracks of meaningful facts about this entity, order it by timeliness and relevance. This will allow you to start building bespoke, expert curated profiles.
	* Relationship Extraction: identify the nature and type of relationships between different entities, such as individuals, organizations, and locations, and to represent them in a structured format that can be easily analyzed and interpreted.
		* Generating accurate connections between across thousands of documents will build expert driven, queryable knowledge graphs in a matter of days
	* Multi-document abstractive summarisation: automatically generated a concise and coherent summary of multiple documents on a given topic, by creating _new_ sentences that capture the most important information from the original texts.
		* Enable users to obtain a concise and coherent summary of the most important information from a large amount of text data.
* Text Summarization
	* Notes from [Summarize Podcast Transcripts and Long Texts Better with NLP and AI](https://towardsdatascience.com/summarize-podcast-transcripts-and-long-texts-better-with-nlp-and-ai-e04c89d3b2cb)
	* Current [{{LangChain}}]{style='color: goldenrod'} implementations:
		* **Recursive Summarization,** in which the long text is split equally into shorter chunks which can fit inside the LLM’s context window. Each chunk is summarized, and the summaries are concatenated together to and then passed through GPT-3 to be further summarized. This process is repeated until one obtains a final summary of desired length.
			* Major downside is that existing implementations e.g. LangChain’s summarize chain using map\_reduce, split the text into chunks with no regard for the logical and structural flow of the text.
				* For example, if the article is 1000 words long, a chunk size of 200 would mean that we would get 5 chunks. What if the author has several main points, the first of which takes up the first 250 words?
		* **Refine method**, which passes a chunk of text, along with a summary of the previous chunks, through the LLM, which progressively refines the summary as it sees more of the text. See [prompt](https://github.com/hwchase17/langchain/blob/master/langchain/chains/summarize/refine_prompts.py) for details.
			* Sequential nature of the process means that it cannot be parallelized and takes linear time, far longer than a recursive method which takes logarithmic time
			* Meaning from the beginning parts of the text could be overrepresented in the final summary. This would be bad if the beginning text has something not germane like advertisements in it.
	* Proposed method:
		* Split the summary outputs from one step of the recursive summarization into chunks to be fed into the next step. We can achieve this through by clustering chunks semantically into topics and passing topics into the next iteration of the summarization. It does not drastically increase the LLM costs — we are still passing just as much input as the conventional method into the LLM, yet we get a much richer summarization.
			* Example uses Biden state of the union speech
			* Preprocessing:
				* Split the raw text it into sentences, restricting sentences to have a minimum length of 20 words and maximum length of 80.
				* Create chunks of sentences. Chunk size should be the number of sentences it generally takes to express a discrete idea. Example uses 5 sentences (but you can experiment with other numbers) with 1-sentence overlap between chunks, just to ensure continuity so that each chunk has some contextual information about the previous chunk.
					* Maybe figure out the average count of sentences per paragraph.
						* This [article](https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736) tried something a little more extreme by chunking each paragraph (At least I think that's what this means. It's a fucking run-on sentence.) Might not of worked because maybe he had a lot of 1 sentence paragraphs?
							* "Initially, I also tried splitting the text blocks into paragraphs, hypothesizing that because a section may contain information about many different topics, the embedding for that entire section may not be similar to an embedding for a text prompt concerned with only one of those topics. This approach, however, resulted in top matches for most search queries disproportionately being single line paragraphs, which turned out to not be terribly informative as search results."
					* Resulted in 65 chunks, with an average chunk length is 148 words, and a range from 46–197 words
			* Produce a Title and Summary for each chunk.

```
# Prompt to get title and summary for each chunk
map_prompt_template = """
    Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:
    [{text}]{style='color: #990000'}
    Return your answer in the following format:
    Title | Summary...
    e.g.
    Why Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.
    TITLE AND CONCISE SUMMARY:
"""
```

* Prompt for gpt3 model with title and summary output separated by a bar, |.

* Transform summaries of chunks into embeddings
	* Pass summaries to an openai or huggingface model to get embeddings
* Group semantically-similar chunks together into topics
	* Create a chunk similarity matrix, where the (i,j)th entry denotes the cosine similarity between the embedding vectors of the ith and jth chunk ([{{scipy}}]{style='color: goldenrod'} for cosine similarity function)
	* Use the [Louvain community detection algorithm](https://towardsdatascience.com/louvain-algorithm-93fde589f58c) to detect topics from the chunks ([{{networkx}}]{style='color: goldenrod'} for Louvain algorithm)
		* Has a hyperparameter called resolution — small resolutions lead to smaller clusters.
		* Additionally, a hyperparameter proximity\_bonus  is created, which bumps up the similarity score of chunks if their position in the original text is closer to each other. You can interpret this as treating the temporal structure of the text as a prior (i.e. chunks closer to each other are more likely to be semantically similar).
		* Another example, using a Bloomberg podcast transcript, shows how this method detected blocks of advertisements.
* Produce a title for each topic
	* For much longer texts like books, repeat this process several times until there are ~10 topics left whose topic summaries can fit into the context window.
	* Each topic has a number of chunks associated with it. Titles were produced for each chunk earlier in the process. So, loop each topic's chunks' list of titles through an LLM to get a refined title for that topic.
	* Do this concurrently (i.e. simultaneously) for all topics to prevent the topics’ titles from being too similar with one another.
	* This output will be the label of each topic (see Visualization section)
* Produce a summary for each topic
	* Similar to the previous section.
	* Feed chunk summaries of each topic to GPT-3, and get a refined summary
	* Same as befeore, this needs to be done concurrently
* Final Summary: To arrive at the overall summary of the text, concatenate the topic summaries together and prompt GPT-3 to summarize them.![](./_resources/NLP,_General.resources/image.3.png)
* Visualization![](./_resources/NLP,_General.resources/image.2.png)

* Search Engine for Docs
	* [How I Turned My Company’s Docs into a Searchable Database with OpenAI](https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736)
	* Steps
		* Converted all of the docs to a unified format
			* Converts all html files to markdown
		* Split docs into blocks and added some automated cleanup
			* Links to code for parsing markdown files to get text and code blocks
		* Computed embeddings for each block
			* Used OpenAI’s [text-embedding-ada-002 model](https://platform.openai.com/docs/guides/embeddings/embedding-models) which is cheap and provides good performance
		* Generated a vector index from these embedding
			* Used Qdrant for the vector embeddings storage
		* Defined the index query
			* Added ablilities for the user to specify looking only in text or code and included other meta data in the output
		* Wrapped it all in a user-friendly command line interface and Python API
	* Potential extensions of the project
		* [Hybrid search](https://www.pinecone.io/learn/hybrid-search-intro/): combine vector search with traditional keyword search
		* Go global: Use [Qdrant Cloud](https://cloud.qdrant.io/) to store and query the collection in the cloud
		* Incorporate web data: use [requests](https://pypi.org/project/requests/) to download HTML directly from the web
		* Automate updates: use [Github Actions](https://docs.github.com/en/actions) to trigger recomputation of embeddings whenever the underlying docs change
		* Embed: wrap this in a Javascript element and drop it in as a replacement for a traditional search bar





















