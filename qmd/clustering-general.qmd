# Clustering

TOC

-   Terms
-   Misc
-   Algorithms
    -   Gaussian Mixture Modeling (GMM)
    -   Latent Profile Analysis (LPA)
    -   t-SNE
    -   UMAP
    -   K-means
    -   Aproximate Nearest Neighbors (ANN)
    -   DBSCAN

## Terms

-   **Cluster Centroid** - The middle of a cluster. A centroid is a vector that contains one number for each variable, where each number is the mean of a variable for the observations in that cluster. The centroid can be thought of as the multi-dimensional average of the cluster.
-   **Hard (or Crisp) Clustering** - each point belongs to a single cluster
-   **Soft (or Fuzzy) Clustering** - each point is allowed to belong to multiple clusters

## Misc

-   Also see
    -   Notebook, pgs 57-58
    -   [Diagnostics, Clustering](Diagnostics,%20Clustering)
-   For static data, i.e., if the values do not change with time, clustering methods are usually divided into five major categories:
    1.  partitioning (or partitional)
    2.  hierarchical
    3.  density-based
    4.  grid-based
    5.  model-based methods
-   Cluster Descriptions
    -   Packages
        -   [{]{style="color: #990000"}[parameters](https://easystats.github.io/parameters/articles/clustering.html){style="color: #990000"}[}]{style="color: #990000"} - provides various functions for describing, analyzing, and visualizing clusters for various methods
        -   [{]{style="color: #990000"}[clustereval](https://erdogant.github.io/clusteval){style="color: #990000"}[}]{style="color: #990000"} - compute the statistical association between the features and the detected cluster labels and whether they are significant.
            -   Categorical: Chi-Square, Fisher's Exact, or Hypergeometric tests
            -   Continuous: Mann-Whitney-U test
    -   Examine variable values at the centroids of each cluster
        -   A higher absolute value indicates that a certain variable characteristic is more pronounced within that specific cluster (as compared to other cluster groups with lower absolute mean values).
    -   Distributional statistics for each cluster
        -   Numeric variables: mean and sd for each variable in that cluster
        -   Categorical variables:
            -   binary: percent where event = 1
            -   multinomial: most prominent category
    -   Run a decision tree on clusters![](./_resources/Clustering.resources/Screenshot%20(279).png)
        -   Each color (orange, blue, green, purple) represents a cluster
        -   Explains how clusters were generated
        -   [[{treeheatr}](https://trang1618.github.io/treeheatr/)]{style="color: #990000"}![](./_resources/Clustering.resources/image.1.png)
    -   Radar charts![](./_resources/Clustering.resources/image.png)
        -   3 clusters: blue (highlighted), red, green
        -   Guessing the mean values for each variable are the points
-   Visualize
    -   Use clustering variables of interest for a scatter plot then label the points with cluster id

        ![](_resources/Clustering.resources/scatter-with-cluster-labels.jpg){width="532"}

Algorithms

-   **Gaussian Mixture Models** **(GMM)**
    -   Notes from Serrano video: https://www.youtube.com/watch?v=q71Niz856KE&ab_channel=LuisSerrano
    -   Soft clustering algorithm
    -   Components of the Algorithm
        -   "Color" points according to gaussians (clusters)
            -   The closer a point is to the center of a gaussian the more intensely it matches the color of that gaussian
            -   Points in between gaussians are a mixture or proportion of the colors of each gaussian 
            -   ![](./_resources/Clustering.resources/Screenshot%20(32).png)
            -   
        -   Fitting a Gaussian
            -   Find the center of mass
                -   2-dim: calculate the mean of x and the mean of y and that's the coordinates of your center of mass
            -   Find the spread of the points
                -   2-dim: calculate the x-variance, y-variance, and covariance

                -   ![](./_resources/Clustering.resources/Screenshot%20(34).png) First equation is the height of Gaussian (multi-variate Gaussian distribution equation). (Second equation is just the familiar 1-D gaussian equation for reference)

                -   Partially "colored" points affect spread and center of mass calculations

                    -   Fully colored points "weigh" more than partially colored points and pull the center of mass and change the orientation
                    -   ![](./_resources/Clustering.resources/Screenshot%20(36).png)
    -   Steps
        1.  Start with random Gaussians
            -   each gaussian has random means, variances
        2.  Color points according to distance to the random gaussians
            -   The heights in the distributions pic above
        3.  (Forget about old gaussians) Calculate new gaussians based on the colored points
        4.  (Forget about old colors) Color points according to distance to the new gaussians
        5.  Repeat until some threshold is reached (i.e. gaussians or colors don't change much)
    -   Tuning
        -   initial conditions (good starting points for the random gaussians at the beginning)
        -   limits on the mean and variance calculations
        -   Number of gaussians, k, can be chosen by minimizing the Davies-Bouldin score
        -   running algorithm multiple times
            -   like CV grid search algs or bootstrapping
-   **Latent Profile Analysis (LPA)**
    -   sort of like k-means + GMM
    -   k number of profiles (i.e. clusters) are chosen
    -   model outputs probabilities that an observation belongs to any particular cluster
    -   GOF metrics available
    -   "As with Exploratory Factor Analysis (EFA )(and other latent-variable models), the assumption of LPA is that the latent (unobserved) factor"causes" (I'm using the term loosely here) observed scores on the indicator variables. So, to refer back to my initial hypothetical example, a monster being a spell caster (the unobserved class) causes it to have high intelligence, low strength, etc. rather than the inverse. This is a worthwhile distinction to keep in mind, since it has implications for how the model is fit."
    -   Bin variables that might dominate the profile. This way the profiles will represent a latent variable and not gradations of the dominate variable (e.g. low, middle, high values of the dominate variable).
    -   Center other variable observations according to dominant variable bin those observations are in. (e.g. subtract values in bin1 from bin1's mean)

```         
# From D&D article where challenge_rating is a likely dominant variable
mons_bin <- mons_df %>%
  mutate(cr_bin = ntile(x = challenge_rating, n = 6))
ab_scores <- c("strength", "dexterity", "constitution", "intelligence", "wisdom", "charisma") 
mons_bin <- mons_bin %>%
  group_by(cr_bin) %>%
  mutate(across(.cols = ab_scores, .fns = mean, .names = "{.col}_bin_mean")) %>%
  ungroup()
```

-   **tSNE**
    -   packages: [{]{style="color: #990000"}[Rtsne](https://cran.r-project.org/web/packages/Rtsne/index.html){style="color: #990000"}[}]{style="color: #990000"}
    -   t-distributed stochastic neighbor embedding
    -   Looks at the local distances between points in the original data space and tries to reproduce them in the low-dimensional representation
        -   Both UMAP and tSNE attempt to do this but fails (Lior Pachter [paper thread](https://twitter.com/lpachter/status/1431325969411821572), [doesn't preserve local structure](https://twitter.com/lpachter/status/1537854533308559361),  [no theorem that it preserves topology](https://twitter.com/lpachter/status/1515783026885550084))
    -   Results depend on a random starting point
    -   Tuning parameters: perplexity
-   **UMAP**
    -   packages: umap
    -   Uniform manifold approximation and projection
    -   See tSNE section for Lior Pachter threads on why not to use tSNE or UMAP
    -   prep
        -   only for numeric variables
        -   standardize
    -   projects variables to a nonlinear space
    -   variation of tSNE
        -   random starting point has less of an impact
    -   can be supervised (give it an outcome variable)
    -   computationally intensive
    -   Low-dimensional embedding cannot be interpreted
        -   no rotation matrix plot like in PCA
    -   try pca - linear method (fast)
        -   if successful (good separation between categories), then prediction may be easier
        -   if not, umap, tsne needed
    -   umap can taking training model and apply it to test data or new data (tSNE can't)
    -   Tuning parameter: neighbors
        -   example used 500 iterations (n_epochs) as limit for convergence
-   **K-Means**
    -   Seeks to assign n points to k clusters and find cluster centers so as to minimize the sum of squared distances from each point to its cluster center.
    -   For choosing the number of clusters, elbow method (i.e. WSS) is usually awful if there are more than few clusters. Recommended: Calinski-Harabasz Index and BIC then Silhouette Coefficient or Davies-Bouldin Index (see [Diagnostics, Clustering](Diagnostics,%20Clustering)) ([article](https://towardsdatascience.com/are-you-still-using-the-elbow-method-5d271b3063bd))
    -   Base R`kmeans` uses the Hartigan-Wong algorithm
        -   For large k and larger n, the density of cluster centers should be proportional to the density of the points to the power (d/d+2). In other words the distribution of clusters found by k-means should be more spread out than the distribution of points. This is not in general achieved by commonly used iterative schemes, which stay stuck close to the initial choice of centers.
-   **Approximate Nearest Neighbor (ANN)**
    -   kNN runs at O(N\*K), where N is the number of items and K is the size of each embedding. Approximate nearest neighbor (ANN) algorithms typically drop the complexity of a lookup to O(log(n)).
    -   Misc
        -   Also see [Maximum inner product search using nearest neighbor search algorithms](https://towardsdatascience.com/maximum-inner-product-search-using-nearest-neighbor-search-algorithms-c125d24777ef)
            -   It shows a preprocessing transformation that is performed before kNN to make it more efficient
            -   It might already be implemented in ANN algorithms
    -   Commonly used in Recommendation algs to cluster user-item embeddings at the end. Also, any NLP task where you need to do a similarity search of one character embedding to other character embeddings.
    -   Generally uses one of two main categories of hashing methods: either data-independent methods, such as locality-sensitive hashing (LSH); or data-dependent methods, such as Locality-preserving hashing (LPH)
    -   Locality-Sensitive Hashing (LSH)
        -   hashes similar input items into the same "buckets" with high probability.
        -   The number of buckets is much smaller than the universe of possible input items
        -   hash collisions are maximized, not minimized, where a **collision** is where two distinct data points have the same hash.
    -   Spotify's Annoy
        -   Uses a type of LSH, Random Projections Method (RPM) (article didn't explain this well)
        -   L RPM hashing functions are chosen. Each data point, p, gets hashed into buckets in each of the L hashing tables. When a new data point, q, is "queried," it gets hash into buckets like p did. All the hashes in the same buckets of p are pulled and the hashes within a certain threshold, c\*R, are considered nearest neighbors.
            -   Wiki [article](https://en.wikipedia.org/wiki/Locality-sensitive_hashing#LSH_algorithm_for_nearest_neighbor_search) on LSH and RPM clears it up a little, but I'd probably have to go to Spotify's paper to totally make sense of this.
        -   Also the Spotify alg might bring trees/forests into this somehow
    -   Facebook AI Similarity Search (FAISS)
        -   Hierarchical Navigable Small World Graphs (HNSW)

        -   HNSW has a polylogarithmic time complexity (O(logN))

        -   Two approximations available Embeddings are clustered and centroids are calculated. The k nearest centroids are returned.

            -   Embeddings are clustered into veroni cells. The k nearest embeddings in a veroni cell or a region of veroni cells is returned.

        -   Both types of approximations have tuning parameters.
    -   Inverted file index + product quantization (IVFPQ)([article](https://towardsdatascience.com/similarity-search-with-ivfpq-9c6348fd4db3))
-   **DBSCAN**![](./_resources/Clustering.resources/1-rLkYqpNmEguxDPuhLaTtVA.png)
    -   Misc
        -   Notes from:
            -   [Understanding DBSCAN and Implementation with Python](https://towardsdatascience.com/understanding-dbscan-and-implementation-with-python-5de75a786f9f)
            -   Clustering with DBSCAN, Clearly Explained [video](https://www.youtube.com/watch?v=RDZUdRSDOok&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=45)
        -   Packages
            -   [{dbscan}]{style="color: #990000"}
            -   [[{parameters}](https://easystats.github.io/parameters/articles/clustering.html#dbscan)]{style="color: #990000"} -
                -   `n_clusters_dbscan` - Given a "min_size" (aka *minPts*?), the function estimates the optimal "eps"
                -   `cluster_analysis` - Shows Sum of Squares metrics and the (standardized) mean value for each variable within each cluster.
        -   HDBSCAN is the hierarchical density-based clustering algorithm
    -   Tuning
        -   *eps* - the maximum distance between two samples for one to be considered to be connected to the other
            -   large eps tend to include more points within a cluster,
            -   too-large eps will include everything in the same single cluster
            -   too-small eps will result in no clustering at all
        -   *minPts* (or *min_samples*) - the minimum number of samples in a neighborhood for a point to be considered as a core point
            -   too-small minPts is not meaningful because it will regard every point as a core point.
            -   larger minPts can be better to deal with noisy data
    -   Algorithm
        -   For each data point, find the points in the neighborhood within *eps* distance, and define the core points as those with at least *minPts* neighbors.![](./_resources/Clustering.resources/Screenshot%20(1418).1.png)
            -   The orange circle represents the *eps* area
            -   If *minPts* = 4, then the top 4 points are core points because they have at least 4 points overlapping the *eps* area
        -   Define groups of connected core points as clusters.![](./_resources/Clustering.resources/Screenshot%20(1419).2.png)
            -   All the green points have been labelled as core points
        -   Assign each non-core point to a nearby cluster if it's directly reachable from a neighboring core point, otherwise define it as an outlier.![](./_resources/Clustering.resources/Screenshot%20(1420).png)
            -   The black points are non-core points but are points that overlap the eps area for the outer-most core points.
            -   Adding these black points finalizes the first cluster
        -   This process is repeated for the next group of core points and continues until all that's left are outliers.
    -   Advantages
        -   Doesn't require users to specify the number of clusters.
        -   Not sensitive to outliers.
        -   Clusters formed by DBSCAN can be any shape, which makes it robust to different types of data.
            -   Example: Nested Cluster Structure
                -   K-Means![](./_resources/Clustering.resources/Screenshot%20(1418).png)
                    -   K-Means wants spherical clusters which makes it grab groups of points it shouldn't
                -   DBSCAN![](./_resources/Clustering.resources/Screenshot%20(1419).png)
                    -   Able correctly identify the oblong shaped cluster
    -   Disadvantages
        -   if the data has a very large variation in densities across clusters because you can only use one pair of parameters, *eps* and *MinPts*, on one dataset
        -   it could be hard to define *eps* without the domain knowledge of the data
        -   Clusters not totally reproducible. Clusters are defined sequentially so depending on which group of core points the algorithm starts with and hyperparameter values, some non-core points that are within the eps area of multiple clusters may be assigned to different clusters on different runs of the algorithm.
