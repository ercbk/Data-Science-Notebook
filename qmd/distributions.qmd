# Distributions

TOC

* Terms
* Tests
* Dirichlet
* Exponential
* Gaussian
* Pareto
* Poisson
* Student's T
* Triangular
* Beta
* Gamma



Terms

* **Conditional Probability Distributions** -
	* Notes from https://www.causact.com/joint-distributions-tell-you-everything.html#joint-distributions-tell-you-everything
	* Notation: P(Y | X) = P(Y and X) / P(X) = P(Y, X) / P(X)
		* i.e. ratio of 2 marginal distributions
	* Example: two tests for cancer are conducted to determine whether a biopsy should be performed
		* Conditional approach: Biopsy everyone at determined to be high risk from test 1; measure the genetic marker (aka test 2) for patients at intermediate risk **and** biopsy those with a probability of cancer past a certain level based on the marker
* **Empirical CDF**![](./_resources/Distributions.resources/1-4Gw70MNFbkKm_3wOAgdreQ.png)
	
	* Where X1, X2, … , Xn are from a population with CDF, Fn(x)
	
	* Process
		* Take n samples from an unknown distribution.
				The more samples you take, the closer the empirical distribution will resemble the true distribution.
				
		* Sort these samples, and place them on the x-axis.
		* Start plotting a ‘step-function’ style line — each time you encounter a datapoint on the x-axis, increase the step by 1/N.
		[Example]{.ribbon-highlight}
		![](./_resources/Distributions.resources/1-U2Y7Q-lFWex2loA6X9Cy_Q.png)
		* The CDF of a normal distribution (green) and its empirical CDF (blue)
* **Joint Probability Distribution** - assigns a probability value to all possible combinations of values for a set of random variables.
	* Notation: P(x1, x2, ... ,xn)
	* Plugging in a value for each random variable returns a probability for that combination of values
	* Example: two tests for cancer are conducted to determine whether a biopsy should be performed
		* Joint approach: biopsy anyone who is **_either_** at high risk of cancer (test 1) **_or_** who was determined to have a probability of cancer past a certain level, based on the marker from the genetic test (test 2)
		* Compare with example in Conditional Probability Distributions
* **Location** - Distribution parameter determines the shift of the distribution
	* e.g. mean, mu, of the normal distribution.
* **Marginal Probability Distribution** - assigns a probability value to all possible combinations of values for a subset of random variables
	* Notation: P(x1)
		* P(x1,x2) is sometimes called the **Joint Marginal Probability Distribution**
	* the marginal distribution, P(Y) where Y is a subset of random variables,  is calculated from the joint distribution, P(Y = y, Z = z) where Z is the subset of random variables not in Y .
		* P(Y) = ΣZ=z P(Y = y, Z = z)
			* If Y is just one variable
				* Says sum all the joint probabilities for all the combinations of values for the variables in Z while holding Y constant
				* Repeat for each value of Y to get this summed probability value
				* The marginal distribution is made up of all these values, one for each value of Y (or combination of values if Y is a subset of variables)
		* When the joint probability distribution is in tabular form, one just sums up the probabilities in each row where  Y = y.
* **Scale** \- Distribution parameter; the larger the scale parameter, the more spread out the distribution
	* e.g. s.d., sigma, of the normal distribtution
	* **rate parameter**: the inverse of the scale parameter (see Gamma distribution)
* **Shape** - Distribution parameter that affects the shape of a distribution rather than simply shifting it (as a location parameter does) or stretching/shrinking it (as a scale parameter does).
	* e.g. "peakedness" refers to how round the main peak is



Tests

	**Kolmogorov–Smirnov test (KS)**
	* Used to compare distributions
		* Can be used as a Normality test or any distribution test
		* Can compare two samples
	* Misc
		* vectors may need to be standardized (e.g. normality test) first _unless_ comparing two samples
			H0: Both distributions are from the same distribution
			
	* Packages
		* [{KSgeneral}]{style='color: #990000'} has tests to use for contiuous, mixed, and discrete distributions written in C++
		* {stats} and {dgof} also have functions, ks.test
			* Both handle continuous and discrete distributions
		* All functions take a numeric vector and a base R density function (e.g. "pnorm", "pexp", etc.) as args
			KSgeneral docs don't say you can supply your own comparison sample (2nd arg) only the density function but with stats and dgof, you can.
			* Although they have function to compute the CDFs, so if you need speed, it might be possible to use their functions and do it manually
	* 2-sample test as the greatest distance between the CDFs (Cumulative Distribution Function) of each sample
		* Specifically, this test determines the distribution of your unknown data sample by constructing and comparing the sample’s _empirical CDF_  (see Terms) with the CDF you hypothesized. If the two CDFs are close, your unknown data sample likely follows the hypothesized distribution.
	* KS statistic, Dn,m = max|CDF1 - CDF2| where n as the number of observations on Sample 1 and m as the number of observations in Sample 2
	* Compare the KS statistic with the respective KS distribution based on parameter "en" to obtain the p-value of the test
		* en = (m \* n) / (m + n)



Dirichlet

* a family of continuous multivariate probability distributions parameterized by a vector α of positive reals



Exponential
![](./_resources/Distributions.resources/image.2.png)

* Notes from
	* [Statistical Rethinking](Statistical Rethinking) >> Ch. 10
* constrained to be zero or positive
* fundamental distribution of distance and duration, kinds of measurements that represent displacement from some point of reference, either in time or space.
* If the probability of an event is constant in time or across space, then the distribution of events tends towards exponential.
* Its shape is described by a single parameter, the rate of events λ, or the average displacement λ −1 .
* This distribution is the core of survival and event history analysis



Gaussian

* Special case of Student’s t-distribution with the ν parameter (i.e., nu, degree of freedom) set to infinity.



Multivariate Gaussian

* if the random variable components in the vector are not normally distributed themselves, the result is not multivariate normally distributed.
* variance-covariance matrix must be semi-definite and therefore symmetric
	* Example of not symmetric for two random variables



Pareto

* Also see [Statistical Concepts](Statistical Concepts) >> Distribution Tail Classification
* "Gaussian distributions tend to prevail when events are completely independent of each other.  As soon as you introduce the assumption of interdependence across events, Paretian distributions tend to surface because positive feedback loops tend to amplify small initial events."
* Pareto has similar relationship with the exponential distribution as lognormal does with normal
* ![](./_resources/Distributions.resources/undefined.1.png)
	* xm is the (positive) minimum of the randomly distributed pareto variable, X that has index α
	* Yexp is exponentially distributed with rate α
* some theoretical statistical moments may not exist
	* If the theoretical moments do not exist, then calculating the sample moments is useless
	* Example: Pareto (α = 1.5) has a finite mean and an infinite variance
		* Need α > 2 for a finite variance
		* Need α > 1 for a finite mean
		* In general you need α > p for the pth moment to exist
		* If the nth moment is not finite, then the (n+1)th moment is not finite.
* fat tails
	* ![](./_resources/Distributions.resources/undefined.png)
		* L(x) is just characterized as slowly varying function that gets dominated by the decaying inverse power law element, x\-α. as x goes to infinity
		* α is a shape parameter, aka "tail index" aka "Pareto index"



Poisson

* Obtained as the limit of the binomial distribution when the number of attempts is high and the success probability low. Or the Poisson distribution can be approximated by a normal distribution when λ is large
* Probability Mass Function![](./_resources/Distributions.resources/Screenshot (907).png)
	* E\[Y\] = Var(Y) = λ
* {distributions3}
	* Stats

```
Y <- Poisson(lambda = 1.5)
print(Y)
## [1] "Poisson distribution (lambda = 1.5)"

mean(Y)
## [1] 1.5
variance(Y)
## [1] 1.5
pdf(Y, 0:5)
## [1] 0.22313 0.33470 0.25102 0.12551 0.04707 0.01412
cdf(Y, 0:5)
## [1] 0.2231 0.5578 0.8088 0.9344 0.9814 0.9955
quantile(Y, c(0.1, 0.5, 0.9))
## [1] 0 1 3
set.seed(0)
random(Y, 5)
## [1] 3 1 1 2 3
```

* Visualize![](./_resources/Distributions.resources/Screenshot (909).png)

```
plot(Poisson(0.5), main = expression(lambda == 0.5), xlim = c(0, 15))
plot(Poisson(2),  main = expression(lambda == 2),  xlim = c(0, 15))
plot(Poisson(5),  main = expression(lambda == 5),  xlim = c(0, 15))
plot(Poisson(10),  main = expression(lambda == 10),  xlim = c(0, 15))
```


Student’s t-distribution

* sd = ![](./_resources/Distributions.resources/latex.php]]
	* ν = degrees of freedom
* when ν is small, the Student’s t-distribution is more robust to multivariate outliers
* the smaller the degree of freedom, the more “heavy-tailed” it is![](./_resources/Distributions.resources/tail_prob-2-5.png)
	* \-3 on the y-axis says that the probability of being in the tail is 1 in 103
	* Don't pay attention to the x-axis. Just note how much the probability of being in the tail gets larger as the dof get smaller
* As the degrees of freedom goes to 1, the t distribution goes to the Cauchy distribution
* As the degrees of freedom goes to infinity, it goes to the normal distribution.



Triangular

* Triangle shaped distribution
* Useful when you have a known min and max value

```
extraDistr::rtriang(n, a, b, c) %>% hist()

# Discrete distribution
extraDistr::rtriang(n, a, b, c) %>% round()
```

* n is the number of random values you wish to draw
* a is the min value
* b is the max value
* c is the mode
	* Can use to adjust the skew of the distribution



Beta
![](./_resources/Distributions.resources/Beta-Distribution4.png)

* Defined on the interval \[0,1\]
* The key difference between the Binomial and Beta distributions is that for the Beta distribution the probability, x, is a random variable, however for the Binomial distribution the probability, x, is a fixed parameter.
* Shape parameters are α and β, usually.
	* α and β are two positive parameters that appear as exponents of the random variable
* pdf![](./_resources/Distributions.resources/image.1.png)
* E(X) = α / (α + β)
* Var(X) = (α\*β) / \[(α + β)2 \* (α + β + 1)\]



Gamma
![](./_resources/Distributions.resources/image.png)

* Also see
* Notes from
	* [Statistical Rethinking](Statistical Rethinking) >> Ch. 10
* constrained to be zero or positive
* like Exponential but can have a peak above zero
* If an event can only happen after two or more exponentially distributed events happen, the resulting waiting times will be gamma distributed.
	* e.g. age of cancer onset is approximately gamma distributed, since multiple events are necessary for onset.
* shape parameter k and a scale parameter θ
* E\[X\] = kθ = α/β
	* where a shape parameter α = k and an inverse scale parameter (aka _rate parameter_) β = 1/θ
	* Therefore if you want a gamma distributions with a certain "mean" and "standard deviation," you'd:
		* Set your mean to E\[X\], your standard deviation to θ (probably but maybe it's β)
		* Calculate β
		* Calculate α
		* `prior(gamma(alpha, beta))`
* Also used in survival



















