---
fig-cap-location: top
---

# Distributions {#sec-distr .unnumbered}

## Misc {#sec-distr-misc .unnumbered}

-   For a skewed distribution, a Winsorized Mean (percentage of points replaced) often has *less bias* than a Trimmed Mean
-   For a symmetric distribution, a Trimmed Mean (percentage of points removed) often has *less variance* than a Winsorized Mean.

## Terms {#sec-distr-terms .unnumbered}

-   [**Conditional Probability Distributions**]{style="color: #009499"}

    -   Notes from <https://www.causact.com/joint-distributions-tell-you-everything.html#joint-distributions-tell-you-everything>
    -   Notation: $P(Y | X) = P(Y \;\text{and}\; X) / P(X) = P(Y, X) / P(X)$
        -   i.e. ratio of 2 marginal distributions
    -   Example: two tests for cancer are conducted to determine whether a biopsy should be performed
        -   Conditional approach: Biopsy everyone at determined to be high risk from test 1; measure the genetic marker (aka test 2) for patients at intermediate risk [**and**]{style="color: #009499"} biopsy those with a probability of cancer past a certain level based on the marker
    -   When we perform regression analysis, we are essentially estimating conditional distributions. The conditional distribution, $P(Y|X_1, \ldots, X_n)$ represents the distribution of the response variable, $Y$, given the specific values of the predictor variables, $X_1, \ldots, X_n$.

-   [**Empirical CDF**]{style="color: #009499"}

    $$
    F_n (x) = \frac {1}{n} \sum_{i = 1}^n I(X_i \leq x)
    $$

    -   Where $X_1, X_2,\ldots,X_n$ are from a population with CDF, $F_n (x)$
    -   Process
        -   Take n samples from an unknown distribution. The more samples you take, the closer the empirical distribution will resemble the true distribution.
        -   Sort these samples, and place them on the x-axis.
        -   Start plotting a 'step-function' style line --- each time you encounter a datapoint on the x-axis, increase the step by 1/N.
    -   [Example]{.ribbon-highlight}\
        ![](./_resources/Distributions.resources/1-U2Y7Q-lFWex2loA6X9Cy_Q.png)
        -   The CDF of a normal distribution (green) and its empirical CDF (blue)

-   [**Joint Probability Distribution**]{style="color: #009499"} - Assigns a probability value to all possible combinations of values for a set of random variables.

    -   Notation: $P(x_1, x_2, ... ,x_n)$
    -   Plugging in a value for each random variable returns a probability for that combination of values
    -   [Example]{.ribbon-highlight}: Two tests for cancer are conducted to determine whether a biopsy should be performed
        -   Joint approach: biopsy anyone who is *either* at high risk of cancer (test 1) *or* who was determined to have a probability of cancer past a certain level, based on the marker from the genetic test (test 2)
        -   Compare with example in Conditional Probability Distributions
    -   In the context of regression modeling: the joint distribution refers to the distribution of all the variables involved in the regression analysis. For example, if you have a regression model with a response variable $Y$ and predictor variables $X_1, \ldots, X_n$, the joint distribution would describe the combined distribution of $Y, X_1, \ldots, X_n$.

-   [**Location**]{style="color: #009499"} - Distribution parameter determines the shift of the distribution

    -   e.g. mean, mu, of the normal distribution.

-   [**Marginal Probability Distribution**]{style="color: #009499"} - Assigns a probability value to all possible combinations of values for a subset of random variables

    -   Notation: $P(x_1)$
        -   $P(x_1,x_2)$ is sometimes called the [**Joint Marginal Probability Distribution**]{style="color: #009499"}
    -   The marginal distribution, $P(Y)$ where $Y$ is a subset of random variables, is calculated from the joint distribution, $P(Y = y, Z = z)$ where $Z$ is the subset of random variables not in $Y$ .
        -   $P(Y) = \sum_{Z=z} P(Y = y, Z = z)$
            -   If $Y$ is just one variable
                -   Says sum all the joint probabilities for all the combinations of values for the variables in $Z$ while holding $Y$ constant
                -   Repeat for each value of $Y$ to get this summed probability value
                -   The marginal distribution is made up of all these values, one for each value of $Y$ (or combination of values if $Y$ is a subset of variables)
        -   When the joint probability distribution is in tabular form, one just sums up the probabilities in each row where $Y = y$.
        -   In the context of regression modeling, the marginal distribution of $Y$ represents the distribution of $Y$ alone, without considering the specific values of the predictor variables.

-   [**Scale**]{style="color: #009499"} - Distribution parameter; the larger the scale parameter, the more spread out the distribution

    -   e.g. s.d., sigma, $\sigma$ of the normal distribtution
    -   *Rate Parameter*: the inverse of the scale parameter (see [Gamma distribution](distributions.qmd#sec-dist-gamma){style="color: green"})

-   [**Shape**]{style="color: #009499"} - Distribution parameter that affects the shape of a distribution rather than simply shifting it (as a location parameter does) or stretching/shrinking it (as a scale parameter does).

    -   e.g. "Peakedness" refers to how round the main peak is

## R's Distribution Functions {#sec-distr-rfun .unnumbered}

### Misc {#sec-distr-rfun-misc .unnumbered}

-   Examples of other probability distributions: `runif`, `pexp`, `qchisq`, `dt`

### Naming Convention {#sec-distr-rfun-name .unnumbered}

-   The functions follow a naming convention based on the distribution and the operation:
    -   `r<distribution>`: Generates random numbers from the distribution
    -   `d<distribution>`: Computes the density (or probability mass function) of the distribution
    -   `p<distribution>`: Computes the cumulative distribution function (CDF)
    -   `q<distribution>`: Computes the quantile function (inverse of the CDF)

### Normal {#sec-distr-rfun-norm .unnumbered}

-   `rnorm(n, mean, sd)`: Generates `n` random numbers from the normal distribution with specified `mean` and standard deviation `sd`.
-   `dnorm(x, mean, sd)`: Computes the density of the normal distribution at `x` with specified `mean` and `sd`.
-   `pnorm(q, mean, sd)`: Computes the CDF of the normal distribution at `q` with specified `mean` and `sd`.
-   `qnorm(p, mean, sd)`: Computes the quantile (inverse CDF) of the normal distribution for probability `p` with specified `mean` and `sd`.

### Binomial {#sec-distr-rfun-bin .unnumbered}

-   `rbinom(n, size, prob)`: Generates `n` random numbers from the binomial distribution with `size` trials and success probability `prob`.
-   `dbinom(x, size, prob)`: Computes the probability mass function of the binomial distribution at `x` with `size` trials and success probability `prob`.
-   `pbinom(q, size, prob)`: Computes the CDF of the binomial distribution at `q` with `size` trials and success probability `prob`.
-   `qbinom(p, size, prob)`: Computes the quantile of the binomial distribution for probability `p` with `size` trials and success probability `prob`.

## Tests {#sec-distr-tests .unnumbered}

-   [Why normality tests are great... as a teaching example and should be avoided in research](https://garstats.wordpress.com/2022/09/30/normtest/)
    -   tl;dr; KS test has very low power as a Normality test as compared to Shapiro-Wilk, and Shapiro-Wilk isn't very good for n \< 100
    -   For detecting moderate skew, you want at least n \> 75 to get 80% power for Shapiro-Wilk
    -   Shapiro-Wilk can detect very fat tails at n \< 100, but would require larger sample sizes to detect more moderately thick tails.
    -   KS is worthless in detecting fat tails and near-worthless at detecting skew
    -   When n gets large (e.g. 1000s), these types of tests will almost always reject the null even when the practical deviation from normality is not practically significant.
-   [Kolmogorov--Smirnov test (KS)]{.underline}
    -   Used to compare distributions
        -   Can be used as a Normality test or any distribution test
        -   Can compare two samples
    -   Misc
        -   Vectors may need to be standardized (e.g. normality test) first *unless* comparing two samples H~0~: Both distributions are from the same distribution
    -   Packages
        -   [{KSgeneral}]{style="color: #990000"} has tests to use for contiuous, mixed, and discrete distributions written in C++
        -   [{stats}]{style="color: #990000"} and [{dgof}]{style="color: #990000"} also have functions, `ks.test`
            -   Both handle continuous and discrete distributions
        -   All functions take a numeric vector and a base R density function (e.g. `pnorm`, `pexp`, etc.) as args
            -   KSgeneral docs don't say you can supply your own comparison sample (2nd arg) only the density function but with stats and dgof, you can.
            -   Although they have function to compute the CDFs, so if you need speed, it might be possible to use their functions and do it manually
    -   2-sample test as the greatest distance between the CDFs (Cumulative Distribution Function) of each sample
        -   Specifically, this test determines the distribution of your unknown data sample by constructing and comparing the sample's *empirical CDF*Â  (see Terms) with the CDF you hypothesized. If the two CDFs are close, your unknown data sample likely follows the hypothesized distribution.
    -   KS statistic, $D_{n,m} = \max|\text{CDF}_1 - \text{CDF}_2|$ where $n$ as the number of observations on Sample 1 and $m$ as the number of observations in Sample 2
    -   Compare the KS statistic with the respective KS distribution based on parameter "en" to obtain the p-value of the test
        -   $en = (m \times n) / (m + n)$

## Beta {#sec-distr-beta .unnumbered}

![](./_resources/Distributions.resources/Beta-Distribution4.png){.lightbox width="424"}

-   Defined on the interval \[0,1\]

-   The key difference between the Binomial and Beta distributions is that for the Beta distribution the probability, x, is a random variable, however for the Binomial distribution the probability, x, is a fixed parameter.

-   Shape parameters are $\alpha$ and $\beta$, usually.

    -   $\alpha$ and $\beta$ are two positive parameters that appear as exponents of the random variable

-   pdf

    $$
    f(x) = \frac {x^{\alpha - 1} (1-x)^{\beta - 1}} {B(\alpha, \beta)}
    $$

-   $\mathbb{E}(X) = \frac {\alpha} {\alpha + \beta}$

-   $\text{Var}(X) = \frac {\alpha \cdot \beta} {(\alpha + \beta)^2 \cdot (\alpha + \beta + 1)}$

## Beta-Binomial {#sec-distr-betbin .unnumbered}

::: {layout-ncol="2"}
![Where k is the number of events in n trials](_resources/Distributions.resources/Beta-binomial_distribution_pmf.webp){.lightbox width="432"}

![Where $\theta$ is the probability of an event](_resources/Distributions.resources/beta-binom-theta-1.png){.lightbox width="512"}
:::

-   Used when the probability of success, p, in a fixed number of Bernoulli trials is unknown or random and can change from trial to trial.
-   Shape parameters Î± and Î² define the probability of success (i.e. the success parameter is modeled by the Beta Distribution).
    -   For large values of Î± and Î², the distribution approaches a binomial distribution.
    -   When Î± and Î² both equal 1, the distribution equals a discrete uniform distribution from 0 to n
-   Accuracy analysis data from psychology follow beta-binomial distributions (Jaeger, 2008; Kruschke, 2014)

## Cauchy {#sec-distr-cauchy .unnumbered}

-   It's a Student t-distribution with one degree of freedom
-   The Hodges-Lehmann estimate is an efficient estimator of the population median (See [Outliers \>\> Statistics](outliers.qmd#sec-outliers-stat){style="color: green"} \>\> Hodges-Lehmann Estimator)

## Dirichlet {#sec-distr-dirichlet .unnumbered}

-   A family of continuous multivariate probability distributions parameterized by a vector Î± of positive reals

## Exponential {#sec-distr-exp .unnumbered}

![](./_resources/Distributions.resources/image.2.png)

-   Notes from
    -   [Statistical Rethinking \>\> Chapter 10](https://ercbk.github.io/Statistical-Rethinking-Notebook/qmd/chapter-10.html){style="color: green"}
-   Constrained to be zero or positive
-   Fundamental distribution of distance and duration, kinds of measurements that represent displacement from some point of reference, either in time or space.
-   If the probability of an event is constant in time or across space, then the distribution of events tends towards exponential.
-   Its shape is described by a single parameter, the rate of events $\lambda$, or the average displacement $\lambda â1$ .
-   This distribution is the core of survival and event history analysis

## Gamma {#sec-distr-gamma .unnumbered}

![](./_resources/Distributions.resources/image.png){.lightbox width="432"}

-   Notes from

    -   [Statistical Rethinking \>\> Chapter 10](https://ercbk.github.io/Statistical-Rethinking-Notebook/qmd/chapter-10.html){style="color: green"}

-   Constrained to be zero or positive

-   Like Exponential but can have a peak above zero

-   [If an event can only happen after two or more exponentially distributed events happen, the resulting waiting times will be gamma distributed.]{.underlined}

    -   e.g. age of cancer onset is approximately gamma distributed, since multiple events are necessary for onset.

-   The gamma can be viewed as the sum of iid n exponential random variables. Exponential random variables have a rate parameter, so it makes sense for the Gamma to inherit a rate parameterization. The rate parameter also happens to be related to a scale parameter, so it makes sense for the Gamma to have a scale parameterization.

-   Shape parameter $k$ and a scale parameter $\theta$

-   $\mathbb{E}[X] = k\theta = \frac{\alpha}{\beta}$

    -   Shape parameter $\alpha = k$ and an
    -   Inverse Scale parameter (aka *Rate Parameter*) $\beta = \frac {1}{\theta}$
    -   Therefore if you want a gamma distributions with a certain "mean" and "standard deviation," you'd:
        -   Set your mean to $\mathbb{E}[X]$, your standard deviation to $\theta$ (probably but maybe it's $\beta$)
        -   Calculate $\beta$
        -   Calculate $\alpha$
        -   `prior(gamma(alpha, beta))`

-   [Example]{.ribbon-highlight}: Gamma distribution as the sums of random exponential variables\
    ![](_resources/Distributions.resources/gamma-sum-of-exp-1.png){.lightbox width="432"}

    ``` r
    n <- 12
    beta <- 1.2

    rvs <- replicate(1000, {
      sum(rexp(n, beta))
    })

    hist(rvs, freq = F)
    curve(dgamma(x, shape = n, rate = beta), col='red', add=T)
    ```

    -   Gamma distribution density overlayed with a histogram of exponential variable sums

-   Used in Survival Regression

## Gumbel {#sec-distr-gumb .unnumbered}

![](_resources/Distributions.resources/Gumbel-Density.svg){.lightbox width="432"}

-   Known as the type-I generalized extreme value distribution
    -   EVT says it is likely to be useful if the distribution of the underlying sample data is of the normal or exponential type.
-   Used to model the distribution of the maximum (or the minimum) of a number of samples of various distributions.
    -   To model minimums, use the negative of the original data.
-   Use Cases
    -   Represent the distribution of the maximum level of a river in a particular year if there was a list of maximum values for the past ten years.
    -   Predicting the chance that an extreme earthquake, flood or other natural disaster will occur.
    -   Distribution of the residuals in Multinomial Logit and Nested Logit models
-   Parameters
    -   Gumbel($\mu, \beta$) (location, scale)
    -   Mean: $\mu + \beta\gamma$ where $\gamma$ is Euler's constant ($\approx$ 0.5772)
    -   Median: $\mu - \beta \ln(\ln(2))$
    -   Mode: $\mu$
    -   Variance: $\frac{\pi^2}{6}\beta^2$
    -   Standard Gumbel: When $\mu = 0$, mean = $\gamma$, median = $-\ln(\ln(2)) \approx 0.3665$ and the standard deviation = $\pi/\sqrt{6}$

## Lognormal {#sec-distr-lognorm .unnumbered}

::: {layout-ncol="2"}
![](_resources/Distributions.resources/Log-normal-pdfs-1.png){.lightbox group="lognorm" width="332"}

![](_resources/Distributions.resources/Log-normal-cdfs-1.png){.lightbox group="lognorm" width="332"}
:::

-   When a random variable $X$ follows a lognormal distribution, $\ln(X)$ follows a normal distribution

    -   $\mu$ is the mean (Normal distribution) of the transformed observed data, i.e. the mean of $\ln x$. (similarly for $\sigma$)
    -   Exponentiating a random varible $X$ that follows a Normal distribution $e^X$ results in variable that follows the Lognormal distribution.

-   Relationship between Normal and Lognormal\
    ![Figure by Pavlovic](_resources/Distributions.resources/lognorm-norm-1.webp){.lightbox width="532"}

-   pdf\
    $$
    \frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{(\ln x - \mu)^2}{2\sigma^2}}
    $$\

-   Mean is $e^{\mu + \sigma / 2}$

    -   MLE for the mean is $\hat \mu = \frac{\sum{\ln x_i}}{n}$

-   Median is $e^\mu$

-   Variance

    $$
    e^{2\mu + \sigma^2}(e^{\sigma^2} - 1)
    $$

    -   MLE for the variance is $\hat \sigma= \frac{\sum (\ln x_i - \hat \mu)^2}{n-1}$

## Multivariate Normal {#sec-distr-multnorm .unnumbered}

-   If the random variable components in the vector are not normally distributed themselves, the result is not multivariate normally distributed.
-   Variance-Covariance matrix must be semi-definite and therefore symmetric
    -   Example of not symmetric for two random variables

## Normal (aka Gaussian) {#sec-distr-norm .unnumbered}

-   Special case of Student's t-distribution with the $\nu$ parameter (i.e. degree of freedom) set to infinity.

## Pareto {#sec-distr-pareto .unnumbered}

-   Also see [Extreme Value Theory \>\> Distribution Tail Classification](extreme-value-theory-(evt).qmd#sec-evt-distrtail){style="color: green"}
-   "Gaussian distributions tend to prevail when events are completely independent of each other. As soon as you introduce the assumption of interdependence across events, Paretian distributions tend to surface because positive feedback loops tend to amplify small initial events."
-   Pareto has similar relationship with the exponential distribution as lognormal does with normal $$
    Y_{exp} = \log \frac {X_{pareto}} {x_m}
    $$
    -   Where $X_{pareto} = x_m e^{Y_{\text{exp}}}$
        -   $x_m$ is the (positive) minimum of the randomly distributed pareto variable, X that has index Î±
        -   $Y_{exp}$ is exponentially distributed with rate $\alpha$
-   Some theoretical statistical moments may not exist
    -   If the theoretical moments do not exist, then calculating the sample moments is useless
    -   [Example]{.ribbon-highlight}: Pareto ($\alpha$ = 1.5) has a finite mean and an infinite variance
        -   Need $\alpha > 2$ for a finite variance
        -   Need $\alpha > 1$ for a finite mean
        -   In general you need $\alpha > p$ for the p^th^ moment to exist
        -   If the n^th^ moment is not finite, then the (n+1)^th^ moment is not finite.
-   Fat Tails $$
    \bar{F} = x^{-\alpha} L(x)
    $$
    -   $L(x)$ is just characterized as slowly varying function that gets dominated by the decaying inverse power law element, $x-\alpha$. as $x$ goes to infinity
        -   $\alpha$ is a shape parameter, aka "tail index" aka "Pareto index"

## Poisson {#sec-distr-poisson .unnumbered}

-   Obtained as the limit of the binomial distribution when the number of attempts is high and the success probability low. Or the Poisson distribution can be approximated by a normal distribution when Î» is large

-   Probability Mass Function $$
    \text{Pr}(Y = y) = f(y; \lambda) = \frac {e^{-\lambda} \cdot \lambda^y} {y!}
    $$

    -   $\mathbb{E}[Y] = \text{Var}(Y) = \lambda$

-   [{distributions3}]{style="color: #990000"}

    -   Stats

        ``` r
        Y <- Poisson(lambda = 1.5) 
        print(Y) 
        ## [1] "Poisson distribution (lambda = 1.5)"

        mean(Y) 
        ## [1] 1.5 
        variance(Y) 
        ## [1] 1.5 
        pdf(Y, 0:5) 
        ## [1] 0.22313 0.33470 0.25102 0.12551 0.04707 0.01412 
        cdf(Y, 0:5) 
        ## [1] 0.2231 0.5578 0.8088 0.9344 0.9814 0.9955 
        quantile(Y, c(0.1, 0.5, 0.9)) 
        ## [1] 0 1 3 
        set.seed(0) 
        random(Y, 5) 
        ## [1] 3 1 1 2 3
        ```

-   Visualize\
    ![](./_resources/Distributions.resources/Screenshot%20(909).png){.lightbox width="632"}

    ``` r
    plot(Poisson(0.5), main = expression(lambda == 0.5), xlim = c(0, 15)) 
    plot(Poisson(2),   main = expression(lambda == 2),   xlim = c(0, 15)) 
    plot(Poisson(5),   main = expression(lambda == 5),   xlim = c(0, 15)) 
    plot(Poisson(10),  main = expression(lambda == 10),  xlim = c(0, 15))
    ```

## Student's t-distribution {#sec-distr-studt .unnumbered}

-   Standard Deviation

    $$
    \text{sd} = \sqrt {\frac {\nu} {\nu - 2}}
    $$

    -   $\nu$ = degrees of freedom

-   When Î½ is small, the Student's t-distribution is more robust to multivariate outliers

-   The smaller the degree of freedom, the more "heavy-tailed" it is\
    ![](./_resources/Distributions.resources/tail_prob-2-5.png){.lightbox width="383"}

    -   -3 on the y-axis says that the probability of being in the tail is 1 in 10^3^\
    -   Don't pay attention to the x-axis. Just note how much the probability of being in the tail gets larger as the dof get smaller

-   As the degrees of freedom goes to 1, the t distribution goes to the Cauchy distribution

-   As the degrees of freedom goes to infinity, it goes to the Normal distribution.

## Triangular {#sec-distr-tri .unnumbered}

-   Triangle shaped distribution

-   Useful when you have a known min and max value

    ``` r
    extraDistr::rtriang(n, a, b, c) %\>% hist()

    # Discrete distribution
    extraDistr::rtriang(n, a, b, c) %\>% round() \`\`\`
    ```

    -   n is the number of random values you wish to draw
    -   a is the min value
    -   b is the max value
    -   c is the mode
        -   Can use to adjust the skew of the distribution
