# General {#sec-cli-gen .unnumbered}

## Misc {#sec-cli-gen-misc .unnumbered}

-   Resources
    -   [Data Science at the Command Line](https://jeroenjanssens.com/dsatcl/)
-   `ctrl-r`shell command history search
    -   [McFly](https://github.com/cantino/mcfly) - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly's suggestions are prioritized in real time with a small neural network
-   Path to a folder that's above root folder:
    -   1 level up: `../desired-folder`
    -   2 levels up: `../../desired-folder`

## R {#sec-cli-gen-r .unnumbered}

-   Make an R script pipeable (From [link](https://livefreeordichotomize.com/posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#piping-to-r))

    ``` bash
    parallel "echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds"
    ```

    ``` r
    #!/usr/bin/env Rscript
    library(readr)
    library(aws.s3)

    # Read first command line argument
    data_destination <- commandArgs(trailingOnly = TRUE)[1]

    data_cols <- list(SNP_Name = 'c', ...)

    s3saveRDS(
      read_csv(
            file("stdin"), 
            col_names = names(data_cols),
            col_types = data_cols 
        ),
      object = data_destination
    )
    ```

    -   By passing `readr::read_csv` the function, `file("stdin")`, it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.

-   Killing a process

    ``` r
    system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
    ```

-   Starting a process in the background

    ``` r
    # start MLflow server
    sys::exec_background("mlflow server")
    ```

-   Delete an opened file in the same R session

    -   You \*\*MUST\*\* unlink it before any kind of manipulation of object

        -   I think this works because readr loads files lazily by default

    -   [Example]{.ribbon-highlight}:

        ``` r
        wisc_csv_filename <- "COVID-19_Historical_Data_by_County.csv"
        download_location <- file.path(Sys.getenv("USERPROFILE"), "Downloads")
        wisc_file_path <- file.path(download_location, wisc_csv_filename)
        wisc_tests_new <- readr::read_csv(wisc_file_path)
        # key part, must unlink before any kind of code interaction
        # supposedly need recursive = TRUE for Windows, but I didn't need it
        # Throws an error (hence safely) but still works
        safe_unlink <- purrr::safely(unlink)
        safe_unlink(wisc_tests_new)

        # manipulate obj
        wisc_tests_clean <- wisc_tests_new %>%
              janitor::clean_names() %>%
              select(date, geo, county = name, negative, positive) %>%
              filter(geo == "County") %>%
              mutate(date = lubridate::as_date(date)) %>%
              select(-geo)
        # clean-up
        fs::file_delete(wisc_file_path)
        ```

-   Find out which process is locking or using a file

    -   Open Resource Monitor, which can be found
        -   By searching for Resource Monitor or resmon.exe in the start menu, or
        -   As a button on the Performance tab in your Task Manager
    -   Go to the CPU tab
    -   Use the search field in the Associated Handles section
        -   type the name of file in the search field and it'll search automatically
        -   35548

## AWK {#sec-cli-gen-awk .unnumbered}

![](./_resources/CLI.resources/DeLcVfSWAAAw6OZ.jpeg){.lightbox width="632"}

-   Misc

    -   Resources
        -   [Docs](https://www.gnu.org/software/gawk/manual/gawk.html)
        -   [Awk - A Tutorial and Introduction](https://www.grymoire.com/Unix/Awk.html)

-   Print first few rows of columns 1 and 2

    ``` awk
    awk -F, '{print $1,$2}' adult_t.csv|head
    ```

-   Filter lines where no of hours/ week (13th column) \> 98

    ``` awk
    awk -F, ‘$13 > 98’ adult_t.csv|head
    ```

-   Filter lines with "Doctorate" and print first 3 columns

    ``` awk
    awk '/Doctorate/{print $1, $2, $3}' adult_t.csv
    ```

-   Random sample 8% of the total lines from a .csv (keeps header)

    ``` awk
    'BEGIN {srand()} !/^$/ {if(rand()<=0.08||FNR==1) print > "rand.samp.csv"}' big_fn.csv
    ```

-   Decompresses, chunks, sorts, and writes back to S3 (From [link](https://livefreeordichotomize.com/posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html))

    ``` awk
    # Let S3 use as many threads as it wants
    aws configure set default.s3.max_concurrent_requests 50

    for chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do

            aws s3 cp s3://$batch_loc$chunk_file - |
            pigz -dc |
            parallel --block 100M --pipe  \
            "awk -F '\t' '{print \$1\",...\"$30\">\"chunked/{#}_chr\"\$15\".csv\"}'"

            # Combine all the parallel process chunks to single files
            ls chunked/ |
            cut -d '_' -f 2 |
            sort -u |
            parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'

            # Clean up intermediate data
            rm chunked/*
    done
    ```

    -   Uses [pigz](https://linux.die.net/man/1/pigz) to parallelize decompression

    -   Uses GNU Parallel ([site](https://www.gnu.org/software/parallel/), [docs](https://www.gnu.org/software/parallel/man.html), [tutorial1](https://jeroenjanssens.com/dsatcl/chapter-8-parallel-pipelines#introducing-gnu-parallel), [tutorial2](https://www.gnu.org/software/parallel/parallel_tutorial.html#gnu-parallel-tutorial)) to parallelize chunking (100MB chunks in 1st section)

    -   Chunks data into smaller files and sorts them into directories based on a chromosome column (I think)

    -   Avoids writing to disk
