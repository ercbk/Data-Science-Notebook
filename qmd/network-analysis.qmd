# Network Analysis

TOC

* Misc
* Layout Algorithms
* Analysis




Misc

* **Edge Bundling** - visually bundle together similar edges to reduce the visual clutter within a graph



Layouts/Algorithms

* Spring
	* [Fruchterman-Reingold force-directed algorithm](https://en.wikipedia.org/wiki/Force-directed_graph_drawing)
		* arranges the nodes so the edges have similar length and minimum crossing edges
* Random - nodes positioned uniformly at random in the unit square
* Circular - nodes on a circle
* Bipartite - nodes in two straight lines
* Spectral - nodes positioned using the eigenvectors of the graph Laplacian



Analysis

* Statistics
	* Types
		* **Centrality**
			* Measures, abstractly, how important a given graph is to the connectivity of the overall graph
			* higher for nodes which lie in paths that efficiently connect many nodes to each other.
			* Types: betweenness and closeness
		* **Clustering coefficient** 
			* Measures the density of a node’s local portion of the graph.
			* Nodes who have neighbors that are all connected to each other will have a higher clustering coefficient
		* **Degree** - total edges a given node has.
	* Issues with Statistics
		* They are unable to leverage node features at all. All nodes with the same values for these summary statistics are indistinguishable from each other.
		* There is no learnable component in the production of these features. We cannot fit a custom objective or train them jointly with a downstream task.
* Node Embeddings
		Learnable vectors of numbers that can be mapped to each node in the graph, allowing us to learn a unique representation for each node.
		
		Use as features in a downstream model.
		
		Methods
			[DeepWalk](https://arxiv.org/pdf/1403.6652.pdf) and [Node2vec](https://arxiv.org/pdf/1607.00653.pdf) papers
				Use the concept of a random walk, which involves beginning at a given node and randomly traversing edges, to produce pairs of nodes that are nearby each other.
				
				Trained by maximizing the cosine similarity between nodes that co-occurred in random walks.
					This training objective leverages the [homophily assumption](https://en.wikipedia.org/wiki/Network_homophily), which states that nodes that are connected to each other tend to be similar to each other.
					
		Issues with Embeddings
			They do not use node features at all. They assume that close-by nodes are similar without actually using the node features to confirm this assumption.
			
		* They rely on a fixed mapping from node to embedding (i.e. this is a transductive method).
			* For dynamic graphs, where new nodes and edges may be added, the algorithm must be re-ran from scratch, and all node embeddings need to be recalculated. In real-world problems, this is quite a big issue, as most online platforms have new users signing up every day, and new edges being created constantly.
* Graph Convolutional Networks (GCN)
	* Learns representations of nodes by learning a function that aggregates a node’s neighborhood (the set of nodes connected to the original node), using both graph structure and node features.
		* These representations are a function of a node’s neighborhood and are not hardcoded per node (i.e. this is an inductive method), so changes in graph structure do not require re-training the model.
	* For unsupervised learning tasks, the method is similar to Node2vec/DeepWalk
	* Layers
		* A layer takes a weighted average of the node features in the original node’s neighborhood, and the weights are learned by training the network
		* Adding layers produces aggregations that use more of the graph.
			* The span of the subgraph used to produce a node’s embedding is expanded by 1 hop.














