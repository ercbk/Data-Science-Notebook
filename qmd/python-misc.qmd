# Python, Misc

TOC

* Misc
* Big Data
* Functions
* ML Set-up Script
* Download and Unzip helper



Misc

* EDA tools
	* Misc
		* Notes from - [3 Tools for Fast Data Profiling](https://towardsdatascience.com/3-tools-for-fast-data-profiling-5bd4e962e482) (overview)
	* [Lux](https://lux-api.readthedocs.io/en/latest/) - Jupyter notebook widget that provides visual data profiling via existing pandas functions which makes this extremely easy to use if you are already a pandas user. It also provides recommendations to guide your analysis with the intent function. However, Lux does not give much indication as to the quality of the dataset such as providing a count of missing values for example.
	* [{{]{style='color: goldenrod'}[pandas_profiling](https://pandas-profiling.ydata.ai/docs/master/index.html){style='color: goldenrod'}[}}]{style='color: goldenrod'} - produces a rich data profiling report with a single line of code and displays this in line in a Juypter notebook. The report provides most elements of data profiling including descriptive statistics and data quality metrics. Pandas-profiling also integrates with Lux.
	* [{{]{style='color: goldenrod'}[sweetviz](https://pypi.org/project/sweetviz/){style='color: goldenrod'}[}}]{style='color: goldenrod'} -  provides a comprehensive and visually attractive dashboard covering the vast majority of data profiling analysis needed. This library also provides the ability to compare two versions of the same dataset which the other tools do not provide.



Big Data

* [{{datatable}}]{style='color: goldenrod'}
	* Misc
		* Resources
			* [An Overview of Python's DataTable](https://towardsdatascience.com/an-overview-of-pythons-datatable-package-5d3a97394ee9)
	* `fread`  for fast loading of large datasets

```
import datatable as dt  # pip install datatable
tps_dt = dt.fread("data/tps_september_train.csv").to_pandas()
```

* For other options (e.g. Dask, Vaex, or cuDF) see this Kaggle [notebook](https://www.kaggle.com/rohanrao/tutorial-on-reading-large-datasets)

* [{{cuDF}}]{style='color: goldenrod'}
	* [Docs](https://docs.rapids.ai/api/cudf/stable/), Beginner's [tutorial](https://developer.nvidia.com/blog/pandas-dataframe-tutorial-beginners-guide-to-gpu-accelerated-dataframes-in-python/)
	* Useful for extreme data sizes (e.g. 100s of billions of rows)
	* similar to pandas syntax
* [{{vaex}}]{style='color: goldenrod'}
	* [Docs](https://vaex.io/docs/index.html)
* [{{dask}}]{style='color: goldenrod'}
	* See [MLOps](MLOps) >> Dask


Functions

* Suppress warnings

```
import warnings
warnings.filterwarnings("ignore")
```


ML Set-up Script
```
# Suppress (annoying) warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
ignore_warnings(category=ConvergenceWarning)
if not sys.warnoptions:
    warnings.simplefilter("ignore")
    os.environ["PYTHONWARNINGS"] = ('ignore::UserWarning,ignore::RuntimeWarning')

# Ensure logging
logging.basicConfig(
    format='%(asctime)s:%(name)s:%(levelname)s - %(message)s',
    level=logging.INFO,
    handlers=[
        logging.FileHandler("churn_benchmarking.log"),
        logging.StreamHandler()
    ],
    datefmt='%Y-%m-%d %H:%M:%S')

# Determine number of cpus available
n_cpus = mp.cpu_count()
logging.info(f"{n_cpus} cpus available")

# Visualize pipeline when calling it
set_config(display="diagram")

# Load prepared (pre-cleaned) files for benchmarking
file_paths = [f for f in glob.glob("00_data/*") if f.endswith('_cleaned.csv')]
file_names = [re.search('[ \w-]+?(?=\_cleaned.)',f)[0] for f in file_paths]
dfs = [pd.read_csv(df, low_memory=False) for df in file_paths]
data_sets = dict(zip(file_names, dfs))
if not data_sets:
    logging.error('No data sets have been loaded')
    raise ValueError("No data sets have been loaded")
logging.info(f"{len(data_sets){style='color: #990000'}[}]{style='color: #990000'} data sets have been loaded.")
```

Download and Unzip helper
```
import urllib.request
from zipfile import ZipFile
import os
def extract(url: str, dest: str, target: str = '') -> None:
    """
    Retrieve online data sources from flat or zipped CSV.
    Places data in data/raw subdirectory (first creating, as needed).
    For zip file, automatically unzip target file. 
    Args:
        url (str): URL path to the source file to be downloaded 
        dest (str): File  for the destination file to land
        target (str, optional): Name of file to extract (in case of zipfile). Defaults to ''.
    """
    # set-up expected directory structure, if not exists
    if not os.path.exists('data'):
        os.mkdir('data')
    if not os.path.exists('data/raw'):
        os.mkdir('data/raw')

    # download file to desired location
    dest_path = os.path.join('data', 'raw', dest)
    urllib.request.urlretrieve(url, dest_path)
    # unzip and clean-up (remove zip) if needed
    if target != '':
        with ZipFile(dest_path, 'r') as zip_obj:
            zip_obj.extract(target, path = "data//raw")
        os.remove(dest_path)

from helpers.extract import extract
url_cps_suppl = 'https://www2.census.gov/programs-surveys/cps/datasets/2020/supp/nov20pub.csv'
extract(url_cps_suppl, 'cps_suppl.csv')
```

* From Riederer ([github](https://github.com/emilyriederer/nc-votes-duckdb/blob/master/etl/helpers/extract.py), [article](https://www.emilyriederer.com/post/duckdb-carolina/))


Extract a section of text
![](./_resources/Python,_Misc.resources/image.png)

* Desired section of text is split between 2 "~~~" strings
* Process
	* String is split into lines
	* Find the start and stop indexes for the 2 "~~~"
	* Extract lines between to the two indexes













