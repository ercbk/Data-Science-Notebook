# RAG {#sec-llms-rag .unnumbered}

-   Packages
    -   [{]{style="color: #990000"}[ragnar](https://github.com/t-kalinowski/ragnar){style="color: #990000"}[}]{style="color: #990000"} - Helps implement Retrieval-Augmented Generation (RAG) workflows.
        -   Focuses on providing a complete solution with sensible defaults, while still giving the knowledgeable user precise control over all the step
        -   Designed to be transparent, so you can inspect outputs at intermediate steps
    -   [{]{style="color: #990000"}[rdocdump](https://www.ekotov.pro/rdocdump/){style="color: #990000"}[}]{style="color: #990000"} - Dump source code, documentation and vignettes of R packages into a single file. Supports installed packages, tar.gz archives, and package source directories
        -   The output is a single plain text file or a ‘character’, which is useful to ingest complete package documentation into a large language model (‘LLM’) or pass it further to other tools, such as {ragnar} to create a Retrieval-Augmented Generation (RAG) workflow.
    -   [{]{style="color: #990000"}[RAGFlowChainR](https://github.com/knowusuboaky/RAGFlowChainR){style="color: #990000"}[}]{style="color: #990000"} - Brings Retrieval-Augmented Generation (RAG) capabilities to R, inspired by LangChain. It enables intelligent retrieval of documents from a local vector store (DuckDB), enhanced with optional web search, and seamless integration with Large Language Models (LLMs).
-   Resources
    -   [Rapid RAG Prototyping](https://blog.tidy-intelligence.com/posts/rapid-rag-prototyping/) - Example using [{ellmer}]{style="color: #990000"} and DuckDB's vss extension (See [Databases, DuckDB \>\> Extensions](db-duckdb.qmd#sec-db-duckdb-ext){style="color: green"} \>\> Vector DBs
    -   [Fresh Stack](https://fresh-stack.github.io/) - Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents
    -   [Ida Silfverskiöld articles](https://towardsdatascience.com/author/ilsilfverskiold/)
-   Process ([source](https://bsky.app/profile/tinztwinshub.bsky.social/post/3lco5lzmhus2e))\
    ![](_resources/LLM-RAG.resources/workflow-1.jpg){.lightbox width="582"}
-   Pros
    -   Context Awareness: Improves accuracy
    -   Source Citation: Access to sources enhances the transparency of the responses
    -   Reduces Hallucinations
    -   Good for domain-specific data
-   Cons
    -   Latency: Response times can be problematic for real-time applications
    -   An external knowledge base is required
-   Use Cases: Document Querying, Coversational Agents, Personalized Recommendation, Content Generation
-   Versus Fine-Tuning ([source](https://towardsdatascience.com/how-to-make-your-llm-more-accurate-with-rag-fine-tuning/))\
    ![](_resources/LLM-RAG.resources/vs-fine-tuning-1.jpg){.lightbox width="532"}
-   Chunking
    -   In RAG applications, the first part is about fetching the correct information, and getting it right is important because you can’t overload an agent with too much irrelevant information.
    -   To make it precise the chunks need to be quite small and relevant to the search query.
    -   However, if you make the chunks too small, you risk giving the LLM too little context. With chunks that are too large, the search system may become imprecise.
-   Retrieval
    -   Often benefits from hybrid search, although it may not be enough.
        -   Semantic search can connect things that answer the question without using the exact wording,
        -   Sparse methods can identify exact keywords. But sparse methods like BM25 are token-based by default, so plain BM25 won’t match substrings.
        -   If you also want to search for substrings (part of product IDs, that kind of thing), you need to add a search layer that supports partial matches as well.
