# Forecasting {#sec-diag-fcast .unnumbered}

## Misc {#sec-diag-fcast-misc .unnumbered}

-   Also see
    -   [Logistics \>\> Decision Impact Metrics](logistics.html#sec-log-dim){style="color: green"}
    -   [Loss Functions](loss-functions.qmd#sec-lossfun){style="color: green"}
-   Loss Functions/Error Metrics
    -   If you care about errors measured in percent (i.e. "relative") and your data are strictly positive, then "relative" metrics such as the MALE and RMSLE or MAPE and sMAPE are also in this class of metrics but have issues (See [Loss Functions](loss-functions.qmd#sec-lossfun){style="color: green"})
    -   If you care about errors measured in real units (e.g. number of apples), or your data can be zero or negative, then "raw" metrics such as MAE or MSE are more appropriate.
        -   MAE and RMSE are scale-dependent, so they can only be used to compare models on 1 series or on multiple series if they have the same units. MAE will lead to forecasts of the median, while minimizing the RMSE will lead to forecasts of the mean
        -   Also see [Loss Functions \>\> Misc](loss-functions.qmd#sec-lossfun-misc){style="color: green"} \>\> re Stochastic Gradient Descent in ML/DL models
    -   If you want to compare or aggregate performance metrics across time series, then you might want to use scaled metrics such as MASE, MASLE
        -   Using MASLE will require your data are strictly positive
-   How does the amount of data affect prediction
    -   Could be useful for choosing a training window for production
    -   [Example]{.ribbon-highlight}: Relative Absolute Error vs number of rows in the training set\
        ![](./_resources/Diagnostics,_Forecasting.resources/0-iuvsqiepyynQVak-.png){.lightbox width="432"}
        -   Interpretation: No pattern?
            -   Might've been useful to only look at the values from 0.5. Looks like a lot more points a cluster at data sizes between 1000 and 1200 rows.
    -   [Example]{.ribbon-highlight}: MAE vs prediction horizon (colored by the number of weeks of data in training set)\
        ![](./_resources/Diagnostics,_Forecasting.resources/0-BGiEy_2VPo9SGDnQ.png){.lightbox width="432"}
        -   Interpretation for this data and model: For prediction horizons greater than a couple weeks, having mored data in the training set leads to worse performance
-   Does the model predict values close to zero\
    ![](./_resources/Diagnostics,_Forecasting.resources/0-0yCF9TwIqRZfKEvT.png){.lightbox width="532"}
    -   "Labels" are the observed values
    -   Bad performance with values close to zero can be the result of the loss function used where lower losses are not penalized as much as higher losses

## Probabilistic {#sec-diag-fcast-prob .unnumbered}

-   **Continuous Ranked Probability Score** (CRPS)
    -   `fabletools::accuracy`
    -   Measures forecast distribution accuracy
    -   Combines a MAE score with the spread of simulated point forecasts
    -   See notebook (pg 172)
-   **Winkler Score**
    -   `fabletools::accuracy`
    -   Measures how well a forecast is covered by the prediction intervals (PI)
    -   See notebook (pg 172)
