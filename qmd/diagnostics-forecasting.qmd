# Diagnostics, Forecasting

TOC

* Misc
* Probabilistic


Misc

* Also see
	* [Demand Planning](Logistics) >> Decision Impact Metrics
	* [Loss Functions](Loss Functions)
* Loss Functions
		If you care about errors measured in percent (i.e. "relative") and your data are strictly positive, then “relative” metrics such as the MALE and RMSLE
			MAPE and sMAPE are also in this class of metrics but have issues (See [Loss Functions](Loss Functions))
			
		If you care about errors measured in real units (e.g. number of apples), or your data can be zero or negative, then “raw” metrics such as MAE or MSE are more appropriate.
			See [Loss Functions](Loss Functions) >> Misc >> re Stochastic Gradient Descent
			
		If you want to compare or aggregate performance metrics across time series, then you might want to use scaled metrics such as MASE, MASLE
			Using MASLE will require your data are strictly positive
			
* How does the amount of data affect prediction
	* Could be useful for choosing a training window for production
	* Example: Relative Absolute Error vs number of rows in the training set![](./_resources/Diagnostics,_Forecasting.resources/0-iuvsqiepyynQVak-.png)
		* Interpretation: No pattern?
			* Might've been useful to only look at the values from 0.5. Looks like a lot more points a cluster at data sizes between 1000 and 1200 rows.
	* Example: MAE vs prediction horizon (colored by the number of weeks of data in training set)![](./_resources/Diagnostics,_Forecasting.resources/0-BGiEy_2VPo9SGDnQ.png)
		* Interpretation for this data and model: For prediction horizons greater than a couple weeks, having mored data in the training set leads to worse performance
* Does the model predict values close to zero![](./_resources/Diagnostics,_Forecasting.resources/0-0yCF9TwIqRZfKEvT.png)
	* "Labels" are the observed values
	* Bad performance with values close to zero can be the result of the loss function used where lower losses are not penalized as much as higher losses




Probabilistic

* **Continuous Ranked Probability Score** (CRPS)
	* `fabletools::accuracy`
	* Measures forecast distribution accuracy
	* combines a MAE score with the spread of simulated point forecasts
	* see notebook (pg 172)
* **Winkler Score**
	* `fabletools::accuracy`
	* Measures how well a forecast is covered by the prediction intervals (PI)
	* see notebook (pg 172)


















