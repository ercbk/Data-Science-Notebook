# Designs

## Misc

-   Causal Hierarchy

![](./_resources/Experiments,_Design.resources/image.png)

-   Preference of experiment in measuring causality where RCT is the most desirable
-   To adequately guide decision making by all stakeholders, report estimates of both the intention-to-treat effect and the per-protocol effect, as well as methods and key conditions underlying the estimation procedures.

## Types

-   RCT
    -   Cannot return valid causal estimates of the treatment effect at the participant level, but it can return a valid causal estimate of the average treatment effect (ATE), in the population
        -   Approaches for estimating  the ATE
            -   Change-Score model (see [Experiments, RCT](Experiments,%20RCT) \>\> Change Score Model)
            -   ANCOVA model (see [ANOVA](ANOVA%20note) \>\> ANCOVA)
    -   Typical procedure
        -   recruit participants from the target population,
        -   measure the outcome variable during a pre-treatment assessment,
        -   randomly assign participants into
            -   a control condition or
            -   an experimental treatment condition,
        -   treat the participants in the treatment condition, and
        -   measure the outcome variable again at the conclusion of treatment.
    -   Reasons for not running a RCT
        1.  It's just not technically feasible to have individual-level randomization of users as we would in a classical A/B test
            -   e.g. randomizing which individuals see a billboard ad is not possible
        2.  We can randomize but expect interference between users assigned to different experiences, either through word-of-mouth, mass media, or even our own ranking systems; in short, the [stable unit treatment value assumption](https://en.wikipedia.org/wiki/Rubin_causal_model#Stable_unit_treatment_value_assumption_(SUTVA)) (SUTVA) would be violated, biasing the results
-   Quasi-Experiemental
    -   Due to the lack of a random assignment, the treatment and control groups are not equivalent before the intervention. So, any differences from these two groups could be caused by the pre-existing differences.
    -   Example
        -   Randomly choose some cities within which to show billboards and other cities to leave without.
        -   We can look for changes in the test regions at-specific-times as compared to the control regions at-specific-times.
        -   Since random changes happen all the time, we need to look historically to figure out what kinds of changes are normal so we can identify the impact of our test.
        -   Because groups of individuals are assigned based on location rather than assigning each individual at random, and without the individual randomization there is a much larger chance for imbalance due to skewness and heterogeneous differences.
    -   Types
        -   Difference-in-Differences, Regression Discontinuity Design, Synthetic Control Method, Interrupted Time Series
-   Observational
    -   Types
        -   Matching, Propensity Score Matching, Propensity Score Stratification, Inverse Probability of Treatment Weighting, and Covariate Adjustment

## Planning

-   Metrics
    -   If using multiple metrics/KPIs, make sure that you and the product manager agree on which metric/KPI should be primary and which should be secondary.
-   Where do users get **randomized**? Can depend on the KPI you're measuring.
    -   App or website login - appropriate for product purchasing
    -   A click on the first screen of the signup flow - appropriate for app subscriptions
-   Will you only be testing a **subset** of your customers?
    -   Example: testing changes in one country or platform and apply the learnings from the test before releasing them to our remaining users
    -   May affect the baseline KPI used to calculate the sample size
        -   Example: if a new feature is only going to be tested for English users on iOS the conversion rate may be different than the rate for all users on iOS. This also affects the number of users expected to enter the test because more users logged into iOS versus just English users.
-   Calculate **sample size**
    -   May take months to reach the sample size needed to determine statistical significance of a measured effect
    -   (approx) Sample Size
        -   See Sample Size/Power/MDE
    -   Issues
        -   getting more samples or running an experiment for a longer time to increase the sample size might not always be easy or feasible
    -   If your sample size is large and therefore test duration is too long, you may need to change the metric/KPI you're measuring
        -   Example
            -   KPI: test whether new feature increased the percentage of new users that returned to the app 30 days after signup.
            -   This meant the test needed to run an additional 30 days to ensure new users in the control didn't get exposed to the new feature within the 30-day engagement window we wanted to measure.
-   Does the time of year matter?
    -   Is there a **seasonality** aspect to your KPI, customer engagement, etc.?
        -   If so, the treatment effect may differ depending on when the test is conducted
-   Monitoring
    -   Confirm group/cohort proportions
        -   Example: If you have 3 treatments (aka variants) and 1 control, make sure each group has 25% of the test participants
        -   Unbalanced groups can result in violations of assumptions for the statistical tests used on the results
    -   Track KPIs
        -   Very bad treatments could substantially affect KPIs negatively. So you need to pull the plug if your business starts to tank.

## Sample Size/Power/MDE

-   Approximate Sample Size
    -   80% Power
        -   n = 8 / (effect size\^2)
            -   You can substitute correlation (?) for effect size
        -   Difference between means of two groups
            -   n = 32 / (effect size\^2)
        -   Using variance
            -   n = (16\* σ2) / δ2
                -   σ is variance of the data (outcome?)
                -   δ is the effect size
    -   90% Power
        -   n = 11 / (effect size\^2)
    -   Bayesian
        -   From https://www.rdatagen.net/post/2021-06-01-bayesian-power-analysis/
        -   Bayesian inference is agnostic to any pre-specified sample size and is not really affected by how frequently you look at the data along the way
        -   A bayesian power analysis to calculate a desired sample size entails using the posterior distribution probability threshold (or another criteria such as the variance of the posterior distribution or the length of the 95% credible interval)
    -   **Minimum Detectable Effect (MDE)** is proportional to 1/sqrt(sample_size)
    -   Example: [Gelman](https://statmodeling.stat.columbia.edu/2020/07/01/the-value-of-thinking-about-varying-treatment-effects-coronavirus-example/) (Confirming sample size of 126 has 80% power)
        -   Assumption: drug (binary treatment) increased survival rate by 25 percentage points (i.e. treatment effect)
            -   Evidently for a survival model, but Gelman uses standard z-test gaussian power calculation. So, I guess the survival model part doesn't matter.
        -   "With 126 people divided evenly in two groups, the standard error of the difference in proportions is bounded above by √(0.5\*0.5/63 + 0.5\*0.5/63) = 0.089, so an effect of 0.25 is at least 2.8 standard errors from zero, which is the condition for 80% power for the z-test."
            -   SE for the difference in 2 proportions\
                ![](./_resources/Experiments,_General.resources/image.1.png)
                -   In the example, the experiment is balanced so both the treatment and control groups have an equal number of participants (i.e. 63 in each group which is a 0.5 proportion of the total sample size)
            -   0.25 / 0.089 = 2.8 s.d. from 0
        -   Gelman's [Explanation](https://statmodeling.stat.columbia.edu/2018/03/15/need-16-times-sample-size-estimate-interaction-estimate-main-effect/): "If you have 80% power, then the underlying effect size for the main effect is 2.8 standard errors from zero. That is, the z-score has a mean of 2.8 and standard deviation of 1, and there's an 80% chance that the z-score exceeds 1.96 (in R, `pnorm(2.8, 1.96, 1, lower.tail = F)` = 0.8)."
            -   [Explanation](https://stats.stackexchange.com/questions/449697/a-power-of-0-8-implies-a-main-effect-of-2-8) of the Explanation: "A two-tail hypothesis with a significance level of 0.05 are assumed. The right-tail critical value is 1.96. The power is the mass of the sampling distribution under the alternative to the right of this decision boundary. Then we want to find a Gaussian with a standard deviation of 1 so that 80% of its mass is to the right of 1.96. Then a mean of 2.8 gives the desired outcome."
            -   Also see Notebook pg 95
-   Increasing Power
    -   Increase the expected magnitude of the effect size by:
        -   Being bold vs incremental with the hypotheses you test.
        -   Testing in new areas of the product
            -   Likely more room for larger improvements in member satisfaction
    -   Increase sample size
        -   Allocate more members (or other units) to the test
        -   Reduce the number of test groups
            -   there is a tradeoff between the sample size in each test and the number of non-overlapping tests that can be run at the same time.
    -   Test in groups where the effect is homogenous
        -   increases power by effectively lowering the variability of the effect in the test population
        -   Netflix [paper](https://arxiv.org/pdf/1910.01305.pdf)
        -   Example: Testing a feature that improves latency
            -   e.g. the delay between a member pressing play and video playback commencing
            -   Latency effects are likely to substantially differ across devices and types of internet connections
            -   Solution: run the test on a set of members that used similar devices with similar web connections
-   [{PUMP}]{style="color: #990000"} Frequentist Multilevel Model Power/Sample Size/MDE Calculation
    -   Misc
        -   [github](https://github.com/MDRCNY/PUMP), [paper](https://arxiv.org/abs/2112.15273)
            -   also has vignettes and shiny app
        -   Notes from
            -   [Video](https://www.youtube.com/watch?v=opVDy7Nxpi0&list=PL77T87Q0eoJhayMV5-dRZHiGPqJVM1WnB&index=13) useR conference 2022
        -   Assumes multi-test correction procedure (MTP) will occur
        -   Bayesian calculation for this specification would be different
    -   Factors affecting power
        -   With at least 1 outcome:
            -   design of the study; assumed model (type of regression)
            -   nbar, J, K: number of levels (e.g. students, schools)
                -   Unless block size differences are extreme, these should not affect power that much
            -   T: proportion of units treated
            -   number of covariates
                -   and R2, the proportion of variance that they explain
            -   ICC: ratio of variance at a particular level (e.g. student, school) to overall variance
        -   Unique to multiple outcomes
            -   Definitions of power
                -   Choose depends on how we define success
                -   Types
                    -   Individual: probability of rejecting a particular H0
                        -   the one you learn in stats classes
                    -   1-Minimal: probability of rejecting at least 1 H0
                    -   D-Minimal: probability of rejecting at least D H0s
                    -   Complete (Strictest): probability of rejecting all H0s
                -   Note: in the video, the presenter wasn't aware of any guidelines (e.g. 80% for Individual) for the different types of power definitions
            -   M: number of outcomes, tests
            -   rho: correlation between test statistics
            -   proportion of outcomes for which there truly are effects
            -   Multiple Testing Procedure (MTP)
    -   Uses a simulation approach
        -   Calculate test statistics under alternative hypothesis
        -   Use these test stats to calculate p-values
        -   Calculate power using the distribution of p-values
    -   `PUMP::pump_power`
        -   options
            -   Experiment
                -   Levels: 1, 2, or 3
                -   Randomization level: 1st , 2nd, or 3rd
            -   Model
                -   Intercepts: fixed or random
                -   Treatment Effects: constant, fixed, or random
            -   MTP
                -   Bonferroni: simple, conservative
                -   Holm: less conservative for larger p-values than Bonferroni
                -   Benjamini-Hochberg: controls for the false discovery rate (less conservative)
                -   Westfall-Young
                    -   permutation-based approach
                    -   takes into account correlation structure of outcomes
                    -   computationally intensive
                    -   Not overly conservative
                -   Romano-Wolf
                    -   See [Statistical Concepts](Statistical%20Concepts) \>\> Null Hypothesis Significance Testing (NHST) \>\> Romano and Wolf's correction
                    -   Similar to Westfall-Young but less restrictive
        -   Example
            -   Description
                -   Outcome: 3 level categorical
                -   2-level Block Design
                    -   "2-level":  students within schools
                    -   "Block Design": treatment/control randomization of students occurs within each school
            -   Power calculation\
                ![](./_resources/Experiments,_General.resources/Screenshot%20(977).png)
                -   d_m is the code for the experimental design (assume these are listed in the documentation)
                -   MDES is a vector of the treatment effects for each of the 3 levels of the outcome
                -   See "Factors affecting power" (above) for descriptions of some of these args.
            -   Results\
                ![](./_resources/Experiments,_General.resources/Screenshot%20(979).png)
                -   See above for descriptions of the types of power (Factors affecting power \>\> Unique to multiple outcomes \>\> Definitions of Power)
                -   None: w/o multi-test correction: 81% power
                -   BF: w/ Bonferroni (multiply p-values by number of outcomes): 67%
                -   D 1,2,3 are individual power for each of the 3 levels of the outcome
                -   min 1, 2 = at least 1, 2 levels  of the outcome
                -   complete is for all 3 levels of the outcome (will always be lowest)
        -   `pump_mdes()` calculates **minimal detectable effect size (MDES)**
        -   `pump_sample()` calculates the sample size given target power (e.g. 0.80) and MDES
            -   Sample Size Types
                -   K: number of level 3 units (e.g. school districts)
                -   J: number of level 2 units (e.g. schools)
                -   nbar: number of level 1 units (e.g. students)
            -   Example\
                ![](./_resources/Experiments,_General.resources/Screenshot%20(981).png)
                -   Results\
                    ![](./_resources/Experiments,_General.resources/Screenshot%20(982).png)
        -   Observe the sensitivity of power for different design parameter values
            -   Example

                ``` r
                pgrid <- update_grid(
                    pow,
                    # vary parameter values
                    rho = seq(0, 0.9, by = 0.1)
                    # compare multiple MTPs
                    MTP = c("BF", "HO", "WY-SS", "BH")
                )
                plot(pgrid, var.vary = "rho")
                ```
-   Outputs facetted multi-line plots with
    -   y = rho, y = power
    -   multiple lines by MTP
    -   facetted by power definition

## Factorial Designs (aka Multifactorial Designs)

-   Two or more independent variables that are qualitatively different
    -   Each has two or more levels
-   Notation
    -   Described in terms of number of IVs and number of levels of each IV
    -   Example 2 X 2 X 3
        -   3 IVs
            -   2 with 2 levels and 1 with 3 levels
        -   Results in 12 conditions
-   Flavors
    -   Between-subjects: different subjects participating in each cell of the matrix

    -   Within-subjects: the same subjects participating in each cell of the matrix

    -   Mixed: a combination where one (or more) factor(s) is manipulated between subjects and another factor(s) is manipulated within subjects![](./_resources/Experiments,_Design.resources/Screenshot%20(501).png)

    -   Combined/Expericorr![](./_resources/Experiments,_Design.resources/Screenshot%20(502).png)

        -   In this example both depressed and non-depressed categories are between-subjects & non-experimental

            -   I think experimental/non-experimental terminology is the same as manipulated/measured

        -   Believe the no

        -   An experimental design that includes one or more manipulated independent variables and one or more preexisting participant variables that are measured rather than manupulated

        -   Sometimes participant continuous variables are dicotomized to keep a strict factorial design but this may bias the results by missing effects that are actually present or obtaining effects that are statistical artifacts. (Should just use multivariable regression instead)

            -   Median-split procedure -- participants who score below the median on the participant variable are classified as low, and participants scoring above the median are classified as high
            -   Extreme groups procedure -- use only participants who score very high or low on the participant variable (such as lowest and highest 25%)

        -   Use cases

            -   Determine whether effects of the independent variable generalize only to participants with particular characteristics
            -   Examine how personal characteristics relate to behavior under different experimental conditions
            -   Reduce error variance by accounting for individual differences among participants

## Observational

-   Matching and Propensity Score Matching
-   Propensity Score Stratification
-   Inverse Probability of Treatment Weighting
-   Covariate Adjustment

## Quasi-Experimental

-   Typical Preconditions
    -   The treated group looks like the control group (similarity for comparability);
    -   A sufficiently large number of observations within each group (a large n)
-   Randomizing at the lowest level possible Notes from: [Key Challenges with Quasi Experiments at Netflix](https://netflixtechblog.com/key-challenges-with-quasi-experiments-at-netflix-89b4f234b852)
    -   Description: RCTs require you to randomize similar units (e.g. individual people) into treatment and control groups. If this isn't possible at the individual level, then randomizing at the lowest level possible is the closest, next best thing.
    -   Example
        -   Netflix: Measure the impact of TV or billboard advertising on member engagement. It is impossible to have identical treatment and control groups at the member level as we cannot hold back individuals from such forms of advertising. Randomize our member base at the smallest possible level. For instance, TV advertising can be bought at TV media market level only in most countries. This usually involves groups of cities in closer geographic proximity.
    -   Problems
        -   small sample sizes
            -   e.g. If randomizing by geographical units, there are probably not too many of these
        -   high variation and uneven distributions in treatment and control groups due to heterogeneity across units
            -   e.g. London with its high population is randomly assigned to the treatment cell, and people in London love sci-fi much more than other cities. London's love for sci-fi would result in an overestimated effect.
    -   Solutions
        -   repeated randomizations (aka re-randomization)
            -   keep randomizing until we find a randomization that gives us the maximum desired level of balance on key variables across treatment cells
            -   Some problems still remain
                1.  Can only simultaneously balance on a limited number of observed variables, and it is very difficult to find identical geographic units on all dimensions
                2.  Can still face noisy results with large confidence intervals due to small sample size
        -   Implement designs involving *multiple interventions* in each treatment cell over an extended period of time whenever possible (i.e. instead of a typical experiment with single intervention period).
            -   This can help us gather enough evidence to run a well-powered experiment even with a very small sample size. Large amounts of data per treatment cell increases the power of the experiment.
        -   Use a Bayesian Dynamic Linear Model (DLM) to estimate the treatment effect
            -   uses a multivariate structure to analyze more than a single point-in-time intervention in a single region.
            -   dlm PKG (see bkmks)

## Randomized Complete Block Design (RCBD)

-   Notes from <https://www.r-bloggers.com/2020/12/accounting-for-the-experimental-design-in-linear-nonlinear-regression-analyses/>
-   The defining feature is that each block sees each treatment exactly once
-   Running a linear regression analysis without taking into account the correlation within blocks
    -   Any block-to-block variability goes into the residual error term, which is, therefore, inflated.
    -   Taking the mea
-   Advantages
    -   Generally more precise than the completely randomized design (CRD).
    -   No restriction on the number of treatments or replicates.
    -   Some treatments may be replicated more times than others.
    -   Missing plots are easily estimated.
-   Disadvantages
    -   Error degrees of freedom is smaller than that for the CRD (problem with a small number of treatments).
    -   Large variation between experimental units within a block may result in a large error term
    -   If there are missing data, a RCBD experiment may be less efficient than a CRD
-   Steps
    1.  Choose the number of blocks (minimum 2) -- e.g. 4
        -   The number of blocks is the number of "replications"
    2.  Choose treatments (assign numbers or letters for each) -- e.g. 6 trt -- A,B, C, D, E, F
        -   Treatments are assigned at random within blocks of adjacent subjects, each treatment once per block.
        -   Any treatment can be adjacent to any other treatment, but not to the same treatment within the block
    3.  Randomize the treatments and blocks
        -   Example

            ```         
            Obs block trt
            1   2     B
            2   2     C
            3   2     A
            4   2     D
            5   2     E
            6   2     F
            7   1     B
            8   1     C
            9   1     E
            10  1     A
            11  1     F
            12  1     D
            13  3     D
            14  3     A
            15  3     C
            16  3     F
            17  3     B
            18  3     E
            19  4     A
            20  4     F
            21  4     B
            22  4     C
            23  4     D
            24  4     E
            ```
