# Confidence & Prediction Intervals {#sec-cipi .unnumbered}

## Misc {#sec-cipi-misc .unnumbered}

-   Also see [Mathematices, Statistics \>\> Descriptive Statistics \>\> Understanding CI, sd, and sem Bars](mathematics-statistics.html#sec-math-statc-desc-cssb){style="color: green"}
-   SE used for CIs of the difference in proportion\
    $$
    \text{SE} = \sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}
    $$

## Terms {#sec-cipi-terms .unnumbered}

-   [**Adaptive Coverage**]{style="color: #009499"}: Setting your Expected Coverage so that your Empirical Coverage = Target Coverage

    -   Example: 90% target coverage
        -   If our model is slightly overfit, you might see that a 90% expected coverage leads to an 85% empirical coverage on a holdout dataset. To align your target and empirical coverage at 90%, may require setting expected coverage at something like 93%

-   [**Confidence Intervals**]{style="color: #009499"}:

    -   From <https://staff.math.su.se/hoehle/blog/2017/06/22/interpretcis.html>

    -   Frequentist: the confidence interval is constructed by a procedure, which, if you were to repeat the experiment and collecting samples many many times, in 95% of the experiments, the corresponding confidence intervals would cover the true value of the population mean.\
        $$
        (1-\alpha)\;100\%\: \text{CI of}\: \hat\beta_i = \hat\beta_i \pm \left[t_{(1-\alpha/2)(n-k)} \cdot \text{SE}(\hat\beta_i)\right]
        $$

        -   $t$ is the t-stat for
            -   $n-k$ = sample size - number of predictors
            -   $1-\alpha$ for 2-sided; $1 - (\alpha/2)$ for 1 sided (I think)
        -   $\text{SE}(\beta_i)$ is the sqrt of the corresponding value on the diagonal of the variance-covariance matrix for the coefficients.

    -   Bayesian: the true value is in that interval with 95% probability

-   [**Empirical Coverage**]{style="color: #009499"}: The level of coverage *actually observed* when evaluated on a dataset, typically a holdout dataset not used in training the model.

    -   Rarely will your model produce the Expected Coverage exactly

-   [**Expected Coverage**]{style="color: #009499"}: The level of confidence in the model for the prediction intervals,

    -   i.e. setting α = 0.05

-   [**Jeffrey's Interval**]{style="color: #009499"}: Bayesian CIs for Binomial proportions (i.e. probability of an event)

    ``` r
    # probability of event
    # n_rain in the number of events (rainy days)
    # n is the number of trials (total days)
    mutate(pct_rain = n_rain / n, 
           # jeffreys interval
           # bayesian CI for binomial proportions
           low = qbeta(.025, n_rain + .5, n - n_rain + .5), 
           high = qbeta(.975, n_rain + .5, n - n_rain + .5))
    ```

-   [**Prediction Intervals**]{style="color: #009499"}

    -   Standard Procedure for computing PIs for predictions (See [link](http://academic.macewan.ca/burok/Stat378/notes/moremultiple.pdf) for examples and further details)\
        $$
        \hat Y_0 \pm t^{n-p}_{\alpha/2} \;\hat\sigma \sqrt{1 + \vec x_0'(X'X)^{-1}\vec x_0}
        $$
        -   $Y_0$ is a single prediction
        -   $t$ is the t-stat for
            -   $n-p$ = sample size - number of predictors
            -   $1 - \alpha$ for 2-sided; $1 - (\alpha/2)$ for 1 sided (I think)
        -   $\hat\sigma$ is the variance given by residual standard error, `summary(Model1)$sigma`\
            $$
            S^2 = \frac{1}{n-p}\;||\;Y-X\hat \beta\;||^2
            $$
            -   $S = \hat \sigma$
            -   I think this is also the $\operatorname{MSE}/\operatorname{dof}$ that you sometimes see in other formulas
        -   $x_0$ is new data for the predictor variable values for the prediction (also would need to include a 1 for the intercept)
        -   $(X'X)^{-1}$ is the variance covariance matrix, `vcov(model)`

-   [**Target Coverage**]{style="color: #009499"}: The level of coverage you *want* to attain on a holdout dataset

    -   i.e. The proportion of observations you want to fall within your prediction intervals

## Diagnostics {#sec-cipi-diag .unnumbered}

-   **Mean Interval Score (MIS)**

    -   (Proper) Score of both coverage and interval width
        -   I don't think there's a closed range, so it's meant for model comparison
        -   Lower is better
    -   `greybox::MIS` and (scaled) `greybox::sMIS`
        -   Online docs don't have these functions, but docs in RStudio do
    -   Also `scoringutils::interval_score`
        -   Docs have formula
            -   The actual paper is dense Need to take the mean of MIS

-   **Coverage**

    -   [Example]{.ribbon-highlight}: Coverage %

        ``` r
            coverage <- function(df, ...){
              df %>%
                mutate(covered = ifelse(Sale_Price >= .pred_lower & Sale_Price <= .pred_upper, 1, 0)) %>% 
                group_by(...) %>% 
                summarise(n = n(),
                          n_covered = sum(
                            covered
                          ),
                          stderror = sd(covered) / sqrt(n),
                          coverage_prop = n_covered / n)
            }
            rf_preds_test %>% 
              coverage() %>% 
              mutate(across(c(coverage_prop, stderror), ~.x * 100)) %>% 
              gt::gt() %>% 
              gt::fmt_number("stderror", decimals = 2) %>% 
              gt::fmt_number("coverage_prop", decimals = 1)
        ```

        -   From [Quantile Regression Forests for Prediction Intervals](https://www.bryanshalloway.com/2021/04/21/quantile-regression-forests-for-prediction-intervals/#coverage)
        -   [Sale_Price]{.var-text} is the outcome variable
        -   [rf_preds_test]{.var-text} is the resulting object from `predict` with a tidymodels model as input

    -   [Example]{.ribbon-highlight}: Test consistency of coverage across quintiles

        ``` r
            preds_intervals %>%  # preds w/ PIs
              mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %>%  # quintiles
              mutate(covered = ifelse(Sale_Price >= .pred_lower & Sale_Price <= .pred_upper, 1, 0)) %>% 
              with(chisq.test(price_grouped, covered))
        ```

        -   p value \< 0.05 says coverage significantly differs by quintile
        -   [Sale_Price]{.var-text} is the outcome variable

-   **Interval Width**

    -   Narrower bands should mean a more precise model

    -   [Example]{.ribbon-highlight}: Average interval width across quintiles

        ``` r
            lm_interval_widths <- preds_intervals %>% 
              mutate(interval_width = .pred_upper - .pred_lower,
                    interval_pred_ratio = interval_width / .pred) %>% 
              mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %>% # quintiles
              group_by(price_grouped) %>% 
              summarise(n = n(),
                        mean_interval_width_percentage = mean(interval_pred_ratio),
                        stdev = sd(interval_pred_ratio),
                        stderror = stdev / sqrt(n)) %>% 
              mutate(x_tmp = str_sub(price_grouped, 2, -2)) %>% 
              separate(x_tmp, c("min", "max"), sep = ",") %>% 
              mutate(across(c(min, max), as.double)) %>% 
              select(-price_grouped) 

            lm_interval_widths %>% 
              mutate(across(c(mean_interval_width_percentage, stdev, stderror), ~.x*100)) %>% 
              gt::gt() %>% 
              gt::fmt_number(c("stdev", "stderror"), decimals = 2) %>% 
              gt::fmt_number("mean_interval_width_percentage", decimals = 1)
        ```

        -   Interval width has actually been transformed into a percentage as related to the prediction (removes the scale of the outcome variable)

## Bootstrapping {#sec-cipi-boot .unnumbered}

-   Misc
    -   Do NOT bootstrap the standard deviation
        -   [article](https://eranraviv.com/bootstrap-standard-error-estimates-good-news/)
        -   bootstrap is "based on a weak convergence of moments"
        -   if you use an estimate based standard deviation of the bootstrap, you are being overly conservative (i.e. overestimate the sd)
    -   bootstrapping uses the original, initial sample as the population from which to resample, whereas Monte Carlo simulation is based on setting up a data generation process (with known values of the parameters of a known distribution). Where Monte Carlo is used to test drive estimators, bootstrap methods can be used to estimate the variability of a statistic and the shape of its sampling distribution
    -   Packages
        -   [{]{style="color: #990000"}[ebtools::get_boot_ci](https://ercbk.github.io/ebtools/reference/get_boot_ci.html){style="color: #990000"}[}]{style="color: #990000"}
-   Steps
    1.  Resample with replacement
    2.  Calculate statistic of resample
    3.  Store statistic
    4.  Repeat 10K or so times
    5.  Calculate mean, sd, and quantiles for CIs across all collected statistics
-   CIs
    -   Plenty of articles for means and models, see bkmks
    -   `rsample::reg_intervals` is a convenience function for lm, glm, survival models
-   PIs
    -   Bootstrapping PIs is a bit complicated
        -   See Shalloway's [article](https://www.bryanshalloway.com/2021/04/05/simulating-prediction-intervals/) (code included)
        -   only use out-of-sample estimates to produce the interval
        -   estimate the uncertainty of the sample using the residuals from a separate set of models built with cross-validation

## Conformal Prediction Intervals {#sec-cipi-conf .unnumbered}

-   Notes from [How to Handle Uncertainty in Forecasts: A deep dive into conformal prediction](https://towardsdatascience.com/how-to-handle-uncertainty-in-forecasts-86817f21bb54)
-   Normal PIs require iid data while conformal PIs only require the "identically distributed" part (not independent) and therefore should provide more robust coverage.
-   Steps
    1.  CV the model
    2.  On the hold-out set, compute the "conformal scores" for each observation\
        $$
        s_i = y_i - \hat p_i(y_i\;|\;X_i)
        $$
        -   Variables
            -   $s_i$: Conformal Score
            -   $y_i$: Binary label corresponding to cow or camel
            -   $\hat p_i$: Predicted probability by our model
            -   $X_i$: Predictors
            -   $i$: Index of the observed data
        -   This is just the residual of sorts.
            -   Low = good, High = bad
        -   $y_i$ is the observed label (e.g. binary 0 or 1)
            -   Good model
                -   If truth = 0, low predicted probability -\> score = low
                -   If truth = 1, high predicted probability -\> score = low
            -   Awful model
                -   If truth = 0, high predicted probability -\> score = high
                -   If truth = 1, low predicted probability -\> score = high
        -   For each observation, we calculate the conformal score
            -   e.g. for a binary target/label, each observation has two conformal scores.
        -   Only need the magnitude, so take the absolute value. Therefore, the range is between 0 and 1
    3.  Order the conformal scores from highest to lowest
    4.  Calculate the critical value for a chosen $\alpha$\
        ![](./_resources/Confidence_&_Prediction_Intervals.resources/1-xhMp1BLRRrikQV_Y2lW3Ag.png){width="432"}
        -   x-axis corresponds to an ordered set of conformal scores
        -   If $\alpha = 0.05$, find the residual value at the the 95^th^ percentile
        -   Blue: Conformal scores are not statistically significant. They're within our prediction interval.
        -   Red: Very large conformal scores indicate high divergence from the true label. These conformal scores are statistically significant and thereby outside of our prediction interval.
    5.  Compute confusion matrix (e.g. binary target where labels are 0 and 1)\
        ![](./_resources/Confidence_&_Prediction_Intervals.resources/1-sGyrptF4r4NGjCay77msbw.png){width="232"}
        -   Interpretation
            -   Top-left: predictions where both labels are not statistically significant (i.e. inside the "prediction interval").
                -   The model predicts both classes well since both labels have low scores.
                -   Depending the threshold, maybe the model could be relatively agnostic (e.g. predicted probabilites like 0.50-0.50, 0.60-0.40)
            -   Bottom-right: predictions where both labels are statistically significant  (i.e. outside the "prediction interval").
                -   Model totally whiffs. Confident it's one label when it's actually another.
                    -   Example
                        -   1 (truth) - low predicted probability = high score -\> Red and significant
                        -   0 - high predicted probability = high score -\> Red and significant
            -   Top-right: predictions where all 0 labels are not statistically significant.
                -   Model predicted the 0=class well (i.e. low scores) but the 1-class poorly (i.e. high scores)
            -   Bottom-left: predictions where all 1 labels are not statistically significant. Here, the model predicted that 1 is the true class.
                -   Vice versa of top-right
-   This can be extended to multinomial classification but it becomes more computationally intensive
    -   Also to continuous cases ([link](https://arxiv.org/pdf/2107.07511.pdf)) using quantile regression
