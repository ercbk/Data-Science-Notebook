# Confidence & Prediction Intervals

TOC

* Misc
* Terms
* Diagnostics
* Bootstrapping
* Conformal Prediction Intervals



Misc

* Also see [Statistical Concepts](Statistical Concepts) >> Fundamentals >> Understanding CI, sd, and sem Bars
* SE used for CIs of the difference in proportions![](./_resources/Confidence_&_Prediction_Intervals.resources/image.png)



Terms

* **Confidence Intervals**:
	* from https://staff.math.su.se/hoehle/blog/2017/06/22/interpretcis.html
	* Frequentist: the confidence interval is constructed by a procedure, which, if you were to repeat the experiment and collecting samples many many times, in 95% of the experiments, the corresponding confidence intervals would cover the true value of the population mean.![](./_resources/Confidence_&_Prediction_Intervals.resources/1_Y58etoD2etRvPk25RYoWTA.png)
		* Where
			* t is the t-stat for
				* n-k = sample size - number of predictors
				* 1 - α for 2-sided; 1 - (α/2) for 1 sided (I think)
			* SE(B^i) is the sqrt of the corresponding value on the diagonal of the variance-covariance matrix for the coefficients.
	* Bayesian: the true value is in that interval with 95% probability
	* **Jeffrey's Interval**: Bayesian CIs for Binomial proportions (i.e. probability of an event)

```
# probability of event
# n_rain in the number of events (rainy days)
# n is the number of trials (total days)
mutate(pct_rain = n_rain / n, 
          # jeffreys interval
          # bayesian CI for binomial proportions
          low = qbeta(.025, n_rain + .5, n - n_rain + .5), 
          high = qbeta(.975, n_rain + .5, n - n_rain + .5))
```

* **Prediction Intervals**
	* Standard Procedure for computing PIs for predictions (see [link](http://academic.macewan.ca/burok/Stat378/notes/moremultiple.pdf) for examples and further details)![](./_resources/Confidence_&_Prediction_Intervals.resources/Screenshot (764).png)
	* Where Y^0 is a single prediction
	* t is the t-stat for
		* n-p = sample size - number of predictors
		* 1 - α for 2-sided; 1 - (α/2) for 1 sided (I think)
	* σ^ is the variance given by residual standard error, `summary(Model1)$sigma` ![](./_resources/Confidence_&_Prediction_Intervals.resources/Screenshot (766).png)
		* σ^  = S
		* I think this is also the MSE/dof that you sometimes see in other formulas
	* x0 is new data for the predictor variable values for the prediction (also would need to include a 1 for the intercept)
	* (X'X)\-1 is the variance covariance matrix, `vcov(model)` 
* **Target Coverage**: The level of coverage you _want_  to attain on a holdout dataset
	* i.e. the proportion of observations you want to fall within your prediction intervals
* **Expected Coverage**: The level of confidence in the model for the prediction intervals,
	* i.e. setting α = 0.05
* **Empirical Coverage**: The level of coverage _actually observed_ when evaluated on a dataset, typically a holdout dataset not used in training the model.
	* Rarely will your model produce the Expected Coverage exactly
* **Adaptive Coverage**: Setting your Expected Coverage so that your Empirical Coverage = Target Coverage
	* Example: 90% target coverage
		* If our model is slightly overfit, you might see that a 90% expected coverage leads to an 85% empirical coverage on a holdout dataset. To align your target and empirical coverage at 90%, may require setting expected coverage at something like 93%



Diagnostics

* **Mean Interval Score (MIS)**
	* (Proper) Score of both coverage and interval width
		* I don't think there's a closed range, so it's meant for model comparison
		* Lower is better
	* `greybox::MIS` and (scaled) `greybox::sMIS`
		* Online docs don't have these functions, but docs in RStudio do
	* Also `scoringutils::interval_score`
		* Docs have formula
			* The actual paper is dense
			Need to take the mean of MIS
			
* Coverage
	* [Example]{.ribbon-highlight}: **Coverage %**

```
    coverage <- function(df, ...){
      df %>%
        mutate(covered = ifelse(Sale_Price >= .pred_lower & Sale_Price <= .pred_upper, 1, 0)) %>% 
        group_by(...) %>% 
        summarise(n = n(),
                  n_covered = sum(
                    covered
                  ),
                  stderror = sd(covered) / sqrt(n),
                  coverage_prop = n_covered / n)
    }
    rf_preds_test %>% 
      coverage() %>% 
      mutate(across(c(coverage_prop, stderror), ~.x * 100)) %>% 
      gt::gt() %>% 
      gt::fmt_number("stderror", decimals = 2) %>% 
      gt::fmt_number("coverage_prop", decimals = 1)
```

* From [Quantile Regression Forests for Prediction Intervals](https://www.bryanshalloway.com/2021/04/21/quantile-regression-forests-for-prediction-intervals/#coverage)
* "Sale\_Price" is the outcome variable
* "rf\_preds\_test" is the resulting object from `predict` with a tidymodels model as input

* [Example]{.ribbon-highlight}: Test **consistency of coverage** across quintiles

```
    preds_intervals %>%  # preds w/ PIs
      mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %>%  # quintiles
      mutate(covered = ifelse(Sale_Price >= .pred_lower & Sale_Price <= .pred_upper, 1, 0)) %>% 
      with(chisq.test(price_grouped, covered))
```

* p value < 0.05 says coverage significantly differs by quintile
* "Sale\_Price" is the outcome variable

* Interval Width
	* Narrower bands should mean a more precise model
	* [Example]{.ribbon-highlight}: **average interval width** across quintiles

```
    lm_interval_widths <- preds_intervals %>% 
      mutate(interval_width = .pred_upper - .pred_lower,
            interval_pred_ratio = interval_width / .pred) %>% 
      mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %>% # quintiles
      group_by(price_grouped) %>% 
      summarise(n = n(),
                mean_interval_width_percentage = mean(interval_pred_ratio),
                stdev = sd(interval_pred_ratio),
                stderror = stdev / sqrt(n)) %>% 
      mutate(x_tmp = str_sub(price_grouped, 2, -2)) %>% 
      separate(x_tmp, c("min", "max"), sep = ",") %>% 
      mutate(across(c(min, max), as.double)) %>% 
      select(-price_grouped) 

    lm_interval_widths %>% 
      mutate(across(c(mean_interval_width_percentage, stdev, stderror), ~.x*100)) %>% 
      gt::gt() %>% 
      gt::fmt_number(c("stdev", "stderror"), decimals = 2) %>% 
      gt::fmt_number("mean_interval_width_percentage", decimals = 1)
```

* Interval width has actually been transformed into a percentage as related to the prediction (removes the scale of the outcome variable)



Bootstrapping

* Misc
	* Do NOT bootstrap the standard deviation
		* [article](https://eranraviv.com/bootstrap-standard-error-estimates-good-news/)
		* bootstrap is "based on a weak convergence of moments"
		* if you use an estimate based standard deviation of the bootstrap, you are being overly conservative (i.e. overestimate the sd)
	* bootstrapping uses the original, initial sample as the population from which to resample, whereas Monte Carlo simulation is based on setting up a data generation process (with known values of the parameters of a known distribution). Where Monte Carlo is used to test drive estimators, bootstrap methods can be used to estimate the variability of a statistic and the shape of its sampling distribution
* Steps
	1. Resample with replacement
	2. Calculate statistic of resample
	3. Store statistic
	4. Repeat 10K or so times
	5. Calculate mean, sd, and quantiles for CIs across all collected statistics
* CIs
	* Plenty of articles for means and models, see bkmks
	* `rsample::reg_intervals` is a convenience function for lm, glm, survival models
* PIs
	* Bootstrapping PIs is a bit complicated
		* See Shalloway's [article](https://www.bryanshalloway.com/2021/04/05/simulating-prediction-intervals/) (code included)
		* only use out-of-sample estimates to produce the interval
		* estimate the uncertainty of the sample using the residuals from a separate set of models built with cross-validation



Conformal Prediction Intervals

* Notes from [How to Handle Uncertainty in Forecasts: A deep dive into conformal prediction](https://towardsdatascience.com/how-to-handle-uncertainty-in-forecasts-86817f21bb54)
* Normal PIs require iid data while conformal PIs only require the "identically distributed" part (not independent) and therefore should provide more robust coverage.
* Steps
	1. CV the model
	2. On the hold-out set, compute the "conformal scores" for each observation![](./_resources/Confidence_&_Prediction_Intervals.resources/1-NPebYHUr5pI-p4FXK327WA.png)
		* This is just the residual of sorts.
			* Low = good, High = bad
		* yi is the observed label (e.g. binary 0 or 1)
			* Good model
				* If truth = 0, low predicted probability -> score = low
				* If truth = 1, high predicted probability -> score = low
			* Awful model
				* If truth = 0, high predicted probability -> score = high
				* If truth = 1, low predicted probability -> score = high
		* For each observation, we calculate the conformal score
			* e.g. for a binary target/label, each observation has two conformal scores.
		* Only need the magnitude, so take the absolute value. Therefore, the range is between 0 and 1
	3. Order the conformal scores from highest to lowest
	4. Calculate the critical value for a chosen α![](./_resources/Confidence_&_Prediction_Intervals.resources/1-xhMp1BLRRrikQV_Y2lW3Ag.png)
		* x-axis corresponds to an ordered set of conformal scores
		* If α = 0.05, find the residual value at the the 95th percentile
		* Blue: Conformal scores are not statistically significant. They’re within our prediction interval.
		* Red: Very large conformal scores indicate high divergence from the true label. These conformal scores are statistically significant and thereby outside of our prediction interval.
	5. Compute confusion matrix (e.g. binary target where labels are 0 and 1)![](./_resources/Confidence_&_Prediction_Intervals.resources/1-sGyrptF4r4NGjCay77msbw.png)
		* Interpretation
			* Top-left: predictions where both labels are not statistically significant (i.e. inside the "prediction interval").
				* The model predicts both classes well since both labels have low scores.
				* Depending the threshold, maybe the model could be relatively agnostic (e.g. predicted probabilites like 0.50-0.50, 0.60-0.40)
			* Bottom-right: predictions where both labels are statistically significant  (i.e. outside the "prediction interval").
				* Model totally whiffs. Confident it's one label when it's actually another.
					* Example
						* 1 (truth) - low predicted probability = high score -> Red and significant
						* 0 - high predicted probability = high score -> Red and significant
			* Top-right: predictions where all 0 labels are not statistically significant.
				* Model predicted the 0=class well (i.e. low scores) but the 1-class poorly (i.e. high scores)
			* Bottom-left: predictions where all 1 labels are not statistically significant. Here, the model predicted that 1 is the true class.
				* Vice versa of top-right
* This can be extended to multinomial classification but it becomes more computationally intensive
	* also to continuous cases ([link](https://arxiv.org/pdf/2107.07511.pdf)) using quantile regression



https://towardsdatascience.com/how-to-handle-uncertainty-in-forecasts-86817f21bb54
https://towardsdatascience.com/mapie-explained-exactly-how-you-wished-someone-explained-to-you-78fb8ce81ff3
https://towardsdatascience.com/conformal-prediction-in-julia-351b81309e30
Notes

* CQR requires three sets of data:
	* Training data: data on which the quantile regression model learns.
	* Calibration data: data on which CQR calibrates the intervals.
		* Maybe "calibration" data is the validation set
		* In the example, he split the data into 3 equal sets
	* Test data: data on which we evaluate the goodness of intervals.
* Steps
	1. Fit quantile regression model on training data.
	2. Use the model obtained at previous step to predict intervals on calibration data.
		* PIs are predictions at the quantiles:
			* (alpha/2)\*100) (e.g 0.025, alpha = 0. 05)
			* (1-(alpha/2))\*100) (e.g. 0.975)
	3. Compute conformity scores on calibration data and intervals obtained at the previous step.
		* residuals are calculated for the PI vectors
		* Scores are calculated by taking the row-wise maximum of both (upper/lower quantile) residual vectors
	4. Get 1-alpha quantile from the distribution of conformity scores
		* This point will be the threshold
	5. Use the model obtained at step 1 to make predictions on test data.
		* Use test data to compute PI vectors (i.e. predictions at the previously stated quantiles)
		* i.e. same calculation as with the calibration data in step 2
	6. Compute lower/upper end of the interval by subtracting/adding the threshold from/to the quantile predictions (aka PIs)
		* lower conformity interval is test set lower PI - threshold
		* upper conformity interval is test set upper PI + threshold
* Py code

```
import numpy as np
from skgarden import RandomForestQuantileRegressor

alpha = .05

# 1. Fit quantile regression model on training data
model = RandomForestQuantileRegressor().fit(X_train, y_train)

# 2. Make prediction on calibration data
y_cal_interval_pred = np.column_stack([
    model.predict(X_cal, quantile=(alpha/2)*100), 
    model.predict(X_cal, quantile=(1-alpha/2)*100)])

# 3. Compute conformity scores on calibration data
y_cal_conformity_scores = np.maximum(
    y_cal_interval_pred[:,0] - y_cal, 
    y_cal - y_cal_interval_pred[:,1])

# 4. Threshold: Get 1-alpha quantile from the distribution of conformity scores
#    Note: this is a single number
quantile_conformity_scores = np.quantile(
    y_cal_conformity_scores, 1-alpha)

# 5. Make prediction on test data
y_test_interval_pred = np.column_stack([
    model.predict(X_test, quantile=(alpha/2)*100), 
    model.predict(X_test, quantile=(1-alpha/2)*100)])

# 6. Compute left (right) end of the interval by
#    subtracting (adding) the quantile to the predictions
y_test_interval_pred_cqr = np.column_stack([
    y_test_interval_pred[:,0] - quantile_conformity_scores,
    y_test_interval_pred[:,1] + quantile_conformity_scores])
```

* Data usage summary
	* The model is fit on the training data
	* The scores are calculated on the calibration data along with the threshold
	* For the conformity intervals, predictions from a model fit on the test data add/subtract the threshold from those predictions
* Positive threshold means the PIs get widened by the threshold amount, upper and lower.
	* e.g. quantile DL model
* Negative threshold means the PIs get shrank by the threshold amount, upper and lower.
	* e.g quantile RF model
* So future data (just like the test set) uses the threshold amount obtained using the training and calibration data to calculate the PIs for the those estimates
* Description
	* Conformity scores express the (signed) distance between each observation and the nearest extreme of the interval.
	* The sign is given by the position of the point, whether it falls inside or outside the interval.
		* When the point lies within the interval, the sign of conformity score is negative
		* When the point lies outside the interval, the sign of conformity score is positive.
		* When the point lies exactly on the interval, the conformity score is zero
	* Since the maximum is taken to get the overall conformity score for that data point,
		* Both the lower quantile prediction and upper quantile predictions have to lie within the interval for the overall score to be negative at that data point
		* Either or both quantile predictions have to lie outside the interval for the overall score to be positive at the data point.
		* Both quantile predictions have to lie exactly on the interval for the overall score to be zero at the data point
		* Therefore, all things being equal, the threshold is more likely to expand the interval than contract it.
			* The alpha can be adjusted if need be to get the desired coverage






Boosted models only able to fit one quantile at a time.
no xgboost quantile regression
`gbm::gbm(distribution = list(name = "quantile",alpha = 0.25)`

* where alpha is the quantile

lightgbm

* args: objective = "quantile", alpha = 0.50,
* (maybe) metric = "quantile"
	* "metric(s) to be evaluated on the evaluation set(s)"

catboost

* catboost.train(params = list(c('Quantile:alpha=0.1'))
	* params syntax might be wrong. R documentation isn't even half-assed. It's like 1% -assed


rf

* {grf}, {ranger}, {quantregForest}, {quantregRanger}
* grf's quantile\_forest method does not actually implement Meinshausen's quantile regression forest algorithm. A major difference is that grf makes splits that are sensitive to quantiles, whereas Meinshausen's method uses standard CART splits. ([Paper](https://arxiv.org/pdf/1610.01271.pdf))


Example: ranger rf quantile regression (shalloway article)

* model

```
rf_mod <- rand_forest() %>%
  set_engine("ranger", importance = "impurity", seed = 63233, quantreg = TRUE) %>%
  set_mode("regression")
set.seed(63233)
rf_wf <- workflows::workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe) %>% 
  fit(ames_train)
```

* quantile predictions

```
preds_bind <- function(data_fit, lower = 0.05, upper = 0.95){
  predict(
      rf_wf$fit$fit$fit,  # parsnip::extract_fit_engine available now
      workflows::pull_workflow_prepped_recipe(rf_wf) %>% bake(data_fit), # preprocessed data
      type = "quantiles",
      quantiles = c(lower, upper, 0.50)
  ) %>% 
  with(predictions) %>% 
  as_tibble() %>% 
  set_names(paste0(".pred", c("_lower", "_upper",  ""))) %>% 
  mutate(across(contains(".pred"), ~10^.x)) %>% 
  bind_cols(data_fit) %>% 
  select(contains(".pred"), Sale_Price, Lot_Area, Neighborhood, Years_Old, Gr_Liv_Area, Overall_Qual, Total_Bsmt_SF, Garage_Area)
}
rf_preds_test <- preds_bind(ames_holdout)
```

* Visualize![](./_resources/Confidence_&_Prediction_Intervals.resources/unnamed-chunk-3-1.png)

```
set.seed(1234)
rf_preds_test %>% 
  mutate(pred_interval = ggplot2::cut_number(Sale_Price, 10)) %>% 
  group_by(pred_interval) %>% 
  sample_n(2) %>% 
  ggplot(aes(x = .pred))+
  geom_point(aes(y = .pred, color = "prediction interval"))+
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper, color = "prediction interval"))+
  geom_point(aes(y = Sale_Price, color = "actuals"))+
  scale_x_log10(labels = scales::dollar)+ # target variable is log10 transformed
  scale_y_log10(labels = scales::dollar)+
  labs(title = "90% Prediction intervals on a holdout dataset",
      subtitle = "Random Forest Model",
        y = "Sale_Price prediction intervals and actuals")+
  theme_bw()+
  coord_fixed()
```

* 




Coverage and interval length tuning
```
def compute_coverage_len(y_test, y_lower, y_upper):
    """ Compute average coverage and length of prediction intervals
    Parameters
    ----------
    y_test : numpy array, true labels (n)
    y_lower : numpy array, estimated lower bound for the labels (n)
    y_upper : numpy array, estimated upper bound for the labels (n)
    Returns
    -------
    coverage : float, average coverage
    avg_length : float, average length
    """
    in_the_range = np.sum((y_test >= y_lower) & (y_test <= y_upper))
    coverage = in_the_range / len(y_test) * 100
    avg_length = np.mean(abs(y_upper - y_lower))
    return coverage, avg_length

```


* Notes
	* training set\_1 = 0.80, test set = 0.20
	* training set\_1 divided into train\_2 and calibration sets (50/50)
		* row indexes: idx\_train, idx\_cal
		* The  train\_2 of the split is split further for figuring out the quantiles with correct coverage
			* train\_3/test = 95/5
	* Predictions from tuned quantiles used to calculate scores
	* Uses alpha as the threshold index
	* hardcoded values
		* coverage\_factor would have to be something guess-timated or based on some previous knowledge about how "off" the algrithm's PI quantiles' coverage is
		* initial best\_avg\_length would require some knowledge of a baseline PI length and scale of the data
		* Maybe these don't have to be hardcoded
	* Values
		* quantiles: \[0.05, 0.95\]
		* quantile factor = 0.85 \* (0.95 - 0.05) = 0.76
		* alpha = 0.10 (i.e. acceptable miscoverage rate; "significance" in the code)
	* asymmetric score function (QuantileRegAsymmetricErrFunc) available



* run\_icp (cqr/helper.py)
	
	* nc, x\_train, y\_train, x\_test, idx\_train, idx\_cal, alpha
	
	* instantiates IcpRegressor (nonconformist/icp.py)
		* input: nc\_function = RegressorNC(QuantileForestRegressorAdapter (cqr/helper.py), QuantileRegErrFunc (nonconformist/nc.py))
			* RegressorNC (nonconformist/nc.py)
				* inherits
					* BaseModelNc(model, errorfunc)
						* methods
							* fit via model.fit
							* score
								* computes nonconformity scores
		* inherits
			* BaseICP
				* inherits sklearn BaseEstimater
					* methods: get\_params, set\_params
				* method
					* fit via nc.fit
					* calibrate
					* other calibrate stuff
		* method: predict
	* Runs  via IcpRegressor (nonconformist/icp.py)
		* fit
			* input: X\_train\[idx\_train,:\], y\_train\[idx\_train\]
			* QuantileForestRegressorAdapter (cqr/helper.py)
				* inherits RegressorAdapter (noncomformist/base.py)
					* inherits BaseModelAdaptor
						* inherits BaseEstimator
						* methods: fit, predict, underlying\_predict
					* methods: underlying\_predict
				* methods
					* fit
						* inputs: X\_train\[idx\_train,:\], y\_train\[idx\_train\]
						* target\_coverage = quantiles\[0\] - quantiles\[1\] (quantiles = \[0.05, 0.95\]) = 0.90
						* coverage\_factor = 0.85
						* target\_coverage = coverage\_factor \* target\_coverage = 0.765
						* range\_vals = 30
						* num\_vals = 10
						* grid\_q = grid of quantiles to search
							* lower = 0.05 to 0.35 (num\_vals = 10 evenly spaced values) (i.e. 0.05 to 0.05 + range\_vals)
							* upper = 0.95 to 0.65  (num\_vals = 10 evenly spaced values) (i.e. 0.95 to 0.95 - range\_vals)
							* concantenated pairwise (0.35, 0.65 gets fit, etc.), so I think he's fitting 10 models
						* calls CV\_quntiles\_rf (cqr/tune\_params\_cv.py)
							* inputs: rf\_params, x, y, target\_coverage, grid\_q, test\_ratio, coverage\_factor)
								* test\_ratio = 0.05
								* So train/test =  95/5
							* process
								* rf fit on training data
								* best\_avg\_length = 1e10 (initial value)
								* loop (row in row of grid\_q)
								* predicts lower and upper quantile vectors (i.e. row of the grid\_q) on test data
								* calls compute\_coverage\_len (cqr/helper.py)
								*         inputs: y\_test, y\_lower (i.e. lower quantile), y\_upper (i.e. upper quantile)
								*         computes coverage and avg length of interval
								*         check if coverage > target\_coverage AND avg length < best\_avg\_length
								*             if so, set best\_q = row of grid\_q, best avg length = avg length
								* return best\_q
							* stored in attribute, self.cv\_quantiles        
					* predict
						* uses self.cv\_quantiles
		* calibrate
			* input: X\_train\[idx\_cal,:\], y\_train\[idx\_cal\]
			* inherits BaseIcp
				* method:calibrate
					* uses nc.function.score which is RegressorNc (nonconformist/nc.py)
						* inherits BaseModelNc
							* Calc predictions with QuantileForestRegressorAdapter (cqr/helper.py) predict method
								* So it does use tuned quantiles to compute scores
							* sends predictions to QuantileRegErrFunc (nonconformist/nc.py) to calc scores
								* max(\\hat{q}\_low - y, y - \\hat{q}\_high)
				* Sorts (desc), and stores scores in self.cal\_scores
		* predict
			* input: X\_test, alpha
			* IcpRegressor.predict makes no sense to me. Looks to me that it would return an array of zeros. ¯\\\_(ツ)\_/¯
			* It uses nc.function.predict(X\_test, self.cal\_scores, significance (aka alpha)) in the code, but the conditional doesn't make sense to me
				* RegressorNC.predict
					* Calc predictions with QuantileForestRegressorAdapter (cqr/helper.py) predict method
					* Uses alpha to find the index of threshold score value through a QuantileRegErrFunc method
					* he has
						* lower PI = lower\_quantile\_pred - threshold score
						* upper PI = upper\_quantile\_pred + threshold score




Classification

* Notes from paper, "[Classification with Valid and Adaptive Coverage](https://arxiv.org/pdf/2006.02544.pdf)"
* Marginal Coverage![](./_resources/Confidence_&_Prediction_Intervals.resources/Screenshot (728).png)
	* The probability that an future observed value is in a predicted set is greater than equal some miscoverage rate.
* Conditional Coverage![](./_resources/Confidence_&_Prediction_Intervals.resources/Screenshot (727).png)
	* Valid coverage conditional on a specific observed value of the features X.
		* Stronger statement than marginal coverage and cannot be achieved in theory without strong modeling assumptions
* The only restrictions are the data are exchangeable and the training algorithm treats all samples exchangeably; i.e., it should be invariant to their order.
	* No typical ML algorithm fails this
* Adaptive classification with CV+ calibration
	* Sample Ui ∼ Uniform(0, 1) for each i ∈ {1, . . . , n + 1}, independently of everything else
	* Typical k-fold CV
		* train model on all folds except k, etc.
	* Construct prediction set, CCV+n,α![](./_resources/Confidence_&_Prediction_Intervals.resources/Screenshot (757).png)
		* Says we sweep over all possible labels y ∈ Y and include y in the final prediction set CCV+ n,α (Xn+1) if the corresponding score E(Xn+1, y, Un+1; ˆπ k(i) ) is smaller than (1 − α)(n + 1) hold-out scores E(Xi , Yi , Ui ; ˆπ k(i) ) evaluated on the true labeled data
		* 



















