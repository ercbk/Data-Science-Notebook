# NLP, LLMs

TOC

-   Misc
-   LangChain

## Misc

-   What chatGPT is:
    -   "What would a response to this question sound like" machine![](./_resources/NLP,_LLMs.resources/image.png) Researchers build (train) large language models like GPT-3 and GPT-4 by using a process called "unsupervised learning," which means the data they use to train the model isn't specially annotated or labeled. During this process, the model is fed a large body of text (millions of books, websites, articles, poems, transcripts, and other sources) and repeatedly tries to predict the next word in every sequence of words. If the model's prediction is close to the actual next word, the neural network updates its parameters to reinforce the patterns that led to that prediction.

        Conversely, if the prediction is incorrect, the model adjusts its parameters to improve its performance and tries again. This process of trial and error, though a technique called "backpropagation," allows the model to learn from its mistakes and gradually improve its predictions during the training process. As a result, GPT learns statistical associations between words and related concepts in the data set.

        In the current wave of GPT models, this core training (now often called "pre-training") happens only once. After that, people can use the trained neural network in "**inference mode**," which lets users feed an input into the trained network and get a result. During inference, the input sequence for the GPT model is always provided by a human, and it's called a "prompt." The prompt determines the model's output, and altering the prompt even slightly can dramatically change what the model produces.Iterative prompting is limited by the size of the model's "context window" since each prompt is appended onto the previous prompt. ![](./_resources/NLP,_LLMs.resources/image.1.png) ChatGPT is different from vanilla GPT-3 because it has also been trained on transcripts of conversations written by humans. "We trained an initial model using **supervised fine-tuning**: human AI trainers provided conversations in which they played both sides---the user and an AI assistant,"

        ChatGPT has also been tuned more heavily than GPT-3 using a technique called "**reinforcement learning from human feedback**," or RLHF, where human raters ranked ChatGPT's responses in order of preference, then fed that information back into the model. This has allowed the ChatGPT to produce coherent responses with fewer confabulations than the base model. The prevalence of accurate content in the data set, recognition of factual information in the results by humans, or reinforcement learning guidance from humans that emphasizes certain factual responses.

        Two major types of falsehoods that LLMs like ChatGPT might produce. The first comes from inaccurate source material in its training data set, such as common misconceptions (e.g., "eating turkey makes you drowsy"). The second arises from making inferences about specific situations that are absent from its training material (data set); this falls under the aforementioned "hallucination" label.

        Whether the GPT model makes a wild guess or not is based on a property that AI researchers call "**temperature**," which is often characterized as a "creativity" setting. If the creativity is set high, the model will guess wildly; if it's set low, it will spit out data deterministically based on its data set. If creativity is set low, "\[It\] answers 'I don't know' all the time or only reads what is there in the Search results (also sometimes incorrect). What is missing is the tone of voice: it shouldn't sound so confident in those situations."

        In some ways, ChatGPT is a mirror: It gives you back what you feed it. If you feed it falsehoods, it will tend to agree with you and "think" along those lines. That's why it's important to start fresh with a new prompt when changing subjects or experiencing unwanted responses.

        "One of the most actively researched approaches for increasing factuality in LLMs is **retrieval augmentation**---providing external documents to the model to use as sources and supporting context," said Goodside. With that technique, he explained, researchers hope to teach models to use external search engines like Google, "citing reliable sources in their answers as a human researcher might, and rely less on the unreliable factual knowledge learned during model training." Bing Chat and Google Bard do this already by roping in searches from the web, and soon, a browser-enabled version of ChatGPT will as well. Additionally, ChatGPT plugins aim to supplement GPT-4's training data with information it retrieves from external sources, such as the web and purpose-built databases.

        Other things that might help with hallucination include, "a more sophisticated data curation and the linking of the training data with **'trust' scores**, using a method not unlike PageRank... It would also be possible to fine-tune the model to hedge when it is less confident in the response." (arstechnica [article](https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/))
-   OpenAI [models](https://platform.openai.com/docs/models/models)
    -   davinci (e.g. davinci-003) text-generation models are 10x more expensive than their chat counterparts (e.g. gpt-3.5-turbo)
    -   For lower usage in the 1000's of requests per day range ChatGPT works out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests per day, open-sourced models deployed in AWS work out cheaper. (As of April 24th, 2023.) ([article](https://towardsdatascience.com/llm-economics-chatgpt-vs-open-source-dfc29f69fec1))
        -   Used AWS Lambda for deployment
    -   davinci hasn't been trained using reinforcement learning from human feedback (RLHF}
    -   chatgpt 3.5 turbo models
        -   Pros
            -   Performs better on 0 shot classification tasks than Davinci-003
            -   Outperforms Davinci-003 on sentiment analysis
            -   Significantly better than Davinci-003 at math
            -   cheaper than davinci
        -   Cons
            -   Tends to produce longer responses than Davinci-003, which may not be ideal for all use cases
            -   Including k-shot examples can lead to inefficient resource usage in multi-turn use cases
    -   davinci-003
        -   Pros
            -   Performs slightly better than GPT-3.5 Turbo with k-shot examples
            -   Produces more concise responses than GPT-3.5 Turbo, which may be preferable for certain use cases
        -   Cons
            -   Less accurate than GPT-3.5 Turbo on 0 shot classification tasks and sentiment analysis
            -   Performs significantly worse than GPT-3.5 Turbo on math tasks
-   Use Cases
    -   Understanding code (Can reduce cognative load)([article](https://www.caitlinhudon.com/posts/programming-beyond-cognitive-limitations-with-ai))
        -   During code reviews or onboarding new programmers
        -   under-commented code
    -   Generating the code scaffold for a problem where you aren't sure where or how to start solving it.
    -   LLMs don't require removing stopwords during preprocessing of documents
-   Vector databases store vectors and provide fast similarity searches for chatbots
-   Cost
    -   For lower usage in the 1000's of requests per day range ChatGPT works out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests per day, open-sourced models deployed in AWS work out cheaper. ([article](https://towardsdatascience.com/llm-economics-chatgpt-vs-open-source-dfc29f69fec1), April 24th, 2023.)
-   Methods for giving chatGPT data
    -   Think you can upload a file
    -   Through prompt
        -   See bizsci video
            -   paste actual data
            -   paste column names and types (glimse() with no values)
        -   Generate a string for each row of data that contains the column name and value
            -   Example
                -   "The <column name> is <cell value>. The <column name> is <cell value>. ..."
                -   "The fico_score is 578.0. The load_amount is 6000.0.Â  The annual income is 57643.54."
-   Evolution of LLMs![](./_resources/NLP,_LLMs.resources/image.2.png)

## LangChain

-   Framework for developing applications powered by language models; connect a language model to other sources of data; allow a language model to interact with its environment
-   Misc
    -   Notes from:
        -   [A Gentle Intro to Chaining LLMs, Agents, and utils via LangChain](https://towardsdatascience.com/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81)
            -   See article for workflow for multi-chains
            -   Also shows some diagnostic methods that are included in the library
    -   [Available vector stores](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html) for document embeddings
        -   Qdrant - open source, free, and easy to use ([example](https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736))
        -   Chroma - can be used as a local in-memory ([example](https://towardsdatascience.com/implementing-a-sales-support-agent-with-langchain-63c4761193e7))
        -   [Pinecone](https://www.pinecone.io/) - Data Elixir is using this store for their chatbot; has a free tier
    -   See [article](https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736) for description and link to code for a manual, more controlled parsing of markdown files to get text and code blocks
-   In general, **chains** are what you get by sequentially connecting one or more large language models (LLMs) in a logical way.
-   Chains can be built using LLMs or "Agents".
    -   **Agents** provide the ability to answer questions that require recent or specialty information the LLM hasn't been trained on.
        -   e.g. "What will the weather be like tomorrow?"
        -   An agent has access to an LLM and a suite of tools for example Google Search, Python REPL, math calculator, weather APIs, etc. ([list](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html) of supported agents)
        -   LLMs will use **tools** to interact with Agents ([list](https://python.langchain.com/en/latest/modules/agents/tools/getting_started.html#list-of-tools) of tools)
            -   Tool process
                -   Uses a `LLMChain` for building the API URL based on our input instructions and makes the API call.
                -   Upon receiving the response, it uses another `LLMChain` that summarizes the response to get the answer to our original question.
        -   [ReAct](https://arxiv.org/abs/2210.03629) (Reason + Act) is a popular agent that picks the most usable tool (from a list of tools), based on what the input query is.
            -   Output Components: *observation*, a *thought*, or it takes an *action*. This is mainly due to the ReAct framework and the associated prompt that the agent is using (See example with `ZERO_SHOT_REACT_DESCRIPTION`)
        -   `serpapi` is useful for answering questions about current events.
-   Chains can be simple (i.e. Generic) or specialized (i.e. Utility).
    -   **Generic** --- A single LLM is the simplest chain. It takes an input prompt and the name of the LLM and then uses the LLM for text generation (i.e. output for the prompt).
        -   Generic chains are more often used as building blocks for Utility chains
    -   **Utility** --- These are specialized chains, comprised of many LLMs to help solve a specific task. For example,
        -   LangChain supports some end-to-end chains (such as [AnalyzeDocumentChain](https://python.langchain.com/en/latest/modules/chains/index_examples/analyze_document.html) for summarization, QnA, etc) and some specific ones (such as [GraphQnAChain](https://python.langchain.com/en/latest/modules/chains/index_examples/graph_qa.html#querying-the-graph) for creating, querying, and saving graphs). [Programme Aided Language Model](https://arxiv.org/pdf/2211.10435.pdf) reads complex math problems (described in natural language) and generates programs (for solving the math problem) as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.
        -   2-Chain Examples
            -   Chain 1 is used to clean the prompt (remove extra whitespaces, shorten prompt, etc) and chain 2 is used to call an LLM with this clean prompt. ([link](https://python.langchain.com/en/latest/modules/chains/generic/transformation.html))
            -   Chain 1 is used to generate a synopsis for a play and chain is used to write a review based on this synopsis. ([link](https://python.langchain.com/en/latest/modules/chains/generic/sequential_chains.html#simplesequentialchain))
-   **Document Loaders** ([docs](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html?highlight=document%20loaders))- various helper functions that take various formats and types of data and produce a document output
    -   Formats like like markdown, word docs, text, PowerPoint, images, HTML, PDF, csvs, AsciiDoc (adoc), etc.
    -   Examples
        -   `GitLoader` function clones the repository and load relevant files as documents
        -   `YoutubeLoader` - gets subtitles from videos
        -   `DataFrameLoader` - converts text columns in panda dfs to documents
    -   Also, tons of other functions for googledrive or dbs like bigquery, duckdb or cloud storage like s3 or confluence or email or discord, etc.
-   **Text Spitters** ([Docs](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)) - After loading the documents, they're usually fed to text splitter to create chunks of text due to LLM context constraints. The chunks of text can then be transformed into embeddings.
    -   From "Sales and Support Chatbot" article in example below

```         
# Define text chunk strategy
splitter = CharacterTextSplitter(
Â  chunk_size=2000,
Â  chunk_overlap=50,
Â  separator=" "
)
# GDS guides
gds_loader = GitLoader(
Â  Â  clone_url="https://github.com/neo4j/graph-data-science",
Â  Â  repo_path="./repos/gds/",
Â  Â  branch="master",
Â  Â  file_filter=lambda file_path: file_path.endswith(".adoc")
Â  Â  and "pages" in file_path,
)
gds_data = gds_loader.load()
# Split documents into chunks
gds_data_split = splitter.split_documents(gds_data)
print(len(gds_data_split)) #771
```

-   **Embeddings**
    -   OpenAI's [text-embedding-ada-002 model](https://platform.openai.com/docs/guides/embeddings/embedding-models) is easy to work with, achieves the highest performance out of all of OpenAI's embedding models (on the [BEIR benchmark](https://arxiv.org/pdf/2104.08663.pdf)), and is also the cheapest (\$0.0004/1K tokens).
    -   HuggingFace's sentence-transformers , which reportedly has better performance than OpenAI's embeddings, but that involves downloading the model and running it on your own server.
-   [Example]{.ribbon-highlight}: Generic
    -   Build Prompt

```         
from langchain.prompts import PromptTemplate
prompt = PromptTemplate(
Â  Â  input_variables=["product"],
Â  Â  template="What is a good name for a company that makes [{product}]{style='color: #990000'}?",
)
print(prompt.format(product="podcast player"))
# OUTPUT
# What is a good name for a company that makes podcast player?
```

-   If using multiple variables, then you need, e.g. `print(prompt.format(product="podcast player", audience="childrenâ)`, to get the updated prompt.

-   Create LLMChain instance and run

```         
from langchain.llms import OpenAI
from langchain.chains import LLMChain
llm = OpenAI(
Â  Â  Â  Â  Â  model_name="text-davinci-003", # default model
Â  Â  Â  Â  Â  temperature=0.9) #temperature dictates how whacky the output should be
llmchain = LLMChain(llm=llm, prompt=prompt)
llmchain.run("podcast player")
```

-   If you had more than one input_variables, then you won't be able to use `run`. Instead, you'll have to pass all the variables as a dict.

    -   e.g., `LLMchain({âproductâ: âpodcast playerâ, âaudienceâ: âchildrenâ})`.

-   Using the less expensive chat models: `chatopenai = ChatOpenAI(model_name="gpt-3.5-turbo")`

-   [Example]{.ribbon-highlight}: Multiple Chains and Multiple Input Variables

    -   Goal: create an age-appropriate gift generator
    -   Chain 1: Find age

```         
# Chain1 - solve math problem, get the age
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain.agents import load_tools

llm = OpenAI(temperature=0)
tools = load_tools(["pal-math"], llm=llm)
agent = initialize_agent(tools,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  llm,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  verbose=True)
```

-   `pal-math` is a math-solving tool

-   React agent uses the tool to answer the age problem

-   Chain 2: Recommend a gift

```         
template = """You are a gift recommender. Given a person's age,\n
it is your job to suggest an appropriate gift for them. If age is under 10,\n
the gift should cost no more than [{budget}]{style='color: #990000'} otherwise it should cost atleast 10 times [{budget}]{style='color: #990000'}.
Person Age:
[{output}]{style='color: #990000'}
Suggest gift:"""
prompt_template = PromptTemplate(input_variables=["output", "budget"], template=template)
chain_two = LLMChain(llm=llm, prompt=prompt_template)
```

-   "[{output}]{style="color: #990000"}" is the name of the output from the 1st chain

    -   Find the name of the output of a chain: `print(agent.agent.llm_chain.output_keys)`

-   The prompt includes a conditional that transforms [{budget}]{style="color: #990000"} (more below)

-   `LLMchain` is used when there are multiple variable in the template

-   Combine Chains and Run

```         
overall_chain = SequentialChain(
Â  Â  Â  Â  Â  Â  Â  Â  input_variables=["input"],
Â  Â  Â  Â  Â  Â  Â  Â  memory=SimpleMemory(memories={"budget": "100 GBP"}),
Â  Â  Â  Â  Â  Â  Â  Â  chains=[agent, chain_two],
Â  Â  Â  Â  Â  Â  Â  Â  verbose=True)
overall_chain.run("If my age is half of my dad's age and he is going to be 60 next year, what is my current age?")
```

-   The prompt is only for the 1st chain, and it's output, Age, will be input for the second chain.

-   `SimpleMemory` is used pass the variable for the second prompt which adds some additional context to the second chain --- the [{budget}]{style="color: #990000"} for the gift.

-   Output

```         
#> Entering new SequentialChain chain...
#> Entering new AgentExecutor chain...
# I need to figure out my dad's current age and then divide it by two.
#Action: PAL-MATH
#Action Input: What is my dad's current age if he is going to be 60 next year?
#Observation: 59
#Thought: I now know my dad's current age, so I can divide it by two to get my age.
#Action: Divide 59 by 2
#Action Input: 59/2
#Observation: Divide 59 by 2 is not a valid tool, try another one.
#Thought: I can use PAL-MATH to divide 59 by 2.
#Action: PAL-MATH
#Action Input: Divide 59 by 2
#Observation: 29.5
#Thought: I now know the final answer.
#Final Answer: My current age is 29.5 years old.
#> Finished chain.
# For someone of your age, a good gift would be something that is both practical and meaningful. Consider something like a nice watch, a piece of jewelry, a nice leather bag, or a gift card to a favorite store or restaurant.\nIf you have a larger budget, you could consider something like a weekend getaway, a spa package, or a special experience.'}
#> Finished chain.
```

-   [Example]{.ribbon-highlight}: Sales and Support Chatbot ([article](https://towardsdatascience.com/implementing-a-sales-support-agent-with-langchain-63c4761193e7))
    -   Create embeddings from text data sources and store in Chroma vector store

```         
# Define embedding model
OPENAI_API_KEY = "OPENAI_API_KEY"
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
sales_data = medium_data_split + yt_data_split
sales_store = Chroma.from_documents(
Â  Â  sales_data, embeddings, collection_name="sales"
)
support_data = kb_data + gds_data_split + so_data
support_store = Chroma.from_documents(
Â  Â  support_data, embeddings, collection_name="support"
)
```

-   Sales data is from Medium articles and YouTube subtitles

-   Support data is from docs in a couple github repos and stackoverflow

-   Instantiate chatgpt

```         
llm = ChatOpenAI(
Â  Â  model_name="gpt-3.5-turbo",
Â  Â  temperature=0,
Â  Â  openai_api_key=OPENAI_API_KEY,
Â  Â  max_tokens=512,
)
```

-   Sales prompt template

```         
sales_template = """As a Neo4j marketing bot, your goal is to provide accurateÂ 
and helpful information about Neo4j, a powerful graph database used forÂ 
building various applications. You should answer user inquiries based on theÂ 
context provided and avoid making up answers. If you don't know the answer,Â 
simply state that you don't know. Remember to provide relevant informationÂ 
about Neo4j's features, benefits, and use cases to assist the user inÂ 
understanding its value for application development.
[{context}]{style='color: #990000'}
Question: [{question}]{style='color: #990000'}"""
SALES_PROMPT = PromptTemplate(
Â  Â  template=sales_template, input_variables=["context", "question"]
)
sales_qa = RetrievalQA.from_chain_type(
Â  Â  llm=llm,
Â  Â  chain_type="stuff",
Â  Â  retriever=sales_store.as_retriever(),
Â  Â  chain_type_kwargs={"prompt": SALES_PROMPT},
)
```

-   "{**context**}" in the template is the data stored in the vector store

-   "retriever" arg points to vector store (e.g. sales_store) and uses the `as_retriever` method to get the embeddings

-   Support prompt template

```         
support_template = """
As a Neo4j Customer Support bot, you are here to assist with any issuesÂ 
a user might be facing with their graph database implementation and Cypher statements.
Please provide as much detail as possible about the problem, how to solve it, and steps a user should take to fix it.
If the provided context doesn't provide enough information, you are allowed to use your knowledge and experience to offer you the best possible assistance.
[{context}]{style='color: #990000'}
Question: [{question}]{style='color: #990000'}"""
SUPPORT_PROMPT = PromptTemplate(
Â  Â  template=support_template, input_variables=["context", "question"]
)
support_qa = RetrievalQA.from_chain_type(
Â  Â  llm=llm,
Â  Â  chain_type="stuff",
Â  Â  retriever=support_store.as_retriever(),
Â  Â  chain_type_kwargs={"prompt": SUPPORT_PROMPT},
)
```

-   See Sales template

-   Add React agent to determine whether LLM should use Sales or Support templates and contexts

```         
tools = [
Â  Â  Tool(
Â  Â  Â  Â  name="sales",
Â  Â  Â  Â  func=sales_qa.run,
Â  Â  Â  Â  description="""useful for when a user is interested in various Neo4j information,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  use-cases, or applications. A user is not asking for any debugging, but is only
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  interested in general advice for integrating and using Neo4j.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Input should be a fully formed question.""",
Â  Â  ),
Â  Â  Tool(
Â  Â  Â  Â  name="support",
Â  Â  Â  Â  func=support_qa.run,
Â  Â  Â  Â  description="""useful for when when a user asks to optimize or debug a Cypher statement or needs
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  specific instructions how to accomplish a specified task.Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Input should be a fully formed question.""",
Â  Â  ),
]

agent = initialize_agent(
Â  Â  tools,Â 
Â  Â  llm,Â 
Â  Â  agent="zero-shot-react-description",Â 
Â  Â  verbose=True
)
agent.run("""What are some GPT-4 applications with Neo4j?""")
```

-   In this example, the tools used by the Agent are custom data sources and prompt templates
