# Design {#sec-surv-design .unnumbered}

## Misc {#sec-surv-design-misc .unnumbered}

-   Resources
    -   [Encyclopedia of Survey Research Methods, Lavrakas 2008](https://methods.sagepub.com/reference/encyclopedia-of-survey-research-methods) (See R \>\> Documents \>\> Surveys)
-   Establishing study objectives is the first step in designing a survey. Defining the universe should be the second step — then, the target population and sampling frame — finally, the sampling methodology.
-   Item-specific surveys perform better/more reliable than agree/disagree surveys. The reason behind this is that participants are more certain while choosing a position in item-specific surveys compared to agree/disagree surveys
-   For self-report surveys, ceiling/floor effects on score distributions disaappear when response options are greater than 2 or 3 ([Thread](https://twitter.com/aidangcw/status/1691043938725961729?s=20), [Paper](https://journals.sagepub.com/doi/abs/10.1177/10731911231190098))
-   Survey Monkey sample size [calculator](https://www.surveymonkey.com/mp/sample-size-calculator/)
-   If asking the Age or Income of a respondant:
    -   If calculating mean or median, exact numbers are best but respondants are usually hesitant to give out exact numbers for such data.
    -   Ranges give less accurate averages but respondants are more likely to answer.
    -   Group based on population your studying
        -   If it's the general population, \$20k or less is a good first rung; \$21--39k is next; from there: \$40--69k, \$70--99k; \$100--150k; and \$150+
            -   If it's college students, the ranges will be lower.
        -   Breaks between groups should line-up with known charcteristics of the population you're studying
-   Use the phrases, jargon, and emotions that your customers are familiar with.
    -   Get a sense of this by getting on the phone and talking to your customers. Or run focus groups. Or run some on-site surveys.
-   Best practices for the format of common demographic questions, [link](http://www.amplituderesearch.com/market-research-questions.shtml)

## Terms {#sec-surv-design-terms .unnumbered}

-   [**Ceiling**]{style="color: #009499"} and [**Floor Effects**]{style="color: #009499"} - An artificial lower limit on the value that a variable can attain, causing the distribution of scores to be skewed.
    -   [Example]{.ribbon-highlight}: The distribution of scores on an ability test will be skewed by a floor effect if the test is much too difficult for many of the respondents and many of them obtain zero scores.
    -   [Example]{.ribbon-highlight}: The distribution of scores on an ability test will be skewed by a ceiling effect if the test is much too easy for many of the respondents and many of them obtain perfect scores
-   [**Element**]{style="color: #009499"} - The basic unit that represents whatever is being sampled and from which survey data are to be gathered, e.g. adults, children, households, employees, businesses, students, schools, school districts, uniformed personnel, police districts.
    -   All the members within the target population are its elements.
    -   All the elements within a **sampling frame** from the target population *that can be listed* constitute the **frame**.
    -   All the elements *that are selected* for study from the sampling frame make up what is commonly called **survey sample**.
    -   All the selected elements *from which data are gathered* also are commonly referred to as the **sample**.
-   [**Empirical Design -**]{style="color: #009499"} When the [inclusion probabilities](surveys-sampling-methods.qmd#sec-surv-sampmeth-terms){style="color: green"} are unknown
    -   See [Non-Probabilistic Sampling Methods](surveys-sampling-methods.qmd#sec-surv-sampmeth-probsamp-nonprob){style="color: green"}
    -   [Examples]{.ribbon-highlight}
        -   [Quota Sampling]{.underline} - Units are selected so to reflect known structures for the population
        -   [Expert Sampling]{.underline} - Units are selected according to expert advice
        -   [Network Sampling]{.underline} - Existing sample units recruit future units from among their 'network'.
-   [**Frame**]{style="color: #009499"} - All the elements from the target population *that can be listed* constitute the frame. This will give each element a non-zero probability of selection for inclusion in the survey sample. (See **Sampling Frame**)
    -   e.g. A housing unit frame would be all citizens that can be feasibly listed as living in a housing unit
    -   A well-defined appropriate frame is essential to the sampling process, the development of weights for use in analyses of survey data, the minimization of coverage error, and the understanding of what coverage error may exist.
-   [**Inferential Population**]{style="color: #009499"} - words
-   [**Multiple Frame Sampling**]{style="color: #009499"} - Surveys in which two or more frames are used and independent samples are respectively taken from each of the frames. Inferences about the target population are based on the combined sample data. Referred to as **Dual-Frame Sampling** when the survey uses two frames.
-   [**Probabilistic Design**]{style="color: #009499"} - When every element in the population has a fixed, known-in-advance inclusion probabilities
-   [**Sampling Bias**]{style="color: #009499"} - The probability distribution in the collected dataset deviates from its true natural distribution one would actually observe in the wilderness.
-   [**Sampling Frame**]{style="color: #009499"} - A frame that *represents* the population of interest. An exhaustive list of all the individuals which comprise the target population. The frame that's used to draw the sample. (Also see [Sources of Error](surveys-design.qmd#sec-surv-design-soe){style="color: green"} \>\> Coverage or Frame Error)\
    ![](./_resources/Surveys,_Sampling_Methods.resources/image.2.png){.lightbox}
    -   Some studies employ **multiple frames**, either because they use multiple modes of data collection, because no single frame has adequate coverage, or to facilitate oversampling of certain groups.
    -   Questions when selecting a sampling frame:
        1.  Does it include members of the universe being studied?
        2.  Is it appropriate for the way the data will be collected?
        3.  What is the quality of the frame in terms of coverage, completeness, and accuracy?
    -   Steps:
        1.  Define your population of interest, such as all residents of a specific city, all customers of a particular business, or all members of an organization.
        2.  Identify potential sources of information about your population, such as public records, membership lists, or customer databases.
        3.  Evaluate the quality of each potential source. Consider factors like completeness, accuracy, and currency, and choose the sources that are most likely to give you an accurate and complete sampling frame.
        4.  Combine your chosen sources to create a final sampling frame. This process might involve merging multiple databases and removing duplicates.
    -   Examples: area frames for in-person interviews, random-digit dialing (RDD) frames for telephone survey samples
-   [**Self-Report Study**]{style="color: #009499"} - Type of survey, questionnaire, or poll in which respondents read the question and select a response by themselves without any outside interference. A self-report is any method which involves asking a participant about their feelings, attitudes, beliefs and so on.
-   [**Survey Population**]{style="color: #009499"} - The subset of the **target population** that has a chance of survey inclusion because of their membership in, or linkage to, the sampling frame.
    -   [Example]{.ribbon-highlight}: Traditional telephone surveys have a survey population of households with landline telephone service but typically are used to make inferences to target populations of all households, regardless of telephone service.
-   [**Target Population**]{style="color: #009499"} - Often similar to the inferential population but excludes some elements that would be very difficult or costly to include on the frame.
    -   Elements in the inferential population but missing from the target population should be easy to describe and note in the survey documentation.
    -   Business surveys must specify the level of the business that comprises the units of the target population
        -   Business Finance Surveys typically define their target populations in terms of the enterprise — the organizational level that has ownership and makes decisions for the entire business.
        -   Labor Force Surveys define their target populations in terms of the establishment, that is, at the organizational level, where employment activities are conducted at or from a particular geographic location.
    -   [Example]{.ribbon-highlight}: Multiple Target Populations
        -   A health care practices survey might have a target population defined in terms of households, another defined in terms of adults, and another defined in terms of children. Such a survey might sample households, collect household-level data, and then sample an adult and a child for interviews at those levels.
-   [**Universe**]{style="color: #009499"} - Consists of all survey elements that qualify for inclusion in the research study. Once defined, the universe is used by the researcher to structure an appropriate sampling methodology, in particular by defining the target population and choosing a sampling frame or frames.
    -   Can be called **Target Population** of **Population of Interest**
    -   [Example]{.ribbon-highlight}: Estimating the size of the universe
        -   if the researcher randomly contacts 1,000 households and finds that 45 of these own both a dog and a cat, the researcher can then estimate that roughly between 3% and 6% of all households in Australia own both types of pets.

## Sources of Error {#sec-surv-design-soe .unnumbered}

![](./_resources/Surveys,_Design.resources/error-flowchart-1.png){.lightbox width="532"}

-   Misc
    -   Notes from [Total Survey Error: Design, Implementation, and Evaluation](https://academic.oup.com/poq/article/74/5/817/1815551) (d/l pdf to see figures)
-   [**Survey Error**]{style="color: #009499"} is defined as the deviation of a survey response from its underlying true value
-   [**Survey Accuracy**]{style="color: #009499"} is defined as the deviation of a survey estimate from its underlying true parameter value
-   [**Total Survey Error**]{style="color: #009499"} - Accumulation of all errors that may arise in the design, collection, processing, and analysis of survey data. It includes sampling variability, interviewer effects, frame errors, response bias, and non-response bias
    -   [Non-Sampling Error]{.underline}
        -   [**Specification Error**]{style="color: #009499"} occurs when the concept implied by the survey question differs from the concept meant to be measured in the survey.
            -   Often caused by poor communication between the researcher, data analyst, or survey sponsor and the questionnaire designer.
        -   [**Coverage or Frame Error**]{style="color: #009499"} typically results from the frame construction process. Discrepancies between the (theoretical) target population of a survey and the frame which is used to draw a sample are equivalent to statistical errors.
            -   See [Surveys, Sampling Methods \>\> Terms](surveys-sampling-methods.qmd#sec-surv-sampmeth-terms){style="color: green"} \>\> Sampling Frame
            -   In practice, frames are not error-free:
                -   They can never encompass the whole target population (because it always takes time to administrativally record an individual in a register),
                -   They also contain individuals which are no longer eligible (e.g. individuals who left the country and may keep recorded in the Register several months after they left).
                -   Coverage Errors or Frame Errors.
            -   Types
                -   **Undercoverage** - Some members of the universe are neither on the frame nor represented on it and hence have no chance of inclusion in the survey.
                    -   Can lead to bias in estimates made from survey data
                -   **Overcoverage** - Some elements on the frame are not members of the universe.
                    -   Along with bias in estimates, cost is even moreso the problem with overcoverage, because ineligibles must be identified and screened out. If the ineligibles can be identified *before* selecting the sample, it is usually better to eliminate them at that time.
            -   Undercoverage Examples
                -   All RDD landline frames exclude households with no telephone service, and those with only cellular phone service.
                -   Frames drawn from telephone directories exclude those households (listed in #1 above) plus those with unpublished and recently published numbers.
                -   New construction may be excluded from lists of addresses used as sampling frames for surveys conducted by mail or personal visit.
                -   Commercial lists of business establishments exclude many new businesses and may underrepresent small ones.
                -   Panels recruited for Internet research, even if the panels were recruited from a survey that used a probability sample with a good frame, may have self-selection bias. (See [Experiments, A/B Testing \>\> Potential Biases](experiments-a_b-testing.qmd#sec-exp-ab-pb){style="color: green"} \>\> Treatment Self Selection)
            -   Overcoverage Examples
                -   Some units may be duplicated an unknown number of times
                -   Some ineligible units may be included on the frame, such as businesses that are not farms in a farm survey.
                -   A sample of students enrolled in a school is selected. One might use a list provided by the school or the district; however, the list might include (overcoverage) students who had dropped out or transferred and omit students (undercoverage) who had enrolled after the list was compiled.
        -   [**Nonresponse Error**]{style="color: #009499"} - When the reason for nonresponse is related to the missing value, parameter estimates can be biased when nonresponse is not accounted for. Includes:
            -   [**Unit Nonresponse**]{style="color: #009499"} - Sampling unit does not respond to any part of the questionnaire
                -   e.g. Calling a person and them choosing not to answer or participate in the the survey
            -   [**Item Nonresponse**]{style="color: #009499"} - The questionnaire is partially completed.
        -   [**Measurement Error**]{style="color: #009499"} occurs when the method of obtaining the measurement affects the recorded value, often involving simultaneously the respondent, the interviewer, and the survey questionnaire.
        -   [**Processing Error**]{style="color: #009499"} refers to errors that arise during the data processing stage, including errors in the editing of the data, data encoding, the assignment of survey weights, and tabulation of the survey data.
    -   [[**Sampling Error**]{.underline}]{style="color: #009499"} - Caused by collecting partial information over a fraction of the population rather than the whole population itself.
        -   Sampling scheme (e.g., Multistage or Multiple-Phase Sample)
        -   Sample size
        -   Choice of Estimator (e.g., a ratio or regression estimator, levels of post-stratification)
-   Keeping sampling errors under control
    -   Survey questionnaires must be prepared with utmost care, intensively pre-tested and field-tested in order to detect issues in question wording, routing problems or any other inconsistency
    -   Modes of data collection must be chosen and combined judiciously in order to get most people to cooperate
    -   Interviewers must be carefully recruited and properly trained
    -   Communication and contact strategies towards participants must be designed and adapted in order to reach highest participation.

## Response Formats {#sec-surv-design-respfor .unnumbered}

-   [**Item-Specific (IS)**]{style="color: #009499"} - Multiple Choice Response Format\
    ![](./_resources/Surveys,_Design.resources/0-R4C3yXJTPpBUZnHM.png){.lightbox width="432"}
-   [**Agree/Disagree (A/D)**]{style="color: #009499"} - Response format where the response are degrees of strength of agreement or disagreement\
    ![](./_resources/Surveys,_Design.resources/0-11yLyNXjXnQYLTxd.png){.lightbox width="432"}

## Response Scales {#sec-surv-design-respsc .unnumbered}

### Misc {#sec-surv-design-respsc-misc .unnumbered}

-   The default values and ranges you use
-   Examples:
    -   Age: 5 yr default range (e.g. 20--25) instead of 10 yrs ranges

### Types {#sec-surv-design-respsc-types .unnumbered}

-   [Dichotomous Scales]{.underline} - Precise data, but they don't allow for nuance in respondents' answers.

    -   Examples: "Yes" or "No"; "True" or "False"; "Fair" or "Unfair"

-   [Quantitative Scales]{.underline}

    -   [Rating Scales]{.underline}
        -   Provides more range than Dichotmous scales. Too generic for attitudes
        -   1--10; 1--7; 1--5 (or Likert scale, also see below)
            -   Label all number or none
                -   A 1--5 scale presented verbal descriptions for only the 1 and 5 endpoints and this led more people to choose the endpoints.
                -   A "school grade" scale is more reliable than other types of word labels
    -   [Ordinal and Interval Scales]{.underline}
        -   With ordinal, the numbers just have an intrinsic order
        -   With interval, the distance between the numbers must also be equal in terms of context
            -   e.g. Rating something a 2 vs 1 doesn't mean that being a 2 is "twice as good" as being a 1
                -   Unless being 5 is also twice as good as being a 4, etc.
        -   There is no practical difference between ordinal or interval scales
    -   [Ratio Scales]{.underline}
        -   where there is a true zero and equal intervals between neighboring points. Unlike on an interval scale, a zero on a ratio scale means there is a total absence of the variable you are measuring. Length, area, and population are examples of ratio scales

-   [Semantic Differential Scales]{.underline}\
    ![](./_resources/Surveys,_Design.resources/Screen-Shot-2016-03-07-at-11.png){.lightbox width="432"}

    -   Gather data and "interpret based on the connotative meaning of the respondent's answer."
    -   Usually have dichotomous words at either end of the spectrum
    -   The more quantifiable the information is (behavior questions, for instance), the smaller the range should be.
        -   When you want to measure attitudes or feelings, using a 5- or 7-point semantic differential scale is a good strategy.

### Likert Scale {#sec-surv-design-respsc-liksc .unnumbered}

-   Notes from: [Likert Scales: Friend or Foe?](https://towardsdatascience.com/likert-scales-friend-or-foe-76f865786fb7)
-   Quantitative, Rating scale
-   Avoid using agree/disagree wording (see biases section below)
-   Examples:
    -   5-point:

        -   \[1\] Strongly Disagree, \[2\] Disagree
        -   \[3\] (4pt removes this response) Neither Disagree Nor Agree
        -   \[4\] Agree, \[5\] Strongly Agree

    -   7-point:

        -   \[1\] Strongly disagree, \[2\] Disagree, \[3\] Somewhat disagree
        -   \[4\] Neither agree nor disagree
        -   \[5\] Somewhat agree, \[6\] Agree, \[7\] Strongly agree

    -   More examples, [Link](https://mwcc.edu/wp-content/uploads/2020/09/Likert-Scale-Response-Options_MWCC.pdf)
-   [Issues]{.underline}
    -   Ordinal and not Interval
        -   Answers are not all equidistant
            -   Respondents may perceive (4) Strongly Agree and (3) Agree very similarly and thus the difference between these two options might be much smaller than the difference between (3) Agree and (2) Disagree, despite having the same distance.\
                ![](./_resources/Surveys,_Design.resources/1-0L81UdRm3R57VmustUwKCw.png){.lightbox width="482"}
        -   Numerical values assigned to the response options cannot be treated as interval data
            -   Parametric statistics (e.g., mean, standard deviation) and parametric statistical methods (e.g., summing up individual questions to find a total survey score, running a regression with survey scores) would NOT yield valid results
        -   Acceptable Analysis Methods:
            -   Median and Mode
            -   Ordinal regression
            -   Chi-Square Test of Independence
            -   Item Response Theory (IRT) modeling
            -   Graphical Tools (e.g. bar charts and correlation matrix plots)
    -   Adding a neutral option (i.e. Neither Disagree Nor Agree)
        -   May increase the accuracy of survey data because respondents who do not have a strong preference may prefer to select the neutral response option, instead of randomly selecting a response option or skipping the question
        -   May produce a bias: research shows that respondents often see the visual midpoint of a scale as representing the middle response option
        -   Solutions:
            -   use a Likert scale that consists of an even number of response options without a neutral option
            -   select survey questions for which respondents would not select the neutral option very easily.
    -   Choosing the number of responses
        -   Research shows that Likert scales with 2 to 5 response options often yield precise results, although smaller numbers of response options may reduce the measurement precision of a survey
        -   Researchers found that there are no clear advantages of using beyond 6 response options on a Likert scale
    -   Positively and negatively worded questions together
        -   Typically used to prevent response bias
        -   Positively and negatively worded questions are not necessarily mirror images of each other.
            -   Therefore, when analyzing survey data, reverse-coding the Likert scale for negatively worded questions (e.g., 1-Strongly agree; 2-Agree; 3-Disagree; 4-Strongly disagree) may not necessarily put these questions in the same direction as positively worded questions
        -   Research shows that negative wording may confuse respondents, leading to less accurate responses to the survey questions
            -   May be increasing response bias instead of reducing it.
        -   Studies indicate that respondents are more likely to disagree with negatively worded questions than to agree with positive ones
            -   Example
                -   A respondent who would select Agree for "My room was clean" might prefer to select Strongly Disagree for "My room was dirty".
        -   Solutions:
            -   Keep the number of negatively worded questions minimal, while taking the impact of negatively worded questions on responses into account.
    -   Sliding Scales
        -   [link](https://www.checkmarket.com/blog/likert-scales-slider-scales/)

### Guttman Scale {#sec-surv-design-respsc-guttsc .unnumbered}

![](./_resources/Surveys,_Design.resources/Screenshot%20(367).png){.lightbox width="532"}

-   Dichotomous or Likert
-   Gradually increases in specificity. The intent of the scale is that the person will agree with all statements up to a point and then will stop agreeing.
-   The scale may be used to determine how extreme a view is, with successive statements showing increasingly extremist positions.
-   Also, a useful tool for measuring satisfaction
-   If needed, the escalation can be concealed by using intermediate questions.

### Net Promoter Score (NPS) {#sec-surv-design-respsc-nps .unnumbered}

-   Customer loyalty metric, 0-9 (or 1-10)
-   "How likely is it that you would recommend our company/product/service to a friend or colleague?"
-   NPS = percent_promoters - percent_detractors
-   Responses get broken up into 3 groups
    -   Promoters (9--10). These are your happiest and most loyal customers who are most likely to refer you to others. Use them for testimonials, affiliates, etc. These customers are key to business growth and thereby sustaining their customer experience is critical to the brand.
    -   Passives or Neutrals (7--8). These customers are happy but are unlikely to refer you to friends. They may be swayed to a competitor fairly easily. A business should look at ways and means of upgrading neutrals to promoters by understanding their requirements.
    -   Detractors (0--6). Detractors are unhappy customers who can be dangerous for your brand, spreading negative messages and reviews. Figure out their problems and fix them.
-   Any NPS that is positive is usually perceived as good, and an NPS score of 50+ is considered excellent. The range of NPS is from -100 (all detractors) to +100 (all promoters).
-   \*\*Don't use as a single predictor of customer loyalty\*\*
    -   A customer might actually be very enthusiastic about the product, but they just might not ever feel the urge to recommend hemorrhoid cream to their pals
-   Combine with other measures
    -   Ask follow-up questions.
        -   Promoters. What's your favorite part about our product/service?
        -   Passives. What would make you love us?
        -   Detractors. What could we do to improve your experience?
    -   Combine it with user research.
        -   Usability testing and other common conversion research techniques
        -   User testing
        -   Customer surveys
        -   Live chat
        -   [Heat maps](https://cxl.com/blog/heat-maps/) (e.g. buttons clicked on websites, scrolling actions)
    -   Find and fix issues.
        -   Use info to prioritize projects that enhance user experience
    -   Market to promoters. Use Promoters, Passives, and Detractors as a segmentation tool.
        -   Give free stuff to "promoters" to incentize more buying or advocates to others by sharing on FB or twitter or write a review.
        -   Find correlations between certain product actions (heatmaps) and a higher NPS. This can help deduce what your product's "magic moment" is when your users are truly activated and likely to derive delight from your product. Then you can focus on product optimizations to get more of your customer base to this point
-   Issues
    -   Response rate for surveys is relatively low
    -   Time-consuming and costly affair to collect a sizeable amount of survey data
    -   Relatively high-cost outlay associated with sending out surveys
    -   Responses are typically quantitative. There is rarely qualitative information explaining the reason for the response
-   Alternative: Use sentiment analysis on customer reviews and social media posts to generate a proxy for NPS based on sentiment scores.
    -   See [Algorithms, Product \>\> Net Promoter Score](algorithms-product.qmd#sec-alg-prod-nps){style="color: green"}

## Response Biases {#sec-surv-design-respsc-biases .unnumbered}

-   [Acquiescence Bias]{.underline}
    -   A tendency to agree with statements rather than disagree
    -   Occurs with Agree/disagree (A/D) or Yes/No (Y/N) questions
    -   Effects can be stronger when the survey is administered by a person compared to self-administered surveys
        -   Some people's personal inclination can lead them to be polite and avoid conflict, ultimately being aggregable.
        -   Some participants may consider themselves in lower social status than the interviewer/researcher. Therefore, they may believe what is offered in questions and unintentionally accept the 'agree' choice.
        -   In many cultures, while interacting with another person, agreeing is more well-suited than disagreeing
    -   Can cause a correlational relation between similarly worded questions, thus, eliminates some important constructs
    -   Even a small survey error stemming from acquiescence bias decreases the quality of the inferences
    -   Solution:
        -   Convert Agree/Disagree (A/D) response format to item-specific (IS) response format
        -   [Example]{.ribbon-highlight}\
            ![](./_resources/Surveys,_Design.resources/1-TyDxWQMp1rrNhnOErUnrGA.png){.lightbox width="532"}
-   [Response Order Effect]{.underline}
    -   Choices presented earlier are more probable to be selected
    -   If the answer options are categorical, respondents tend to conceive them consecutively
    -   Effect is relatively small in rating scales
    -   Some respondents stop when they come to an acceptable answer and never see the rest of the options.
    -   If respondents spend more time on the first half of the response scale, they are more likely to choose one of the answers here.
    -   Solution:
        -   If using Agree/Disagree (A/D), convert to item-specific (IS) response format
            -   Respondants spend more time on IS format thus more likely to read all the anwsers
        -   Change the order of the responses randomly for different participants
            -   Some platforms like Survey Monkey and Qualdtrics websites have a randomize response order option
        -   Effect is smaller in vertical arrangement of responses
-   [Cognitive Errors]{.underline}
    -   Understanding the question
        -   Provide definitions for key terms in the questions
    -   Retrieval of information (Remembering)
        -   Unless it's something really memorable, don't ask about something that happened 6 months ago
        -   Use memory cues
            -   like sequencing the questions as the events would've been sequenced
            -   eliciting life events in the question
        -   Somethings are encoded into memories
            -   When we paid cash, we paid more attention to prices of certain items, because we had to get out the cash and count it. Being able to just swipe a card doesn't encode such information as well.
    -   Integration of that information into an estimate or judgment
        -   Participants may underreport or overreport a behavior when the frequency is asked
    -   Reporting of that judgment (picking a response)
        -   There's a cognitive burden when a respondant tries to bin there answer into one of the provided responses
        -   Greater burden with Agree/Disagree (A/D) format
            -   item-specific questions are less sequential compared to A/D questions. Thus, respondents may experience less burden on the cognitive process of reporting an answer
