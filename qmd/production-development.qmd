# Production, Development

TOC

* Checklist
* Architecture
	Documentation
	



Misc

* Improving your data will likely improve your model more than testing different algorithms or tuning more hyperparameters
	* Good Data:
		* is defined consistently (definition of labels, y, is unambiguous)
			* e.g. consistent object labeling procedures
		* has coverage of important cases (good coverage of inputs, x)
			* e.g.  samples of every combination of features the model will encounter
		* has timely feedback from production data (distribution covers data drift/concept drift)
		* is sized appropriately
	* Examples
		* Image
			* Improve consistency of labeling procedures![](./_resources/Production,_Development.resources/Screenshot (530).png)
				* (top-left) clear separation of boxes; (top-right)tail of left iguana included; (bottom) include all iguanas in one bounding box
					* fyi 1 is probably best
				* solution: Improve procedure by making sure only 1 of these ways is used by labellers given this type of picture
		* Audio
			* You discover you speech recognition model performs poorly when there's car noise in the background
				* solution: get more training data with car noise in the background
	* Steps
		* Train a model
		* Error analysis: investigate which types of data you model performs poorly on. The points the model performs really poorly on will be edge cases or errors in data collection, measurement, entry, etc.
		* Improve these data
			* For edge cases, collect more data, augment your data, generate more data (simulation?) (i.e. change inputs of x)
			* For errors, make labeling procedure more consistent if some are found to be ambiguous (i.e. change the labels of y)


Checklist

* Misc
	* If you’re using database for cross-validation splits, make sure the correct order of data is maintained by making sure a logic to sort or keep the ordering of data is set.
	* Notes From:
		* [How to Build a Data Product that Won’t Come Back to Haunt You](https://towardsdatascience.com/how-to-build-a-data-product-that-wont-come-back-to-haunt-you-1a220f4c75fb)
* Broad strokes
	1. a way to ingest data for training
	2. a way to ingest data for prediction
	3. a place to output the predictions
	4. a place to train the model (a machine)
	5. some insight into what the process is doing: logging
	6. notifications to the right people when something goes wrong: alerting
* More finely grained
	1. lintr run / all probs fixed on model-related code
	2. Model can be run independently (single R file)
	3. Sign-off from engineering on model input / output / purpose
	4. Code review
	5. Algorithm review
	6. Standardized headers in R file that describe input, output, data, algorithm, description
	7. Code in Bitbucket
	8. All acceptance criteria are met
	9. Model validation documented
	10. Model validation review meeting held
* Self-Explanatory
	* The more self-explanatory the data product is, the easier it will be to support future users and maintain by future developers.
		Data Inputs:
		* Is the data I am using well-documented?
		* Am I using the data in line with its intended use?
		* Am I reusing as much of the existing engineering pipeline as possible to lower overall maintenance effort and ensure that my use is consistent with other uses of the same data items?
		Pipeline:
		* Are the requirements, constraints and implementation of the data process documented well enough that someone else who may be taking over maintenance from me can understand it?
		Final Product:
		* Is the report or dashboard presented in a way that is easily accessible and understandable even to people who will be viewing it, and without explanation from me?
		* Am I using vocabulary and visualizations that the end user understands, and that they understand in exactly the same way I do?
		* Am I providing good supporting documentation and information in a way that is intuitive and easily accessible from the data product itself?Data Inputs:
	Trustworthy
	* Trust is hard to gain and easy to lose. Usually, when you first deliver a data product, there is a lot of hope and some trust.
		Data Inputs:
		* Am I connected into the input data in a way that is well-supported in the production pipeline?
		* Do I have explicit buy-in from those maintaining the data sets I am using?
		* Is this input data likely to be maintained for a significant time in the future?
		* When might I need to check back to be sure the data is still being maintained?
		* How do I report any problems that I see in the input data?
		* Who is responsible for notifying me of issues with the data?
		* Who is responsible for fixing those issues?
		Pipeline:
		* Have I set up a regular schedule to review the data and report to ensure that the data pipeline is still functioning well and the report is conforming to the requirements?
		* What are the conditions under which this report should be marked as deprecated?
		* How can I ensure that the user is informed should the report become deprecated?
		Final Product:
		* How does the user know when they can trust the report is accurate and up to date?
		* Is there an efficient and/or automated way of communicating possible problems to the end user?
		* Is there a clear and accessible process in place for the user to report concerns with the data or report, and for the user to be notified of any remediation processes in place?
	Adaptable
	* Data inputs and shapes change over time. In addition to monitoring relevant issues, when someone notices a problem, you need to be set up to react and formulate a solution without undue complications.
		Data Inputs:
		* What features in the input data am I depending on for my analysis?
		* How will I know if those features stop being supported, are affected by a schema change, or change shape in a way that may affect my analysis?
		* How will I know if the scale of the data grows to a point where I need to refactor my process in order to keep up with my product’s requirements for freshness?
		Pipeline:
		* Have I set up a regular schedule for re-examining the requirements to ensure that I am still producing what the user needs and expects?
		* What is the process for users to indicate changes in requirements, and for those changes to be addressed?
		* What is the process for refactoring and retesting the data pipeline when the inputs change in some relevant way?
		Final Product:
		* Is the product or report set up in a way that it is easy to request a change and/or a new feature?
	Reliable
	* Ensure that the different parts that make up your pipeline are reliable and the processes that orchestrate those parts are robust.
		Data Inputs:
		* Does my process fit well into the data practices and engineering production system in my organization?
		* Do I have an automatic notification system in place to monitor the availability, freshness and reliability of my input data?
		Pipeline:
		* Is each stage in my pipeline executing and completing in a timely manner?
		* Is there drift in the processing time and/or amount of data being processed at any stage that may indicate a degradation in pipeline function?
		Final Product:
		* Who is responsible for the ongoing monitoring, reviewing, troubleshooting, and maintenance of the dashboard itself?
		* Are responsibilities and procedures clearly in place for reporting and resolving issues internally?



Architecture

* Plan (also see Examples >> E-Commerce Analysis Pipeline)
	* Questions
		* What are your priorities and burning issues? — prioritize the use cases the data platform should resolve for you promptly, which can generate immediate business value.
		* What are your constraints? — think and quantify everything — from software and human resources to time and effort required, level of internal knowledge, and monetary resources.
	* Goals
		* Start with quick wins — don't dive directly into data science and machine learning model development, but instead start with quick win use cases (usually descriptive statistic use cases).
		* Be realistic — when setting the data platform goals, the important thing is to be realistic about what's feasible to achieve given current constraints.
	* Components
		* Building data pipelines — properly developed data pipelines will save you money, time, and nerves. Developing pipelines is the most crucial part of the development, i.e. that your pipelines are properly tested and deliver new data to business users without constant brakes due to various data and system exceptions.
		* Organizing and maintaining the data warehouse — with the new data sources, a data warehouse can quickly become messy. Implement development standards and naming conventions for a better data warehouse organization.
		* Data preprocessing — think about acquiring data preprocessing tool(s) as early as possible to improve the dashboard performance and reduce computational costs by de-normalizing your datasets.
		* Data governance and security — set the internal standards and data policies on the data lifecycle (data gathering, storing, processing, and disposal).
* Notes from [Automating the end-to-end lifecycle of Machine Learning applications](https://martinfowler.com/articles/cd4ml.html?utm_campaign=Data_Elixir&utm_source=Data_Elixir_250) and https://www.thoughtworks.com/insights/blog/curse-data-lake-monster 
* This is for a "embedded model" type deployment. Take model object, insert into app, and deploy.
	1. EDA
		* Pull data from data lake or warehouse
		* Missing values
		* distributions of variables
		* Values that may not fit analysis (e.g. negative sales aka returns)
		* correlations between variables
		* investigate potential outliers
	2. Clean
		* remove values not pertinent to analysis
		* column names, types
		* dummy variables, other categorical coding
		* variable transformations
		* feature engineering
			* can this at least be partially done with spark?
		* Join dataframes
		* save to file (.csv, rds, .feather, .fst, .parquet)
	3. Transfer data file to a storage system. 
		* If project subject matter isn't company-wide, then transfer the cleaned, pertinent subset of data to a data mart. (A data mart is focused on a single functional area of an organization and contains a subset of data stored in a Data Warehouse or Lake)
		* May have data from multiple sources (lakes, warehouses, etc)
		* Does apache airflow coordinate multiple sources? see bkmk'ed video
			* use spark to feed data to model
	4. Model Selection experiments in git branches. MLflow ($?) can monitor experiments and their metrics
		* research potential models
		* pick a base model for comparison
		* tune and train models
		* cross-validation
		* choose model
	5. Version control pipeline (DVC, Git, etc) - DVC (dvc.org) (open source) allows for version control of the large files like data sets and model objects. Works is conjunction with Git and cloud storage platforms.
		* pull data
		* validate data
		* split data to training and validation sets (and test set?)
		* train model
		* test with validation set
		* validate model within metric thresholds
		* validate model fairness and not biased for specific variable values (race, gender, etc)
		* model, training data, and metrics need to be linked, i.e. version controlled
		* communicate metrics (See FluentD service below)
		* write model object to file
		* write training set (or maybe all data sets?) to file
		* dvc push files to storage
	6. Embed model obj + application (shiny app) in docker image
		* dvc pull model file
		* the Docker image becomes our application+model artifact that gets versioned and deployed to production
		* (before or after creating docker image?)Perform integration test with validation data: make sure the model produces the same results inside the application as in the modelling pipeline
		* push image
	7. Deploy image to a Kubernetes production cluster
	8. CI/CD Pipeline skeleton can be orchestrated through a GoCD ($?)
	9. Monitor Cluster performance using the EFK stack which is composed of three main tools:
		1. Elasticsearch: an open source search engine.
		2. FluentD: an open source data collector for unified logging layer, i.e. for log ingestion and forwarding
		3. Kibana: an open source web UI that makes it easy to explore and visualize the data indexed by Elasticsearch.
		4.  Add code to model script to log the model inputs and predictions as an event in FluentD
		5. Other tools: Logstash (alternate to FluentD), Splunk
* Examples
* ![](./_resources/Production,_Development.resources/Screenshot (725).png)
* ![](./_resources/Production,_Development.resources/r-production-architecture.png)
* ![](./_resources/Production,_Development.resources/1-2MwL61vF1oin1e51S5_m9A.png)
* ![](./_resources/Production,_Development.resources/1-iDVLVmaC5Z7t_CHVnaT28g.jpeg)
* ![](./_resources/Production,_Development.resources/1-EZhsWUaK45a0kdmgwsDVow.jpeg)
* ![](./_resources/Production,_Development.resources/1-nQzIax2nekhHmsU6azN9NQ.png)
	* Described in [An introduction to Monzo’s data stack](https://medium.com/data-monzo/an-introduction-to-monzos-data-stack-827ae531bc99)
	* "analytics events processor + shipper" is something custom they built to "sanitize" (i.e. remove personally identifiable information (PII)) events data
	* I think NSQ is like Kafka and it's there for redundancy(?)
* [Paper: Machine Learning Operations (MLOps): Overview, Definition, and Architecture](https://arxiv.org/abs/2205.02302)

![](./_resources/Production,_Development.resources/image.png)

* E-Commerce Analysis Pipeline
	* Initial Stage![](./_resources/Production,_Development.resources/image.1.png)
		* Data collection layer: presents the most relevant data sources that had to be initially imported to our data warehouse.
		* Data integration layer: presents cron jobs used for importing e-commerce datasets and the Funnel.io platform for importing performance marketing datasets to our data warehouse.
		* Data storage layer: presents the selected data warehouse solution, i.e. BigQuery.
		* Data modelling and presentation layer: presents the data analytics platform of choice, i.e. Looker.
		* Resources
			* 2 tools — BigQuery and Looker,
			* 6 people — for managing data pipelines (cron jobs + Funnel.io platform) and initial analytical requirements (data modelling),
			* 3 months —from acquiring Google Cloud to presenting the first analytical insights.
	* Advanced Stage![](./_resources/Production,_Development.resources/image.2.png)
		* Cloud storage — for storing our external files in Google Cloud.
		* Cloud Run — used for deploying analytical pipelines developed in Python and wrapped as Flask applications.
		* Google Functions — for writing simple, single-purpose functions attached to events emitted from the cloud services.
		* Google Workflows — used for orchestrating connected analytical pipelines that needed to be executed in a specific order.
		* Google Colab — for creating quick PoC data science models.
		* Resources
			* From 2 to 7 tools — from using only BigQuery and Looker, we started using Cloud storage, Cloud Run, Google Functions, Google Workflows, and Google Colab.
			* From 6 people in two teams (IT and Business Development) to 8 people in one team (Data and Analytics) — the Data and Analytics team was established and now has complete ownership over all data layers.
			* From 3 months for creating initial insights to 2+ years of continuous development — we are gradually developing more advanced analytical use cases.



Documentation

* Model / architecture selection
* Hyper-parameters
* Rough description of the data (origin, size, date, features…)
* Results (ie: precision, recall, f1…).
* A link to a snapshot of data (if possible)
* Commentary and learnings
* Models
	* model object, training, testing data
	* Model name and description (origin, goal, size, date, features…)
	* Development stage (Implemented for use, under development or recently retired)
	* Diagnostic Metrics
	* Model assumptions
	* Model limitations
	* The model owner: the model owner is responsible for ensuring that the models are appropriately developed, implemented and used.
	* The model developers: model developers follow the lead of the model owner to create and implement the Machine Learning models.
	* The model users: model users can be either internal to the business or external. For both cases it is important to clearly identify their needs and expectations. They should also be involved in the model development and can help validate the model’s assumptions.
	* Other comments
		* What was learned
* Projects
	* An introduction
	* A description of the problem
	* A description of the data set
	* The methodology that you used:
	* Methodology to prepare the data
	* Machine Learning / statistical analysis approach taken to achieve the results
	* The Results
	* Recommendations based off the results







