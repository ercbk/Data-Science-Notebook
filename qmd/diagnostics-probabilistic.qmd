# Diagnostics, Probabilistic

TOC

* Misc
* Scores
* Visual Inspection


Misc

* AIC vs BIC ([paper](https://t.co/6Lfs0ilrKF))![](./_resources/Diagnostics,_Probabilistic.resources/image.png)
	* AIC
		* penalizes parameters by 2 points per parameter
		* Ideal AIC scenario
			* Numerous hypotheses are considered
			* You have a conviction that all of them are to differing degrees wrong
	* BIC
		* penalizes parameters by ln(sample size) points per parameter and ln(20) = 2.996
		* almost always a stronger penalty in practice
		* Ideal BIC scenario
			* Only a few potential hypotheses are considered
			* One of the hypotheses is (essentially) correct


Scores

* **Continuous Ranked Probability Score** (CRPS)
	* `fabletools::accuracy`
	* [{]{style='color: #990000'}[loo](https://cran.r-project.org/package=loo){style='color: #990000'}[}]{style='color: #990000'} - crps(), scrps(), loo\_crps(), and loo\_scrps() for computing the (Scaled) Continuously Ranked Probability Score
	* Manual calculation ([article](https://towardsdatascience.com/crps-a-scoring-function-for-bayesian-machine-learning-models-dd55a7a337a8))
	* Measures forecast distribution accuracy
	* combines a MAE score with the spread of simulated point forecasts
	* see notebook (pg 172)
* **Winkler Score**
	* `fabletools::accuracy`
	* Measures how well a forecast is covered by the prediction intervals (PI)
	* see notebook (pg 172)


Visual Inspection

* Check how well the predicted distribution matches the observed distribution
* [{]{style='color: #990000'}[topmodels](https://topmodels.r-forge.r-project.org/){style='color: #990000'}[}]{style='color: #990000'} currently supported models:
	* lm, glm, glm.nb, hurdle, zeroinfl, zerotrunc, crch, disttree, and models from disttree, crch packages
	* Also see video, [Probability Distribution Forecasts: Learning from Random Forests and Graphical Assessment](https://www.youtube.com/watch?v=iMBgmfdKs8g)
* `autoplot` produces a ggplot object that can be used for further customization
* **(Randomized) quantile-quantile residuals plot**![](./_resources/Diagnostics,_Probabilistic.resources/Screenshot (310).png)

```
qqrplot(distr_forest_fit)
```

* quantiles of the standard normal distribution vs quantile residuals (regular ole q-q plot)
* Interetation
	* pretty good fit as the points stick pretty close to the line (red dot is the laser pointer from the dude giving the talk)
	* left and right tails show deviation.
	* The left tail also shows increased uncertainty due the censored distribution that was used to fit the model
* Compare with a bad model![](./_resources/Diagnostics,_Probabilistic.resources/Screenshot (327).png)

```
c(qqrplot(distr_forest_fit, plot = FALSE), qqrplot(lm_fit, plot = FALSE)) |> autoplot(legend = TRUE, single_graph = TRUE, col = 1:2)
```


* **Probability Integral Transform (PIT) histogram**![](./_resources/Diagnostics,_Probabilistic.resources/Screenshot (312).png)

```
pithist(distr_forest_fit)
```

* Compares the value that the predictive CDF attains at the observation with the uniform distribution
* The flatter the histogram, the better the model.
* Interpretation: As with the q-q, this model shows some deviations at the tails but is more or less pretty flat
* Compare with a bad model![](./_resources/Diagnostics,_Probabilistic.resources/Screenshot (325).png)

```
c(pithist(distr_forest_fit, plot = FALSE), pithist(lm_fit, plot = FALSE) |> autoplot(legend = TRUE, style = "lines", single_graph = TRUE, col = 1:2)
```

* **(Hanging) Rootogram**![](./_resources/Diagnostics,_Probabilistic.resources/Screenshot (314).png)

```
rootogram(distr_forest_fit)
```

* Compares whether the observed frequencies match the expected frequencies
* Observed frequencies (bars) are hanging off the expected frequencies (model predictions, red line)
* "robs" is the outcome values
* Interpretation: Near perfect prediction for 0 precipitation (outcom variable), underfitting values of "1" precipitation
* Compare with a bad model![](./_resources/Diagnostics,_Probabilistic.resources/Screenshot (323).png)

```
c(rootogram(distr_forest_fit, breaks = -9:14), rootogram(lm_fit,
breaks = -9:14) |> autoplot(legend = TRUE)
```

* lm model shows overfitting of outcome variable values 1-5 and underfitting the zeros.
* The lm model doesn't use a censored distribution so there's an expectation of negative values

* **Reliability Diagram**![](./_resources/Diagnostics,_Probabilistic.resources/Screenshot (316).png)

```
reliagram(fit)
```

* forecasted probabilities of an event vs observed frequencies
	* basically a fitted vs observed plot
	* Forecast probabilites are binned (points on the line), 10 in this example, and averaged
* Close to the dotted line indicates a good model

* **Worm plot**![](./_resources/Diagnostics,_Probabilistic.resources/Screenshot (318).png)

```
wormplot(fit)
```

* ? ( he didn't describe this chart)
* Guessing the dots on the zero line indicates a perfect model and dots inside the dashed lines indicates a good model
	* He said this model fit was reasonable but doesn't look that great to me.

