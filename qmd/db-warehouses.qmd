# Warehouses {#sec-db-ware .unnumbered}

## Misc {#sec-db-ware-misc .unnumbered}

-   The main difference between a "relational database" and a "data warehouse" is that the former is created and optimized to "record" data, whilst the latter is created and built to "react to analytics."
-   Optimized for read-heavy workloads that scan a small number of columns across a very large number of rows and can easily scale to petabytes of data
-   Cons
    -   Can become expensive when an organization needs to scale them
    -   Do not perform well when handling unstructured or complex data formats.
-   Pros
    -   Integrating multiple data sources in a single database for single queries
    -   Maintaining data history, improving data quality, and keeping data consistency
    -   Providing a central view for multiple source system across the enterprise
    -   Restructuring data for fast performance on complex queries

## OLAP vs OLTP

![](./_resources/DB,_Warehouses.resources/image.png){width="400"}

-   **OLAP** (Online Analytical Processing)(aka the Cube)(Data Warehouses)

    -   db designed to optimize performance in analysis-intensive applications
    -   Aggregates transactions to be less frequent but more complex
    -   Examples: Snowflake, Bigquery

-   **OLTP** (Online Transaction Processing) db designed for frequent, small transactions

    -   Executes a number of transactions occurring concurrently (i.e. at the same time)
    -   Use cases: online banking, shopping, order entry, or sending text messages

-   **Data model**: OLTP systems typically use a normalized data model, which means that data is stored in multiple tables and relationships are defined between the tables. This allows for efficient data manipulation and ensures data integrity. OLAP systems, on the other hand, often use a denormalized data model, where data is stored in a single table or a small number of tables. This allows for faster querying, but can make data manipulation more difficult.

-   **Data volume**: OLTP systems typically deal with smaller amounts of data, while OLAP systems are designed to handle large volumes of data.

-   **Query complexity**: OLTP systems are designed to handle simple, short queries that involve a small number of records. OLAP systems, on the other hand, are optimized for more complex queries that may involve aggregating and analyzing large amounts of data.

-   **Data updates**: OLTP systems are designed to support frequent data updates and insertions, while OLAP systems are optimized for read-only access to data.

-   **Concurrency**: OLTP systems are designed to support high levels of concurrency and handle a large number of transactions simultaneously. OLAP systems, on the other hand, are optimized for batch processing and may not perform as well with high levels of concurrency.

## Brands {#sec-db-ware-brands .unnumbered}

-   [Amazon Redshift, DynamoDB, RDS, S3]{.underline}
    -   Redshift is best when you have data engineers who want control over infrastructure costs and tuning.
    -   RDS (Relational Database Service)
        -   Benefits over hosting db on EC2: AWS handles scaling, availability, backups, and software and operating system updates
    -   S3 is like googledrive or dropbox
        -   Con: only contains data *about* the files, not what's inside them, i.e. no querying
        -   Ideal use cases
            -   Backup for logs,
            -   Raw sensor data for your IoT application,
            -   Text files from user interviews
            -   Images
            -   Trained machine learning models (with the database simply storing the path to the object)
-   [Google BigQuery]{.underline}
    -   Best when you have very spiky workloads.
-   [Snowflake]{.underline}
    -   A cloud data warehouse for analytics. It's columnar, which means that data is stored (under the hood) in entire columns instead of rows; this makes large analytical queries faster, so it's a common choice for how to build analytical DBs.
    -   Best when you have a more continuous usage pattern
    -   Support for semi-structured data, data sharing, and data lake integration
    -   Resource: [Snowflake Data Warehouse Tutorials](https://sparkbyexamples.com/snowflake-data-warehouse-database-tutorials/)
-   [Azure Synapse Analytics]{.underline}
    -   Fully managed, cloud-based data warehousing service offered by Microsoft Azure. It offers integration with Azure Machine Learning and support for real-time analytics.
-   [Data Bricks]{.underline}
    -   Company behind spark technology and have built a cloud-based data warehousing service.
-   [Teradata]{.underline}
-   [SAP HANA]{.underline}
-   [ClickHouse]{.underline}
    -   Opensource, built by Yandex (Russian search engine)
-   [Apache Hadoop running Apache Hive]{.underline}
    -   Hive: an open-source data warehouse solution for Hadoop infrastructure. It is used to process structured data of large datasets and provides a way to run HiveQL queries.
        -   Resource: [Apache Hive Tutorial with Examples](https://sparkbyexamples.com/apache-hive-tutorial/)

## Strategies {#sec-db-ware-strat .unnumbered}

-   [Inmon]{.underline}\
    ![](./_resources/DB,_Warehouses.resources/1-YOtTVLthg3JnKwv1-o_AcQ.png){width="475"}
    -   Prioritizes accuracy and consistency of data above all else.
    -   Querying is pretty fast (data marts)
    -   Tends to be a lot of upfront work, however subsequent modifications and additions are quite efficient.
    -   Recommended if:
        -   Data accuracy is the most important characteristic of your warehouse
        -   You have time/resources to do a lot of upfront work
-   [Kimball]{.underline}\
    ![](./_resources/DB,_Warehouses.resources/1-sd-VOYKboT6-IzPz_XvNIQ.png){width="475"}
    -   Less structured approach, which speeds up the initial development cycle.
    -   Future iterations require the same amount of work, which can be costly if you're constantly updating the warehouse Fast querying but very few quality checks
    -   Recommended if:
        -   If you're business requirements are well-defined and stable
        -   You are querying lots of data often
-   [Data Vault]{.underline}\
    ![](./_resources/DB,_Warehouses.resources/1-jBUzU3Qk4MSnQnJLgNy3XA.png){width="475"}
    -   Trys to fix disadvantages of Kimball and Inmon strategies by waiting to the last minute to develop any kind of structure
    -   Workflow: Sources --\> unstructured storage (data lake) --\> Staging which supports operations such as batch and streaming processes --\> data vault which stores all raw data virtually untouched (non-relational db?)
    -   Advantages: efficient, fast to implement, and highly dynamic
    -   Disadvantages: querying can be quite slow
        -   Uh doesn't seem to be much cleaning either
    -   Recommended if:
        -   Your business goals change often
        -   You need cheap server and storage costs

## Design a Warehouse {#sec-db-ware-dsgn .unnumbered}

-   **Misc**

    -   [Oracle Data Model Documentation](https://docs.oracle.com/cd/E16338_01/doc.112/e20361/toc.htm)

-   **Considerations**

    -   7 Vs
        -   **Volume**: How big is the incoming data stream and how much storage is needed?
        -   **Velocity**: Refers to speed in which the data is generated and how quickly it needs to be accessed.
        -   **Variety**: What format the data needs to be stored? Structured such as tables or Unstructured such as text, images, etc.
        -   **Value**: What value is derived from storing all the data?
        -   **Veracity**:How trustworthy the data source, type and its processing are?
        -   **Viscosity**: How the data flows through the stream and what is the resistance and the processability?
        -   **Virality**: Ability of the data to be distributed over the networks and its dispersion rate across the users\_
    -   Data Quality (See [Database, Engineering \>\> Data Quality](db-engineering.qmd#sec-db#sec-db-eng-datqual){style="color: green"}) completeness, uniqueness, timeliness, validity, accuracy, and consistency

-   **Components**\
    ![](./_resources/DB,_Warehouses.resources/image.1.png){width="480"}

    -   [Metamodeling]{.underline}:
        -   Defines how the conceptual, logical, and physical models are consistently linked together.
        -   Provides a standardized way of defining and describing models and their components (i.e. grammar, vocabulary), which helps ensure consistency and clarity in the development and use of these models.
        -   Data ownership should be assigned based on a mapping of data domains to the business architecture domains (i.e. market tables to the marketing department?)
    -   [Conceptual Modeling]{.underline} - Involves creating business-oriented views of data that capture the major entities, relationships, and attributes involved in particular domains such as Customers, Employees, and Products.
    -   [Logical Modeling]{.underline} - Involves refining the conceptual model by adding more detail, such as specifying data types, keys, and relationships between entities, and by breaking conceptual domains out into logical attributes, such as Customer Name, Employee Name, and Product SKU.
    -   [Physical Data Modeling]{.underline} - Involves translating the logical data model into specific database schemas that can be implemented on a particular technology platform

-   **Process** ([article](https://towardsdatascience.com/how-to-create-a-data-warehouse-in-5-important-steps-95a8f893a3fd), [article](https://towardsdatascience.com/designing-a-data-warehouse-from-the-ground-up-tips-and-best-practices-e355b6799b99), [article](https://towardsdatascience.com/a-maturity-model-for-data-modeling-and-design-b516d978655c))

    -   [Understand the Core Business Requirements]{.underline}
        -   Create a catalogue of *reporting stories* for each stakeholder to an idea of the reports that each will want generated
            -   These will inform you of the data requirements
            -   e.g. "As a marketing manager, I need to know the number of products the customer bought last year in order to target them with an upsell offer."
                -   From the story above, I can determine that we will need to aggregate the number of products per customer based on sales from the previous year.
    -   [Select the tools and technologies]{.underline}:
        -   Used to build and manage the data warehouse. This may include selecting a database management system (DBMS), data integration and extraction tools, and analysis and visualization tools.
        -   Warehouses - See [Brands](db-warehouses.qmd#sec-db-ware-brands){style="color: green"}
        -   See Production, Tools \>\>
            -   [Orchestration](production-tools.qmd#sec-prod-tools-orch){style="color: green"}
            -   [ELT/ETL Operations](production-tools.qmd#sec-prod-tools-etlelt){style="color: green"}
    -   [Choose a data model]{.underline}
        -   Identify Business Processes
            -   Focus on business process and not business departments as many departments share the same business process
            -   If we focus on department, we might end up with multiple copies of models and have different sources of truth.
        -   Choose a data model from the Business Process
            -   Start with the most impactful model with the lowest risk
                -   Consult with the stakeholders
            -   Should be used frequently and be critical to the business and also it must be built accurately
        -   Decide on the data granularity
            -   Most atomic level is the safest choice since all the types of queries is typically unknown
            -   Need to consider the size and complexity of the data at the various granularities, as well as the resources available/costs for storing and processing it.
            -   [Examples]{.ribbon-highlight}
                -   Customer Level - easy to answer questions about individual customers, such as their purchase history or demographic information.
                -   Transaction Level - easy to answer questions about individual transactions, such as the products purchased and the total amount spent.
                -   Daily or Monthly?
    -   [Create Conceptual Data Models (Tables)]{.underline}
        -   These represent abstract relationships that are part of your business process.
        -   Explains at the highest level what respective domains or concepts are, and how they are related.
        -   The elements within the reporting stories should be consistent with these models
            -   [Example]{.ribbon-highlight}: Retail Sales
                -   Time, Location, Product, and Customer.
                    -   Time might be used to track sales data over different time periods (e.g. daily, monthly, yearly).
                    -   Location might be used to track sales data by store or region.
                    -   Product might be used to track sales data by product category or specific product.
                    -   Customer might be used to track sales data by customer demographics or customer loyalty status.
        -   [Example]{.ribbon-highlight}:\
            ![](./_resources/DB,_Warehouses.resources/image.2.png){width="428"}
            -   Transactions form a key concept, where each transaction can be linked to the Products that were sold, the Customer that bought them, the method of Payment, and the Store the purchase was made in --- each of which constitute their own concept.
            -   Connectors show that each individual transaction can have at most one customer, store, or employee associated with it, but these in turn can be associated with many transactions (multi-prong connector into Transactions)
        -   [Example]{.ribbon-highlight}:\
            ![](./_resources/DB,_Warehouses.resources/1-UFAmuNtWcrOVD-oybhMM0Q.png){width="256"}
            -   Each Customer (1 prong connector) can have 0 or more Orders (multi-prong connector)
            -   Each Order can have 1 or more Products
            -   Each Product can have 0 or more Orders
    -   [Create Logical Data Models]{.underline}
        -   Breakdown each entity of the conceptual model into attributes
            -   [Example]{.ribbon-highlight}:\
                ![](./_resources/DB,_Warehouses.resources/image.3.png){width="528"}
            -   [Example]{.ribbon-highlight}:\
                ![](./_resources/DB,_Warehouses.resources/1-WWi7v6feM7eUfn82M0_5zQ.png){width="357"}
    -   [Create Physical Data Models]{.underline}
        -   Details are added on where exactly (e.g., in what table), and in what format, these data attributes exist.

            -   e.g. finalizing table names, column names, data types, indexes, constraints, and other database objects

        -   Translate the logical data model into specific database schemas that can be implemented on a particular technology platform

            -   e.g. dimensional modelling in a star schema or normalisation in a 3rd normal form in a snowflake model.

        -   [Example]{.ribbon-highlight}:\
            ![](./_resources/DB,_Warehouses.resources/image.4.png){width="528"}

        -   [Example]{.ribbon-highlight}: Dimension model in a star schema\
            ![](./_resources/DB,_Warehouses.resources/1-o3mvnKMXnilbG0SIJJXjUw.png){width="525"}

            -   fact\_ (quantitative) and dim\_ (qualitative)
    -   [Make Design and Environment decisions]{.underline}
        -   Decide on:
            -   Physical data models
            -   History requirements
            -   Environment provisions & set up
    -   [Build a prototype (aka wireframe) of the end product]{.underline}
        -   The business end-user may have a vision, they couldn't coherently articulate at the requirement phase.
        -   The prototype need not use real-world data or be in the reporting tool.
    -   [\*\* Profile known sources data \*\*]{.underline}
        -   Learn about the data quality issues, and try and remediate those issues before designing your data pipelines.
            -   If an issue cannot be resolved, you will have to handle it in your data pipeline
    -   [Build, Test, and Iterate]{.underline}
        -   Create ETL jobs or data pipelines
            -   Iteratively need to unit test the individual components of the pipeline.
        -   The data will need to be moved from the source system into our physical warehouse
        -   Profile data
            -   Data types, and if conversion is required
            -   The amount of history that needs to be pulled
        -   Validate the model's output numbers with the business end-user
    -   Progress towards Data Maturity (see [Job, Organizational and Team Development \>\> Data Maturity](Organizational%20and%20Team%20Development))

## Database Triggers {#sec-db-ware-trig .unnumbered}

![](./_resources/DB,_Warehouses.resources/db-trigger-arch1.webp)

-   A database trigger is a function that gets triggered every time a record is created or updated (or even deleted) in the source table (in this case, a transactional table)
-   Database triggers provide an effective, solution to extracting data from the transactional system and seamlessly integrating it into the data warehouse while also not adversely impacting that system.
-   Use case --- You see a couple of data points in your transactional system's tables that you would require for your reporting metrics but these data points are not being provided by your transactional system's API endpoints. So, there is no way you can write a script in Python or Java to grab these data points using the API. You cannot use direct querying on your transactional system as it can negatively impact its performance.
-   Misc
    -   Notes from [Harnessing Triggers in the Absence of API Endpoints](https://towardsdatascience.com/unlocking-data-access-harnessing-triggers-in-the-absence-of-api-endpoints-4b5c8b775058)
        -   Provides a detailed step-by-step
    -   If your transactional system does not have a lot of traffic (or) is not directly used by end-user applications, then it can be set up as a synchronous process. In that case, the lambda or the Azure functions would need to have the trigger event as the transactional database's staging table. The appropriate database connection information would also need to be provided.
-   Database Triggers
    -   **DDL Triggers** - Set up whenever you want to get notified of structural changes in your database
        -   Useful when you wish to get alerted every time a new schema is defined; or when a new table is created or dropped. Hence, the name DDL (Data Definition Language) triggers.
    -   **DML Triggers** - Fired when new records are inserted, deleted, or updated
        -   i.e. You're notified anytime a data manipulation change happens in a system.
-   Syntax: `<Timing> <Event>`
    -   Trigger Event - The action that should activate the trigger.
    -   Trigger Timing - Whether you need the trigger to perform an activity before the event occurs or after the event occurs.
-   Specialized triggers provided by cloud services
    -   AWS
        -   Lambda Triggers: These triggers help initiate a lambda function when a specified event happens. Events can be internal to AWS, or external in nature. Internal events can be related to AWS services such as Amazon S3, Amazon DynamoDB streams, or Amazon Kinesis. External events can come in from the database trigger of a transactional system outside of AWS or an IoT event.
        -   Cloudwatch Events: If you have used standalone relational databases such as Microsoft SQL Server and SQL Server Management Studio (SSMS), you may have used SQL Server Agent to notify users of a job failure. Cloudwatch is specific to AWS and is used not only to notify users of a job failure but also to trigger Lambda functions and to respond to events. The important difference between a CloudWatch Event and a Lambda Trigger is that while Lambda triggers refer to the capability of AWS Lambda to respond to events, CloudWatch Events is a broader event management service that can handle events from sources beyond Lambda. On a side note, while SQL Server Agent requires an email server to be configured, Cloudwatch has no such requirement.
    -   Azure
        -   Blob Trigger: Azure blobs are similar to S3 buckets offered by AWS. Similar to how Amazon S3 notifications can be used to get alerts about changes in S3 buckets; blob triggers can be used to get notified of changes in Azure blob containers.
        -   Azure Function Trigger: These are the Azure equivalent of AWS Lambda Function Triggers. These triggers can be used to initiate an Azure function in response to an event within Azure or an external event, such as an external transactional database trigger, an HTTP request, or an IoT event hub stream. Azure functions can also be initiated based on a pre-defined schedule using a Timer Trigger.
-   [Example]{.ribbon-highlight}: Transfer data from a transactional database to a warehouse (See article for further details)
    -   Identify table in transactional db with data you want
    -   Create a staging table that's exactly like the transaction table
        -   Ensure that you don't have any additional constraints copied over from the source transactional table. This is to ensure as minimal impact as possible on the transactional system.
        -   For a bulk data transfer of historical transaction data:
            -   `CREATE TABLE AS SELECT` (`SELECT * INTO` in SQL Server) while creating the staging table. This will create the staging table pre-populated with all the data currently available in the transaction table.
            -   Do an empty UPDATE on all the records in the transaction table
                -   e.g. `UPDATE TABLE Pricing_info SET OperationDate=OperationDate`
                -   This is not a recommended approach as it could bog down the transactional system due to the number of updates and undo statements generated. Moreover, the transaction table will also be locked during the entire update operation and will be unavailable for other processes thus impacting the transactional system. This method is okay to use if your transaction table is extremely small in size.
        -   In addition to that, also have a column to indicate the operation performed such as Insert, Update, Delete).
    -   Set up a DML trigger directly on the transaction table
        -   All DML events namely Insert, Delete, and Update in the transaction table should have a separate trigger assigned to them.

            -   The below example shows the trigger for Insert. The rest of the triggers are created similarily --- just by substituting 2 INSERTs (trigger event, select statement) for DELETE or UPDATE (See article for code) and using a different name in CREATE

        -   Insert trigger in (SQL Server)

            ``` sql
            -- Create the trigger
            CREATE TRIGGER TransactionTrigger_pricing_Insert
            ON Pricing_info
            --Trigger Event
            AFTER INSERT
            AS
            BEGIN
                -- Insert new records into the staging table
                INSERT INTO StagingTable_pricing (ID, Column1, Column2, OperationType)
                SELECT ID, Column1, Column2, 'INSERT'
                FROM inserted
            END;
            ```

            -   "Pricing_info" is the name of transactional table with the data you want
            -   "StagingTable_pricing" is the name of the staging table
            -   `AFTER INSERT` where AFTER is the trigger timing and INSERT is the trigger event
            -   In the SELECT statement, "INSERT" is the value for that extra column in the staging table that tells us which type of operation this was.
    -   Set-up the specialized trigger in the warehouse
        -   AWS ![](./_resources/DB,_Warehouses.resources/db-trigger-arch2.webp)
            -   A database DML trigger in the transactional system's database. Whenever a new record comes into the transactional database table, the trigger would insert the new data into a staging table within the transactional database.
                -   If you based it on a schedule (using AWS Cloudwatch events), the Lambda trigger would trigger a lambda function to grab the data from the staging table to a table in the datawarehouse (Redshift)
        -   Azure ![](./_resources/DB,_Warehouses.resources/db-trigger-arch3.webp)
            -   When the timer trigger activates, it would run the Azure Function which would then pick up the new/updated/deleted records from the staging table.
