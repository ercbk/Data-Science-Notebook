# Normalization {#sec-db-norm .unnumbered}

## Misc {#sec-db-norm-misc .unnumbered}

-   Used to organize relational databases
-   Process
    -   Start with an Entity-Relationship (ER) Model
    -   Then apply normalization rules to refine the model and create a normalized database structure.
-   Benefits
    -   Organizing according to data attributes to reduce or eliminate data redundancy (i.e. having the same data in multiple places).
        -   It gives you a set of rules to be able to start categorizing your data and forming a layout
        -   Creates a *single point of truth*
            -   Example: If you want to change the name of one airport in a transportation database, you will only need to update a single data value.
    -   By establishing structure in a database, you are able to help establish a couple of important things: data integrity and scalability.
        -   Integrity ensures that data is entered correctly and accurately. (e.g. field types, tests)
        -   Scalability ensures you have organized the data in a way that it is more computationally efficient when you start to run SQL queries.
    -   Enhanced user adoption and self-service analytics
    -   Flexibility and scalability to adapt to changing business needs
-   Notes from
    -   [When Spreadsheets Aren't Good Enough: A Lesson in Relational Databases](https://towardsdatascience.com/when-spreadsheets-arent-good-enough-a-lesson-in-relational-databases-2e5b0b847f5a)
        -   Gives an example of normalizing a dataset through a MySQL analysis
-   Packages
    -   [{{]{style="color: goldenrod"}[autonormalize](https://github.com/FeatureLabs/autonormalize){style="color: goldenrod"}[}}]{style="color: goldenrod"} - Analyzes transaction df and creates relational tables - python library for automated dataset normalization
    -   [{]{style="color: #990000"}[dm](https://dm.cynkra.com/){style="color: #990000"}[}]{style="color: #990000"} - A grammar of joined tables that provides a consistent set of verbs for consuming, creating, and deploying relational data models
        -   dm objects encapsulate relational data models constructed from local data frames or lazy tables connected to an RDBMS.
        -   dm objects support the full suite of dplyr data manipulation verbs along with additional methods for constructing and verifying relational data models, including key selection, key creation, and rigorous constraint checking.
        -   Once a data model is complete, dm provides methods for deploying it to an RDBMS. This allows it to scale from datasets that fit in memory to databases with billions of rows.
        -   It's a big package, but for ER and normalization, these functions would seem to be useful
            -   `enum_pk_candidates` and `dm_enum_fk_candidates` - Examines table(s) or df(s) for primary/foreign key candidates
            -   `decompose_table` - Eextracts two new tables and creates a new key [model_id]{.var-text} that links both tables.
            -   `dm_examine_constraints` - Checks referential integrity, i.e. every foreign key holds a primary key that is present in the parent table.
                -   If a foreign key contains a reference where the corresponding row in the parent table is not available, that row is an orphan row and the database no longer has referential integrity.
            -   `dm_examine_cardinalities` - Compares number rows between tables using keys
            -   `dm_draw` - Draws a diagram, a visual representation of the data model with edges linking primary and foreign keys.
-   Factors that influence normalizing dimension tables
    -   Data redundancy concerns: If minimizing redundancy is crucial, normalization might be preferred.
    -   Query performance priorities: If query performance is paramount, denormalization often offers advantages.
    -   Data consistency requirements: High consistency needs might favor normalization.
    -   Maintenance complexity: Denormalized dimensions can be simpler to maintain in some cases.
-   Don't use external IDs as primary keys
    -   Since you don't control those IDs, they can change the format and break your queries.

## Forms {#sec-db-norm-form .unnumbered}

-   Databases are often considered as "normalized" if they meet the third normal form
-   See
    -   [A Complete Guide to Database Normalization in SQL](https://towardsdatascience.com/a-complete-guide-to-database-normalization-in-sql-6b16544deb0) for details on the other 4 forms.
        -   Also gives an example of normalizing a dataset through a posgresSQL analysis
    -   [SQL Explained: Normal Forms](https://towardsdatascience.com/sql-explained-normal-forms-e2a8b8ce1122)
        -   More examples of applying the first 3 forms
-   First Normal Form (1NF)
    -   Every value in each column of a table must be reduced to its most simple value, also known as atomic.
        -   An atomic value is one where there are no sets of values within a column. (i.e. 1 value per cell)
    -   There are no repeating columns or rows within the database.
    -   Each table should have a primary key which can be defined as a non-null, unique value that identifies each row insertion.
-   Second Normal Form (2NF)
    -   Conforms to first normal form rules.
    -   Adjust columns so that each table only contains data relating to the primary key.
    -   Foreign keys are used to establish relationships between tables.
-   Third Normal Form (3NF)
    -   Conforms to both first and second normal form rules.
    -   Necessary to shift or remove columns (attributes) that are transitively dependent, which means they rely on other columns that aren't foreign or primary keys.
