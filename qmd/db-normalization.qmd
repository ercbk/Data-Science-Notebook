# Normalization {#sec-db-norm .unnumbered}

## Misc {#sec-db-norm-misc .unnumbered}

-   Organizing according to data attributes to reduce or eliminate data redundancy (i.e. having the same data in multiple places).
    -   It gives you a set of rules to be able to start categorizing your data and forming a layout
-   By establishing structure in a database, you are able to help establish a couple of important things: data integrity and scalability.
    -   Integrity ensures that data is entered correctly and accurately.
    -   Scalability ensures you have organized the data in a way that it is more computationally efficient when you start to run SQL queries.
-   Notes from [When Spreadsheets Aren't Good Enough: A Lesson in Relational Databases](https://towardsdatascience.com/when-spreadsheets-arent-good-enough-a-lesson-in-relational-databases-2e5b0b847f5a)
    -   Gives an example of normalizing a dataset through a MySQL analysis
-   Resources
    -   [Using DuckDB-WASM for in-browser Data Engineering](https://tobilg.com/using-duckdb-wasm-for-in-browser-data-engineering#heading-example-data-engineering-pipeline) has a complete example that builds a snowflake schema with SQL code.
-   Packages
    -   [{{]{style="color: goldenrod"}[autonormalize](https://github.com/FeatureLabs/autonormalize){style="color: goldenrod"}[}}]{style="color: goldenrod"} - analyzes transaction df and creates relational tables - python library for automated dataset normalization

## Terms {#sec-db-norm-terms .unnumbered}

-   [**Dimension Tables**]{style="color: #009499"} - Contains data about how the data in **Fact Table** is being analyzed. They facilitate the fact table in gathering different dimensions on the measures which are to be taken.
    -   Provides descriptive attributes for the fact table data. Think of them as adding context and details to the "what" and "how much". Each dimension table focuses on a specific category of information.
    -   [Example]{.ribbon-highlight}: An online retail store wants to analyze sales data.
        -   Customer Dimension - Provides information about individual customers, like their name and location. The customer id field is the primary key that links it back to the fact table.
        -   Product Dimension - Describes each product, including its category, brand, and other relevant details. The product id field is the primary key that links it back to the fact table.
-   [**Fact Tables**]{style="color: #009499"} - Contain data corresponding to any business process. Every row represents any event that can be associated with any process. It stores quantitative information for analysis.
    -   Focuses on quantitative data representing business events or transactions. Think of it as storing the "what" and "how much" of your data.
    -   [Example]{.ribbon-highlight}: An online retail store wants to analyze sales data.
        -   Fields: quantity, price, and total say what products were ordered, by whom, when, and how much they cost.
        -   An order id would be the primary key. and the table would also contain the foreign keys for all the dimension tables (e.g product id and customer id) associated with it.

## Forms {#sec-db-norm-form .unnumbered}

-   Databases are often considered as "normalized" if they meet the third normal form
-   See [A Complete Guide to Database Normalization in SQL](https://towardsdatascience.com/a-complete-guide-to-database-normalization-in-sql-6b16544deb0) for details on the other 4 forms.
    -   Also gives an example of normalizing a dataset through a posgresSQL analysis
-   First normal form (1NF)
    -   Every value in each column of a table must be reduced to its most simple value, also known as atomic.
        -   An atomic value is one where there are no sets of values within a column. (i.e. 1 value per cell)
    -   There are no repeating columns or rows within the database.
    -   Each table should have a primary key which can be defined as a non-null, unique value that identifies each row insertion. Second normal form (2NF)
    -   Conforms to first normal form rules.
    -   Adjust columns so that each table only contains data relating to the primary key.
    -   Foreign keys are used to establish relationships between tables. Third normal form (3NF)
    -   Conforms to both first and second normal form rules.
    -   Necessary to shift or remove columns (attributes) that are transitively dependent, which means they rely on other columns that aren't foreign or primary keys.

## Schema {#sec-db-norm-schema .unnumbered}

-   Misc
    -   Factors that influence normalizing dimension tables
        -   Data redundancy concerns: If minimizing redundancy is crucial, normalization might be preferred.
        -   Query performance priorities: If query performance is paramount, denormalization often offers advantages.
        -   Data consistency requirements: High consistency needs might favor normalization.
        -   Maintenance complexity: Denormalized dimensions can be simpler to maintain in some cases.
    -   Don't use external IDs as primary keys
        -   Since you don't control those IDs, they can change the format and break your queries.
-   [Star]{.underline}\
    ![](./_resources/DB,_Engineering.resources/1-k6MxKPbs51gXooJ3ZOS1tQ.png){.lightbox group="schema" width="532"}
    -   [Example]{.ribbon-highlight}: A Star schema of sales data with dimensions such as customer, product & time*.*
    -   In a star schema, as the structure of a star, there is one fact table in the middle and a number of associated dimension tables.
    -   The fact table consists of primary information. It surrounds the smaller dimension lookup tables which will have details for different fact tables. The primary key which is present in each dimension is related to a foreign key which is present in the fact table.
    -   The fact tables are in 3NF form and the dimension tables are in denormalized form. Every dimension in star schema should be represented by the only one-dimensional table.
-   [Snowflake]{.underline}\
    ![](_resources/DB-Normalization.resources/Snowflake-Schema.webp){.lightbox group="schema" width="532"}
    -   Snowflake schema acts like an extended version of a star schema. There are additional subdimensions added to dimensions.
    -   Unlike the Star schema, dimensions are normalized.
    -   Can be slower than star schemas due to complex joins across multiple tables, but achieves better storage efficiency compared to star schemas due to reduced data redundancy.
    -   There are hierarchical relationships and child tables involved that can have multiple parent tables.
    -   The advantage of snowflake schema is that it uses small disk space. The implementation of dimensions is easy when they are added to this schema.
-   [Fact Constellation]{.underline} or [Galaxy\
    ]{.underline}![](_resources/DB-Normalization.resources/schema-fact-constellation-1.jpg){.lightbox group="schema" width="532"}
    -   A fact constellation can consist of multiple fact tables. These are more than two tables that share the same dimension tables â€” like connected Star schema.
    -   The shared dimensions in this schema are known as **conformed dimensions**. Denormalization in shared dimension tables might increase storage size compared to fully normalized schemas.
    -   Dimensions can be normalized but is rare in this schema due the level of complexity already present.
    -   Useful when aggregation of fact tables is necessary. Fact constellations are considered to be more complex than star or snowflake schemas. Therefore, more flexible but harder to implement and maintain. Joins across multiple fact and dimension tables can lead to complex queries with potential performance impacts.

## Design {#sec-db-norm-dsgn .unnumbered}

-   **Misc**

    -   [Oracle Data Model Documentation](https://docs.oracle.com/cd/E16338_01/doc.112/e20361/toc.htm)

-   **Considerations**

    -   7 Vs
        -   **Volume**: How big is the incoming data stream and how much storage is needed?
        -   **Velocity**: Refers to speed in which the data is generated and how quickly it needs to be accessed.
        -   **Variety**: What format the data needs to be stored? Structured such as tables or Unstructured such as text, images, etc.
        -   **Value**: What value is derived from storing all the data?
        -   **Veracity**:How trustworthy the data source, type and its processing are?
        -   **Viscosity**: How the data flows through the stream and what is the resistance and the processability?
        -   **Virality**: Ability of the data to be distributed over the networks and its dispersion rate across the users\_
    -   Data Quality (See [Database, Engineering \>\> Data Quality](db-engineering.qmd#sec-db#sec-db-eng-datqual){style="color: green"}) completeness, uniqueness, timeliness, validity, accuracy, and consistency

-   **Components**\
    ![](./_resources/DB,_Warehouses.resources/image.1.png){.lightbox width="480"}

    -   [Metamodeling]{.underline}:
        -   Defines how the conceptual, logical, and physical models are consistently linked together.
        -   Provides a standardized way of defining and describing models and their components (i.e. grammar, vocabulary), which helps ensure consistency and clarity in the development and use of these models.
        -   Data ownership should be assigned based on a mapping of data domains to the business architecture domains (i.e. market tables to the marketing department?)
    -   [Conceptual Modeling]{.underline} - Involves creating business-oriented views of data that capture the major entities, relationships, and attributes involved in particular domains such as Customers, Employees, and Products.
    -   [Logical Modeling]{.underline} - Involves refining the conceptual model by adding more detail, such as specifying data types, keys, and relationships between entities, and by breaking conceptual domains out into logical attributes, such as Customer Name, Employee Name, and Product SKU.
    -   [Physical Data Modeling]{.underline} - Involves translating the logical data model into specific database schemas that can be implemented on a particular technology platform

-   **Process** ([article](https://towardsdatascience.com/how-to-create-a-data-warehouse-in-5-important-steps-95a8f893a3fd), [article](https://towardsdatascience.com/designing-a-data-warehouse-from-the-ground-up-tips-and-best-practices-e355b6799b99), [article](https://towardsdatascience.com/a-maturity-model-for-data-modeling-and-design-b516d978655c))

    -   [Understand the Core Business Requirements]{.underline}
        -   Create a catalogue of *reporting stories* for each stakeholder to an idea of the reports that each will want generated
            -   These will inform you of the data requirements
            -   e.g. "As a marketing manager, I need to know the number of products the customer bought last year in order to target them with an upsell offer."
                -   From the story above, I can determine that we will need to aggregate the number of products per customer based on sales from the previous year.
    -   [Select the tools and technologies]{.underline}:
        -   Used to build and manage the data warehouse. This may include selecting a database management system (DBMS), data integration and extraction tools, and analysis and visualization tools.
        -   Warehouses - See [Brands](db-warehouses.qmd#sec-db-ware-brands){style="color: green"}
        -   See Production, Tools \>\>
            -   [Orchestration](production-tools.qmd#sec-prod-tools-orch){style="color: green"}
            -   [ELT/ETL Operations](production-tools.qmd#sec-prod-tools-etlelt){style="color: green"}
    -   [Choose a data model]{.underline}
        -   Identify Business Processes
            -   Focus on business process and not business departments as many departments share the same business process
            -   If we focus on department, we might end up with multiple copies of models and have different sources of truth.
        -   Choose a data model from the Business Process
            -   Start with the most impactful model with the lowest risk
                -   Consult with the stakeholders
            -   Should be used frequently and be critical to the business and also it must be built accurately
        -   Decide on the data granularity
            -   Most atomic level is the safest choice since all the types of queries is typically unknown
            -   Need to consider the size and complexity of the data at the various granularities, as well as the resources available/costs for storing and processing it.
            -   [Examples]{.ribbon-highlight}
                -   Customer Level - easy to answer questions about individual customers, such as their purchase history or demographic information.
                -   Transaction Level - easy to answer questions about individual transactions, such as the products purchased and the total amount spent.
                -   Daily or Monthly?
    -   [Create Conceptual Data Models (Tables)]{.underline}
        -   These represent abstract relationships that are part of your business process.
        -   Explains at the highest level what respective domains or concepts are, and how they are related.
        -   The elements within the reporting stories should be consistent with these models
            -   [Example]{.ribbon-highlight}: Retail Sales
                -   Time, Location, Product, and Customer.
                    -   Time might be used to track sales data over different time periods (e.g. daily, monthly, yearly).
                    -   Location might be used to track sales data by store or region.
                    -   Product might be used to track sales data by product category or specific product.
                    -   Customer might be used to track sales data by customer demographics or customer loyalty status.
        -   [Example]{.ribbon-highlight}:\
            ![](./_resources/DB,_Warehouses.resources/image.2.png){.lightbox group="concept-dm" width="428"}
            -   Transactions form a key concept, where each transaction can be linked to the Products that were sold, the Customer that bought them, the method of Payment, and the Store the purchase was made in --- each of which constitute their own concept.
            -   Connectors show that each individual transaction can have at most one customer, store, or employee associated with it, but these in turn can be associated with many transactions (multi-prong connector into Transactions)
        -   [Example]{.ribbon-highlight}:\
            ![](./_resources/DB,_Warehouses.resources/1-UFAmuNtWcrOVD-oybhMM0Q.png){.lightbox group="concept-dm" width="256"}
            -   Each Customer (1 prong connector) can have 0 or more Orders (multi-prong connector)
            -   Each Order can have 1 or more Products
            -   Each Product can have 0 or more Orders
    -   [Create Logical Data Models]{.underline}
        -   Breakdown each entity of the conceptual model into attributes
            -   [Example]{.ribbon-highlight}:\
                ![](./_resources/DB,_Warehouses.resources/image.3.png){.lightbox group="log-dm" width="528"}
            -   [Example]{.ribbon-highlight}:\
                ![](./_resources/DB,_Warehouses.resources/1-WWi7v6feM7eUfn82M0_5zQ.png){.lightbox group="log-dm" width="357"}
    -   [Create Physical Data Models]{.underline}
        -   Details are added on where exactly (e.g., in what table), and in what format, these data attributes exist.

            -   e.g. finalizing table names, column names, data types, indexes, constraints, and other database objects

        -   Translate the logical data model into specific database schemas that can be implemented on a particular technology platform

            -   e.g. dimensional modelling in a star schema or normalisation in a 3rd normal form in a snowflake model.

        -   [Example]{.ribbon-highlight}:\
            ![](./_resources/DB,_Warehouses.resources/image.4.png){.lightbox group="phys-dm" width="528"}

        -   [Example]{.ribbon-highlight}: Dimension model in a star schema\
            ![](./_resources/DB,_Warehouses.resources/1-o3mvnKMXnilbG0SIJJXjUw.png){.lightbox group="phys-dm" width="525"}

            -   fact\_ (quantitative) and dim\_ (qualitative)
    -   [Make Design and Environment decisions]{.underline}
        -   Decide on:
            -   Physical data models
            -   History requirements
            -   Environment provisions & set up
    -   [Build a prototype (aka wireframe) of the end product]{.underline}
        -   The business end-user may have a vision, they couldn't coherently articulate at the requirement phase.
        -   The prototype need not use real-world data or be in the reporting tool.
    -   [\*\* Profile known sources data \*\*]{.underline}
        -   Learn about the data quality issues, and try and remediate those issues before designing your data pipelines.
            -   If an issue cannot be resolved, you will have to handle it in your data pipeline
    -   [Build, Test, and Iterate]{.underline}
        -   Create ETL jobs or data pipelines
            -   Iteratively need to unit test the individual components of the pipeline.
        -   The data will need to be moved from the source system into our physical warehouse
        -   Profile data
            -   Data types, and if conversion is required
            -   The amount of history that needs to be pulled
        -   Validate the model's output numbers with the business end-user
    -   Progress towards Data Maturity (see [Job, Organizational and Team Development \>\> Data Maturity](Organizational%20and%20Team%20Development))
