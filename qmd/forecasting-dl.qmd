# Forecasting, DL

TOC

* Misc
* Preprocessing
* Feed Forward
* LSTM
* RNN
* Transformers



Misc

* A **callback** is a function that performs some action during the training process
	* e.g. saving a model after each training epoch; early stopping when a threshold is reached
	* [List](https://keras.io/api/callbacks/) of Keras callbacks



Preprocessing

* Misc
	* Notes from
		* [EXAMPLE py, global, keras LSTM, preprocessing - Deep Learning for Forecasting: Preprocessing and Training | by Vitor Cerqueira | Mar, 2023 | Towards Data Science](https://towardsdatascience.com/deep-learning-for-forecasting-preprocessing-and-training-49d2198fc0e2)
* Scale by the mean
	* For global forecasting, this brings all series into a common value range. Therefore, the scale of the values won't be a factor in model training.
	* Example: global forecasting

```
from sklearn.model_selection import train_test_split

# leaving last 20% of observations for testing
train, test = train_test_split(data, test_size=0.2, shuffle=False)

# computing the average of each series in the training set
mean_by_series = train.mean()

# mean-scaling: dividing each series by its mean value
train_scaled = train / mean_by_series
test_scaled = test / mean_by_series
```

* Logging
	* Log series after scaling transformation
	* Handles heterskedacity
	* Can create more compact ranges, which then enables more efficient neural network training
	* Helps avoid saturation areas of the neural network.
		* Saturation occurs when the neural network becomes insensitive to different inputs. This hampers the learning process, leading to a poor model.
	* Example

```
import numpy as np

class LogTransformation:

    @staticmethod
    def transform(x):
        xt = np.sign(x) * np.log(np.abs(x) + 1)

        return xt

    @staticmethod
    def inverse_transform(xt):
        x = np.sign(xt) * (np.exp(np.abs(xt)) - 1)

        return x

# log transformation
train_scaled_log = LogTransformation.transform(train_scaled)
test_scaled_log = LogTransformation.transform(test_scaled)
```

* Create a matrix with lags and leads
	* Example: Univariate![](./_resources/Forecasting,_DL.resources/image.png)

```
# src module here: https://github.com/vcerqueira/blog/tree/main/src
from src.tde import time_delay_embedding


# using 3 lags as explanatory variables
N_LAGS = 3
# forecasting the next 2 values
HORIZON = 2


# using a sliding window method called time delay embedding
X, Y = time_delay_embedding(series, n_lags=N_LAGS, horizon=HORIZON, return_Xy=True)
```

* "Target variables" are lead variables

* Example: Global![](./_resources/Forecasting,_DL.resources/image.1.png)

```
# src module here: https://github.com/vcerqueira/blog/tree/main/src
from src.tde import time_delay_embedding


N_FEATURES = 1 # time series is univariate
N_LAGS = 3 # number of lags
HORIZON = 2 # forecasting horizon


# transforming time series for supervised learning
train_by_series, test_by_series = {}, {}
# iterating over each time series
for col in data:
    train_series = train_scaled_log[col]
    test_series = test_scaled_log[col]

    train_series.name = 'Series'
    test_series.name = 'Series'

    # creating observations using a sliding window method
    train_df = time_delay_embedding(train_series, n_lags=N_LAGS, horizon=HORIZON)
    test_df = time_delay_embedding(test_series, n_lags=N_LAGS, horizon=HORIZON)

    train_by_series[col] = train_df
    test_by_series[col] = test_df

train_df = pd.concat(train_by_series, axis=0) # combine data row-wise
```


Feed-Forward

* From Hyndman paper on Local vs Global modeling, [Principles and Algorithms for Forecasting Groups of Time Series:Locality and Globality](https://www.monash.edu/business/ebs/research/publications/ebs/wp45-2020.pdf)
	* Deep Network Autoregressive (Keras):
		* ReLu MLP with 5 layers, each of 32 units width
		* Linear activation in the final layer
		* Adam optimizer with default learning rate.
		* Early stopping on a cross-validation set at 15% of the dataset
		* Batch size is set to 1024 for speed
		* Loss function is the mean absolute error (MAE).



LSTM

* Extensions
	* CNN-LSTM - utilizes the CNN layers to improve the feature extraction before sequence prediction by the LSTM
	* Autoregressive LSTM (AR-LSTM) -takes in n timesteps worth of data for a product and then makes a prediction for the n+1 week. The prediction for the n+1 week is then used to generate the features as input for the n+2 th week’s prediction
		GRU - bi-directional model - In NLP, it uses the preceding value and a succeeding value to predict the middle value. In forecasting, the preceding value is used as a substitute for the succeeding value
		![](./_resources/Forecasting,_DL.resources/1-4c3fWQw_g-ChBJg7xKPQNg.png)
		* Example: You have a sequence 15,20,22,24 and you want to predict the next value
			* One GRU which takes the input 15,20,22,24 often called the **forward GRU**.
				* This input sequence of the forward model is often called the **forward context**
			* Then you use another representation of the same sequence in reverse order i.e. 24,22,20 and 15 which is used by another GRU called the **backward GRU**.
				* This input sequence of the forward model is often called the **backward context**
			* The final prediction is a function of the prediction of both the GRUs.
		* GRU Extension: "bi-directional model of forecasting with truly bi-directional features" (BD-BLSTM) ([article](https://towardsdatascience.com/feature-engineering-and-deep-learning-b90fc863b07))![](./_resources/Forecasting,_DL.resources/1-cO3WSouhdpvuj7AruVfAYQ.png)
			* Basically adds seasonality to the GRU
				* Example: Forecasting the value for May 24th 8:00am
					* Forward GRU: 07:00 AM, 07:15 AM, and 07:45 AM values from 24th May AND 07:00 AM, 07:15 AM, 07:45 AM values from 23rd May
					* Backward GRU: 08:15 AM, 08:30 AM and 08:45 AM values from 23rd May





RNN

* Essentially a bunch of neural nets stacked on top of each other
* overly simplistic in their assumptions about what should be passed to the next hidden layer
	* Long Short Term Memory (LSTM) and Gate Recurring Units (GRU) layers provide filters for what information get’s passed down the chain
* RNN Example

![](./_resources/Forecasting,_DL.resources/1-ENY_yH2HIWsjVQY9D6jXxA.png)

* x’s in blue are predictor variables
* h’s in yellow are hidden layers
* y’s in green are predicted values
* output of the model at h1 feeds into the next model at h2, etc.
	* Not sure if he output is y1 or some kind of embedding from h1 that feeds into h2
* I think each predictor variable



Transformers

* Misc
	* I think most of the research is in using these as forecast models themselves and not necessarily as a categorical/discrete feature transformation
	* Notes from
		* [Transformers in Time Series: A Survey](https://arxiv.org/pdf/2202.07125v3.pdf)
	* **Attention heads** -  enable the Transformer to learn relationships between a time step and every other time step in the input sequence
* Architecture Comparisons
	* RNNs implement sequential processing: The input (let’s say sentences) is processed word by word.
	* Transformers use non-sequential processing: Sentences are processed as a whole, rather than word by word
	* The LSTM requires 8 time-steps to process the sentences, while BERT requires only 2
		* BERT is better able to take advantage of parallelism, provided by modern GPU acceleration
	* RNNs are forced to compress their learned representation of the input sequence into a single state vector before moving to future tokens.
	* LSTMs solved the vanishing gradient issue that vanilla RNNs suffer from, but they are still prone to exploding gradients. Thus, they are struggling with longer dependencies
	* Transformers, on the other hand, have much higher bandwidth. For example, in the Encoder-Decoder Transformer model, the Decoder can directly attend to every token in the input sequence, including the already decoded.
	* Transformers use a special case called _Self-Attention_: This mechanism allows each word in the input to reference every other word in the input.
	* Transformers can use large Attention windows (e.g. 512, 1048). Hence, they are very effective at capturing contextual information in sequential data over long ranges.
* Issues
	* The initial BERT model has a limit of 512 tokens. The naive approach to addressing this issue is to truncate the input sentences.
		* Alternatively, we can create Transformer Models that surpass that limit, making it up to 4096 tokens. However, the cost of _self-attention_ is quadratic with respect to the sentence length.
* Robustness and Model Size![](./_resources/Forecasting,_DL.resources/Screenshot (801).png)
	* Robustness: As the time series gets longer (aka Input Len), Autoformer holds it's performance best
		* The rest start to crumble after the length gets past 192 time points
	* Model Size: Autoformer is again the best and holds its performance pretty much up through 24 layers
	* Hyper-parameters:
		* embedding dimension
		* number of heads
		* number of layers
			* In general, 3-6 layers yields the best performance
			* More layers typically adds more accuracy. In NLP and Computer Vision (CV), their models are usually 12 to 128 layers, so this will be an area of improvement.
	* Autoformer \[[Wu et al., 2021](https://github.com/thuml/autoformer)\] has a moving average trend decomposition component that can be added to other transformer architectures to greatly enhance performance![](./_resources/Forecasting,_DL.resources/Autoformer.png)




