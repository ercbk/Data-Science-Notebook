# Missingness {#sec-missing}
TOC

-   Misc
-   Choosing an Imputation Method
-   Bayesian
-   Multiple Imputation
-   Time Series

Misc

-   Also see

    -   [EDA](EDA) \>\> Missingness
    -   [Model Building, tidymodels](Model%20Building,%20tidymodels) \>\> Imputation
        -   bagging and knn methods for cross-sectional data
        -   rolling method for time series data
    -   Harrell RMS 3.5 [Strategies for Developing an Imputation Model](https://hbiostat.org/rmsc/missing.html#strategies-for-developing-an-imputation-model)

-   \*\*Don't impute missing values before your training/test split

-   Imputing Types full-information maximum likelihood

    -   multiple imputation
    -   one-step Bayesian imputation

-   Missness Types (MCAR, MAR, and MNAR)![](./_resources/Missingness.resources/Screenshot%20(541).png)

-   Multivariate Imputation with Chained Equation (MICE) assumes MAR

    -   method entails creating multiple imputations for each missing value as opposed to just one. The algorithm addresses statistical uncertainty and enables users to impute values for data of different types.

-   Stochastic Regression Imputation is problematic

    -   Popular among practitioners though
    -   Issues
        -   Stochastic regression imputation might lead to **implausible values** (e.g. negative incomes).
        -   Stochastic regression imputation has **problems with heteroscedastic data**
    -   Bayesian PMM handles these issues

-   Missingness in RCT due dropouts (aka *loss to follow-up*)

    -   Notes from [To impute or not: the case of an RCT with baseline and follow-up measurements](https://www.rdatagen.net/post/2022-04-12-to-impute-or-not-the-case-of-an-rct-with-baseline-and-follow-up-measurements/)
        -   [{mice}]{style="color: #990000"} used for imputation
    -   Bias in treatment effect due to missingness
        -   If there are adjustment variables that affect unit dropout then bias increases as variation in treatment effect across units increases (aka hetergeneity)
            -   In the example, a baseline measurement of the outcome variable, used an explanatory variable, was also causal of missingness. Greater values of this variable resulted in greater bias
            -   Using multiple imputation resulted in less bias than just using complete cases, but still underestimated the treatment effect.
        -   If there are no such variables, then there is no bias due to hetergeneous treatment effects
            -   Complete cases of the data can be used
    -   Last observation carried forward
        -   Sometimes used in clinical trials because it tends to be conservative, setting a higher bar for showing that a new therapy is significantly better than a traditional therapy.
        -   Must assume that the previous value (e.g. 2008 score) is similar to the ahead value (e.g. 2010 score).
        -   Information about trajectories over time is thrown away.

-   Assessment of Imputations

    -   See [{naniar}]{style="color: #990000"} vignette - [Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations \| Journal of Statistical Software](https://www.jstatsoft.org/article/view/v105i07)

Choosing an Imputation Method

-   (\*\* Don't use this. Just putting it here to be aware of \*\*) Standard Procedure for choosing an imputation method
    -   Issues
        -   Some methods will be favored based on the metric used
            -   conditional means methods (RMSE)
            -   conditional medians methos (MAE) Chosen methods tend to artificially strengthen the association between variables. As a consequence, statistical estimation and inference techniques applied to the so-imputed data set can be invalid.
    -   Steps
        1.  Select some observations
        2.  Set their status to missing
        3.  Impute them with different methods
        4.  Compare their imputation accuracy
            -   For numeric variables, RMSE or MAE typically used
            -   For categoricals, percentage of correct predictions (PCP)
-   Initial Considerations
    -   If a dataset's feature has missing data in more than 80% of its records, it is probably best to remove that feature altogether.

    -   If a feature with missing values is strongly correlated with other missing values, it's worth considering using advanced imputation techniques that use information from those other features to derive values to replace the missing data.

    -   If a feature's values are missing not at random (MNAR), remove methods like MICE from consideration. I-Score [{]{style="color: #990000"}[Iscores](https://github.com/missValTeam/Iscores){style="color: #990000"}[}]{style="color: #990000"}, [Paper](https://arxiv.org/abs/2106.03742)

    -   A proper scoring rule metric

    -   Consistent for MCAR, but MAR requires additional assumptions

        -   "valid under missing at random (MAR) if we restrict the random projections in variable space to always include all variables, which in turn requires access to some complete observations"

    -   Kinda complicated. I need to read the paper

Bayesian

-   Predictive Mean Matching (PMM)
    -   Notes from:
        -   [Predictive Mean Matching Imputation (Theory & Example in R)](https://statisticsglobe.com/predictive-mean-matching-imputation-method/)
        -   [Predictive Mean Matching Imputation in R (mice Package Example)](https://opencodecom.net/post/2021-06-01-predictive-mean-matching-imputation-in-r-mice-package-example/)
    -   Uses a bayesian regression to predict a missing value, then randomly picks a value from a group of observed values that are closest to the predicted value.
    -   Steps
        1.  Estimate a linear regression model:
            -   Use the variable we want to impute as Y.
            -   Use a set of good predictors as X (Guidelines for the selection of X can be found in van Buuren, 2012, p. 128).
            -   Use only the observed values of X and Y to estimate the model.
        2.  Draw randomly from the posterior predictive distribution of β\^ and produce a new set of coefficients β∗.
            -   This bayesian step is needed for all multiple imputation methods to create some random variability in the imputed values.
        3.  Calculate predicted values for observed and missing Y.
            -   Use β\^ to calculate predicted values for observed Y.
            -   Use β∗ to calculate predicted values for missing Y.
        4.  For each case where Y is missing, find the closest predicted values among cases where Y is observed.
            -   Example:
                -   Yi is missing. Its predicted value is 10 (based on β∗).
                -   Our data consists of five observed cases of Y with the values 6, 3, 22, 7, and 12.
                -   In step 3, we predicted the values 7, 2, 20, 9, and 13 for these five observed cases (based on β\^).
                -   The predictive mean matching algorithm selects the closest observed values (typically three cases) to our missing value Yi. Hence, the algorithm selects the values 7, 9, and 13 (the closest values to 10).
        5.  Draw randomly one of these three close cases and impute the missing value Yi with the observed value of this close case.
            -   Example continued:
                -   The algorithm draws randomly from 6, 7, and 12 (the observed values that correspond to the predicted values 7, 9, and 13).
                -   The algorithm chooses 12 and substitutes this value to Yi.
        6.  In case of multiple imputation (which I strongly advise), steps 1-5 are repeated several times.
            -   Each repetition of steps 1-5 creates a new imputed data set.
            -   With multiple imputation, missing data is typically imputed 5 times.
    -   Example

```         
        data_imp <- complete(mice(data,
                                  m = 5,
                                  method = "pmm"))
```

-   m is the number of times to impute the data
-   `complete` formats the data into different shapes according to an `action` arg
-   Running `parmice` instead of `mice` imputes in parallel

Multiple Imputation

-   aka "multiply" imputed data
-   Fitting a regression model
    -   See [If you fit a model with multiply imputed data, you can still plot the line](https://solomonkurz.netlify.app/post/2021-10-21-if-you-fit-a-model-with-multiply-imputed-data-you-can-still-plot-the-line/)
    -   Methods
        -   Predict then Combine (PC)
        -   Combine then Predict (CP)

Time Series

-   If seasonality is present, mean, median, mode, random assignment, or previous value methods shouldn't be used.
