# Feature Engineering, Tokenization

TOC

* Misc
* Terms
* Preprocessing
* Textrecipes
* Tokenization Algorithms
* Engineering
* Monitoring Embeddings
* Augmentation



Misc

* Also see
	* [Diagnostics, NLP](Diagnostics, NLP)
	* [EDA, Text](EDA, Text)
	* [NLP, General](NLP, General)
* Embeddings can be aggregated (e.g. averaged) to make larger groupings
	* Example: averaging the embeddings of food items to create an embedding for a meal
* Text augmentation
	* Useful for imbalanced outcomes with text predictors
		* Better performance than subsampling
	* words are randomly swapped, deleted, as well as replaced or inserted with synonyms using pretrained word embeddings![](./_resources/Feature_Engineering,_Tokenization.resources/1-P5z8tXzjoLxh3kUDqeO7zQ.png)
	* [{{textattack}}]{style='color: goldenrod'}, [article](https://towardsdatascience.com/nlp-project-with-augmentation-attacks-aspect-based-sentiment-analysis-3342510c90e7), [ipynb](https://github.com/misha345a/E-commerce_Reviews_Classifier/blob/main/Create_Balanced_Dataset.ipynb)
	* Adversarial Text Attack
		* Diagnostic to test how a model performs on a test set where data augmentation techniques have been applied![](./_resources/Feature_Engineering,_Tokenization.resources/1-odITQb5uQ0QZY0UDgRrp8w.png)
		* [{{textattack}}]{style='color: goldenrod'}, [article](https://towardsdatascience.com/nlp-project-with-augmentation-attacks-aspect-based-sentiment-analysis-3342510c90e7), [ipynb](https://github.com/misha345a/E-commerce_Reviews_Classifier/blob/main/LSTM_Models_And_Attack.ipynb)![](./_resources/Feature_Engineering,_Tokenization.resources/1-ZD9uA9_XR5iIz-TIer6UlQ.png)
	* 




Terms

* **Out-of-Vocabulary** **(OOV)** - Sometimes vocabularies a trimmed to the only the most common tokens. Words that aren't in the vocabulary get assigned OOV tokens.
* **Tokenization** -  process of splitting a phrase, sentence, paragraph, one or multiple text documents into smaller units. Different algorithms follow different processes in performing tokenization
* **Token** - output from tokenization — a word, a subword (e.g. prefix, suffix, or root), or even a character.
* **Vocabulary** - a set of unique tokens in a **corpus** (a dataset in NLP) that is then converted into numbers (IDs) which helps us in modeling



Preprocessing

* Misc
	* [Primer on Cleaning Text Data](https://towardsdatascience.com/primer-to-cleaning-text-data-7e856d6e5791) - py code for cleaning
	* [The Ultimate Preprocessing Pipeline for Your NLP Models](https://towardsdatascience.com/the-ultimate-preprocessing-pipeline-for-your-nlp-models-80afd92650fe) - py code for cleaning
		* Clusters embeddings to remove "boilerplate language" (i.e. noise) from data
		* Parts of Speech (POS) tagging
	* \*\* Cleaning needs to happen before any clustering, POS tagging, Lemmatization or Stemming takes place. \*\*
		* Similarly, clustering should happen before POS tagging and lemmatizing, since using the entire text will make the compute extremely costly and and less effective.
* replace NA values with empty spaces
* lowercase all text
* replace digits with words for the numbers
* remove punctuation
	* ``#$%&\’()*+,-./:;?@[\\]^_{|}`~`` 
* remove emojis
	* if you are trying to sentiment analysis, trying to transform emojis into some text format instead of outright removing them may be beneficial
* Spell out contractions
* Strip HTML Tags
* remove stopwords
* Remove accented characters
* Remove URLs, Mentions (@), hastags (#) and special characters
* remove whitespace
* Remove boilerplate language (see article)![](./_resources/Feature_Engineering,_Tokenization.resources/image.4.png)
	* Text embeddings are created. The embeddings are clustered to find repeatedly occurring sentences and words and removes them, with an assumption that something that is repeated more than a threshold number of times, is probably “noise”.
	* The resultant text is a cleaner, more meaningful, summarized form of the input text. By removing noise, we are pointing our algorithm to concentrate on the important stuff only.
	* Article reduces size of text by 88%
* Parts of Speech (POS) tagging
	* Labels each word in a sentence as a noun, verb, adjective, pronoun, preposition, adverb, conjunction, or interjection.
	* Context can largely affect the natural language understanding (NLU) processes of algorithms.
* Lemmatization and Stemming
	* Stemming removes suffixes from words to bring them to their base form.
	* Lemmatization uses a vocabulary and a form of morphological analysis to bring the words to their base form.
	* Both reduce the dimensionality of the input feature space.
	* Lemmatization is generally more accurate than stemming but is computationally expensive
	* Lemmatization preserves the semantics of the input text.
		* Algorithms that are meant to work on sentiment analysis, might work well if the tense of words is needed for the model. Something that has happened in the past might have a different sentiment than the same thing happening in the present.
	* Stemming is fast, but less accurate.
		* In instances where you are trying to achieve text classification, where there are thousands of words that need to be put into categories, stemming might work better than lemmatization purely because of the speed.
	* Some deep-learning models have the ability to automatically learn word representations which makes using either of these techniques, moot.



Textrecipes

* **Misc**
	* Every text preprocessing sequence should probably start with `step_tokenize` and end with`step_tf` or `step_tfidf` 
		* Example sequence of steps: step\_tokenize, step\_stopwords, step\_ngrams, step\_tokenfilter, step\_tfidf
	* Models with large numbers (100s) of features increases the opportunity for feature drift (i.e. use step\_tokenfilter)
	* If you have a number of text columns, it'll be easier to create a char var and use that in the recipe

```
text_cols <- c("text_col1, "text_col2", "text_col3")
step_whatever(all_of(text_cols))
```

* Examine Tokens from recipe

```
token_list_tbl <- text_recipe %>% prep() %>% juice()

# look at tokens created from 1st row of text column (e.g. text_col)
token_list_tbl$text_col[1] %>%
    attr("unique_tokens")
```

* Can use this after every text recipe step to examine the results

* **Debugging**
	* "length of 'dimnames' \[2\] not equal to array extent"
		* Example:
			from DRob's [Prediction box office performance](https://www.youtube.com/watch?v=IkTfKnUoYf0)
			* Using the min\_times arg with `step_tokenfilter` and `step_stopwords` with one of the tokenized variables resulted in this error.
* **step\_tokenize**
	* take delimited text and created tokens

```
    textrecipes::step_tokenize(genres, production_countries,
                              token = 'regex',
                              options = list(pattern = ";"))
```

* genres has values like "action;horror;comedy" per row
* created using function that extract values from json columns
	* see script Code >> JSON >> extract-values-from-json-column.R

* **step\_stopwords**
	* Remove words like "the" and "at"

```
    textrecipes::step_stopwords(tokenized_var1, tokenizedvar2)
```

* custom\_stopword\_source: provide a vector of stopwords
* keep: provide a vector of words you don't want filtered out

* **step\_ngram**
	* Creates ngrams (i.e. combines tokens into n number of words to create phrases)

```
step_ngram(
    tokenized_var1,
    num_tokens = 3,
    min_num_tokens = 1
)
```

* example creates uni-gram (aka tokens), bi-grams, and tri-grams (n = 1, 2, and 3)
* Tokens combined (with underscores) sequentially as they occur in the original string
	* e.g. bigram: 1st word combined with 2nd word, then 2nd word combined with 3rd word, etc.

* **step\_tokenfilter**
	* Useful for reducing features
		* This step can cause the number of features to explode into the thousands, so if your sample size is in the 100s, you'll need to reduce the number of features
	* Remove tokens if they have a count below a specified number

```
    textrecipes::step_tokenfilter(genres, production_countries
                                min_times = 50
```

* See above for example of genre variable
* so if "action" doesn't appear in at least 50 rows of the dataset, a token won't be created for it
* if you set arg, percentage=T, then min\_times can be a percentage
* `max_times` also available

* Only keep tokens with a count in the top\_n

```
    textrecipes::step_tokenfilter(genres, production_countries
                                max_tokens = 50
```

* can use with min/max\_times but max\_tokens gets applied _after_ min/max times
* In the example, there are 2 text columns (genres, production countries) and max\_tokens = 50, therefore 50\*2 = 100 columns get created.
* default = 100

* **step\_tf**
	* term frequency columns; creates indicator variables for your tokens

```
    textrecipes::step_tf(genres, production countries)
```

* format: "tf\_variableName\_tokenName"
* example: "tf\_genres\_action"

* **step\_tfidf**![](./_resources/Feature_Engineering,_Tokenization.resources/image.png)
	* Mixture of tf and idf
		* Term frequency (tf) measures how many times each token appears in each observation
			* i.e. number of times a word appears in a document, divided by the total number of words in that document
		* Inverse document frequency (idf) is a measure of how informative a word is, e.g., how common or rare the word is across all the observations.
			* i.e. logarithm of the number of documents in the corpus divided by the number of documents where the specific term appears
				* weighs down the frequent words and scaling up the rare ones
	* The "mixture" seems to be the product of these two values

```
    step_tfidf(var_thats_been_tokenized_and_filtered)
```

* strongly advised to **use**`step_tokenfilter`**before** using step\_tfidf to limit the number of variables created; otherwise you may run into memory issues

* **Other**
	* (not sure what this weight scheme does and how it helps)

```
    step_tokenize(market_category) %>%
    step_tokenfilter(market_category, min_times = 0.05, max_times = 1, percentage = TRUE) %>%
    step_tf(market_category, weight_scheme = "binary")
```
****
****
Tokenization Algorithms

* Word-based
	* Splits a piece of text into words based on a delimiter
	* Advantages:
			meaningful tokens
			
	* Issues:
		* Vocabularies can be very large
			* Assigns different IDs to the words like “boy” and “boys”
			* Causing the model to be heavier and requiring more computational resources
			* Solution: Restrict size of the vocabulary to only most common tokens. Tradeoff is that you're losing information
		* Misspelled words in the corpus get assigned an OOV token
		* All semantic uses for a word are combined into one representation.
			* Example, the word “play” in “I’m going to see a play” and “I want to play” will have the same embedding, without the ability to distinguish context
	* Types: space and punctuation, rule-based
	* NLP models that use this type:
		* **Transformer XL** (vocabulary = 267,735)
		* **Word2Vec** (?)
* Character-based
	* Splits the text into individual characters
	* Language has many different words but has a fixed number of characters. This results in a very small vocabulary.
		* English Language
			* 256 different characters (letters, numbers, special characters)
			* 170,000 words in its vocabulary
	* Advantages:
		* Can create a representation of the unknown words (words not seen during training) using the representation for each character
		* Misspelled words can be spelled correctly rather can marking them as OOV tokens and losing information (?)
		* Fast and requires less compute resources
	* Issues:
		* Less meaningful tokens for some languages:
			* Characters have meaning in some languages (e.g. Chinese) but not others (e.g. English).
		* Tokenized sequence is much longer than the initial raw text (e.g. “knowledge” will have 9 different tokens)
* Subword-based
	* Splits based on rules:
		* Do not split the frequently used words into smaller subwords.
		* Split the rare words into smaller meaningful subwords.
	* Uses a special symbol to indicate which word is the start of the token and which word is the completion of the start of the token.
		* “tokenization” can be split into “token” and “##ization” which indicates that “token” is the start of the word and “##ization” is the completion of the word.
		* Different models use different special symbols (Wordpiece uses ##)
	* Advantages:
		* Meaningful tokens but with a more managable vocabulary size
		* Possible for a model to process a word which it has never seen before as the decomposition can lead to known subwords
	* Types
		* WordPiece used by **BERT** and **DistilBERT**
		* Unigram by **XLNet** and **ALBERT**
		* Bye-Pair Encoding by **GPT-2** and **RoBERTa**



Engineering

* e.g. comments on a social media platform
* Sentiment
	* categorize each comment as positive, negative, or neutral
* number of new comments
* create consumer profiles
	* clustering based on consumer characteristics and use a feature
	* social media characteristics
		* location of comments
			* location tags
		* language spoken
		* biographical data in profile
	* shared links



Embeddings

* Embeddings are numerical representations of words or sentences
* Misc
* Capture analogies well![](./_resources/Feature_Engineering,_Tokenization.resources/Screenshot (1443).png)
	* Horizontally: Puppy is to Dog as Calf is to Cow
		* X-Axis could represent a latent variable like Age
	* Vertically: Pupy is to Calf as Dog is to Cow
		* Y-Axis could represent a latent variable like Size
* Each latent variable is a dimension in the embedding![](./_resources/Feature_Engineering,_Tokenization.resources/Screenshot (1446).png)
* Word and Sentence embedding matrices can have lengths in the 1000s![](./_resources/Feature_Engineering,_Tokenization.resources/Screenshot (1445).png)![](./_resources/Feature_Engineering,_Tokenization.resources/Screenshot (1444).png)
* 
* 



Monitoring Embeddings

* Tracking embeddings drift
* Misc
	* Notes from
		* [Measuring Embedding Drift](https://towardsdatascience.com/measuring-embedding-drift-aa9b7ddb84ae)
* Reasons for drift
	* Change in your model’s architecture: This is a bigger change than before. Changing your model’s architecture can change the dimensionality of the embedding vectors. If the layers become larger/smaller, your vectors will too.
	* Use another extraction method: In the event of the model not changing, you can still try several embedding extraction methods and compare between them.
	* Retraining your model: Once you retrain the model from which you extract the embeddings, the parameters that define it will change. Hence, the values of the components of the vectors will change as well.
	* Vision
		* What are the unique situations, events, people or objects that are observed in production data that are missing from the training set?
			* Example: the training set might include pictures of a single apple but not pictures of multiple apples or other fruit.
	* Text
		* When a word, category or language that does not exist in the training data emerges in production
		* Any changes in terminology in the data or changes to the context or meaning of words or phrases over time can contribute to drift.
			* [Low-resource languages](https://ai.facebook.com/research/no-language-left-behind/) and cultural gaps in speech can also compound these difficulties
		* Example: a sentiment classification model trained on apparel product reviews in English but in production, encounters reviews in Spanish for the first time
			* Or is asked to predict the sentiment of reviews of specialized medical devices.
* Extraction Methods
	* Extract embeddings from the current model in production.
		* Extracting the last fully connected layer before the classification to create an image embedding is advisable.
			* The latent structure in this layer will contain information about structure in the image such as objects and actions, in addition to general quality information relative to images in the training set
		* In the case of a vision transformer (ViT), it is recommended that you extract the embedding that the multilayer perceptron (MLP) is acting on to make an image-level decision
		* Example on how to extract embeddings from a well-known Hugging Face model ([article](https://colab.research.google.com/github/Arize-ai/tutorials_python/blob/main/Arize_Tutorials/Embeddings/CV/Arize_Tutorial_CV_Image_Classification_HuggingFace.ipynb))
	* Using a model to extract the embedding
		* e.g. a foundational model like BERT
		* Advantages
			* No modification is needed on the production model
			* Easy option for testing and running on internal data.
		* Disadvantage: the model is only looking at the data itself and is not looking at the internal model decisions.
* Metrics
	* hyperbox IOU, euclidean distance, cosine distance, and clustering-based group purity scores
	* Euclidean distance
		* Smaller the distance, the more similar the embeddings
		* Recommended: more stable, sensitive and scalable compared to the other methods.
		* Calculate centroid for embedding vectors in the production model (and baseline model)![](./_resources/Feature_Engineering,_Tokenization.resources/image.1.png)
			* Note: vertical dots are missing in the vectors
			* The centroid is calculated by taking the row-wise averages
		* Compare production centroid to baseline centroid![](./_resources/Feature_Engineering,_Tokenization.resources/image.2.png)
	* Cosine Distance (1 - cosine similarity)![](./_resources/Feature_Engineering,_Tokenization.resources/image.3.png)
		* Smaller the distance, the more similar the embeddings
* Use the 2-sample Kolmogorov–Smirnov test (KS)
	* "Multiple samples from the embedding set can be taken calculating the euclidean distance metric for each sample set separately, and the KS test can be used to determine if drift has or hasn’t occurred."
	* See [Distributions](Distributions) >> Tests for more details



Augmentation

* Useful for imbalanced outcomes with text predictors
	* Better performance than subsampling
* words are randomly swapped, deleted, as well as replaced or inserted with synonyms using pretrained word embeddings![](./_resources/Feature_Engineering,_Tokenization.resources/1-P5z8tXzjoLxh3kUDqeO7zQ.png)
* [{{textattack}}]{style='color: goldenrod'}, [article](https://towardsdatascience.com/nlp-project-with-augmentation-attacks-aspect-based-sentiment-analysis-3342510c90e7), [ipynb](https://github.com/misha345a/E-commerce_Reviews_Classifier/blob/main/Create_Balanced_Dataset.ipynb)
* Adversarial Text Attack
	* Diagnostic to test how a model performs on a test set where data augmentation techniques have been applied![](./_resources/Feature_Engineering,_Tokenization.resources/1-odITQb5uQ0QZY0UDgRrp8w.png)
	* [{{textattack}}]{style='color: goldenrod'}, [article](https://towardsdatascience.com/nlp-project-with-augmentation-attacks-aspect-based-sentiment-analysis-3342510c90e7), [ipynb](https://github.com/misha345a/E-commerce_Reviews_Classifier/blob/main/LSTM_Models_And_Attack.ipynb)![](./_resources/Feature_Engineering,_Tokenization.resources/1-ZD9uA9_XR5iIz-TIer6UlQ.png)
	* 














