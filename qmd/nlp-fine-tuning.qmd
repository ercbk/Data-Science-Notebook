# NLP, Fine Tuning

TOC

* Misc



Misc

* Workflow Example ([paper](https://huyenchip.com/2023/04/11/llm-engineering.html))![](./_resources/NLP,_Fine_Tuning.resources/image.png)
* [Raschka - An introduction to the core ideas and approaches to Finetuning Large Language Models - by Sebastian Raschka](https://magazine.sebastianraschka.com/p/finetuning-large-language-models?utm_source=post-email-title&publication_id=1174659&post_id=115943216&isFreemail=true&utm_medium=email)
* With ChatGPT, you can have it answer questions from context that contains thousands of documents.![](./_resources/NLP,_Fine_Tuning.resources/image.1.png)
	* Store all these documents as small chunks of text (allowable size of the context window) in a database.
	* Create embeddings of documents and question
	* The documents of relevance can then be found by computing similarities between the question and the document chunks. This is done typically by converting the chunks and question into word embedding vectors, and computing cosine similarities between chunks and question, and finally choosing only those chunks above a certain cosine similarity as relevant context.
	* Finally, the question and context can be combined into a prompt as below, and fed into an LLM API like ChatGPT: `prompt=f"Answer the question. Context: {context}\n Question: {question}"`






















