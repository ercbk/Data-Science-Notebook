# Quantile {#sec-reg-quant .unnumbered}

## Misc {#sec-reg-quant-misc .unnumbered}

::: {.callout-tip collapse="true"}
## Packages

-   [{]{style="color: #990000"}[quantregRanger](https://cran.r-project.org/web/packages/quantregRanger/quantregRanger.pdf){style="color: #990000"}[}]{style="color: #990000"} - uses **Ranger** to fit quantile RFs
    -   In [{tidymodels}]{style="color: #990000"}, `quantreg = TRUE` tells ranger that you're estimating quantiles rather than averages. Also `predict(airquality, type = 'quantiles')`
-   [{]{style="color: #990000"}[grf](https://grf-labs.github.io/grf/reference/quantile_forest.html){style="color: #990000"}[}]{style="color: #990000"} - generalized random forest
-   [{]{style="color: #990000"}[quantreg](https://cran.r-project.org/web/packages/quantreg/index.html){style="color: #990000"}[}]{style="color: #990000"} - Estimation and inference methods for models for conditional quantile functions: **Linear** and **nonlinear** **parametric** and **non-parametric** (total variation penalized) models for conditional quantiles of a univariate response.
-   [{]{style="color: #990000"}[partykit](https://cran.r-project.org/web/packages/partykit/index.html){style="color: #990000"}[}]{style="color: #990000"} - Conditional inference **trees**; model-based recursive partitioning trees
    -   [{]{style="color: #990000"}[bonsai](https://bonsai.tidymodels.org/){style="color: #990000"}[}]{style="color: #990000"}: tidymodels partykit conditional trees, forests; successor to treesnip - Model Wrappers for Tree-Based Models
-   [{{]{style="color: goldenrod"}[quantile-forest](https://github.com/zillow/quantile-forest){style="color: goldenrod"}[}}]{style="color: goldenrod"} - Zillow's sklearn compatible quantile **forest**. Compared to other python implementations, optimized for training and inference speed, enabling it to scale to millions of samples with a runtime that is orders of magnitude faster than less-optimized solutions. It also allows specifying prediction quantiles after training, permitting a trained model to be reused to estimate conditional quantiles as needed.
    -   Out-of-Bag Scoring: OOB scoring can be used to obtain unbiased estimates of prediction errors and quantile-specific metrics without the need for additional validation datasets.
    -   Quantile Rank Calculation: Provide a measure of relative standing for each data point in the distribution. Allows you to compare and rank observations based on their position within the quantile distribution, providing valuable insights for various applications, such as risk assessment and anomaly detection.
    -   Proximity and Similarity Estimation: Quantifies the similarity between pairs of observations based on their paths through the forest. Useful for clustering, anomaly detection, and identifying influential observations.
-   [{{]{style="color: goldenrod"}[skgarden](https://scikit-garden.github.io/examples/QuantileRegressionForests/){style="color: goldenrod"}[}}]{style="color: goldenrod"} - Extension for sklearn tree and forest models. Produces **online training** models called **Mondrian Forests** ([paper](https://arxiv.org/abs/1406.2673)). Has a quantile random forest flavor.
-   [{]{style="color: #990000"}[qrnn](https://cran.r-project.org/web/packages/qrnn/index.html){style="color: #990000"}[}]{style="color: #990000"}: Quantile Regression **Neural Network**
    -   Fit quantile regression neural network models with optional **left censoring**, partial monotonicity constraints, **generalized additive model** constraints, and the ability to fit multiple **non-crossing quantile functions**.
-   [{]{style="color: #990000"}[qrcm](https://cran.r-project.org/web/packages/qrcm/index.html){style="color: #990000"}[}]{style="color: #990000"} - A parsimonious parametric approach that directly models the linear regression coefficients as smooth functions of q, which succeeds in effectively pooling information across quantile levels. It also estimates different quantile coefficients *simultaneously*.
    -   Note that Quantile RFs *simulaneously* estimate the entire conditional distribution
    -   Benefits of **Simultaneous Estimation**:
        -   Computational Efficiency: Reduces overall computation time compared to fitting each quantile separately.
        -   No Quantile Crossing: Crossing violates the basic principle that higher quantiles should always have higher values than lower quantiles for any given set of predictor variables. This also violates the fundamental properties of cumulative distribution functions, which should be monotonically increasing.
        -   Improved Stability: The joint estimation can lead to more stable estimates, especially in smaller samples or when dealing with extreme quantiles. In regions where data is sparse, borrowing information across quantiles can lead to more robust estimates.
        -   Enhanced inference: Simultaneous estimation allows for easier joint hypothesis testing across multiple quantiles.
-   [{]{style="color: #990000"}[qrcmNP](https://cran.r-project.org/web/packages/qrcmNP/index.html){style="color: #990000"}[}]{style="color: #990000"} - Uses the method in [{qrcm}]{style="color: #990000"} for **nonlinear and penalized parametric** modeling of quantile regression coefficient functions.
-   [{]{style="color: #990000"}[fastkqr](https://cran.r-project.org/web/packages/fastkqr/index.html){style="color: #990000"}[}]{style="color: #990000"} ([paper](https://arxiv.org/abs/2408.05393)) - A **Fast** Algorithm for **Kernel** Quantile Regression
    -   Efficient algorithm to fit and tune kernel quantile regression models based on the majorization-minimization (MM) method.
    -   Fits multiple quantile curves simultaneously without crossing.
-   [{]{style="color: #990000"}[rquest](https://cran.r-project.org/web/packages/rquest/index.html){style="color: #990000"}[}]{style="color: #990000"} - Functions to conduct **hypothesis tests** and derive **confidence intervals** for quantiles, linear combinations of quantiles, ratios of dependent linear combinations and differences and ratios of all of the above for comparisons between independent samples. Additionally, quantile-based measures of inequality are also considered.
-   [{]{style="color: #990000"}[bayesQR](https://cran.r-project.org/web/packages/bayesQR/index.html){style="color: #990000"}[}]{style="color: #990000"} - **Bayesian** quantile regression using the **Asymmetric Laplace** (AL) distribution, both continuous as well as binary dependent variables are supported
    -   CIs have bad coverage for n \< 500 because the standard errors are extremely biased (See [{IJSE}]{style="color: #990000"} paper)
-   [{]{style="color: #990000"}[pqrBayes](https://cran.r-project.org/web//packages//pqrBayes/index.html){style="color: #990000"}[}]{style="color: #990000"} ([Paper](https://arxiv.org/abs/2306.11880)) - **Bayesian Penalized** Quantile Varying Coefficient Regression
    -   Incorporates spike-and-slab prior
    -   The varying part seems to be a spline function
-   [{]{style="color: #990000"}[randomForestSRC](https://www.randomforestsrc.org/){style="color: #990000"}[}]{style="color: #990000"} - **Fast** Unified Random Forests for Survival, Regression, and Classification (RF-SRC) (also Quantile Regression)
-   [{]{style="color: #990000"}[vinereg](https://tnagler.github.io/vinereg/){style="color: #990000"}[}]{style="color: #990000"} - **D-vine copula** based mean and quantile regression.
-   [{]{style="color: #990000"}[expectreg](https://cran.r-project.org/web/packages/expectreg/index.html){style="color: #990000"}[}]{style="color: #990000"} - **Expectile** and quantile regression of models with **nonlinear effects** e.g. **spatial, random, ridge** using least asymmetric weighed squares / absolutes as well as **boosting**; also supplies expectiles for **common distributions**
    -   See [Loss Functions \>\> Expectile Loss](loss-functions.qmd#sec-lossfun-exploss){style="color: green"}
-   [{]{style="color: #990000"}[hdqr](https://cran.r-project.org/web/packages/hdqr/index.html){style="color: #990000"}[}]{style="color: #990000"} - Implements an **efficient** algorithm to fit and tune **penalized** quantile regression models using the generalized coordinate descent algorithm.
    -   Designed to handle **high-dimensional** datasets effectively, with emphasis on precision and computational efficiency.
-   [{]{style="color: #990000"}[erboost](https://cran.r-project.org/web/packages/erboost/index.html){style="color: #990000"}[}]{style="color: #990000"} - **Nonparametric Multiple** **Expectile** Regression via ER-Boost
-   [{]{style="color: #990000"}[QR.break](https://cran.r-project.org/web/packages/QR.break/index.html){style="color: #990000"}[}]{style="color: #990000"} - Methods for **detecting structural breaks**, determining the number of breaks, and estimating break locations in linear quantile regression, using one or multiple quantiles
:::

-   Used to estimate the conditional quantiles of a target variable
    -   [Example]{.ribbon-highlight}: Assume we have a quantile regression model predicting the demand for apples tomorrow. Our model forecasts the 90th quantile as 100, which means that according to the model, there is a 90% probability that the actual demand will be 100 or lower.
-   Quantile regression is a weighted Least Absolute Deviation (LAD) Regression
    -   Quantile: Minimizes a weighted residual sum of absolute deviations
    -   LAD: Minimizes the residual sum of absolute deviations (i.e. MAE)
        -   [{{]{style="color: goldenrod"}[scikit-lego::LADRegression](https://koaning.github.io/scikit-lego/api/linear-model/?h=#sklego.linear_model.LADRegression){style="color: goldenrod"}[}}]{style="color: goldenrod"}
        -   Can optimize for the lowest MAPE (Mean Average Percentage Error), by providing `sample_weight=np.abs(1/y_train)`
-   Also see
    -   [Loss Functions \>\> Quantile Loss](loss-functions.qmd#sec-lossfun-quant){style="color: green"}
-   Resources
    -   Handbook of Quantile Regression - Koenker ([{quantreg}]{style="color: #990000"} book) (see R \>\> Documents \>\> Regression)
    -   Applied Machine Learning Using mlr3 in R, [Ch.13.6](https://mlr3book.mlr-org.com/chapters/chapter13/beyond_regression_and_classification.html#sec-quantile-regression)
-   Papers
    -   [Cross validation for penalized quantile regression with a case-weight adjusted solution path](https://arxiv.org/abs/1902.07770)
        -   Code for efficiently finding influential observations
    -   [Valid standard errors for Bayesian quantile regression with clustered and independent data](https://arxiv.org/abs/2407.09772)
        -   The most commonly used likelihood is the asymmetric Laplace (AL) likelihood
        -   AL-based quantile regression has been shown to produce good finite-sample Bayesian point estimates and to be consistent. However, if the AL distribution does not correspond to the data-generating distribution, credible intervals based on posterior standard deviations can have poor coverage.
        -   Yang, Wang, and He (2016) adjustment to the posterior covariance matrix is sensitive to the choice of scale parameter of the AL likelihood and also leads to CIs with poor coverage for small data.
        -   Most common prior for $\sigma$ is an inverse gamma
            -   Others are a uniform prior, $\mathbb{U}(0,10)$ and a half-t distribution with 3 dof which is used by [{brms}]{style="color: #990000"}
        -   [{]{style="color: #990000"}[IJSE](https://cran.r-project.org/web/packages/IJSE/index.html){style="color: #990000"}[}]{style="color: #990000"} - Infinitesimal jackknife standard errors for [{brms}]{style="color: #990000"} models — clustered and independent data. Only requires the model object and the clustering variable if appropiate.
            -   Applicable for other models besides quantile regression, whenever frequentist standard errors are required or model-based posterior standard deviations are not valid due to model misspecification.
    -   [Bayesian Smoothed Quantile Regression](https://arxiv.org/abs/2508.01738)
        -   Solved issues w/aymmetric Laplace distribution (ALD) likelihood:
            -   The non-differentiability of the check loss precludes gradient-based Markov chain Monte Carlo (MCMC) methods
            -   The posterior mean provides biased quantile estimates.
        -   Replaces the check loss with a kernel-smoothed version, creating a continuously differentiable likelihood. This smoothing has two crucial consequences: it enables efficient Hamiltonian Monte Carlo sampling, and it yields a consistent posterior distribution, thereby resolving the inferential bias of the standard approach.
        -   Achieves up to a 50% reduction in predictive check loss at extreme quantiles compared to ALD-based methods, while improving MCMC efficiency by 20-40% in effective sample size
-   For quantiles \> 0.80, see quantile models in [Extreme Value Theory (EVT)](extreme-value-theory-(evt).qmd#sec-evt){style="color: green"}
    -   Quantile Loss is not effective at predicting tail events
-   [Harrell](http://hbiostat.org/bbr/nonpar.html#sec-nonpar-ecdf): To characterize an entire distribution or in other words, have a "high degree of confidence that no estimated quantile will be off by more than a probability of 0.01, n = 18,400 will achieve this.
    -   For example, with n = 18,400, the sample 0.25 quantile (first quartile) may correspond to population quantiles 0.24-0.26.

    -   To achieve a $\pm$ 0.1 MOE requires n = 180, and to have $\pm$ 0.05 requires n = 730 (see table)

        ``` r
        #>        n   MOE
        #> 1     20 0.294
        #> 2     50 0.188
        #> 3    100 0.134
        #> 4    180 0.100
        #> 5    250 0.085
        #> 6    500 0.060
        #> 7    730 0.050
        #> 8    750 0.049
        #> 9   1000 0.043
        #> 10  2935 0.025
        #> 11  5000 0.019
        #> 12 10000 0.014
        #> 13 18400 0.010
        ```
-   Harrell has a pretty cool text effect to display quantile values in his [{HMisc::describe}]{style="color: #990000"} that uses [{gt}]{style="color: #990000"} under the hood (See [EDA \>\> Packages](eda-general.qmd#sec-eda-gen-pkgs){style="color: green"} \>\> HMisc)\
    ![](./_resources/Regression,_Quantile.resources/Screenshot%20(1434).png){.lightbox width="532"}
    -   Histogram is a sparkline
