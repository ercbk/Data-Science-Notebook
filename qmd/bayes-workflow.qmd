# Workflow {#sec-bayes-wkflw .unnumbered}

## Misc {#sec-bayes-wkflw-misc .unnumbered}

-   Also see [Model Building, Concepts \>\> Misc](model-building-concepts.qmd#sec-modbld-misc){style="color: green"} \>\> Regression Workflow
-   Notes from
    -   [Bayesian Workflow](http://www.stat.columbia.edu/~gelman/research/unpublished/Bayesian_Workflow_article.pdf) (Gelman, Vehtari) (arXiv [link](https://arxiv.org/abs/2011.01808))
-   Resources
    -   Vehtari Video: [On Bayesian Workflow](https://www.youtube.com/watch?v=ppKpwtGy8KQ&ab_channel=Generable) (2022) (Based on the paper, but I haven't watched it, yet)
    -   [Nabiximols treatment efficiency](https://users.aalto.fi/~ave/casestudies/Nabiximols/nabiximols.html): Vehtari's example of applyijng his workflow in the context of comparing continuous and discrete observation models.
    -   [Supporting Bayesian modelling workflows with iterative filtering for multiverse analysis](https://arxiv.org/abs/2404.01688) (Haven't read yet)
-   Current Checklist
    -   Check convergence diagnostics
    -   Do posterior predictive checking
    -   Check residual plots
    -   Model comparison (if prediction)
-   Analysis Checklist ([Thread](https://vis.social/@robertGrant/109743940709993385))
    -   A suitably flexible Bayesian regression adjustment model,
    -   Chosen by cross-validation/LOO,
    -   Including Gaussian processes for the unit-level effects over time (and space/network if relevant),
    -   Imputation of missing data, and
    -   Informative priors for biases in the data collection process.
-   Discrete Parameters
    -   Models with discrete parameters arise in a range of statistical motifs including hidden Markov models, finite mixture models, and generally in the presence of unobserved categorical data
    -   HMC cannot operate on models containing discrete parameters. HMC relies on gradient information to guide its exploration of the parameter space. Discrete parameters don't have well-defined gradients.
        -   Using HMC would require marginalization of the likelihood to remove these discrete dimensions from the sampling problem (i.e. integrating out discrete variables).
            -   This can result in loss of information
            -   Depending on the number of discrete parameters, it can be computationally instensive or intractable.
            -   The direct relationship between discrete parameters and the data is obscured.
            -   Potentially requiring more samples to achieve the same level of accuracy due to slower mixing
            -   Can be complex and error-prone for intricate models
    -   [{]{style="color: #990000"}[nimbleHMC](https://cran.r-project.org/web/packages/nimbleHMC/index.html){style="color: #990000"}[}]{style="color: #990000"} ([JOSS](https://joss.theoj.org/papers/10.21105/joss.06745)) can perform HMC sampling of hierarchical models that also contain discrete parameters. It allows for HMC sampling operating alongside discrete samplers.
        -   A workflow for a problem with discrete parameters should consist of testing combinations of samplers in order to optimize MCMC efficiency
        -   [{nimbleHMC}]{style="color: #990000"} allows you to mix-and-match samplers from a large pool of candidates.
        -   Efficiency is measured by Effective Sample Size (ESS) (See [Diagnostics, Bayes \>\> Convergence \> Metrics \>\> Autocorrelation Metrics](diagnostics-bayes.qmd#sec-diag-bay-conv-autoc){style="color: green"})
        -   [{]{style="color: #990000"}[compareMCMCs](https://cran.r-project.org/web/packages/compareMCMCs/index.html){style="color: #990000"}[}]{style="color: #990000"} ([JOSS](https://joss.theoj.org/papers/10.21105/joss.03844)) - Compares MCMC Efficiency from 'nimble' and/or Other MCMC Engines
            -   Built-in metrics include two methods of estimating effective sample size (ESS), posterior summaries such as mean and common quantiles, efficiency defined as ESS per computation time, rate defined as computation time per ESS, and minimum efficiency per MCMC.
-   Amortized Bayesian Workflow ([Paper](https://arxiv.org/abs/2409.04332))\
    ![](_resources/Bayes-Workflow.resources/amortized-wrkflw-1.png){.lightbox width="732"}
    -   **Amortized Bayesian Inference** uses deep neural networks to learn a direct mapping from observables, $y$, to the corresponding posterior, $p(\theta | y)$
        -   i.e. Approximates posterior distributions for faster parameter estimation.
        -   Popular for simulation-based inference (SBI) but is expanding beyond SBI
        -   Resources
            -   [Awesome Amortized Inference](https://github.com/bayesflow-org/awesome-amortized-inference) - Github repo that lists resources
        -   Packages
            -   [{{]{style="color: goldenrod"}[BayesFlow](https://bayesflow.org/){style="color: goldenrod"}[}}]{style="color: goldenrod"} - A library for amortized Bayesian inference with neural networks.
                -   Multi-backend via Keras 3: Use PyTorch, TensorFlow, or JAX.
                -   Modern nets: Flow matching, diffusion, consistency models, normalizing flows, transformers
                -   Built-in diagnostics and plotting
        -   2-Stage Approach
            -   Training Stage: neural networks learn to distill information from the probabilistic model based on simulated examples of observations and parameters, $(\theta | y) \sim p(\theta) \;p(y|\theta)$
            -   Inference Stage: neural networks approximate the posterior distribution for an unseen data set, $y_\text{obs}$ in near-instant time without repeating the training stage
