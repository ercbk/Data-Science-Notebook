# NLP {#sec-diag-nlp .unnumbered}

## Misc {#sec-diag-nlp-misc .unnumbered}

-   Also see
    -   [Diagnostics, Model Agnostic \>\> DALEX \>\> Instance Level \>\> LIME](diagnostics-model-agnostics.qmd#sec-diag-modagn-dalex-instlev-lime){style="color: green"}
    -   [Feature Engineering, Tokenization](feature-engineering-tokenization.qmd#sec-feat-eng-tok){style="color: green"}
    -   [Feature Engineering, Embeddings](feature-engineering-embeddings.qmd#sec-feat-eng-emb){style="color: green"}
    -   [EDA, Text](eda-text.qmd#sec-eda-text){style="color: green"}
    -   [NLP, General](nlp-general.qmd#sec-nlp-gen){style="color: green"}

## Coherence Measures {#sec-diag-nlp-cohmeas .unnumbered}

-   There goal is to evaluate topics quality with respect to interpretability

-   Misc

    -   Paper uses Twitter data to show lack of performance of these measures ([paper](https://aclanthology.org/2021.naacl-main.300/))

-   [Normalized Pointwise Mutual Information (NPMI)]{.underline}

    $$
    \mbox{NPMI} = \frac{\log(p(x)p(y))}{\log (p(x,y))} - 1
    $$

    -   [Normalized Pointwise Mutual Information in Collocation Extraction](https://www.researchgate.net/publication/267306132_Normalized_Pointwise_Mutual_Information_in_Collocation_Extraction)
    -   Estimates how likely is the co-occurrence of two words x and y than we would expect by chance
    -   Range: -1 to 1
    -   NPMI = 0 means independence between the occurrences of x and y
        -   This makes it sound like a correlation measure
    -   If this is a correlation type of measure, then I'm guessing you want something close to 1 for each topic. As this would imply, all the words are likely to appear together.

## Explore Predictions {#sec-diag-nlp-exppred .unnumbered}

-   Scenario: Feature importance from a text model indicates a word or phrase from a text predictor variable is highly predictive of the outcome variable
    -   Explore which values of the outcome variable are associated with high tfidf values of the value of the text variable
        -   [Example]{.ribbon-highlight}

            -   The token, *course*, for the text variable, *improvements*, scores high in feature importance when predicting the course satisfaction rating
            -   All text variables were tokenized, ngram engineered and the values assigned tfidf scores

            ``` r
            bake(prep(text_recipe), testing(splits)) %>%
                select(tfidf_improvements_course) %>% # tfidf naming format is tfidf_textColumn_token
                bind_cols(
                    testing(splits) %>% select(satisfaction_rating)
                ) %>%
                group_by(satisfaction_rating) %>%
                summarize(mean_tfidf_course = mean(tfidf_improvements_course)) %>%
                ungroup()
            ```

            -   Interpretation
                -   Customers that give a satisfaction rating (outcome var) of 6 use the word, "course," a lot (i.e. higher mean tfidf score)

## Behavioral Tests {#sec-diag-nlp-behtest .unnumbered}

-   Misc
    -   Notes from: [Metrics are not enough --- you need behavioral tests for NLP](https://towardsdatascience.com/metrics-are-not-enough-you-need-behavioral-tests-for-nlp-5e7bb600aa57)
    -   Packages: [{{]{style="color: goldenrod"}[checklist](https://github.com/marcotcr/checklist){style="color: goldenrod"}[}}]{style="color: goldenrod"}
-   Robustness Criteria
    -   [Sex/Ethnicity Bias]{.underline} - does your model discriminate against males/females or a specific nationality?
    -   [Equivalent Words/Synonyms]{.underline} - If a candidate replaces "Good python knowledge" with "good python 3 knowledge" how does your model react?
    -   [Skill Grading]{.underline} - do your model assign a higher score for "very good knowledge" vs. "good knowledge" vs. "basic knowledge". Are adjectives adequately understood? Candidates with "exceptional skill" should not be rated below one with "basic skill".
    -   [Sentence Ordering]{.underline} - If we reverse the order of job experience, is the model prediction consistent?
    -   [Typos]{.underline} - I've seen a lot of models where a typo in a completely unimportant word changed the model prediction completely. We may argue that job applications should not contain typos, but we can all agree that, in general, this is an issue in NLP
    -   [Negations]{.underline} - I know that is difficult. But if your task requires understanding them, do you measure it? (for example, "I have no criminal records" vs. "I have criminal records" or "I finished vs. I did not finish". How about double negations?
