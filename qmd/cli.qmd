# CLI {#sec-cli .unnumbered}

## Misc {#sec-cli-misc .unnumbered}

-   Resources
    -   [Data Science at the Command Line](https://jeroenjanssens.com/dsatcl/)
-   `ctrl-r`shell command history search
    -   [McFly](https://github.com/cantino/mcfly) - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly's suggestions are prioritized in real time with a small neural network
-   Path to a folder that's above root folder:
    -   1 level up: `../desired-folder`
    -   2 levels up: `../../desired-folder`
-   Debian vs. Ubuntu (from ChatGPT)
    -   Stability vs. Freshness:
        -   Debian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.

        -   Ubuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.
    -   Package Management:
        -   Debian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.

        -   Ubuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.
    -   Community and Support:
        -   Debian: Debian has a large and dedicated community, and it's known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu's.

        -   Ubuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.
    -   Variants and Flavors:
        -   Debian: Debian offers different flavors, known as "Debian spins," catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.

        -   Ubuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.
    -   Licensing:
        -   Debian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.

        -   Ubuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.
    -   Performance (Google Search AI)
        -   Debian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.
    -   Hardware (Google Search AI)
        -   Debian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.

## R {#sec-cli-r .unnumbered}

-   Make an R script pipeable (From [link](https://livefreeordichotomize.com/posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#piping-to-r))

    ``` bash
    parallel "echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds"
    ```

    ``` r
    #!/usr/bin/env Rscript
    library(readr)
    library(aws.s3)

    # Read first command line argument
    data_destination <- commandArgs(trailingOnly = TRUE)[1]

    data_cols <- list(SNP_Name = 'c', ...)

    s3saveRDS(
      read_csv(
            file("stdin"), 
            col_names = names(data_cols),
            col_types = data_cols 
        ),
      object = data_destination
    )
    ```

    -   By passing `readr::read_csv` the function, `file("stdin")`, it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.

-   Killing a process

    ``` r
    system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
    ```

-   Starting a process in the background

    ``` r
    # start MLflow server
    sys::exec_background("mlflow server")
    ```

-   Delete an opened file in the same R session

    -   You \*\*MUST\*\* unlink it before any kind of manipulation of object

        -   I think this works because readr loads files lazily by default

    -   [Example]{.ribbon-highlight}:

        ``` r
        wisc_csv_filename <- "COVID-19_Historical_Data_by_County.csv"
        download_location <- file.path(Sys.getenv("USERPROFILE"), "Downloads")
        wisc_file_path <- file.path(download_location, wisc_csv_filename)
        wisc_tests_new <- readr::read_csv(wisc_file_path)
        # key part, must unlink before any kind of code interaction
        # supposedly need recursive = TRUE for Windows, but I didn't need it
        # Throws an error (hence safely) but still works
        safe_unlink <- purrr::safely(unlink)
        safe_unlink(wisc_tests_new)

        # manipulate obj
        wisc_tests_clean <- wisc_tests_new %>%
              janitor::clean_names() %>%
              select(date, geo, county = name, negative, positive) %>%
              filter(geo == "County") %>%
              mutate(date = lubridate::as_date(date)) %>%
              select(-geo)
        # clean-up
        fs::file_delete(wisc_file_path)
        ```

-   Find out which process is locking or using a file

    -   Open Resource Monitor, which can be found
        -   By searching for Resource Monitor or resmon.exe in the start menu, or
        -   As a button on the Performance tab in your Task Manager
    -   Go to the CPU tab
    -   Use the search field in the Associated Handles section
        -   type the name of file in the search field and it'll search automatically
        -   35548

## AWK {#sec-cli-awk .unnumbered}

![](./_resources/CLI.resources/DeLcVfSWAAAw6OZ.jpeg){width="632"}

-   Misc

    -   Resources
        -   [Docs](https://www.gnu.org/software/gawk/manual/gawk.html)
        -   [Awk - A Tutorial and Introduction](https://www.grymoire.com/Unix/Awk.html)

-   Print first few rows of columns 1 and 2

    ``` awk
    awk -F, '{print $1,$2}' adult_t.csv|head
    ```

-   Filter lines where no of hours/ week (13th column) \> 98

    ``` awk
    awk -F, ‘$13 > 98’ adult_t.csv|head
    ```

-   Filter lines with "Doctorate" and print first 3 columns

    ``` awk
    awk '/Doctorate/{print $1, $2, $3}' adult_t.csv
    ```

-   Random sample 8% of the total lines from a .csv (keeps header)

    ``` awk
    'BEGIN {srand()} !/^$/ {if(rand()<=0.08||FNR==1) print > "rand.samp.csv"}' big_fn.csv
    ```

-   Decompresses, chunks, sorts, and writes back to S3 (From [link](https://livefreeordichotomize.com/posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html))

    ``` awk
    # Let S3 use as many threads as it wants
    aws configure set default.s3.max_concurrent_requests 50

    for chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do

            aws s3 cp s3://$batch_loc$chunk_file - |
            pigz -dc |
            parallel --block 100M --pipe  \
            "awk -F '\t' '{print \$1\",...\"$30\">\"chunked/{#}_chr\"\$15\".csv\"}'"

            # Combine all the parallel process chunks to single files
            ls chunked/ |
            cut -d '_' -f 2 |
            sort -u |
            parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'

            # Clean up intermediate data
            rm chunked/*
    done
    ```

    -   Uses [pigz](https://linux.die.net/man/1/pigz) to parallelize decompression

    -   Uses GNU Parallel ([site](https://www.gnu.org/software/parallel/), [docs](https://www.gnu.org/software/parallel/man.html), [tutorial1](https://jeroenjanssens.com/dsatcl/chapter-8-parallel-pipelines#introducing-gnu-parallel), [tutorial2](https://www.gnu.org/software/parallel/parallel_tutorial.html#gnu-parallel-tutorial)) to parallelize chunking (100MB chunks in 1st section)

    -   Chunks data into smaller files and sorts them into directories based on a chromosome column (I think)

    -   Avoids writing to disk

## Bash {#sec-cli-bash .unnumbered}

### Misc {#sec-cli-bash-misc .unnumbered}

-   Notes from
    -   [Bash for Data Scientists, Data Engineers & MLOps Engineers](https://towardsdatascience.com/bash-for-data-scientists-data-engineers-mlops-engineers-a8e389621e2e)
        -   Bunch of other stuff that I didn't take notes on
-   Resources
    -   Bash Scripting [Cheatsheet](https://devhints.io/bash)
    -   Curl [Docs](https://curl.se/docs/manpage.html)
-   `man <command>` displays documentation for command
-   Special Characters\
    ![](./_resources/CLI.resources/image.1.png){width="532"}
    -   "\>" redirects the output from a program to a file.
        -   "\>\>" does the same thing, but it's appending to an existing file instead of overwriting it, if it already exists.

### Commands {#sec-cli-bash-com .unnumbered}

#### Basic Commands {#sec-cli-bash-com-bas .unnumbered}

![](./_resources/CLI.resources/image.png){width="532"}

-   `echo $SHELL` - prints the type of shell you're using
-   `echo $PATH` - prints all stored pathes
-   `export PATH="my_new_path:$PATH"` - store a new path
-   Command Syntax: `command -options arguments`
-   Piping Commands: `cat user_names.txt|sort|uniq`

#### Aliases {#sec-cli-bash-com-ali .unnumbered}

-   Custom commands that you can define in order to avoid typing lengthy commands over and over again

-   [Examples]{.ribbon-highlight}

    ``` bash
    alias ll="ls -lah"
    alias gs="git status"
    alias gp="git push origin master"
    ```

-   Create safeguards for yourself

    ``` bash
    alias mv="mv -i"
    ```

    -   `mv` will automatically use the [i]{.arg-text} flag, so the terminal will warn you if the file you're about to move does already exist under the new directory,
        -   This way you don't accidentally overwrite files that you didn't mean to overwrite.

#### Files/Directories {#sec-cli-bash-com-file .unnumbered}

-   List\
    ![](./_resources/CLI.resources/image.2.png)

    -   List 10 most recently modified files: `ls -lt | head`
    -   List files sorted by file size: `ls -l -S`

-   Create/Delete Directories

    ``` bash
    mkdir <dir_name>
    rmdir <dir_name>
    ```

-   Output to file: `echo “This is an example for redirect” > file1.txt`

-   Append line to file: `echo “This is the second line of the file” >> file1.txt`

-   Create/Delete file(s):

    ``` bash
    # Create files
    touch file1.txt
    touch file1.txt file2.tx

    # Delete files
    rm file1.txt
    rm file1.txt file2.txt
    ```

-   Move files/dir; Rename

    ``` bash
    # Move single file
    mv my_file.txt /tmp
    # Move multiple files
    mv file1 file2 file3 /tmp
    # Move a directory or multiple directories
    mv d1 d2 d3 /tmp
    # Rename the file using move command
    mv my_file1.txt my_file_newname.txt
    ```

    -   File(s) and directories being moved to "tmp" directory

-   Search

    -   Find

        ``` bash
        # syntax find <path> <expression>
        # Find by name
        find . -name “my_file.csv"
        #Wildcard search
        find . -name "*.jpg"
        # Find all the files in a folder
        find /temp
        # Search only files
        find /temp -type f
        # Search only directories
        find /temp -type d
        # Find file modified in last 3 hours
        find . -mmin -180
        # Find files modified in last 2 days
        find . -mtime -2
        # Find files not modified in last 2 days
        find . -mtime +2
        # Find the file by size
        find -type f -size +10M
        ```

-   Locate (faster)

    -   [Docs](https://linuxize.com/post/locate-command-in-linux/)

    -   Install

        `bash sudo apt install mlocate # Debian`

    -   Usage

    ``` bash
    sudo updatedb # update before using
    locate .csv
    ```

-   Split files

    ``` bash
    # default: 1000 lines per file, names of new files: xaa, xab, xac, etc.
    split my_file

    # add a prefix to new file names
    split my_file my_prefix

    # specify split threshold (e.g. 5000) by number of lines
    split --lines=5000 my_file

    # specify split threshold by size (e.g. 10MB)
    split --bytes=10 MB my_file
    ```

-   Permissions

    -   `ls -l` See list of files and the permissions
    -   `-rwxrwxrwx` - sytax of permissions for a folder or directory
        -   "rwx" stand for read, write, and execute rights, respectively
        -   The 3 "rwx" blocks are for (1) user, (2) user group, and (3) everyone else.
            -   In the given example, all 3 of these entities have read, write, as well as execute permissions.
        -   The dash indicates that this is a file. Instead of the dash, you can also see a "d" for directory or "l" for a symbolic link.
    -   `chmod` - edit permissions
        -   [Example]{.ribbon-highlight}: `chmod u+x my_program.py` - makes this file executable for *yourself*
    -   `sudo` - "super user" - using this prefix gives you all the permissions to all the files
        -   `sudo su` - opens a stand alone super user shell

#### Print {#sec-cli-bash-com-print .unnumbered}

-   Print file content

    ``` bash
    cat < my_file.txt
    # or
    cat my_file.txt
    ```

-   Print 1 pg at a time: `less my_file.txt`

-   Print specific number of lines: `head -n<num_lines> <file.csv>`

-   Print file content from bottom to top: `tac my_file.txt`

-   `cat -b log.txt | grep error` : shows all lines in log.txt that contain the string 'error', along with the line number (-b)

#### Logicals and Conditionals {#sec-cli-bash-com-logcond .unnumbered}

-   [Logicals]{.underline}
    -   [;]{.arg-text} : `command1 ; command2`

        -   command 1 and command 2 run independently of each other

    -   [&]{.arg-text} : `command1 & command2`

        -   command 1 runs in the background and command 2 runs in the background

    -   [&&]{.arg-text} : `command1 && command2`

        -   If the first command errors out then the second command is not executed

    -   [\|\|]{.arg-text} : `command1 || command2`

        -   The second commmand is only execute if the first command errors

    -   [Example]{.ribbon-highlight}

        ``` bash
        cd my_dir && pwd || echo “No such directory exist.Check”
        ```

        -   If the my_dir exists, then the current working directory is printed. If the my_dir doesn't exist, then the message "No such directory exists. check" message is printed.
-   [Conditionals]{.underline}
    -   Use `[[ ]]` for conditions in `if` / `while` statements, instead of `[ ]` or `test`.
        -   `[[ ]]` is a bash builtin, and is more powerful than `[ ]` or `test`.
        -   Example: `if [[ -n "${TRACE-}" ]]; then set -o xtrace; fi`

#### String Matching {#sec-cli-bash-com-stri .unnumbered}

-   [Example]{.ribbon-highlight}: Search for "error" and write to file

    ``` bash
    #output to a file again
    cat file1 file2 file3 | grep error | cat > error_file.txt
    #Append to the end
    cat file1 file2 file3 | grep error | cat >> error_file.txt
    ```

    -   Prints lines into grep which searches for "error" in each line. Lines with "error" get written to "error_file.txt"

-   Filter lines

    ``` bash
    grep -i “Doctorate” adult_t.csv |grep -i “Husband”|grep -i “Black”|csvlook
    # -i, --ignore-case-Ignore  case  distinctions,  so that characters that differ only in case match each other.
    ```

    -   Select all the candidates who have doctorates and a husband and race are Black
    -   `csvlook` is pretty printing from csvkit package (see [Big Data \>\> Larger Than Memory](big-data.html#sec-bgdat-lgmem){style="color: green"} \>\> csvkit)

-   Count how many rows fit the criteria

    ``` bash
    grep -i “Doctorate” adult_t.csv | wc -l
    ```

    -   Counts how many rows have "Doctorate"
        -   [-wc]{.arg-text} is "word count"

### Variables {#sec-cli-bash-var .unnumbered}

-   Local Variable:

    -   Declared at the command prompt
    -   Use lower case for name
    -   Available only in the current shell
    -   Not accessible by child processes or programs
    -   All user-defined variables are local variables

-   Environment (global) variables:

    -   Create with `export` command
    -   Use upper case for name
    -   Available to child processes

-   Declare local and environment variables then access via "\$"

    ``` bash
    # local
    ev_car=’Tesla’
    echo 'The ev car I like is' $ev_car

    # environment
    export EV_CAR=’Tesla’
    echo 'The ev car I like is' $EV_CAR
    ```

    -   No spaces in variable assignment

-   Always quote variable accesses with double-quotes.

    -   One place where it's *okay* not to is on the *left-hand-side* of an `[[ ]]` condition. But even there I'd recommend quoting.
    -   When you need the unquoted behaviour, using bash arrays will likely serve you much better.

-   Functions

    -   Use local variables in functions.
    -   Accept multiple ways that users can ask for help and respond in kind.
        -   Check if the first arg is [-h]{.arg-text} or [--help]{.arg-text} or `help` or just `h` or even [-help]{.arg-text}, and in all these cases, print help text and exit.
    -   When printing error messages, please redirect to stderr.
        -   Use `echo 'Something unexpected happened' >&2` for this

### Scripting {#sec-cli-bash-script .unnumbered}

-   Use the .sh (or .bash) extension for your script

-   Use long options, where possible (like [--silent]{.arg-text} instead of [-s]{.arg-text}). These serve to document your commands explicitly.

-   If appropriate, change to the script's directory close to the start of the script.

    -   And it's usually always appropriate.
    -   Use `cd "$(dirname "$0")"`, which works in *most* cases.

-   Use `shellcheck`. Heed its warnings.

-   Shebang line

    -   Contains the absolute path of the bash interpreter
        -   List paths to all shells: `cat/etc/shells`
    -   Use as the first line even if you don't give executable permission to the script file.
    -   Starts with "#!" the states the path of the interpreter
    -   Example: `#!/bin/bash`
        -   Interpreter installed in directory "/bin"
    -   Example: `#!/usr/bin/env bash`

-   Commands that should start your script

    -   Use `set -o errexit`
        -   So that when a command fails, bash exits instead of continuing with the rest of the script.
    -   Use `set -o nounset`
        -   This will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.
        -   When you want to access a variable that may or may not have been set, use `"${VARNAME-}"` instead of `"$VARNAME"`, and you're good.
    -   Use `set -o pipefail`
        -   This will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.
    -   Use `set -o xtrace`, with a check on `$TRACE` env variable.
        -   For copy-paste: `if [[ -n "${TRACE-}" ]]; then set -o xtrace; fi`.
        -   This helps in debugging your scripts, a lot.
        -   People can now enable debug mode, by running your script as `TRACE=1 ./script.sh` instead of `./script.sh` .

-   [Example]{.ribbon-highlight}: Basic Execution a Bash Script

    -   Create a directory bash_script: `mkdir bash_script`

    -   Create a hello_world.sh file: `touch hello_script.sh`

    -   Open hello_script.sh (text editor?)

    -   Add code, save, and close

        ```         
            #!/bin/bash
            echo ‘Hello World’
        ```

    -   Make file executable: `chmod +x hello_world.sh`

    -   Execute file: `./hello_world.sh`

-   Template

    ``` bash
    #!/usr/bin/env bash
    set -o errexit
    set -o nounset
    set -o pipefail
    if [[ -n "${TRACE-}" ]]; then
        set -o xtrace
    fi
    if [[ "$1" =~ ^-*h(elp)?$ ]]; then
        echo 'Usage: ./script.sh arg-one arg-two
    This is an awesome bash script to make your life better.
    '
        exit
    fi
    cd "$(dirname "$0")"
    main() {
        echo do awesome stuff
    }
    main "$@"
    ```

### Job Management {#sec-cli-bash-jobm .unnumbered}

-   Programs/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.
-   While program is running:
    -   [control+c]{.arg-text} - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).
    -   [control+z]{.arg-text} - Will pause the program.
        -   After pausing the program can be continued either by bringing it to the foreground (`fg`), or by sending it to the backgroud (`bg`).
-   Execute script to run in the background: `python run.py &`
-   `jobs` - shows all running jobs and process ids (PIDS)
-   `kill` - sends signals to jobs running in the background
    -   `kill -STOP %1` sends a STOP signal, pausing program 1.
    -   `kill -KILL %1` sends a KILL signal, terminating program 1 permanently.

### tmux ('terminal multiplexer') {#sec-cli-bash-tmux .unnumbered}

-   Enables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.
-   [Installation]{.underline} (if necessary): `sudo apt install tmux`
    -   Typically comes with the linux installation
-   [Sessions]{.underline}
    -   `tmux` - starts an unnamed session
    -   `tmux new -s moose` creates new terminal session with name 'moose'
    -   `tmux ls` - lists all running sessions
    -   `tmux kill-session -t moose` - kills session named "moose"
    -   `exit` - stops and quits the current session
    -   Kill all sessions (various opinions on how to do this)
        -   `tmux kill-session`
        -   `tmux kill-server`
        -   `tmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill`
-   [Attach/Detach]{.underline}
    -   When you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, *close* your computer, and come back to that shell later as if you've never been logged out.
    -   `tmux detach` - detach current session
    -   [control+b]{.arg-text}`then press`d\`: When you have multiple sesssions running, this will allow you to select the session to detach
    -   From inside bash and not inside a session
        -   `tmux a` : attach to latest created session
        -   `tmux a -t moose` : attach to session called 'moose'
-   [Pane Creation and Navigation]{.underline}
    -   [control+b]{.arg-text} then press ["]{.arg-text} (i.e. [shift+']{.arg-text}): add another terminal pane below
    -   [control+b]{.arg-text} then press [%]{.arg-text} (i.e. [shift+5]{.arg-text}) : add another terminal pane to the right
    -   [control+b]{.arg-text} then press [→]{.arg-text} : move to the terminal pane on the right (similar for left, up, down)

### SSH {#sec-cli-bash-ssh .unnumbered}

-   Typically uses a key pair to log into remote machines
    -   Key pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)
    -   "ssh-keygen" is a program for generating such a key pair.
        -   If you run ssh-keygen, it will by default create a public key named "id_rsa.pub" and a private key named "id_rsa", and place both into your "\~/.ssh" directory
        -   You'll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator
            -   `cat .ssh/id_rsa.pub | ssh user@remote 'cat >> ~/.ssh/authorized_keys'`
-   Connect to the remote machine: `ssh remote -i ~/.ssh/id_rsa`
-   Create a config file instead
    -   Location: "\~/.ssh/config"

    -   Contents

        ```         
        Host dev
          HostName remote
          IdentityFile ~/.ssh/id_rsa
        ```
-   Connect using config: `ssh dev`
-   For Windows and using Putty, see
    -   [AWS \>\> EC2 \>\> Connect to/ Terminate Instance](aws.qmd#sec-aws-ec2-conterm){style="color: green"}
    -   Projects Notebook \>\> Article, Nested Cross Validation \>\> Notes \>\> Running EC2 instances checklist

### Vim {#sec-cli-bash-vim .unnumbered}

-   Command-line based text editor
-   Common Usage
    -   Logging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.
    -   When running `git commit`, by default git opens vim for writing a commit message. So at the very least you'll want to know how to write, save, and close a file.
-   2 modes: Navigation Mode; Edit Mode
    -   When Vim is launched you're in Navigation mode
    -   Press [i]{.arg-text} to start edit mode, in which you can make changes to the file.
    -   Press [Esc]{.arg-text} key to leave edit mode and go back to navigation mode.
-   Commands ([Cheatsheet](https://devhints.io/vim))
    -   `x` deletes a character
    -   `dd` deletes an entire row
    -   `b` (back) goes to the previous word
    -   `n` (next) goes to the next word
    -   `:wq` saves your changes and closes the file
    -   `:q!` ignores your changes and closes the file

### Packages {#sec-cli-bash-pkg .unnumbered}

-   Common package managers: apt, Pacman, yum, and portage
-   [APT]{.underline} (Advanced Package Tool)
    -   Install Packages

        ``` bash
        # one pkg
        sudo apt-get install <package_name>
        # multiple
        sudo apt-get install <pkg_name1> <pkg_name2>
        ```

        -   Install but no upgrade: `sudo apt-get install <pkg_name> --no-upgrade`

    -   Search for an installed package: `apt-cache search <pkg_name>`

    -   Update package information prior to "upgrading" the packages

        ``` bash
        sudo apt-get update
        ```

        -   Downloads the package lists from the repositories and "updates" them to get information on the newest versions of packages and their dependencies.

    -   Upgrade

        ``` bash
        # all installed packages
        sudo apt-get upgrade

        # To upgrade only a specific program
        sudo apt-get upgrade <package_name>

        # Upgrades and handles dependencies; delete obsolete, add new
        apt-get dist-upgrade

        # together
        sudo apt-get update && sudo apt-get dist-upgrade
        ```

### Expressions {#sec-cli-bash-expr .unnumbered}

-   Sort data, filter only unique lines, and write to file: `cat adult_t.csv | sort | uniq -c > sorted_list.csv`

## Powershell {#sec-cli-powsh .unnumbered}

-   Comments: `<# comment #>`

-   [Multi-line Commands]{.underline}

    ``` powershell
    ffmpeg -i input.mkv -map 0:v:0 `
           -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `
           -map 0:s -c copy `
           -disposition:a:0 default `
           reordered.mkv
    ```

    -   In bash, it's a backslash (\\), but in Powershell, it's a backtick ( \` )

    -   \*Don't forget that there's a space between the last character and the backtick.\*

    -   In practice, this will look like

        ``` powershell
        ffmpeg -i .input.mkv -map 0:v:0 `
        >> -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `
        >> -map 0:s -c copy `
        >> -disposition:a:0 default `
        >> reordered.mkv
        ```

-   [String Matching]{.underline}

    -   Print line with pattern

        ```         
        Select-String -Path "file*.txt" -Pattern "error"
        file1.txt:3:This is the error line of the file
        file2.txt:3:This is the error line of the file
        file3.txt:3:This is the error line of the file
        ```

        -   Matches the 3rd line of each file

-   [Environment Variables]{.underline}

    -   Set an environment variable

        ``` powershell
        Set-Item -Name PYTHONSTARTUP -Value C:\path\to\pythonstartup.py
        ```

        -   Same expression to modify existing environment variable

    -   Delete environment variable

        ``` powershell
        Remove-Item -Name <variable_name>
        ```

    -   Verify value of an environment variable

        ``` powershell
        $env:<variable_name>
        ```

-   [Ports]{.underline}

    -   Find application using a port.

        ``` powershell
        netstat -aon | findstr ':80'
        netstat -anp | find ":80"
        ```

        -   If port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager \>\> Processess

    -   List all Listening and Established ports

        ``` powershell
        netstat -anob
        ```

    -   Check for processes using a port

        ``` powershell
        Get-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess
        ```

    -   Test connection to local port to see if it's open

        ``` powershell
        Test-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded
        ```

    -   Check firewall settings for an app

        ``` powershell
        netsh advfirewall firewall show rule name="name_of_app"
        ```

## Batch Scripting {#sec-cli-batscri .unnumbered}

-   Misc

    -   Resources
        -   [Windows Batch Scripting](https://en.wikibooks.org/wiki/Windows_Batch_Scripting)

-   [Example]{.ribbon-highlight}: Create variables and execute

    ``` bash
    @echo off

    rem Set the path to the Rscript executable
    set RSCRIPT="C:\Users\user\AppData\Local\Programs\R\R-4.2.3\bin\Rscript.exe"

    rem Set the path to the R script to execute
    set RSCRIPT_FILE="C:\Users\user\my_r_script.R"

    rem Execute the R script
    %RSCRIPT% %RSCRIPT_FILE%

    rem Pause so the user can see the output
    exit
    ```

    -   `@echo off` - This line turns off the echoing of commands in the command prompt window, making the output cleaner.
    -   `rem` - Keyword that denotes a comment in a batch file.
    -   `set RSCRIPT=` - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.
    -   `set RSCRIPT_FILE=` - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.
    -   `%RSCRIPT% %RSCRIPT_FILE%` - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.
    -   `exit` - This command exits the batch file and closes the command prompt window.

-   [Example]{.ribbon-highlight}: Exit if script errors

    ``` bash
    Rscript "C:\Users\ercbk\Documents\R\Projects\Indiana-COVID-19-Tracker\R\collection\build-opentab-dat.R"

    REM if the data building script errors, bat script terminates without running other scripts or commands
    if %errorlevel% neq 0 exit /b %errorlevel%

    cd "C:\Users\ercbk\Documents\R\Projects\Indiana-COVID-19-Tracker"

    git add data/YoY_Seated_Diner_Data.csv
    git commit -m "opentab data update"
    git pull
    git push

    EXIT
    ```

## WSL {#sec-cli-wsl .unnumbered}

-   Resources
    -   [Docs](https://learn.microsoft.com/en-us/windows/wsl/basic-commands)
    -   To update password ([link](https://learn.microsoft.com/en-us/windows/wsl/setup/environment#set-up-your-linux-username-and-password)) using username
-   Load Linux: `wsl -d Ubuntu-22.04` where -d is for --distribution
-   WSL Help: `wsl --help`
-   Exit linux terminal back to command prompt or powershell: `exit` 
