---
title: "scrapsheet-DL"
format: html
---

## Deep Learning with R

### Terms

-   The core building block of neural networks is the **layer**. You can think of a layer as a filter for data: some data goes in, and it comes out in a more useful form. Specifically, layers extract representations out of the data fed into them
    -   [Example]{.ribbon-highlight}: `layer_dense(units = 512, activation = "relu")`

        ``` r
        # psedo code
        output <- relu(dot(W, input) + b)

        # r
        for (i in seq_len(1000)) {
          z <- x + y
          z[z < 0] <- 0
        }
        ```

        -   A dot product (`dot`) between the input tensor and a tensor named $W$
        -   An addition (+) between the resulting matrix and a vector $b$
        -   $W$ and $b$ are weights or trainable parameters of the layer (the kernel and bias attributes, respectively)
        -   A relu operation: `relu(x)` is an element-wise $\max(x, 0)$; relu stands for rectified linear unit
-   **densely connected** (also called **fully connected**) neural layers
-   Run the model on x (a step called the **forward pass**) to obtain predictions
-   a 10-way **softmax** classification layer, which means it will return an array of 10 probability scores (summing to 1)
-   Axes
    -   First Axis is called the **batch axis** or **batch dimension** or **samples axis** or **samples dimensionn**.
    -   Second Axis is the features axis for vector data and time steps (or time index) for time series data.
-   Data stored in multidimensional (aka axes) arrays, also called **tensors**
-   **Broadcasting** consists of the following
    -   Steps:

        1.  Axes (called broadcast axes) are added to the smaller tensor to match the `length(dim(x))` of the larger tensor.
        2.  The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor

    -   [Example]{.ribbon-highlight}: Make y's shape the same as X's, so they can be added

        ``` r
        random_array <- function(dim, min = 0, max = 1)
          array(runif(prod(dim), min, max),
                dim)

        X <- random_array(c(32, 10))
        # vector with 10 elements
        y <- random_array(c(10))
        y

        # shape is now (1, 10)
        dim(y) <- c(1, 10)
        y
        #>   [,1]     [,2]      [,3]      [,4]      [,5]      [,6]      [,7]      [,8]      [,9]     [,10]
        #> [1,] 0.4076222 0.173166 0.4457272 0.7208785 0.2414163 0.9192088 0.3056461 0.2270679 0.6933124 0.1063865

        # repeats 1st row 32 times to get a (32, 10) shape matrix
        Y <- y[rep(1, 32), ]
        str(Y)
        ```

        -   Only a mental model. Broadcasting "happens at the algorithmic level rather than at the memory level."
-   **Tensor Product** (aka dot product)
-   **Tensor Reshaping** means rearranging its rows and columns to match a target shape
-   The derivative of a tensor operation (or tensor function) is called a **gradient**. Gradients are just the generalization of the concept of derivatives to functions that take tensors as inputs.
    -   The gradient of a tensor function represents the curvature of the multidimensional surface described by the function

### Tensors

-   Tensors are a generalization of matrices to an arbitrary number of dimensions (note that in the context of tensors, a *dimension* is often called an **axis**)
-   `base::array` is a tensor
-   In deep learning, you’ll generally manipulate tensors with ranks 0 to 4, although you may go up to 5 if you process video data.
-   Components
    -   Number of axes (rank) - `length(dim(train_images))`
    -   Shape - `dim(train_images)`
        -   This is an integer vector that describes how many dimensions the tensor has along each axis,
    -   Data type - `typeof(train_images)`
        -   R’s built-in data types like double and integer
        -   Other tensor implementations also provide support for types like like float16, float32, float64 (corresponding to R’s double), int32 (R’s integer type), etc.
        -   String
-   Types
    -   Scalars (rank 0 tensors, 0 axis) (an R vector of length 1 is conceptually similar to a scalar)

    -   Vectors (rank 1 tensors, 1 axis)

        ``` r
        x <- as.array(c(12, 3, 6, 14, 7))
        str(x)
        #> num [1:5(1d)] 12 3 6 14 7
        ```

    -   Matrices (rank 2 tensors, 2 axes)

        ``` r
        x <- array(seq(3 * 5), dim = c(3, 5))
        x
        #>  [,1] [,2] [,3] [,4] [,5]
        #> [1,] 1 4 7 10 13
        #> [2,] 2 5 8 11 14
        #> [3,] 3 6 9 12 15
        dim(x)
        #> [1] 3 5
        ```

        -   *first axis* are called the rows, and the entries from the *second axis* are called the columns

    -   Cubes (rank 3, 3 axes or a stack of rank 2 tensors)

        ``` r
        x <- array(seq(2 * 3 * 4), dim = c(2, 3, 4))
        str(x)
        #> int [1:2, 1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...

        length(dim(x))
        #> [1] 3
        ```

    -   A rank 4 tensor is a stack of rank 3 tensors
-   Data Types and Their Ranks
    -   *Vector data*: Rank 2 tensors of shape (samples, features), where each sample is a vector of numerical attributes (“features”)
        -   e.g. A dataset with 3 features (age, income, gender) and a 10,000 observations would have the shape (10000 3)
        -   e.g. A dataset of 500 text documents where each document has been encoded into a vector with 20,000 values. This dataset would have the shape (500, 20000)
    -   *Times-series data* or *sequence data*: Rank 3 tensors of shape (samples, timesteps, features), where each sample is a sequence (of length timesteps) of feature vectors
        -   Each sample is a matrix (rank 2, features by time steps.j). All the samples make up the rank 3 tensor.
        -   e.g. Stock data where the current, lowest, highest stock price is recorded every minute. There are 390 minutes in a trading day and the dataset has 250 days of data. The shape would be (
    -   *Images*: Rank 4 tensors of shape (samples, height, width, channels), where each sample is a 2D grid of pixels, and each pixel is represented by a vector of values (“channels”)
        -   e.g. A batch of 128 grayscale images of size 256 × 256 could thus be stored in a tensor of shape (128, 256, 256, 1). Grayscale has a single color channel.
        -   e.g. A batch of 128 color images could be stored in a tensor of shape (128, 256, 256, 3). Color images have 3 color channels (R,G,B)
    -   *Video*: Rank 5 tensors of shape (samples, frames, height, width, channels), where each sample is a sequence (of length frames) of images
        -   e.g A 60-second, 144 × 256 YouTube video clip sampled at 4 frames per second would have 240 frames. A batch of four such video clips would be stored in a tensor of shape (4, 240, 144, 256, 3)
-   Operations
    -   Selecting specific elements (subsetting) in a tensor is called **tensor slicing**.

        -   Select images 10 through 99

            ``` r
            my_slice <- train_images[10:99, , ]
            dim(my_slice)
            #> [1] 90 28 28
            ```

        -   Allowed to slice along up to two axes at one time

            ``` r
            my_slice <- train_images[, 15:28, 15:28]
            dim(my_slice)
            #> [1] 60000 14 14
            ```

            -   Selects a 14 × 14 pixel area in the bottom-right corner of all images

    -   Reshaping

        ``` r
        x <- array(1:6)
        x
        #> [1] 1 2 3 4 5 6

        array_reshape(x, dim = c(3, 2))
        #>     [,1] [,2]
        #> [1,]   1    2
        #> [2,]   3    4
        #> [3,]   5    6

        array_reshape(x, dim = c(2, 3))
        #>     [,1] [,2] [,3]
        #> [1,]   1    2    3
        #> [2,]   4    5    6

        array(1:6, dim = c(3, 2))
        #>     [,1] [,2]
        #> [1,]   1    4
        #> [2,]   2    5
        #> [3,]   3    6
        ```

        -   When reshaping a vector into an array, the values get placed rowwise, but when creating a vector of the same shape, the values get placed columnwise.

### Examples

-   Example: Basic MNIST Classification

    ``` r
    library(tensorflow)
    library(keras)
    mnist <- dataset_mnist()

    train_images <- array_reshape(train_images, c(60000, 28 * 28))
    train_images <- train_images / 255
    test_images <- array_reshape(test_images, c(10000, 28 * 28))
    test_images <- test_images / 255

    model <- 
      keras_model_sequential(list(
        layer_dense(units = 512, activation = "relu"),
        layer_dense(units = 10, activation = "softmax")
      ))

    compile(
      model,
      optimizer = "rmsprop",
      loss = "sparse_categorical_crossentropy",
      metrics = "accuracy"
    )

    fit(
      model, 
      train_images, 
      train_labels, 
      epochs = 5, 
      batch_size = 128
    )
    #> Epoch 1/5
    #> 60000/60000 [===========================] - 5s - loss: 0.2524 - acc:
    #> ➥ 0.9273
    #> Epoch 2/5
    #> 51328/60000 [=====================>.....] - ETA: 1s - loss: 0.1035 -
    #> ➥ acc: 0.9692

    test_digits <- test_images[1:10, ]
    predictions <- predict(model, test_digits)

    str(predictions)
    #> num [1:10, 1:10] 3.10e-09 3.53e-11 2.55e-07 1.00 8.54e-07 ...

    predictions[1, ]
    #> [1] 3.103298e-09 1.175280e-10 1.060593e-06 4.761311e-05 4.189971e-12
    #> [6] 4.062199e-08 5.244305e-16 9.999473e-01 2.753219e-07 3.826783e-06

    # max probability index for first image
    which.max(predictions[1, ])
    #> [1] 8
    # max probability value for first image
    predictions[1, 8]
    #> [1] 0.9999473
    # corresponding test label of first image
    test_labels[1]
    #> [1] 7

    metrics <- 
      evaluate(model, 
               test_images, 
               test_labels)
    metrics["accuracy"]
    #> accuracy
    #> 0.9795
    ```

    -   Training Data - A stack of 60,000 matrices of 28 × 28 integers. Each such matrix is a grayscale image, with coefficients between 0 and 255 of pixel intensity values
    -   Preprocessing
        -   Scale all values to be in the \[0, 1\] interval
            -   Previously, training image values were in the \[0, 255\] interval.
        -   Coerce array into the shape the model expects, a double array
            -   Previously, training images were stored in a triple array of shape (60000, 28, 28) of type integer and the test images were in a triple array of shape (10000, 28, 28) of type integer
    -   Layers
        -   2 dense layers
        -   Final Layer - 10-way softmax classification layer, which returns an array of 10 probability scores (summing to 1)
    -   Compilation Components:
        -   Optimizer - The mechanism through which the model will update itself based on the training data it sees, so as to improve its performance.
        -   Loss Function - How the model will be able to measure its performance on the training data, and thus how it will be able to steer itself in the right direction.
        -   Metrics - Monitored during training and testing. Here, we care only about accuracy (the fraction of the images that were correctly classified).
    -   Fitting the model
        -   For each training epoch, the time elapsed, loss value, and metric value are shown

        -   The last training epoch's (not shown) accuracy is 0.989 (98.9%)

        -   The batch size is 128 which means it fits 128 samples at a time

            ``` r
            batch_1 <- train_images[1:128, , ]

            batch_2 <- train_images[129:256, , ]

            n <- 3
            batch_n <- train_images[seq(to = 128 * n, length.out = 128), , ]
            ```
    -   Predicting on the test set
        -   10 rows are subsetted for testing
        -   A 2-dim array is returned where each number of index i in that array (predictions\[1, \]) corresponds to the probability that digit image test_digits\[1, \] belongs to class i.
            -   i.e. the first row of predictions corresponds to the probabilities that the first test image belongs each of the labels (0-9 digits)
            -   e.g. there's a 9.999473e-01 (99.9%) chance that the first test image is a "7".
    -   Evaluation
        -   The test set accuracy turns out to be 97.9%—that’s quite a bit lower than the training set accuracy (98.9%) (Overfitted)

-   View an image

    ``` r
    digit <- train_images[5, , ]
    plot(as.raster(abs(255 - digit), max = 255))
    ```
