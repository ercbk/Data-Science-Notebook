---
title: "scrapsheet-DL"
format: html
---

## Deep Learning with R

### Terms

-   The core building block of neural networks is the **layer**. You can think of a layer as a filter for data: some data goes in, and it comes out in a more useful form. Specifically, layers extract representations out of the data fed into them
    -   [Example]{.ribbon-highlight}: `layer_dense(units = 512, activation = "relu")`

        ``` r
        # psedo code
        output <- relu(dot(W, input) + b)

        # r
        for (i in seq_len(1000)) {
          z <- x + y
          z[z < 0] <- 0
        }
        ```

        -   A dot product (`dot`) between the input tensor and a tensor named $W$
        -   An addition (+) between the resulting matrix and a vector $b$
        -   $W$ and $b$ are weights or trainable parameters of the layer (the kernel and bias attributes, respectively)
        -   A relu operation: `relu(x)` is an element-wise $\max(x, 0)$; relu stands for rectified linear unit
-   **densely connected** (also called **fully connected**) neural layers
-   Run the model on x (a step called the **forward pass**) to obtain predictions
-   Each iteration over all the training data is called an **epoch**
-   a 10-way **softmax** classification layer, which means it will return an array of 10 probability scores (summing to 1)
-   Axes
    -   First Axis is called the **batch axis** or **batch dimension** or **samples axis** or **samples dimension**.
    -   Second Axis is the features axis for vector data and time steps (or time index) for time series data.
-   Data stored in multidimensional (aka axes) arrays, also called **tensors**
-   **Broadcasting** consists of the following
    -   Steps:

        1.  Axes (called broadcast axes) are added to the smaller tensor to match the `length(dim(x))` of the larger tensor.
        2.  The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor

    -   [Example]{.ribbon-highlight}: Make y's shape the same as X's, so they can be added

        ``` r
        random_array <- function(dim, min = 0, max = 1)
          array(runif(prod(dim), min, max),
                dim)

        X <- random_array(c(32, 10))
        # vector with 10 elements
        y <- random_array(c(10))
        y

        # shape is now (1, 10)
        dim(y) <- c(1, 10)
        y
        #>   [,1]     [,2]      [,3]      [,4]      [,5]      [,6]      [,7]      [,8]      [,9]     [,10]
        #> [1,] 0.4076222 0.173166 0.4457272 0.7208785 0.2414163 0.9192088 0.3056461 0.2270679 0.6933124 0.1063865

        # repeats 1st row 32 times to get a (32, 10) shape matrix
        Y <- y[rep(1, 32), ]
        str(Y)
        ```

        -   Only a mental model. Broadcasting "happens at the algorithmic level rather than at the memory level."
-   **Tensor Product** (aka dot product)
-   **Tensor Reshaping** means rearranging its rows and columns to match a target shape
-   A **gradient** is the derivative of a tensor operation (or tensor function). Gradients are just the generalization of the concept of derivatives to functions that take tensors as inputs.\
    $$
    \begin{align}
    &w = f(x, y, z) \\
    &\nabla f(x, y, z) = \left[ \frac{\partial w}{\partial x}\: \frac{\partial w}{\partial y}\: \frac{\partial w}{\partial z}\right]
    \end{align}
    $$
    -   Another way of saying that is “If you added 1 to x before plugging it into the function, this is how much w would change, if the function was a straight line”
    -   The gradient of a tensor function represents the curvature of the multidimensional surface described by the function
    -   Tor a specific point, the gradient is a vector that points in the direction of the biggest increase in the function, or equivalently, in the steepest uphill direction
        -   The derivative of the loss curve at the weight value of $W_0$ is `grad(loss_value, W_0)` and this is the *direction of steepest ascent* of the loss function around $W_0$.
    -   The gradient (multiplied by a step size) is subtracted from a point to move it down hill.
        -   You can reduce loss value by moving $W$ in the opposite direction from the gradient. This will put you lower on the loss curve.
-   **Stochastic Gradient Descent (SGD)**
    -   Solving `grad(f(W), W) = 0` for W (i.e. points where derivative is 0) is intractable for real neural networks, where the number of parameters is never less than a few thousand and can often be several tens of millions
    -   Each weight (W) value is a dimension of the loss function space. There could be millions.
    -   Mini-Batch SGD
        1.  Draw a random batch of training samples, x, and corresponding targets, y_true.
        2.  Run the model on x to obtain predictions, y_pred (this is called the **forward pass**).
        3.  Compute the loss of the model on the batch, a measure of the mismatch between y_pred and y_true.
        4.  Compute the gradient of the loss with regard to the model’s parameters (this is called the **backward pass**).
        5.  Move the parameters a little in the opposite direction from the gradient—for example, W = W – (learning_rate \* gradient)—thus reducing the loss on the batch a bit. The learning rate (learning_rate here) would be a scalar factor modulating the “speed” of the gradient descent process.
-   **Optimizers** (aka optimization methods) are variants of SGD which differ in how they execute the weights update (last step)
-   **Momentum**
    -   Addresses two issues with SGD: convergence speed and local minima
    -   It updates the parameter w based not only on the current gradient value but also on the previous parameter update
-   **Backpropagation**
    -   A way to use the derivatives of simple operations (such as addition, relu, or tensor product) to easily compute the gradient of arbitrarily complex combinations of these atomic operations

    -   Backpropagation is the application of the chain rule to a computation graph

    -   Example:

        ``` r
        grad(loss_val, w) = 1 * 1 * 2 = 2
        grad(loss_val, b) = 1 * 1 = 1
        ```

        -   If there are multiple paths linking the two nodes of interest, a and b, `grad(b, a)` obtained by summing the contributions of all the paths

### Tensors

-   Tensors are a generalization of matrices to an arbitrary number of dimensions (note that in the context of tensors, a *dimension* is often called an **axis**)
-   `base::array` is a tensor
-   In deep learning, you’ll generally manipulate tensors with ranks 0 to 4, although you may go up to 5 if you process video data.
-   Weight tensors, which are attributes of the layers, are where the knowledge of the model persists.
-   Components
    -   Number of axes (rank) - `length(dim(train_images))`
    -   Shape - `dim(train_images)`
        -   This is an integer vector that describes how many dimensions the tensor has along each axis,
    -   Data type - `typeof(train_images)`
        -   R’s built-in data types like double and integer
        -   Other tensor implementations also provide support for types like like float16, float32, float64 (corresponding to R’s double), int32 (R’s integer type), etc.
        -   String
-   Types
    -   Scalars (rank 0 tensors, 0 axis) (an R vector of length 1 is conceptually similar to a scalar)

    -   Vectors (rank 1 tensors, 1 axis)

        ``` r
        x <- as.array(c(12, 3, 6, 14, 7))
        str(x)
        #> num [1:5(1d)] 12 3 6 14 7
        ```

    -   Matrices (rank 2 tensors, 2 axes)

        ``` r
        x <- array(seq(3 * 5), dim = c(3, 5))
        x
        #>  [,1] [,2] [,3] [,4] [,5]
        #> [1,] 1 4 7 10 13
        #> [2,] 2 5 8 11 14
        #> [3,] 3 6 9 12 15
        dim(x)
        #> [1] 3 5
        ```

        -   *first axis* are called the rows, and the entries from the *second axis* are called the columns

    -   Cubes (rank 3, 3 axes or a stack of rank 2 tensors)

        ``` r
        x <- array(seq(2 * 3 * 4), dim = c(2, 3, 4))
        str(x)
        #> int [1:2, 1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...

        length(dim(x))
        #> [1] 3
        ```

    -   A rank 4 tensor is a stack of rank 3 tensors
-   Data Types and Their Ranks
    -   *Vector data*: Rank 2 tensors of shape (samples, features), where each sample is a vector of numerical attributes (“features”)
        -   e.g. A dataset with 3 features (age, income, gender) and a 10,000 observations would have the shape (10000 3)
        -   e.g. A dataset of 500 text documents where each document has been encoded into a vector with 20,000 values. This dataset would have the shape (500, 20000)
    -   *Times-series data* or *sequence data*: Rank 3 tensors of shape (samples, timesteps, features), where each sample is a sequence (of length timesteps) of feature vectors
        -   Each sample is a matrix (rank 2, features by time steps.j). All the samples make up the rank 3 tensor.
        -   e.g. Stock data where the current, lowest, highest stock price is recorded every minute. There are 390 minutes in a trading day and the dataset has 250 days of data. The shape would be (
    -   *Images*: Rank 4 tensors of shape (samples, height, width, channels), where each sample is a 2D grid of pixels, and each pixel is represented by a vector of values (“channels”)
        -   e.g. A batch of 128 grayscale images of size 256 × 256 could thus be stored in a tensor of shape (128, 256, 256, 1). Grayscale has a single color channel.
        -   e.g. A batch of 128 color images could be stored in a tensor of shape (128, 256, 256, 3). Color images have 3 color channels (R,G,B)
    -   *Video*: Rank 5 tensors of shape (samples, frames, height, width, channels), where each sample is a sequence (of length frames) of images
        -   e.g A 60-second, 144 × 256 YouTube video clip sampled at 4 frames per second would have 240 frames. A batch of four such video clips would be stored in a tensor of shape (4, 240, 144, 256, 3)
-   Operations
    -   Selecting specific elements (subsetting) in a tensor is called **tensor slicing**.

        -   Select images 10 through 99

            ``` r
            my_slice <- train_images[10:99, , ]
            dim(my_slice)
            #> [1] 90 28 28
            ```

        -   Allowed to slice along up to two axes at one time

            ``` r
            my_slice <- train_images[, 15:28, 15:28]
            dim(my_slice)
            #> [1] 60000 14 14
            ```

            -   Selects a 14 × 14 pixel area in the bottom-right corner of all images

    -   Reshaping

        ``` r
        x <- array(1:6)
        x
        #> [1] 1 2 3 4 5 6

        array_reshape(x, dim = c(3, 2))
        #>     [,1] [,2]
        #> [1,]   1    2
        #> [2,]   3    4
        #> [3,]   5    6

        array_reshape(x, dim = c(2, 3))
        #>     [,1] [,2] [,3]
        #> [1,]   1    2    3
        #> [2,]   4    5    6

        array(1:6, dim = c(3, 2))
        #>     [,1] [,2]
        #> [1,]   1    4
        #> [2,]   2    5
        #> [3,]   3    6
        ```

        -   When reshaping a vector into an array, the values get placed rowwise, but when creating a vector of the same shape, the values get placed columnwise.

### Examples

-   Example: Basic MNIST Classification

    ``` r
    library(tensorflow)
    library(keras)
    mnist <- dataset_mnist()

    train_images <- array_reshape(train_images, c(60000, 28 * 28))
    train_images <- train_images / 255
    test_images <- array_reshape(test_images, c(10000, 28 * 28))
    test_images <- test_images / 255

    model <- 
      keras_model_sequential(list(
        layer_dense(units = 512, activation = "relu"),
        layer_dense(units = 10, activation = "softmax")
      ))

    compile(
      model,
      optimizer = "rmsprop",
      loss = "sparse_categorical_crossentropy",
      metrics = "accuracy"
    )

    fit(
      model, 
      train_images, 
      train_labels, 
      epochs = 5, 
      batch_size = 128
    )
    #> Epoch 1/5
    #> 60000/60000 [===========================] - 5s - loss: 0.2524 - acc:
    #> ➥ 0.9273
    #> Epoch 2/5
    #> 51328/60000 [=====================>.....] - ETA: 1s - loss: 0.1035 -
    #> ➥ acc: 0.9692

    test_digits <- test_images[1:10, ]
    predictions <- predict(model, test_digits)

    str(predictions)
    #> num [1:10, 1:10] 3.10e-09 3.53e-11 2.55e-07 1.00 8.54e-07 ...

    predictions[1, ]
    #> [1] 3.103298e-09 1.175280e-10 1.060593e-06 4.761311e-05 4.189971e-12
    #> [6] 4.062199e-08 5.244305e-16 9.999473e-01 2.753219e-07 3.826783e-06

    # max probability index for first image
    which.max(predictions[1, ])
    #> [1] 8
    # max probability value for first image
    predictions[1, 8]
    #> [1] 0.9999473
    # corresponding test label of first image
    test_labels[1]
    #> [1] 7

    metrics <- 
      evaluate(model, 
               test_images, 
               test_labels)
    metrics["accuracy"]
    #> accuracy
    #> 0.9795
    ```

    -   Training Data - A stack of 60,000 matrices of 28 × 28 integers. Each such matrix is a grayscale image, with coefficients between 0 and 255 of pixel intensity values
    -   Preprocessing
        -   Scale all values to be in the \[0, 1\] interval
            -   Previously, training image values were in the \[0, 255\] interval.
        -   Coerce array into the shape the model expects, a double array
            -   Previously, training images were stored in a triple array of shape (60000, 28, 28) of type integer and the test images were in a triple array of shape (10000, 28, 28) of type integer
    -   Layers
        -   2 dense layers
        -   Final Layer - 10-way softmax classification layer, which returns an array of 10 probability scores (summing to 1)
    -   Compilation Components:
        -   Optimizer - The mechanism through which the model will update itself based on the training data it sees, so as to improve its performance.
        -   Loss Function - How the model will be able to measure its performance on the training data, and thus how it will be able to steer itself in the right direction.
        -   Metrics - Monitored during training and testing. Here, we care only about accuracy (the fraction of the images that were correctly classified).
    -   Fitting the model
        -   For each training epoch, the time elapsed, loss value, and metric value are shown

        -   Process

            -   The model will start to iterate on the training data in mini-batches of 128 samples, five times over (each iteration over all the training data is called an epoch).
            -   For each batch, the model will compute the gradient of the loss with regard to the weights (using the backpropagation algorithm, which derives from the chain rule in calculus) and move the weights in the direction that will reduce the value of the loss for this batch.
            -   After these five epochs, the model will have performed 2,345 gradient updates (469 per epoch), and the loss of the model will be sufficiently low that the model will be capable of classifying handwritten digits with high accuracy.

        -   The last training epoch's (not shown) accuracy is 0.989 (98.9%)

        -   The batch size is 128 which means it fits 128 samples at a time

            ``` r
            batch_1 <- train_images[1:128, , ]

            batch_2 <- train_images[129:256, , ]

            n <- 3
            batch_n <- train_images[seq(to = 128 * n, length.out = 128), , ]
            ```
    -   Predicting on the test set
        -   10 rows are subsetted for testing
        -   A 2-dim array is returned where each number of index i in that array (predictions\[1, \]) corresponds to the probability that digit image test_digits\[1, \] belongs to class i.
            -   i.e. the first row of predictions corresponds to the probabilities that the first test image belongs each of the labels (0-9 digits)
            -   e.g. there's a 9.999473e-01 (99.9%) chance that the first test image is a "7".
    -   Evaluation
        -   The test set accuracy turns out to be 97.9%—that’s quite a bit lower than the training set accuracy (98.9%) (Overfitted)

-   View an image

    ``` r
    digit <- train_images[5, , ]
    plot(as.raster(abs(255 - digit), max = 255))
    ```

### TensorFlow

-   `Variable` is a specific kind of tensor meant to hold mutable state — for instance, the weights of a neural network.
    -   Process

        ``` r
        x <- tf$Variable(0) # <1>
        with(tf$GradientTape() %as% tape, { # <2>
          y <- 2 * x + 3 # <3>
        })
        grad_of_y_wrt_x <- tape$gradient(y, x) # <4>
        ```

        1.  Instantiate a scalar `Variable` with an initial value of 0
        2.  Open a `GradientTape` scope
        3.  Inside the scope, apply some tensor operations to our variable
        4.  Use the tape to retrieve the gradient of the output [y]{.var-text} with respect to our variable [x]{.var-text}

    -   Example

        ``` r
        W <- tf$Variable(random_array(c(2, 2))) # <1>
        b <- tf$Variable(array(0, dim = c(2))) # <2>
        x <- random_array(c(2, 2)) # <3>

        with(tf$GradientTape() %as% tape, {
          y <- tf$matmul(x, W) + b # <4>
        })
        grad_of_y_wrt_W_and_b <- tape$gradient(y, list(W, b))

        str(grad_of_y_wrt_W_and_b) # <5>
        #> List of 2
        #> $ :<tf.Tensor: shape=(2, 2), dtype=float64, numpy=…>
        #> $ :<tf.Tensor: shape=(2), dtype=float64, numpy=array([2., 2.])>
        ```

        1.  Instantiated a 2x2 array (matrix) with random values stored in a mutable `Variable` class
        2.  Instantiated a length 2 vector with 0 values
        3.  Created a 2x2 array (matrix) with random values
        4.  `matmul` performs a dot product
        5.  [grad_of_y_wrt_W_and_b]{.var-text} is a list of two tensors with the same shapes as [W]{.var-text} and [b]{.var-text}, respectively
-   `GradientTape`
    -   It’s a context manager that will “record” the tensor operations that run inside its scope, in the form of a computation graph (sometimes called a “tape”).
    -   This graph can then be used to retrieve the gradient of any output with respect to any variable or set of variables

## TensorFlow Install

-   See
    -   [System and Software Requirements](https://www.tensorflow.org/install/pip#windows-wsl2)
        -   Make sure your hardware and software are supported
    -   [Misc \>\> Update Python \>\> Linux](misc.qmd#sec-misc-updatepy-lin){style="color: green"} to update Python
-   Python
    -   Make sure the pre-installed version of Python (or the version you'd prefer to use) and pip on your Ubuntu version are supported.
        -   Check

            ``` bash
            # See which pip you're using
            which pip
            pip --version

            # See where packages would be installed
            python3.12 -m site --user-site
            ```

        -   Install

            ``` bash
            # Install pip for Python 3.12
            curl -sS https://bootstrap.pypa.io/get-pip.py | python3.12
            ```
-   Process
    -   Set-up the venv and install a few base packages

        ``` bash
        # Create a dedicated ML environment
        python3.12 -m venv tf_env
        source tf_env/bin/activate

        pip install --upgrade pip setuptools wheel
        ```

        -   setuptools - Handles build/distribution functionality
