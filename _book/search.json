[
  {
    "objectID": "qmd/surveys-census-data.html",
    "href": "qmd/surveys-census-data.html",
    "title": "Census Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "title": "Census Data",
    "section": "",
    "text": "Notes from\n\nTidycensus Workshop 2024\n\nResources\n\nAnalyzing Census Data: Methods, Maps, and Models in R by Kyle Walker\n\nFor details on joining census data to other data, see Chapter 7.2 in Analyzing Census Data\nFIPS GEOID\n\npopular variable calculations from variables in ACS\nCensus Geocoder (link)\n\nEnter an address and codes for various geographies are returned\nBatch geocoding available for up to 10K records\n\nCodes for geographies returned in a .csv file\n\n\nTIGERweb (link)\n\nAllows you to get geography codes by searching for an area on a map\nOnce zoomed-in on your desired area, you turn on geography layers to find the geography code for your area.\n\nUS Census Regions",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "title": "Census Data",
    "section": "Geographies",
    "text": "Geographies\n\n\nMisc\n\n{tidycensus} docs on various geographies, function arguments, and which surveys (ACS, Census) they‚Äôre available in.\nACS Geography Boundaries by Year (link)\n\nTypes\n\nLegal/Administrative\n\nCensus gets boundaries from outside party (state, county, city, etc.)\ne.g.¬†election areas, school districts, counties, county subdivisions\n\nStatistical\n\nCensus creates these boundaries\ne.g.¬†regions, census tracts, ZCTAs, block groups, MSAs, urban areas\n\n\nNested Areas\n\n\nCensus Tracts\n\nAreas within a county\nAround 1200 to 8000 people\nSmall towns, rural areas, neighborhoods\n** Census tracts may cross city boundaries **\n\nBlock Groups\n\nAreas within a census tract\nAround 600 to 3000 people\n\nCensus Blocks\n\nAreas within a block group\nNot for ACS, only for the 10-yr census\n\n\nPlaces\n\nMisc\n\nOne place cannot overlap another place\nExpand and contract as population or commercial activity increases or decreases\nMust represent an organized settlement of people living in close proximity.\n\nIncorporated Places\n\ncities, towns, villages\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\nCensus Designated Places (CDPs)\n\nAreas that can‚Äôt become Incorporated Places because of state or city regulations\nConcentrations of population, housing, commericial structures\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\n\nCounty Subdivisions\n\nMinor Civil Divisions (MCDs)\n\nLegally defined by the state or county, stable entity. May have elected government\ne.g.¬†townships, charter townships, or districts\n\nCensus County Divisions (CCDs)\n\nno population requirment\nSubcounty units with stable boundaries and recognizable names\n\n\nZip Code Tabulation Areas (ZCTAs)\n\n\nMisc\n\nRegular zip codes are problematic ‚Äî can cross state lines.\n{crosswalkZCTA} - Contains the US Census Bureau‚Äôs 2020 ZCTA to County Relationship File, as well as convenience functions to translate between States, Counties and ZIP Code Tabulation Areas (ZCTAs)\n\nApproximate USPS Code distribution for housing units\n\nThe most frequently occurring zip code within an census block is assigned to a census block\nThen blocks are aggregated into areas (ZCTAs)\n\nZCTAs do NOT nest within any other geographies\n\nI guess the aggregated ZCTA blocks can overlap block groups\n\n2010 ZCTAs exclude large bodies of water and unpopulated areas",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "title": "Census Data",
    "section": "American Community Survey (ACS)",
    "text": "American Community Survey (ACS)\n\nMisc\n\nDefault MOE is a 90%CI\nFor variables, vars &lt;- load_variables(2022, \"acs5\")\n\nFor the 2022 5-year ACS,\n\n\"acs5\" for the Detailed Tables;\n\"acs5/profile\" for the Data Profile; aggregated statistics for acs5\n\np (suffix): Percentage with appropriate denominator\n\n\"acs5/subject\" for the Subject Tables; and\n\"acs5/cprofile\" for the Comparison Profile\n\nGeographies only shown for 5-year ACS\n\n\nAbout\n\nYearly estimates based on samples of the population over a 5yr period\n\nTherefore a Margin of Error (MoE) is included with the estimates.\n\nAvailable as 1-year estimates (for geographies of population 65,000 and greater) and 5-year estimates (for geographies down to the block group)\nDetailed social, economic, housing, and demographic characteristics. Variables covering e.g.¬†income, education, language, housing characteristics\ncensus.gov/acs\n\nACS Release Schedule (releases)\n\nSeptember - 1-Year Estimates (from previous year‚Äôs collection)\n\nEstimates for areas with populations of &gt;65K\n\nOctober - 1-Year Supplemental Estimates\n\nEstimates for areas with populations between 20K-64999\n\nDecember - 5-Year Estimates\n\nEstimates for areas including census tract and block groups\n\n\nData Collected\n\nPopulation\n\nSocial\n\nAncestry, Citizenship, Citizen Voting Age¬† Population, Disability, Education Attainment, Fertility, Grandparents, Language, Marital Status, Migration, School Enrollment, Veterans\n\nDemographic\n\nAge, Hispanic Origin, Race, Relationship, Sex\n\nEconomic\n\nClass of worker, Commuting, Employment Status, Food Stamps (SNAP), Health Insurance, Hours/Week, Weeks/Year, Income, Industry & Occupation\n\n\nHousing\n\nComputer & Internet Use, Costs (Mortgage, Taxes, Insurance), Heating Fuel, Home Value, Occupancy, Plumbing/Kitchen Facilities, Structure, Tenure (Own/Rent), Utilities, Vehicles, Year Built/Year Movied In\n\n\nExample: Median Household Income for Texas Counties\ntexas_income &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\", # median household income\n  state = \"TX\",\n  year = 2022\n)\n\nDefault MOE is a 90%CI (i.e.¬†estimate \\(\\pm\\) MOE)\n\nExample: Census Tract for Multiple Counties in NY\nnyc_income &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  state = \"NY\",\n  county = c(\"New York\", \"Kings\", \"Queens\",\n             \"Bronx\", \"Richmond\"),\n  year = 2022,\n  geometry = TRUE\n)\nmapview(nyc_income, zcol = \"estimate\")\nExample: Multiple Races Percentages for San Diego County\n\nsan_diego_race_wide &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    Hispanic = \"DP05_0073P\",\n    White = \"DP05_0079P\",\n    Black = \"DP05_0080P\",\n    Asian = \"DP05_0082P\"\n  ),\n  state = \"CA\",\n  county = \"San Diego\",\n  geometry = TRUE,\n  output = \"wide\",\n  year = 2022\n)\nfaceted_choro &lt;- ggplot(san_diego_race, aes(fill = estimate)) + \n  geom_sf(color = NA) + \n  theme_void() + \n  scale_fill_viridis_c(option = \"rocket\") + \n  facet_wrap(~variable) + \n  labs(title = \"Race / ethnicity by Census tract\",\n       subtitle = \"San Diego County, California\",\n       fill = \"ACS estimate (%)\",\n       caption = \"2018-2022 ACS | tidycensus R package\")\n\nAllows for comparison, but for groups with less variation as compared to other groups since scaled according to all groups\n\nYou‚Äôd want to make a separate map for the Black population in order to compare variation between counties.\n\n\nMigrations Flows\n\nExample:\nfulton_inflow &lt;- \n  get_flows(\n    geography = \"county\",\n    state = \"GA\",\n    county = \"Fulton\",\n    geometry = TRUE,\n    year = 2020\n  ) %&gt;%\n  filter(variable == \"MOVEDIN\") %&gt;%\n  na.omit()\n\nfulton_top_origins &lt;- \n  fulton_inflow %&gt;%\n    slice_max(estimate, \n              n = 30) \n\nlibrary(rdeck)\n\nSys.getenv(\"MAPBOX_ACCESS_TOKEN\")\n\nfulton_top_origins$centroid1 &lt;- \n  st_transform(fulton_top_origins$centroid1, 4326)\nfulton_top_origins$centroid2 &lt;- \n  st_transform(fulton_top_origins$centroid2, 4326)\n\nflow_map &lt;- \n  rdeck(\n    map_style = mapbox_light(), \n    initial_view_state = view_state(center = c(-98.422, 38.606), \n                                    zoom = 3, \n                                    pitch = 45)\n  ) %&gt;%\n  add_arc_layer(\n    get_source_position = centroid2,\n    get_target_position = centroid1,\n    data = as_tibble(fulton_top_origins),\n    get_source_color = \"#274f8f\",\n    get_target_color = \"#274f8f\",\n    get_height = 1,\n    get_width = scale_linear(estimate, range = 1:5),\n    great_circle = TRUE\n  )\n\nWidth of lines is scaled to counts",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "title": "Census Data",
    "section": "Dicennial US Census",
    "text": "Dicennial US Census\n\nMisc\n\nA complete count ‚Äî not based on samples like the ACS\nApplies differential privacy to preserve respondent confidentiality\n\nAdds noise to data. Greater effect at lower levels (i.e.¬†block level)\nThe exception is that is no differetial privacy for household-level data.\n\n\n\n\nPL94-171\n\nPopulation data which the government needs for redistricting\nsumfile = ‚Äúpl‚Äù\nState Populations\npop20 &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"P1_001N\",\n    year = 2020\n  )\n\nFor 2020, default is sumfile = ‚Äúpl‚Äù\n\n\n\n\nDHC\n\nAge, Sex, Race, Ethnicity, and Housing Tenure (most popular dataset)\nsumfile = ‚Äúdhc‚Äù\nCounty\ntx_population &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\nCensus Block (analogous to a city block)\nmatagorda_blocks &lt;- \n  get_decennial(\n    geography = \"block\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    county = \"Matagorda\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\n\n\n\nDemographic Profile\n\nPretabulated percentages from dhc\nsumfile = ‚Äúdp‚Äù\n\nTabulations for 118th Congress and Island Areas (i.e.¬†Congressional Districts)\n\nsumfile = ‚Äúcd118‚Äù\n\n\nC suffix variables are counts while P suffix variables are percentages\n\n0.4 is 0.4% not 40%\n\nExample: Same-sex married and partnered in California by County\nca_samesex &lt;- \n  get_decennial(\n    geography = \"county\",\n    state = \"CA\",\n    variables = c(married = \"DP1_0116P\",\n                  partnered = \"DP1_0118P\"),\n    year = 2020,\n    sumfile = \"dp\",\n    output = \"wide\"\n  )\n\n\n\nDetailed DHC-A\n\nDetailed demographic data; Thousands of racial and ethnic groups; Tabulation by sex and age.\nDifferent groups are in different tables, so specific groups can be hard to locate.\nAdaptive design means the demographic group (i.e.¬†variable) will only be available in certain areas. For privacy, data gets supressed when the area has low population.\n\nThere‚Äôs typically considerable sparsity especially when going down census tract\n\nArgs\n\nsumfile = ‚Äúddhca‚Äù\npop_group - Population group code (See get_pop_groups below)\n\n‚Äúall‚Äù for all groups\npop_group_label = TRUE - Adds group labels\n\n\nget_pop_groups(2020, \"ddhca\") - Gets group codes for ethnic groups\n\nFor various groups there could be at least two variables (e..g Somaili, Somali and any combination)\nFor time series analysis, analagous groups to 2020‚Äôs for 2000 is SF2/SF4 and for 2010 is SF2. (SF stands for Summary File)\n\ncheck_ddhca_groups - Checks which variables are available for a specific group\n\nExample: Somali\ncheck_ddhca_groups(\n  geography = \"county\", \n  pop_group = \"1325\", \n  state = \"MN\", \n  county = \"Hennepin\"\n)\n\nExample: Minnesota group populations\nload_variables(2020, \"ddhca\") %&gt;% \n  View()\nmn_population_groups &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"all\", # for all groups\n    pop_group_label = TRUE\n  )\n\nIncludes aggregate categories like European Alone, Other White Alone, etc., so you can‚Äôt just aggregate the value column to get the total population in Minnesota.\n\nSo, in order to calculate ethnic group ratios of the total state or county, etc. population, you need to get those state/county totals from other tables (e.g.¬†PL94-171)\n\n\nUse dot density and not chloropleths to visualize these sparse datasets\n\nExample: Somali populations by census tract in Minneapolis\n\nhennepin_somali &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    county = \"Hennepin\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"1325\", # somali\n    pop_group_label = TRUE,\n    geometry = TRUE\n  )\n\nsomali_dots &lt;- \n  as_dot_density(\n    hennepin_somali,\n    value = \"value\", # column name which is by default, \"value\"\n    values_per_dot = 25\n  )\n\nmapview(somali_dots, \n        cex = 0.01, \n        layer.name = \"Somali population&lt;br&gt;1 dot = 25 people\",\n        col.regions = \"navy\", \n        color = \"navy\")\n\nvalues_per_dot = 25 says make each dot worth 25 units (e.g.¬†people or housing units)\n\n\n\n\n\nTime Series Analysis\n\n{tidycensus} only has 2010 and 2020 censuses\n\nSee https://nhgis.org for older census data\n\nIssue: county names and boundaries change over time (e.g.¬†Alaska redraws a lot)\n\nCensus gives a different GeoID to counties that get renamed even though they‚Äôre the same county.\nNA values showing up after you calculate how the value changes over time is a good indication of this problem. Check for NAs: filter(county_change, is.na(value10))\n\nExample: Join 2010 and 2020 and Calculate Percent Change\ncounty_pop_10 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P001001\", \n    year = 2010,\n    sumfile = \"sf1\"\n  )\n\ncounty_pop_10_clean &lt;- \n  county_pop_10 %&gt;%\n    select(GEOID, value10 = value) \n\ncounty_pop_20 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    year = 2020,\n    sumfile = \"dhc\"\n  ) %&gt;%\n    select(GEOID, NAME, value20 = value)\n\ncounty_joined &lt;- \n  county_pop_20 %&gt;%\n    left_join(county_pop_10_clean, by = \"GEOID\") \n\ncounty_joined\n\ncounty_change &lt;- \n  county_joined %&gt;%\n    mutate( \n      total_change = value20 - value10, \n      percent_change = 100 * (total_change / value10) \n    ) \nExample: Age distribution over time in Michigan\n\n\nCode available in the github repo or R/Workshops/tidycensus-umich-workshop-2024-main/census-2020/bonus-chart.R\nDistribution shape remains pretty much the same, but decreasing for most age cohorts, i.e.¬†people are leaving the state across most age groups.\n\ne.g.¬†The large hump representing the group of people in there mid-40s in 2000 steadily decreases over time.",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "title": "Census Data",
    "section": "tidycensus",
    "text": "tidycensus\n\nGet an API key\n\nRequest a key, then activate the key from the link in your email.(https://api.census.gov/data/key_signup.html)\n\nRequired for hitting the census API over 500 times per day which isn‚Äôt as hard as you‚Äôd think.\n\nSet as an environment variable: census_api_key(\"&lt;api key&gt;\", install = TRUE)\n\nOr add this line to .Renviron file, CENSUS_API_KEY=‚Äò&lt;api key‚Äô\n\n\nSearch Variables\n\nColumns\n\nName - ID of the variable (Use this in the survey functions)\nLabel - Detailed description of the variable\nContext - Subject of the table that the variable is located in.\n\nPrefixes (Variables can have combinations of prefixes)\n\nP: i.e.¬†Person; Data available at the census block and larger\nCT: Data available at the census track and larger\nH: Data available at the Housing Unit level\n\nI think housing unit is an alternatve unit. So instead of the unit being a person, which I assume is the typical unit, it‚Äôs a housing unit (~family).\nNot affected by Differential Privacy (i.e.¬†no noise added; true value)\nExample: Total Deleware housing units at census block level\ndp_households &lt;- \n      get_decennial(\n            geography = \"block\",\n            variables = \"H1_001N\",\n            state = \"DE\",\n            sumfile = \"dhc\",\n            year = 2020\n      )\n\n\nExample: DHC data in census for 2020\n\nvars &lt;- load_variables(2020, \"dhc\")\n\nView(vars)\n\nView table, click filter, and then search for parameters (e.g.¬†Age, Median, etc.) with the Label, Context boxes, and overall search box\n\n\nsummary_var - Argument for supplying an additional variable that you need to calculate some kind of summary statistic\n\nExample: Race Percentage per Congressional District\n\nrace_vars &lt;- c(\n  Hispanic = \"P5_010N\", # all races identified as hispanic\n  White = \"P5_003N\", # white not hispanic\n  Black = \"P5_004N\", # black not hispanic\n  Native = \"P5_005N\", # native american not hispanic\n  Asian = \"P5_006N\", # asian not hispanic\n  HIPI = \"P5_007N\" # hawaiian, islander not hispanic\n)\n\ncd_race &lt;- \n  get_decennial(\n    geography = \"congressional district\",\n    variables = race_vars,\n    summary_var = \"P5_001N\", # total population for county\n    year = 2020,\n    sumfile = \"cd118\"\n)\n\ncd_race_percent &lt;- \n  cd_race %&gt;%\n    mutate(percent = 100 * (value / summary_value)) %&gt;% \n    select(NAME, variable, percent)\n\ngeometry = TRUE- Joins shapefile with data and returns a SF (Simple Features) dataframe for mapping\n\nMisc\n\nYou can create a discrete color palette with the at argument in the mapview function.\n\nExample\n# check min and max of your data to select range of bins\nmin(iowa_over_65, na.rm = TRUE) # 0\nmax(iowa_over_65, na.rm = TRUE) # 38.4\n\nm1 &lt;- \n  mapview(iowa_over_65, \n          zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1),\n          at = c(0, 10, 20, 30, 40))\n\nThis will result in a discrete palette with bins of 0-10, 10-20, etc. Looks like an overlap, so I‚Äôm sure which bin contains the endpoints.\n\n\n\nExample: Over 65 in Iowa by census tract\n\nlibrary(mapviw); library(viridisLite)\n\niowa_over_65 &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"DP1_0024P\",\n    state = \"IA\",\n    geometry = TRUE,\n    sumfile = \"dp\",\n    year = 2020\n  )\nm1 &lt;- \n  mapview(iowa_over_65, zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1))\n\n{mapview} is interactive and great for exploration of data\n\nExport as an HTML file\nhtmlwidgets::saveWidget(m1@map, \"iowa_over_65.html\")\n\nCan embed it elsewhere (html report or website) by adding it as an asset\n\nIn {ggplot}\ntexas_income_sf &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\",\n  state = \"TX\",\n  year = 2022,\n  geometry = TRUE\n)\n\nplot(texas_income_sf['estimate'])\n\nRemove water from geographies\nnyc_income_tiger &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  state = \"NY\",\n  county = c(\"New York\", \"Kings\", \"Queens\",\n             \"Bronx\", \"Richmond\"),\n  year = 2022,\n  cb = FALSE,\n  geometry = TRUE\n)\n\nlibrary(tigris)\nlibrary(sf)\nsf_use_s2(FALSE)\n\nnyc_erase &lt;- erase_water(\n  nyc_income_tiger,\n  area_threshold = 0.5,\n  year = 2022\n)\n\n\nmapview(nyc_erase, zcol = \"estimate\")\nBubble Maps are better for better for counts\n\nsan_diego_race_counts &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    Hispanic = \"DP05_0073\",\n    White = \"DP05_0079\",\n    Black = \"DP05_0080\",\n    Asian = \"DP05_0082\"\n  ),\n  state = \"CA\",\n  county = \"San Diego\",\n  geometry = TRUE,\n  year = 2022\n)\n\nsan_diego_hispanic &lt;- filter(\n  san_diego_race_counts, \n  variable == \"Hispanic\"\n)\n\ncentroids &lt;- st_centroid(san_diego_hispanic)\n\n\ngrad_symbol &lt;- ggplot() + \n  geom_sf(data = san_diego_hispanic, color = \"black\", fill = \"lightgrey\") + \n  geom_sf(data = centroids, aes(size = estimate),\n          alpha = 0.7, color = \"navy\") + \n  theme_void() + \n  labs(title = \"Hispanic population by Census tract\",\n       subtitle = \"2018-2022 ACS, San Diego County, California\",\n       size = \"ACS estimate\") + \n  scale_size_area(max_size = 6) \nDot Density to show heterogeneity and mixing between groups\nsan_diego_race_dots &lt;- as_dot_density(\n  san_diego_race_counts,\n  value = \"estimate\",\n  values_per_dot = 200,\n  group = \"variable\"\n)\n\nsan_diego_race_dots\n\ndot_density_map &lt;- ggplot() + \n  geom_sf(data = san_diego_hispanic, color = \"lightgrey\", fill = \"white\") + \n  geom_sf(data = san_diego_race_dots, aes(color = variable), size = 0.01) + \n  scale_color_brewer(palette = \"Set1\") + \n  guides(color = guide_legend(override.aes = list(size = 3))) + \n  theme_void() + \n  labs(color = \"Race / ethnicity\",\n       caption = \"2018-2022 ACS | 1 dot = approximately 200 people\")",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html",
    "href": "qmd/model-building-concepts.html",
    "title": "Concepts",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html#sec-modbld-misc",
    "href": "qmd/model-building-concepts.html#sec-modbld-misc",
    "title": "Concepts",
    "section": "",
    "text": "Packages\n\n{multiverse} - makes it easy to specify and execute all combinations of reasonable analyses of a dataset\n\n\n\nPaper, Summary of it‚Äôs usage\nLots of vignettes\n\n\nRegression Workflow (Paper)\n\nMake ML model pipelines reusable and reproducible\n\n\nNotes from 7 Tips to Future-Proof Machine Learning Projects\nModularization - Useful for debugging and iteration\n\nDon‚Äôt used declarative programming. Create functions/classes for preprocessing, training, tuning, etc., and keep in separate files. You‚Äôll call these functions in the main script\n\nHelper function\n## file preprocessing.py ##\ndef data_preparation(data):\n    data = data.drop(['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am'], axis=1)\n    numeric_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']\n    data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)\n    return data\nMain script\nfrom preprocessing import data_preparation \ntrain_preprocessed = data_preparation(train_data)\ninference_preprocessed = data_preparation(inference_data)\n\nKeep parameters in a separate config file\n\nConfig file\n## parameters.py ##\nDROP_COLS = ['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am']\nNUM_COLS = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']\nProprocessing script\n## preprocessing.py ##\nfrom parameters import DROP_COLS, NUM_COLS\ndef data_preparation(data):\n    data = data.drop(DROP_COLS, axis=1)\n    data[NUM_COLS] = data[NUM_COLS].fillna(data[NUM_COLS].mean())\n    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)\n    return data\n\n\nVersioning Code, Data, and Models - Useful for investigating drift\n\nSee tools like DVC, MLFlow, Weights and Biases, etc. for model and data versioning\n\nImportant to save data snapshots throughout the project lifecycle, for example: raw data, processed data, train data, validation data, test data and inference data.\n\nGithub and dbt for code versioning\n\nConsistent Structures - Consistency in project structures and naming can reduce human error, improve communication, and just make things easier to find.\n\nNaming examples:\n\n&lt;model-name&gt;-&lt;parameters&gt;-&lt;model-version&gt;\n&lt;model-name&gt;-&lt;data-version&gt;-&lt;use-case&gt;\n\nExample: Reduced project template based on {{cookiecutter}}\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ output      &lt;- The output data from the model. \n‚îÇ   ‚îú‚îÄ‚îÄ processed      &lt;- The final, canonical data sets for modeling.\n‚îÇ   ‚îî‚îÄ‚îÄ raw            &lt;- The original, immutable data dump.\n‚îÇ\n‚îú‚îÄ‚îÄ models             &lt;- Trained and serialized models, model predictions, or model summaries\n‚îÇ\n‚îú‚îÄ‚îÄ notebooks          &lt;- Jupyter notebooks. \n‚îÇ\n‚îú‚îÄ‚îÄ reports            &lt;- Generated analysis as HTML, PDF, LaTeX, etc.\n‚îÇ   ‚îî‚îÄ‚îÄ figures        &lt;- Generated graphics and figures to be used in reporting\n‚îÇ\n‚îú‚îÄ‚îÄ requirements.txt   &lt;- The requirements file for reproducing the analysis environment, e.g.\n‚îÇ                         generated with `pip freeze &gt; requirements.txt`\n‚îÇ\n‚îú‚îÄ‚îÄ code              &lt;- Source code for use in this project.\n    ‚îú‚îÄ‚îÄ __init__.py    &lt;- Makes src a Python module\n    ‚îÇ\n    ‚îú‚îÄ‚îÄ data           &lt;- Scripts to generate and process data\n    ‚îÇ   ‚îú‚îÄ‚îÄ data_preparation.py\n    ‚îÇ   ‚îî‚îÄ‚îÄ data_preprocessing.py\n    ‚îÇ\n    ‚îú‚îÄ‚îÄ models         &lt;- Scripts to train models and then use trained models to make\n    ‚îÇ   ‚îÇ                 predictions\n    ‚îÇ   ‚îú‚îÄ‚îÄ inference_model.py\n    ‚îÇ   ‚îî‚îÄ‚îÄ train_model.py\n    ‚îÇ\n    ‚îî‚îÄ‚îÄ analysis  &lt;- Scripts to create exploratory and results oriented visualizations\n        ‚îî‚îÄ‚îÄ analysis.py\n\n\nModel is performing well on the training set but much worse on the validation/test set\n\n\nAndrew Ng calls the validation set the ‚ÄúDev Set‚Äù üôÑ\nTest: Random sample the training set and use that as your validation set. Score your model on this new validation set\n\n‚ÄúTrain-Dev‚Äù is the sampled validation set\nPossibilities\n\nVariance: The data distribution of the training set is the same as the validation/test sets\n\n\nThe model has been overfit to the training data\n\nData Mismatch: The data distribution of the training set is NOT the same as the validation/test sets\n\n\nUnlucky and the split was bad\n\nSomething maybe is wrong with the splitting function\n\nSplit ratio needs adjusting. Validation set isn‚Äôt getting enough data to be representative.\n\n\n\n\nModel is performing well on the validation/test set but not in the real world\n\nInvestigate the validation/test set and figure out why it‚Äôs not reflecting real world data. Then, apply corrections to the dataset.\n\ne.g.¬†distributions of your validation/tests sets should look like the real world data.\n\nChange the metric\n\nConsider weighting cases that your model is performing extremely poorly on.\n\n\nSplits\n\nHarrell: ‚Äúnot appropriate to split data into training and test sets unless n&gt;20,000 because of the luck (or bad luck) of the split.‚Äù\nIf your dataset is over 1M rows, then having a test set of 200K might be overkill (e.g.¬†ratio of 60/20/20).\n\nMight be better to use a ratio of 98/1/1 for big data projects and 60/20/20 for smaller data projects\n\nlink\n\nShows that simple data splitting does not give valid confidence intervals (even asymptotically) when one refits the model on the whole dataset. Thus, if one wants valid confidence intervals for prediction error, we can only recommend either data splitting without refitting the model (which is viable when one has ample data), or nested CV.",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/cli.html",
    "href": "qmd/cli.html",
    "title": "CLI",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-misc",
    "href": "qmd/cli.html#sec-cli-misc",
    "title": "CLI",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly‚Äôs suggestions are prioritized in real time with a small neural network\n\nPath to a folder that‚Äôs above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder\n\nDebian vs.¬†Ubuntu (from ChatGPT)\n\nStability vs.¬†Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it‚Äôs known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu‚Äôs.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as ‚ÄúDebian spins,‚Äù catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-r",
    "href": "qmd/cli.html#sec-cli-r",
    "title": "CLI",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n¬† ¬† ¬† janitor::clean_names() %&gt;%\n¬† ¬† ¬† select(date, geo, county = name, negative, positive) %&gt;%\n¬† ¬† ¬† filter(geo == \"County\") %&gt;%\n¬† ¬† ¬† mutate(date = lubridate::as_date(date)) %&gt;%\n¬† ¬† ¬† select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it‚Äôll search automatically\n35548",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-awk",
    "href": "qmd/cli.html#sec-cli-awk",
    "title": "CLI",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‚Äò$13 &gt; 98‚Äô adult_t.csv|head\nFilter lines with ‚ÄúDoctorate‚Äù and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-bash",
    "href": "qmd/cli.html#sec-cli-bash",
    "title": "CLI",
    "section": "Bash",
    "text": "Bash\n\nMisc\n\nNotes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn‚Äôt take notes on\n\nBash Scripting on Linux: The Complete Guide - video course\n\nResources\n\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n‚Äú&gt;‚Äù redirects the output from a program to a file.\n\n‚Äú&gt;&gt;‚Äù does the same thing, but it‚Äôs appending to an existing file instead of overwriting it, if it already exists.\n\n\n\n\n\nCommands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you‚Äôre using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you‚Äôre about to move does already exist under the new directory,\n\nThis way you don‚Äôt accidentally overwrite files that you didn‚Äôt mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\n\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\nOutput to file: echo ‚ÄúThis is an example for redirect‚Äù &gt; file1.txt\nAppend line to file: echo ‚ÄúThis is the second line of the file‚Äù &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to ‚Äútmp‚Äù directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name ‚Äúmy_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n‚Äúrwx‚Äù stand for read, write, and execute rights, respectively\nThe 3 ‚Äúrwx‚Äù blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a ‚Äúd‚Äù for directory or ‚Äúl‚Äù for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - ‚Äúsuper user‚Äù - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‚Äòerror‚Äô, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo ‚ÄúNo such directory exist.Check‚Äù\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn‚Äôt exist, then the message ‚ÄúNo such directory exists. check‚Äù message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for ‚Äúerror‚Äù and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for ‚Äúerror‚Äù in each line. Lines with ‚Äúerror‚Äù get written to ‚Äúerror_file.txt‚Äù\n\nFilter lines\ngrep -i ‚ÄúDoctorate‚Äù adult_t.csv |grep -i ‚ÄúHusband‚Äù|grep -i ‚ÄúBlack‚Äù|csvlook\n# -i, --ignore-case-Ignore¬† case¬† distinctions,¬† so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i ‚ÄúDoctorate‚Äù adult_t.csv | wc -l\n\nCounts how many rows have ‚ÄúDoctorate‚Äù\n\n-wc is ‚Äúword count‚Äù\n\n\n\n\n\n\nVariables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via ‚Äú$‚Äù\n# local\nev_car=‚ÄôTesla‚Äô\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=‚ÄôTesla‚Äô\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it‚Äôs okay not to is on the left-hand-side of an [[ ]] condition. But even there I‚Äôd recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or ‚Äìhelp or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this\n\n\n\n\n\nScripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like ‚Äìsilent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script‚Äôs directory close to the start of the script.\n\nAnd it‚Äôs usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck. Heed its warnings.\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don‚Äôt give executable permission to the script file.\nStarts with ‚Äú#!‚Äù the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory ‚Äú/bin‚Äù\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you‚Äôre good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n¬† ¬† #!/bin/bash\n¬† ¬† echo ‚ÄòHello World‚Äô\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n¬† ¬† set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n¬† ¬† echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n¬† ¬† exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n¬† ¬† echo do awesome stuff\n}\nmain \"$@\"\n\n\n\nJob Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.\n\n\n\n\ntmux (‚Äòterminal multiplexer‚Äô)\n\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‚Äòmoose‚Äô\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named ‚Äúmoose‚Äù\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you‚Äôve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‚Äòmoose‚Äô\n\n\nPane Creation and Navigation\n\ncontrol+b then press ‚Äù (i.e.¬†shift+‚Äô): add another terminal pane below\ncontrol+b then press % (i.e.¬†shift+5) : add another terminal pane to the right\ncontrol+b then press ‚Üí : move to the terminal pane on the right (similar for left, up, down)\n\n\n\n\nSSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n‚Äússh-keygen‚Äù is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named ‚Äúid_rsa.pub‚Äù and a private key named ‚Äúid_rsa‚Äù, and place both into your ‚Äú~/.ssh‚Äù directory\nYou‚Äôll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: ‚Äú~/.ssh/config‚Äù\nContents\nHost dev\n¬† HostName remote\n¬† IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist\n\n\n\n\nVim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you‚Äôll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you‚Äôre in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file\n\n\n\n\nPackages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to ‚Äúupgrading‚Äù the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and ‚Äúupdates‚Äù them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade\n\n\n\n\nExpressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-powsh",
    "href": "qmd/cli.html#sec-cli-powsh",
    "title": "CLI",
    "section": "Powershell",
    "text": "Powershell\n\nMisc\n\nDocs, Sample Scripts\nUse Get-Help &lt;cmdlet-name&gt;¬†to access help information for specific cmdlets.\n\n$_ (Docs)\n\nAutomatic Variable; Alias for $PSItem\n\nDocs for automatic variables\n\nUseful for looping through objects in a directory. See ‚ÄúChange Name (or Extensions) of Multiple Files‚Äù for an example of usage\nGet the properties of an object\n\nExample: Using Get-ItemProperty (Docs)\nGet-ItemProperty scrapsheet.txt | Format-List\n\nProperty names will be on the left side of the output\n\nExample: Using Get-Member (Docs)\n Get-ChildItem *.txt | Select-Object -First 1 | Get-Member\n\nSelects the first text file in the directory and gets the Properties and a bunch of other stuff like methods which also can be used with $_\nDescriptions are awful for a lot of the stuff, but for the most part, you can guess what the property is.\n\n\nSome properties that can be used with $_\n\nFile System Objects:\n\n.Name: Returns the name component of a file\n.FullName: Returns the full path of the file.\n.Length: Returns the size of the file in bytes.\n.CreationTime: Returns the date and time the file was created.\n.LastWriteTime: Returns the date and time the file was last modified.\n.IsReadOnly: Returns $true if the file is read-only, $false otherwise.\n\nProcess Objects:\n\n.Id: Returns the unique identifier (PID) of the process.\n.Name: Returns the name of the process.\n.MainWindowTitle: Returns the title of the process‚Äôs main window (if applicable).\n.WorkingDirectory: Returns the working directory of the process.\n.CPU: Returns the CPU usage of the process.\n\nRegistry Objects:\n\n.Name: Returns the name of the registry key.\n.Hive: Returns the hive of the registry key (e.g., HKLM, HKCU).\n.Value: Returns the value of the registry key (if applicable).\n.Path: Returns the full path of the registry key.\n\n\n\nComments: &lt;# comment #&gt;\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you‚Äôre already in the directory that you want the folder in. You can also use a path, e.g.¬†\"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nChange Name (or Extensions) of Multiple Files\nGet-ChildItem *.md | Rename-Item -NewName { $_.Name -replace '.md','.qmd' }\n\nGet-ChildItem: cmdlet gets all the files in the current folder that have a .mc file extension\nResults piped to Rename-Item\n\nNewName: Has a value that‚Äôs a script block that runs before the value is submitted to the NewName parameter.\n\n$_: (automatic variable) It represents each file object as it comes to the command through the pipeline.\n-replace: Operator replaces the file extension of each file with .qmd.\n\nNotice that matching using the -replace operator is not case sensitive.\n\n\n\n\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it‚Äôs a backslash (\\), but in Powershell, it‚Äôs a backtick ( ` )\n*Don‚Äôt forget that there‚Äôs a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it‚Äôs open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-batscri",
    "href": "qmd/cli.html#sec-cli-batscri",
    "title": "CLI",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g.¬†timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-wsl",
    "href": "qmd/cli.html#sec-cli-wsl",
    "title": "CLI",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for ‚Äìdistribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/json.html",
    "href": "qmd/json.html",
    "title": "JSON",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-misc",
    "href": "qmd/json.html#sec-json-misc",
    "title": "JSON",
    "section": "",
    "text": "Packages\n\n{yyjsonr} - A fast JSON parser/serializer, which converts R data to/from JSON and NDJSON. It is around 2x to 10x faster than jsonlite at both reading and writing JSON.\n{RcppSimdJson} - Comparable to {yyjsonr} in performance.\n\nAlso see\n\nBig Data &gt;&gt; Larger than Memory\nSQL &gt;&gt; Processing Expressions &gt;&gt; Nested Data\nDatabases &gt;&gt; DuckDB &gt;&gt; Misc\n\nhrbmstr recommends trying duckdb before using the cli tools in ‚ÄúBig Data‚Äù\n\n\nTools\n\n{listviewer}: Allows you to interactively explore and edit json files through the Viewer in the IDE. Docs show how it can be embedded into a Shiny app as well.\n\nExample\nlibrary(listviewer)\nmoose &lt;- jsonlite::read_json(\"path/to/file.json\")\njsonedit(moose)\nreactjson(moose)\n\nI‚Äôve also used this a .config file which looked like a json file when I opened in a text editor, so this seems to work on anything json-like.\nreactjson has a copy button which is nice so that you can paste your edited version into a file.\njsonedit seems like it has more features, but I didn‚Äôt see a copy button. But there‚Äôs a view in which you can manually select everything a copy it via keyboard shortcut.",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-jsonlite",
    "href": "qmd/json.html#sec-json-jsonlite",
    "title": "JSON",
    "section": "{jsonlite}",
    "text": "{jsonlite}\n\nRead",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-py",
    "href": "qmd/json.html#sec-json-py",
    "title": "JSON",
    "section": "Python",
    "text": "Python\n\nExample: Parse Nested JSON into a dataframe (article)\n\nRaw JSON\n\n\n‚Äúentry‚Äù has the data we want\n‚Äú‚Ä¶‚Äù at the end indicates there are multiple objectss inside the element, ‚Äúentry‚Äù\n\nProbably other root elements other than ‚Äúfeed‚Äù as well\n\n\nRead a json file from a URL using {{requests}} and convert to list\n\nimport requests\n\nurl = \"https://itunes.apple.com/gb/rss/customerreviews/id=1500780518/sortBy=mostRecent/json\"\n\nr = requests.get(url)\n\ndata = r.json()\nentries = data[\"feed\"][\"entry\"]\n\nIt looks like the list conversion also ordered the elements alphabetically\nThe output list is subsetted by the root element ‚Äúfeed‚Äù and the child element ‚Äúentry‚Äù\n\nGet a feel for the final structure you want by hardcoding elements into a df\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    parsed_data[\"author_uri\"].append(entry[\"author\"][\"uri\"][\"label\"])\n    parsed_data[\"author_name\"].append(entry[\"author\"][\"name\"][\"label\"])\n    parsed_data[\"author_label\"].append(entry[\"author\"][\"label\"])\n    parsed_data[\"content_label\"].append(entry[\"content\"][\"label\"])\n    parsed_data[\"content_attributes_type\"].append(entry[\"content\"][\"attributes\"][\"type\"])\n    ... \nGeneralize extracting the properties of each object in ‚Äúentry‚Äù with a nested loop\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    for key, val in entry.items():\n        for subkey, subval in val.items():\n            if not isinstance(subval, dict):\n                parsed_data[f\"{key}_{subkey}\"].append(subval)\n            else:\n                for att_key, att_val in subval.items():\n                    parsed_data[f\"{key}_{subkey}_{att_key}\"].append(att_val)\n\ndefaultdict creates a key from a list element (e.g.¬†‚Äúauthor‚Äù) and groups the properties into a list of values where the value may also be a dict.\n\nSee Python, General &gt;&gt; Types &gt;&gt; Dictionaries\n\nFor each item in ‚Äúentry‚Äù, it looks at the first key-value pair knowing that value is always a dictionary (object in JSON)\nThen handles two different cases\n\nFirst Case: The value dictionary is flat and does not contain another dictionary, only key-value pairs.\n\nCombine the outer key with the inner key to a column name and take the value as column value for each pair.\n\nSecond Case: Dictionary contains a key-value pair where the value is again a dictionary.\n\nAssumes at most two levels of nested dictionaries\nIterates over the key-value pairs of the inner dictionary and again combines the outer key and the most inner key to a column name and take the inner value as column value.\n\n\n\nRecursive function that handles json elements with deeper structures\n\ndef recursive_parser(entry: dict, data_dict: dict, col_name: str = \"\") -&gt; dict:\n    \"\"\"Recursive parser for a list of nested JSON objects\n\n    Args:\n        entry (dict): A dictionary representing a single entry (row) of the final data frame.\n        data_dict (dict): Accumulator holding the current parsed data.\n        col_name (str): Accumulator holding the current column name. Defaults to empty string.\n    \"\"\"\n    for key, val in entry.items():\n        extended_col_name = f\"{col_name}_{key}\" if col_name else key\n        if isinstance(val, dict):\n            recursive_parser(entry[key], data_dict, extended_col_name)\n        else:\n            data_dict[extended_col_name].append(val)\n\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    recursive_parser(entry, parsed_data, \"\")\n\ndf = pd.DataFrame(parsed_data)\n\nNotice the check for a deeper structure with isinstance. If there is one, then the function is called again.\nFunction outputs a dict which is coerced into dataframe\nTo get rid of ‚Äúlabel‚Äù in column names: df.columns = [col if not \"label\" in col else \"_\".join(col.split(\"_\")[:-1]) for col in df.columns]\nobject types can be cast into more efficient types: df[\"im:rating\"] = df[\"im:rating\"].astype(int)",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html",
    "href": "qmd/cli-linux.html",
    "title": "Linux",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-misc",
    "href": "qmd/cli-linux.html#sec-cli-lin-misc",
    "title": "Linux",
    "section": "",
    "text": "Notes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn‚Äôt take notes on\n\nBash Scripting on Linux: The Complete Guide - video course\n\nResources\n\nDoc - All on one page so you can just ctrl + f\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n‚Äú&gt;‚Äù redirects the output from a program to a file.\n\n‚Äú&gt;&gt;‚Äù does the same thing, but it‚Äôs appending to an existing file instead of overwriting it, if it already exists.\n\n\nDebian vs.¬†Ubuntu (from ChatGPT)\n\nStability vs.¬†Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it‚Äôs known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu‚Äôs.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as ‚ÄúDebian spins,‚Äù catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-com",
    "href": "qmd/cli-linux.html#sec-cli-lin-com",
    "title": "Linux",
    "section": "Commands",
    "text": "Commands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you‚Äôre using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you‚Äôre about to move does already exist under the new directory,\n\nThis way you don‚Äôt accidentally overwrite files that you didn‚Äôt mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\nList multiple directories: ls ./docs ./text ./data\n\nLook at first 3 rows: head -n3 students.csv\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\n\n-v means ‚Äúverbose‚Äù so it tells us if it was successful\n\nOutput to file: echo ‚ÄúThis is an example for redirect‚Äù &gt; file1.txt\nAppend line to file: echo ‚ÄúThis is the second line of the file‚Äù &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# move only .csv files to data directory and be verbose\nmv -v *.csv ./data/\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to ‚Äútmp‚Äù directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name ‚Äúmy_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nUnzip: unzip ./foia.zip\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n‚Äúrwx‚Äù stand for read, write, and execute rights, respectively\nThe 3 ‚Äúrwx‚Äù blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a ‚Äúd‚Äù for directory or ‚Äúl‚Äù for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - ‚Äúsuper user‚Äù - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‚Äòerror‚Äô, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo ‚ÄúNo such directory exist.Check‚Äù\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn‚Äôt exist, then the message ‚ÄúNo such directory exists. check‚Äù message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for ‚Äúerror‚Äù and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for ‚Äúerror‚Äù in each line. Lines with ‚Äúerror‚Äù get written to ‚Äúerror_file.txt‚Äù\n\nFilter lines\ngrep -i ‚ÄúDoctorate‚Äù adult_t.csv |grep -i ‚ÄúHusband‚Äù|grep -i ‚ÄúBlack‚Äù|csvlook\n# -i, --ignore-case-Ignore¬† case¬† distinctions,¬† so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i ‚ÄúDoctorate‚Äù adult_t.csv | wc -l\n\nCounts how many rows have ‚ÄúDoctorate‚Äù\n\n-wc is ‚Äúword count‚Äù",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-var",
    "href": "qmd/cli-linux.html#sec-cli-lin-var",
    "title": "Linux",
    "section": "Variables",
    "text": "Variables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via ‚Äú$‚Äù\n# local\nev_car=‚ÄôTesla‚Äô\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=‚ÄôTesla‚Äô\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it‚Äôs okay not to is on the left-hand-side of an [[ ]] condition. But even there I‚Äôd recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or ‚Äìhelp or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-script",
    "href": "qmd/cli-linux.html#sec-cli-lin-script",
    "title": "Linux",
    "section": "Scripting",
    "text": "Scripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like ‚Äìsilent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script‚Äôs directory close to the start of the script.\n\nAnd it‚Äôs usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck ‚Äî analysis too for shell scripts. Heed its warnings. (link)\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don‚Äôt give executable permission to the script file.\nStarts with ‚Äú#!‚Äù the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory ‚Äú/bin‚Äù\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you‚Äôre good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n#!/bin/bash\necho ‚ÄòHello World‚Äô\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nSetting and Executing Scripts with Arguments\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n¬† ¬† set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n¬† ¬† echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n¬† ¬† exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n¬† ¬† echo do awesome stuff\n}\nmain \"$@\"",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-jobm",
    "href": "qmd/cli-linux.html#sec-cli-lin-jobm",
    "title": "Linux",
    "section": "Job Management",
    "text": "Job Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-tmux",
    "href": "qmd/cli-linux.html#sec-cli-lin-tmux",
    "title": "Linux",
    "section": "tmux",
    "text": "tmux\n\nTerminal Multiplexer\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‚Äòmoose‚Äô\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named ‚Äúmoose‚Äù\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you‚Äôve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‚Äòmoose‚Äô\n\n\nPane Creation and Navigation\n\ncontrol+b then press ‚Äù (i.e.¬†shift+‚Äô): add another terminal pane below\ncontrol+b then press % (i.e.¬†shift+5) : add another terminal pane to the right\ncontrol+b then press ‚Üí : move to the terminal pane on the right (similar for left, up, down)",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-ssh",
    "href": "qmd/cli-linux.html#sec-cli-lin-ssh",
    "title": "Linux",
    "section": "SSH",
    "text": "SSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n‚Äússh-keygen‚Äù is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named ‚Äúid_rsa.pub‚Äù and a private key named ‚Äúid_rsa‚Äù, and place both into your ‚Äú~/.ssh‚Äù directory\nYou‚Äôll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: ‚Äú~/.ssh/config‚Äù\nContents\nHost dev\n¬† HostName remote\n¬† IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-vim",
    "href": "qmd/cli-linux.html#sec-cli-lin-vim",
    "title": "Linux",
    "section": "Vim",
    "text": "Vim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you‚Äôll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you‚Äôre in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-pkg",
    "href": "qmd/cli-linux.html#sec-cli-lin-pkg",
    "title": "Linux",
    "section": "Packages",
    "text": "Packages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to ‚Äúupgrading‚Äù the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and ‚Äúupdates‚Äù them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-expr",
    "href": "qmd/cli-linux.html#sec-cli-lin-expr",
    "title": "Linux",
    "section": "Expressions",
    "text": "Expressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-general.html",
    "href": "qmd/cli-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-misc",
    "href": "qmd/cli-general.html#sec-cli-gen-misc",
    "title": "General",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly‚Äôs suggestions are prioritized in real time with a small neural network\n\nPath to a folder that‚Äôs above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-r",
    "href": "qmd/cli-general.html#sec-cli-gen-r",
    "title": "General",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n¬† ¬† ¬† janitor::clean_names() %&gt;%\n¬† ¬† ¬† select(date, geo, county = name, negative, positive) %&gt;%\n¬† ¬† ¬† filter(geo == \"County\") %&gt;%\n¬† ¬† ¬† mutate(date = lubridate::as_date(date)) %&gt;%\n¬† ¬† ¬† select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it‚Äôll search automatically\n35548",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-awk",
    "href": "qmd/cli-general.html#sec-cli-gen-awk",
    "title": "General",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‚Äò$13 &gt; 98‚Äô adult_t.csv|head\nFilter lines with ‚ÄúDoctorate‚Äù and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html",
    "href": "qmd/cli-windows.html",
    "title": "Windows",
    "section": "",
    "text": "PowerShell",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-powsh",
    "href": "qmd/cli-windows.html#sec-cli-win-powsh",
    "title": "Windows",
    "section": "",
    "text": "Misc\n\nDocs, Sample Scripts\nUse Get-Help &lt;cmdlet-name&gt; to access help information for specific cmdlets.\nCheck version: $PSVersionTable\n\nFor a breakdown of the version number (e.g.¬†build, revison, etc.): $PSVersionTable.PSVersion\n\nUpdate to latest stable version: github\nComments: &lt;# comment #&gt;\nClear terminal: clear or cls or Clear-Host\nBefore you‚Äôll be able to run a script, you need to open PowerShell as administrator and execute this command: Set-ExecutionPolicy RemoteSigned\nSingle Wildcard: ?\n\nExample: Matching for am? would give you files named ‚Äúamy‚Äù ‚Äúamd‚Äù and ‚Äúam3.‚Äù\n\nShortcuts\n\nRun selected PowerShell code in current terminal using F8\nLaunch online help for the symbol under the cursor using Ctrl + F1\n\n\n\n\nLoops\n\nIterables\n\nArrays : $folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n$_ (Docs)\n\nAutomatic Variable; Alias for $PSItem\n\nDocs for automatic variables\n\nUseful for looping through objects in a directory. See ‚ÄúChange Name (or Extensions) of Multiple Files‚Äù for an example of usage\nGet the properties of an object\n\nExample: Using Get-ItemProperty (Docs)\nGet-ItemProperty scrapsheet.txt | Format-List\n\nProperty names will be on the left side of the output\n\nExample: Using Get-Member (Docs)\n Get-ChildItem *.txt | Select-Object -First 1 | Get-Member\n\nSelects the first text file in the directory and gets the Properties and a bunch of other stuff like methods which also can be used with $_\nDescriptions are awful for a lot of the stuff, but for the most part, you can guess what the property is.\n\n\nSome properties that can be used with $_\n\nFile System Objects:\n\n.Name: Returns the name component of a file\n.FullName: Returns the full path of the file.\n.Length: Returns the size of the file in bytes.\n.CreationTime: Returns the date and time the file was created.\n.LastWriteTime: Returns the date and time the file was last modified.\n.IsReadOnly: Returns $true if the file is read-only, $false otherwise.\n\nProcess Objects:\n\n.Id: Returns the unique identifier (PID) of the process.\n.Name: Returns the name of the process.\n.MainWindowTitle: Returns the title of the process‚Äôs main window (if applicable).\n.WorkingDirectory: Returns the working directory of the process.\n.CPU: Returns the CPU usage of the process.\n\nRegistry Objects:\n\n.Name: Returns the name of the registry key.\n.Hive: Returns the hive of the registry key (e.g., HKLM, HKCU).\n.Value: Returns the value of the registry key (if applicable).\n.Path: Returns the full path of the registry key.\n\n\n\n\n\n\nForeach\n\nUses a typical for-loop structure\nSee Snippets for an example of iterating over the output of Get-ChildItem\nIterate over an array\n# Create an array of folders\n$folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n\n# Perform iteration to create the same file in each folder\nforeach ($i in $folders) {\n    Add-Content -Path \"$i\\SampleFile.txt\" -Value \"This is the content of the file\"\n}\n\n$i is the for-loop variable and $folders is the iterable\nAdd-Content creates a text file in each of the folders in the array.\n\n\n\n\nForEach-Object\n\nSimilar to {purrr::map}\nIterable is piped into ForEach-Object\n$folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n$folders | ForEach-Object (Add-Content -Path \"$_\\SampleFile.txt\" -Value \"This is the content of the file\")\n\nDoes the same thing as the first example in the Foreach section\nAdd-Content creates a text file in each of the folders in the array.\n$_ is the for-loop variable ‚Äî called an ‚Äúautomatic variable.‚Äù See Iterables section.\n\n\n\n\nForEach Method\n\nSimilar to using Pyhon‚Äôs apply on an iterable.\nMethod applied an array\n$folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n$folders.ForEach({\n    Add-Content -Path \"$_\\SampleFile.txt\" -Value \"This is the content of the file\"\n})\n\nDoes the same thing as the first example in the Foreach section\nAdd-Content creates a text file in each of the folders in the array.\n$_ is the for-loop variable ‚Äî called an ‚Äúautomatic variable.‚Äù See Iterables section.\n\n\n\n\n\nCommands\n\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you‚Äôre already in the directory that you want the folder in. You can also use a path, e.g.¬†\"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nChange Name (or Extensions) of Multiple Files\nGet-ChildItem *.md | Rename-Item -NewName { $_.Name -replace '.md','.qmd' }\n\nGet-ChildItem: cmdlet gets all the files in the current folder that have a .mc file extension\nResults piped to Rename-Item\n\nNewName: Has a value that‚Äôs a script block that runs before the value is submitted to the NewName parameter.\n\n$_: (automatic variable) It represents each file object as it comes to the command through the pipeline.\n-replace: Operator replaces the file extension of each file with .qmd.\n\nNotice that matching using the -replace operator is not case sensitive.\n\n\n\n\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it‚Äôs a backslash (\\), but in Powershell, it‚Äôs a backtick ( ` )\n*Don‚Äôt forget that there‚Äôs a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it‚Äôs open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"\n\n\n\n\nSnippets\n\nRead in name of servers and ping each of them\n\n$servers = Get-Content .\\servers.txt\n\nforeach ($server in $servers) {\n    try {\n        $null = Test-Connection -ComputerName $server -Count 1 -ErrorAction STOP\n        Write-Output \"$server - OK\"\n    }\n    catch {\n        Write-Output \"$server - $($_.Exception.Message)\"\n    }\n}\n\nGet-Content reads the server names from each line in the the server.txt file\nforeach iterates through the server names\ntry tests the connection and catch outputs an error message if a server fails.\nIf Test-Connection fails the error message is stored in the $null variable\nThe error message line has an interesting syntax\n\n$_ is an automatic variable that represents $null which contains the error message which is selected by .Exception-Message.\n$() evaluates the expression\n\n\nTake files from a directory and iterate them as inputs to a function.\n$directory = \"C:\\Users\\me\\Documents\\AnyCap Screen Recorder\"\n\n# Define the FFmpeg command\n$ffmpegCommand = '-i {0} ' +\n                 '-c:v libx265 ' +\n                 '-crf 28 ' +\n                 '-preset medium ' +\n                 '-vf scale=-1:720 ' +\n                 '-c:a copy ' +\n                 'C:\\Users\\me\\Documents\\temp-storage\\{1}'\n\n# Get all files in the directory\n$files = Get-ChildItem -Path $directory `\n                       -Filter \"*.mp4\" \n\n# Loop through each file and apply the FFmpeg command\nforeach ($file in $files) {\n  # Construct the full command with the current file path\n  $fullCommand = $ffmpegCommand -f \"`\"$($file.FullName)`\"\", $file.Name\n  # Execute the FFmpeg command\n  Start-Process -FilePath \"ffmpeg.exe\" `\n                -ArgumentList $fullCommand `\n                -Wait `\n                -NoNewWindow\n}\n\nWrite-Host \"Finished processing files!\"\n\n$ffmpegCommand variable is a concantenated string using multiple lines for readability. {0} and {1} are placeholders to be filled in later.\n\nNote the space included at the end of each argument before the single quote since there‚Äôs no space included during concantenation.\n\nGet-ChildItem retrieves files from the specified directory ($directory).\n\n-Filter filters files that match the pattern (e.g., *.mp4).\n\nThe foreach loop iterates through each file ($file) in the $files collection.\n\n-f flag stands for format. Says to replace {0} and {1} in the $ffmpegCommand template with these properties.\n\"`\"$($file.FullName)`\"\"\n\nSince the directory name has spaces in it, extra quotes must included in order for the path to be quoted within the output string. A quoted file path in necessary forffmpeg to be able to read a directory name with spaces in it.\n$file.FullName: This is the full path of the current file. It is enclosed in $() to ensure that the property is properly evaluated and its value is included in the string.\n\nIf there were no spaces in the directory name, then $file.FullName is only thing that would be required. Everything else in this description could be discarded\n\n`\"$($file.FullName)`\": The double quotes \"...\" are used to create a string literal. Placing the entire expression $($file.FullName) within these double quotes ensures that the value of $file.FullName is treated as a single string, even if it contains spaces or special characters.\n\nThe backticks are escape characters in PowerShell and indicate that the double quotes should be treated as literal characters and not as operators formatting a string.\n\n\"`\"\\$(\\$file.FullName)`\"\": The additional double quotes at the beginning and end are used to format the expression as string for when it‚Äôs used as an argument in Start-Process.\nThe resulting path in the ffmpeg argument will look like: \"&lt;full file path&gt;\".\n\nStart-Process launches ffmpeg.exe with the constructed $fullCommand arguments.\n\n-Wait ensures the command finishes before continuing.\n-NoNewWindow says run ffmpeg in the same console window and don‚Äôt open a new one.",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-batscri",
    "href": "qmd/cli-windows.html#sec-cli-win-batscri",
    "title": "Windows",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g.¬†timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-wsl",
    "href": "qmd/cli-windows.html#sec-cli-win-wsl",
    "title": "Windows",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nNotes from\n\nBeware the IDEs of Windows (Subsystem for Linux)\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for ‚Äìdistribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit\nPaths\n\nUbuntu mounts the Windows C: drive at /mnt/c/\nWindows locates the Ubuntu root directory at \\wsl.localhost\\Ubuntu-22.04\nRoot Paths\n\n\n\n\n\n\n\n\nFolder:\nUbuntu path (bash):\nWindows path (PowerShell):\n\n\n\n\nUbuntu user directory\n/home/&lt;linux user name&gt;\n\\wsl.localhost\\Ubuntu-22.04\\home\\&lt;linux user name&gt;\n\n\nWindows user directory\n/mnt/c/Users/&lt;windows user name&gt;\nC:\\Users\\&lt;windows user name&gt;\n\n\n\nExample:\n\nLocating the Projects folder on the Linux file system while in Bash, /home/&lt;linux user name&gt;/Projects\nLocating the Projects folder on the Linux file system while in PowerShell, \\wsl.localhost\\Ubuntu-22.04\\home\\&lt;linux user name&gt;\\Projects",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html",
    "href": "qmd/confidence-and-prediction-intervals.html",
    "title": "Confidence & Prediction Intervals",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-misc",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-misc",
    "title": "Confidence & Prediction Intervals",
    "section": "",
    "text": "Also see Mathematices, Statistics &gt;&gt; Descriptive Statistics &gt;&gt; Understanding CI, sd, and sem Bars\nSE used for CIs of the difference in proportion\n\\[\n\\text{SE} = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\n\\]",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-terms",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-terms",
    "title": "Confidence & Prediction Intervals",
    "section": "Terms",
    "text": "Terms\n\nConfidence Intervals: A range of values within which we are reasonably confident the true parameter (e.g mean) of a population lies, based on a sample statistic (e.g.¬†t-stat).\n\nFrequentist Interpretation: The confidence interval is constructed by a procedure, which, if you were to repeat the experiment and collecting samples many many times, in 95% of the experiments, the corresponding confidence intervals would cover the true value of the population mean. (link)\n\\[\n[100\\cdot(1-\\alpha)]\\;\\%\\: \\text{CI for}\\: \\hat\\beta_i = \\hat\\beta_i \\pm \\left[t_{(1-\\alpha/2)(n-k)} \\cdot \\text{SE}(\\hat\\beta_i)\\right]\n\\]\n\n\\(t\\) is the t-stat for\n\n\\(n-k\\) = sample size - number of predictors\n\\(1-\\alpha\\) for 2-sided; \\(1 - (\\alpha/2)\\) for 1 sided (I think)\n\n\\(\\text{SE}(\\beta_i)\\) is the sqrt of the corresponding value on the diagonal of the variance-covariance matrix for the coefficients.\n\nBayesian Interpretation: the true value is in that interval with 95% probability\n\nCoverage or Empirical Coverage: The level of coverage actually observed when evaluated on a dataset, typically a holdout dataset not used in training the model. Rarely will your model produce the Expected Coverage exactly\n\nAdaptive Coverage: Setting your Expected Coverage so that your Empirical Coverage = Target Coverage. A conformal prediction algorithm is adaptive if it not only achieves marginal coverage, but also (approximately) conditional coverage\n\nExample: 90% target coverage\n\nIf our model is slightly overfit, you might see that a 90% expected coverage leads to an 85% empirical coverage on a holdout dataset. To align your target and empirical coverage at 90%, may require setting expected coverage at something like 93%\n\n\nExpected Coverage: The level of confidence in the model for the prediction intervals.\nConditional Coverage: The coverage for each individual class of the outcome variable or subset of data specified by a grouping variable.\nMarginal Coverage: The overall average coverage across all classes of the outcome variable. All conformal methods achieve at or near the Expected Coverage averaged across classes but not necessarily for each individual class.\nTarget Coverage: The level of coverage you want to attain on a holdout dataset\n\ni.e.¬†The proportion of observations you want to fall within your prediction intervals\n\n\nJeffrey‚Äôs Interval: Bayesian CIs for Binomial proportions (i.e.¬†probability of an event)\n# probability of event\n# n_rain in the number of events (rainy days)\n# n is the number of trials (total days)\nmutate(pct_rain = n_rain / n,¬†\n¬† ¬† ¬† ¬†# jeffreys interval\n¬† ¬† ¬† ¬†# bayesian CI for binomial proportions\n¬† ¬† ¬† ¬†low = qbeta(.025, n_rain + .5, n - n_rain + .5),¬†\n¬† ¬† ¬† ¬†high = qbeta(.975, n_rain + .5, n - n_rain + .5))\nPrediction Interval: Used to estimate the range within which a future observation is likely to fall\n\nStandard Procedure for computing PIs for predictions (See link for examples and further details)\n\\[\n\\hat Y_0 \\pm t^{n-p}_{\\alpha/2} \\;\\hat\\sigma \\sqrt{1 + \\vec x_0'(X'X)^{-1}\\vec x_0}\n\\]\n\n\\(Y_0\\) is a single prediction\n\\(t\\) is the t-stat for\n\n\\(n-p\\) = sample size - number of predictors\n\\(1 - \\alpha\\) for 2-sided; \\(1 - (\\alpha/2)\\) for 1 sided (I think)\n\n\\(\\hat\\sigma\\) is the variance given by residual standard error, summary(Model1)$sigma\n\\[\nS^2 = \\frac{1}{n-p}\\;||\\;Y-X\\hat \\beta\\;||^2\n\\]\n\n\\(S = \\hat \\sigma\\)\nI think this is also the \\(\\operatorname{MSE}/\\operatorname{dof}\\) that you sometimes see in other formulas\n\n\\(x_0\\) is new data for the predictor variable values for the prediction (also would need to include a 1 for the intercept)\n\\((X'X)^{-1}\\) is the variance covariance matrix, vcov(model)",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-diag",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-diag",
    "title": "Confidence & Prediction Intervals",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nMean Interval Score (MIS)\n\n(Proper) Score of both coverage and interval width\n\nI don‚Äôt think there‚Äôs a closed range, so it‚Äôs meant for model comparison\nLower is better\n\ngreybox::MIS and (scaled) greybox::sMIS\n\nOnline docs don‚Äôt have these functions, but docs in RStudio do\n\nAlso scoringutils::interval_score\n\nDocs have formula\n\nThe actual paper is dense Need to take the mean of MIS\n\n\n\nCoverage\n\nExample: Coverage %\ncoverage &lt;- function(df, ...){\n¬† df %&gt;%\n¬† ¬† mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price pred_upper, 1, 0)) %&gt;%¬†\n¬† ¬† group_by(...) %&gt;%¬†\n¬† ¬† summarise(n = n(),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† n_covered = sum(\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† covered\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† stderror = sd(covered) / sqrt(n),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† coverage_prop = n_covered / n)\n}\nrf_preds_test %&gt;%¬†\n¬† coverage() %&gt;%¬†\n¬† mutate(across(c(coverage_prop, stderror), ~.x * 100)) %&gt;%¬†\n¬† gt::gt() %&gt;%¬†\n¬† gt::fmt_number(\"stderror\", decimals = 2) %&gt;%¬†\n¬† gt::fmt_number(\"coverage_prop\", decimals = 1)\n\nFrom Quantile Regression Forests for Prediction Intervals\nSale_Price is the outcome variable\nrf_preds_test is the resulting object from predict with a tidymodels model as input\n\nExample: Test consistency of coverage across quintiles\npreds_intervals %&gt;%¬† # preds w/ PIs\n¬† mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;%¬† # quintiles\n¬† mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price &lt;= .pred_upper, 1, 0)) %&gt;%¬†\n  with(chisq.test(price_grouped, covered))\n\np value &lt; 0.05 says coverage significantly differs by quintile\nSale_Price is the outcome variable\n\n\nInterval Width\n\nNarrower bands should mean a more precise model\nExample: Average interval width across quintiles\nlm_interval_widths &lt;- preds_intervals %&gt;%¬†\n¬† mutate(interval_width = .pred_upper - .pred_lower,\n¬† ¬† ¬† ¬† interval_pred_ratio = interval_width / .pred) %&gt;%¬†\n¬† mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;% # quintiles\n¬† group_by(price_grouped) %&gt;%¬†\n¬† summarize(n = n(),\n¬† ¬† ¬† ¬† ¬† ¬† mean_interval_width_percentage = mean(interval_pred_ratio),\n¬† ¬† ¬† ¬† ¬† ¬† stdev = sd(interval_pred_ratio),\n¬† ¬† ¬† ¬† ¬† ¬† stderror = stdev / sqrt(n)) %&gt;%¬†\n¬† mutate(x_tmp = str_sub(price_grouped, 2, -2)) %&gt;%¬†\n¬† separate(x_tmp, c(\"min\", \"max\"), sep = \",\") %&gt;%¬†\n¬† mutate(across(c(min, max), as.double)) %&gt;%¬†\n¬† select(-price_grouped)\n\nlm_interval_widths %&gt;%¬†\n¬† mutate(across(c(mean_interval_width_percentage, stdev, stderror), ~.x*100)) %&gt;%¬†\n¬† gt::gt() %&gt;%¬†\n¬† gt::fmt_number(c(\"stdev\", \"stderror\"), decimals = 2) %&gt;%¬†\n¬† gt::fmt_number(\"mean_interval_width_percentage\", decimals = 1)\n\nInterval width has actually been transformed into a percentage as related to the prediction (removes the scale of the outcome variable)",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-boot",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-boot",
    "title": "Confidence & Prediction Intervals",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nMisc\n\nDo NOT bootstrap the standard deviation\n\narticle\nbootstrap is ‚Äúbased on a weak convergence of moments‚Äù\nif you use an estimate based standard deviation of the bootstrap, you are being overly conservative (i.e.¬†overestimate the sd)\n\nbootstrapping uses the original, initial sample as the population from which to resample, whereas Monte Carlo simulation is based on setting up a data generation process (with known values of the parameters of a known distribution). Where Monte Carlo is used to test drive estimators, bootstrap methods can be used to estimate the variability of a statistic and the shape of its sampling distribution\nPackages\n\n{ebtools::get_boot_ci}\n\n\nSteps\n\nResample with replacement\nCalculate statistic of resample\nStore statistic\nRepeat 10K or so times\nCalculate mean, sd, and quantiles for CIs across all collected statistics\n\nCIs\n\nPlenty of articles for means and models, see bkmks\nrsample::reg_intervals is a convenience function for lm, glm, survival models\n\nPIs\n\nBootstrapping PIs is a bit complicated\n\nSee Shalloway‚Äôs article (code included)\nonly use out-of-sample estimates to produce the interval\nestimate the uncertainty of the sample using the residuals from a separate set of models built with cross-validation",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-conf",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-conf",
    "title": "Confidence & Prediction Intervals",
    "section": "Conformal Prediction Intervals",
    "text": "Conformal Prediction Intervals\n\nMisc\n\nPackages\n\n{{mapie}} - Handles scikit-learn, tf, pytorch, etc. with wrappers. Computes conformal PIs for Regression, Classification, and Time Series models.\n\nRegression\n\nMethods: naive, split, jackknife, jackknife+, jackknife-minmax, jackknife-after-bootstrap, CV, CV+, CV-minmax, ensemble batch prediction intervals (EnbPI).\n‚ÄúSince the typical coverage levels estimated by jackknife+ follow very closely the target coverage levels, this method should be used when accurate and robust prediction intervals are required.‚Äù\n‚ÄúFor practical applications where N is large and/or the computational time of each leave-one-out simulation is high, it is advised to adopt the CV+ method‚Äù even though the interval width will be slightly larger than jackknife+\n‚ÄúThe jackknife-minmax and CV-minmax methods are more conservative since they result in higher theoretical and practical coverages due to the larger widths of the prediction intervals. It is therefore advised to use them when conservative estimates are needed.‚Äù\n‚ÄúThe conformalized quantile regression method allows for more adaptiveness on the prediction intervals which becomes key when faced with heteroscedastic data.‚Äù\nEnbPI is for time series and residuals must be updated each time new observations are available\n\nClassification\n\nMethods: LAC, Top-K, Adaptive Prediction Sets (APS), Regularized Adaptive Prediction Sets (RAPS), Split and Cross-Conformal methods.\nThe difference between these methods is the way the conformity scores are computed\nLAC method is not adaptive: the coverage guarantee only holds on average (i.e.¬†marginal coverage). Difficult classification cases may have prediction sets that are too small, and easy cases may have sets that are too large. (See below for details on process). Doesn‚Äôt seem like to great a task to manually make it adaptive though (See example below).\nAPS‚Äô conformity score used to determine the threshold is a constrained sum of the predicted probabilities for that observation. Only the predicted probabilites \\(\\ge\\) the predicted probability of the true class are included in the sum. Everything else is the same as the LAC algorithm, although the default behavior is to keep the last class that crosses the threshold through the argument, include_last_label = [True, ‚Äúrandomized‚Äù, False]. The value of the argument can determine whether conditional coverage is (approximately) attained with True being the most liberal setting. Note that only ‚Äúrandomized‚Äù can produce empty predicted class sets. Algorithm tends to produce large predicted class sets when there are many classes in the outcome variable.\nRAPS attenuates the lengthier predicted class sets in APS through regularization. A penalty, \\(\\lambda\\), is added to predicted probabilities with ranks greater that some value, \\(k\\). Everything else is the same as APS.\nNot sure what Split is, but Cross-Conformal is CV applied to LAC and APS.\n\n\n\nNotes from\n\nHow to Handle Uncertainty in Forecasts: A deep dive into conformal prediction\n\nThe conformity score formula used in this article, \\(s_i = |\\;y_i - \\hat p_i(y_i\\;|\\;X_i)\\;|\\) where \\(y_i\\) is the observed class and \\(\\hat p\\) is the predicted probability, has the same results as to the one below, but it‚Äôs not workable in production since there is no observed class.\n\nConformal Prediction for Machine Learning Classification ‚Äî From the Ground Up\n‚ÄúMAPIE‚Äù Explained Exactly How You Wished Someone Explained to You\n\nResources\n\nIntroduction To Conformal Prediction With Python A Short Guide for Quantifying Uncertainty of Machine Learning Models\n\nSee R &gt;&gt; Documents &gt;&gt; Machine Learning\n\n\nNormal PIs require iid data while conformal PIs only require the ‚Äúidentically distributed‚Äù part (not independent) and therefore should provide more robust coverage.\nContinuous outcome (link) using quantile regression\n\n\n\nClassification\n\nLAC (aka Score Method) Process\n\nSplit data into Train, Calibration (aka Validation), and Test\nTrain the model on the training set\nOn the calibration (aka validation) set, compute the conformity scores only for the observed class (i.e.¬†true label) for each observation\n\\[\ns_{i, j}  = 1 - \\hat p_{i,j}(y_i | X_i)\n\\]\n\nVariables\n\n\\(s_{i,j}\\): Conformity Score for the ith observation and class \\(j\\)\n\\(y_i\\): Observed Class\n\\(\\hat p_{i,j}\\): Predicted probability by the model for class \\(j\\)\n\\(X_i\\): Predictors\n\\(i\\): Index of the observed data\n\\(j\\): Class of the outcome variable\n\nRange: [0, 1]\nIn general, Low = good, High = bad\nIn R, the predicted probabilities for statistical models are always for the event (i.e.¬†\\(y_i = 1\\)) in a binary outcome context, so when the observed class = 0, the score will be \\(s_{i,0} = 1-(1- \\hat p_{i, 1}(y_i | X_i)) = \\hat p_{i, 1}(y_i | X_i)\\) which is just the predicted probability.\n\nOrder the conformity scores from highest to lowest\nAdjust the chosen the \\(\\alpha\\) using a finite sample correction, \\(q_{\\text{level}} = 1- \\frac{ceil((n_{\\text{cal}}+1)\\alpha)}{n_{\\text{cal}}}\\) and calculate the quantile.\nCalculate the critical value or threshold for the quantile\n\n\nx-axis corresponds to an ordered set of conformity scores\nIf \\(\\alpha = 0.05\\), find the score value at the the 95th percentile (e.g.¬†quantile(scores, 0.95))\nBlue: conformity scores are not statistically significant. They‚Äôre within our prediction interval.\nRed: Very large conformity scores indicate high divergence from the true label. These conformal scores are statistically significant and thereby outside of our prediction interval.\n\nPredict on the Test set and calculate conformity scores for each class\nFor each test set observation, select classes that have scores below the threshold score as the model prediction.\n\nAn observation could potentially have both classes or no classes selected. ( Not sure if this is true in a binary outcome situation)\n\n\nExample: LAC Method, Multinomial\n\nModel\nclassifier = LogisticRegression(random_state=42)\nclassifier.fit(X_train, y_train)\nScores calculated using only the predicted probability for the true class on the Validation set (aka Calibration set)\n# Get predicted probabilities for calibration set\ny_pred = classifier.predict(X_Cal)\ny_pred_proba = classifier.predict_proba(X_Cal)\nsi_scores = []\n# Loop through all calibration instances\nfor i, true_class in enumerate(y_cal):\n    # Get predicted probability for observed/true class\n    predicted_prob = y_pred_proba[i][true_class]\n    si_scores.append(1 - predicted_prob) \nThe threshold determines what¬†coverage our predicted labels will have\nnumber_of_samples = len(X_Cal)\nalpha = 0.05\nqlevel = (1 - alpha) * ((number_of_samples + 1) / number_of_samples)\nthreshold = np.percentile(si_scores, qlevel*100)\nprint(f'Threshold: {threshold:0.3f}')\n#&gt; Threshold: 0.598\n\nFinite sample correction for the 95th quantile: multiply 0.95 by¬†(n+1)/n\n\nThreshold is then used to get predicted labels of the test set\n# Get standard predictions for comparison\ny_pred = classifier.predict(X_test)\n# Calc scores, then only take scores in the 95% conformal PI\nprediction_sets = (1 - classifier.predict_proba(X_test) &lt;= threshold)\n\n# Get labels for predictions in conformal PI\ndef get_prediction_set_labels(prediction_set, class_labels):\n    # Get set of class labels for each instance in prediction sets\n    prediction_set_labels = [\n        set([class_labels[i] for i, x in enumerate(prediction_set) if x]) for prediction_set in \n        prediction_sets]\n    return prediction_set_labels\n\n# Compare conformal prediction with observed and traditional preds\nresults_sets = pd.DataFrame()\nresults_sets['observed'] = [class_labels[i] for i in y_test]\nresults_sets['conformal'] = get_prediction_set_labels(prediction_sets, class_labels)\nresults_sets['traditional'] = [class_labels[i] for i in y_pred]\nresults_sets.head(10)\n#&gt;    observed  conformal        traditional\n#&gt; 0  blue      {blue}           blue\n#&gt; 1  green     {green}          green\n#&gt; 2  blue      {blue}           blue\n#&gt; 3  green     {green}          green\n#&gt; 4  orange    {orange}         orange\n#&gt; 5  orange    {orange}         orange\n#&gt; 6  orange    {orange}         orange\n#&gt; 7  orange    {blue, orange}   blue\n#&gt; 8  orange    {orange}         orange\n#&gt; 9  orange    {orange}         orange\n\nconformity scores are calculated for each potential class using the predicted probabilities on the test set\nThe predicted class for an observation is determined by whether a class has a score below the threshold.\nTherefore, an observation may have 1 or more predicted classes or 0 predicted classes.\n\nStatistics (See Statistics section for functions)\n\nOverall\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n#&gt; Overall coverage: 0.947\n#&gt; Average set size: 1.035\n\nOverall coverage is very close to the target coverage of 95%, therefore, marginal coverage is achieved which is expected for this method\n\nPer Class\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n#&gt;         Class counts  Coverage   Average set size\n#&gt; blue    241           0.817427   1.087137\n#&gt; orange  848           0.954009   1.037736\n#&gt; green   828           0.977053   1.016908\n\nOverall coverage (i.e.¬†for all labels) will be at or very near 95% but coverage for individual classes may vary.\n\nAn illustration of how this method lacks Conditional Coverage\nSolution: Get thresholds for each class. (See next example)\n\nNote that the blue class had substantially fewer observations that the other 2 classes.\n\n\n\nExample: LAC-adapted - Threshold per Class\n\nDon‚Äôt think {{mapie}} has this option.\nAlso possible do this for subgroups of data, such as ensuring equal coverage for a diagnostic across racial groups, if we found coverage using a shared threshold led to problems.\nCalculate individual class thresholds\n# Set alpha (1 - coverage)\nalpha = 0.05\nthresholds = []\n# Get predicted probabilities for calibration set\ny_cal_prob = classifier.predict_proba(X_Cal)\n# Get 95th percentile score for each class's s-scores\nfor class_label in range(n_classes):\n    mask = y_cal == class_label\n    y_cal_prob_class = y_cal_prob[mask][:, class_label]\n    s_scores = 1 - y_cal_prob_class\n    q = (1 - alpha) * 100\n    class_size = mask.sum()\n    correction = (class_size + 1) / class_size\n    q *= correction\n    threshold = np.percentile(s_scores, q)\n    thresholds.append(threshold)\nApply individual class thresholds to test set scores\n# Get Si scores for test set\npredicted_proba = classifier.predict_proba(X_test)\nsi_scores = 1 - predicted_proba\n\n# For each class, check whether each instance is below the threshold\nprediction_sets = []\nfor i in range(n_classes):\n    prediction_sets.append(si_scores[:, i] &lt;= thresholds[i])\nprediction_sets = np.array(prediction_sets).T\n\n# Get prediction set labels and show first 10\nprediction_set_labels = get_prediction_set_labels(prediction_sets, class_labels)\nStatistics\n\nOverall\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n#&gt; Overall coverage: 0.95\n#&gt; Average set size: 1.093\n\nSimilar to previous example\n\nPer Class\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n#&gt;         Class counts  Coverage   Average set size\n#&gt; blue    241           0.954357   1.228216\n#&gt; orange  848           0.956368   1.139151\n#&gt; green   828           0.942029   1.006039\n\nCoverages now very close to 95% and the average set sizes have increased, especially for Blue.\n\n\n\n\n\n\nContinuous\n\nConformalized Quantile Regression Process\n\nSplit data into Training, Calibration, and Test sets\n\nTraining data: data on which the quantile regression model learns.\nCalibration data: data on which CQR calibrates the intervals.\n\nIn the example, he split the data into 3 equal sets\n\nTest data: data on which we evaluate the goodness of intervals.\n\nFit quantile regression model on training data.\nUse the model obtained at previous step to predict intervals on calibration data.\n\nPIs are predictions at the quantiles:\n\n(alpha/2)*100) (e.g 0.025, alpha = 0. 05)\n(1-(alpha/2))*100) (e.g.¬†0.975)\n\n\nCompute conformity scores on calibration data and intervals obtained at the previous step.\n\nResiduals are calculated for the PI vectors\nScores are calculated by taking the row-wise maximum of both (upper/lower quantile) residual vectors (e.g s_i &lt;- pmax(lower_pi_res, upper_pi_res))\n\nGet 1-alpha quantile from the distribution of conformity scores (e.g threshold &lt;- quantile(s_i, 0.95)\n\nThis score value will be the threshold\n\nUse the model obtained at step 1 to make predictions on test data.\n\nCompute PI vectors (i.e.¬†predictions at the previously stated quantiles) on Test set\ni.e.¬†Same calculation as with the calibration data in step 2 where you use the model to predict at upper and lower PI quantiles.\n\nCompute lower/upper end of the interval by subtracting/adding the threshold from/to the quantile predictions (aka PIs)\n\nLower conformity interval: lower_pi &lt;- test_lower_pred  - threshold\nUpper conformity interval: upper_pi &lt;- test_upper_pred + threshold\n\n\nExample: Quantile Random Forest\nimport numpy as np\nfrom skgarden import RandomForestQuantileRegressor\n\nalpha = .05\n\n# 1. Fit quantile regression model on training data\nmodel = RandomForestQuantileRegressor().fit(X_train, y_train)\n\n# 2. Make prediction on calibration data\ny_cal_interval_pred = np.column_stack([\n¬† ¬† model.predict(X_cal, quantile=(alpha/2)*100),¬†\n¬† ¬† model.predict(X_cal, quantile=(1-alpha/2)*100)])\n\n# 3. Compute conformity scores on calibration data\ny_cal_conformity_scores = np.maximum(\n¬† ¬† y_cal_interval_pred[:,0] - y_cal,¬†\n¬† ¬† y_cal - y_cal_interval_pred[:,1])\n\n# 4. Threshold: Get 1-alpha quantile from the distribution of conformity scores\n#¬† ¬† Note: this is a single number\nquantile_conformity_scores = np.quantile(\n¬† ¬† y_cal_conformity_scores, 1-alpha)\n\n# 5. Make prediction on test data\ny_test_interval_pred = np.column_stack([\n¬† ¬† model.predict(X_test, quantile=(alpha/2)*100),¬†\n¬† ¬† model.predict(X_test, quantile=(1-alpha/2)*100)])\n\n# 6. Compute left (right) end of the interval by\n#¬† ¬† subtracting (adding) the quantile to the predictions\ny_test_interval_pred_cqr = np.column_stack([\n¬† ¬† y_test_interval_pred[:,0] - quantile_conformity_scores,\n¬† ¬† y_test_interval_pred[:,1] + quantile_conformity_scores])\n\n\n\nStatistics\n\nAverage Set Size\n\nThe average number of predicted classes per observation since there can be more than 1 predicted class in the conformal PI\nExample:\n# average set size for each class\ndef get_average_set_size(prediction_sets, y_test):\n    average_set_size = []\n    for i in range(n_classes):\n        average_set_size.append(\n            np.mean(np.sum(prediction_sets[y_test == i], axis=1)))\n    return average_set_size   \n\n# Overall average set size (weighted by class size)\n# Get class counts\ndef get_class_counts(y_test):\n    class_counts = []\n    for i in range(n_classes):\n        class_counts.append(np.sum(y_test == i))\n    return class_counts\n\ndef get_weighted_set_size(set_size, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_set_size = np.sum((set_size * class_counts) / total_counts)\n    weighted_set_size = round(weighted_set_size, 3)\n    return weighted_set_size\n\nCoverage\n\nClassification: Percentage of correct classifications\nExample: Classification\n# coverage for each class\ndef get_coverage_by_class(prediction_sets, y_test):\n    coverage = []\n    for i in range(n_classes):\n        coverage.append(np.mean(prediction_sets[y_test == i, i]))\n    return coverage\n\n# overall coverage (weighted by class size)\n# Get class counts\ndef get_class_counts(y_test):\n    class_counts = []\n    for i in range(n_classes):\n        class_counts.append(np.sum(y_test == i))\n    return class_counts\n\ndef get_weighted_coverage(coverage, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_coverage = np.sum((coverage * class_counts) / total_counts)\n    weighted_coverage = round(weighted_coverage, 3)\n    return weighted_coverage\n\n\n\n\nVisualization\n\nConfusion Matrix\n\n\nBinary target where labels are 0 and 1\nInterpretation\n\nTop-left: predictions where both labels are not statistically significant (i.e.¬†inside the ‚Äúprediction interval‚Äù).\n\nThe model predicts both classes well since both labels have low scores.\nDepending the threshold, maybe the model could be relatively agnostic (e.g.¬†predicted probabilites like 0.50-0.50, 0.60-0.40)\n\nBottom-right: predictions where both labels are statistically significant¬† (i.e.¬†outside the ‚Äúprediction interval‚Äù).\n\nModel totally whiffs. Confident it‚Äôs one label when it‚Äôs actually another.\n\nExample\n\n1 (truth) - low predicted probability = high score -&gt; Red and significant\n0 - high predicted probability = high score -&gt; Red and significant\n\n\n\nTop-right: predictions where all 0 labels are not statistically significant.\n\nModel predicted the 0=class well (i.e.¬†low scores) but the 1-class poorly (i.e.¬†high scores)\n\nBottom-left: predictions where all 1 labels are not statistically significant. Here, the model predicted that 1 is the true class.\n\nVice versa of top-right",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/big-data.html",
    "href": "qmd/big-data.html",
    "title": "Big Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-misc",
    "href": "qmd/big-data.html#sec-bgdat-misc",
    "title": "Big Data",
    "section": "",
    "text": "RcppArmadillo::fastLmPure Not sure what this does but it‚Äôs rcpp so maybe faster than lm for big data.\n.lm.fit is a base R lm function that is 30%-40% faster than lm.",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-hghperf",
    "href": "qmd/big-data.html#sec-bgdat-hghperf",
    "title": "Big Data",
    "section": "High Performance",
    "text": "High Performance\n\n{rpolars}: arrow product; uses SIMD which is a low-level vectorization that can be used to speed up simple operations like addition, subtraction, division, and multiplication\n\nResources\n\nCookbook Polars for R\n\nExample: Read and Summarize\ndf &lt;- pl$scan_csv(file_name)$\n    group_by(\"state\")$\n    agg(\n        pl$\n          col(\"measurement\")$\n          min()$\n          alias(\"min_m\"),\n        pl$\n          col(\"measurement\")$\n          max()$\n          alias(\"max_m\"),\n        pl$\n          col(\"measurement\")$\n          mean()$\n          alias(\"mean_m\")\n    )$\n    collect()\n\nFastest at this operation according to this benchmark\n\nExample: groupby state + min, max, mean\n# polars sql\nlf &lt;- polars::pl$LazyFrame(D) \npolars::pl$SQLContext(frame = lf)$execute(\n  \"select min(measurement) as min_m, \n          max(measurement) as max_m, \n          avg(measurement) as mean_m \n  from frame \n  group by state\")$collect()\n\n# polars\npolars::pl$\n  DataFrame(D)$\n  group_by(\"state\")$\n  agg(polars::pl$\n        col(\"measurement\")$\n        min()$alias(\"min_m\"),\n      polars::pl$\n        col(\"measurement\")$\n        max()$alias(\"max_m\"),\n      polars::pl$\n        col(\"measurement\")$\n        mean()$alias(\"mean_m\"))\n\n{collapse}: Fast grouped & weighted statistical computations, time series and panel data transformations, list-processing, data manipulation functions, summary statistics and various utilities such as support for variable labels. Class-agnostic framework designed to work with vectors, matrices, data frames, lists and related classes i.e.¬†xts, data.table, tibble, pdata.frame, sf.\n\noptions(collapse_mask = \"all\")\nlibrary(collapse)\n\nCode chunk above can optimize any script. No other changes necessary. Quick demo.\nvs arrow/polars (benchmark)\n\nDepends on the data/groups ratio\n\nIf you have ‚Äúmany groups and little data in each group‚Äù then use collapse\n\nIf your calculations involve ‚Äúmore complex statistics algorithms like the median (involving selection) or mode or distinct value count (involving hashing)(cannot, to my knowledge, benefit from SIMD)‚Äù then use collapse.\n\nset_collapse(mask = \"manip\"|\"all\") to remove f- prefixes\nExample: groupby state + min, max, mean\nD |&gt;\n  fgroup_by(state) |&gt; \n  fsummarise(min = fmin(measurement), \n             max = fmax(measurement), \n             mean = fmean(measurement)) |&gt;\n  fungroup()\n\n{r2c}: Fast grouped statistical computation; currently limited to a few functions, sometimes faster than {collapse}\n{data.table}: Enhanced data frame class with concise data manipulation framework offering powerful aggregation, extremely flexible split-apply-combine computing, reshaping, joins, rolling statistics, set operations on tables, fast csv read/write, and various utilities such as transposition of data.\n\nExample: groupby state + min, max, mean\nD[ ,.(mean = mean(measurement),\n      min = min(measurement),\n      max = max(measurement)),\n   by=state]\n\n# Supposedly faster\nrbindlist(lapply(unique(D$state), \n                 \\(x) data.table(state = x, \n                                 y[state == x, \n                                   .(mean(measurement), \n                                     min(measurement), \n                                     max(measurement))\n                                   ]\n                                 )))\n\n{rfast}: A collection of fast (utility) functions for data analysis. Column- and row- wise means, medians, variances, minimums, maximums, many t, F and G-square tests, many regressions (normal, logistic, Poisson), are some of the many fast functions\n\nThe vast majority of the functions accept matrices only, not data.frames.\nDo not have matrices or vectors with have missing data (i.e NAs). There are no checks and C++ internally transforms them into zeros (0), so you may get wrong results.\nExample: groupby state + min, max, mean\nlev_int &lt;- as.numeric(D$state)\nminmax &lt;- Rfast::group(D$measurement, lev_int, method = \"min.max\")\ndata.frame(\n    state = levels(D$state),\n    mean = Rfast::group(D$measurement, lev_int, method = \"mean\"),\n    min = minmax[1, ],\n    max = minmax[2, ]\n)\n\n{matrixStats}: Efficient row-and column-wise (weighted) statistics on matrices and vectors, including computations on subsets of rows and columns.\n{kit}: Fast vectorized and nested switches, some parallel (row-wise) statistics, and some utilities such as efficient partial sorting and unique values.\n{fst}: A compressed data file format that is very fast to read and write. Full random access in both rows and columns allows reading subsets from a ‚Äò.fst‚Äô file.",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-lgmem",
    "href": "qmd/big-data.html#sec-bgdat-lgmem",
    "title": "Big Data",
    "section": "Larger than Memory",
    "text": "Larger than Memory\n\nOnly work with a sample of the data\n\nRandom sample in CLI\n\nSee binder for code\nAlso this snippet from Healy for a zipped csv.\n\n\nImproved version\ngzip -cd giantfile.csv.gz | (read HEADER; echo $HEADER; perl -ne 'print if (rand() &lt; 0.001)‚Äô) &gt; sample.csv\n\nRemoves the need to decompress the file twice, adds the header row, and removes the risk of a double header row\n\n\n\nOnly read the first n lines\n\nset n_max arg in readr::read_*\n\n\ndatasette.io - App for exploring and publishing data. It helps people take data of any shape, analyze and explore it, and publish it as an interactive website and accompanying API.\n\nWell documented, many plugins\n\nRill - A tool for effortlessly transforming data sets into powerful, opinionated dashboards using SQL.\n\nDocs, Example Projects\nPowered by Sveltekit & DuckDB = conversation-fast, not wait-ten-seconds-for-result-set fast\nWorks with your local and remote datasets ‚Äì imports and exports Parquet and CSV (s3, gcs, https, local)\nNo more data analysis ‚Äúside-quests‚Äù ‚Äì helps you build intuition about your dataset through automatic profiling\nNo ‚Äúrun query‚Äù button required ‚Äì responds to each keystroke by re-profiling the resulting dataset\nRadically simple interactive dashboards ‚Äì thoughtful, opinionated, interactive dashboard defaults to help you quickly derive insights from your data\nDashboards as code ‚Äì each step from data to dashboard has versioning, Git sharing, and easy project rehydration\n\nOnline duckdb shell for parquet files (gist, https://shell.duckdb.org/)\nselect max(wind)¬†\nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/gridwatch/gridwatch_2023-01-08.parquet';\n-- Takes 6 seconds on the first query, 200ms on subsequent similar queries\n\nselect *¬†\nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/NSPL/NSPL.parquet'¬†\nwhere pcd = 'SW1A1AA';\n-- Takes 13 seconds on the first query, 100ms on subsequent similar queries\nCSV Editors\n\nFor editing or reformatting cells\nPopular spreadsheet programs like googlesheets (100MB) and excel (25MB online) have file size limits and they‚Äôre slow to upload to. The following programs are free(-ish) local alternatives only limited by your RAM.\nSuggest for files over a few hundred MBs that you open as Read-Only\n\nOpening the files as ‚ÄúEditable‚Äù will probably balloon the memory cost to at least 5 times the file size. (e.g.¬†350MB csv \\(\\rightarrow\\) 2GB RAM)\n\nModern CSV - Nice modern interface, read-only mode that can open large csvs (100s of MBs) without making much of a dent in your RAM, fully featured (moreso if you pay a small-ish one time fee)\n\nDocs, Feature free/upgrade list\nStill has some functionality in read-only mode (e.g.¬†search, sort)\n\nOpenRefine - Has read-only, Several add-ons, Completely open source.\n\nDocs, List of Extensions\nNo functionality when read-only (must create a project to do anything) ‚Äî just reading\nStarts with a 1024 MB RAM usage limit which is proably fine for editing around a 100MB csv. Need to set the limit higher in a config file in order to edit larger files.\nOnce you create a project, I think it has some editing features that you‚Äôd have to pay for with Modern CV.\nOpens other file formats besides csv (e.g.¬†xlsx, xml, json, etc)\n\n\nxsv - A command line program for indexing, slicing, analyzing, splitting, and joining CSV files. Written in Rust.\n\nIsn‚Äôt well maintained. But it is written in Rust, so may be able handle larger files that would make csvkit to slow to use.\n\ncsvkit - Suite of command-line tools for converting to and working with CSV. Written in Python.\n\nInstallation docs\n\nOne of the articles your terminal has to be a bash terminal but I dunno\n\nIf so, they recommend cmder or enabling the Linux subsystem with WSL2.\n\n\nNotes from\n\nArticle with additional examples and options\n\nFeatures\n\nPrint CSV files out nicely formatted\nCut out specific columns\nGet statistical information about columns\n\nConvert excel files to CSV files:\nin2csv excel_file.xlsx &gt; new_file.csv\n# +remove .xlsx file\nin2csv excel_file.xlsx &gt; new_file.csv && rm excel_file\nSearch within columns with regular expressions:\ncsvgrep -c county -m \"HOLT\" new_file.csv\n# subset of columns (might be faster) with pretty formatting\ncsvcut -c county,total_cost new_file.csv | csvgrep -c county -m \"HOLT\" | csvlook\n\nSearches for ‚ÄúHOLT‚Äù in the ‚Äúcounty‚Äù column\n\nQuery with SQL\n\nsyntax csvsql --query \"ENTER YOUR SQL QUERY HERE\" FILE_NAME.csv\nExample\n\n\nView top lines: head new_file.csv\nView columns names: csvcut -n new_file.csv\nSelect specific columns: csvcut -c county,total_cost,ship_date new_file.csv\n\nWith pretty output: csvcut -c county,total_cost,ship_date new_file.csv | csvlook\nCan also use column indexes instead of names\n\nJoin 2 files: csvjoin -c cf data1.csv data2.csv &gt; joined.csv\n\n‚Äúcf‚Äù is the common column between the 2 files\n\nEDA-type stats:\ncsvstat new_file.csv\n# subset of columns\ncsvcut -c total_cost,ship_date new_file.csv | csvstat\n\nJSONata - a lightweight, open-source query and transformation language for JSON data, inspired by the ‚Äòlocation path‚Äô semantics of XPath 3.1.\n\nMisc\n\nNotes from: Hrbrmstr‚Äôs article\nJSONata also doesn‚Äôt throw errors for non-existing data in the input document. If during the navigation of the location path, a field is not found, then the expression returns nothing.\n\nThis can be beneficial in certain scenarios where the structure of the input JSON can vary and doesn‚Äôt always contain the same fields.\n\nTreats single values and arrays containing a single value as equivalent\nBoth JSONata and¬†jq¬†can work in the browser (JSONata embedding code, demo), but¬†jq¬†has a slight speed edge thanks to WASM. However, said edge comes at the cost of a slow-first-start\n\nFeatures\n\nDeclarative syntax that is pretty easy to read and write, which allows us to focus on the desired output rather than the procedural steps required to achieve it\nBuilt-in operators and functions for manipulating and combining data, making it easier to perform complex transformations without writing custom code in a traditional programming language like python or javascript\nUser-defined functions that let us extend JSONata‚Äôs capabilities and tailor it to our specific needs\nFlexible output structure that lets us format query results into pretty much any output type\n\n\njq + jsonlite - json files\njsoncrack.com - online editor/tool to visualize nested json (or regular json)\njj - cli tool for nested json. Full support for ndjson as well as setting/updating/deleting values. Plus it lets you perform similar pretty/ugly printing that jq does.\nsqlite3 - CLI utility allows the user to manually enter and execute SQL statements against an SQLite database or against a ZIP archive.\n\nalso directly against csv files (post)\n\ntextql - Execute SQL against structured text like CSV or TSV\n\nRequire Go language installed\nOnly for Macs or running a docker image\n\ncolumnq-cli - sql query json, csv, parquet, arrow, and more\nfread + CLI tools\n\nFor large csvs and fixing large csv with jacked-up formating see article, RBlogger version\n\n{arrow}\n\nconvert file into parquet files\n\npass the file path to open_dataset, use group_by to partition the Dataset into manageable chunks\nuse write_datasetto write each chunk to a separate Parquet file‚Äîall without needing to read the full CSV file into R\n\ndplyr support\n\nmultiplyr\n\nOption for data &gt; 10M rows and you only have access to one machine\nSpreads data over local cores\n\n{sparklyr}\n\nspin up a spark cluster\ndplyr support\nSet-up a cloud bucket and load data into it. Then, read into a local spark cluster. Process data.\n\n{h2o}\n\nh2o.import_file(path=path) holds data in the h2o cluster and not in memory\n\n{disk.frame}\n\nsupports many dplyr verbs\nsupports¬† future package to take advantage of multi-core CPUs but single machine focused\nstate-of-the-art data storage techniques such as fast data compression, and random access to rows and columns provided by the fst package to provide superior data manipulation speeds\n\nMatrix ops\n\nsee bkmks: mathematics &gt;&gt; packages\n\n{ff}\n\nsee bkmks: data &gt;&gt; loading/saving/memory\nThink it converts files to a ff file type, then you load them and use ffapply to perform row and column operations with base R functions and expressions\nmay not handle character and factor types but may work with {bit} pkg to solve this",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-viz",
    "href": "qmd/big-data.html#sec-bgdat-viz",
    "title": "Big Data",
    "section": "Viz",
    "text": "Viz\n\nScatter¬†plots\n\n{scattermore}, {ggpointdensity}\n{ggrastr}\n\nRasterize only specific layers of a ggplot2 plot (for instance, large scatter plots with many points) while keeping all labels and text in vector format. This allows users to keep plots within a reasonable size limit without losing the vector properties of scale-sensitive information.\ngithub; tweet\n\n\nH2O\n\nh2o.aggregator Reduces data size to a representive sample, then you can visualize a clustering-based method for reducing a numerical/categorical dataset into a dataset with fewer rows A count column is added to show how many rows is represented by the exemplar row (I think)\n\nAggregator maintains outliers as outliers but lumps together dense clusters into exemplars with an attached count column showing the member points.\nFor cat vars:\n\nAccumulate the category frequencies.\nFor the top 1,000 or fewer categories (by frequency), generate dummy variables (called one-hot encoding by ML people, called dummy coding by statisticians).\nCalculate the first eigenvector of the covariance matrix of these dummy variables.\nReplace the row values on the categorical column with the value from the eigenvector corresponding to the dummy values.\n\ndocs; article\n\n\n{dbplot}\n\nplots data that are in databases\n\nAlso able to plot data within a spark cluster\n\ndocs\n\nObservableHQ\n\n{{{deepscatter}}}\n\nThread (using Arrow, duckdb)",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/bayes-workflow.html",
    "href": "qmd/bayes-workflow.html",
    "title": "Workflow",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Bayes",
      "Workflow"
    ]
  },
  {
    "objectID": "qmd/bayes-workflow.html#sec-bayes-wkflw-misc",
    "href": "qmd/bayes-workflow.html#sec-bayes-wkflw-misc",
    "title": "Workflow",
    "section": "",
    "text": "Also see Model Building, Concepts &gt;&gt; Misc &gt;&gt; Regression Workflow\nNotes from\n\nBayesian Workflow (Gelman, Vehtari) (arXiv link)\n\nResources\n\nVehtari Video: On Bayesian Workflow (2022) (Based on the paper, but I haven‚Äôt watched it, yet)\nNabiximols treatment efficiency: Vehtari‚Äôs example of applyijng his workflow in the context of comparing continuous and discrete observation models.\n\nCurrent Checklist\n\nCheck convergence diagnostics\nDo posterior predictive checking\nCheck residual plots\nModel comparison (if prediction)\n\nAnalysis Checklist (Thread)\n\nA suitably flexible Bayesian regression adjustment model,\nChosen by cross-validation/LOO,\nIncluding Gaussian processes for the unit-level effects over time (and space/network if relevant),\nImputation of missing data, and\nInformative priors for biases in the data collection process.",
    "crumbs": [
      "Bayes",
      "Workflow"
    ]
  },
  {
    "objectID": "qmd/glossary-ds-terms.html",
    "href": "qmd/glossary-ds-terms.html",
    "title": "Glossary: DS terms",
    "section": "",
    "text": "200 Status - An API serving an ML model returns a HTTP 200 OK success status response code indicates that the request has succeeded.\nAMI - amazon machine image. Thing that has R and the main¬†packages you need to load onto the cloud server\nAnti-Patterns - certain patterns in software development that are considered bad programming practices.\n\nAs opposed to design patterns which are common approaches to common problems which have been formalized and are generally considered a good development practice, anti-patterns are the opposite and are undesirable.\n\nArm -¬†a group of patients receiving a specific treatment (or no treatment). Trials involving several arms, or randomized trials, treat randomly-selected groups of patients with different therapies in order to compare their medical outcomes. Experimental arms, which receive an experimental drug, are compared with control arms.¬†Single-arm or non-randomized trials, in which everyone enrolled in a trial receives the experimental therapy\nArtifacts - objects that are created as a result of a process. e.g.¬†model objects, cleaned data sets, visuals, etc.\nAsynchronous Programming - code runs (or must run) after something else happens and also not sequentially (e.g.¬†when a function calls a callback function in JS).\nAthena - amazon query service that works with S3. Best for analyses using kubernetes. ODBC drivers are best with interactive app\nB2C, B2B - business-to-consumer, business-to-business, describes a business that‚Äôs end-product is being sold to a consumer or a business.\nBalanced Design (aka orthogonal) has an equal number of observations for all possible level combinations. For example in an experiment where gender is an independent variable, an equal number of males receive the treatment as do females receive treatment. If the male/female counts were unequal, then the experiment is unbalanced.\n\nStat tests have greater power for balanced designs\nTest stat less susceptible to to small departures from the assumption of equal variances (homoscedasticity).\n\nBatch - collect a large number of data points, process them periodically and store results somewhere (contrasts with real-time in which a data input leads to an immediate prediction)\nBootstrapping (CS) - usually applies to a situation where a system depends on itself to start, sort of a chicken and egg problem. (e.g.¬†How do you start an OS initialization process if you don‚Äôt have the OS running yet?) Typically a simple file that starts a large process.\nBounce, Email - When an email cannot be delivered to an email server.\n\nHard Bounce - indicates a permanent reason an email cannot be delivered (e.g.¬†Recipient email address doesn‚Äôt exist; Recipient email server has completely blocked delivery)\nSoft Bounce - indicates a temporary delivery issue (for details on the reasons, see link)\n\nBounce Rate - the percentage of visitors to a particular website who navigate away from the site after viewing only one page. Low bounce rate can indicate the landing page needs improvement\nBPI - Business process improvement is a management exercise in which enterprise leaders use various methodologies to analyze their procedures to identify areas where they can improve accuracy, effectiveness and/or efficiency and then redesign those processes to realize the improvements.\nBLUE - best linear unbiased estimator, e.g.¬†regression line\nCAC - customer acquisition cost - measures how much an organization spends to acquire new customers. The total cost of sales and marketing efforts, as well as property or equipment, needed to convince a customer to buy a product or service.\nCapEx - Capital Expenditure - 1 of 2 main forward budgeting mechanisms for a corporation (also see OpEx). Often used to undertake new projects or investments or large-scale asset acquisitions (buildings and vehicles)\nClinical Trial - research studies (e.g.¬†RCT) performed in people that are aimed at evaluating a medical, surgical, or behavioral intervention\nCDI - Customer Data Infrastructure - built to collect behavioral data from primary or first-party data sources, but some solutions also support a handful of secondary data sources (third-party tools)\nCDP - Customer Data Platform - add-ons from CDI vendors; a layer on top of CDI that offers a set of capabilities to analyze data using a visual interface.\nCDN - content delivery network - a system of distributed servers (network) that deliver pages and other web content to a user, based on the geographic locations of the user, the origin of the webpage and the content delivery server.\nCLV/CLTV - Customer Lifetime Value - how much money a customer will bring your brand throughout their entire time as a paying customer.\nCOGS - Cost of goods sold (aka Cost of Sales) - refers to the direct costs of producing the goods sold by a company. This amount includes the cost of the materials and labor directly used to create the good. It excludes indirect expenses, such as distribution costs and sales force costs.\nComplete Factorial Design - a research study involving two or more independent variables in which every possible combination of the levels of each variable is represented. For instance, in a study of two drug treatments, one (A) having two dosages and the other (B) having three dosages, a complete factorial design would pair the dosages administered to different individuals or groups of participants as follows: A1 with B1, A1 with B2, A1 with B3, A2 with B1, A2 with B2, and A2 with B3.\nCPG - Consumer packaged goods are items used daily by average consumers that require routine replacement or replenishment, such as food, beverages, clothes, tobacco, makeup, and household products.\nCPC - Cost Per Click¬†- refers to the cost an advertiser pays each time an online user clicks on his or her digital ad\nCRM - customer relationship management i.e.¬†customer service. Salesforce tracks this data. Example: what features your salesperson promised, and when? How much revenue you have from each customer? Or which salesperson sold the most in the past year?\ncron- standard tool used on Unix and Unix-like systems to schedule the periodic execution in the background of a command or script (like a batch script)\nCrossed Factors - when every category of one factor co-occurs in the design with every category of the other factor. In other words, there is at least one observation in every combination of categories for the two factors. (in contrast to ‚Äúnested factors‚Äù). As a consequence, interaction terms involving these two factors is allowed.\nCrossover Study - A type of clinical trial in which the study participants receive each treatment in a random order. With this type of study, every patient serves as his or her own control. Crossover studies are often used when researchers feel it would be difficult to recruit participants willing to risk going without a promising new treatment.\nCross-Section Data - randomly sampled data from a population. Like a survey. Aka observational data. See experimental data for comparison.\n\nPooled - differs from panel data in that it is observations of different subjects (instead of the same subjects) in different time periods.\nRolling - both the presence of an individual in the sample and the time at which the individual is included in the sample are determined randomly.\n\nCross-Tabs - section of survey analysis where the aggregated results are broken down by demography, party affiliation, etc.\nCTA - marketing term, call-to-action.¬†any device designed to prompt an immediate response or encourage an immediate sale; words or phrases that can be incorporated into sales scripts, advertising messages or web pages that encourage consumers to take prompt action\nCTR - click through rate: the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement. It is commonly used to measure the success of an online advertising campaign for a particular website as well as the effectiveness of email campaigns.\nCRM - Customer Relationship Management - acquiring new customers but especially about retaining existing ones\nDAU - daily active users, ex: daily avg # of registered users of the site over past 30 days\nDBA - Database Administrator is an admin role that understands the particular database technology and how to get the best out of it. This includes improving performance, backups and recovery.\nDDL - Data definition or description language - Subset of SQL. Used to:\n\nKeep a snapshot of the database structure\nSet up a test system where the database acts like the production system but contains no data\nProduce templates for new objects that you can create based on existing ones. For example, generate the DDL for the Customer table, then edit the DDL to create the table Customer_New with the same schema.\n\nDesparate Impact Analysis - Analysis of the result of the application of a standard, requirement, test or other screening tool used for selection that‚Äîthough appearing neutral‚Äîhas an adverse effect on individuals who belong to a legally protected class Differential Dropout**]{style=‚Äòcolor: #009499‚Äô} -¬†Differing dropout rates between treatment arms\nDMA - Designated Market Area; a geographic region where Nielsen, the ratings company, analyzes and quantifies how television is viewed. Residents can receive the same local TV and radio stations\nDNS -¬†¬†Domain Name System**]{style=‚Äòcolor: #009499‚Äô} -¬† translates domain names to IP addresses so browsers can load Internet resources.\nDSL - domain-specific language - a computer language specialized to a particular application domain\nEMR - Amazon version of a spark cluster used for big data processing and analysis.\nEndogenous - A model variable is correlated with other variables excluded from the model (omitted variable bias). Determined by measuring the correlation between the variable and residuals of the model. If a predictor variable hasn‚Äôt been randomly assigned, it‚Äôs likely to be endogenous.\nEquitability - concept that says¬†a dependence measure should give equal importance to linear and nonlinear relationships. Consistent strength measurements across different variable relationships that have similar amounts of noise.\nERP - enterprise resource planning, sort of a catch-all for manufacturing, supply-chain, etc, see the wiki\nETL - extract, transfer, load - usually refers to transferring data from one location to another\nEndpoint (biostats) - Outcome variable measured in a medical study. e.g.¬†Death, stroke, or quality of life are good endpoints. Blood tests and images on scans are not good endpoints.\n\nA composite endpoint is one that consists of two or more events\n\nExample: death due to cardiovascular causes or hospitalization due to heart failure\n\nSo the binary outcome would be a 1 if either of those events took place or a 0 if they did not. Or in a survival model, time until either of those events.\n\n\n\nEOF - End of file - Input from a terminal never really ‚Äúends‚Äù (unless the device is disconnected), but it is useful to enter more than one ‚Äúfile‚Äù into a terminal, so a key sequence is reserved to indicate end of input.\nex ante - based on assumption and prediction and being essentially subjective and estimative\nex post - based on knowledge and retrospection and being essentially objective and factual\nExperimental Data - data from a RCE/RCT. Compare with observational data\nFaaS - Function as a service - type of cloud service for developing, running, and managing apps (e.g.¬†AWS Lambda)\nFactorial Design - Experiment where you‚Äôre interested in the effect of two or more independent variables.\nFraud Rules - fraud scores are calculated based on rules, which add or subtract points. The user action may be a transaction, signup or login. Rules look at data points such as an email address, IP address, or social media presence.\nFraud Score - assigned values to how risky a user action is. Scoring determined by fraud rules.\nFuzzy Design - See Sharp Design\nGHA - Github Actions\nGMV - Gross merchandises value - the total value of merchandise sold over a given period of time through a customer-to-customer (C2C) exchange site\nGRP - Gross Rating Point. A standard measure in advertising, it measures advertising impact. You calculate it as a percent of the target market reached multiplied by the exposure frequency. Thus, if you get advertise to 30% of the target market and give them 4 exposures, you would have 120 GRP.\nHTE - Heterogeneous Treatment Effect - Also called differential treatment effect, includes difference of means, odds ratios, and Hazard ratios for time-to-event outcome vars\n\nAscertaining subpopulations for which a treatment is most beneficial (or harmful) is an important goal of many clinical trials.\nOutcome heterogeneity is due to wide distributions of baseline prognostic factors. When strong risk factors exist, there is hetergeneity in the outcome variable.\n\nSolution: add baseline predictors to your model that account for these strong risk factors.\n\nHeterogeneity of Treatment Effects - The degree to which different treatments have differential causal effects on each unit.\n\nHit Ratio - percent of records that were read in order to complete a query in a database. Cloud db providers often charge by the number of records searched\nHomogeneity of Treatment Effects - See Heterogeneity of Treatment Effects\nHPC - High Performance Computing\nHoneypot - data (for example, in a network site) that appears to be a legitimate part of the site, but is actually isolated and monitored, and that seems to contain information or a resource of value to attackers, who are then blocked.\nIaaS - infrastructure-as-a-service ( Hardware is provided by an external provider and managed for you)\nIAM - identity and access management, keys and passwords etc\nIRB - institutional review board, reviews studies ethical and moral issues\nITT - Intent-to-Treat analysis¬†includes all randomized patients in the groups to which they were randomly assigned, regardless of their adherence with the entry criteria, regardless of the treatment they actually received, and regardless of subsequent withdrawal from treatment or deviation from the protocol. Avoids overoptimistic estimates of the efficacy of an intervention resulting from the removal of non-compliers by accepting that noncompliance and protocol deviations are likely to occur in actual clinical practice. So mimics likely situation in the real world, but not good for estimating the causal effect of a treatment.\nKernels - (article) - system kernels - the interface between the operating system, i.e.¬†the software, and the hardware components in a device. It is used in all devices with an operating system, for example, computers, laptops, smartphones, smartwatches, etc.\n\nWhen we use a program on a computer, such as Excel, we handle it on the so-called Graphical User Interface (GUI). The program converts every button click or other action into machine code and sends it to the operating system kernel. If we want to add a new column in an Excel table, this call goes to the system core. This in turn passes the call on to the computer processing unit (CPU), which executes the action.\nJupyter Kernels - an engine that executes notebook code and is specific to a particular programming language (e.g.¬†python kernel)\nKaggle Kernels - a free platform from Kaggle to run Jupyter notebooks in the browser. Advantage is that you don‚Äôt have to set-up an environment locally.\n\nKPI- key performance indicator\nKYC - Know-Your-Customer is info a company collects to verify your identity to combat fraud. Used by telecoms and financial services\nLazy Evaluation - ‚Äù never pulls data into R unless you explicitly ask for it. It delays doing any work until the last possible moment. It collects together everything you want to do and then sends it to the database in one step.‚Äù\nLikelihood - probability of seeing this data given a specific value for a distribution parameter (eg mean, sd). Goal is to search for parameter values until the likelihood is maximized.\nLOB - Line of Business is a general term which refers to a product or a set of related products that serve a particular customer transaction or business need. (i.e.¬†product categories)\n\nExamples\n\nConsumer Banking: credit cards, line of credit or loan program, mortgages, and corporate, small business and personal bank accounts.\nFinancial services and brokerages: mergers and acquisitions or partnerships, real estate investments, and wealth management\nProperty and casualty insurance companies: property and casualty insurance (i.e., homeowners, car, boat, renters, etc.), life insurance, health insurance, and commercial business insurance.\n\nSub-lines of Business would be sub-categories within each LOB\n\nLongitudinal Data - see panel data\nLTV - see CLV/CLTV\nManual Review - A human is reviews the case to determine whether action is needed. In fraud, an model output may trigger a ‚Äúmanual review‚Äù to determine whether an event was indeed fraudulent.\nMLlib - Apache Spark machine learning library\nMVC - Minimum Viable Corpus - a data size threshold; such that below this threshold, the data simply isn‚Äôt useful/valuable. Used in data products business.\nMVP - minimum viable project, agile term. Version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort\nNamespace - allows you to use two functions with the same name but from different packages, e.g.¬†dplyr::select or in general, package::function. https://stackoverflow.com/questions/3384204/what-are-namespaces/3384384#3384384\nNNH - Numbers Needed to Harm - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a particular adverse outcome. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNNT - Numbers Needed to Treat - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a favorable outcome such as treatment response. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNPS - Net Promoter Score - a measure of customer loyalty. Widely used market research metric that typically takes the form of a single survey question asking respondents to rate the likelihood that they would recommend a company, product, or a service to a friend or colleague.\nNRT - near real-time, aka streaming data\nObservational Data - see cross sectional data\nOEM - original equipment manufacturer\nOKR - Objectives and Key Results is a popular management strategy for goal setting within organizations. A framework for turning strategic intent into measurable outcomes for an organization.\nOnline Machine Learning - A method of machine learning where the model incrementally learns from a stream of data points in real-time. It‚Äôs a dynamic process that adapts its predictive algorithm over time, allowing the model to change as new data arrives.\nOn-Prem - on-premises ‚Äî working with servers in the the building and not in the cloud.\nOOD - out-of-distribution - data which differ from the training data and on which a model might underperform\nOpen Cohort - subjects can leave or be added over time.\nOpEx - Operational Expenditures - 1 of 2 main forward budgeting mechanisms for a corporation (also see CapEx). Relates to day-to-day expenses (such as payroll and software subscriptions). Smaller payouts over time.\nOpportunity Sizing - Quantitative analysis to select a subset of ideas to which to devote resources in product development\nNested Factors - happens when all the levels of one factor only occur in combination with one level of another factor (in contrast to ‚Äúcrossed factors‚Äù). As a consequence, your model can‚Äôt have an interaction term involving these two variables.\nP&L - Profit and Loss Statement Panel data - cross section data with a time element. Repeated measures of the same subject over time. Synonym for Longitudinal Data\nParcel - a land record that defines the boundary of a piece of land. These boundaries are the basic administrative unit of local government in regards to land and property. Managing ownership and tax records are the primary reason local governments generate these files. So these are boundaries differentiating ownership of properties.\nPEP8 - style guide for python\nPI - principal investigator\nPivot Table - Excel name for a group_by %\\&gt;% summarize calculation\n\ne.g.¬†from a table of individual fruit sales: group_by(fruit_type, country) %\\&gt;% summarize(total_amt = sum(amount))\n\nPLG - Product-led growth is an end user-focused growth model that relies on the product itself as the primary driver of customer acquisition, conversion, and expansion. e.g.¬†open source a product, let the customer go through the documentation and use and experiment with the product on their own time. In contrast to sales pitching a product to a customer and letting them use it for a trial basis.\nPM - product manager\nPoC - Proof of Concept\nPOS - point of sale, The point of sale or point of purchase is the time and place where a retail transaction is completed. It can be in a physical store, where POS terminals and systems are used to process card payments or a virtual sales point such as a computer or mobile electronic device.\nRCE - randomized controlled experiment, subjects randomly assigned to two groups, treatment and control. Double blind means the researcher doesn‚Äôt know who is in which group.\nRCT - randomized clinical trial\nRDD - Regression discontinuity design\nRedis - REmote DIctionary Server - is an in-memory, key-value database, commonly referred to as a data structure server. Used when volume of read and write operations exceed the capabilities of traditional databases. With Redis‚Äôs capability to easily persist the data to disk, it is a superior alternative to the traditional memcached solution for caching.\nRefactoring - updating or optimizing code\nRegression Testing - checks if changes made to a system negatively impacted or broke any of the existing features. It is often performed right after each update or commit to the code base to identify new bugs and ensure that your system works properly.\nRFI - Request for Information - Used to collect written information about the capabilities of various suppliers. Normally it follows a format that can be used for comparative purposes. An RFI is primarily used to gather information to help make a decision on what steps to take next. RFIs are therefore seldom the final stage and are instead often used in combination with request for proposal (RFP), request for tender (RFT), and request for quotation (RFQ).\nRFM - recency, frequency, monetary value - method of estimating customer value; common in retail\nRFP - Request for Proposal - A document that an organization, often a government agency or large enterprise, posts to elicit a response ‚Äì a formal bid ‚Äì from potential vendors for a desired solution. The RFP specifies what the customer is looking for and describes each evaluation criterion on which a vendor‚Äôs proposal will be assessed.\n\nROAS - return on ad spend\nRUG - Regional User Group\nS3 - Amazon simple storage service, database\nSaaS - Software-as-a-service is a mechanism through which companies offer the functionality of their apps, which remain on their company servers, to other companies or customers.\nSCO - sales cycle optimization, active process of providing content on your site (and beyond) that speaks to each of the key phases\nSEO - Search engine optimization, generating high page rankings for key search terms\nSDK - software development kit\nSharp Design - Each individual or group receives the same ‚Äúamount‚Äù of treatment (e.g.¬†a state law or medication dosage). Opposite being fuzzy design (?)\nSKU - Stock Keeping Unit**]{style=‚Äòcolor: #009499‚Äô} - Usually a bar code that has all the information to distinguish it from another product. These attributes can include manufacturer, description, material, size, color, packaging, and warranty terms. When a business takes inventory of its stock, it counts the quantity it has of each SKU.\nSLA - service level agreement - a contract between a service provider and its internal or external customers that documents what services the provider will furnish and defines the service standards this provider is obligated to meet. service. Important for holding prediction latency of an app to a certain standard or maintaining data reliability with vendors. (see link for more details on SLA, SLO, and SLI)\nSLI - service level indicators - metrics that measure compliance with an SLO (see link for more details on SLA, SLO, and SLI)\nSLO - service level objectives - objectives your team must meet in order to meet the conditions of the SLA (see link for more details on SLA, SLO, and SLI)\nSMB - (small to medium-sized business) generally defined as companies with fewer than 1000 employees and less than $1 billion in annual revenue.\nSME - Subject Matter Experts\nSPC - Statistical process control is a method of quality control which employs statistical methods to monitor and control a process\nSpill - missed opportunity metric, measures ‚Äúlost trading days‚Äù on which flights or hotels filled too quickly (the result of pricing too low)\nSpoil - missed opportunity metric, measures empty seats or rooms (often the result of pricing too high)\nSSH - secure shell is a cryptographic Network protocol for operating Network Services securely over an unsecured Network. Typical applications include remote command line login in remote command execution\nstdout - standard output, which is the terminal by default\nTDD - Test-driven development is a style of programming where coding, testing, and design are tightly interwoven\nTF-IDF- stands for term frequency-inverse document frequency, and is often used in information retrieval and text mining.\nThroughput - the amount of material or items passing through a system or process.\ntx - treatment, seen as variable with different treatments as values\nURI - Uniform Resource Identifier - a string of characters that unambiguously identifies a particular resource. e.g.¬†s3//bucket/path/to/folder or http://127.0.0.1:5000or c:\\Users\\me\\path\\to\\folder\nUTM - Urchin Traffic Monitor - used to identify marketing channels\n\ne.g.¬†http://yourwebsite.com/your-post-title/?utm_source=google\n\nutm code = string after ‚Äú?‚Äù\n\nThis person clicked a google ad to get to your site\n\nName comes from Urchin Tracker, a web analytics software that served as the base for Google Analytics.\n\nVPS - virtual private server\nWIP - Work-in-Progress\nWithin Person Study - multiple treatments on each person either all in the same period or different treatments in different periods\nYear-Over-Year - used to make comparisons between one time period and another that is one year earlier.\n\nFormula (percentage): (value_this_year / value_previous_year) - 1\nExample: (sales_Jul_2023 / sales_Jul_2022) - 1",
    "crumbs": [
      "Glossary: DS terms"
    ]
  },
  {
    "objectID": "qmd/python-general.html",
    "href": "qmd/python-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-misc",
    "href": "qmd/python-general.html#sec-py-gen-misc",
    "title": "General",
    "section": "",
    "text": "Tools (see article, article for installation and usage)\n\nruff - rust-based, linter and sorts imports\n\nFast, sensible default settings, focuses on more important things out of the box, and has less legacy burden\n\npydocstring - tool for checking compliance with Python docstring conventions\nblack - code formatter\nisort - sorts your imports\npytest, pytest-watch - unit tests\ncommitizen - guides you through a series of steps to create a commit message that conforms to the structure of a Conventional Commit\nnbQA - linting in jupyter notebooks\nmypy - type checker; good support and docs\npylance - checks type hinting in VSCode (see Functions &gt;&gt; Documentation &gt;&gt; Type Hinting)\ndoit - task runner; {targets}-like tool; tutorial\npre-commit - specify which checks you want to run against your code before committing changes to your git repository\nREADME templates - link\n\nPut as much config as possible into pyproject.toml. A lot of configurations tools will happily read from it, and it will give you one source of truth.\nAn underscore _ at the beginning is used to denote private variables in Python.\ndef set_temperature(self, value):\n¬† ¬† ¬† ¬† if value &lt; -273.15:\n¬† ¬† ¬† ¬† ¬† ¬† raise ValueError(\"Temperature below -273.15 is not possible.\")\n¬† ¬† ¬† ¬† self._temperature = value\n\nyou can still access ‚Äú_temperature‚Äù but it‚Äôs just meant for internal use by the class and the underscore indicates this\n\n{{warnings::warnings.filterwarnings(‚Äòignore‚Äô)}}\nsys.getsizeof(obj) to get the size of an object in memory.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#terms",
    "href": "qmd/python-general.html#terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nclasses - code template for creating objects, we can think of it as a blueprint. It describes the possible states and behaviors that every object of a certain type could have.\nobject - data structure storing information about the state and behavior of a certain entity and is an instance of a class\nstub file - a file containing a skeleton of the public interface of that Python module, including classes, variables, functions ‚Äì and most importantly, their types. (Source)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#base",
    "href": "qmd/python-general.html#base",
    "title": "General",
    "section": "Base",
    "text": "Base\n\nInfo method\n\nX.info()\nRemove an object: del\nCheck object type\n\ntype() : outputs the type of an object\nisinstance() : outputs type and inheritance of an object\nSee article for details on differences\n\nImport Libraries\nimport logging\nimport bentoml\nfrom transformers import (\n¬† ¬† SpeechT5Processor,\n¬† ¬† SpeechT5ForTextToSpeech,\n¬† ¬† SpeechT5HifiGan,\n¬† ¬† WhisperForConditionalGeneration,\n¬† ¬† WhisperProcessor,\n)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-fund",
    "href": "qmd/python-general.html#sec-py-gen-fund",
    "title": "General",
    "section": "Fundamentals",
    "text": "Fundamentals\n\nSlicing\n\nFormat my_list[start:stop:step]\n\n** start value is inclusive and the end value is exclusive\n\n1 element and more than 1 element\n\"Python\"[0] # P\n\"Python\"[0:1] # P\n\"Python\"[0:5] # Pytho\nillustrates how when using a range, the last element is exclusive\nNegative indexing my_list[0:-1]\n\nEverything but the last object\n\nSkip every second element\nmy_list = list(\"Python\")\nmy_list[0:len(my_list):2]\n&gt;&gt; ['P', 't', 'o']\nstart at 0, end at len(my_list), step = 2\nShortcuts\nmy_list[0:-1] == my_list[:-1]\nmy_list[0:len(my_list):2] == my_list[::2]\n\"Python\"[::-1] == \"Python\"[-1:-7:-1]\n\nDefaults\n\n0 for the start value\nlen(list) for the stop value\n1 for the step value\n\nDefaults for negative step value\n\n-1 for the start value\n-len(list) - 1 for the stop value\n\n\nAlias vs new object\nb = a # alias\nb = a[:] # new object\n\nWith the alias, changes to a will happen to b as well\n\nCommon use cases\n\n\n\n\n\n\n\nEvery element but the first and the last one\n[1:-1]\n\n\nEvery element in reverse order\n[::-1]\n\n\nEvery element but the first and the last one in reverse order\n[-2:0:-1]\n\n\nEvery second element but the first and the last one in reverse order\n[-2:0:-2]\n\n\n\nUsing slice function\nsequence = list(\"Python\")\nmy_slice = slice(None, None, 2) # equivalent to [::2]\nindices = my_slice.indices(len(sequence))\n&gt;&gt; (0, 6, 2)\n\nShows start = 0, stop = 6, step = 2\n\n\n\n\nF-Strings\n\nCheatsheet\nParameterize with {}\n&gt;&gt; x = 5\n&gt;&gt; f\"One icecream is worth [{x}]{style='color: #990000'} dollars\"\n'One icecream is worth 5 dollars'\n! - functions\n\n!r ‚Äî Shows the string delimiter, calls the repr() method.\n\nrepr‚Äôs goal is to be unambiguous and str‚Äôs is to be readable. For example, if we suspect a float has a small rounding error, repr will show us while str may not\n\n!a ‚Äî Shows the Ascii for the characters.\n!s ‚Äî Converts the value to a string.\n\nGuessing this the str() method (see !r for details)\n\n\nfood2brand = \"Mcdonalds\"\nfood2 = \"French fries\"\nf\"I like eating {food2brand} {food2!r}\"\n\"I like eating Mcdonalds 'French fries'\"\nChange format with ‚Äú:‚Äù\n&gt;&gt; import datetime\n&gt;&gt; date = datetime.datetime.utcnow()\n&gt;&gt; f\"The date is {date:%m-%Y %d}\"\n'The date is 02-2022 15'\nFormatting with ‚Äú&gt;‚Äù and ‚Äú&lt;‚Äù\n\n\n&lt;6 says width is 6 characters and text starts at the left edge\n&gt;10.2f says width is 10 characters, text starts the right hand edge, and number is rounded to 2 decimal places\n\n\n\n\nOperators\n\n(docs)\nExponential: 5**3\nInteger division: 5//3\nModulo: 5%3\nIdentity: is\nx = 5\ny = 3\nprint(\"The result for x is y is\", x is y)\nThe result for x is y is false\n\nThink you can also use == here too\n\nLogical: and and or\nprint(\"The result for 5 &gt; 3 and 6 &gt; 8 is\", 5 &gt; 3 and 6 &gt; 8)\nprint(\"The result for 5 &gt; 3 or 6 &gt; 8 is\", 5 &gt; 3 or 6 &gt; 8)\nThe result for 5 &gt; 3 and 6 &gt; 8 is False\nThe result for 5 &gt; 3 or 6 &gt; 8 is True\nSubset: in and not in\nprint(\"Is the number 3 in the list [1,2,3]?\", 3 in [1,2,3])\nIs the number 3 in the list [1,2,3]? True\n\nprint(\"Is the number 3 not in the list [1,2,3]?\", 3 not in [1,2,3])\nIs the number 3 not in the list [1,2,3]? False\nAssignment",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-dattyp",
    "href": "qmd/python-general.html#sec-py-gen-dattyp",
    "title": "General",
    "section": "Types",
    "text": "Types\n\nScalars\n\nCreate scalars by subsetting a list\ninputs = [1, 0.04, 0.9]\n# 1 numeric¬†\nrmse = inputs[0] # rmse = 1 and is type 'float'\n# multiple numerics\nrmse, mape, rsq = inputs\n\nTuples\n\nLists are mutable and tuples are not\n\ni.e.¬†we can add or remove elements to a list after we create it but we cannot do such thing to a tuple\n\nSyntax: name_of_tuple = (a, b)\n\nLists\n\nCreate list of objects (e.g.¬†floats)\nacc_values = [rmse, mape, rsq]\n\nalt method: asterisk-comma notation\n*acc_names, = \"RMSE\", \"MAPE\", \"R-SQ\"\n\nasterisk is ‚Äúunzipping operator‚Äù\n\n\nMake a copy\nold_list = [2, 3, 4]\nnew_list = list(old_list)\n\nDictionaries\n\n** if creating a simple dict, more performant to use curly braces **\n\nAvoid d = dict(1=1, x='x')\n\nJoin 2 dicts -¬† d.update(d2)\n\nIf d and d2 share keys, d2‚Äôs values for those keys will be used\n\nAccess a value from a key: sample_dict['key_name']\nMake a copy\nold_dict = {stuff: 2, more_stuff: 3}\nnew_dict = dict(old_dict)\nConvert list of tuples to a dict\nacc_dict = dict(acc_list)\n\nzip creates lists of tuples (See Loops &gt;&gt; zip section)\n\nAdd key, value pair to a dict\ntransaction_data['user_address'] = '221b Baker Street, London - UK'\n# or\ntransaction_data.update(user_address='221b Baker Street, London - UK')\nUnpack dict into separate tuples for key:value pairs\nrmse, mape, rsq = acc_dict.items()\nrmse\n('RMSE', 1)\n\n** fastest way to iterate over both keys and values in a dict **\ncan also use zip to unpack pairs into a list (see loops &gt;&gt; zip)\n\nUnpack dict into separate lists for keys and values\nacc_keys = list(acc_dict.keys())¬†\nacc_values = list(acc_dict.values())\n\n** fastest way to iterate over a dict‚Äôs keys or values **\n\nUnpack values from dicts into separate scalars\nrmse, mape, rsq = acc_dict.values()\nrmse\n1\nPull the value for a key (e.g.¬†k) or return the default value - d.get(k, default)\n\nDefault is ‚ÄúNone‚Äù. I think this can be set with d.setdefault(k, default)\n\nCheck for specific key (logical)\n‚Äòsend_currency‚Äô in transaction_data\n‚Äòsend_currency‚Äô in transaction_data.keys()\n‚Äòsend_currency‚Äô not in transaction_data.keys()\n\nLike %in% in R\n\nCheck for specific value (logical)\n‚ÄòGBP‚Äô in transaction_data.values()\nCheck for key, value pair\n(‚Äòsend_currency‚Äô, ‚ÄòGBP‚Äô) in transaction_data.items()\nPretty printing of dictionaries\n¬† ¬† _ = [print(k, \":\", f'{v:.1f}') for k,v in acc_dict.items()]\n¬† ¬† RMSE : 1.00\n¬† ¬† MAPE : 0.04\n¬† ¬† R-sq : 0.90\n\nfor-in loop format (see Loops &gt;&gt; Comprehension)\nprint returns ‚Äúnone‚Äù for each key:value at the bottom of the output for some reason. Assigning the print statement to a variable fixes it.\n\ndefaultdict\n\nCreates a key from a list element and groups the properties into a list of values where the value may also be a dict.\nFrom {{collections}}\nAlso see\n\nPybites video\nJSON &gt;&gt; Python &gt;&gt; Example: Parse Nested JSON into a dataframe\n\n\n\nSets\n\nIf performing set logic, always more performant to use sets instead of dicts or lists\n\n\nIf using numpy/pandas, using the .unique() syntax is more efficient for arrays/series‚Äô with numeric values\nIf using strings, it‚Äôs more efficient to use list(set(my_array))\n\n\nStrings\n\nOperators\nOperator Description\n%d Signed decimal integer\n%u unsigned decimal integer\n%c Character\n%s String\n%f Floating-point real number\nExample\n\nname = \"india\"\nage = 19\nmarks = 20.56\nstring1 = 'Hey %s' % (name)\nprint(string1)\nstring2 = 'my age is %d' % (age)\nprint(string2)\nstring3= 'Hey %s, my age is %d' % (name, age)\nprint(string3)\nstring3= 'Hey %s, my subject mark is %f' % (name, marks)\nprint(string3)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-pip",
    "href": "qmd/python-general.html#sec-py-gen-pip",
    "title": "General",
    "section": "pip",
    "text": "pip\n\nLooks for packages on https://pypi.org, downloads, and installs it\nMisc\n\nIf you installed python using the app-store, replace python with python3.\nDon‚Äôt use sudo to install libraries, since it will install things outside of the virtual environment.\nNor should you use ‚Äú‚Äìuser‚Äù, since it‚Äôs made to install things outside of the virtual environment.\nDon‚Äôt mix pip, venv and Anaconda. Avoid Anaconda if you can. If you have to use Anaconda, don‚Äôt use pip and venv. Limit yourself to Anaconda tools.\nIf you get SSL errors (common if you are in a hotel or a company network) use the ‚Äìtrusted-host pypi.org ‚Äìtrusted-host files.pythonhosted.org options with pip to work around the problem.\n\ne.g.¬†python -m pip install pendulum --trusted-host pypi.org --trusted-host files.pythonhosted.org\n\nIf you are behind a corporate proxy that requires authentication (common if you are in a company network), you can use the ‚Äìproxy option with pip to give the proxy address and your credentials.\n\ne.g.¬†python -m pip install pendulum --proxy http://your_username:yourpassword@proxy_address\nIt also works with the https_proxy environment variables\n\npip config set global.require-virtualenv True will only allow pip to install packages while a virtual environment is activated.\n\nProbably needs to be set back to False when updating pip.\nMay become the default in Python 3.13\n\n\nInstall library\n$ python -m pip install &lt;library_name&gt;\n\n# inside ipython or a colab notebook, \"!\" signifies a shell command\n!pip install &lt;library_name&gt;\nInstall library from github\npython -m pip install git+https://github.com/bbalasub1/glmnet_python.git@1.0\n\n‚Äú@1.0‚Äù is the version number\n\nUninstall library\n$ python -m pip uninstall &lt;library_name&gt;\n\nWon‚Äôt uninstall the dependencies of this library.\nIf you wish to also uninstall the unused dependencies as well, take a look at pip-autoremove\n\nRemove all packages in environment\n$ python -m pip uninstall -y -r &lt;(pip freeze)\nRemove all packages in environment but write the names of the packages to a requirements.txt file first\n$ python -m pip freeze &gt; requirements.txt && python3 -m pip uninstall -r         requirements.txt -y\nInstall requirements.txt\n$ python -m pip install -r requirements.txt\nWrite names of all the packages in your environment to a requirement.txt file\n$ python -m pip freeze &gt; requirements.txt\n\nWrites the specific version of the packages that you have installed in your environment (e.g.¬†pandas==1.0.0)\n\nThis may not be what you always want, so you‚Äôll need to manually change to just the library name in that case (e.g.¬†pandas)\n\nOnly aware of the packages installed using the pip install command\n\ni.e.¬†any packages installed using a different approach such as peotry, setuptools, condaetc. won‚Äôt be included in the final requirements.txt file.\n\nDoes not account for dependency versioning conflicts\nSaves all packages in the environment including those that are not relevent to the project\nIf you are not using a virtual environment, pip freeze generates a requirement file containing all the libraries in including those beyond the scope of your project.\n\nList your installed libraries\n$ python -m pip list\nSee if you have a particular library installed\n$ python -m pip list | grep &lt;library_name&gt;\nGet library info (name, version, summary, license, dependencies and other)\n$ python -m pip show &lt;library_name&gt;\nCheck that all installed packages are compatible\n$ python -m pip check\nUpdate package\n$ python -m pip install package_name --upgrade\nSearch for PyPI libraries (pip source for libraries)\n$ python -m pip search &lt;search_term&gt;\n\nreturns all libraries matching search term\n\nDownload a package without installing it\npython -m pip download &lt;library name&gt;\n\nIt will download the package and all its dependencies in the current directory (the files, called, wheels, have a .whl extension).\nYou can then install them offline by doing python -m pip install on the wheels.\n\nBuild Wheel archives for the libraries and dependencies in your environment\n$ python -m pip wheel\n\nI think these are binaries, so they don‚Äôt need compiled if installed in a future environment\nReal Python Tutorial\n\nManage configuration\n$ python -m pip config &lt;action name&gt;\n\nActions: edit, get, list, set or unset\nExample\n$ python -m pip config set global.index-url https://your.global.index\n\nDisplay debug information specific to pip\n$ python -m pip debug",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-anac",
    "href": "qmd/python-general.html#sec-py-gen-anac",
    "title": "General",
    "section": "Anaconda",
    "text": "Anaconda\n\nCheck configuration of current environment\nconda list\n\nShows python version used, package names installed and their versions\n\nInstall packages\nconda install &lt;package1&gt; &lt;package2&gt;\nInstall a package from a specific channel\nconda install &lt;package_name&gt; -c &lt;channel_name&gt; -y # Short form\nconda install &lt;package_name&gt; --channel &lt;channel_name&gt; -y # Long form\nPackage installation channels (some packages not available in default channel)\n\nCheck current channels\nconda config --show channels\n\nThe order in which these channels are displayed shows the channel priority.\n\nWhen a package is installed, anaconda will the check the channel at the top of list first then work it‚Äôs way down\n\n\nAdd a channel\nconda config --add channels conda-forge\n\nAdds ‚Äúconda-forge‚Äù to list of available channels\n\nRemove a channel\nconda config --remove channels conda-forge\n\nRemoves the ‚Äúconda-forge‚Äù channel",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-env",
    "href": "qmd/python-general.html#sec-py-gen-env",
    "title": "General",
    "section": "Environments",
    "text": "Environments\n\nMisc\n\nWhen you‚Äôre in a virtual environment\n\nAnytime you use the ‚Äúpython‚Äù command while your virtual environment is activated, it will be only the one from this env.\nIf you start a Python shell now, it will see only the things in the current directory, and the things installed in the virtual environment.\nIf you run a script, it will see only the things in the current directory, and the things installed in the virtual environment.\nIf you run a command, the command will be taken from the virtual environment.\nAnd they will only use exactly the version of Python of the virtual environment.\n\nStore environment files with project\nBreakage\n\nYou cannot move a virtual environment, it will stop working. Create a ‚Äúrequirements.txt‚Äù file, delete the virtual environment and create a new one.\nDon‚Äôt rename a directory containing a virtual environment. Or if you do, prepare yourself to create a ‚Äúrequirements.txt‚Äù file, delete the virtual environment and create a new one.\nIf you change the Python used in the virtual environment, such as when uninstalling it, it will stop working.\n\nCreate one big virtual environment for all small scripts.\n\nIf you make a lot of venv, you may be tempted to install everything at the system level for convenience. After all, it‚Äôs a bore to create and activate a virtual environment each time you want to write a five liner. A good balance is one single virtual environment you use for all things quick and dirty.\n\nCreate several virtual environments per versions of python if your project needs to support several versions. You may need several requirements.txt files as well, one for each env.\nRecommendations for a stable dependency environment for your project (article)\n\nDon‚Äôt install the latest major version of Python\n\nMaximum: 1 version under the latest version\nMinimum: 4 versions under the latest version (e.g.¬†latest = 3.11, min = 3.7)\n\nUse only the python.org installer on Windows and Mac, or official repositories on Linux.\nNever install or run anything outside of a virtual environment\nLimit yourself to the basics: ‚Äúpip‚Äù and ‚Äúvenv‚Äù\nIf you run a command, use ‚Äú-m‚Äù\n\nIt lets you run any importable Python module, no matter where you are. Because most commands are Python modules, we can use this to say, ‚Äúrun the module X of this particular python‚Äù.\nThere is currently no way for you to run any python command reliably without ‚Äú-m‚Äù.\nExamples:\n# Don't do :\npip install\n# Do:\npython -m pip install\n\n# Don't do :\nblack\n# Do:\npython -m black\n\n# Don't do :\njupyter notebook\n# Do:\npython -m jupyter notebook\n\nWhen creating a virtual environment, be explicit about which Python you use\n\nGet current python versions installed: py --list-paths (windows)\n\n\n\n\n\npyenv\n\nJust a simple Python version manager ‚Äî think {rig}\n{{pyenv}}, {{pyenv-win}}\nSet-up in RStudio (article)\nCompiles Python under the hood when you install it. But compiling can fail in a thousand ways\npyenv install --list: To see what python versions are available to install\npyenv install &lt;version number&gt;: To install a specific version\npyenv versions: To see what python versions are installed on your system\npyenv global &lt;version number&gt;: The set one python version as a global default\npyenv local &lt;version number&gt;: The set a python version to be used within a specific directory/project\\\n\n\n\npdm\n\nDocs\nPackage and dependency manager similar to npm. Doesn‚Äôt require virtual environments.\nFeatures: auto-updating pyproject.toml, isolating dependencies from dependencies-of-dependencies, active development and error handling\n\n\n\nvenv\n\nMisc\n\nShipped with Python\nDon‚Äôt mix pip, venv and Anaconda. Avoid Anaconda if you can. If you have to use Anaconda, don‚Äôt use pip and venv. Limit yourself to Anaconda tools.\n\nCreate\n\nWindows: py -&lt;py version&gt; -m venv &lt;env name&gt;\nMac/Linux: python3.8 -m venv .venv\n\nWhere the python version is 3.8 and the environment name is ‚Äú.venv‚Äù\nMac and Linux hide folders with names that have preceding ‚Äú.‚Äù by default, so make sure you have ‚Äúdisplay hiddent folders‚Äù activated or you won‚Äôt see it.\n\nNaming Environments\n\nName your environment directory ‚Äú.venv‚Äù, because:\n\nSome editors check for this name and automatically load it.\nIt contains ‚Äúvenv‚Äù, so it‚Äôs explicit.\nIt has a dot, so it‚Äôs hidden on Linux.\n\nIf you have more than one environment directory, use a suffix to distinguish them.\n\ne.g.¬†A project that must work with two different versions of Python (3.9 and 3.10), I will have a ‚Äú.venv39‚Äù and a ‚Äú.venv310‚Äù\n\nNaming enviroments for misc uses\n\n‚Äú.venv_test‚Äù: located in a personal directory to install new tools you want to play with. It‚Äôs disposable, and often broken, so you can delete it and recreate it regularly.\n‚Äú.venv_scripts‚Äù: Used for all the small scripts. You don‚Äôt want to create one virtual environment for each script, so centralize everything. It‚Äôs better than installing outside of a virtual environment, but is not a big constraint.\n\n\n\nActivate\n\nWindows: .venv\\Scripts\\activate\n\nWhere .venv is the name of the virtual environment\nMay need .bat as extension to activate\n\nMac/Linux: source .venv/bin/activate\n\nAfter that, you can use python -m pip install to install packages.\nDeactivate: deactivate\n\n\n\nvirtualenv\n\nDocs\nCreate a virtual environment\n python3 -m venv &lt;env_name&gt;\n\n-m venv tells python to run the virtual environment module, venv\nMake sure you‚Äôre in your projects directory\nRemember to add ‚Äú&lt;env_name&gt;/‚Äù to .gitignore\n\nActivate environment\nsource venv/bin/activate # Mac or Linux\nvenv\\Scripts\\activate # Windows\n\n&gt;&gt; (&lt;env_name&gt;) $\n\nPrompt should change if the environment is activated\nAll pip¬† installed packages will now be installed into the ‚Äú&lt;env_name&gt;/lib/python3.9/site-packages‚Äù directory\n\nUse the python contained within your virtual environment\npython main.py\n\nNot sure why you wouldn‚Äôt just activate the environment.\n\nDeactivate environment\ndeactivate\n\nno python¬† or env_name needed?\n\nReproducing environment\n\nDone using requirements.txt (see pip section for details on writing and installing)\n\nI don‚Äôt think the python version is included, so that will need to communicated manually\n\n\n\n\n\nAnaconda\n\nList environments\nconda env list # method 1\nconda info --envs # method 2\n\ndefault environment is called ‚Äúbase‚Äù\nActive environment will be in parentheses\nActive environment will be the one in the list with an asterix\n\nCreate a new conda environment\nconda create -n &lt;env name&gt;\nconda activate &lt;env name&gt;\nCreate a new conda environment with a specific python version\nconda create -n py310 python=3.10\nconda activate py310\nconda install jupyter jupyterlab\njupyter lab\n\nAlso install and launch jupyter lab\n\nCreate an environment from a yaml file\nconda env create -f environment.yml # Short form\nconda env create --file environment.yml # Long form\nRemove an environment\nconda deactivate &lt;env_name&gt; # Need to deactivate the environment first\nconda env remove -n &lt;env_name&gt;\n\nShould also delete environment folders (conda env list shows path to folders)\n\nClone an existing environment\nconda create -n testclone --clone test # Short form\nconda create --name testclone --clone test # Long form\n\n‚Äútestclone‚Äù is a copy of ‚Äútest‚Äù\n\nActivate an environment\nconda activate &lt;env_name&gt;\nActivate environment with reticulate in R\nreticulate::use_python(\"/usr/local/bin/python\")¬† ¬†\nreticulate::use_condaenv(\"&lt;env name&gt;\", \"/home/jtimm/anaconda3/bin/conda\")\nDeactivate an environment\nconda activate # Option 1: activates base\nconda deactivate test # Option 2\nExport the specifications of the current environment into a YAML file into the current directory\nconda env export &gt; environment.yml # Option 1\nconda env export -f environment.yml # Option 2\nExample: Conda workflow\n\nCreate an environment that uses a specific python version\n\nWithout a specified python version, the environment will use the same version as ‚Äúbase‚Äù\n\nconda create -n anothertest python=3.9.7 -y\n\n-n is the name flag and ‚Äúanothertest‚Äù is the name of the environment\nUses Python 3.9.7\nWithout the -y flag, there‚Äôd be a prompt you‚Äôd have to answer ‚Äúyes‚Äù to\n\nActivate the environment\nconda activate anothertest\nInstall packages\n\n\nInstalling packages one at time can lead to dependency conflicts.\nConda‚Äôs official documentation recommends to install all packages at the same time so that the dependency conflicts are resolved\nconda install \"numpy&gt;=1.11\" nltk==3.6.2 jupyter -y # install specific versions\nconda install numpy nltk jupyter -y # install all latest versions\n\nDo work and deactivate environment\nconda deactivate anothertest\n\nExample Raschka workflow\n# create & activate\nconda create¬† --prefix ~/code/myproj python=3.8\nconda activate ~/code/myproj\n# export env\nconda env export &gt; myproj.yml\n# create new env from yaml\nconda env create --file myproj.yml --prefix ~/code/myproj2\n\n\n\nPoetry\n\nDocs (like renv)\nApparently buggy (article)\npip‚Äôs dependency resolver is more flexible and won‚Äôt die on you if the package specifies bad metadata, while poetry‚Äôs strictness may mean you can‚Äôt install some packages at all.\nCreate project\n\npoetry new &lt;project-dir-name&gt;\n\nautomatically creates a directory for your project with a skeleton\n‚Äúpyproject.toml‚Äù maintains dependencies for the project with the following sections:\n\ntool.poetry provides an area to capture information about your project such as the name, version and author(s).\ntool.poetry.dependencies lists all dependencies for your project.\ntool.poetry.dev-dependencies lists dependencies your project needs for development that should not be present in any version deployed to a production environment.\nbuild-system references the fact that Poetry has been used to manage the project.\n\n\nAdd library and create lock file: poetry add &lt;library name&gt;\n\nWhen the first library is added, a ‚Äúpoetry.lock‚Äù file wil be generated\n\nActivate environment: poetry shell\n\nDeactivate environment: exit\n\nRun script: poetry run python my_script.py\nPackage the project: poetry build\n\nCreates tar.gz and wheel files (.whl) in ‚Äúdist‚Äù dir\n\nExample: poetry workflow (+pyenv, virtualenv)\n# Create a virtual environment called \"my-new-project\"\n# using Python 3.8.8\npyenv virtualenv 3.8.8 my-new-project\n# Activate the virtual environment\npyenv activate my-new-project\n\n{{pyenv}} - For managing the exact version of Python and activating the environment\nName your package the same name as the directory which is the same name as the virtual environment.\n\nDashes for the latter two and underscores for the package\n\nIntitialize the project and add packages (similar to renv) bash              poetry init     poetry add numpy\nReinstall dependencies\n# navigate to my project directory and run\npoetry install\nTurn off virtualenv management\n# right after installing poetry, run:\npoetry config virtualenvs.create false\n\nDefault poetry behavior is that it will manage your virtual environments for you. This may not be desirable because:\n\nCan‚Äôt just run a script from the command line. Instead, have to run poetry run my-script\n\nAwkward when you want to dockerize your code\n\nEnforces a virtual environment management framework on everybody in a shared codebase\nYour Makefile now needs to know about poetry",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-deps",
    "href": "qmd/python-general.html#sec-py-gen-deps",
    "title": "General",
    "section": "Dependencies",
    "text": "Dependencies\n\nMisc\n\nAfter mastering pip, {{pip-tools}} is recommended.\n\nGet a complete list of dependencies (e.g.¬†dependencies of dependencies) with {{deptree}}\ndeptree\n# output\nFlask==2.2.2¬† # flask\n¬† Werkzeug==2.2.2¬† # Werkzeug&gt;=2.2.2\n¬† ¬† MarkupSafe==2.1.1¬† # MarkupSafe&gt;=2.1.1\n¬† Jinja2==3.1.2¬† # Jinja2&gt;=3.0\n¬† ¬† MarkupSafe==2.1.1¬† # MarkupSafe&gt;=2.0\n¬† itsdangerous==2.1.2¬† # itsdangerous&gt;=2.0\n¬† click==8.1.3¬† # click&gt;=8.0\n# deptree and pip trees\n\nFlask depends on Werkzeug which depends on MarkupSafe\n\nWerkzeug and MarkupSafe qualify as transitive dependencies for this project\n\nCommented part on the right is the compatible range\n\nrequirements.txt format\n# comment\npandas==1.0.0\npyspark\npip: write names of all the packages in your environment to a requirement.txt file\n$ python3 -m pip freeze &gt; requirements.txt\n\nSee pip section for issues with this method\n\n{{pipx}}\n\nA tool for installing Python CLI utilities that gives them their own hidden virtual environment for their dependencies\nAdds the tool itself to your PATH - so you can install stuff without worrying about it breaking anything else\nInstall\npipx install datasette\n\n{{pipreqs}}\n\nScans all the python files (.py) in your project, then generates the requirements.txt file based on the import statements in each python file of the project\nSet-up: pip install pipreqs\nGenerate requirements.txt file: pipreqs /&lt;your_project_root_path&gt;/\nUpdate requirements.txt:¬† pipreqs --force /&lt;your_project_root_path&gt;/¬†\nIgnore the libraries of some python files from a specific subfolder\npipreqs /&lt;your_project_root_path&gt;/ --ignore  /&lt;your_project_root_path&gt;/folder_to_ignore/\n\n{{pip-compile-multi}}\n\nNotes from:\n\nEnd Python Dependency Hell with pip-compile-multi\n\nCreates and nests multiple requirement files\n\ne.g.¬†Able to keep dev environment from production environment separate\n\nAutoresolution of cross-requirement file conflicts\n\nDependency DAG (how all requirement files are connected) must have exactly one ‚Äúsink‚Äù node\n\nOrganize your most ubiquitous dependencies into a single ‚Äúcore‚Äù set of dependencies that all other nodes require (a source node), and all of your development dependencies in a node that requires all others (directly or indirectly) require (a sink).\n\nSimplifies and allows use of autoresolution functionality\n\nExample: DAG (directionality of the arrows is opposite compared to library docs)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-loadsav",
    "href": "qmd/python-general.html#sec-py-gen-loadsav",
    "title": "General",
    "section": "Loading/Saving",
    "text": "Loading/Saving\n\nMisc\n\n{{pickle}} needs custom class(es) to be defined in another module/file and then imported. Otherwise, PicklingError will be raised.\n\n\n\nFile paths\n\nMisc\n\n{{pathlib}} is recommended\n\n{{os}}\n\nGet current working directory: os.getcwd()\nList all files and directories in working directory: os.listdir()\nList all files and directories from a subdirectory: os.listdir(os.getcwd()+'\\\\01_Main_Directory')\nUsing os.walk(): gathers paths, folders, and files\n\nPaths\n\n\npath = os.getcwd()+'\\\\01_Main_Directory'\nfor folder_path, folders, files in os.walk(path):\n¬† ¬† print(folder_path)\nFolders\n\n\nSimilar code, just replace print(folder_path) with print(folders)\n\nFiles\n\n\n{{glob}}\n\nGet a file path string\nimport glob\npath = os.getcwd()+'\\\\01_Main_Directory'\nfor filepath in glob.glob(path):\n¬† ¬† print(filepath)\n# C:\\Users\\Suraj\\Challenges\\01_Main_Directory\nList all files and subdirectories from a path\npath = os.getcwd()+'\\\\01_Main_Directory\\\\*'\nfor filepath in glob.glob(path):\n¬† ¬† print(filepath)\n\nNote the * wildcard\n\nList all files and subdirectories with a ‚Äú1‚Äù in the name\npath = os.getcwd()+'\\\\01_Main_Directory\\\\*1.*'\nfor filepath in glob.glob(path):\n¬† ¬† print(filepath)\nGet a list of csv file paths from a directory: all_files = glob.glob(\"C:/Users/path/to/dir/*.csv\")\n\nNote that you don‚Äôt need a loop to save to an object\n\nList all files and subdirectories and files in those subdirectories\npath = os.getcwd()+'\\\\01_Main_Directory\\\\**\\\\*.txt'\nfor filepath in glob.glob(path, recursive=True):\n¬† ¬† print(filepath)\n#Output\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_2.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_3.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_4.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_5.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_1\\File_1_in_SubDict_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_1\\File_2_in_SubDict_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_2\\File_1_in_SubDict_2.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_2\\File_2_in_SubDict_2.txt\n\nComponents: ‚Äú**‚Äù and ‚Äúrecursive=True‚Äù\n\n\n{{pathlib}}\n\nProvides a single Path class with a range of methods (instead of separate functions) that can be used to perform various operations on a path.\nCreate a path object for a directory\nfrom pathlib import Path\npath = Path('origin/data/for_arli')\nCheck if a folder or a file is available in a given path\nif path.exists():\n¬† ¬† print(f\"[{path}]{style='color: #990000'} exists.\")\n¬† ¬† if path.is_file():\n¬† ¬† ¬† ¬† print(f\"[{path}]{style='color: #990000'} is a file.\")\n¬† ¬† elif path.is_dir():\n¬† ¬† ¬† ¬† print(f\"[{path}]{style='color: #990000'} is a directory.\")\nelse:\n¬† ¬† raise ValueError(f\"[{path}]{style='color: #990000'} does not exists\")\n\nChecks if the path ‚Äòorigin/data/for_arli‚Äô exists\n\nif it does, it will check whether it is a file or a directory.\nIf the path does not exist, it will print a raise an Error indicating that the path does not exist.\n\n\nList all files/folders in a path\nfor f in path.iterdir():\n¬† ¬† print(f)\n\nUse it in combination with the previous is_dir() and is_file()¬† methods to list either files or directories.\n\nDelete files/folders in a path\nfor f in path.iterdir():\n¬† ¬† f.unlink()\n\npath.rmdir()\n\nunlink deletes each file in the path\nrmdir deletes the directory.\n\ndirectory must be empty\n\n\nCreate a sequence of directories\n# existing directory: D:\\scripts\\myfolder\np = Path(\"D:\\scripts\\myfolder\\logs\\newfolder\")\np.mkdir(parents=True, exist_ok=True)\n\nCreate path object with desired sequence of directories (e.g.¬†logs\\newfolder)\nmkdir with parents=True creates the sequence of directories\n\nW/exist_ok=True no error with occur if the directory already exists\n\n\nRename directory: path.rename('origin/data/new_name')\nConcatenate a path with string\npath = Path(\"/origin/data/for_arli\")\n# Join another path to the original path\nnew_path = path.joinpath(\"la\")\nprint(new_path) # prints 'origin/data/for_arli/bla'\n\nIt also handles the join between two Path objects\n\nDirectory stats\nprint(path.stat()) # print statistics¬†\nprint(path.owner()) # print owner\n\ne.g.¬†creation time, modification time, etc.\n\nWrite to a file\n# Open a file for writing\npath = Path('origin/data/for_arli/example.txt')\nwith path.open(mode='w') as f:\n¬† ¬† # Write to the file\n¬† ¬† f.write('Hello, World!')\n\nYou do not need to create manually example.txt.\n\nRead a file\npath = Path('example.txt')\nwith path.open(mode='r') as f:\n¬† ¬† # Read from the file\n¬† ¬† contents = f.read()\n¬† ¬† print(contents) # Output: Hello World!\n\n\n\n\nModels\n\nSaving and Loading an estimator as a binary using {{joblib}} (aside: pipelines are estimators)\nimport joblib\n#saving the pipeline into a binary file\njoblib.dump(pipe, 'wine_pipeline.bin')\n#loading the saved pipeline from a binary file\npipe = joblib.load('wine_pipeline.bin')\nSaving and loading a trained model as a pickle file\nimport pickle\n# open file connection\npickle_file = open('model.pkl', 'ab')\n# save the model\npickle.dump(model_obj, pickle_file)\n# close file connection¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\npickle_file.close()\n\n# Open conn and save\ntest_dict = {\"Hello\": \"World!\"}\nwith open(\"test.pickle\", \"wb\") as outfile:\n# \"wb\" argument opens the file in binary mode\npickle.dump(test_dict, outfile)\n\n# open file connection\npickle_file = open('model.pkl', 'rb')\n# load saved model\nmodel = pickle.load(pickle_file)\n\n# open conn and load\n# Deserialization\nwith open(\"test.pickle\", \"rb\") as infile:\n¬† ¬† test_dict_reconstructed = pickle.load(infile)\n\nCan serialize almost everything including classes and functions\n\n\n\n\nEnvironment Variables\n\n{{os}}\n\nCheck existence\nenv_var_exists = 'ENV' in os.environ\n# or\nenv_var_exists = os.environ.has_key('ENV')\nList environment variables: print(os.environ)\nLoading\nimport os\n# Errors when not present\nenv_var = os.environ['ENV'] # where ENV is the name of the environment variable\n# Returns None when not present\nenv_var = os.environ.get('ENV', 'DEFAULT_VALUE') # using default value is optional\nSet/Export or overwrite\nos.environ['ENV'] = 'dev'\nLoad or create if not present\ntry:\n¬† ¬† env_var = os.environ['ENV']\nexcept KeyError:\n¬† ¬† os.environ['ENV'] = 'dev'\nDelete\nif 'ENV' in os.environ:\n¬† ¬† del os.environ['ENV']\n\n{{python-decouple}}\n\nAccess environment variables from whatever environment it is running in.\nCreate a .env file in the project root directory: touch .env\nOpen .env in nano text editor: nano .env\n\nNano text editor is pre-installed on macOS and most Linux distros\nCheck if installed/version: nano --version\nBasic usage tutorial\n\nAdd environment variables to file\nUSER=alex\nKEY=hfy92kadHgkk29fahjsu3j922v9sjwaucahf\n\nSave: Ctrl+o\nExit: Ctrl+x\n\n* Add .env to your .gitignore file *\nAccess\nfrom decouple import config\nAPI_USERNAME = config('USER')\nAPI_KEY = config('KEY')\n\n{{python-dotenv}}\n\nReads .env files\nProbably more popular than {{python-decouple}}\nHas a companion R package, {dotenv}, so .env files can be used in projects that use both R and Python.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-funs",
    "href": "qmd/python-general.html#sec-py-gen-funs",
    "title": "General",
    "section": "Functions",
    "text": "Functions\n\nMisc\n\nBenchmarking a function\n\nUsing IPython function\n%time dat['col1001'] = some_function(dat['col1'], dat['col2'], dat['col3'])\n\n%%time prints the wall time for the entire cell whereas %time gives you the time for first line only\n\nUsing a decorator\n\n\nAssigning functions based on arg type\ndef process_data(data):\n¬† ¬† if isinstance(data, dict):\n¬† ¬† ¬† ¬† process_dict(data)¬†\n¬† ¬† else:\n¬† ¬† ¬† ¬† process_list(data)¬†\ndef process_dict(data: dict):\n¬† ¬† print(\"Dict is processed\")\ndef process_list(data: list):\n¬† ¬† print(\"List is processed\")\n\nAssigns data to a particular function depending on whether it‚Äôs a dict or a list\nisinstance checks that the passed argument is of the proper type or a subclass\n\nWrapping functions\nfrom functools import partial\nget_count_df = partial(get_count, df=df)\n\nWraps function to make df the default value for df arg\n\n\n\n\nDocumentation\n\nFunctions should at least include docstrings and type hinting\nDocstrings\n\nTypes: Google-style, Numpydoc, reStructured Text, EpyTex\nInformation to include\n\nFunction description, arg description, return value description, Description of errors, Optional extra notes or examples of usage.\n\nAccess functions docstring:\n\nprint(func_name.__doc__)\nFor large docstrings\nimport inspect\nprint(inspect.getdoc(func_name))\n\nExample: Google-style\ndef send_request(key: str, lat: float = 0, lon: float = 0):\n¬† ¬† \"\"\"Send a request to Climacell Weather API\n¬† ¬† to get weather info based on lat/lon.\n\n¬† ¬† Climacell API provides realtime weather\n¬† ¬† information which can be accessed using\n¬† ¬† their 'Realtime Endpoint'.\n\n¬† ¬† Args:\n¬† ¬† ¬† key (str): an API key with length of 32 chars.\n¬† ¬† ¬† lat (float, optional): value for latitude.\n¬† ¬† ¬† ¬† Default=0\n¬† ¬† ¬† lon (float, optional): value for longitude.\n¬† ¬† ¬† ¬† Default=0\n\n¬† ¬† Returns:\n¬† ¬† ¬† int: status code of the result¬†\n¬† ¬† ¬† dict: Result of the call as a dict\n\n¬† ¬† Notes:\n¬† ¬† ¬† See https://www.climacell.co/weather-api/¬†\n¬† ¬† ¬† for more info on Weather API. You can get\n¬† ¬† ¬† API key from there, too.\n¬† ¬† \"\"\"\n\nFirst sentence should contain the purpose of the function\n\nExample: Numpydoc\ndef send_request(key: str, lat: float = 0, lon: float = 0):\n¬† ¬† \"\"\"\n¬† ¬† Send a request to Climacell Weather API\n¬† ¬† to get weather info based on lat/lon.\n\n¬† ¬† Climacell API provides realtime weather\n¬† ¬† information which can be accessed using\n¬† ¬† their 'Realtime Endpoint'.\n\n¬† ¬† Parameters\n¬† ¬† ----------\n¬† ¬† ¬† key (str): an API key with length of 32 chars.\n¬† ¬† ¬† lat (float, optional): value for latitude.\n¬† ¬† ¬† ¬† Default=0\n¬† ¬† ¬† lon (float, optional): value for longitude.\n¬† ¬† ¬† ¬† Default=0\n\n¬† ¬† Returns\n¬† ¬† -------\n¬† ¬† ¬† int: status code of the result¬†\n¬† ¬† ¬† dict: Result of the call as a dict\n\n¬† ¬† Notes\n¬† ¬† -----\n¬† ¬† ¬† See https://www.climacell.co/weather-api/¬†\n¬† ¬† ¬† for more info on Weather API. You can get\n¬† ¬† ¬† API key from there, too.\n¬† ¬† \"\"\"\n\nType Hinting\n\nThis doesn‚Äôt check the type; it‚Äôs just metadata\n\nsee isinstance (see below), NotImplementedError (see below), or {{typecheck}} and {{mypy} (see bkmks) for type checking that will throw errors\n\nUsing type hints enables you to perform type checking. If you use an IDE like PyCharm or Visual Studio Code, you‚Äôll get visual feedback if you‚Äôre using unexpected types:\nVariables: my_variable_name: tuple[int, ...]\n\nvariable should be a tuple that contains only integers. The ellipsis says the total quantity is unimportant.\n\nFunctions\ndef get_count(threshold: str, column: str, df: pd.DataFrame) -&gt; int:\n¬† ¬† return (df[column] &gt; threshold).sum()\n\n‚Äúthreshold‚Äù, ‚Äúcolumn‚Äù should be strings (str)\n‚Äúdf‚Äù should be a pandas dataframe (pd.DataFrame)\nOutput should be an integer (int)\n\nFunction as an arg: Callable[[Arg1Type, Arg2Type], ReturnType]\n\nExample:\nfrom collections.abc import Callable\ndef foo(bar: Callable[[int, int], int], a: int, b: int) -&gt; int:\n¬† ¬† return bar(a, b)\n\n‚Äúbar‚Äù is a function arg for the function, ‚Äúfoo‚Äù\n‚Äúbar‚Äù is supposed to take: 2 integer args ([int, int]) and return an integer (int)\n\nExample:\ndef calculate(i: int, action: Callable[..., int], *args: int) -&gt; int:\n¬† ¬† return action(i, *args)\n\n‚Äúaction‚Äù takes any number and type of arguments but must return an integer.\nWith *args: int, you also allow a variable number of optional arguments, as long as they‚Äôre integers.\n\nExample: Lambda\nf: Callable[[int, int], int] = lambda x, y: 3*x + y\n\nMay not work\n\n\n\n\n\n\nArgs and Operators\n\nMisc\n\n** Args are not reset to default values after each call **\n\nExample:\n\ndef func(list1=[]):¬† ¬† ¬† # here l1 is a default argument set to []\n¬† ¬† list1.append(\"Temp\")\n¬† ¬† return list1\n\n‚ÄúNone‚Äù + conditional must be used to get the arg to reset back to the default value\n\ndef func(l1=None):¬† ¬† ¬†\n¬† ¬† if l1 is None:¬†\n¬† ¬† ¬† ¬† l1 = []\n¬† ¬† l1.append(\"Temp\")¬†\n¬† ¬† return l1\n\n*\n\nUnpacks Lists\n\nnum_list = [1,2,3,4,5]\nnum_list_2 = [6,7,8,9,10]\n\nprint(*num_list)\n# 1 2 3 4 5\nnew_list = [*num_list, *num_list_2] # merge multiple lists\n# [1,2,3,4,5,6,7,8,9,10]\n*args\n\nFunctions that can accept a varying number of values\n\ndef names_tuple(*args):\n¬† ¬† return args\n\nnames_tuple('Michael', 'John', 'Nancy')\n# ('Michael', 'John', 'Nancy')\nnames_tuple('Jennifer', 'Nancy')\n# ('Jennifer', 'Nancy')\n**\n\nUnpacks Dictionaries\n\nnum_dict = {‚Äòa‚Äô: 1, ‚Äòb‚Äô: 2, ‚Äòc‚Äô: 3}\nnum_dict_2 = {‚Äòd‚Äô: 4, ‚Äòe‚Äô: 5, ‚Äòf‚Äô: 6}\n\nprint(*num_dict) # only keys printed\n# a b c\nnew_dict = {**num_dict, **num_dict_2} # merge dictionaries\n# {‚Äòa‚Äô: 1, ‚Äòb‚Äô: 2, ‚Äòc‚Äô: 3, ‚Äòd‚Äô: 4, ‚Äòe‚Äô: 5, ‚Äòf‚Äô: 6}\n**kwargs\n\nFunctions that can accept a varying number of variable/value pairs (like a ‚Ä¶ in R)\n\ndef names_dict(**kwargs):\n¬† ¬† return kwargs\n\nnames_dict(Jane = 'Doe')\n# {'Jane': 'Doe'}\nnames_dict(Jane = 'Doe', John = 'Smith')\n# {'Jane': 'Doe', 'John': 'Smith'}\nFunction as an arg\ndef classic_boot(df, estimator, seed=1):\n¬† ¬† df_boot = df.sample(n=len(df), replace=True, random_state=seed)\n¬† ¬† estimate = estimator(df_boot)\n¬† ¬† return estimate\n\nBootstrap function with an ‚Äúestimator‚Äù function (e.g.¬†mean) as arg\nUsing a Callable\n\nClass as an arg\nfrom dataclasses import dataclass\n\n@dataclass\nclass Person:\n    name: str\n    age: int\n    address: str\n    phone: str\n    email: str\n\ndef process_data(person: Person):\n    print(f\"Processing data for {person.name}, {person.age}, living at {person.address}. Contact info: {person.phone}, {person.email}\")\n\nperson = Person(\"Alice\", 30, \"123 Main St\", \"555-1234\", \"alice@example.com\")\nprocess_data(person)\n\nMakes function more readable when the function requires a bunch of args\n\nClass as an arg (safer alternative)\nfrom typing import NamedTuple\n\nclass Person(NamedTuple):\n    name: str\n    age: int\n    address: str\n    phone: str\n    email: str\n\ndef process_data(person: Person):\n    print(f\"Processing data for {person.name}, {person.age}, living at {person.address}. Contact info: {person.phone}, {person.email}\")\n\nperson = Person(\"Alice\", 30, \"123 Main St\", \"555-1234\", \"alice@example.com\")\nprocess_data(person)\n\nUsing NamedTuple means that the attributes cannot be overridden\n\ne.g.¬†Executing person.name = \"Bob\" will result in an error because tuples can‚Äôt be modified.\n\n\nMake an arg optional\nlass Address:\n¬† ¬† def __init__(self, street, city, state, zipcode, street2=''):\n¬† ¬† ¬† ¬† self.street = street\n¬† ¬† ¬† ¬† self.street2 = street2\n¬† ¬† ¬† ¬† self.city = city\n¬† ¬† ¬† ¬† self.state = state\n¬† ¬† ¬† ¬† self.zipcode = zipcode\n\n‚Äústreet2‚Äù has default value of an empty string, so it‚Äôs optional\n\n\n\n\nLambda\n\nUseful if you just have 1 expression that you need to execute.\nBest Practices\n\nlambda is an anonymous function, hence it is not a good idea to store it in a variable for future use\nDon‚Äôt use lambdas for single functions (e.g.¬†sqrt). Make sure it‚Äôs an expression.\n\nExample\n# bad\nsqrt_list = list(map(lambda x: math.sqrt(x), mylist))\n# good\nsqrt_list = list(map(math.sqrt, mylist))\n\nAffects performance\n\n\nDon‚Äôt use for complex expressions that require more than 1 line (meh)\n\nPer PEP8 guidelines, Limit all lines to a maximum of 79 characters\nExample\n# bad (118 characters)\ndf[\"FinalStatus\"] = df[\"Status\"].map(lambda x: 'Completed' if x ==\n'Delivered' or x == 'Shipped' else 'Not Completed')\n# instead\ndf[\"FinalStatus\"] = ''\ndf.loc[(df[\"Status\"] == 'Delivered') |\n¬† ¬† ¬† (df[\"Status\"] == 'Shipped'),\n¬† ¬† ¬† 'FinalStatus'] = 'Completed'\ndf.loc[(df[\"Status\"] == 'Not Delivered') |\n¬† ¬† ¬† (df[\"Status\"] == 'Not Shipped'),\n¬† ¬† ¬† 'FinalStatus'] = 'Not Completed'\n\n\nExample: 1 arg\n# py\nlambda x: np.sin(x / period * 2 * np.pi)\n# r\n~sin(.x / period * 2 * pi)\n# r\n\\(x) {sin(x / period * 2 * pi)}\nExample: 2 args\nGreater = lambda x, y : x if(x &gt; y) else y\nGreater(0.002, 0.5897)\nLambda-Filter\n\nFaster than a comprehension\n\nsee Loops &gt;&gt; Comprehensions\n\nFormat: filter(function, data_object)\n\nReturns a filter object, which needs to be converted into data structure such as list or set\n\nExample: Basic\nyourlist = list(np.arange(2,50,3))\nlist(filter(lambda x:x**2&lt;100, yourlist))\n# Output¬†\n[2, 5, 8]\nExample: Filter w/logical\nimport pandas as pd\nimport datetime as dt\n# create a list of 10,000 dates\ndatlist = pd.date_range(dt.datetime.today(), periods=10000).tolist()¬†\n# convert the dates to strings via list comprehension\ndatstrlist = [d.strftime(\"Day %d in %B of year %Y is a %A\") for d in datlist]\ndatstrlist[:4]\n['Day 21 in October of year 2021 is a Thursday', 'Day 22 in October of year 2021 is a Friday', 'Day 23 in October of year 2021 is a Saturday', 'Day 24 in October of year 2021 is a Sunday']\n\nstrLamb = filter(lambda d: ((d.endswith(\"urday\") or d.endswith(\"unday\")) and \"Oc\" in d), datstrlist)\n\nSearches for Saturdays and Sundays in the month of October of all years in list of strings\n\nExample: Nested Lists\ngroup1 = [1,2,3,43,23,42,8,3,7]\ngroup2 = [[3, 34, 23, 32, 42], [6, 11, 9], [1, 3,9,7,2,8]]\n[list(filter(lambda x: x in group1, sublist)) for sublist in group2]\n&gt;&gt; [[3, 23, 42], [], [1, 3, 7, 2, 8]]\n\nProbably useful for json\nfor-loop attached to the end of the list-filter combo\nEach sublist of group 2 is fed into the lambda-filter and compared to the group 1 list\n\n\nIterating over each element of a list\n\nExample: map\nlist(map(lambda x: x**2+x**3, yourlist))\n\nmap returns a map object that needs to be converted\n\nExample: 2 Lists\nmylist = list(np.arange(4,52,3))\nyourlist = list(np.arange(2,50,3))\nlist(map(lambda x,y: x**2+y**2, yourlist, mylist))\n\nLike a pmap\n\n\nNested lambdas\n\nExample: map\narr = [1,2,3,4,5]\nlist(map(lambda x: x*2, filter(lambda x: x%2 == 0, arr)))\n&gt;&gt; [4,8]\n\nWork inside out (locate where the data object, arr, appears)\n‚Äúarr‚Äù is filtered by the first lambda function for even numbers then iterated by map to be squared by the second lambda function\n\n\nIterate over rows of a column in a df\n\nExample: Using formula over rows\ngrade['evaluate']=grade['MathScore'].apply(lambda x: round((x**x)/2,2))\n\n‚Äúgrade‚Äù is the df; ‚ÄúMathScore‚Äù is a numeric column; ‚Äúevaluate‚Äù is the new column in the df\nFormula applied to each value of ‚ÄúMathScore‚Äù to generate each value of evaluate\n\nExample: Conditional over rows\ngrade['group']=grade['MathScore'].apply(lambda x: 'Excellent' if x&gt;=3.0 else 'Average')\n\n‚Äúgrade‚Äù is the df; ‚ÄúMathScore‚Äù is a numeric column; ‚Äúgroup‚Äù is the new column in the df\nConditional applied to each value of ‚ÄúMathScore‚Äù to generate each value of ‚Äúgroup‚Äù\n\nUsing {{swifter}} for parallelization\nimport swifter\ndf['e'] = df.swifter.apply(lambda x: infer(x['a'], x['b'], x['c'], x['d']), axis = 1)\n\nIn a Pivot Table (like a crosstab)\n\nExample\n\ngrades_df\n\n2 names (‚Äúname‚Äù)\n6 scores (‚Äúscore‚Äù)\nOnly 2 letter grades associated with these scores (‚Äúletter grade‚Äù)\n\nTask: drop lowest score for each letter grade, then calculate the average score for each letter grade\n\ngrades_df.pivot_table(index='name',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† columns='letter grade',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† values='score',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† aggfunc = lambda series : (sorted(list(series))[-1] + sorted(list(series))[-2]) / 2)\n\nletter grade¬† ¬† A¬† ¬† B\nname\nArif¬† ¬† ¬† ¬† ¬† 96.5¬† 87.0\nKayla¬† ¬† ¬† ¬† 95.5¬† 84.0\n\nindex: each row will be a ‚Äúname‚Äù\ncolumns: each column will be a ‚Äúletter grade‚Äù\nvalues: value in the cells will be from the ‚Äúscore‚Äù column according to each combination columns in the index and columns args\naggfunc: uses a lambda to compute the aggregated values\n\n‚Äúseries‚Äù is used a the variable¬† in the lambda function\nsorts series (ascending), takes the top two values (using negative list indexing), and averages them\n\n\n\n\n\n\nScope\n\nPopulated objects within functions persist if you instantiate the object in the argument\n\n\n‚Äúall_numbers‚Äù retained it‚Äôs previous value when the 2nd call to the function was made\n\n\n\n\nClosures\n\nInner functions that can access values in the outer function, even after the outer function has finished its execution\nExample\n\n# closure way\ndef balanceOwed(roomN,rate,nights):\n¬† ¬† def increaseByMeals(extra):\n¬† ¬† ¬† ¬† amountOwned=rate*nights+extra\n¬† ¬† ¬† ¬† print(f\"Dear Guest of Room [{roomN}]{style='color: #990000'}, you have\",¬†\n¬† ¬† ¬† ¬† \"a due balance:\", \"${:.2f}\".format(amountOwned))\n¬† ¬† ¬† ¬† return amountOwned\n¬† ¬† return increaseByMeals\n\nba = balanceOwned(201,400,3)\nba(200)\nba(150)\nba(180)\nba(190)\nDear Guest of Room 201, you have a due balance: $1400.00\nDear Guest of Room 201, you have a due balance: $1350.00\nDear Guest of Room 201, you have a due balance: $1380.00\nDear Guest of Room 201, you have a due balance: $1390.00\n\nTedious way: For each value of ‚Äúextra‚Äù (e.g.¬†meals), the function needs to be called even if the other values of the arguments don‚Äôt change.\nClosure way:\n\nincreaseByMeals() is a closure function, because it remembers the values of the outer function balanceOwed(), even after the execution of the latter\nbalanceOwed() is called with its three arguments only once and then after its execution, we call it four times with the meal expenses (‚Äúextra‚Äù).",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-mods",
    "href": "qmd/python-general.html#sec-py-gen-mods",
    "title": "General",
    "section": "Modules",
    "text": "Modules\n\n.py files are called ‚Äúmodules.‚Äù\nA directory with .py files in which one of the files is an ‚Äú__init__.py‚Äù is called a package.\nMisc\n\nResource: Make your Python life easier by learning how imports find things\nsys.path contains the list of paths where Python is looking for things to import. Your virtual environment and the directory containing your entry point are automatically added to sys.path.\n\nsys.path¬†is a list. Which means you can¬†.append(). Any directory you add there will have its content importable. It‚Äôs a useful hack, but use it as a last resort.\n\nWhen using -m flag to run a script, if you pass a package instead of a module, the package must contain a ‚Äú__main__.py‚Äù file for it to work. This __main__.py module will run.\nIf you have scripts in your projects, don‚Äôt run them directly. Run them using ‚Äú-m‚Äù, and you can assume everything starts from the root.\n\nExample:\ntop_dir\n‚îú‚îÄ‚îÄ foo\n‚îÇ   ‚îú‚îÄ‚îÄ bar.py\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îî‚îÄ‚îÄ blabla.py\n‚îî‚îÄ‚îÄ blabla.py\n\nRunning python foo/bar.py, ‚Äútop_dir‚Äù is the current working directory, but ‚Äúfoo‚Äù is added to the sys.path.\nRunning python -m foo.bar, ‚Äútop_dir‚Äù is the current working directory and added to sys.path.\n\nImports can all start from the root of the project and opened file paths as well.\n\n\n\n\nUsage\n\nProject Structure\n‚îú‚îÄ‚îÄ main.py\n‚îú‚îÄ‚îÄ packages\n‚îÇ¬† ‚îî‚îÄ‚îÄ __init__.py\n‚îÇ¬† ‚îî‚îÄ‚îÄ module_1.py\n‚îÇ¬† ‚îî‚îÄ‚îÄ module_2.py\n‚îÇ¬† ‚îî‚îÄ‚îÄ module_3.py\n‚îî‚îÄ‚îÄ ‚îî‚îÄ‚îÄ module_4.py\n\n‚Äú__init__.py‚Äù contains only 1 line which declares all the functions (or classes?) that are in the modules\n__all__ = [\"func1\", \"func2\"]\n\nIf the module files contained classes with multiple functions, I think you‚Äôd just declare the classes and not every function in that class.\n\nIf using classes, each module should only have 1 class.\n\n\nScripts need to include ‚Äú_main_‚Äù in order to used in other scripts\n# test_function.py\ndef function1():¬†\n¬† ¬† print(\"Hello world\")¬†\nfunction1()\n\n# Define the __main__ script\nif __name__ == '__main__':¬† ¬†\n¬† ¬† # execute only if run as a script\n¬† ¬† function1()\n\nSays if this file is being run non-interactively (i.e.¬†as a script), run this chunk\nAdd else: chunk, then that chunk will be run only if the file is imported as a module\nAllows you to allow or prevent parts of code from being run when the modules are imported\nImporting a module without _main_ in a jupyter notebook results in this\n\n\nLoading\n\nDO NOT USE from &lt;library&gt; import *\n\nThis will import anything and everything from that library and causes several problems:\n\nYou don‚Äôt know what is in that package, so you have no idea what you just imported, or even if what you want is in there.\nYou just filled your local namespace with an unknown quantity of mysterious names, and you don‚Äôt know what they will shadow.\nYour editor will have a hard time helping you since it doesn‚Äôt know what you imported.\nYour colleague will hate you because they have no idea what variables come from where.\n\nException: In the shell, it‚Äôs handy. Sometimes, you want to import all things in __init__.py and you have ‚Äú__all__‚Äù defined (see above)\n\nFrom the working directory, it‚Äôs like importing from a library: from file1 import function1\nFrom a subdirectory, from subdirectory.file1 import function1\nFrom a directory outside the project, add the module to sys.path before importing it\nimport sys\nsys.path.append('/User/moduleDirectory')\n\nWhen a module is imported, it first searches for built-in modules, then the paths listed in sys.path\nThis appends the new path to the end of the sys.path\nimport sys\nsys.path.insert(1, '/User/moduleDirectory')\nPuts this path at the front of the sys.path directory list.\nimport sys\nsys.path.remove('/User/NewDirectory')\n\n*delete path from sys.path after you finish*\nPython will also search this path for future projects unless they are removed",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-cond",
    "href": "qmd/python-general.html#sec-py-gen-cond",
    "title": "General",
    "section": "Conditionals",
    "text": "Conditionals\n\nIf-Else\n\nSyntax\nif &lt;expression&gt;:\n¬† ¬† do something\nelse:\n¬† ¬† do something else\nExample\nregenerate = False\nif regenerate:\n    concepts_list = df2Graph(df, model='zephyr:latest')\n    dfg1 = graph2Df(concepts_list)\n    if not os.path.exists(outputdirectory):\n        os.makedirs(outputdirectory)\n\n    dfg1.to_csv(outputdirectory/\"graph.csv\", sep=\"|\", index=False)\n    df.to_csv(outputdirectory/\"chunks.csv\", sep=\"|\", index=False)\nelse:\n    dfg1 = pd.read_csv(outputdirectory/\"graph.csv\", sep=\"|\")\n\nTry-Except\n\nExample\nimport os\ntry:\n¬† ¬† env_var = os.environ['ENV']\nexcept KeyError:\n¬† ¬† # Do something\n\nIf ‚ÄúENV‚Äù is not a present a KeyError is thrown. Then, except section executed.\n\n\nMatch (&gt; Python 3.10) (switch function)\nmatch object:\n¬† ¬† case &lt;pattern_1&gt;:\n¬† ¬† ¬† ¬† &lt;action_1&gt;\n¬† ¬† case &lt;pattern_2&gt;:\n¬† ¬† ¬† ¬† &lt;action_2&gt;\n¬† ¬† case &lt;pattern_3&gt;:\n¬† ¬† ¬† ¬† &lt;action_3&gt;\n¬† ¬† case _:\n¬† ¬† ¬† ¬† &lt;action_wildcard&gt;\n\n‚Äúobject‚Äù is just a variable name; could be anything\n‚Äúcase_‚Äù is the value used when none of the other cases are a match\nExample: function input inside user function\ndef http_error(status):\n¬† ¬† match status:\n¬† ¬† ¬† ¬† case 200:\n¬† ¬† ¬† ¬† ¬† ¬† return 'OK'\n¬† ¬† ¬† ¬† case 400:\n¬† ¬† ¬† ¬† ¬† ¬† return 'Bad request'\n¬† ¬† ¬† ¬† case 401 | 403 | 404:\n¬† ¬† ¬† ¬† ¬† ¬† return 'Not allowed'\n¬† ¬† ¬† ¬† case _:\n¬† ¬† ¬† ¬† ¬† ¬† return 'Something is wrong'\nExample: dict input inside a function\ndef get_service_level(user_data: dict):\n¬† ¬† match user_data:\n¬† ¬† ¬† ¬† case {'subscription': _, 'msg_type': 'info'}:\n¬† ¬† ¬† ¬† ¬† ¬† print('Service level = 0')\n¬† ¬† ¬† ¬† case {'subscription': 'free', 'msg_type': 'error'}:\n¬† ¬† ¬† ¬† ¬† ¬† print('Service level = 1')\n¬† ¬† ¬† ¬† case {'subscription': 'premium', 'msg_type': 'error'}:\n¬† ¬† ¬† ¬† ¬† ¬† print('Service level = 2')\nExample: inside a class\nclass ServiceLevel:\n¬† ¬† def __init__(self, subscription, msg_type):\n¬† ¬† ¬† ¬† self.subscription = subscription\n¬† ¬† ¬† ¬† self.msg_type = msg_type\n\n¬† ¬† def get_service_level(user_data):\n¬† ¬† ¬† ¬† match user_data:\n¬† ¬† ¬† ¬† ¬† ¬† case ServiceLevel(subscription=_, msg_type='info'):\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print('Level = 0')\n¬† ¬† ¬† ¬† ¬† ¬† case ServiceLevel(subscription='free', msg_type='error'):\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print('Level = 1')\n¬† ¬† ¬† ¬† ¬† ¬† case ServiceLevel(subscription='premium', msg_type='error'):\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print('Level = 2')\n¬† ¬† ¬† ¬† ¬† ¬† case _:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print('Provide valid parameters')\n\nNote that inside the function, the change from ‚Äú:‚Äù to ‚Äú=‚Äù¬† and ‚Äú()‚Äù following the class name in the ‚Äúcase‚Äù portion of the match\n\n\nAssert\n\nUsed to confirm a condition\n\nIncorrect: assert condition, message¬†\n\nCorrect method:¬†\nif not condition:¬†\n¬† ¬† raise AssertionError\n\nassert is useful for debugging code because it lets you test if a condition in your code returns True, if not, the program will raise an AssertionError.\n** Do not use in production, because when code is executed with the -O (optimize) flag, the assert statements are removed from the bytecode. **",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-loops",
    "href": "qmd/python-general.html#sec-py-gen-loops",
    "title": "General",
    "section": "Loops",
    "text": "Loops\n\nMisc\n\nList Comprehensions vs Generators in terms of memory usage\n\n{{tqdm}} - progress bar for loops\nfrom tqdm import tqdm\nfor i in tqdm(range(10000))\n¬† ¬† ...\nbreak terminates the loop containing it\n\nIf in a nested loop, it will terminate the inner-most loop containing it\n\ncontinue is used to skip the remaining code inside a loop for the current iteration only; forces the start of the next iteration of the loop\npass does nothing\n\nused when a statement or a condition is required to be present in the program but we do not want any command or code to execute\n\n\n\n\nIterators\n\nRemembers values\nExample\nD = {\"123\":\"Y\",\"111\":\"PT\",\"313\":\"Y\",\"112\":\"Y\",\"201\":\"PT\"}\nff = filter(lambda e:e[1]==\"Y\", D.items())\n\nprint(next(ff))\n&gt;&gt; ('123', 'Y')\nprint(next(ff))\n&gt;&gt; ('313', 'Y')\napply\n\naxis\n\n0 or ‚Äòindex‚Äô: apply function to each column.\n1 or ‚Äòcolumns‚Äô: apply function to each row.\n\nExample: Function applied to rows of a column of a dataframe (i.e.¬†cells)\ndef df2Graph(dataframe: pd.DataFrame, model=None) -&gt; list:\n  # dataframe.reset_index(inplace=True)\n  results = dataframe.apply(\n    lambda row: graphPrompt(row.text, {\"chunk_id\": row.chunk_id}, model), axis=1\n  )\n\ntext and chunk_id are column names of the dataframe\nrow is the row of the dataframe since axis=1, and from that row, the columns text and chunk_id are subsetted in the arguments of user-defined function.\n\nExample: Formula applied to rows of a column of a dataframe (i.e.¬†cells)\ngrade['evaluate']=grade['MathScore'].apply(lambda x: round((x**x)/2,2))\n\ngrade is the df; MathScore is a numeric column; evaluate is the new column in the df\n\n\n\n\n\nGenerators\n\nGenerators are iterators, a kind of iterable you can only iterate over once. (normal iterators like lists, strings, etc. can be repeatedly iterated over)\nGenerators do not store all the values in memory, they generate the values on the fly\n\nyield - Pauses the function saving all its states and later continues from there on successive calls.\n\nAllows you to consume one element at a time and work with it without requiring you to have every element in memory.\nProduces a generator\n\n\nMisc\n\n{{itertools}} islice can slice a generator.\nAlso see APIs &gt;&gt; {{requests}} for an example\n\nExample: Using a comprehension¬†\nmygenerator = (x*x for x in range(3))\nfor i in mygenerator:\n...¬† ¬† print(i)\n\nProduce a list and ( ) produce a generator¬†\n\nExample: Using a function\ndef create_generator():\n¬† ¬† mylist = range(3)\n¬† ¬† for i in mylist:\n¬† ¬† ¬† ¬† yield i*i\n\nfor i in mygenerator:\n¬† ¬† print(i)\n0\n1\n4\n\nThe first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then it‚Äôll return the first value of the loop.\nThen, each subsequent call will run another iteration of the loop you have written in the function and return the next value.\nThis will continue until the generator is considered empty, which happens when the function runs without hitting yield.\n\nThat can be because the loop has come to an end, or because you no longer satisfy an ‚Äúif/else‚Äù\n\n\nExample: Sending values to (yield)\ndef grep(pattern):\nprint \"Looking for %s\" % pattern\nwhile True:\n¬† ¬† line = (yield)\n¬† ¬† if pattern in line:\n¬† ¬† ¬† ¬† print line,\ng = grep(\"python\")¬† # instantiate with \"python\" pattern to search for\n\ng.next() # Prime it\n&gt;&gt; Looking for python\n\ng.send(\"A series of tubes\") # \"python\" not present so returns nothing\ng.send(\"python generators rock!\") # \"python\" present so returns line\n&gt;&gt; python generators rock!\ng.close() # closes coroutine\n\n(yield) receives the input of the .send method and creates a generator object which is assigned to ‚Äúline‚Äù.\nAll coroutines must be ‚Äúprimed‚Äù by first calling .next() (or send(None))\n\nThis advances execution to the location of the first yield expression\n\n\nExample: Sending values to (yield)\ndef writer():\n¬† ¬† \"\"\"A coroutine that writes data *sent* to it to fd, socket, etc.\"\"\"\n¬† ¬† while True:\n¬† ¬† ¬† ¬† w = (yield)\n¬† ¬† ¬† ¬† print('&gt;&gt; ', w)\ndef writer_wrapper(coro):\n¬† ¬† # TBD\n¬† ¬† pass\nw = writer()\nwrap = writer_wrapper(w)\nwrap.send(None)¬† # \"prime\" the coroutine\nfor i in range(4):\n¬† ¬† wrap.send(i)\n&gt;&gt;¬† 0\n&gt;&gt;¬† 1\n&gt;&gt;¬† 2\n&gt;&gt;¬† 3\n\nA more complex framework if you want to break the workflow into multiple functions\n\n\n\nUsing yield from\n\nAllows for two-way usage (reading/sending) of generators\nExample (reading from a generator)\ndef reader():\n¬† ¬† \"\"\"A generator that fakes a read from a file, socket, etc.\"\"\"\n¬† ¬† for i in range(4):\n¬† ¬† ¬† ¬† yield '&lt;&lt; %s' % i\n\n# with yield\ndef reader_wrapper(g):\n¬† ¬† # Manually iterate over data produced by reader\n¬† ¬† for v in g:\n¬† ¬† ¬† ¬† yield v\n# OR with yield from\ndef reader_wrapper(g):\n¬† ¬† yield from g\nwrap = reader_wrapper(reader())\nfor i in wrap:\n¬† ¬† print(i)\n\nBasic; only eliminates 1 line of code\n\nExample (sending to a generator)\n# with (yield)\ndef writer_wrapper(coro):\n¬† ¬† coro.send(None)¬† # prime the coro\n¬† ¬† while True:\n¬† ¬† ¬† ¬† try:\n¬† ¬† ¬† ¬† ¬† ¬† x = (yield)¬† # Capture the value that's sent\n¬† ¬† ¬† ¬† ¬† ¬† coro.send(x)¬† # and pass it to the writer\n¬† ¬† ¬† ¬† except StopIteration:\n¬† ¬† ¬† ¬† ¬† ¬† pass\n# OR with yield from\ndef writer_wrapper(coro):\n¬† ¬† yield from coro\n\nNeed to see example 4 for the writer() code and the use case\nShows the other advantage of using ‚Äúyield from‚Äù: it automatically includes the code to stop prime and stop the loop.\n\nReusable generator\n\n\nreading example using ‚Äúyield from‚Äù\n\nSlicing a generator\nfrom itertools import islice\ndef gen():\n¬† ¬† yield from range(1,11)\ng = gen()\nmyslice = islice(g, 2)\n&gt;&gt; list(myslice)\n[1, 2]\n&gt;&gt; [i for i in g]\n[3,4,5,6,7,8,9,10]\n\n\n\n\nFor\n\n\nSyntax - for &lt;sequence&gt;: &lt;loop body&gt;\nNumeric Range\nfor i = 1 to 10\n¬† ¬† &lt;loop body&gt;\n\n# from 0 to 519\nfor i in range(520)\n¬† ¬† &lt;loop body&gt;\n\nres = 0\nfor idx in np.arange(0, 100000):\n¬† res += df.loc[idx, 'int']\n\nnp.arange() ran 8000 times faster than the same chunk using range()\n\nList\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\neven_numbers = []\nfor item in numbers:\n¬† ¬† if item % 2 == 0:\n¬† ¬† ¬† ¬† even_numbers.append(item)\nprint(even_numbers)\n\n# results: [2, 4, 6, 8]\nList: index and value\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nfor index, element in enumerate(numbers):\n¬† ¬† if element % 2 != 0:\n¬† ¬† ¬† ¬† numbers[index] = element * 2\n¬† ¬† else:\n¬† ¬† ¬† ¬† continue\nprint(numbers)\n# results: [2, 2, 6, 4, 10, 6, 14, 8, 18]\n\nenumerate also gets the index of the respective element at the same time\n\nWith three expressions\nfor (for i = 1; i &lt;= 10; i+=1)\n¬† ¬† &lt;loop body&gt;\nCollection-Based\n\nIf the collection is a dict, then this just iterates over the keys\n\nfor i in &lt;collection&gt;:\n¬† ¬† &lt;loop body&gt;\nIterate over a sliding window\n\nOver dictionary keys and values of a dict\nfor a,b in transaction_data.items():\n¬† ¬† print(a,‚Äô~‚Äô,b)\n\nThe .items method includes both key and value, so it iterates over the pairs.\n\nOver nested dictionaries\nfor k, v in transaction_data_n.items():\n¬† ¬† if type(v) is dict:\n¬† ¬† ¬† ¬† for nk, nv in v.items():¬†\n¬† ¬† ¬† ¬† ¬† ¬† print(nk,‚Äô ‚Üí‚Äô, nv)\n\nIf the item of the dict is itself a dict then another loop iterates through its items.\nnk and nv stand for nested key and nested value\n\nSelecting a specific item in a nested dictionary\nfor k, v in transaction_data_n.items():\n¬† ¬† if type(v) is dict and k == 'transaction_2':\n¬† ¬† ¬† ¬† for sk, sv in v.items():\n¬† ¬† ¬† ¬† ¬† ¬† print(sk,'--&gt;', sv)\n\nOnly transaction_2‚Äô s items are printed\n\nRows of a data.frame\nres = 0\nfor row in df.itertuples():\n¬† res += getattr(row, 'int')\n\nitertuples()¬† is 30x faster than iterrows()\n\n\n\n\nzip\n\nCombine lists into 1 list of tuples\nacc_values = [1, 0.04, 0.9]\nacc_names = [\"RMSE\", \"MAPE\", \"R-sq\"]\nacc_list = list(zip(acc_names, acc_values))\nacc_list\n[('RMSE', 1), ('MAPE', 0.04), ('R-sq', 0.9)]\n\nzip does take lists of different lengths but will create shortest length list with corresponding elements\nCombine lists of unequal lengths but keep the non-paired elements\nfrom itertools import zip_longest\nacc_names3 = [\"RMSE\", \"MAPE\", \"R-sq\", \"MSE\"]\nacc_values3 = [rmse, mape, rsq]¬†\nacc_list3 = list(zip_longest(acc_names3, acc_values3))\n\nUnzip list of tuples into separate lists\nnames, values = zip(*acc_list)\n\nAsterisk is the ‚Äúunzipping operator‚Äù\n\nUnpack dict into a list of separate tuples for key:value pairs\nacc_tuples = list(zip(acc_dict.keys(), acc_dict.values()))\nacc_tuples\n[('RMSE', 1), ('MAPE', 0.04), ('R-sq', 0.9)]\n\n\n\nComprehensions\n\nMisc\n\n‚Äòfor ‚Äî in‚Äô construct within comprehensions is faster than the traditional for-loops\n\nnot faster than (all?) lambda-filters (see functions &gt;&gt; lambda)\n\nReturns lists or dicts (just change the bracket types)\n\nDicts\n\nSyntax: mydict = {key:val for key, val in zip(keys_list, vals_list)}\nCombine key:value lists into a dictionary\nacc_dict = {k:v for k,v in zip(acc_names, acc_values)}\nReturn value and output of expression\nmydict = {v: v**2 for v in numberslist}\nIf numberslist =[1,2,3], then mydict = {1:1, 2:4, 3:9}\n\nLists\n\nSyntax: newlist = [expression for item in iterable if condition == True]\nWith expression\nmylist = [x**2 for x in numberslist]\n\nif numberslist =[1,2,3], then mylist = [1,4,9]\n\nSet values in a list to uppercase\nnewlist = [x.upper() for x in fruits]\nWith conditional expression (if ‚Äî else)\n\nAppend to the comprehension to filter the dictionary or list\nSyntax: mylist = [expressionA if (condition2==True) else expressionB for item in list if (condition1==True)]\nExample: newlist = [x if x != \"banana\" else \"orange\" for x in fruits]\n\nReturn ‚Äúorange‚Äù instead of ‚Äúbanana‚Äù\n\nExample: new_list = [(x**2) if (x&gt;90) else (x**3) for x in old_list if (x%2==0)]\n\nSays\n\nSquare an argument if it exceeds 90, else cube it¬†\nReturn all the exponentiated results only if the argument was an even number\n\n\nExample: c = [d for d in datstrlist if ((d.endswith(\"urday\") or d.endswith(\"unday\")) and \"Oc\" in d)]\n\nString filter than looks for strings with saturdays and sundays in october\n*Slower than a lamda-filter* (See Functions &gt;&gt; lambda)\n\n\n\nNested\n\nSyntax: myset = {{expression(itemA, itemB) for itemA in setA} for itemB in setB}\nExample: {j for i in range(2, int(N**0.5)+1) for j in range(i**2, N, i)}\n\nN = 100000\nCreates a set of all the integers from 2 to 100,000.\nPaces through all the integers i up to the square root of N\nDiscards from the set of 100,000 those numbers j which are equal or larger than the square of i\n\nExample: From link\n# Function to get set labels\ndef get_prediction_set_labels(prediction_set, class_labels):\n    # Get set of class labels for each instance in prediction sets\n    prediction_set_labels = [\n        set([class_labels[i] for i, x in enumerate(prediction_set) if x]) for prediction_set in \n        prediction_sets]\n    return prediction_set_labels\n\nReturns a list where each object in the list is a set object (e.g.¬†{green}, {green, orange})",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-debug",
    "href": "qmd/python-general.html#sec-py-gen-debug",
    "title": "General",
    "section": "Debugging",
    "text": "Debugging\n\nMisc\nTerms\n\nException Errors - Raised when the syntax is correct but the program results in an error.\nSyntax Errors - Occur when the interpreter detects invalid syntax (relatively easier to fix)\n\ne.g.¬†unmatched parenthesis\n\nTraceback - A report that helps us understand the reason for an exception.\n\nContains function calls made in the code along with their line numbers",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-errhand",
    "href": "qmd/python-general.html#sec-py-gen-errhand",
    "title": "General",
    "section": "Error Handling",
    "text": "Error Handling\n\ntry + except\n\nSays try the main code snippet, but if an exception (error) occurs, run the secondary code snippet, the workaround.\n\ndef pct_difference_error_handling(n1, n2):\n¬† '''Function that takes two numbers and return the percentual difference\n¬† between n1 and n2, being n1 the reference number'''\n\n¬† # Try the main code\n¬† try:\n¬† ¬† pct_diff = (n1-n2)/n1\n¬† ¬† return f'The difference between {n1} and {n2} is {n1-n2}, which is {pct_diff*100}% of {n1}'\n\n¬† # If you find an error, use this code instead\n¬† except:\n¬† ¬† pct_diff = (int(n1)-int(n2))/int(n1)\n¬† ¬† return f'The difference between {n1} and {n2} is {int(n1)-int(n2){style='color: #990000'}[}]{style='color: #990000'}, which is {pct_diff*100}% of {n1}'\n\n¬† # Optional\n¬† finally:\n¬† ¬† print(\"Code ended\")\n\nAssumes the error will be the user enters a string instead of a numeric. If errors, converts string to numeric and calcs.\nfinally: - This argument will always run, regardless if the try block raises an error or not. So it could be a completion message or a summary, for example.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/distributions.html",
    "href": "qmd/distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-terms",
    "href": "qmd/distributions.html#sec-distr-terms",
    "title": "Distributions",
    "section": "Terms",
    "text": "Terms\n\nConditional Probability Distributions\n\nNotes from https://www.causact.com/joint-distributions-tell-you-everything.html#joint-distributions-tell-you-everything\nNotation: \\(P(Y | X) = P(Y \\;\\text{and}\\; X) / P(X) = P(Y, X) / P(X)\\)\n\ni.e.¬†ratio of 2 marginal distributions\n\nExample: two tests for cancer are conducted to determine whether a biopsy should be performed\n\nConditional approach: Biopsy everyone at determined to be high risk from test 1; measure the genetic marker (aka test 2) for patients at intermediate risk and biopsy those with a probability of cancer past a certain level based on the marker\n\nWhen we perform regression analysis, we are essentially estimating conditional distributions. The conditional distribution, \\(P(Y|X_1, \\ldots, X_n)\\) represents the distribution of the response variable, \\(Y\\), given the specific values of the predictor variables, \\(X_1, \\ldots, X_n\\).\n\nEmpirical CDF\n\\[\nF_n (x) = \\frac {1}{n} \\sum_{i = 1}^n I(X_i \\leq x)\n\\]\n\nWhere \\(X_1, X_2,\\ldots,X_n\\) are from a population with CDF, \\(F_n (x)\\)\nProcess\n\nTake n samples from an unknown distribution. The more samples you take, the closer the empirical distribution will resemble the true distribution.\nSort these samples, and place them on the x-axis.\nStart plotting a ‚Äòstep-function‚Äô style line ‚Äî each time you encounter a datapoint on the x-axis, increase the step by 1/N.\n\nExample\n\n\nThe CDF of a normal distribution (green) and its empirical CDF (blue)\n\n\nJoint Probability Distribution - Assigns a probability value to all possible combinations of values for a set of random variables.\n\nNotation: \\(P(x_1, x_2, ... ,x_n)\\)\nPlugging in a value for each random variable returns a probability for that combination of values\nExample: Two tests for cancer are conducted to determine whether a biopsy should be performed\n\nJoint approach: biopsy anyone who is either at high risk of cancer (test 1) or who was determined to have a probability of cancer past a certain level, based on the marker from the genetic test (test 2)\nCompare with example in Conditional Probability Distributions\n\nIn the context of regression modeling: the joint distribution refers to the distribution of all the variables involved in the regression analysis. For example, if you have a regression model with a response variable \\(Y\\) and predictor variables \\(X_1, \\ldots, X_n\\), the joint distribution would describe the combined distribution of \\(Y, X_1, \\ldots, X_n\\).\n\nLocation - Distribution parameter determines the shift of the distribution\n\ne.g.¬†mean, mu, of the normal distribution.\n\nMarginal Probability Distribution - Assigns a probability value to all possible combinations of values for a subset of random variables\n\nNotation: \\(P(x_1)\\)\n\n\\(P(x_1,x_2)\\) is sometimes called the Joint Marginal Probability Distribution\n\nThe marginal distribution, \\(P(Y)\\) where \\(Y\\) is a subset of random variables, is calculated from the joint distribution, \\(P(Y = y, Z = z)\\) where \\(Z\\) is the subset of random variables not in \\(Y\\) .\n\n\\(P(Y) = \\sum_{Z=z} P(Y = y, Z = z)\\)\n\nIf \\(Y\\) is just one variable\n\nSays sum all the joint probabilities for all the combinations of values for the variables in \\(Z\\) while holding \\(Y\\) constant\nRepeat for each value of \\(Y\\) to get this summed probability value\nThe marginal distribution is made up of all these values, one for each value of \\(Y\\) (or combination of values if \\(Y\\) is a subset of variables)\n\n\nWhen the joint probability distribution is in tabular form, one just sums up the probabilities in each row where \\(Y = y\\).\nIn the context of regression modeling, the marginal distribution of \\(Y\\) represents the distribution of \\(Y\\) alone, without considering the specific values of the predictor variables.\n\n\nScale - Distribution parameter; the larger the scale parameter, the more spread out the distribution\n\ne.g.¬†s.d., sigma, \\(\\sigma\\) of the normal distribtution\nRate Parameter: the inverse of the scale parameter (see Gamma distribution)\n\nShape - Distribution parameter that affects the shape of a distribution rather than simply shifting it (as a location parameter does) or stretching/shrinking it (as a scale parameter does).\n\ne.g.¬†‚ÄúPeakedness‚Äù refers to how round the main peak is",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-tests",
    "href": "qmd/distributions.html#sec-distr-tests",
    "title": "Distributions",
    "section": "Tests",
    "text": "Tests\n\nWhy normality tests are great‚Ä¶ as a teaching example and should be avoided in research\n\ntl;dr; KS test has very low power as a Normality test as compared to Shapiro-Wilk, and Shapiro-Wilk isn‚Äôt very good for n &lt; 100\nFor detecting moderate skew, you want at least n &gt; 75 to get 80% power for Shapiro-Wilk\nShapiro-Wilk can detect very fat tails at n &lt; 100, but would require larger sample sizes to detect more moderately thick tails.\nKS is worthless in detecting fat tails and near-worthless at detecting skew\nWhen n gets large (e.g.¬†1000s), these types of tests will almost always reject the null even when the practical deviation from normality is not practically significant.\n\nKolmogorov‚ÄìSmirnov test (KS)\n\nUsed to compare distributions\n\nCan be used as a Normality test or any distribution test\nCan compare two samples\n\nMisc\n\nVectors may need to be standardized (e.g.¬†normality test) first unless comparing two samples H0: Both distributions are from the same distribution\n\nPackages\n\n{KSgeneral} has tests to use for contiuous, mixed, and discrete distributions written in C++\n{stats} and {dgof} also have functions, ks.test\n\nBoth handle continuous and discrete distributions\n\nAll functions take a numeric vector and a base R density function (e.g.¬†pnorm, pexp, etc.) as args\n\nKSgeneral docs don‚Äôt say you can supply your own comparison sample (2nd arg) only the density function but with stats and dgof, you can.\nAlthough they have function to compute the CDFs, so if you need speed, it might be possible to use their functions and do it manually\n\n\n2-sample test as the greatest distance between the CDFs (Cumulative Distribution Function) of each sample\n\nSpecifically, this test determines the distribution of your unknown data sample by constructing and comparing the sample‚Äôs empirical CDF¬† (see Terms) with the CDF you hypothesized. If the two CDFs are close, your unknown data sample likely follows the hypothesized distribution.\n\nKS statistic, \\(D_{n,m} = \\max|\\text{CDF}_1 - \\text{CDF}_2|\\) where \\(n\\) as the number of observations on Sample 1 and \\(m\\) as the number of observations in Sample 2\nCompare the KS statistic with the respective KS distribution based on parameter ‚Äúen‚Äù to obtain the p-value of the test\n\n\\(en = (m \\times n) / (m + n)\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-beta",
    "href": "qmd/distributions.html#sec-distr-beta",
    "title": "Distributions",
    "section": "Beta",
    "text": "Beta\n\n\nDefined on the interval [0,1]\nThe key difference between the Binomial and Beta distributions is that for the Beta distribution the probability, x, is a random variable, however for the Binomial distribution the probability, x, is a fixed parameter.\nShape parameters are \\(\\alpha\\) and \\(\\beta\\), usually.\n\n\\(\\alpha\\) and \\(\\beta\\) are two positive parameters that appear as exponents of the random variable\n\npdf\n\\[\nf(x) = \\frac {x^{\\alpha - 1} (1-x)^{\\beta - 1}} {B(\\alpha, \\beta)}\n\\]\n\\(\\mathbb{E}(X) = \\frac {\\alpha} {\\alpha + \\beta}\\)\n\\(\\text{Var}(X) = \\frac {\\alpha \\cdot \\beta} {(\\alpha + \\beta)^2 \\cdot (\\alpha + \\beta + 1)}\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-betbin",
    "href": "qmd/distributions.html#sec-distr-betbin",
    "title": "Distributions",
    "section": "Beta-Binomial",
    "text": "Beta-Binomial\n\n\n\n\n\n\n\n\nWhere k is the number of events in n trials\n\n\n\n\n\n\n\nWhere \\(\\theta\\) is the probability of an event\n\n\n\n\n\n\n\nUsed when the probability of success, p, in a fixed number of Bernoulli trials is unknown or random and can change from trial to trial.\nShape parameters Œ± and Œ≤ define the probability of success (i.e.¬†the success parameter is modeled by the Beta Distribution).\n\nFor large values of Œ± and Œ≤, the distribution approaches a binomial distribution.\nWhen Œ± and Œ≤ both equal 1, the distribution equals a discrete uniform distribution from 0 to n\n\nAccuracy analysis data from psychology follow beta-binomial distributions (Jaeger, 2008; Kruschke, 2014)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-dirichlet",
    "href": "qmd/distributions.html#sec-distr-dirichlet",
    "title": "Distributions",
    "section": "Dirichlet",
    "text": "Dirichlet\n\nA family of continuous multivariate probability distributions parameterized by a vector Œ± of positive reals",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-exp",
    "href": "qmd/distributions.html#sec-distr-exp",
    "title": "Distributions",
    "section": "Exponential",
    "text": "Exponential\n\n\nNotes from\n\nStatistical Rethinking &gt;&gt; Chapter 10\n\nConstrained to be zero or positive\nFundamental distribution of distance and duration, kinds of measurements that represent displacement from some point of reference, either in time or space.\nIf the probability of an event is constant in time or across space, then the distribution of events tends towards exponential.\nIts shape is described by a single parameter, the rate of events \\(\\lambda\\), or the average displacement \\(\\lambda ‚àí1\\) .\nThis distribution is the core of survival and event history analysis",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gamma",
    "href": "qmd/distributions.html#sec-distr-gamma",
    "title": "Distributions",
    "section": "Gamma",
    "text": "Gamma\n\n\nNotes from\n\nStatistical Rethinking &gt;&gt; Chapter 10\n\nConstrained to be zero or positive\nLike Exponential but can have a peak above zero\nIf an event can only happen after two or more exponentially distributed events happen, the resulting waiting times will be gamma distributed.\n\ne.g.¬†age of cancer onset is approximately gamma distributed, since multiple events are necessary for onset.\n\nThe gamma can be viewed as the sum of iid n exponential random variables. Exponential random variables have a rate parameter, so it makes sense for the Gamma to inherit a rate parameterization. The rate parameter also happens to be related to a scale parameter, so it makes sense for the Gamma to have a scale parameterization.\nShape parameter \\(k\\) and a scale parameter \\(\\theta\\)\n\\(\\mathbb{E}[X] = k\\theta = \\frac{\\alpha}{\\beta}\\)\n\nShape parameter \\(\\alpha = k\\) and an\nInverse Scale parameter (aka Rate Parameter) \\(\\beta = \\frac {1}{\\theta}\\)\nTherefore if you want a gamma distributions with a certain ‚Äúmean‚Äù and ‚Äústandard deviation,‚Äù you‚Äôd:\n\nSet your mean to \\(\\mathbb{E}[X]\\), your standard deviation to \\(\\theta\\) (probably but maybe it‚Äôs \\(\\beta\\))\nCalculate \\(\\beta\\)\nCalculate \\(\\alpha\\)\nprior(gamma(alpha, beta))\n\n\nExample: Gamma distribution as the sums of random exponential variables\n\nn &lt;- 12\nbeta &lt;- 1.2\n\nrvs &lt;- replicate(1000, {\n  sum(rexp(n, beta))\n})\n\nhist(rvs, freq = F)\ncurve(dgamma(x, shape = n, rate = beta), col='red', add=T)\n\nGamma distribution density overlayed with a histogram of exponential variable sums\n\nUsed in Survival Regression",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gauss",
    "href": "qmd/distributions.html#sec-distr-gauss",
    "title": "Distributions",
    "section": "Gaussian",
    "text": "Gaussian\n\nSpecial case of Student‚Äôs t-distribution with the \\(\\nu\\) parameter (i.e.¬†degree of freedom) set to infinity.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gumb",
    "href": "qmd/distributions.html#sec-distr-gumb",
    "title": "Distributions",
    "section": "Gumbel",
    "text": "Gumbel\n\n\nKnown as the type-I generalized extreme value distribution\n\nEVT says it is likely to be useful if the distribution of the underlying sample data is of the normal or exponential type.\n\nUsed to model the distribution of the maximum (or the minimum) of a number of samples of various distributions.\n\nTo model minimums, use the negative of the original data.\n\nUse Cases\n\nRepresent the distribution of the maximum level of a river in a particular year if there was a list of maximum values for the past ten years.\nPredicting the chance that an extreme earthquake, flood or other natural disaster will occur.\nDistribution of the residuals in Multinomial Logit and Nested Logit models\n\nParameters\n\nGumbel(\\(\\mu, \\beta\\)) (location, scale)\nMean: \\(\\mu + \\beta\\gamma\\) where \\(\\gamma\\) is Euler‚Äôs constant (\\(\\approx\\) 0.5772)\nMedian: \\(\\mu - \\beta \\ln(\\ln(2))\\)\nMode: \\(\\mu\\)\nVariance: \\(\\frac{\\pi^2}{6}\\beta^2\\)\nStandard Gumbel: When \\(\\mu = 0\\), mean = \\(\\gamma\\), median = \\(-\\ln(\\ln(2)) \\approx 0.3665\\) and the standard deviation = \\(\\pi/\\sqrt{6}\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-multgauss",
    "href": "qmd/distributions.html#sec-distr-multgauss",
    "title": "Distributions",
    "section": "Multivariate Gaussian",
    "text": "Multivariate Gaussian\n\nIf the random variable components in the vector are not normally distributed themselves, the result is not multivariate normally distributed.\nVariance-Covariance matrix must be semi-definite and therefore symmetric\n\nExample of not symmetric for two random variables",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-pareto",
    "href": "qmd/distributions.html#sec-distr-pareto",
    "title": "Distributions",
    "section": "Pareto",
    "text": "Pareto\n\nAlso see Extreme Value Theory &gt;&gt; Distribution Tail Classification\n‚ÄúGaussian distributions tend to prevail when events are completely independent of each other. As soon as you introduce the assumption of interdependence across events, Paretian distributions tend to surface because positive feedback loops tend to amplify small initial events.‚Äù\nPareto has similar relationship with the exponential distribution as lognormal does with normal \\[\nY_{exp} = \\log \\frac {X_{pareto}} {x_m}\n\\]\n\nWhere \\(X_{pareto} = x_m e^{Y_{\\text{exp}}}\\)\n\n\\(x_m\\) is the (positive) minimum of the randomly distributed pareto variable, X that has index Œ±\n\\(Y_{exp}\\) is exponentially distributed with rate \\(\\alpha\\)\n\n\nSome theoretical statistical moments may not exist\n\nIf the theoretical moments do not exist, then calculating the sample moments is useless\nExample: Pareto (\\(\\alpha\\) = 1.5) has a finite mean and an infinite variance\n\nNeed \\(\\alpha &gt; 2\\) for a finite variance\nNeed \\(\\alpha &gt; 1\\) for a finite mean\nIn general you need \\(\\alpha &gt; p\\) for the pth moment to exist\nIf the nth moment is not finite, then the (n+1)th moment is not finite.\n\n\nFat Tails \\[\n\\bar{F} = x^{-\\alpha} L(x)\n\\]\n\n\\(L(x)\\) is just characterized as slowly varying function that gets dominated by the decaying inverse power law element, \\(x-\\alpha\\). as \\(x\\) goes to infinity\n\n\\(\\alpha\\) is a shape parameter, aka ‚Äútail index‚Äù aka ‚ÄúPareto index‚Äù",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-poisson",
    "href": "qmd/distributions.html#sec-distr-poisson",
    "title": "Distributions",
    "section": "Poisson",
    "text": "Poisson\n\nObtained as the limit of the binomial distribution when the number of attempts is high and the success probability low. Or the Poisson distribution can be approximated by a normal distribution when Œª is large\nProbability Mass Function \\[\n\\text{Pr}(Y = y) = f(y; \\lambda) = \\frac {e^{-\\lambda} \\cdot \\lambda^y} {y!}\n\\]\n\n\\(\\mathbb{E}[Y] = \\text{Var}(Y) = \\lambda\\)\n\n{distributions3}\n\nStats\nY &lt;- Poisson(lambda = 1.5) \nprint(Y) \n## [1] \"Poisson distribution (lambda = 1.5)\"\n\nmean(Y) \n## [1] 1.5 \nvariance(Y) \n## [1] 1.5 \npdf(Y, 0:5) \n## [1] 0.22313 0.33470 0.25102 0.12551 0.04707 0.01412 \ncdf(Y, 0:5) \n## [1] 0.2231 0.5578 0.8088 0.9344 0.9814 0.9955 \nquantile(Y, c(0.1, 0.5, 0.9)) \n## [1] 0 1 3 \nset.seed(0) \nrandom(Y, 5) \n## [1] 3 1 1 2 3\n\nVisualize\n\nplot(Poisson(0.5), main = expression(lambda == 0.5), xlim = c(0, 15)) \nplot(Poisson(2),   main = expression(lambda == 2),   xlim = c(0, 15)) \nplot(Poisson(5),   main = expression(lambda == 5),   xlim = c(0, 15)) \nplot(Poisson(10),  main = expression(lambda == 10),  xlim = c(0, 15))",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-studt",
    "href": "qmd/distributions.html#sec-distr-studt",
    "title": "Distributions",
    "section": "Student‚Äôs t-distribution",
    "text": "Student‚Äôs t-distribution\n\nStandard Deviation\n\\[\n\\text{sd} = \\sqrt {\\frac {\\nu} {\\nu - 2}}\n\\]\n\n\\(\\nu\\) = degrees of freedom\n\nWhen ŒΩ is small, the Student‚Äôs t-distribution is more robust to multivariate outliers\nThe smaller the degree of freedom, the more ‚Äúheavy-tailed‚Äù it is\n\n\n-3 on the y-axis says that the probability of being in the tail is 1 in 103\n\nDon‚Äôt pay attention to the x-axis. Just note how much the probability of being in the tail gets larger as the dof get smaller\n\nAs the degrees of freedom goes to 1, the t distribution goes to the Cauchy distribution\nAs the degrees of freedom goes to infinity, it goes to the Normal distribution.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-tri",
    "href": "qmd/distributions.html#sec-distr-tri",
    "title": "Distributions",
    "section": "Triangular",
    "text": "Triangular\n\nTriangle shaped distribution\nUseful when you have a known min and max value\nextraDistr::rtriang(n, a, b, c) %\\&gt;% hist()\n\n# Discrete distribution\nextraDistr::rtriang(n, a, b, c) %\\&gt;% round() \\`\\`\\`\n\nn is the number of random values you wish to draw\na is the min value\nb is the max value\nc is the mode\n\nCan use to adjust the skew of the distribution\n\n\n\n\n\n\nWhere k is the number of events in n trials\nWhere \\(\\theta\\) is the probability of an event",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/mathematics-glossary.html",
    "href": "qmd/mathematics-glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "A priori - a type of knowledge that can be derived by reason alone\n\nA priori analyses are performed as part of the research planning process.\n\nA posteriori - a type of knowledge that expresses an empirical fact unknowable by reason alone.\n\nSame as post-hoc. Post-Hoc analysis is conducted after the experiment.\n\nbias - see unbiased estimator\nceteris paribus - latin for ‚Äúall things being equal‚Äù or ‚Äúother things held constant.‚Äù\nclosed form - a mathematical expression that uses a finite number of standard operations. It may contain constants, variables, certain well-known operations, and functions, but usually no limit, differentiation, or integration.\nconsistency - Requires that the outcome of the procedure with unlimited data should identify the underlying truth. Usage is restricted to cases where essentially the same procedure can be applied to any number of data items. In complicated applications of statistics, there may be several ways in which the number of data items may grow. For example, records for rainfall within an area might increase in three ways: records for additional time periods; records for additional sites with a fixed area; records for extra sites obtained by extending the size of the area. In such cases, the property of consistency may be limited to one or more of the possible ways a sample size can grow\ndegrees of freedom - When discussed about variable-sample size tradeoff, usually means n-p, where is the number of rows and p is the number of variables. The more variables used in the model the fewer degrees of freedom and therefore less power and precision.\nexchangeability - means we can swap around, or reorder, variables in the sequence without changing their joint distribution.\n\nEvery IID (independent, identically distributed) sequence is exchangeable - but not the other way around. Every exchangeable sequence is identically distributed, though\n\nExample: If you draw a sequence of red and blue marbles from a bag without replacement, the sample is exchangeable but not independent. e.g.¬†drawing a red marble affects the probability of drawing a red or blue marble next.\n\n\nefficiency - A test, estimator, etc. is more efficient than another test, estimator, etc. if it requires fewer observation to obtain the same level of performance.\nergodicity - the idea that a point of a moving system, either a dynamical system or a stochastic process, will eventually visit all parts of the space that the system moves in, in a uniform and random sense\nexternal validity - Our estimates are externally valid if inferences and conclusions can be generalized from the population and setting studied to other populations and settings. (also see internal validity)\nidentifiable (aka point-indentifiable) - theoretically possible to learn the true values of this model‚Äôs underlying parameters after obtaining an infinite number of observations from it (see non-identifiability, partially-indentifiable)\nill-conditioned - In SVD decomposition, when there‚Äôs a huge difference between largest and smallest eigenvalue of¬†the original matrix, A, the ratio of which is called condition number.\ninternal validity - our estimates are internally valid if statistical inferences about causal effects are valid for the population being studied. (also see external validity)\nintractable - problems for which there exist no efficient algorithms to solve them. Most intractable problems have an algorithm ‚Äì the same algorithm ‚Äì that provides a solution, and that algorithm is the brute-force search\nlocality - effects have causes and chains of cause and effect must be unbroken in space and time (not the case in ‚Äòentanglement‚Äô)\nmarginalization - The process of eliminating one or more variables from a joint probability distribution or a multivariate statistical model to obtain the distribution or model for a subset of variables. The resulting distribution or model is called a marginal distribution or marginal model. It allows you to focus on the behavior of specific variables while considering the uncertainty associated with others.\n\nFor example, marginalizing over a joint distribution (i.e.¬†many variables) gets you a marginal distribution (i.e.¬†fewer variables). In other words, if you have a joint probability distribution for two variables \\(X\\) and \\(Y\\), the marginal distribution of \\(X\\) is obtained by summing or integrating over all possible values of \\(Y\\). Similarly, the marginal distribution of \\(Y\\) is obtained by summing or integrating over all possible values of \\(X\\).\nNotation: \\(P(X) = \\sum_Y P(X,Y) \\;\\text{or}\\; P(X) = \\int P(X,Y)\\;dY\\)\nOnce you have to the marginal distribution, this allows you compute conditional distributions. For example, after obtaining the marginal distribution, \\(P(X,Y)\\), from the joint distribution, \\(P(X,Y,Z)\\), you can compute the conditional distributions, \\(P(X|Y)\\) and \\(P(Y|X)\\).\nThe uncertainty associated with \\(Z\\) is indirectly considered in the sense that the marginal distribution \\(P(X,Y)\\) accounts for all possible values of \\(Z\\) by integrating over them. However, \\(P(X,Y)\\) itself doesn‚Äôt provide explicit information about the uncertainty associated with \\(Z\\).\n\nnon-identifiability - the structure of the data and model do not make it possible to estimate the parameter‚Äôs value. Multicollinearity is a type of non-identifiability problem. (i.e.¬†two or more parametrizations of the model are observationally equivalent) (see identifiable, partially-indentifiable)\noverdetermined system - In linear regression, when there are more observations than features, n &gt; p\npartial coefficient - The coefficient of a variable in a multivariable regression. In a simple regression, the coefficient of the variable is just called the ‚Äúregression coefficient.‚Äù\npartially-indentifiable (aka set identifiable) - non-identifiable but possible to learn the true values of a certain subset of the model parameters\nrobust - a ‚Äúrobust‚Äù estimator in statistics is one that is insensitive to outliers, whereas a ‚Äúrobust‚Äù estimator in econometrics is insensitive to heteroskedasticity and autocorrelation (hyndman)\nsupport (aka range) - the set of values that the random variable can take.\n\nFor discrete random variables, it is the set of all the realizations that have a strictly positive probability of being observed.\nFor continuous random variables, it is the set of all numbers whose probability density is strictly positive.\nSee link for examples\n\nunderspecification - In general, the solution to a problem is underspecified if there are many distinct solutions that solve the problem equivalently.\nAn unbiased estimator is an accurate statistic that‚Äôs used to approximate a population parameter.\n\n‚ÄúAccurate‚Äù in this sense means that it‚Äôs neither an overestimate nor an underestimate. If an overestimate or underestimate does happen, the mean of the difference is called a ‚Äúbias.‚Äù\n\nWeak Law of Large Numbers (Bernoulli‚Äôs theorem) - states that if you have a sample of independent and identically distributed random variables, as the sample size grows larger, the sample mean will tend toward the population mean",
    "crumbs": [
      "Mathematics",
      "Glossary"
    ]
  },
  {
    "objectID": "qmd/git-general.html",
    "href": "qmd/git-general.html",
    "title": "25¬† General",
    "section": "",
    "text": "25.1 Misc",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#misc",
    "href": "qmd/git-general.html#misc",
    "title": "25¬† General",
    "section": "",
    "text": "View HTML file in browser\n\nSyntax: ‚Äúhttps://raw.githack.com/&lt;acct name&gt;/&lt;repo name&gt;/&lt;branch name&gt;/&lt;directory name&gt;/&lt;file name&gt;.html‚Äù\n\nInstalling from a git repo (From link)\n\nMake a fork of the repo and then clone it to your local machine.\nTo update, after setting an upstream remote (git remote add upstream git://github.com/benfulcher/hctsa.git) you can use git pull upstream main.\nTo update the submodule in the repo, git submodule update --init\n\nStart R project and Git repo in whichever order (I think)\n\nCreate R project in RStudio\n\nChoose ‚ÄúNew Directory‚Äù for all the templated projects (e.g.¬†quarto book, shiny, etc.). None of the other choices have them.\n\nIf you‚Äôve already created a directory, it will NOT overwrite this directory or add to it. So you‚Äôll either have alter the name of your old directory or choose a new name.\n\n\nCreate repo on Github\n\nAdd license and readme\n\nDo work\nTools &gt;&gt; Version Control &gt;&gt; Project Set-up &gt;&gt; Version Control System &gt;&gt; Select Git\nOpen terminal and go to working directory of project\ngit checkout -B main\ngit pull origin main --allow-unrelated-histories\ngit add .\ngit commit -m \"initial commit\"\ngit push --set-upstream origin main \n\nTurn off ‚ÄúLF will be replaced by CRLF the next time Git touches it‚Äù\n\nMessage spams terminal when committing changes from a window machines. Has to do with line endings in windows vs unix.\nTurn off: git config core.autocrlf true\nSee SO post for more details\n\nURL format to download files from repositories\n\nhttps://raw.githubusercontent.com/user/repository/branch/filename\n\n# Or evidently this way works too\n# adds ?raw=true to the end of the url\nfeat_all_url &lt;- url(\"https://github.com/notast/hierarchical-forecasting/blob/main/3feat_all.RData?raw=true\")\nload(feat_all_url)\nclose(feat_all_url)\nGet filelist from repo and download to a directory\n\n** Directory urls change as commits are made **\n\nlibrary(httr)\n\n# example: get url for the data dir of covidcast repo\nreq &lt;- httr::GET(\"https://api.github.com/repos/ercbk/Indiana-COVIDcast-Dashboard/git/trees/master?recursive=1\") %&gt;%¬†\n¬† httr::content()\n# alphabetical order\ntrees &lt;- req$tree %&gt;%¬†\n¬† map(., ~pluck(.x, 1)) %&gt;%¬†\n¬† as.character()\n# returns 20 which is first instance, so 19 should the \"data\" folder\ndetect_index(trees, ~str_detect(., \"data/\"))\n# url for data dir\nreq$tree[[19]]$url\n\n# example\n# Get all the file paths from a repo\nreq &lt;- GET(\"https://api.github.com/repos/etiennebacher/tidytuesday/git/trees/master?recursive=1\")\n# any request errors get printed\nstop_for_status(req)\nfile_paths &lt;- unlist(lapply(content(req)$tree, \"[\", \"path\"), use.names = F)\n# file_path wanted &lt;- filter file path to file you want\n# gets the very last part of the path\nfile_wanted &lt;- basename(file_path_wanted)\norigin &lt;- paste0(\"https://raw.githubusercontent.com/etiennebacher/tidytuesday/master/\", file_wanted)\ndestination &lt;- \"output-path-with-filename-ext\"\n# if file doesn't already exist, download it from repo into destination\nif (!file.exists(destination)) {\n¬†     # if root dir doesn't exist create it\n¬†     if (!file.exists(\"_gallery/img\")) {\n¬† ¬†     dir.create(\"_gallery/img\")\n¬†     }\n¬†     download.file(origin, destination)\nThe insides of .git",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#config-options",
    "href": "qmd/git-general.html#config-options",
    "title": "25¬† General",
    "section": "25.2 Config Options",
    "text": "25.2 Config Options\n\nNotes from: Popular git config options - More options listed that are not presented here.\nSetting Options\n\nAdd via CLI: git config --global &lt;name&gt; &lt;value&gt;\n\nExample: git config --global diff.algorithm histogram\n\nDelete by going into ~/.gitconfig and delete the parameter and value\n\nmerge.conflictstyle diff3 - Provides extra information on merge conflicts\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\ndef parse(input):\n    return input.split(\"\\n\")\n||||||| b9447fc\ndef parse(input):\n    return input.split(\"\\n\\n\")\n=======\ndef parse(text):\n    return text.split(\"\\n\\n\")\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; somebranch\n\nBelow &lt;&lt;&lt;&lt;&lt;&lt; HEAD: This is your local code that you‚Äôre trying to push\nBetween |||||||| b9447fc and =======: This is the original version of the code\nAbove &lt;&lt;&lt;&lt;&lt;&lt; somebranch: This is code from the branch that got merged before yours (I think)\nTherefore, the correct merge conflict resolution is return text.split(\"\\n\"), since that combines the changes from both sides.\n\nmerge.conflictstyle zdiff3 - A newer version of merge.conflictstyle diff3\nA\nB\nC\nD\nE\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; ours\nF\nG\n||||||| base\n# Add More Letters\n=======\nX\nY\nZ\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; theirs\n\nAbove &lt;&lt;&lt;&lt;&lt;&lt; ours: This is the original code plus the code that belongs to the branch that got merged that is not in conflict with your code\nBelow &lt;&lt;&lt;&lt;&lt;&lt; ours: This is the code that is in conflict with the branch (e.g.¬†main) your merging into.\nBelow |||||||| base: This is the code that has been removed from the original code for both mergers\nAbove &lt;&lt;&lt;&lt;&lt;&lt; theirs: This is code for another branch that was merged before yours that is in conflict with your code.\n\npush.default current - Says that when using git push to always push the local branch to a remote branch with the same name.\n\npush.default simple is the default in Git. Means git push only works if your branch is already tracking a remote branch.\nI guess it‚Äôs possible to push a local branch to a remote branch of a different name.\n\ninit.defaultBranch main - Create a main branch instead of a master branch when creating a new repo. I normally do this on Github.\ncommit.verbose true - This adds the whole commit diff in the text editor where you‚Äôre writing your commit message, to help you remember what you were doing.\nrerere.enabled true - This enables rerere (‚Äùreuse recovered resolution‚Äù), which remembers how you resolved merge conflicts during a git rebase and automatically resolves conflicts for you when it can.\ncore.pager delta - The ‚Äúpager‚Äù is what git uses to display the output of git diff, git log, git show, etc.\n\nValues:\n\ndelta: A fancy diff viewing tool with syntax highlighting\nless -x5,9 - Sets tabstops, which I guess helps if you have a lot of files with tabs in them?\nless -F -X - Not sure about this one, -F seems to disable the pager if everything fits on one screen if but her git seems to do that already anyway\ncat - To disable paging altogether\n\nDelta also suggests that you set up interactive.diffFilter delta ‚Äìcolor-only to syntax highlight code when you run git add -p.\n\ndiff.algorithm histogram - Improves the Patience algorithm for presenting diffs. See link in article for more details.\n\nDefault (I think the default algorithm is Myers.)\n-.header {\n+.footer {\n     margin: 0;\n }\n\n-.footer {\n+.header {\n     margin: 0;\n+    color: green;\n }\n\nfooter didn‚Äôt actually have margin: 0 and color: green in the original code like this diff makes it seem. In reality, the two rules have switched order with header gaining the additional property, color: green.\n\nHistogram\n-.header {\n-    margin: 0;\n-}\n-\n .footer {\n     margin: 0;\n }\n\n+.header {\n+    margin: 0;\n+    color: green;\n+}\n\nThis shows header‚Äôs old rule without color: green at the top and being removed. footer is accurately depicted as unchanged. Then, it shows header with the addtional property, color: green, added below footer.\n\n\nincludeIf - Allows you to use different options depending which directory your project is in.\n\nExample: Use this config file only if you‚Äôre in the ‚Äúwork‚Äù directory\n[includeIf \"gitdir:~/code/&lt;work&gt;/\"]\n    path = \"~/code/&lt;work&gt;/.gitconfig\"\n\nGood if, for example, you want to have a work email set for work repos and personal email for set for personal repos\n\n\ninsteadOf - Useful to correct little mistakes often you make\n\nSee article for other usecases\nExample: If you accidently clone using http when you want to use SSH\n[url \"git@github.com:\"]\n    insteadOf = \"https://github.com/\"\n\nNow when you accidently clone a repo using the http address, it‚Äôll change it to the ssh address in .git/config. Now you‚Äôll be using ssh to push changes which is more secure.\n\n\nSubmodules\nstatus.submoduleSummary true\ndiff.submodule log\nsubmodule.recurse true\n\nSee thread for details\nThe top two ‚Äúmake git status and git diff display some more useful information on how things differ in submodules.‚Äù\nThe bottom one aids in the updating of submodules when switching branches\n\ndiff.colorMoved default - Uses different colours to highlight lines in diffs that have been ‚Äúmoved‚Äù\n\ndiff.colorMovedWS allow-indentation-change - With diff.colorMoved set, also ignores indentation changes\n\ngpg.format ssh - Allows you to sign commits with SSH keys\nmerge.tool meld (or nvim, or nvimdiff) - Enables use git mergetool to help resolve merge conflicts",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#optimizations",
    "href": "qmd/git-general.html#optimizations",
    "title": "25¬† General",
    "section": "25.3 Optimizations",
    "text": "25.3 Optimizations\n\nFor large repos, simple actions, like running git status or adding new commits can take many seconds. Cloning repos can take many hours.\nBenefits\n\nIt improves the overall performance of your development workflow, allowing you to work more efficiently. This is especially important when working with large organizations and open source projects, where multiple developers are constantly committing changes to the same repository. A faster repository means less time waiting for Git commands such as git clone or git push to finish. It helps to optimize the storage space, as large files are replaced by pointers which take up less space. This can help avoid storage issues, especially when working with remote servers.\n\nMisc\n\nSee How to Improve Performance in Git: The Complete Guide\n\nExplainer, config settings, advanced gc, checkout, and clone commands\n\n\nUse .gitignore\n\nGenerated files, like cache or build files\n\nThey will be modified at each different generation ‚Äî and there‚Äôs no need to keep track of those changes.\n\nThird-party libraries\n\nInstead, aim for a list of the required dependencies (and the correct version) so that everyone can download and install them whenever the repo is cloned.\n\nFor example, with a package.json file for JavaScript projects you can (and should) exclude the /node_modules folder.\n.DS_Store files (which are automatically created by macOS) are another good candidate\n\n\n\nGit LFS\n\nDesigned specifically to handle large file versioning. LFS saves your local repositories from becoming unnecessarily big, preventing you from downloading unnessary data.\n\nGit LFS intercepts any large files and sends them to a separate server, leaving a smaller pointer file in the repository that links to the actual asset on the Git LFS server.\n\nThis is an extension to the standard Git feature set, so you will need to make sure that your code hosting provider supports it (all the popular ones do).\nAlso need to download and install the CLI extension on your machine before installing it in your repository.\nSet-Up\n$ git lfs install\n$ git lfs track \"*.wav\"\n$ git lfs track \"images/*.psd\"\n$ git lfs track \"videos\"\n$ git add .gitattributes\n\nTells Git LFS which file extensions it should manage.\n.gitattributes notes the file names and patterns in this text file and, just like any other change, it should be staged and committed to the repository.\nCan now add files and commit as normal\nList all file extensions being tracked: git lfs track\nList all files being managed: git lfs ls-files\n\n\nDon‚Äôt download the version history if you don‚Äôt need to\n\ngit clone ‚Äìdepth 1 gitj@github.com:name/repo.git",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#troubleshooting",
    "href": "qmd/git-general.html#troubleshooting",
    "title": "25¬† General",
    "section": "25.4 Troubleshooting",
    "text": "25.4 Troubleshooting\n\nDiverged Branches\n\n\nKeeps asking for username/password when pushing\n\nSolution: You (or if you used usethis::use_github/git) probably set-up a https connection when you need a ssh connection.\n\nsee https://docs.github.com/en/get-started/getting-started-with-git/managing-remote-repositories#changing-a-remote-repositorys-url to change from https to ssh.\n\n\nUndo a commit, but save changes made (e.g.¬†you forgot to pull before you pushed)\n\nSteps\n\ngit log - Shows commit history. Copy the hash for your last commit\ngit diff &lt;last commit hash&gt; &gt; patch - save the diff of the latest commit to a file\ngit reset --hard HEAD^ to revert to the previous commit\n\n**After this, your changes will be lost locally **\n\ngit log - confirm that you are now at the previous commit\ngit pull - correct the mistake you made in first place\npatch -p1 &lt; patch - apply the changes you originally made\ngit diff - to confirm that the changes have been reapplied\nNow, you do the regular commit, push routine\n\n\nUndo uncommitted changes: git stash followed by git stash drop\n\n‚Äúbut only use if you commit often‚Äù - guessing this is not good if your commit is somehow large and/or involves multiple files\n\nSearch commits by string: git log --grep &lt;string&gt;\nPinpoint bugs in your commit history\n\nInstead of sequentially searching each previous commit to look for the bad commit, git bisect helps you perform a bisect search for the commit which saves time.\nScenario: A bug is introduced in a codebase, but it is not discovered until later. The feature used to work, but now, it does not. The feature was definitely known to work 3 weeks ago.\nManual Workflow\n\nMake sure you‚Äôre in the current commit that‚Äôs bad and start git bisect\ngit bisect start\n1git bisect bad\n2git log --before=\"3 weeks\"\n3git checkout 3348b0\n\n1\n\nThis labels the current commit as bad (i.e.¬†bug is present)\n\n2\n\nThis lists every commit for last 3 weeks\n\n3\n\nSwitch to the commit that‚Äôs the version of the project that was 3 weeks ago when supposedly the feature was working. The first commit listed (i.e.¬†top) will be the commit closest to 3 weeks ago ‚Äî with older commits below it. You only need to use the first 6 or so digits of the commit hash.\n\n\nRecompile code and test commit for bug\ndevtools::load_all()\n\nload_all will recompile your package using this current version‚Äôs code\nAfter recompiling code, use your reproducible examplet to see if the bug is present in this version\nIf the bug is stil present, then go to the next older commit and repeat process. Keep loading older commits until you find one that doesn‚Äôt have the bug.\n\nIf ths is the case and assuming you don‚Äôt have to go back too much further to find a ‚Äúgood‚Äù commit, then you can stop here since you‚Äôll have found the bad commit that introduced the bug.\nIf you don‚Äôt find a good commit around this time period, then quit the current git bisect session using git bisect reset and choose whichever commit you stop at as the new starting point for a new git bisect session and repeat this whole workflow.\n\n\nGo to terminal and mark this commit as good\ngit bisect good\n\nGit will automatically switch you to commit that‚Äôs the midway point between the ‚Äústart‚Äù commit and the commit you labeled as ‚Äúgood.‚Äù\nIt tells you how many commits that are currently between you and the ‚Äústart‚Äù commit which is the same amount as between this midway commit and the commit you labelled as ‚Äúgood.\nIt also tells you how many more bisections (‚Äústeps‚Äù) you‚Äôll have to go through to find the commit resposible for the bug.\n\nRepeat Step 2 and test verstion for the bug. Then label commit as good or bad\ngit bisect bad\n\nAfterwards, git will automatically checkout to the commit that is either midway between this commit and ‚Äústart‚Äù or the end commit based on whether you label this current commit as good or bad.\n\nContinue labelling commits until git‚Äôs message is ‚Äú&lt;some commit hash&gt; is the first bad commit.‚Äù\n\nGit will also show you the commit message and a list of files that were changed.\n\nUse git show &lt;commit hash&gt; to see the diff\n(Optional) Use git bisect log &gt; file-name to save the session to a file.\nUse git bisect reset to exit and return you to where you were at the start of this workflow (HEAD)\n\nAutomatic Workflow\n\nWrite script that includes you reproducible exaample and have it return an error code of 0 if it does not contain the bug or return an non-zero error if it does contain the bug.\n\nExample\ndevtools::load_all()\n\nif (nr != nrow(df)) {\n  stop(\"error\")\n}\n\nload_allwill recompile your package using this current version‚Äôs code\nReturns non-zero error code if condition is not triggered (i.e.¬†False) and a 0 error code when the condition is triggered (i.e.¬†True).\nCould also use stopifnot here.\n\n\n(Optional) If you already know the commit hash of commit from 3 weeks ago and that does not have the bug, you can bypass the step 3.\n# Make sure you're in the current commit that's got the bug\ngit bisect start\n1git bisect bad\n2git bisect good 3348b0\n\n1\n\nThis labels the current commit as bad (i.e.¬†bug is present)\n\n2\n\nThis labels the commit from 3 weeks ago that you know doesn‚Äôt have the bug\n\n\nDo steps 1, 2, and 3 of the Manual Workflow\nRun auto-bisect\ngit bisect run Rscript test.R\n\ntest.R is the script from step 1 that determines whether the version (i.e.¬†commit hash) of your code has the bug.\nThis run through all the steps of the Manual Workflow and determine with the version of the code is ‚Äúgood‚Äù or ‚Äúbad‚Äù by whether the script returns an error code of zero or non-zero.\n\nRead final message to get the commit hash with bug in it.\n\nMessage will be ‚Äú&lt;some commit hash&gt; is the first bad commit.‚Äù\nGit will also show you the commit message and a list of files that were changed.\n\nSee steps 6, 7, and 8 of the Manual Workflow",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#pulling",
    "href": "qmd/git-general.html#pulling",
    "title": "25¬† General",
    "section": "25.5 Pulling",
    "text": "25.5 Pulling\n\nSave your changes, pull in an update, apply your changes\ngit stash\ngit pull\ngit stash pop\n\ngit stash pop throws away the (topmost, by default) stash after applying it, whereas\ngit stash apply leaves it in the stash list for possible later reuse (or you can then git stash drop it).\n\nRe potential merge conflicts\n\n‚ÄúFor instance, say your stashed changes conflict with other changes that you‚Äôve made since you first created the stash. Both pop and apply will helpfully trigger merge conflict resolution mode, allowing you to nicely resolve such conflicts‚Ä¶ and neither will get rid of the stash, even though perhaps you‚Äôre expecting pop too. Since a lot of people expect stashes to just be a simple stack, this often leads to them popping the same stash accidentally later because they thought it was gone.‚Äù\n\nPulling is fetching + merging\n\nFetching just gets the info about the commits made to the remote repo\ngit fetch origin\nSome technical discussion for always using git pull ‚Äìff\n\nhttps://blog.sffc.xyz/post/185195398930/why-you-should-use-git-pull-ff-only-git-is-a\nhttps://megakemp.com/2019/03/20/the-case-for-pull-rebase/\nit‚Äôs still confusing but pull rebase sounds fine to me\n‚Äìglobal tag says do it for all my repos\nnot sure what the true and only are for\n\ngit pull ‚Äìhelp will open doc in browser\n\n\nPulling by rebase\n\nLocal: using this method as default\ngit config pull.rebase true\ngit pull\nRemote\ngit pull --rebase\n\nPulling by fast-forward\n\nLocal: using this method as default\ngit config --global pull.ff only\ngit pull\nRemote\ngit pull --ff",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#branching",
    "href": "qmd/git-general.html#branching",
    "title": "25¬† General",
    "section": "25.6 Branching",
    "text": "25.6 Branching\n\nMisc\n\nCreate a new branch for each ticket you are working on or each data model. It can get sloppy when you put all your code changes on one branch.\nHEAD\n\nDetached HEAD\n\n\nCreate a branch (e.g.¬†‚Äútesting‚Äù)\ngit branch testing\nWork in a branch\ngit checkout testing\nThe files in your working directory change to the version saved in that branch\nIt adds, removes, and modifies files automatically to make sure your working copy is what the branch looked like on your last commit to it.\nCreate and work in a branch\n# new way\ngit switch -c testing\nor\ngit checkout -b testing\nor\ngit branch testing\ngit checkout testing\ncreates the branch and switches you to working in that branch\nIf you did a bunch of changes in a codebase, only to realize that you‚Äôre working on `master`,¬† switch will bring those local changes with you to the new branch. So I guess they won‚Äôt affect master then.\n\nUnless If you already committed to main, then those changes are both in your new branch and in main. So you would still have to clean up the main branch.\n\nDeleting a branch\n\nlocal branch\ngit branch -d testing\n\nremote branch\ngit push &lt;remoteName&gt; --delete &lt;branchName&gt;\nSee existing branches\ngit branch\nSee what has been commited the remote repo branches\ngit fetch origin\ngit branch -vv\n‚Äúorigin‚Äù is the name of the remote\nresult\ntesting¬† ¬† 7e424c3 [origin/testing: ahead 2, behind 1] change abc¬†\nmaster¬† ¬† ¬† 1ae2a45 [origin/master] Deploy index fix\n* issue¬† ¬† f8674d9 [origin/issue: behind 1] should do it¬† ¬† ¬† ¬† ¬†\ncart¬† ¬† ¬† ¬† 5ea463a Try something new\nformat: branch, last commit sha-1, local branch status vs remote branch status, commit message\nthe star indicates the HEAD pointer‚Äôs location (where you‚Äôre at, i.e.¬†checkout)\ntesting branch\n\n‚Äúahead 2‚Äù means¬† I committed twice to the local testing branch¬†and this work has not been pushed to the remote testing branch repo yet.\n‚Äúbehind 1‚Äù means someone has pushed a commit to the remote testing branch repo and we haven‚Äôt merged this work to our local testing branch\n\nGet the last 10 branches that you‚Äôve committed to locally:\ngit branch --sort=-committerdate | head -n 10\nRename branch\n# change locally\ngit branch --move &lt;bad-branch-name&gt; &lt;corrected-branch-name&gt;\n# change remotely in repo\ngit push --set-upstream origin &lt;corrected-branch-name&gt;\n# confirm change\ngit branch --all\nHEAD determines to which branch new commits are added\n\nExample\n\n‚Äútesting‚Äù branch is created (not shown in above picture)\n\nHEAD points at ‚Äúmaster‚Äù branch\n‚Äúmaster‚Äù branch and the new ‚Äútesting‚Äù branch both point at commit, f30ab.\nf30ab commit points to previous commit 34ac2\n\nuser executes checkout to ‚Äútesting‚Äù branch (not shown in picture)\n\nHEAD now points to testing branch\n\nuser commits 87ab2 (shown in pic)\n\n87ab2 is committed to the ‚Äútesting‚Äù branch\n‚Äútesting‚Äù branch is now ahead of the ‚Äúmaster‚Äù branch by 1 commit\n\n\nExample\n\nEverything above happens but now another user commits the master branch.\n\nBoth branches are in conflict. The testing branch is ahead and behind by 1 commit\n\n\n\nMerging\n\n\nNotes\n\nNEVER merge your branch locally on your machine with the master branch, ALWAYS merge online via pull request\n\nSteps\n\nPush final changes and use of a pull request\nSwitch to master branch locally and pull the merged changes\n\n\n\nUpdate branch with work that‚Äôs been done in master branch\n\nAfter updating your local branch, push to remote repo (no commit necessary)\n# while in branch\ngit merge master\n\n\nFast-Forward\n\nExample\n\nBefore the merge\n\nthe testing branch is 1 commit ahead of the master branch and the master branch doesnt have a new commit\n\nAfter the merge\n\nmaster is moved forward to the testing branch commit\n\n\nCode (merging work in branch with the master branch for production)\n# currently in test branch\ngit checkout master\ngit merge testing\n\nLines in file are marked\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.html\n# &lt;div id=\"footer\"&gt;contact : email.support@github.com&lt;/div&gt;\n# =======\n# &lt;div id=\"footer\"&gt;\n# please contact us at support@github.com\n# &lt;/div&gt;\n# &gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html\nAbove ======= is the master branch version of the code and below is the iss53 branch version\nMake necessary changes and save the file\ngit add . or git add &lt;resolved file&gt;\n\nTells git that conflict is resolved\n\nCheck status to confirm everything has been resolved\ngit status\n\n    On branch master\n    All conflicts fixed but you are still merging.\n      (use \"git commit\" to conclude merge)\n    Changes to be committed:\n      modified:  index.html\ngit commit\n\nno message required (there‚Äôs a default message) but you can add one if you want\n\nExample\n\niss53 branch ahead of master by 2 commits (c3, c5) and behind 1 commit (c2)\nSame code as Fast-Forward merge but git handles the merge a bit differently\ngit checkout master¬†\ngit merge iss53\n\n\n\nC6 (right pic) is called a ‚Äúmerge commit.‚Äù Its created by git and points to two commits instead of one.\nNo need to merge with master (i.e.¬†update local iss53 branch with c4 changes in master) before committing final changes\n\nIf there are changes in the same lines of code C4 and C5, then there will be a conflict (See below, Conflicts &gt;&gt; Example)\n\n\nConflicts\n\nExample\n\nChanged files in C4 (see above example) are in the same lines of the same files that you made changes to in C5\n\nRemember: you‚Äôre now in the master branch since you did checkout master as part of the merge code\nSteps\n\nCheck status to which files are causing the conflict (e.g.¬†index.html)\ngit status\n¬† Unmerged paths:\n¬† (use \"git add &lt;file&gt;...\" to mark resolution)¬†\n¬† ¬† both modified:¬† ¬† ¬† index.html\n\n\n\n\nMoving between branches\n\nfrom master to testing\ngit checkout testing\n\nlocal files are deleted and replaced with branch versions\n\nalternative: worktree\n\nExample\n\nWhat happens when you move from branch-a to branch-b\nBRANCH-A¬† ¬† ¬† ¬† BRANCH-B\nalpha.txt¬† ¬† ¬† alpha.txt\nbravo.txt\ncharlie.txt¬† ¬† charlie.txt\n                delta.txt\n\nbravo text is deleted from your local disc and delta.txt is added\nIf any changes to alpha.txt or charlie.txt have been made and no commit has been made, the checkout will be aborted\n\nSo either revert the changes or commit the changes\n\nUntracked files or newly created files\n\nIf you have branch-A checked out and you create a new file called echo.txt, Git will not touch this file when you checkout branch-B. This way, you can decide that you want to commit echo.txt against branch-B without having to go through the hassle of (1) move the file outside the repo, (2) checkout the correct branch, and (3) move the file back into the repo.",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#collaboration",
    "href": "qmd/git-general.html#collaboration",
    "title": "25¬† General",
    "section": "25.7 Collaboration",
    "text": "25.7 Collaboration\n\nAdd collaborators to your repository\nOne person invites the others and provides them with read/write access (github docs)\n\nSteps\n\nGo to the settings for your repository\nmanage access &gt;&gt; ‚Äúinvite a collaborator‚Äù\n\nSearch for each collaborator by full name, acct name, or email\nClick ‚ÄúAdd &lt;name&gt; to &lt;repo&gt;‚Äù\n\nEach collaborator will need to accept the invitation\n\nSent by email",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/js.html",
    "href": "qmd/js.html",
    "title": "JS",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-misc",
    "href": "qmd/js.html#sec-js-misc",
    "title": "JS",
    "section": "",
    "text": "Resources\n\nOnline Interactive Cheat Sheet\nLearn Just Enough JavaScript\n\nBasics: variables, objects, arrays, functions, conditionals, loops\n\nHow to run R code in the browser with webR\n\nNice breakdown of generic JS code to run scripts on a webpage\n\nJavaScript for Data Science\n\nhrbmstr: ‚Äújavascript has the advantage over R/Python for both visualization speed ‚Äî thanks to GPU integration ‚Äî and interface creation ‚Äî thanks to the ubiquity of HTML5 ‚Äî means that people will increasingly bring their own data to websites for initial exploration first‚Äù\nconsole.log is the print method",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-basics",
    "href": "qmd/js.html#sec-js-basics",
    "title": "JS",
    "section": "Basics",
    "text": "Basics\n\nOperators\n\n// : comments\n... : If you want to copy all the values in your array, and add some new ones, you can use the {‚Ä¶} notation.\n${&lt;code&gt;} : Anything within the${} get ran as code\n\nExample:\n`${b.letter}: ${ (b.frequency*100).toFixed(2) }%`\n\nBackticks indicate it‚Äôs like a glue string or f string (i.e.¬†uses code)\nb.letter and b.frequency are properties in an array\nto.Fixed is a method that rounds the value to to 2 decimal places\nThis was an example of a tooltip, so output would look like ‚ÄúF: 12.23%‚Äù\n\n\n\nVariables\nmyNumber = 10 * 1000\nvariableSetToCodeBlock = {\n¬† const today = new Date();\n¬† return today.getFullYear()\n}\nObject: myObject = ({name: \"Paul\", age: 25})\n\nContained within curly braces, { }\nSubset property, name:\n\nmyObject.name which returns value, Paul\nmyObject[\"name\"] which is useful if you have spaces, etc. in your property names\n\nTypes\n\nMap: Object holds key-value pairs and remembers the original insertion order of the keys\n\ne.g.¬†See Stats &gt;&gt; By Group\nD3 Groups, Rollup, Index Docs\n\n\n\nArrays\n\nList of objects\n\nContained within brackets, [ ]\nEach row is an object and each column is a property of that object and that property has a value associated with it\n\nBasic examples\nmyArray = [1, 2, 3, 4]\nmyArray = [[1, 2], [3, 4]] // arrays within arrays\nmyArray = [1, 'cat', {name: 'kitty'}] // objects within arrays\nDF-like array\nmyData = [\n¬† {name: 'Paul', city: 'Denver'},\n¬† {name: 'Robert', city: 'Denver'},\n¬† {name: 'Ian', city: 'Boston'},\n¬† {name: 'Cobus', city: 'Boston'},\n¬† {name: 'Ayodele', city: 'New York'},\n¬† {name: 'Mike', city: 'New York'},\n]\n\nEquivalent Functions: Traditional vs Arrow\n// traditional\nfunction myFunctionWithParameters(firstName, lastName) {\n¬† return `My first name is ${firstName}, and my last name is ${lastName}.`\n}\n// arrow\nmyModernFunctionWithParameters = (firstName, lastName) =&gt; {\n¬† return `My first name is ${firstName}, and my last name is ${lastName}.`\n}\n\nArrow: Arguments are in the parentheses and the function is inside the curly braces\nString with variables needs to be surrounded by backticks\n\nFunctions Inside Methods: Traditional vs Arrow\n// traditional\n[1, 2, 3, 4, 5].filter(function(d) { return d &lt; 3 })\n// arrow\n[1, 2, 3, 4, 5].filter(d =&gt; d &lt; 3)\n\nThe argument is d but without parentheses and the function is d &lt; 3 without the curly braces\nThe function inputs each row/value of the array, so d is a row/value of the array. Then, the function does something to that row.\n\nConditionals\n\n== vs ===\n1 == '1' // true\n1 === '1' // false\n\n== is a logical test to see if two values are the same\n\n=== is a logical test to see if two values are the same and also checks if the value types are the same\n\nIf/Then\nif(1 &gt; 2) {¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† // If this statement is true\n¬† ¬† return 'Math is broken'¬† ¬† ¬† ¬† ¬† ¬† ¬† // return this\n} else {¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† // if the first statement was not true\n¬† ¬† return 'Math still works!'¬† ¬† ¬† ¬† ¬† // return this\n}\n\n// using ternary operator \"?\"\n\nUsing ternary operator ‚Äú?‚Äù\n\nSyntax: condition ? exprIfTrue : exprIfFalse\nExample: d =&gt; d.frequency &gt;= minFreq ? \"steelblue\" : \"lightgray\"\n\nSays if the frequency property is &gt;= the variable, minFreq, value, then use steelblue otherwise use lightgray\n\n\n\n\nFor-Loop\nlet largestNumber = 0; // Declare a variable for the largest number\n\nfor(let i = 0; i &lt; myValues.length - 1; i++) {¬† ¬† // Loop through all the values in my array\n¬† ¬† if(myValues[i] &gt; largestNumber) {¬† ¬† ¬† ¬† ¬† ¬† ¬† // Check if the value in the array is larger that the largestNumber\n¬† ¬† ¬† largestNumber = myValues[i]¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† // If so, assign the value as the new largest number\n¬† ¬† }\n}\n\nreturn largestNumber\n\nThe first statement sets a variable (let i = 0)\nThe second statement provides a condition for when the loop will run (whenever i &lt; myValues.length - 1)\nThe third statement says what to do each time the code block is executed (i++, which means to add 1 to i)\n\nWhile-Loop\nlet largestNumber = 0;¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† // Create a variable for the largest number\nlet i = 0;\nwhile(i &lt; myValues.length - 1) {\n¬† ¬† if(myValues[i] &gt; largestNumber) {¬† ¬† ¬† ¬† // Check if the value in the array is larger that the largestNumber\n¬† ¬† ¬† largestNumber = myValues[i]¬† ¬† ¬† ¬† ¬† ¬† // If so, assign the value as the new largest number\n¬† ¬† }\n¬† ¬† i++;\n}\nreturn largestNumber",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-cleaning",
    "href": "qmd/js.html#sec-js-cleaning",
    "title": "JS",
    "section": "Cleaning",
    "text": "Cleaning\n\nMisc\n\nNotes from: Horst article\n\nFilter objects: myData.filter(d =&gt; d.city == 'Denver')\nSelect properties: myNewArray = salesData.map(d =&gt; ({ date: d.date, product: d.product, totalRevenue: d.totalRevenue }))\n\nIn some contexts, this, d =&gt; d[\"mileage (mpg)\"] , is also used to select columns\n\nArrange objects: salesData.sort((a, b) =&gt; a.totalRevenue - b.totalRevenue)\n\nReorders salesData by totalRevenue (low to high)\n\nMutate properties: salesData.map(d =&gt; ({...d, discountedPrice: 0.9 * d.unitPrice }))\n\nAdds a new column to salesData with a discountedPrice, which takes 10% off each unitPrice.\n\nGroup_By: d3.rollup(salesData, v =&gt; d3.sum(v, d =&gt; d.totalRevenue), d =&gt; d.region)\n\nReturn the sum of totalRevenue for each region in salesData.\nrollup might actually be a summarize and the group_by is handled in the syntax\n\nRename: salesData.map(d =&gt; ({...d, saleDate: d.date }))\n\nAdds a new column called saleDate by storing a version of the date with new name saleDate and keeping all other columns.\n\nSubset value: salesData.map(d =&gt; d.description)[3]\n\nAccess the fourth value from the description property in salesData\n\nUnite:\nsalesData.map(d =&gt; ({...d, fullDescription: `${d.product} ${d.description}`}))\n\nUnite the product and description columns into a single column called fullDescription, using a comma as a separator.\n\nLeft Join: *using {{{arquero}}} tables* salesData.join_left(productDetails, ['product', 'product_id'])\n\nJoin information from a productDetails table to salesData. Join on product in salesData and product_id in productDetails.",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-stats",
    "href": "qmd/js.html#sec-js-stats",
    "title": "JS",
    "section": "Stats",
    "text": "Stats\n\nMisc\n\nNotes from: Horst article\n\nIn examples, waterUsage is the array; waterGallons is the property.\n\n\nMean: d3.mean(waterUsage.map(d =&gt; d.waterGallons))\n\nReturns a Value\n\nStd.Dev: d3.deviation(waterUsage.map(d =&gt; d.waterGallons))\nMedian: d3.median(waterUsage.map(d =&gt; d.waterGallons))\nMin/Max: d3.min(waterUsage.map(d =&gt; d.waterGallons))\nTotal Observations (i.e.¬†nrow ): waterUsage.length\nBy Group:\n\npropertyId is the discrete, grouping variable\nMean: waterMeans = d3.rollup(waterUsage, v =&gt; d3.mean(v, d =&gt; d.waterGallons), d =&gt; d.propertyId)\n// Returns a map object\nwaterMeans\n{\n¬† \"A001\" =&gt; 39.53389830508475\n¬† \"B002\" =&gt; 53.57627118644068\n¬† \"C003\" =&gt; 27.45762711864407\n¬† \"D004\" =&gt; 80.1864406779661\n}\n\n// View in a JS Table\n// ** Must be in a separate cell **\nInputs.table(waterMeans.map(([propertyId, meanWaterGallons]) =&gt; ({propertyId, meanWaterGallons})))\nCount: d3.rollup(waterUsage, v =&gt; d3.count(v, d =&gt; d.waterGallons), d =&gt; d.propertyId)\n\nConditional Counts: waterUsage.filter(d =&gt; d.waterGallons &gt; 90 && d.propertyId == \"B002\").length\n\nApplies two conditionals and counts the observations\n\nRanks\nwaterUsage.map((d, i) =&gt; ({...d, rank: d3.rank(waterUsage.map(d =&gt; d.waterGallons), d3.descending)[i] + 1}))\n\n1 is added so that ranks start at 1 instead of 0\n\nPercentiles: d3.quantile(waterUsage.map(d =&gt; d.waterGallons), 0.9) (e.g.¬†90th)",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-obs",
    "href": "qmd/js.html#sec-js-obs",
    "title": "JS",
    "section": "Observable",
    "text": "Observable\n\nA collaborative, online notebook platform that comes with libraries loaded to make it fairly straightforward to dive into ad hoc data analysis or produce complete reports.\nIn Observable, if you‚Äôre running a JavaScript cell that contains more than just a simple variable assignment (like myVariable = 'Hello World' ), you need to run a code block (i.e.¬†bracket lines of code in curly braces, {}).\nYou can open your notebook in Safe Mode and edit your work without running it.\n\nGood for debugging (e.g.¬†infinite while-loops)",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-def",
    "href": "qmd/js.html#sec-js-def",
    "title": "JS",
    "section": "Definitions",
    "text": "Definitions\n\nJSON vs R List\n{¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† list(\n¬† ¬† boolean: true,¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† boolean = TRUE,\n¬† ¬† string: \"hello\",¬† ¬† ¬† ¬† ¬† ¬† ¬† string = \"hello\",\n¬† ¬† vector: [1,2,3]¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† vector = c(1,2,3)\n}¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† )\n\n// Access¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # Access\njson.vector¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† list$vector\nDependencies\nHTML¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† R (shiny)\n&lt;head&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† tags$head(\n¬† ¬† &lt;!-- JavaScript --&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† tags$script(src = \"path/to/file.js\")\n¬† ¬† &lt;script src=\"path/to/file.js\"&gt;&lt;/script&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† tags$link(\n¬† ¬† &lt;!-- CSS --&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† rel = \"stylesheet\",\n¬† ¬† &lt;link rel=\"stylesheet\" href=\"path/to/file.css&gt;¬† ¬† ¬† ¬† href = \"path/to/file.css\n&lt;/head&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ))\nd is each row and =&gt; is function\n(d) =&gt; d.year === 2020\n\nSays for each row in your data, the year column must equal 2020\n\nCallback Function - A function that is passed to another function as a parameter. In other words, a function ‚Äúcalls back‚Äù to previously defined function.\nfunction print(callback) {¬†\n¬† ¬† callback();\n}\n\ncallback is the callback function and is a parameter of the print function\nCallbacks make sure that a function is not going to run before a task is completed but will run right after the task has completed.\nExample:\n// \"Click here\" button in a web app\n&lt;button id=\"callback-btn\"&gt;Click here&lt;/button&gt;\ndocument.queryselector(\"#callback-btn\")\n¬† ¬† .addEventListener(\"click\", function() {¬† ¬†\n¬† ¬† ¬† console.log(\"User has clicked on the button!\");\n});\n\nFirst, button selected by its id, and then we add an event listener with the addEventListener method. It takes 2 parameters. The first one is its type, click, and the second parameter is a callback function, which logs the message when the button is clicked.\n\n\nAnonymous Function - Same as a callback but unnamed. It‚Äôs a¬† function that is defined within another function.\nsetTimeout(function() {¬†\n¬† ¬† console.log(\"This message is shown after 3 seconds\");\n}, 3000);\n\n// if the function were named\nconst message = function() {¬†\n¬† ¬† console.log(\"This message is shown after 3 seconds\");\n}\n\n// as an arrow function\nsetTimeout(() =&gt; {¬†\n¬† ¬† console.log(\"This message is shown after 3 seconds\");\n}, 3000);\n\nThe function used as a parameter has no name. console.log is the contents of the function.",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-nfcd",
    "href": "qmd/js.html#sec-js-nfcd",
    "title": "JS",
    "section": "Notes From Covidcast Dashboard",
    "text": "Notes From Covidcast Dashboard\n\nNotes from\n\nCovidcast Dashboard: reactable + sparkline tooltip (link)\n\ndiv = vertical label or container , span = horizontal\nFormat: type, styling, value\n2 divs would result in a 2 element vertical label while 2 spans would be a 2 element horizontal label\nExample: A div container holding 2 spans which creates a ‚Äúdate value‚Äù horizontal label\n\"function (_ref) {\nvar datum = _ref.datum;\nreturn React.createElement(\n¬† 'div',\n¬† null,\n¬† datum.date && React.createElement(\n¬† ¬† ¬† 'span',\n¬† ¬† ¬† {style: {\n¬† ¬† ¬† ¬† ¬† backgroundColor: 'black', color: 'white',\n¬† ¬† ¬† ¬† ¬† padding: '3px', margin: '0px 4px 0px 0px', textAlign: 'center'\n¬† ¬† ¬† ¬† }},\n¬† ¬† ¬† datum.date[0].split('-').slice(1).join('/')\n¬† ),\n¬† React.createElement(\n¬† ¬† ¬† 'span',\n¬† ¬† ¬† {style: {\n¬† ¬† ¬† ¬† fontWeight: 'bold', fontSize: '1.1em',\n¬† ¬† ¬† ¬† padding: '2px'\n¬† ¬† ¬† }},\n¬† ¬† ¬† datum.y ? datum.y.toLocaleString(undefined, {maximumFractionDigits: 0}) : '--'\n¬† )\n¬† );\n}\"\n\nCSS: margin, padding\n\nFormat is top, right, bottom, left (ordered like a clock)\nRequires units like ‚Äúpx‚Äù\nNo commas separate the values\n{margin: '0px 4px', padding: '0px 0px 0px 4px'}\n\nMaybe for 0s it doesn‚Äôt matter\nSee bkmk in css/definitions for explanations behind specifications with less than 4 numbers\n\ne.g.¬†2 is ‚Äòtop/bottom left/right‚Äô\n\n\n\nString manipulation\ndatum.endDate[0].split('-').slice(1).join('/')\n\nTreats variable as a string object\nLooks in data arg, finds endDate variable\nIts a list variable so requires the [0] (0 part an index?)\nDate format is ymd, so splits value by ‚Äú-‚Äù separator, removes 1st value (year), joins the rest of the values (month, day) with ‚Äú/‚Äù\n\nIf slice(2), removes first 2 values (left to right)\n\n\nConditional\nlabelPosition = htmlwidgets::JS(\"(d, i) =&gt; (i === 0 || i === 1 ? 'right' : 'left')\")\n\nSays that if index of data value, d, is 0 or 1 then label should be positioned on the right of the point, else place the label on the left of the point",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/simulation-data.html",
    "href": "qmd/simulation-data.html",
    "title": "Simulation, Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Simulation, Data"
    ]
  },
  {
    "objectID": "qmd/simulation-data.html#sec-sim-data-misc",
    "href": "qmd/simulation-data.html#sec-sim-data-misc",
    "title": "Simulation, Data",
    "section": "",
    "text": "Todo - for the ‚ÄútrtAssign‚Äù mess with ratio and the number of ratios\nAlso see\n\nBkmks Data &gt;&gt; Data Simulation\nPandas-Time Series-Simulation\n\nA Gaussian and Standard GARCH time-series that‚Äôs frequently encountered in econometrics\n\n\nPackages\n\n{structmcmc} - A set of tools for performing structural inference for Bayesian Networks using MCMC\n\nPapers\n\nGeneration and analysis of synthetic data via Bayesian networks: a robust approach for uncertainty quantification via Bayesian paradigm",
    "crumbs": [
      "Simulation, Data"
    ]
  },
  {
    "objectID": "qmd/simulation-data.html#sec-sim-data-simstudy",
    "href": "qmd/simulation-data.html#sec-sim-data-simstudy",
    "title": "Simulation, Data",
    "section": "{simstudy}",
    "text": "{simstudy}\n\nMisc\n\nDocs\n\n\n\nReference\n\nAvailable distributions (link)\n\nProbability Distributions\nnonrandom: For constants; can be a numeric or a string with a formula that defines a dependency on another variable\nclusterSize: For variable cluster sizes but a constant total sample size\n\nformula: The (fixed) total sample size\nvariance: A (non-negative) dispersion measure that represents the variability of size across clusters\n\nIf the dispersion is set to 0, then cluster sizes are constant\n\n\ntrtAssign: For treatment assignment\n\nformula: Ratio which is separated by semicolons and number of treatments\n\ne.g.¬†2 values = 2 groups and ‚Äú1;2‚Äù says group 2 has twice as many units and group 1\n\nvariance: Stratification; ratio in formula is used as the stratification ratio (e.g.¬†unbalanced treatment groups ‚Üí unbalanced stratification)\nExample\ndef &lt;- \n  defData(def, \n          varname = \"rx\", \n          dist = \"trtAssign\",\n          formula = \"1;1;2\", \n          variance = \"male;over65\")\n\ncount(studytbl, rx)\n#&gt; # A tibble: 3 √ó 2\n#&gt; ¬† ¬† rx¬† ¬† n\n#&gt; ¬† &lt;int&gt; &lt;int&gt;\n#&gt; 1¬† ¬† 1¬† ¬† 84\n#&gt; 2¬† ¬† 2¬† ¬† 82\n#&gt; 3¬† ¬† 3¬† 164\n\ncount(studytbl, male, rx)\n#&gt; # A tibble: 6 √ó 3\n#&gt; ¬† male¬† ¬† rx¬† ¬† n\n#&gt; ¬† &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1¬† ¬† 0¬† ¬† 1¬† ¬† 40\n#&gt; 2¬† ¬† 0¬† ¬† 2¬† ¬† 39\n#&gt; 3¬† ¬† 0¬† ¬† 3¬† ¬† 78\n#&gt; 4¬† ¬† 1¬† ¬† 1¬† ¬† 44\n#&gt; 5¬† ¬† 1¬† ¬† 2¬† ¬† 43\n#&gt; 6¬† ¬† 1¬† ¬† 3¬† ¬† 86\n\ncount(studytbl, over65, rx)\n#&gt; # A tibble: 6 √ó 3\n#&gt; ¬† over65¬† ¬† rx¬† ¬† n\n#&gt; ¬† &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1¬† ¬† ¬† 0¬† ¬† 1¬† ¬† 66\n#&gt; 2¬† ¬† ¬† 0¬† ¬† 2¬† ¬† 65\n#&gt; 3¬† ¬† ¬† 0¬† ¬† 3¬† 130\n#&gt; 4¬† ¬† ¬† 1¬† ¬† 1¬† ¬† 18\n#&gt; 5¬† ¬† ¬† 1¬† ¬† 2¬† ¬† 17\n#&gt; 6¬† ¬† ¬† 1¬† ¬† 3¬† ¬† 34\n\n\nFunctions\n\ndefData(dtDefs = NULL, varname, formula, variance = 0, dist = \"normal\", link = \"identity\", id = \"id\") - Initially creates a data.table or adds a column to a data.table with instructions about creating a variable\n\nformula: Numeric constant or string formula for the mean, probability of event (binary), probability of success (binomial), etc.\n\ndefDataAdd(dtDefs = NULL, varname, formula, variance = 0, dist = \"normal\", link = \"identity\") - Creates a variable definition like defData but is used to augment a already generated dataset. Used as input to addColumns which will generate the variable data from the instructions in this object and add it as a column to the already generated dataset.\ngenCluster(dtClust, cLevelVar, numIndsVar, level1ID, allLevel2 = TRUE) - After generating cluster-level data, this function takes the number of clusters and the sizes of each cluster from that data, and does something like expand.grid to generate an individual-level dataset. Also, adds an id variable.\n\ndtClust: Cluster-Level Data\ncLevelVar: Cluster variable from the cluster-level data\nnumIndsvar: Variable with the number of units per cluster from the cluster-level data\nlevel1ID: Name you want for your individual-level ID variable\n\n\n\n\n\nVariable Dependence\n\nBinary depends on a Binary\n\nDefinitions\ndef &lt;- defData(varname = \"male\", dist = \"binary\",\n               formula = .5 , id=\"cid\")\ndef &lt;- defData(def, varname = \"over65\", dist = \"binary\",\n               formula = \"-1.7 + .8*male\", link=\"logit\")\nWhat‚Äôs happening\nmale &lt;- c(1,1,0,1,0,0,0,1,0,1)\nlogits &lt;- -1.7 + 0.8 * male\nprobabilities &lt;- boot::inv.logit(logits)\nover65 &lt;- rbinom(n = 10, size = 1, prob = probabilities)\n\nThe formula in the logits line defines the relationship between being male and being over 65yrs old.\nMales in this sample will have a higher probability (0.2890505) of being over 65yrs old than females (0.1544653)\nTo sample from a Bernoulli distribution, set size = 1\nover65 is an indicator where each value is determined by a separate probability parameter for a Bernoulli distribution\n\n\n\n\n\nClustered with Cluster-Level Random Effect\n\nExample: Fixed Cluster sizes; Balanced\n\nCluster Definitions\nd0 &lt;- defData(varname = \"n\", formula = 20, dist = \"nonrandom\")\nd0 &lt;- defData(d0, varname = \"a\", formula = 0, variance = 0.33)\nd0 &lt;- defData(d0, varname = \"rx\", formula = \"1;1\", dist = \"trtAssign\")\nd1 &lt;- defDataAdd(varname = \"y\", formula = \"18 + 1.6 * rx + a\",\n                 variance = 16, dist = \"normal\")\n\nn: sample size for the cluster\n\ndist = ‚Äúnonrandom‚Äù and formula = 20 says use a constant for the cluster sizer\n\nrx: treatment indicator\n\ndist = ‚ÄútrtAssign‚Äù and formula = ‚Äú1;1‚Äù says 2 treatment groups and they‚Äôre balanced\n\ny: the individual-level outcome is a function of the treatment assignment and the cluster effect, as well as random individual-level variation\na: random individual-level variation (i.e.¬†random effect)\n\nRandom Effects are sampled from \\(\\mathcal{N}(0, \\sigma)\\) where the variance is typically estimated in a Mixed Effects model.\n\n\nGenerate Cluster-Level Data\nset.seed(2761)\ndc &lt;- genData(10, d0, \"site\")\ndc\n##¬† ¬† site¬† n¬† ¬† ¬† a rx\n##¬† 1:¬† ¬† 1 20 -0.3548¬† 1\n##¬† 2:¬† ¬† 2 20 -1.1232¬† 1\n##¬† 3:¬† ¬† 3 20 -0.5963¬† 0\n##¬† 4:¬† ¬† 4 20 -0.0503¬† 1\n##¬† 5:¬† ¬† 5 20¬† 0.0894¬† 0\n##¬† 6:¬† ¬† 6 20¬† 0.5294¬† 1\n##¬† 7:¬† ¬† 7 20¬† 1.2302¬† 0\n##¬† 8:¬† ¬† 8 20¬† 0.9663¬† 1\n##¬† 9:¬† ¬† 9 20¬† 0.0993¬† 0\n## 10:¬† 10 20¬† 0.6508¬† 0\n\nGenerates 10 clusters labelled as site according to the instructions in d0\n\nGenerate Individual Level Data\ndd &lt;- genCluster(dc, \"site\", \"n\", \"id\")\ndd &lt;- addColumns(d1, dd)\ndd\n##¬† ¬† ¬† site¬† n¬† ¬† ¬† a rx¬† id¬† ¬† y\n##¬† 1:¬† ¬† 1 20 -0.355¬† 1¬† 1 17.7\n##¬† 2:¬† ¬† 1 20 -0.355¬† 1¬† 2 16.2\n##¬† 3:¬† ¬† 1 20 -0.355¬† 1¬† 3 19.2\n##¬† 4:¬† ¬† 1 20 -0.355¬† 1¬† 4 20.6\n##¬† 5:¬† ¬† 1 20 -0.355¬† 1¬† 5 14.7\n##¬† ---¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n## 196:¬† 10 20¬† 0.651¬† 0 196 25.3\n## 197:¬† 10 20¬† 0.651¬† 0 197 22.1\n## 198:¬† 10 20¬† 0.651¬† 0 198 13.2\n## 199:¬† 10 20¬† 0.651¬† 0 199 15.6\n## 200:¬† 10 20¬† 0.651¬† 0 200 13.8\n\ngenCluster performs an expand.grid to generate an individual-level dataset along with adding an ID variable\naddColumns uses individual-level data and outcome variable definition to generate the outcome variable and add it to the dataset.\n\n\nExample: Varying Cluster Sizes and therefore Varying Sample Size\nd0 &lt;- defData(varname = \"n\", formula = 20, dist = \"poisson\")\ngenData(10, d0, \"site\")\n##¬† ¬† site¬† n\n##¬† 1:¬† ¬† 1 13\n##¬† 2:¬† ¬† 2 18\n##¬† 3:¬† ¬† 3 21\n##¬† 4:¬† ¬† 4 26\n##¬† 5:¬† ¬† 5 25\n##¬† 6:¬† ¬† 6 27\n##¬† 7:¬† ¬† 7 23\n##¬† 8:¬† ¬† 8 30\n##¬† 9:¬† ¬† 9 23\n## 10:¬† 10 20\n\nFormula sets the poisson distribution parameter, \\(\\lambda = 20\\). So sizes are sampled from poisson distribution with that mean/variance\nTo increase the variability between clusters, use the negative binomial distribution\nMost likely leads to an unbalanced design\n\nExample: Varying Cluster Sizes but Constant Sample Size\n# moderately varying cluster sizes\nd0 &lt;- defData(varname = \"n\", formula = 200, variance = 0.2, dist = \"clusterSize\")\ngenData(10, d0, \"site\")\n\n##¬† ¬† site¬† n\n##¬† 1:¬† ¬† 1 20\n##¬† 2:¬† ¬† 2 28\n##¬† 3:¬† ¬† 3 25\n##¬† 4:¬† ¬† 4 24\n##¬† 5:¬† ¬† 5 28\n##¬† 6:¬† ¬† 6 22\n##¬† 7:¬† ¬† 7¬† 7\n##¬† 8:¬† ¬† 8 13\n##¬† 9:¬† ¬† 9 22\n## 10:¬† 10 11\n\n# Very highly varying cluster sizes\nd0 &lt;- defData(varname = \"n\", formula = 200, variance = 5, dist = \"clusterSize\")\ngenData(10, d0, \"site\")\n##¬† ¬† site¬† n\n##¬† 1:¬† ¬† 1¬† 10\n##¬† 2:¬† ¬† 2¬† 2\n##¬† 3:¬† ¬† 3¬† 17\n##¬† 4:¬† ¬† 4¬† 2\n##¬† 5:¬† ¬† 5¬† 49\n##¬† 6:¬† ¬† 6 110\n##¬† 7:¬† ¬† 7¬† 1\n##¬† 8:¬† ¬† 8¬† 4\n##¬† 9:¬† ¬† 9¬† 1\n## 10:¬† 10¬† 4\n\nTotal sample size is fixed at 200 (formula), but individual cluster sizes are allowed to vary.\nvariance: A dispersion parameter that controls the amount of varying of the cluster sizes",
    "crumbs": [
      "Simulation, Data"
    ]
  },
  {
    "objectID": "qmd/association-general.html",
    "href": "qmd/association-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-misc",
    "href": "qmd/association-general.html#sec-assoc-gen-misc",
    "title": "General",
    "section": "",
    "text": "Also see\n\nEDA &gt;&gt; Correlation\nNotebook &gt;&gt; Statistical Inference &gt;&gt; Correlation\n\nE(œÖ|x)=0 is equivalent to Cov(x,œÖ)=0 or Cor(x,œÖ)=0\nA negative correlation between variables is also called anticorrelation or inverse correlation\nIndependence - Two random variables are independent if the product of their individual probability density functions equals the joint probability density function",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-partcor",
    "href": "qmd/association-general.html#sec-assoc-gen-partcor",
    "title": "General",
    "section": "Partial Correlation",
    "text": "Partial Correlation\n\nStatistical Formula\n\\[\n\\frac{\\mbox{Cov}(X, Y) - \\mbox{Cov}(X, Z) \\cdot \\mbox{Cov}(Y, Z)}{\\sqrt{\\mbox{Var}(X) - \\mbox{Cov}(X, Z)^2}\\cdot \\sqrt{\\mbox{Var}(Y) - \\mbox{Cov}(Y, Z)^2}}\n\\]\nMeasures the association (or correlation) between two variables when the effects of one or more other variables are removed from such a relationship.\n\nIn the above equation, I think it‚Äôs the partial correlation between x and y given z.\n\nMisc\n\nResources\n\nDealing with correlation in designed field experiments: part I\n\nExcellent tutorial on partial, joint correlations in block design\n\nppcor pkg: An R Package for a Fast Calculation to Semi-partial Correlation Coefficients\n\nExplainer for semi-partial, partial correlation\n\nAlso see notebook for a method using regression models\n\n\nExample: psych::partial.r(y ~ x - z, data)\nExample: {correlation}\nhead(correlation::correlation(mtcars, partial = TRUE))\n\n#&gt; # Correlation Matrix (pearson-method)\n\n#&gt; Parameter1 | Parameter2 |     r |         95% CI | t(30) |      p\n#&gt; -----------------------------------------------------------------\n#&gt; mpg        |        cyl | -0.02 | [-0.37,  0.33] | -0.13 | &gt; .999\n#&gt; mpg        |       disp |  0.16 | [-0.20,  0.48] |  0.89 | &gt; .999\n#&gt; mpg        |         hp | -0.21 | [-0.52,  0.15] | -1.18 | &gt; .999\n#&gt; mpg        |       drat |  0.10 | [-0.25,  0.44] |  0.58 | &gt; .999\n#&gt; mpg        |         wt | -0.39 | [-0.65, -0.05] | -2.34 | &gt; .999\n#&gt; mpg        |       qsec |  0.24 | [-0.12,  0.54] |  1.34 | &gt; .999\n#&gt; \n#&gt; p-value adjustment method: Holm (1979)\n#&gt; Observations: 32\n\nVisualization\n\npacman::p_load(see, ggraph)\ncorrelation::correlation(mtcars, partial = TRUE) |&gt; \n  plot()\n\nGraphical LASSO\n\nComputing covariance matrices are computationally expensive while computing its inverse can be less so. This algorithm calculates the inverse covariance matrix (ICT), aka Precision Matrix, and it‚Äôs based on an interplay between probability theory and graph theory, in which the properties of an underlying graph specify the conditional independence properties of a set of random variables.\n\nSee Statistical Learning With Sparsity (Hastie, Tibshirani, Wainright)\n\nMathematical introduction to graphical models and Graphical LASSO, pg 241 (252 in pdf), See R &gt;&gt; Documents &gt;&gt; Regression\n\n\nAssumes that the observations have a multivariate Gaussian distribution\nMisc\n\nPackages\n\n{glasso} - The original package by the authors of the algorithm. Estimation of a sparse inverse covariance matrix using a lasso (L1) penalty. Facilities are provided for estimates along a path of values for the regularization parameter. Can be slow or nonconvergent for large dimension datasets.\n{huge} - Provides functions for estimating high dimensional undirected graphs from data. Also provides functions for fitting high dimensional semiparametric Gaussian copula models (Vignette)\n{cglasso} - Conditional Graphical Lasso Inference with Censored and Missing Values (Vignette)\n\n\nPreprocessing: All variables should be standardized.\nThe terms in the ICT are not equivalent but are proportional to the partial correlation between the two corresponding variables\n\nTransform the ICT, \\(\\Omega\\) into a partial correlation matrix, \\(R\\)\n\\[\nR_{j,k} = \\frac{-\\Omega_{i,j}}{\\sqrt{\\Omega_{j,j}\\Omega_{k,k}}}\n\\]\nparr.corr &lt;- matrix(nrow=nrow(P), ncol=ncol(P))\nfor(k in 1:nrow(parr.corr)) {\n  for(j in 1:ncol(parr.corr)) {\n    parr.corr[j, k] &lt;- -P[j,k]/sqrt(P[j,j]*P[k,k])\n  }\n}\ncolnames(parr.corr) &lt;- colnames(P)\nrownames(parr.corr) &lt;- colnames(P)\ndiag(parr.corr) &lt;- 0\nSetting the terms on the diagonal to zero prevents variables from having connections with themselves in a network graph if you want to visualize the relationships\n\nWhere the nodes are variables and edges are the partial correlations.\n\n\nHyperparameter, \\(\\rho\\) , adjusts the sparsity of the matrix output\n\nHigher: Isolates the strongest relationships in your data (more sparse)\nLower: Preserving more tenuous connections, perhaps identifying variables with connections to multiple groups (less sparse)\n\nCheck symmetry. Assymmetry in the ICT can arise due to numerical computation and rounding errors, which can cause problems later depending on what you want to do with the matrix.\nExample: Stock Analysis using {glasso} (link)\nrho &lt;- 0.75\ninvcov &lt;- glasso(S, rho=rho)  \n\n# inverse covariance matrix\nP &lt;- invcov$wi\ncolnames(P) &lt;- colnames(S)\nrownames(P) &lt;- rownames(S)\n\n# check symmetry\nif(!isSymmetric(P)) {\n  P[lower.tri(P)] = t(P)[lower.tri(P)]  \n}\n\nGoal: Remove stocks relationship with market Beta and other confounding stocks to get the true relationsip between stock pairs.\nPost also has a network visualization. Data was put through PCA, then DBSCAN to get clusters. The cluster assignments were used to color the clusters in the network graph.\nPost also examines output from a lower \\(\\rho\\) and has an interesting analysis of the non-connected variables (i.e.¬†no partial correlation).",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-cont",
    "href": "qmd/association-general.html#sec-assoc-gen-cont",
    "title": "General",
    "section": "Continuous",
    "text": "Continuous\n\nSpearman‚Äôs Rank\n\\[\n\\rho = 1 - \\frac{6\\sum_i d_i^2}{n(n^2-1)}\n\\]\n\n\\(d_i\\): The difference in ranks for the ith observation\nMeasures how well the relationship between the two variables can be described by a monotonic function\nRank correlation measures the similarity of the order of two sets of data, relative to each other (recall that PCC did not directly measure the relative rank).\n\nValues range from -1 to 1 where 0 is no association and 1 is perfect association\nNegative values don‚Äôt mean anything in ranked correlation, so just remove the negative\n\nLinear relationship is a specific type of monotonic relationship where the rate of increase remains constant ‚Äî in other words, unlike a linear relationship, the amount of change (increase or decrease) in a monotonic relationship can vary.\nSee bkmks for CIs\nPackages\n\n{stats::cor.test(method = ‚Äúspearman‚Äù)}\n{DescTools::SpearmanRho}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\n\nKendall‚Äôs Tau\n\nNon-parametric rank correlation\n\nNon-parametric because it only measures the rank correlation based on the relative ordering of the data (and not the specific values of the data).\n\nShould be pretty close to Sspearman‚Äôs Rank but a potentially faster calculation\nFlavors: a, b (makes adjustment for ties), c (for different sample sizes for each variable)\n\nUse Tau-b if the underlying scale of both variables has the same number of possible values (before ranking) and Tau-c if they differ.\ne.g.¬†One variable might be scored on a 5-point scale (very good, good, average, bad, very bad), whereas the other might be based on a finer 10-point scale. In this case, Tau-c would be recommended.\n\nPackages\n\n{stats::cor.test(method = ‚Äúkendall‚Äù)} - Doesn‚Äôt state specifically but I think it calculates a and b depending on whether ties are present or not\n{DescTools} - has all 3 flavors\n\n\nHoeffding‚Äôs D\n\nRank-based approach that measures the difference between the joint ranks of (X,Y) and the product of marginal ranks.(?) A non-parametric test of independence. the product of their marginal ranks.\nUnlike the Pearson or Spearman measures, it can pick up on nonlinear relationships.\nRange: [-.5,1]\nGuidelines: Larger values indicate a stronger relationship between the variables.\nPackages\n\n{Hmisc::hoeffd}\n{DescTools::HoeffD}\n\n\nBayesian\n\nSteps: {brms}\n\nList the variables you‚Äôd like correlations for within mvbind().\nPlace the mvbind() function within the left side of the model formula.\nOn the right side of the model formula, indicate you only want intercepts (i.e., ~ 1).\nWrap that whole formula within bf().\nThen use the + operator to append set_rescor(TRUE), which will ensure brms fits a model with residual correlations.\nUse non-default priors and the resp argument to specify which prior is associated with which criterion variable\n\nGaussian\n\nExample: multiple variables\nf9 &lt;-¬†\n ¬† brm(data = d,\n ¬† ¬†family = gaussian,\n ¬† ¬†bf(mvbind(x_s, y_s, z_s) ~ 0,\n ¬†   ¬† sigma ~ 0) +\n    set_rescor(TRUE),\n¬† ¬† prior(lkj(2), class = rescor),\n¬† ¬† chains = 4, cores = 4,\n¬† ¬† seed = 1)\n\n## Residual Correlations:¬†\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## rescor(xs,ys)¬† ¬† 0.90¬† ¬† ¬† 0.02¬† ¬† 0.87¬† ¬† 0.93 1.00¬† ¬† 3719¬† ¬† 3031\n## rescor(xs,zs)¬† ¬† 0.57¬† ¬† ¬† 0.07¬† ¬† 0.42¬† ¬† 0.69 1.00¬† ¬† 3047¬† ¬† 2773\n## rescor(ys,zs)¬† ¬† 0.29¬† ¬† ¬† 0.09¬† ¬† 0.11¬† ¬† 0.46 1.00¬† ¬† 2839¬† ¬† 2615\nStandardized data is used here but isn‚Äôt required\n\nWill need to set priors though (see article for further details)\n\nSince the data is standardized, the sd can be fixed at 1\n\nbrms models log of sd by default, hence sigma ~ 0 since log 1 = 0\n\nCorrelations are the estimates for rescor(xs,ys), rescor(xs,zs) rescor(ys,zs)\n\nStudent t-distribution\n\nIf the data has any outliers, pearson‚Äôs coefficient is substantially biased.\nExample: correlation between x and y\n\\\nf2 &lt;-¬†\n¬† ¬† brm(data = x.noisy,¬†\n¬† ¬† family = student,\n¬† ¬† bf(mvbind(x, y) ~ 1) + set_rescor(TRUE),\n¬† ¬† prior = c(prior(gamma(2, .1), class = nu),\n    ¬† ¬† ¬† ¬† ¬† prior(normal(0, 100), class = Intercept, resp = x),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(normal(0, 100), class = Intercept, resp = y),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(normal(0, 100), class = sigma, resp = x),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(normal(0, 100), class = sigma, resp = y),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(lkj(1), class = rescor)),\n¬† ¬† iter = 2000, warmup = 500, chains = 4, cores = 4,¬†\n¬† ¬† seed = 210191)\n\n## Population-Level Effects:¬†\n##¬† ¬† ¬† ¬† ¬† ¬† Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## x_Intercept¬† ¬† -2.07¬† ¬† ¬† 3.59¬† ¬† -9.49¬† ¬† 4.72 1.00¬† ¬† 2412¬† ¬† 2651\n## y_Intercept¬† ¬† 1.93¬† ¬† ¬† 7.20¬† -11.31¬† ¬† 16.81 1.00¬† ¬† 2454¬† ¬† 2815\n##¬†\n## Family Specific Parameters:¬†\n##¬† ¬† ¬† ¬† Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma_x¬† ¬† 18.35¬† ¬† ¬† 2.99¬† ¬† 13.12¬† ¬† 24.76 1.00¬† ¬† 2313¬† ¬† 2816\n## sigma_y¬† ¬† 36.52¬† ¬† ¬† 5.90¬† ¬† 26.13¬† ¬† 49.49 1.00¬† ¬† 2216¬† ¬† 3225\n## nu¬† ¬† ¬† ¬† ¬† 2.65¬† ¬† ¬† 0.99¬† ¬† 1.36¬† ¬† 4.99 1.00¬† ¬† 3500¬† ¬† 2710\n## nu_x¬† ¬† ¬† ¬† 1.00¬† ¬† ¬† 0.00¬† ¬† 1.00¬† ¬† 1.00 1.00¬† ¬† 6000¬† ¬† 6000\n## nu_y¬† ¬† ¬† ¬† 1.00¬† ¬† ¬† 0.00¬† ¬† 1.00¬† ¬† 1.00 1.00¬† ¬† 6000¬† ¬† 6000\n##¬†\n## Residual Correlations:¬†\n##¬† ¬† ¬† ¬† ¬† ¬† Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## rescor(x,y)¬† ¬† -0.93¬† ¬† ¬† 0.03¬† ¬† -0.97¬† ¬† -0.85 1.00¬† ¬† 2974¬† ¬† 3366\n\nN = 40 simulated from a multivariate normal with 3 outliers\nCorrelation is the rescor(x,y) estimate -0.93; true value is -0.96\n\nUsing a pearson coefficient, cor = -0.6365649\nUsing brms::brm with family = gaussian, rescor(x,y) estimate -0.61",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-disc",
    "href": "qmd/association-general.html#sec-assoc-gen-disc",
    "title": "General",
    "section": "Discrete",
    "text": "Discrete\n\nMisc\n\nAlso see\n\nMultiple Correspondence Analysis (MCA) (see bkmks &gt;&gt; Features &gt;&gt; Reduction)\nDiscrete Analysis Notebook\n\nPackages\n\n{PAsso} - Assesses the Partial Association Between Ordinal Variables\n\nAllows users to perform a wide spectrum of assessments, including quantification, visualization, and hypothesis testing.\nVignette\n\n\nBinary vs Binary Similarity measures (paper)\n\nNote that a pearson correlation between binaries can be useful (see EDA &gt;&gt; Misc &gt;&gt; {correlationfunnel})\nTypes:\n\nJaccard-Needham\nDice\nYule\nRussell-Rao\nSokal-Michener\nRogers-Tanimoto\nKulzinsky\n\nPackages\n\n{{scipy}} - Also has other similarity measures\n\n\n\nPhi Coefficient - Used for binary variables when the categories are truly binary and not crudely measuring some underlying continuous variable (i.e.¬†dichotomization of a continuous variable)\n\n‚ÄúA Pearson correlation coefficient estimated for two binary variables will return the phi coefficient‚Äù (Phi coefficient wiki)\n(Contingency Table) Two binary variables are considered positively associated if most of the data falls along the diagonal cells. In contrast, two binary variables are considered negatively associated if most of the data falls off the diagonal\nAlso see StackExchange discussion on the difference between Phi Coefficient and Tetrachoric correlation\n{DescTools::Phi}\n\nCramer‚Äôs V - Association between two nominal variables\n\nSee Discrete Analysis notebook\n{DescTools::CramerV}\n\nPolychoric - Suppose each of the ordinal variables was obtained by categorizing a normally distributed underlying variable, and those two unobserved variables follow a bivariate normal distribution. Then the (maximum likelihood) estimate of that correlation is the polychoric correlation.\n\n{polycor}\n{psych::polychoric}\n\nFor correct=FALSE, the results agree perfectly with {polycor}\nFor very small data sets, the correction for continuity for the polychoric correlations can lead to difficulties, particularly if using the global=FALSE option, or if doing just one correlation at a time. Setting a smaller correction value (i.e., correct =.1) seems to help.\n\n{DescTools::CorPolychor}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\nTetrachoric - Used for binary variables when those variables are a sort of crude measure of an underlying continuous variable\n\nAlso see StackExchange discussion on the difference between Phi Coefficient and Tetrachoric correlation\nExample of appropriate use case: Suppose there are two judges who judge cakes, say, on some continuous scale, then based on a fixed, perhaps unknown, cutoff, pronounce the cakes as ‚Äúbad‚Äù or ‚Äúgood‚Äù. Suppose the latent continuous metric of the two judges has correlation coefficient œÅ.\n‚Äúthe contingency tables are ‚Äòbalanced‚Äô row-wise and col-wise, you get good correlation between the two metrics, but the tetrachoric tends to be a bit larger than the phi coefficient. When the cutoffs are somewhat imbalanced, you get slightly worse correlation between the metrics, and the phi appears to ‚Äòshink‚Äô towards zero.‚Äù\nThe estimation procedure is two stage ML.\n\nCell frequencies for each pair of items are found. Cells with zero counts are replaced with .5 as a correction for continuity (correct=TRUE).\nThe marginal frequencies are converted to normal theory thresholds and the resulting table for each item pair is converted to the (inferred) latent Pearson correlation that would produce the observed cell frequencies with the observed marginals\n\n{psych::tetrachoric}\n\nThe correlation matrix gets printed, but the correlations can also be extracted with $rho\nCan be sped up considerably by using multiple cores and using the parallel package. The number of cores to use when doing polychoric or tetrachoric may be specified using the options command. (e.g options(\"mc.cores\"=4);)\nsmooth = TRUE - For sets of data with missing data, the matrix will sometimes not be positive definite. Uses a procedure to transform the negative eigenvalues.\nFor relatively small samples with dichotomous data if some cells are empty, or if the resampled matrices are not positive semi-definite, warnings are issued. this leads to serious problems if using multi.cores. The solution seems to be to not use multi.cores (e.g., options(mc.cores =1)\n\nGoodman and Kruskal‚Äôs Gamma\n\nA measure of rank correlation, i.e., the similarity of the orderings of the data when ranked by each of the quantities. It measures the strength of association of the cross tabulated data when both variables are measured at the ordinal level.\nFor 2-way contingincy tables (i.e.¬†2x2 tables)\nIt makes no adjustment for either table size or ties.\nValues range from ‚àí1 (100% negative association, or perfect inversion) to +1 (100% positive association, or perfect agreement). A value of zero indicates the absence of association.\n{DescTools::GoodmanKruskalGamma}",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-mix",
    "href": "qmd/association-general.html#sec-assoc-gen-mix",
    "title": "General",
    "section": "Mixed",
    "text": "Mixed\n\nMisc\n\nAlso see\n\nPaper: JEL Ratio Test is non-parametric test that uses the categorical Gini covariance.\n\n{psych::mixedCor} - Finds Pearson correlations for the continous variables, polychorics for the polytomous items, tetrachorics for the dichotomous items, and the polyserial or biserial correlations for the various mixed variables (no polydi?)\n\nBiserial - correlation between a continuous variable and binary variable, which is assumed to have resulted from a dichotomized normal variable\n\n{psych::biserial}\n\nPolydi - correlation between multinomial variable and binary variable\n\n{psych::polydi}\n\nPolyserial - polychoric correlation between a continuous variable and ordinal variable\n\nBased on the assumption that the joint distribution of the quantitative variable and a latent continuous variable underlying the ordinal variable is bivariate normal\n{polycor}\n{psych::polyserial}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\nX2Y\n\nHandles types: continuous-continuous, continuous-categorical, categorical-continuous and categorical-categorical\nCalculates the % difference in prediction error after fitting a decision tree between two variables of interest and the mean (numeric) or most frequent (categorical)\nFunction is available through a script (Code &gt;&gt; statistical-testing &gt;&gt; correlation)\n\narticle with documentation and usage, https://rviews.rstudio.com/2021/04/15/an-alternative-to-the-correlation-coefficient-that-works-for-numeric-and-categorical-variables/\n\nAll x2y values where the y variable is continuous will be measuring a % reduction in MAE. All x2y values where the y variable is categorical will be measuring a % reduction in Misclassification Error. Is a 30% reduction in MAE equal to a 30% reduction in Misclassification Error? It is problem dependent, there‚Äôs no universal right answer.\n\nOn the other hand, since (1) all x2y values are on the same 0-100% scale (2) are conceptually measuring the same thing, i.e., reduction in prediction error and (3) our objective is to quickly scan and identify strongly-related pairs (rather than conduct an in-depth investigation), the x2y approach may be adequate.\n\nNot symmetric, but can average both scores to get a pseudo-symmetric value\nBootstrap CIs available\n\nCopulas\n\nlatentcor PKG: semi-parametric latent Gaussian copula models",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-nonlin",
    "href": "qmd/association-general.html#sec-assoc-gen-nonlin",
    "title": "General",
    "section": "Non-linear",
    "text": "Non-linear\n\nMisc\n\nAlso see General Additive Models &gt;&gt; Diagnostics for a method of determining a nonlinear relationship for either continuous or categorical outcomes.\n\nŒæ (xi) coefficient\n\nPaper: A New Coefficient of Correlation\nArticle: Exploring the XI Correlation Coefficient\nExcels at oscillatory and highly non-monotonic dependencies\nXICOR::xicor - calculates Œæ and performs a significance test (H0: independent)\n\nXICOR::calculateXI just calculates the Œæ coefficient\n\nProperties (value ranges; interpretation)\n\nIf y is a function of x, then Œæ goes to 1 asymptotically as n (the number of data points, or the length of the vectors x and y) goes to Infinity.\nIf y and x are independent, then Œæ goes to 0 asymptotically as n goes to Infinity.\n\nValues can be negative, but this negativity does not have any innate significance other than being close to zero\nn &gt; 20 necessary\n\nn larger than about 250 probably sufficient to get a good estimate\n\nFairly efficient (O(nlogn), compared to some more powerful methods, which are O(n2))\nIt measures dependency in one direction only (is y dependent on x not vice versa)\nDoesn‚Äôt tell you if the relationship is direct or inverse",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/base-r.html",
    "href": "qmd/base-r.html",
    "title": "Base R",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-misc",
    "href": "qmd/base-r.html#sec-baser-misc",
    "title": "Base R",
    "section": "",
    "text": "Magrittr + base\nmtcars %&gt;% {plot(.$hp, .$mpg)}\nmtcars %$% plot(hp, mpg)\n\nBy wrapping the RHS in curly braces, we can override the rule where the LHS is passed to the first argument ## Options {#sec-baser-opts .unnumbered}\n\nRemove scientific notation\noptions(scipen = 999)\nWide and long printing tibbles\n# in .Rprofile\nmakeActiveBinding(\".wide\", function() { print(.Last.value, width = Inf) }, .GlobalEnv)\n\nAfter printing a tibble, if you want to see it in wide, then just type .wide + ENTER.\nCan have similar bindings for `.long` and `.full`.\n\nHeredocs - Powerful feature in various programming languages that allow you to define a block of text within the code, preserving line breaks, indentation, and other whitespace.\ntext &lt;- r\"(\nThis is a\nmultiline string\nin R)\"\n\ncat(text)",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-usrfuns",
    "href": "qmd/base-r.html#sec-baser-usrfuns",
    "title": "Base R",
    "section": "User Defined Functions",
    "text": "User Defined Functions\n\nAnonymous (aka lambda) functions: \\(x) {} (&gt; R 4.1)\nfunction(x) {\n¬† x[which.max(x$mpg), ]\n}\n# equivalent to the above\n\\(x) {\n¬† x[which.max(x$mpg), ]\n}\nDots (‚Ä¶)\n\nMisc\n\n{ellipsis}: Functions for testing functions with dots so they fail loudly\n{rlang} dynamic dots: article\n\nSplice arguments saved in a list with the splice operator, !!! .\nInject names with glue syntax on the left-hand side of := .\n\n\nUser Defined Functions\nmoose &lt;- function(...) {\n¬† ¬† dots &lt;- list(...)\n¬† ¬† dots_names &lt;- names(dots)\n¬† ¬† if (is.null(dots_names) || \"\" %in% dots_names {\n¬† ¬† ¬† ¬† stop(\"All arguments must be named\")\n¬† ¬† }\n}\nNested Functions\nf02 &lt;- function(...){\n  vv &lt;- list(...)\n  print(vv)\n}\nf01 &lt;- function(...){\n  f02(b = 2,...)\n}\n\nf01(a=1,c=3)\n#&gt; $b\n#&gt; [1] 2\n#&gt; \n#&gt; $a\n#&gt; [1] 1\n#&gt; \n#&gt; $c\n#&gt; [1] 3\nSubset dots values\nadd2 &lt;- function(...) {\n¬† ¬† ..1 + ..2\n}\nadd2(3, 0.14)\n# 3.14\nSubset dots dynamically: ...elt(n)\n\nSet a value to n and get back the value of that argument\n\nNumber of arguments in ‚Ä¶ : ...length()",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-funs",
    "href": "qmd/base-r.html#sec-baser-funs",
    "title": "Base R",
    "section": "Functions",
    "text": "Functions\n\ndo.call - allows you to call other functions by constructing the function call as a list\n\nArgs\n\nwhat ‚Äì Either a function or a non-empty character string naming the function to be called\nargs ‚Äì A list of arguments to the function call. The names attribute of args gives the argument names\nquote ‚Äì A logical value indicating whether to quote the arguments\nenvir ‚Äì An environment within which to evaluate the call. This will be most useful if what is a character string and the arguments are symbols or quoted expressions\n\nExample: Apply function to list of vectors\nvectors &lt;- list(c(1, 2, 3), c(4, 5, 6), c(7, 8, 9))\ncombined_matrix &lt;- do.call(rbind, vectors)\n\ncombined_matrix\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    5    6\n## [3,]    7    8    9\nExample: Apply multiple functions\ndata_frames &lt;- list(\n  data.frame(a = 1:3), \n  data.frame(a = 4:6), \n  data.frame(a = 7:9)\n  )\nmean_results &lt;- do.call(\n  rbind, \n  lapply(data_frames, function(df) mean(df$a))\n  )\n\nmean_results\n##      [,1]\n## [1,]    2\n## [2,]    5\n## [3,]    8\n\nFirst the mean is calculated for column a of each df using lapply\n\nlapply is supplying the data for do.call in the required format, which is a list or character vector.\n\nSecond the results are combined into a matrix with rbind\n\n\nsink - used to divert R output to an external connection.\n\nUse Cases: exporting data to a file, logging R output, or debugging R code.\nArgs\n\nfile: The name of the file to which R output will be diverted. If file is NULL, then R output will be diverted to the console.\nappend: A logical value indicating whether R output should be appended to the file (TRUE) or overwritten (FALSE). The default value is FALSE.\ntype: A character string. Either the output stream or the messages stream. The name will be partially match so can be abbreviated.\nsplit: logical: if TRUE, output will be sent to the new sink and the current output stream, like the Unix program tee.\n\nExample: Logging output of code to file\nsink(\"r_output.log\")      # Redirect output to this file\n# Your R code goes here\nsink()                    # Turn off redirection\n\noutput file could also have an extension like ‚Äú.txt‚Äù\n\nExample: Debugging\nsink(\"my_function.log\")   # Redirect output to this file\nmy_function()\nsink()                    # Turn off redirection\nExample: Appending output to a file\nsink(\"output.txt\", append = TRUE)  # Append output to the existing file\ncat(\"Additional text\\n\")  # Append custom text\nplain text\nsink()  # Turn off redirection\n\npmin and pmax\n\nFind the element-wise maximum and minimum values across vectors in R\nExample\nvec1 &lt;- c(3, 9, 2, 6)\nvec2 &lt;- c(7, 1, 8, 4)\npmax(vec1, vec2)\n#&gt; [1] 7 9 8 6\npmin(vec1, vec2)\n#&gt; [1] 3 1 2 4\nExample: With NAs\ndata1 &lt;- c(7, 3, NA, 12)\ndata2 &lt;- c(9, NA, 5, 8)\npmax(data1, data2, na.rm = TRUE)\n#&gt; [1] 9 3 5 12\n\nswitch\n\nExample:\nswitch(parallel,\n         windows = \"snow\" -&gt; para_proc,\n         other = \"multicore\" -&gt; para_proc,\n         no = \"no\" -&gt; para_proc,\n         stop(sprintf(\"%s is not one of the 3 possible parallel argument values. See documentation.\", parallel)))\n\nparallel is the function argument. If it doesn‚Äôt match one of the 3 values, then an error is thrown.\nIf the argument value is matched, then the quoted value is stored in para_proc\n\n\ndynGet\n\nLooks for objects in the environment of a function.\nWhen an object from the outer function is an input for a function nested around 3 layers deep or more, it may not be found by that most inner function. dynGet allows that function to find the object in the outer frame\nArguments\n\nminframe: Integer specifying the minimal frame number to look into (i.e.¬†how far back to look for the object)\ninherits: Should the enclosing frames of the environment be searched?\n\nExample:\n1function(args) {\n  if (method == \"kj\") {\n      ncv_list &lt;- purrr::map2(grid$dat, \n                              grid$repeats, \n                              function(dat, reps) {\n         rsample::nested_cv(dat,\n                            outside = vfold_cv(v = 10, \n                                               repeats = dynGet(\"reps\")),\n                            inside = bootstraps(times = 25))\n      })\n  }\n}\n\n2function(data) {\n    if (chk::vld_used(...)) {\n        dots &lt;- list(...)\n        init_boot_args &lt;-\n          list(data = dynGet(\"data\"),\n               stat_fun = cles_boot, # internal function\n               group_variables = group_variables,\n               paired = paired)\n        get_boot_args &lt;-\n          append(init_boot_args,\n                 dots)\n    }\n    cles_booted &lt;-\n      do.call(\n        get_boot_ci,\n        get_boot_args\n      )\n}\n\n1\n\nExample from Nested Cross-Validation Comparison\n\n2\n\nExample from {ebtools::cles}\n\n\n\nmatch.arg\n\nPartially matches a function‚Äôs argument values to list of choices. If the value doesn‚Äôt match the choices, then an error is thrown\nExample:\nkeep_input &lt;- \"input_le\"\nkeep_input_val &lt;- \n  match.arg(keep_input,\n            choices = c(\"input_lags\",\n                        \"input_leads\",\n                        \"both\"),\n            several.ok = FALSE)\nkeep_input_val\n#&gt; [1] \"input_leads\"\n\nseveral.ok = FALSE says only 1 match is allowed otherwise an error is thrown.\nThe error message is pretty informative btw.\n\n\nmatch.fun\n\nExample\nf &lt;- function(a,b) {\n  a + b\n}\ng &lt;- function(a,b,c) {\n  (a + b) * c\n}\nh &lt;- function(d,e) {\n  d - e\n}\nyolo &lt;- function(FUN, ...) {\n  FUN &lt;- match.fun(FUN)\n  params &lt;- list(...)\n  FUN_formals &lt;- formals(FUN)\n  idx &lt;- names(params) %in% names(FUN)\n  do.call(FUN, params[idx])\n}\nyolo(h, d = 2, e = 3)\n#&gt; -1",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-pipe",
    "href": "qmd/base-r.html#sec-baser-pipe",
    "title": "Base R",
    "section": "Pipe",
    "text": "Pipe\n\n\nBenefits of base pipe\n\nMagrittr pipe is bloated with special features which may make it slower than the base pipe\nIf not using tidyverse, it‚Äôs one less dependency (maybe one day it will be deprecated in tidyverse)\n\nBase pipe with base and anonymous functions\n# verbosely\nmtcars |&gt; (function(.) plot(.$hp, .$mpg))()\n# using the anonymous function shortcut, emulating the dot syntax\nmtcars |&gt; (\\(.) plot(.$hp, .$mpg))()\n# or if you prefer x to .\nmtcars |&gt; (\\(x) plot(x$hp, x$mpg))()\n# or if you prefer to be explicit with argument names\nmtcars |&gt; (\\(data) plot(data$hp, data$mpg))()\nUsing ‚Äú_‚Äù placeholder:\n\nmtcars |&gt; lm(mpg ~ disp, data = _)\nmtcars |&gt; lm(mpg ~ disp, data = _) |&gt; _$coef\n\nBase pipe .[ ]¬† hack\nwiki |&gt;\n¬† read_html() |&gt;\n¬† html_nodes(\"table\") |&gt;\n¬† (\\(.) .[[2]])() |&gt;\n¬† html_table(fill = TRUE) |&gt;\n¬† clean_names()\n# instead of\ndjia &lt;- wiki %&gt;%\n¬† read_html() %&gt;%\n¬† html_nodes(\"table\") %&gt;%\n¬† .[[2]] %&gt;%\n¬† html_table(fill = TRUE) %&gt;%\n¬† clean_names()\nMagrittr, base pipe differences\n\nmagrittr: %&gt;% allows you change the placement with a . placeholder.\n\nbase: R 4.2.0 added a _ placeholder to the base pipe, with one additional restriction: the argument has to be named\n\nmagrittr: With %&gt;% you can use . on the left-hand side of operators like ‚Äú\\(\", \\[\\[, \\[ and use in multiple arguments (e.g. df %&gt;% {split(.\\)x, .$y))\n\nbase: can hack this by using anonymous function\n\nsee Base pipe with base and anonymous functions above\nsee Base pipe .[ ]¬† hack above\n\n\nmagrittr: %&gt;% allows you to drop the parentheses when calling a function with no other arguments (e.g.¬†dat %&gt;% distinct)\n\nbase: |&gt; always requires the parentheses. (e.g.¬†dat |&gt; distinct())\n\nmagrittr: %&gt;% allows you to start a pipe with . to create a function rather than immediately executing the pipe\n\nPurrr with base pipe\ndata_list |&gt;\n¬† map(\\(x) clean_names(x))\n# instead of\ndata_list %&gt;%\n¬† map( ~.x %&gt;% clean_names)\n# with split\nstar |&gt;\n¬† split(~variable) |&gt;\n¬† map_df(\\(.) hedg_g(., reading ~ value), .id = \"variable\")\n# instead of\nstar %&gt;%\n¬† split(.$variable) %&gt;%\n¬† map_df(. %&gt;% hedg_g(., reading ~ value), .id = \"variable\")",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-str",
    "href": "qmd/base-r.html#sec-baser-str",
    "title": "Base R",
    "section": "Strings",
    "text": "Strings\n\nsprintf\nx &lt;- 123.456               # Create example data\n\nsprintf(\"%f\", x)           # sprintf with default specification\n#&gt; [1] \"123.456000\"\n\nsprintf(\"%.10f\", x)        # sprintf with ten decimal places\n#&gt; [1] \"123.4560000000\"\n\nsprintf(\"%.2f\", x)         # sprintf with two rounded decimal places\n#&gt; [1] \"123.46\"\n\nsprintf(\"%1.0f\", x)        # sprintf without decimal places\n#&gt; [1] \"123\"\n\nsprintf(\"%10.0f\", x)       # sprintf with space before number\n#&gt; [1] \"       123\"\n\nsprintf(\"%10.1f\", x)       # Space before number & decimal places\n#&gt; [1] \"     123.5\"\n\nsprintf(\"%-15f\", x)        # Space on right side\n#&gt; [1] \"123.456000     \"\n\nsprintf(\"%+f\", x)          # Print plus sign before number\n#&gt; [1] \"+123.456000\"\n\nsprintf(\"%e\", x)           # Exponential notation\n#&gt; [1] \"1.234560e+02\"\n\nsprintf(\"%E\", x)           # Exponential with upper case E\n#&gt; [1] \"1.234560E+02\"\n\nsprintf(\"%g\", x)           # sprintf without decimal zeros\n#&gt; [1] \"123.456\"\n\nsprintf(\"%g\", 1e10 * x)    # Scientific notation\n#&gt; [1] \"1.23456e+12\"\n\nsprintf(\"%.13g\", 1e10 * x) # Fixed decimal zeros\n#&gt; [1] \"1234560000000\"\n\npaste0(sprintf(\"%f\", x),   # Print %-sign at the end of number\n       \"%\")\n#&gt; [1] \"123.456000%\"\n\nsprintf(\"Let's create %1.0f more complex example %1.0f you.\", 1, 4)\n#&gt; [1] \"Let's create 1 more complex example 4 you.\"\nstr2lang - Allows you to turn plain text into code.\ngrowth_rate &lt;- \"circumference / age\"\nclass(str2lang(growth_rate))\n#&gt; [1] \"call\"\n\nExample: Basic\neval(str2lang(\"2 + 2\"))\n#&gt; [1] 4\n\neval(str2lang(\"x &lt;- 3\"))\nx\n#&gt; [1] 3\nExample: Run formula against a df\ngrowth_rate &lt;- \"circumference / age\"\nwith(Orange, eval(str2lang(growth_rate)))\n\n#&gt;   [1] 0.25423729 0.11983471 0.13102410 0.11454183 0.09748172 0.10349854\n#&gt;   [7] 0.09165613 0.27966102 0.14256198 0.16716867 0.15537849 0.13972380\n#&gt;  [13] 0.14795918 0.12831858 0.25423729 0.10537190 0.11295181 0.10756972\n#&gt;  [19] 0.09341998 0.10131195 0.08849558 0.27118644 0.12809917 0.16867470\n#&gt;  [25] 0.16633466 0.14541024 0.15233236 0.13527181 0.25423729 0.10123967\n#&gt;  [31] 0.12198795 0.12450199 0.11535337 0.12682216 0.11188369",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-cond",
    "href": "qmd/base-r.html#sec-baser-cond",
    "title": "Base R",
    "section": "Conditionals",
    "text": "Conditionals\n\n&& and || are intended for use solely with scalars, they return a single logical value.\n\nSince they always return a scalar logical, you should use && and || in your if/while conditional expressions (when needed). If an & or | is used, you may end up with a non-scalar vector inside if (‚Ä¶) {} and R will throw an error.\n\n& and | work with multivalued vectors, they return a vector whose length matches their input arguments.\nAlternative way of negating a condition or set of conditions: if (!(condition))\n\nMakes it less readable IMO, but maybe for a complicated set of conditions if makes more sense in your head to do it this way\nExample\nif (!(nr == nrow(iris) || (nr == nrow(iris) - 2))) {print(\"moose\")}\n\nUsing else if\nif (condition1) {\n  expr1\n} else if (condition2) {\n  expr2\n} else {\n  expr3\n}\nstopifnot\npred_fn &lt;- function(steps_forward, newdata) {\n¬† stopifnot(steps_forward &gt;= 1)\n¬† stopifnot(nrow(newdata) == 1)\n¬† model_f = model_map[[steps_forward]]\n¬† # apply the model to the last \"before the test period\" row to get\n¬† # the k-steps_forward prediction\n¬† as.numeric(predict(model_f, newdata = newdata))\n}\n%||%\n\nCollapse operator which acts like:\n`%||%` &lt;- function(x, y) {\n   if (is_null(x)) y else x\n}\n\nSays if the first (left-hand) input x is NULL, return y. If x is not NULL, return the input\n\nUse Cases\n\nDetermine whether a function argument is NULL\ngithub_remote &lt;- \n  function(repo, username = NULL, ...) {\n    meta &lt;- parse_git_repo(repo)\n    meta$username &lt;- username %||%\n      getOption(\"github.user\") %||%\n      stop(\"Unknown username\")\n  }\nWithin the print argument collapse\nlibrary(rlang)\n\nadd_commas &lt;- function(x) {\n  if (length(x) &lt;= 1) {\n    collapse_arg &lt;- NULL\n  } else {\n    collapse_arg &lt;- \", \"\n  }\n  print(paste0(x, collapse = collapse_arg %||% \"\"))\n}\n\nadd_commas(c(\"apples\"))\n[1] \"apples\"\nadd_commas(c(\"apples\", \"bananas\"))\n[1] \"apples, bananas\"",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-sort",
    "href": "qmd/base-r.html#sec-baser-sort",
    "title": "Base R",
    "section": "Ordering Columns and Sorting Rows",
    "text": "Ordering Columns and Sorting Rows\n\nAscending:\ndf[with(df, order(col2)), ]\n# or\ndf[order(df$col2), ]\n\ncol2 is the column used to sort the dfby\n\nDescending: df[with(df, order(-col2)), ]\nBy Multiple Columns\n\nDescending then Ascending: df[with(df, order(-col2, id)), ]\n\nChange position of columns\n# Reorder column by index manually\ndf2 &lt;- df[, c(5, 1, 4, 2, 3)]\ndf3 &lt;- df[, c(1, 5, 2:4)]\n# Reorder column by name manually\nnew_order = c(\"emp_id\",\"name\",\"superior_emp_id\",\"dept_id\",\"dept_branch_id\")\ndf2 &lt;- df[, new_order]",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#set-operations",
    "href": "qmd/base-r.html#set-operations",
    "title": "Base R",
    "section": "Set Operations",
    "text": "Set Operations\n\nUnique values in A that are not in B\na &lt;- c(\"thing\", \"object\")\nb &lt;- c(\"thing\", \"gift\")\n\nunique(a[!(a %in% b)])\n#&gt; [1] \"object\"\n\nsetdiff(a, b)\n\nsetdiff is slower\n\nUnique values of the two vectors combined\nunique(c(a, b))\n#&gt; [1] \"thing\"  \"object\" \"gift\"\n\nunion(a, b)\n\nunion is just a wrapper for unique",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-subset",
    "href": "qmd/base-r.html#sec-baser-subset",
    "title": "Base R",
    "section": "Subsetting",
    "text": "Subsetting\n\nLists and Vectors\n\nRemoving Rows\n# Remove specific value from vector\nx[!x == 'A']\n\n# Remove multiple values by list\nx[!x %in% c('A', 'D', 'E')]\n\n# Using setdiff\nsetdiff(x, c('A','D','E'))\n\n# Remove elements by index\nx[-c(1,2,5)]\n\n# Using which\nx[-which(x %in% c('D','E') )]\n\n# Remove elements by name\nx &lt;- c(C1='A',C2='B',C3='C',C4='E',C5='G')\nx[!names(x) %in% c('C1','C2')]\n\nDataframes\n\nRemove specific Rows\ndf &lt;- df[-c(25, 3, 62), ]\nRemove column by name\ndf &lt;- df[, which(names(df) == \"col_name\")]\ndf &lt;- subset(df, select = -c(col_name))\ndf &lt;- df[, !names(df) %in% c(\"col1\", \"col2\"), drop = FALSE]\nFilter and Select\ndf &lt;- subset(df, subset = col1 &gt; 56, select = c(col2, col3))",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#joins",
    "href": "qmd/base-r.html#joins",
    "title": "Base R",
    "section": "Joins",
    "text": "Joins\n\nInner join: inner &lt;- merge(flights, weather, by = mergeCols)\nLeft (outer) join: left  &lt;- merge(flights, weather, by = mergeCols, all.x = TRUE)\nRight (outer) join: right &lt;- merge(flights, weather, by = mergeCols, all.y = TRUE)\nFull (outer) join: full &lt;- merge(flights, weather, by = mergeCols, all = TRUE)\nCross Join (Cartesian product): cross &lt;- merge(flights, weather, by = NULL)\nNatural join: natural &lt;- merge(flights, weather)",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-err",
    "href": "qmd/base-r.html#sec-baser-err",
    "title": "Base R",
    "section": "Error Handling",
    "text": "Error Handling\n\nstop\n\nExample:\nswitch(parallel,\n       windows = \"snow\" -&gt; para_proc,\n       other = \"multicore\" -&gt; para_proc,\n       no = \"no\" -&gt; para_proc,\n       stop(sprintf(\"%s is not one of the 3 possible parallel argument values. See documentation.\", parallel)))\n\nparallel is the function argument. If it doesn‚Äôt match one of the 3 values, then an error is thrown.\n\n\ntry\n\nIf something errors, then do something else\nExample\ncurrent &lt;- try(remDr$findElement(using = \"xpath\",\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† '//*[contains(concat( \" \", @class, \" \" ),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† concat( \" \", \"product-price-value\", \" \" ))]'),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† silent = T)\n#If error : current price is NA\nif(class(current) =='try-error'){\n¬† ¬† currentp[i] &lt;- NA\n} else {\n¬† ¬† # do stuff\n}\n\ntryCatch\n\nRun the main code, but if it ‚Äúcatches‚Äù an error, then the secondary code (the workaround) will run.\nExample: Basic\nfor (r in 1:nrow(res)) {\n  cat(r, \"\\n\")\n\n  tmp_wikitext &lt;- get_wikitext(res$film[r], res$year[r])\n\n  # skip if get_wikitext fails\n  if (is.na(tmp_wikitext)) next\n  if (length(tmp_wikitext) == 0) next\n\n  # give the text to openai\n  tmp_chat &lt;- tryCatch(\n    get_results(client, tmp_wikitext),\n    error = \\(x) NA\n  )\n\n  # if openai returned a dict of 2\n  if (length(tmp_chat) == 2) {\n    res$writer[r] &lt;- tmp_chat$writer\n    res$producer[r] &lt;- tmp_chat$producer\n  }\n}\n\nget_results is called during each iteration, and if there‚Äôs an error a NA is returned.\n\nExample\npct_difference_error_handling &lt;- function(n1, n2) {\n# Try the main code\n¬† tryCatch(pct_diff &lt;- (n1-n2)/n1,\n¬† ¬† ¬† ¬† # If you find an error, use this code instead\n¬† ¬† ¬† ¬† ¬† error = return(\n¬† ¬† ¬† ¬† ¬† ¬† cat( 'The difference between', as.integer(n1), 'and', as.integer(n2), 'is',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† (as.integer(n1)-as.integer(n2)), 'which is',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 100*(as.integer(n1)-as.integer(n2))/as.integer(n1),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† '% of', n1 )#cat\n¬† ¬† ¬† ¬† ¬† ¬† ),\n¬† ¬† ¬† ¬† ¬† # finally = print('Code ended') # optional\n¬† ¬† ¬† ¬† ¬† )#trycatch\n¬† # If no error happens, return this statement\n¬† return ( cat('The difference between', n1, 'and', n2, 'is', n1-n2,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ', which is', pct_diff*100, '% of', n1) )\n}\n\nAssumes the error will be the user enters a string instead of a numeric. If errors, converts string to numeric and calcs.\n‚Äúfinally‚Äù - This argument will always run, regardless if the try block raises an error or not. So it could be a completion message or a summary, for example.",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-mods",
    "href": "qmd/base-r.html#sec-baser-mods",
    "title": "Base R",
    "section": "Models",
    "text": "Models\n\nreformulate - Create formula sytax programmatically\n# Creating a formula using reformulate()\nformula &lt;- reformulate(c(\"hp\", \"cyl\"), response = \"mpg\")\n\n# Fitting a linear regression model\nmodel &lt;- lm(formula, data = mtcars)\n\nformula\n##&gt; mpg ~ hp + cyl\n\nCan also use as.formula\n\nDF2formula - Turns the column names from a data frame into a formula. The first column will become the outcome variable, and the rest will be used as predictors\nDF2formula(Orange)\n#&gt; Tree ~ age + circumference\nformula - Provides a way of extracting formulae which have been included in other objects\nrec_obj |&gt; prep() |&gt; formula()\n\nWhere ‚Äúrec_obj‚Äù is a tidymodels recipe object\n\nData from Model Object\n\nmodel$model return the data dataframe\ndeparse(model$call$data) gives you that name of your data object as a string.\n\nmodel$call$data gives you the data as an unevaluated symbol;\n\neval(model$call$data) gives you back the original data object, if it is available in the current environment.",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/privacy.html",
    "href": "qmd/privacy.html",
    "title": "Privacy",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/privacy.html#sec-priv-misc",
    "href": "qmd/privacy.html#sec-priv-misc",
    "title": "Privacy",
    "section": "",
    "text": "Also see Simulation, Data\nPackages\n\n{xxhashlite} - Very fast hash functions using xxHash",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/privacy.html#sec-priv-tag",
    "href": "qmd/privacy.html#sec-priv-tag",
    "title": "Privacy",
    "section": "Tags",
    "text": "Tags\n\nTag sensitive information in dataframes\nnames(df)\n[1] \"date\" \"first_name\" \"card_number\" \"payment\"\n# assign pii tags\nattr(df, \"pii\") &lt;- c(\"name\", \"ccn\", \"transaction\")\n\nPersonally Identifiable Information (PII)\n\nTag dataframes with the names of regulations that are applicable\nattr(df, \"regs\") &lt;- c(\"CCPA\", \"GDPR\", \"GLBA\")\n\nCCPA is the privacy regulation for California\nGDPR is the privacy regulation for the European Union\nGLBA is the financial regulation for the United States\n\nNeeded because df has credit card and financial information\n\nSaving objects as .rds files preserves tags",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/privacy.html#sec-priv-hash",
    "href": "qmd/privacy.html#sec-priv-hash",
    "title": "Privacy",
    "section": "Hashing",
    "text": "Hashing\n\n{digest}\n\nHash Function\n\nApply Hash Function to PII Fields",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#debugging",
    "href": "qmd/cli-linux.html#debugging",
    "title": "Linux",
    "section": "Debugging",
    "text": "Debugging\n\n\nAlso see set -o xtrace in Scripting &gt;&gt; Commands that should start your script",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html",
    "href": "qmd/regression-survival.html",
    "title": "Survival",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-misc",
    "href": "qmd/regression-survival.html#sec-reg-surv-misc",
    "title": "Survival",
    "section": "",
    "text": "Model for estimating the time until a particular event occurs\n\ne.g.¬†death of a patient being treated for a disease, failure of an engine part in a vehicle\n\nPrediction models for survival outcomes are important for clinicians who wish to estimate a patient‚Äôs risk (i.e.¬†probability) of experiencing a future outcome. The term ‚Äòsurvival‚Äô outcome is used to indicate any prognostic or time-to-event outcome, such as death, progression, or recurrence of disease. Such risk estimates for future events can support shared decision making for interventions in high-risk patients, help manage the expectations of patients, or stratify patients by disease severity for inclusion in trials.1 For example, a prediction model for persistent pain after breast cancer surgery might be used to identify high risk patients for intervention studies\nOutcome variable: Time until event occurs\nPackages\n\nCRAN Task View\n{survival}\n{censored} - {tidymodels} for censored and survival modelling\n{quantreg} - Quantile Survival Regression\n{msm} - Multi-State Models\n\nVignette\nSee Multistate Models for Medical Applications\n\nTutorial using a heart transplant dataset\n\nStandard survival models only directly model two states: alive and dead. Multi-state models enable directly modeling disease progression where patients are observed to be in various states of health or disease at random intervals, but for which, except for death, the times of entering or leaving states are unknown.\nMulti-state models easily accommodate interval censored intermediate states while making the usual assumption that death times are known but may be right censored.\n\n{grf} - Generalized Random Forest; Causal forest with time-to-event data\n{partykit} - Conditional inference trees; Model-based recursive partitioning trees; can be used with {survival} to create random survival forests\n\n{bonsai}: tidymodels, partykit conditional trees, forests; successor to treesnip ‚Äî Model Wrappers for Tree-Based Models\n\n{aorsf} - Optimized software to fit, interpret, and make predictions with oblique random survival forests (ORSFs)\n{{sklearn}} - Random Survival Forests, Survival Support Vector Machine\n{rmstbart} - Prognostic model that directly targets the RMST (See Terms) as a function of baseline covariates. The interpretation of each estimated RMST is transparent and does not require a proportional hazards assumption plus additional modeling of a baseline hazard function.\n\nFrom paper: Generalized Bayesian Additive Regression Trees for Restricted Mean Survival Time Inference. (Code)\n\n{survAH} - Performs two-sample comparisons based on average hazard with survival weight (AHSW). (See Terms)\n\nNotes from\n\nWhat is Cox‚Äôs proportional hazards model?\n\nWhy not use a standard regression model?\n\nUnits that ‚Äúsurvive‚Äù until the end of the study will have a censored survival time.\n\ni.e.¬†We won‚Äôt have an observed survival time for these units because they survive for an unknown time after the study is completed.\nWe don‚Äôt want to discard these units though, as they still have useful information.\n\n\nSample Size\nModels\n\nKaplan Meier model (i.e.¬†K-M survival curve)\n\nOften used as a baseline in survival analysis\nCan not be used to compare risk between groups and compute metrics like the hazard ratio\n\nExponential model, the Weibull model, Cox Proportional-Hazards, Log-logistic and the Accelerated Failure Time (AFT)\nMulti-State Models\nHazard rates and Cumulative Hazard rates are typical quantities of interest\n\nLog-Rank Test (aka Mantel-Cox test) - tests if two groups survival curves are different\n\nNon-Parametric; a special case with one binary X\nThe intuition behind the test is that if the two groups have different hazard rates, the two survival curves (so their slopes) will differ.\nCompares the observed number of events in each group to what would be expected if the survival curves were identical (i.e., if the null hypothesis were true).\nExample\nlibrary(survival)\ndat &lt;- data.frame(\n¬† group = c(rep(1, 6), rep(2, 6)),\n¬† time = c(4.1, 7.8, 10, 10, 12.3, 17.2, 9.7, 10, 11.1, 13.1, 19.7, 24.1),\n¬† event = c(1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0)\n)\ndat\n##¬† ¬† group time event\n## 1¬† ¬† ¬† 1¬† 4.1¬† ¬† 1\n## 2¬† ¬† ¬† 1¬† 7.8¬† ¬† 0\n## 3¬† ¬† ¬† 1 10.0¬† ¬† 1\n## 4¬† ¬† ¬† 1 10.0¬† ¬† 1\n## 5¬† ¬† ¬† 1 12.3¬† ¬† 0\n## 6¬† ¬† ¬† 1 17.2¬† ¬† 1\n## 7¬† ¬† ¬† 2¬† 9.7¬† ¬† 1\n## 8¬† ¬† ¬† 2 10.0¬† ¬† 1\n## 9¬† ¬† ¬† 2 11.1¬† ¬† 0\n## 10¬† ¬† 2 13.1¬† ¬† 0\n## 11¬† ¬† 2 19.7¬† ¬† 1\n## 12¬† ¬† 2 24.1¬† ¬† 0\nsurvdiff(Surv(time, event) ~ group,\n¬† data = dat\n)\n##¬† ¬† ¬† ¬† N Observed Expected (O-E)^2/E (O-E)^2/V\n## group=1 6¬† ¬† ¬† ¬† 4¬† ¬† 2.57¬† ¬† 0.800¬† ¬† ¬† 1.62\n## group=2 6¬† ¬† ¬† ¬† 3¬† ¬† 4.43¬† ¬† 0.463¬† ¬† ¬† 1.62\n##¬†\n##¬† Chisq= 1.6¬† on 1 degrees of freedom, p= 0.2\n\n# plot curves with pval from test\nfit &lt;- survfit(Surv(time, event) ~ group, data = dat)\nggsurvplot(fit,\n¬† pval = TRUE,\n¬† pval.method = TRUE\n)\n\npval &gt; 0.05, so there isn‚Äôt enough evidence to that they‚Äôre different.",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-terms",
    "href": "qmd/regression-survival.html#sec-reg-surv-terms",
    "title": "Survival",
    "section": "Terms",
    "text": "Terms\n\nAverage Hazard With Survival Weight (AH) aka Censoring-Free Incidence Rate (CFIR): Alternative to RMST that‚Äôs based on a statistical measure called person-time incidence rate, which factors in the period that patients are potentially tracked in a clinical trial. (See vignette for {survAH} an example and further details)\n\n‚ÄúIf you follow 100 patients for a year ‚Äî which would be 100 years of person-time ‚Äî we divide that into the number of times we would observe an event such as a patient‚Äôs death or cancer recurrence over that period. We can calculate this for each group of patients in the trial with the effect of lost-to-follow-up being removed.‚Äù\n\nPerson-Time Incidence Rate is defined by the ratio of the total number of observed events and the total person-time of exposure.\n\nAH-based tests can be more powerful than the log-rank test and the standard RMST-based tests in detecting delayed treatment effects.\nFormula\n\\[\n\\hat{\\eta}(\\tau) =\\frac{1-\\hat{S}(\\tau)}{\\int_0^\\tau \\hat{S}(t)dt} = \\frac{\\mathbb{E}[I(T \\le \\tau)]}{\\mathbb{E}[T\\wedge \\tau]}\n\\]\n\n\\(\\hat S(\\tau)\\) is the Kaplan-Meier estimator\nLast Term: The ratio of the expected total number of events we observe by \\(\\tau\\) and the expected total observation time by \\(\\tau\\) when there is no censoring until \\(\\tau\\).\nDifference in AH: \\(\\hat \\eta_1 - \\hat \\eta_0\\)\nRatio of AH: \\(\\frac{\\hat \\eta_1}{\\hat \\eta_0}\\)\n\nLong-Term Average Hazard\n\\[\n\\hat{\\eta}_k(\\tau_1, \\tau_2) = \\frac{\\hat S_k(\\tau_2) + \\hat S_k(\\tau_1)}{\\int_0^{\\tau_2} \\hat{S}(t)dt - \\int_0^{\\tau_1} \\hat{S}(t)dt} = \\frac{\\mathbb{E}[I(\\tau_1 \\lt T \\le \\tau_2)]}{\\mathbb{E}[T\\wedge \\tau_2] - \\mathbb{E}[T\\wedge \\tau_1]}\n\\]\n\nLast Term: The numerator is the probability of having an event between \\((\\tau_1,\\tau_2)\\), and the denominator is the expected time of being alive between \\((\\tau_1,\\tau_2)\\).¬†\n\nThus, \\(\\eta_k (\\tau_1,\\tau_2)\\) can be interpreted as an average intensity (think rate) of having an event over the time window \\((\\tau_1,\\tau_2)\\).\n\nPaper shows this metric having more power than AH or LT-RMST for delayed treatment effect situations such as immunotherapy trials.\n\n\nCensoring Time (C): Time at which censoring occurs\n\nFor each unit, we observe Survival Time (T) or C: \\(Y = \\min(T, C)\\)\nRight Censoring: occurs when the event has happened after the enrollment (but the time is unknown).\n\nThe patient does not experience the event for the whole duration of the study.\nThe patient withdraws from the study.\nThe patient is lost to follow-up.\n\nLeft Censoring: occurs when the event has happened before the enrollment (but the time is unknown).\n\nCumulative hazard function (aka Cumulative Hazard Rates)\n\nShows the total accumulated risk of an event occurring at time t\nThe area under the hazard function\n\nHazard Rate (aka Risk Score), \\(h(t \\;|\\; X)\\)\n\nThe hazard rate is the probability that a unit with predictors, \\(X\\), will experience an event at time, \\(t\\), given that the unit has survived just before time, \\(t\\).\nThe formula for the Hazard Rate is the Hazard function.\n\nHazard Ratio (aka Relative Risk of an event): Risk of an event given category / risk of an event given by reference category\n\nThe ratio of two instantaneous event rates\nCoefficient of the Cox Proportional Hazards model (e.g.¬†paper)\n\n\n\\(e^\\beta &gt; 1\\) (or \\(\\beta &gt; 0\\)) for an increased risk of event (e.g.¬†death).\n\\(e^\\beta &lt; 1\\) (or \\(\\beta &lt; 0\\)) for a reduced risk of event.\nHR of 2 is equivalent to raising the entire survival curve for a control subject to the second power to get the survival curve for an exposed subject\n\nExample: If a control subject has 5yr survival probability of 0.7 and the exposed:control HR is 2, the exposed subject has a 5yr survival probability of 0.49\nIf the HR is 1/2, the exposed subject has a survival curve that is the square root of the control, so \\(S(5)\\) would be \\(\\sqrt{0.7} = 0.837\\)\n\n\n\nRestricted Mean Survival Time (RMST) - The expected survival duration up to a pre-specified truncation time, \\(\\tau\\). It has direct clinical interpretation, which holds regardless of the survival model used in an analysis. Changes in RMST are often cited as an alternative to hazard ratios in the context of treatment comparisons and in survival regression modeling.\n\nUnlike proportional hazards, the interpretation of change in RMST holds regardless of whether or not a particular survival regression model holds.\nSee Restricted Mean Survival Time for Survival Analysis: A Quick Guide for Clinical Researchers\nStatistical comparisons based on the standard RMST provides lower power than the conventional log-rank test when detecting delayed treatment effects (e.g.¬†immunotherapy trials). (paper)\n\nSolution: Long-Term (aka Windowed) RMST which defines the lower limit of the definite integral at the point where the K-M curves separate, \\(\\eta\\), instead of 0, and keeps the upper limit at \\(\\tau\\).\nAlso see LT-AH\n\n\nStatus indicator, \\(\\delta\\)\n\n\\(\\delta = 1\\), if \\(T \\le C\\) (e.g.¬†unit fails before study ends)\n\nTrue survival time is observed\n\n\\(\\delta = 0\\), if \\(T \\gt C\\) (e.g.¬†unit survives until end of study or has dropped out)\n\nCensoring time is observed\n\n\nSurvival function (aka Survival Rate), \\(S(T \\lt t)\\):\n\nOutputs the probability of a subject surviving (i.e., not experiencing the event) beyond time t\nMonotonically decreasing (i.e.¬†level or decreasing)\nBaseline survival curve illustrates the survival function when all the covariates are set to their median value\n\nSurvival Time (T) (aka Death, Failure Time, Event Time): Time at which the event occurs",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-km",
    "href": "qmd/regression-survival.html#sec-reg-surv-km",
    "title": "Survival",
    "section": "Kaplan-Meir",
    "text": "Kaplan-Meir\n\nMisc\n\nUseful for validation of Proportional Hazards assumption. When lines cross the assumption, hazards are found to be non-proportional.\nHarrell RMS (Ch. 20.3):\n\nFor external validation: at least 200 events\nNeed 184 subjects with an event, or censored late, to estimate to within a margin of error of 0.1 everywhere, at the 0.95 confidence level\n\n\nOrder event times (\\(T\\)) of units from smallest to largest, \\(t_1 \\lt .... \\lt t_k\\)\nCalculate probability that a unit survives past event time, \\(t_i\\), given that they survived up until event time, \\(t_i\\) (i.e.¬†past \\(t_{i-1}\\)) (conditional probability)\n\ne.g.¬†for \\(t_1\\), it‚Äôs \\(\\frac{(n_1 - d_1)}{n_1}\\)\n\n\\(n_1\\) is the number of units that have survived at \\(t_1\\)\n\\(d_1\\) is the number of units that have experienced the event (e.g.¬†died) at \\(t_1\\)\nSimilar for other \\(t\\) values\n\nMedian survival time is where the survival probability equals 0.5\n\nSurvival function\n\\[\nS(t) = \\prod_{i\\;:\\;t_t \\le t} \\left(1- \\frac{d_i}{n_i}\\right)\n\\]\n\nThe survival function computes the products of these probabilities resulting in the K-M survival curve\nThe product of these conditional probabilities reflects the fact that to survive past event time, \\(t\\), a unit must have survived all previous event times and the current event time.\n\nExample: 50 patients\n\n\nDotted lines represent \\(95\\%\\) CI\nRed dots indicate time when patients died (aka event times)\nMedian survival time is ~ 13 yrs",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-exp",
    "href": "qmd/regression-survival.html#sec-reg-surv-exp",
    "title": "Survival",
    "section": "Exponential",
    "text": "Exponential\n\nAssumes that the hazard rate is constant\n\ni.e.¬†Risk of the event of interest occurring remains the same throughout the period of observation\n\nSurvival function\n\\[\nS(t) = e^{-\\frac{t}{\\lambda}}\n\\]\nHazard function\n\\[\nh(t) = \\frac{1}{\\lambda}\n\\]\n\n\\(h(t)\\) is the constand hazard rate\n\nEstimated parameter: \\(\\lambda\\)",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-weibull",
    "href": "qmd/regression-survival.html#sec-reg-surv-weibull",
    "title": "Survival",
    "section": "Weibull",
    "text": "Weibull\n\nAssumes the change in hazard rate is linear.\nSurvival function\n\\[\nS(t) = e^{-(\\frac{t}{\\lambda})^\\rho}\n\\]\nHazard function\n\\[\nh(t) = \\frac{\\rho}{\\lambda}\\cdot \\left(\\frac{t}{\\lambda}\\right)^{\\rho - 1}\n\\]\nEstimated parameters: \\(\\lambda\\) and \\(\\rho\\)\n\n\\(\\lambda\\) parameter indicates how long it takes for 63.2% of the subjects to experience the event.\n\\(\\rho\\) parameter indicates whether the hazard rate is increasing, decreasing, or constant.\n\nIf \\(\\rho\\) is greater than 1, the hazard rate is constantly increasing.\nIf \\(\\rho\\) is less than 1, the hazard rate is constantly decreasing.",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-coxph",
    "href": "qmd/regression-survival.html#sec-reg-surv-coxph",
    "title": "Survival",
    "section": "Cox‚Äôs Proportional Hazards",
    "text": "Cox‚Äôs Proportional Hazards\n\nMultivariable regression model\nAllows the hazard rate to fluctuate\nHarrell: ‚Äúunder PH assumption and absence of covariate interactions, HR is a good overall effect estimate for binary treatment‚Äù\nMisc\n\nPackages\n\n{glmnet} - Regularized Cox Regression\n{coxphf} - Cox Regression with Firth‚Äôs Penalized Likelihood\n\nSee Regression, Regularized &gt;&gt; Firth‚Äôs Estimator\n\n\nSample Size\n\nHarrell RMS (Ch. 20.3):\n\nTo achieve a Multiplicative Margin of Error (MMOE ) of 1.2 (?) in estimating \\(e^{\\hat \\beta}\\) with equal numbers of events in the two groups (balanced, binary treatment variable) and \\(\\alpha = 0.05 \\rightarrow\\) requires a total of 462 events\n\n\nResources\n\nMethods for non-proportional hazards in clinical trials: A systematic review\n\nTest for Differences in Treatment Effects\n\nFrom Statistical tests for comparing the associations of multiple exposures with a common outcome in Cox proportional hazard models\n\nAlso, examples show code for relatively complex designs.\n\nReminds me of some of the post-hoc ANOVA tests, but for survival models. (See Post-Hoc Analysis, ANOVA &gt;&gt; Diagnostics)\nPaper shows procedure for different exposure (i.e.¬†treatment) types, different numbers of exposures, etc., plus coded examples.\nProcedure:\n\nFit separate survival models for each treatment\nCreate a long format dataframe where the values of the treatment variables are one column (e.g.¬†exposures) and the names of the treatments are in another (e.g.¬†type)\nFit survival model where the there‚Äôs an interaction between the newly created variables (e.g.¬†exposures and types). There are also other interactions that should be included (see paper).\nWhen the exposures \\(\\times\\) types interaction coefficient has a p-value \\(\\lt\\) 0.05, it suggests that the difference in the exposure effects was statistically significant.\n\n\n\nAssumes\n\nHazard ratios (ratio of hazard rates or \\(e^\\beta\\) ) between groups/units remain constant over time (aka Proportional Hazards Assumption).\n\ni.e.¬†No matter how the hazard rates of the subjects change during the period of observation, the hazard rate of one group relative to the other will always stay the same\n\nHazard Ratios are independent of time\nExample: Immunotherapy typically violates PH assumptions (post)\n\nThe survival probabilities between the treatment (blue) and the chemo (red) cross at around the 4.2 months\n\nThe distance between the lines should remain somewhat constant throughout the trial in order to adhere to the PH assumptions (1st assumption)\nAlso think the lines should be somewhat straight. (2nd assumption)\n\nPatients in immunotherapy drug trials often experience a period of toxicity, but if they survive this period, they have a much better outcome down the road.\n\nExample: Delayed Difference in Immunotherapy Trials\n\n\nLines overlap until around 6 to 7 month then separate showing that the treatment effect is delayed.\nSolution:\n\nTests\n\nGrambsch and Therneau (G&T)\nSee Harrell RMS (Ch. 20.6.2)\n\nIf assumptions are violated,\n\nGelman says to try and ‚Äúexpand the model, at the very least by adding an interaction.‚Äù (post)\nSee Harrell RMS (Ch. 20.7)\nUse a different model\n\nAccelerated Failure Time (AFT) model (See ML &gt;&gt; Gradient Boosting Survival Trees)\nAdjusted Cox PH model with Time-Varying Coefficients\n\nMust choose a functional form describing how the effect of the treatment changes over time\n\nRecommended to use AIC criteria to guide one‚Äôs choice among a large number of candidates\n\n\n\n\n\nModels event time (T) outcome variable and outputs parameter estimates for treatment (X) effects\n\nProvides a way to have time-dependent (i.e.¬†repeated measures) explanatory variables (e.g.¬†age, health status, biomarkers)\nCan handle other types of censoring such as left or interval censoring.\nHas extensions such as lasso to handle high dimensional data\nDL and ML models also have versions of this method\n\nHazard function\n\\[\nh(t|X,L) = h_0(t)\\;e^{\\beta_1 X +\\beta_2 L}\n\\]\n\nThe hazard rate for a unit with exposure, \\(X\\), and adjustment variable, \\(L\\), is the product of a baseline hazard, \\(h_0(t)\\) (corresponding to \\(X = 0\\) and \\(L=0\\)) and a factor that depends on \\(X\\) and \\(L\\) and the regression parameters, \\(\\beta_1\\) and \\(\\beta_2\\).\nOptimized to yield partial maximum likelihood estimates, \\(\\hat \\beta\\).\n\nDoesn‚Äôt require the specification of \\(h_0(t)\\) which makes the method flexible and robust\n\n\nInterpretation:\n\n\\(\\beta\\): Represents the increase in the expected log of the relative hazard for each one unit increase in the predictor, holding other predictors constant.\nHazard Rate: The risk (probability) of an event given that the participant has survived up to a specific time. (Formula depends on treatment category, see example below.)\n\nEven though it‚Äôs a probability, it actually represents the expected number of events per one unit of time. As a result, the hazard rate in a group can exceed 1.\nExample\n\nIf the hazard rate is \\(0.2\\) at time \\(t\\) and the time units are months, then on average, \\(0.2\\) events are expected per person at risk per month.\nOr the reciprocal, \\(1/0.2 = 5\\), which is the expected event-free time (5 months) per person at risk.\n\n\nRelative Risk (of an event): Risk of an event given category divided by the risk of an event given by reference category\n\nHow many times greater (or less) of a risk of an event given a category compared to the risk of an event given a reference category. (i.e.¬†odds ratio)\nExample\n\n\\(e^\\beta = 0.68\\) means the change in category from the reference category results in a \\((1 - 0.68) = 0.32 = 32\\%\\) decrease in the hazard on average.\nFor a continuous variable (e.g.¬†age), if \\(\\beta = 0.11149\\) and \\(e^\\beta = 1.118\\), then there is an 11.8% increase in the expected hazard relative to a one year increase in age (or the expected hazard is 1.118 times higher in a person who is one year older than another)\n\n\nExample: Treatment (\\(x\\)) = Smoking\n\nRisk Score (aka Hazard Rate) given by smoking: (\\(\\boldsymbol{x=1}\\)): \\(h_0(t)e^{\\beta \\cdot x} = h_0(t)e^{\\beta \\cdot 1} = \\boldsymbol{h_0(t)e^{\\beta}}\\)\nRisk Score (aka Base Hazard Rate) given by not smoking: (\\(\\boldsymbol{x=0}\\)): \\(h_0(t)e^{\\beta \\cdot x} = h_0(t)e^{\\beta \\cdot 0} = \\boldsymbol{h_0(t)}\\)\nRelative Risk (aka Hazard Ratio) is the risk given by smoking divided by the risk given by not smoking: \\(\\frac{h_0(t)e^\\beta}{h_0(t)} = \\boldsymbol{e^\\beta}\\)",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-ml",
    "href": "qmd/regression-survival.html#sec-reg-surv-ml",
    "title": "Survival",
    "section": "ML",
    "text": "ML\n\nMisc\n\nSplit data so partitions have the same censoring distribution.\n\nThe censoring distribution might be obtained from a Kaplan-Meier estimator applied to the data.\n\nDynamic AUC is a recommended metric\n\n\n\nRandom Survival Forests\n\nThe main difference from a standard RF lies in the metric used to assess the quality of a split: log-rank (see Misc) which is typically used when comparing survival curves among two or more groups.\nPackages\n\n{{sklearn}}\n{aorsf} - Optimized software to fit, interpret, and make predictions with oblique random survival forests (ORSFs)\n\nInstead of using one variable to split the data, use a weighted combination of variables, i.e.¬†\\(\\text{instead of}\\;\\; x_1 &lt; \\text{cutpoint (left), use}\\;\\; c_1x_1 + c_2x_2 &lt; \\text{cutpoint (right)}\\)\n\nPredictions of Standard RF vs Oblique RF\n\n\n\n\n\n\n\n\nStandard Random Forest\n\n\n\n\n\n\n\nOblique Random Forest\n\n\n\n\n\n\n\nIn the standard rf, the decision boundaries are essentially perpendicular while the oblique rf boundaries are more angular. This should make the oblique model more flexible.\n\nKaplan-Meir Curves are fit in the leaves of the trees\n\n\nTime is on the x-axis and probability of survival on the y-axis\n\n\n\nExample: {aorsf}\n\nFrom Machine Learning for Risk Prediction using Oblique Random Survival Forests (Video; Slides & Code)\nVia package\n# equivalent syntaxes\nfit_orsf &lt;- orsf(data = pbc_orsf, \n                 formula = Surv(time + status) ~ . - id)\nfit_orsf &lt;- orsf(data = pbc_orsf, \n                 formula = time + status ~ . - id)\n\nTop model is fit with the typical survival::coxph syntax\ntime: time to event\nstatus: dummy variable indicating whether event occurred\nid: unit or patient id which is excluded\n\nVia {tidymodels}\nlibrary(parsnip)\nlibrary(censored) # must be version 0.2.0 or higher\nrf_spec &lt;- \n  rand_forest(trees = 200) %&gt;%\n  set_engine(\"aorsf\") %&gt;% \n  set_mode(\"censored regression\") \nfit_tidy &lt;- \n  rf_spec %&gt;% \n  parsnip::fit(data = pbc_orsf, \n               formula = Surv(time, status) ~ . - id)\nEstimated Expected Risk via Partial Dependence (PD)\n\nPD and importance rank for variables\norsf_summarize_uni(fit_orsf, n_variables = 1)\n## \n## -- bili (VI Rank: 1) ----------------------------\n## \n##         |---------------- risk ----------------|\n##   Value      Mean    Median     25th %    75th %\n##  &lt;char&gt;     &lt;num&gt;     &lt;num&gt;      &lt;num&gt;     &lt;num&gt;\n##    0.80 0.2343668 0.1116206 0.04509389 0.3729834\n##     1.4 0.2547884 0.1363122 0.05985486 0.4103148\n##     3.5 0.3698634 0.2862611 0.16196924 0.5533383\n## \n##  Predicted risk at time t = 1788 for top 1 predictors\n\nComputes expected risk (predicted probability) at different quantiles as a predictor variable varies.\n\nValue is 3 values of the predictor which are the 25th, 50th, and 75th quantile.\n\nn_variables says how many ‚Äúimportant‚Äù variables to look at\n\ne.g.¬†n_variables = 2 would look at the top 2 variables in terms of variable importance.\nVI Rank: 1 indicates the bili is ranked first in variable importance\n\nbili: serum bilirubin (mg/dl); continuous predictor variable\nIt choses time = 1788 because that‚Äôs the median\nAlso see Diagnostics, Model Agnostic &gt;&gt; DALEX &gt;&gt; Dataset Level &gt;&gt; Partial Dependence Profiles\n\nPD at specified predictor values and time values\npd_by_gender &lt;- orsf_pd_oob(fit_orsf, \n                  pred_spec = list(sex = c(\"m\", \"f\")),\n                  pred_horizon = 365 * 1:5)\npd_by_gender %&gt;% \n  dplyr::select(pred_horizon, sex, mean) %&gt;% \n  tidyr::pivot_wider(names_from = sex, values_from = mean) %&gt;% \n  dplyr::mutate(ratio = m / f)\n\n## # A tibble: 5 x 4\n##   pred_horizon      m      f ratio\n##          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1          365 0.0768 0.0728  1.06\n## 2          730 0.125  0.111   1.13\n## 3         1095 0.230  0.195   1.18\n## 4         1460 0.298  0.251   1.19\n## 5         1825 0.355  0.296   1.20\n\norsf_pd_oob - Computes expected risk using out-of-bag only\n\nBoth values (m,f) for sex are specified\npred_horizon specifies the time values\n\nHere, time is in days, so these values specify expected risk (predicted probabilities) at years 1 through 5.\n\n\nratio is the risk ratio of males compared to females.\nOthers\n\norsf_pd_inb - Computes expected risk using all training data\norsf_pd_new - Computes expected risk using new data\n\n\n\n\n\n\n\nGradient Boosting Survival Trees\n\nLoss Functions\n\nPartial likelihood loss of Cox‚Äôs proportional hazards model\nSquared regression loss\nInverse probability of censoring weighted least squares error.\n\nAllows the model to accelerate or decelerate the time to an event by a constant factor. It is known as the Accelerated Failure Time (AFT). It contrasts with the Cox proportional hazards model where only the features influence the hazard function.\n\n\nPackages\n\n{{sklearn}}\n\n\n\n\nSurvival Support Vector Machine\n\nPredictions cannot be easily related to the standard quantities of survival analysis, that is, the survival function and the cumulative hazard function.\nPackages\n\n{{sklearn}}",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-diag",
    "href": "qmd/regression-survival.html#sec-reg-surv-diag",
    "title": "Survival",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nMisc\n\nNotes from How to Evaluate Survival Analysis Models\nPackages\n\n{survex} - Explainable Machine Learning in Survival Analysis\n\nFrom Dalex group\n\n\nThe likelihood-ratio test, Wald test, and score logrank statistics are asymptotically equivalent. For large enough N, they will give similar results. For small N, they may differ somewhat. The Likelihood ratio test has better behavior for small sample sizes, so it is generally preferred.\n\nConcordance Index (C-Index, Harrell‚Äôs C)\n\nConsider a pair of patients \\((i, j)\\). Intuitively, a higher risk should result in a shorter time to the adverse event. Therefore, if a model predicts a higher risk score for the first patient (\\(\\eta_i \\gt \\eta_j\\)), we also expect a shorter survival time in comparison with the other patient (\\(T_i \\lt T_j\\)).\nEach pair (\\(i, j\\)) that fulfills this expectation (\\(\\eta_i \\gt \\eta_j \\\\: T_i \\lt T_j\\) or \\(\\eta_i \\lt \\eta_j \\\\: T_i \\gt T_j\\)) as concordant pair, and discordant otherwise.\n\nA high number of concordant pairs is an evidence of the quality of the model, as the predicted higher risks correspond to an effectively shorter survival time compared to lower risks\n\nFormula\n\\[\n\\begin{aligned}\nC &= \\frac{\\operatorname{Number of Concordant Pairs}}{\\operatorname{Number of Comparable Pairs}}\\\\\n&= \\frac{\\operatorname{Number of Concordant Pairs}}{\\operatorname{Number of Concordant Pairs} + \\operatorname{Number of Disconcordant Pairs}}\n\\end{aligned}\n\\]\n\nIf both patients \\(i\\) and \\(j\\) are censored, we have no information on \\(T_i\\) and \\(T_j\\), hence the pair is discarded.\nIf only one patient is censored, we keep the pair only if the other patient experienced the event prior to the censoring time. Otherwise, we have no information on which patient might have experienced the event first, and the pair is discarded\n\nProgrammatic Formula\n\\[\nC = \\frac{\\sum_{i,j} I(|T_i \\gt T_j) \\cdot I(\\eta_j \\gt \\eta_i) \\cdot \\Delta_j}{\\sum_{i,j} I(T_i \\gt T_j) \\cdot \\Delta_j}\n\\]\n\nWhere the variable \\(\\Delta_j\\) indicates whether \\(T_j\\) has been fully observed (\\(\\Delta_j = 1\\)) or not (\\(\\Delta_j = 0\\)). Therefore, the multiplication by \\(\\Delta_j\\) discards noncomparable pairs, because the smaller survival time is censored (\\(\\Delta_j = 0\\)).\n\nGuidelines\n\n\\(C = 1\\): Perfect concordance between risks and event times.\n\\(C = 0\\): Perfect anti-concordance between risks and event times.\n\\(C = 0.5\\): Random assignment. The model predicts the relationship between risk and survival time as well as a coin toss.\nDesirable values range between 0.5 and 1.\n\nThe closer to 1, the more the model differentiates between early events (higher risk) and later occurrences (lower risk).\n\n\nIssues\n\nThe C-index maintains an implicit dependency on time.\nThe C-index becomes more biased (upwards) the more the amount of censoring (see Uno‚Äôs C below)\n\n\nUno‚Äôs C\n\n** Preferable to Harrell‚Äôs C in the presence of a higher amount of censoring. **\nVariation of Harrell‚Äôs C that includes the inverse probability of censoring weighting\n\nWeights based on the estimated censoring cumulative distribution\nUses the Kaplan-Meier estimator for the censoring distribution\n\nSo the disribution of censored units should be independent of the covariate variables\n\nIn the paper, Uno showed through simulation this measure is still pretty robust even when the censoring is dependent on the covariates\n\n\n\n\nDynamic AUC\n\nAUC where the False Positive Rates (FPR) and True Positive Rates (TPR) are time-dependent\n\nSince a unit is a True Negative until the event then becomes a True Positive\nRecommended for tasks when you want to measure performance over a specific period of time (e.g.¬†predicting churn in the first year of subscription).\n\nFormula\n\\[\n\\widehat{AUC}(t) = \\frac{\\sum_{i=1}^n \\sum_{j=1}^n I(y_j \\gt t)\\cdot I(y_i \\le t)\\cdot \\hat \\omega_i \\cdot I(\\hat f(\\boldsymbol{x}_j) \\le \\hat f(\\boldsymbol{x}_i))}{(\\sum_{i=1}^n I(y_i \\gt t)) \\;\\cdot\\; (\\sum_{i=1}^n I(y_i \\le t) \\; \\hat \\omega_i)}\n\\]\n\n\\(\\hat f\\) are predicted risk scores\n\\(\\hat \\omega\\) is the inverse probability of censoring weight (see Uno‚Äôs C)\n\\(I(y_{i,j} \\gt t)\\) indicates whether the unit pair‚Äôs, \\(i\\) and \\(j\\), event time is greater or less the time, \\(t\\). (I think)\n\n\n\n\n\n\nStandard Random Forest\nOblique Random Forest",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/vs-code.html",
    "href": "qmd/vs-code.html",
    "title": "VS Code",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "VS Code"
    ]
  },
  {
    "objectID": "qmd/vs-code.html#sec-vsc-misc",
    "href": "qmd/vs-code.html#sec-vsc-misc",
    "title": "VS Code",
    "section": "",
    "text": "Press ‚Äú.‚Äù inside any page in github to create a vscode instance that opens that file.",
    "crumbs": [
      "VS Code"
    ]
  },
  {
    "objectID": "qmd/vs-code.html#sec-vsc-shcts",
    "href": "qmd/vs-code.html#sec-vsc-shcts",
    "title": "VS Code",
    "section": "Shortcuts",
    "text": "Shortcuts\n\n\n\n\n\n\n\nShortcut\nDescription\n\n\n\n\nCTRL + ` (backtick)\nSwitch focus to terminal\n\n\nCTRL+ Enter\nInsert a new line directly below, regardless of where you are in the current line\n\n\nALT+Shift + Up/Down\nDuplicate your current row up or down\n\n\nALT + Up/Down\nMove the current row up or down\n\n\nALT + Shift + Right\nHit this twice to select everything within a current bracket(this option is called smartSelect.grow , if it needs to be re-mapped)\n\n\nCTRL + /\nComment out the current line\n\n\nCTRL + [ or ]\nIndent lines inward or outward",
    "crumbs": [
      "VS Code"
    ]
  },
  {
    "objectID": "qmd/vs-code.html#preferences",
    "href": "qmd/vs-code.html#preferences",
    "title": "VS Code",
    "section": "Preferences",
    "text": "Preferences\n\nSetting shortcut for running a line of powershell code to ctrl + enter\n\nPress Ctrl+Shift+P, type ‚ÄúPreferences: Open Keyboard Shortcuts (JSON)‚Äù, and press Enter.\nAdd this code\n{\n    \"key\": \"ctrl+enter\", // Replace with your preferred shortcut\n    \"command\": \"workbench.action.terminal.runSelectedText\",\n    \"when\": \"editorTextFocus && editorLangId == 'powershell'\"\n}",
    "crumbs": [
      "VS Code"
    ]
  },
  {
    "objectID": "qmd/aws.html",
    "href": "qmd/aws.html",
    "title": "AWS",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-misc",
    "href": "qmd/aws.html#sec-aws-misc",
    "title": "AWS",
    "section": "",
    "text": "Notes from Skillshare: Absolute Beginners Introduction to Amazon Web Services\nHow to choose the right GPU for Deep Learning\nOptimizations\n\nIncreased the number of threads that the AWS CLI uses to some large number (the default is 10) with¬†aws configure set default.s3.max_concurrent_requests 50\nIf downloading data is more of a bottleneck than cpu power, use a network speed optimized ec2 instance (‚Äún‚Äù in the name) such as c5n.4xl.",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-creman",
    "href": "qmd/aws.html#sec-aws-creman",
    "title": "AWS",
    "section": "Create and Manage Account",
    "text": "Create and Manage Account\n\naws.amazon.com¬† ‚Äì create a free account (top right)\n\ne: &lt;email&gt;\np: &lt;password&gt;\nacct name:\nall info needs entered:\n\ntick personal,\nfull name,phone number, country, address, city, state, postal code,\ntick you‚Äôve read user agreement\n\n\nPayment info\n\ncredit card, exp date, name\n\nVerify phone\n\ncountry, phone number (again), the ‚Äúi am not a robot‚Äù thing, click call me now\n\n4 digit code pops up on screen, computer calls you, and you enter code\nclick continue\n\n\nSelecting the Basic/free acct option (1-year),\nyou can skip the Personalize your experience stuff. Hit ‚Äúsign into the console‚Äù (mid-right)\n\nenter email and password\n\nBasic account page set-up\n\nUpper right corner ‚Äì&gt; name of the account ‚Äì&gt; dropdown menu ‚Äì&gt; My Account\n\nMain Body ‚Äì&gt; Alternate Contact info ‚Äì&gt; right side, click edit\nMain Body ‚Äì&gt; Security Challenge Questions ‚Äì&gt; click edit\nMain Body ‚Äì&gt; IAM User and Role Access ‚Äì&gt;¬† click edit ‚Äì&gt; check Activate box ‚Äì&gt; click update\n\nthink this is just for generating roles that have access to billing\n\nMain Body ‚Äì&gt; Manage Communication preferences ‚Äì&gt; edit\n\nnewsletters, services info, etc.\n\nMain Body ‚Äì&gt; Manage AWS support plan\n\nalready selected free earlier\nwhere you can upgrade from free tier to a paid plan\n\n(nothing to fill out) left panel ‚Äì&gt;¬†Dashboard, Bills, Cost Explorer, Reports, allocation tags, credits, misc accounting stuff\n\nstats on your spending\ninvoice summary of this month‚Äôs usage\nVisual breakdown of the cost/services you‚Äôre incurring\nreports\ncreate tags to designate which departments/projects are using which services\nwhere to input promo codes\n\nCreate Spending Alerts\n\nleft panel ‚Äì&gt; Budgets ‚Äì&gt; Create Budget\nselect type: Cost, usage, reservation, utilization\n\nuse cost\n\n$20\n\n\nGive budget a name\nPeriod: monthly, etc.\n\nuse monthly, it‚Äôs the smallest period available\n\nselect start date and end date\nAmount\nCan set limits per service\nNotifications\n\nactual, greater than, 50% of budget amount\n\nother option besides actual is forecasted\n\nemail address\noption for SNS service but didn‚Äôt discuss what that is\ncreate new notification\n\nadd additional email alert for 75%\n\n\nclick create budget¬†(bottom right)\n\nleft panel ‚Äì&gt; Preferences\n\nset notifications for when you exceed the free tier services\nget emailed billing invoice\nBilling alerts, reports\n\n\n\nIdentity Access Management\n\nhome console ‚Äì top search box under AWS Services, type ‚ÄúIAM‚Äù\n\nselect Identity and access management\n\nChange User Sign-in URL to something more memorable\n\nexample: root account name\n\nercbk\n\nclick customize (mid-right) and type name\nURL will be changed to .signin.aws.amazon.com/console\n\nSecurity Status section (main body\n\nActivate multi-factor authentication (MFA)\n\nThis is for root user, see change password section below if you‚Äôre a user that‚Äôs part of a group\nphysical MFA (e.g.¬†usb drive) or virtual MFA\n\nchoose virtual\n\nclick next twice to the bar code\nuse authenticator app to scan bar code\n\nclick + in app, then click bar code, and scan bar code on screen\n\nAsks for 2 consecutive pins that display on the app\nClick finish\n(may need to refresh page on main screen in order to see green tick mark)\n\n\nCreate individual IAM users\n\nAllows you to distribute various permissions to people with different roles in your org\nNeed to set up an admin user to get access keys so you can use AWS programmatically\nleft panel ‚Äì&gt; Users ‚Äì&gt; add users ‚Äì&gt; enter user name ‚Äì select access types\n\nchoose access type\n\ntick programmatic and management console\n\nenter console password\nrequire password reset: nope (untick box)\nclick next (bottom right) to set permissions\n\nSet permissions\n\nif you‚Äôre creating user groups (left panel), you can skip this and set the permissions group-wide\nSelect Attach existing policies directly\n\ntick box ‚ÄúAdministratorAccess‚Äù ‚Äì&gt; click next (bottom right)\n\ndude did this through groups and policies, so may have to go that route if AdministratorAccess isn‚Äôt available, then add the user to the group. Also there‚Äôs a button to attach additional ‚Äúpolicies‚Äù later on if needed\n\n\n\nReview and click create User\na lot more to this‚Ä¶ groups, policies¬† (see vid for details)¬†\n\nSelect password policy\n\ntick allow users to set password and untick everything else ‚Äì&gt; apply policy\nactivate/deactivate security token regions - he didn‚Äôt go into this but all US and EU regions were activated by default, so probably doesn‚Äôt need to be messed with.\n\nCopy URL, log out as root user, Now you can starting going to URL and sign in under the administrator acct you made\n\nTo change password\n\nleft panel ‚Äì&gt; Users ‚Äì&gt; select yourself ‚Äì&gt; security credentials tab ‚Äì&gt; Console password\nCan also create MFA in same tab\n\n\n\nCloud Trail\n\nlog of actions taken on account\ntype cloud trail in search box\nCreate trail -¬† lets you store the logs in a S3 bucket",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-basserv",
    "href": "qmd/aws.html#sec-aws-basserv",
    "title": "AWS",
    "section": "Overview of Basic Services",
    "text": "Overview of Basic Services\n\nCompute\n\nEC2 - Elastic Cloud Compute - Computer Clusters\n\nClosely related to on-premise set-ups\n\nElastic Container Service (ECS) - Spin up containers on top of EC2\nLambda - Serverless Architecture\n\nComputing resources can scale and descend automatically based on real-time demands.\nHandles security patches and OS updates automatically\nIssue with ‚Äútimeouts‚Äù - not optimal for long running applications\n\nA timeout defines the maximum amount of time a Lambda function can execute before it‚Äôs terminated.\nDefault timeout is set at 3 seconds, but you can configure it in the AWS console in increments of 1 second, with a maximum of 15 minutes (900 seconds).\n\nAWS charges for Lambda usage in increments of 100 milliseconds (0.1 seconds). Setting the timeout in 1-second increments helps ensure you‚Äôre not billed for extra time if your function runs slightly longer than expected.\nPrice optimization entails tuning this timeout parameter to be as close to your run time as possible. Need to take into account timeouts of other services included in your lambda function. (Guide)\n\nMonitor your Lambda functions for timeouts using CloudWatch metrics.\n\nDependency limit at 50 MB, can add 512 MB more to a tmp file after function has executed\nSpins down when not in use, so you don‚Äôt pay for downtime, but takes 5 or more secs to spin back up\n\nCan ping server from time to time to keep it ‚Äúwarm‚Äù\n\nLess tweakable than EC2, if problems occur, less flexible in terms of your team handling it\nThe code you run on AWS Lambda is uploaded as a ‚ÄúLambda function‚Äù. Each function has associated configuration information, such as its name, description, entry point, and resource requirements. The code must be written in a ‚Äústateless‚Äù style i.e.¬†it should assume there is no affinity to the underlying compute infrastructure. Local file system access, child processes, and similar artifacts may not extend beyond the lifetime of the request, and any persistent state should be stored in Amazon S3, Amazon DynamoDB, or another Internet-available storage service. Lambda functions can include libraries, even native ones.\n\n\nStorage\n\nS3\n\nDatabase -automanaged by AWS\n\nrelational db service (RDS)\ndynamoDB - noSQL\nElasticCache - redis\nRedshift - cadillac data warehouse service\n\nManagement Tools\n\nCloudWatch\n\nmonitor cpu usage\nlatency\nset alarms around metrics",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-cli",
    "href": "qmd/aws.html#sec-aws-cli",
    "title": "AWS",
    "section": "CLI",
    "text": "CLI\n\ngoogle ‚Äúdownload aws cli‚Äù\nGet keys\n\nlog into aws\nsearch IAM ‚Äì&gt; goto iam\nleft panel ‚Äì&gt; users ‚Äì&gt; your usesrname ‚Äì&gt; security credentials tab ‚Äì&gt; create access key ‚Äì&gt; 2 keys are created ‚Äì&gt; click download csv file\n\naccess key id (kind of like a username)\nsecret access key (kind of like a password)\n\n\nSet-up\n\nConfigure profile\n\naws configure --profile &lt;name&gt;\n\nchoose a name, can be anything\n\nused ercbk\n\n\nYou‚Äôll be asked for your\n\n‚Äúaccess key id‚Äù and ‚Äúsecret access key‚Äù, enter those\n\nused eb_admin values\n\noptional: default region: us-east-2 or hit enter to skip\n\nused us-east-2\n\noptional: default output format: just hit enter (he didn‚Äôt go into this)\n\n\n\ncommands\n\nHelp commands\n\ngives description, flags, inputs, etc.\naws help\naws &lt;service&gt; help\n\neg aws iam help\n\naws &lt;service&gt; &lt;command&gt; help\n\neg aws iam list-users help\n\nCan also go to AWS documentation at website",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-ec2",
    "href": "qmd/aws.html#sec-aws-ec2",
    "title": "AWS",
    "section": "EC2",
    "text": "EC2\n\nOn home screen, in search window, type ‚Äúec2‚Äù, click link\n\n\nEC2 Page\n\nleft panel ‚Äì Dashboard\n\nMain Body ‚Äì Under Resources\n\nInstances - which cluster instances you have running\nVolumes - which storage services you have\nkey pairs - you get a key pair for each running instance\nOther stuff‚Ä¶\n\n\nleft panel ‚Äì Instances\n\ninstances\n\nwhere you launch on-demand instances\nno bidding, you pay full price\n\nlaunch templates\n\nstored instance configurations\nalternate method to launch instances\n\nspot instances (requests)\n\nbid for available instances for a cheaper price\nIf the Spot price increases above your bid price, capacity is no longer available, or the spot request has constraints that can‚Äôt be met, then the Spot Instance can be ‚Äúinterrupted.‚Äù\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/spot-instance-terminate/\nEBS volumes can be attached, snapshots taken, or results sent to s3 buckets to prepare for a potential terminations (see above link)\n\n\nreserved instances\n\nyou can make a reservation for an instance to guarantee it‚Äôs availability at the time you specify. Can be cheaper than on-demand.\n\ndedicated host\n\nWhen you buy instances you share hosts (servers) with other people. Here you can guarantee only you are on the host with your instances.\n\n\nleft panel ‚Äì images\n\nAMI\n\nscenario: you launch an ec2 and load more packages onto it and want to save that AMI to reuse at a later time. Here you can take a snapshot of that image.\nyou can share these images with other users, sell the image in the marketplace\n\n\nleft panel ‚Äì Elastic Block Store\n\nvolumes\n\nhard drives - hdd (cold or optimized, ssd, iops ssd )\n\nsnapshots\n\nback-up of the volume\ncan be used to launch another instance if the current one is terminated\n\n\nleft_panel ‚Äì Network and Security\n\nSecurity Groups (also see docker notebook ‚Äì aws ‚Äì running a task)\n\nfirewall you place in front of an EC2 instance\nspecify allowed ports and allowed ip connections\ncan‚Äôt restrict to inbound or outbound traffic; open is open\n\nElastic IPs\n\nallows you to fix static ips (up to 5) to your instances\n\none less thing you‚Äôd have to configure if you‚Äôre starting and stopping ec2 instances a lot.\n\n\nPlacement Groups\n\ndecreases latency by placing all of your instances in the same or closely placed hosts\n\nKey Pairs\n\na form of tags that you can use to access your instance more easily,\n\nuse values as descriptors of use case for instance to organize and monitor\nCan use to SSH into your instance\npairs are key:value. Example: key= sales, value¬† = sales_forecast\n\n\nNetwork Interfaces\n\ngives a network card to your instance so it can connect to the internet\n\n\nleft panel ‚Äì Load Balancing\n\nscaling managed by aws\nroutes traffic to different instances\nLoad Balancers (also see Docker notebook, aws create load balancer section)\n\ntypes: application, network\n\nTarget Groups\n\nleft panel ‚Äì auto scaling\n\nadds instances if triggers set in load balancers (latency, cpu usage, memory)\nshuts down instances once load decreases\nkeeps cost lower\n\nleft panel ‚Äì systems manager\n\nallows you to run a command across multiple instances by specifying tags, ids, etc\ncan use scripts that update packages\n\n\n\n\nLaunch\n\nSearch ec2 in the search window\nSteps\n\ntop-right header ‚Äì choose a region\n\nwhere are the users located (latency)\n\nleft panel ‚Äì Instances ‚Äì instances (or Spot Instances)\n\nclick launch instances\nchoose an AMI, click select on right-hand side\nchoose compute option by ticking box on the left and click next on bottom right\n\neg t2 - micro (free tier option)\n\nconfigure instance details\n\nselect how many instances you want to launch\nnetwork, subnet, auto-assign public ip\n\ndefaults should be fine\n\nIAM role - mentions something about databases\nshutdown behavior: ‚Äústop‚Äù terminates instance when you stop it\nenable termination protection: protects against accidental termination\n\nthink this might have to do with spot instances being terminated\n\nmonitoring - enable CloudWatch ($)(see¬†Overview of basic services section)\n\nfree version updates metrics every 5 min\ndetailed version updates metrics every 1 min\n\nT2 unlimited\n\nenabling says if your t2 instance cpu goes over 20% usage, then start using my credits, and once my credits are used, then bill me for the rest\nguessing if this is unticked then your t2 is throttled below 21% to remain totally free\n\n\nAdd storage\n\nsee Elastic Block Store above for details on available drives\ncomes with a ‚Äúroot‚Äù drive by default\n\nChoose HD type, size of storage, and whether to delete on termination (volume gets deleted)\nGeneral Purpose SSD and up to 30 GB available for free tier.\nAlso saw a snapshot ID I think but he didn‚Äôt discuss\n\nclick add volume to add multiple HDs\n\nAdd tags\n\nadd tag\nenter key and value (different from key pair below)\ntick which resources you want to use the tag for: instance and/or volume\n\ncan create unique tags for different resources\n\n\nConfigure security group\n\nAssign a new security group or select an existing group\n\ntick select existing ‚Äì&gt; tick default\n\n\nClick review and launch\n\nshows all the options you selected above\nHit launch\n\nChoose existing or create new key pair\n\nfile that‚Äôs necessary to ssh (linux) or log into (windows)\nCreate new key pair ‚Äì&gt; enter name ‚Äì&gt; download key pair file\n\nor you can select previous key pair and use the same file\n\nLaunch instance\n\nLaunch Status\n\nClick View Instances\n\nWatch status column, usually takes a couple minutes to launch\nscreen is split in half, use divider to increase size of lower screen if you want to view the details of the instance you started\nI think you‚Äôre in¬†left panel ‚Äì instances ‚Äì instances\n\n\n\n\n\n\n\nConnect to/ Terminate Instance\n\nAlso see Docker, AWS &gt;&gt; Create Cluster: EC2 with SSH Access\nleft panel ‚Äì instances ‚Äì instances\n\nhighlight instance you want to connect to\nIn bottom of split EC2 screen, copy IPv4 Public IP and save it in notebook++ or somewhere\n\nOpen an inbound port to instance\n\nleft panel ‚Äì¬†Network and Security ‚Äì Security Groups\n\nclick on security group you selected during launch of instance\nlower half of screen ‚Äì inbound tab\n\n‚Äúall traffic‚Äù means full communication allowed between all instances within this security group\nclick edit ‚Äì add rule\n\nType ‚Äì click dropdown ‚Äì choose SSH\n\nafter choosing SSH, it also inputs port 22\n\nSource ‚Äì ditto ‚Äì choose ‚ÄúMy IP‚Äù\n\nautomatically finds your ip¬† and inputs it\n\nDescription -¬† ‚ÄúSSH for  ip‚Äù or whatever you want\n\nfor Windows Server AMI instance\n\nType ‚Äì click dropdown ‚Äì choose RDP\n\nchooses port 3389\n\nSource ‚Äì ditto ‚Äì choose ‚ÄúMy IP‚Äù\n\nautomatically finds your ip¬† and inputs it\n\nDescription - ‚ÄúRDP for  ip‚Äù or whatever you want\n\n\n\n\nIf on a linux machine (locally), click instance, choose connect, follow instructions\nIf on windows (locally):\n\nOpen Puttygen (key generator)\n\nClick load ‚Äì find and select the key pair file (.pem) you downloaded before launching the instance\nClick Save private key\n\nIt‚Äôll ask if you‚Äôre sure you don‚Äôt want to attach a passphrase ‚Äì click yes\nenter name of .ppk file (no need to add extension) ‚Äì Click Save\n\n\nOpen Putty\n\nConfiguration window opens\n\nIn Host Name (IP address) box ‚Äì paste IPv4 Public IP (that you copied from earlier)\nleft panel ‚Äì Connections ‚Äì SSH ‚Äì Auth\n\nOptions-for-controlling-SSH-authentication window opens\n\nprivate-key-for-authentication box ‚Äì click browse\n\nselect the .ppk file you made\n\n\n\nClick Open (bottom right) to open connection\n\n¬†Windows pop-up ‚Äì click yes\n\n\n\nPutty CLI opens\n\n‚Äúlogin as‚Äù¬†\n\ntype username for the AMI you used\n\nexample was a basic linux AMI with username ‚ÄúEC2-user‚Äù\nFor RStudio AMI, should be ‚Äúubuntu‚Äù\n\n\nHit enter and should be connected\n\n\nTerminate\n\nleft panel ‚Äì instances ‚Äì instances\n\nselect instance you want to terminate ‚Äì right click instance_id (or anywhere on row) ‚Äî instance state ‚Äì terminate\n\n\nConnect to a Windows Server AMI instance\n\nWindows Search ‚ÄúRemote Desktop Connection‚Äù\n\nopen it\n\nFor ‚ÄúComputer:‚Äù, type in the IPv4 public ip ‚Äì click connect\nFor base ami\n\nusername: administrator\npassword\n\nleft panel ‚Äì instance ‚Äì instance\n\nright click instance row ‚Äì Get Windows Password\n\nclick Choose FIle ‚Äì select key pair file .pem file (not .ppk)\nclick Decrypt Password (bottom right)\nCopy password, paste into Remote Desktop Connection\n\n\nClick OK\n\n\na window with opens up with Windows OS on it (takes a minute or two to completely load)\n\n\n\n\nConfigure Load Balancer and Application Ports\n\nAlso see Docker, AWS &gt;&gt; Create Application Load Balancer (ALB)\nleft panel ‚Äì Network and Security ‚Äì Security Groups¬†(also see docker notebook ‚Äì aws ‚Äì running a task)\nExample: We want the Application Instances to only talk to the load balancer (inbound) and the load balancer talks to the public (inbound) and application instances (outbound) (Reminder all inbound ports are also outbound ports)\n\nCreate security group for EC2 Instance\n\nCreate Security Group (blue button, upper left)\n\nEnter name and description\nclick create\n\nInbound tab (lower half)\n\nclick edit\n\nType: HTTP (auto-inputs protocol TCP and port 80)\nSource: type name of Load Balancer security group (long box)\n\nautocomplete will help\nafter clicking name, a group id is entered into the field\n\nWeird, but id resembles the group id listed in the top half of screen but doesn‚Äôt exactly match. Should be correct though.\n\ndrop down should be ‚Äúcustom‚Äù\n\nclick add rule\nType: HTTPS\n\nsame thing but auto-inputs port 443\n\nClick Save\n\n\noutbound tab\n\nBy default all outbound traffic goes out on the inbound ports, but you can specify additional ports\nclick edit\n\nType: all traffic (auto-inputs protocol all, port range 0-65535)\nSource: same as for inbound\n\n\n\nCreate security group for Load Balancer\n\nCreate Security Group\n\nSame procedure as above, different name\n\nInbound tab\n\nclick edit\n\nType: HTTP\n\nkeep Source as is\n\n0.0.0.0 means accept traffic from everywhere\n\n\nclick add rule\nType: HTTPS\n\nsame\n\nclick Save\n\n\nOutbound tab\n\nclick edit\n\nType: HTTP¬†(auto-inputs protocol TCP and port 80)\nSource: type the application security group name (long box)\n\nsee load balancer section for other details\n\nclick add rule\nType: HTTP\n\nsame\n\nclick Save",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-s3",
    "href": "qmd/aws.html#sec-aws-s3",
    "title": "AWS",
    "section": "S3",
    "text": "S3\n\nSearch S3\nautoscales,, replicates your data to prevent total loss, ability to version data, max file size 5TB\ncharged for what you use (GB/mo)\n3 different classes\n\nstandard\n\nhighest availability (99.999%), durability (replicated across hosts multiple times)\nmost expensive of the classes\n\ninfrequently accessed\n\nless durability (replicates), should be data that doesn‚Äôt end your world if lost\n\nglacial\n\nfor archiving purposes\n\n\nSecurity\n\nIAM\n\ngive certain persons or departments permissions\n\nS3 policies\n\nmake certain buckets public, private, etc.\n\n\nBuckets\n\nmust have unique name across all AWS\n\nassume this is taken care of by some auto-generated id\nCan‚Äôt contain periods (dns-compliant)\n\nregion-specific\ncross-region replication\n\ncopy objects from one bucket to another for a fee\n\n\nObjects\n\nfiles, artifacts, etc\nkey:value look-up\n\nthe keys are essentially just file paths\n\nbucket_name/folder/file.txt\nbucket_name/folder/*\n\ngets everything\n\n\nthe values are the objects\n\nYou can directly download files from the console but not folders\n\nselect file, click on ‚Äúmore‚Äù button (top left), select ‚Äúdownload as‚Äù, follow directions\n\nTo make publicly available through a web link you have to specify that policy (see below)\n\n¬†Create a basic bucket\n\nClick Create Bucket (top left)\n\nOpens Wizard\n\nName and Region\n\nEnter unique DNS compliant name (no periods)\nEnter Region\nClick next\n\nSet Properties\n\nkeep defaults\nclick next\n\nSet Permissions\n\nkeep defaults\nclick next\n\nReview\n\nclick create bucket\n\n\n\nUpload to bucket\n\nUpload via aws console\n\nClick bucket name\nclick upload button (top left)\nOpens Wizard\n\nSelect files\n\nclick add files or drag and drop folders into the window\nclick next\n\nSet Permissions\n\nkeep defaults\nclick next\n\nSet Properties\n\nStorage class\n\nstandard, standard-ia (infrequently accessed), reduced redundancy\n\nencryption\nkeep defaults\nclick next\n\nReview\n\nclick upload\n\n\n\nSee below for uploading via CLI\n\n\nCreate Bucket Policy to allow for public download (from a weblink)\n\nclick bucket name\nclick Permissions tab (top mid)\nBucket policy requires a json expression\n\nclick policy generator (bottom left)\n\nopens up new browser tab\n\nSelect type of policy\n\nS3 Bucket Policy\n\neffect (allow or deny)\n\nselect allow\n\nPrincipal\n\ntype ‚Äú*‚Äù (asterisk) to give everyone access\n\nActions\n\ncan select more than one action if you want\nselect ‚ÄúGetObject‚Äù\n\nAmazon Resource Name\n\nThe format of the required expression is given below the box\nyour substituting your bucket_name/key_name into the expression\n\nfor key name he used * to signify ‚Äúeverything‚Äù, but I think this would be your path to the resources (not including bucket_name)\n\ne.g.¬†folder1/folder2/file.csv or folder1/folder2/*\n\n\nClick add statement\nClick generate policy\ncopy json expression\n\n\nGo back to previous browser tab with the permissions tab and paste the expression into the window\nClick Save (mid right)\n\nThere‚Äôs also a DELETE button if you want to remove a policy from the bucket\n\nShould see a ‚Äúpublic‚Äù tag on the permissions tab confirming the policy has been set\n\nDownload from bucket\n\nGet download link for a file that has a public permission set\n\ntick box of selected file\n\nwindow with info about the file should open on the right side\n\ncopy download link in overview section, paste in browser or use programatically\n\nDownload via console\n\nclick file name ‚Äì overview tab\n\n¬†various downloading options\n\n\nProgramatically: see section Upload to bucket via CLI below (uses copy command)\n\nGive bucket viewing (and downloading) access via IAM (need to have administrator permissions)\n\nsearch IAM\nleft panel ‚Äì Policies\nclick create policy (top left)\nclick choose service (mid)\n\ntype or select ‚ÄúS3‚Äù\n\nclick select actions\n\nUnder Access Level\n\nList\n\nselect all (4)\n\nRead\n\nView policy\n\nGetBucketAcl, GetBucketCORS, GetBucketLocation, GetBucketLogging, GetBucketPolicy, GetBucketTagging, GetObjectAcl, ListBucketByTags, ListBucketVersions\n\nDownload policy (should be a separate policy from View)\n\nGetObject\n\n\nResources\n\nyou have to select resources (e.g.¬†buckets, objects (i.e.¬†folders, files) that the actions above affect\n\nView policy\n\nchoose all resources (which is buckets and objects)\n\nDownload policy\n\nchoose specific\nclick ARN\n\nEnter the bucket name you want to give access to (or tick ‚Äúany‚Äù box to give access to all buckets)\nenter object name (or tick ‚Äúany‚Äù box for access to all objects)\n\nClick add\n\n\n\nClick Review Policy (bottom right)\n\nEnter a Name\n\neg ListAllBucketsObjsS3\n\nClick Create Policy (bottom right)\nSelect or click newly created policy (should be back to left panel ‚Äì policies console\n\nnew policy name should be in a clickable banner at top of screen\nOr choose it from list of policies that‚Äôs displayed\n\nGo to Attached entities tab\n\nclick attach\nselect users or groups you want the policy to apply to\nclick attach (bottom right)\n\n\n\nView buckets you have permissions with: aws s3 ls or aws s3api list-buckets --output text\nUpload to bucket via CLI\n\nAlso see above for uploading via aws console\naws s3 help, aws s3  help\naws s3 cp     can copy files via:\n\nbucket to bucket\nlocal to bucket\nbucket to local\n\nlocal to bucket\n\naws s3 cp &lt;C:\\\\Users\\\\path\\\\to\\\\file.csv&gt; &lt;s3://&lt;bucket/path/to/folder/name.csv&gt; --region &lt;bucket? region&gt; --profile &lt;profile name&gt;\nsee cli section above on how to create profile\nrefresh console if you already have it open to see the file\n\nbucket to local¬†\n\naws s3 cp &lt;s3://&lt;bucket/path/to/folder/name.csv&gt; &lt;C:\\\\Users\\\\path\\\\to\\\\file.csv&gt;¬†--region &lt;bucket? region&gt; --profile &lt;profile name&gt;\nsame thing as before, just reversing the  and  URIs\n\n\nVersioning\n\nIf you enable versioning and then disable it, then the versioned objects will remain but new objects won‚Äôt be versioned\n\n¬†Would have to manually delete the versioned objects manually\n\nbucket name ‚Äì properties tab\n\nclick on ‚ÄúVersioning‚Äù card\ntick Enable versioning¬†\nclick save\n\nTo see the different file versions\n\nbucket name ‚Äì folder\nclick versions: show button (upper left)\n\nOnce versions are displayed you can download and delete files/versions in various menus\n\nclick file name ‚Äì ‚Äúlatest version‚Äù drop down (next to file name, top left)\n\nshows all versions with options to download or delete\n\nclick file name ‚Äì overview tab\n\nvarious download options\n\ntick file name/version box ‚Äì click more button (upper left) ‚Äì select delete ‚Äì click delete button",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html",
    "href": "qmd/cloud-services.html",
    "title": "4¬† Cloud Services",
    "section": "",
    "text": "4.1 Misc",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#misc",
    "href": "qmd/cloud-services.html#misc",
    "title": "4¬† Cloud Services",
    "section": "",
    "text": "See Cloud Costs Every Programmer Should Know for various service estimates in order to perform back-of-the-napkin calculations of project costs\nFor ‚ÄúStead-State Workloads‚Äù requiring HPC, cloud compute doesn‚Äôt make economic sense\n\nSteady-State workloads are projects that are run near constantly\n\nSee Thread for discussion on scenarios, issues, and risks of your data center (DC) in the Cloud vs on-prem.\nExamples:\n\nAcademia: Where academics are in a queue to run experiments on the a HPC cluster\nWeather Forecasting: Forecasts are required nearly in real time, so these models run constantly\nFinancial transaction processing at a bank: The bank‚Äôs systems handle a constant stream of transactions\nInventory management system for a manufacturing plant: The system constantly receives updates on raw materials, production output, and finished goods.\nOthers: Week or two long analysis runs at hedge funds, genomic analysis jobs, a swath of AI training / fine tuning, Oil and Gas where they are plowing through seismic data constantly\n\n\n\nRStudio Server on your docker image allows you to access an ide connected to the server through a browser. Useful so you can make sure the correct packages are installed.\nServerless computing is a method of providing backend services on an as-used basis.\n\nA serverless provider allows users to write and deploy code without the hassle of worrying about the underlying infrastructure\nCharged based on their computation and do not have to reserve and pay for a fixed amount of bandwidth or number of servers, as the service is auto-scaling\ne.g.¬†AWS Lambda (i.e.¬†resources only get spun-up when an event is triggered)\n\nNVIDIA GPU Guide (thread)\n\nRTX 20-series or 30-series GPUs are forbidden from inclusion in data centers\nGeneral Recommendations (Oct 2022)\n\nA100 for model training\nT4 for inference workloads\n\nK80\n\nReleased in 2015, the K80 contained a lot of VRAM for the time (24 GB)\nCame before tensor cores and is relatively weak by today‚Äôs standards\nOnly okay for learning purposes\n\nP4\n\nReleased in 2016\nValue came from its low power consumption\nMay find it priced higher than its upgraded version (the T4), so recommended to avoid it\n\nT4\n\nReleased in 2018\nSignificant upgrade for inference workloads compared to the P4\nExtremely low power consumption, tensor cores, and plenty (16GB) of VRAM\nCheap, so if you have an inference workload, recommended to strongly consider a T4\n\nP100\n\nBig improvement for model training workloads over the K80 when released\nLess RAM (16GB) than K80\nWay more compute¬† than K80\n\nCan see memory savings from using mixed-precision training\n\nNo tensor cores\n\nV100\n\nHuge upgrade over the P100\nSame VRAM as P100 many but more CUDA cores\nIntroduces Tensor Cores\nMore cost-efficient than the P100\n\nA100\n\nnewest data center GPU\nupgraded tensor cores\nmost benchmarks show 3x+ faster training compared to the V100\n80GB VRAM\nPrice tag might be big, but it‚Äôs usually worth it over the V100",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#price-management",
    "href": "qmd/cloud-services.html#price-management",
    "title": "4¬† Cloud Services",
    "section": "4.2 Price Management",
    "text": "4.2 Price Management\n\nspot instances for cheaper machines\nautoscaling (kubernetes?) to handle peak usage times (spin-up more machines) while saving during slow times (spin down excess machines)\nUse opensource project management tools (dvc, airflow, etc)\nGoogle\n\nThe Google Kubernetes Engine (GKE) control plane is free, whereas Amazon‚Äôs (EKS) costs $0.20 an hour.\n\nAWS\n\nWith a well-defined framework of tag keys and values applied across different AWS resources, billing breakdowns by tag prove extremely useful for greater insight on the source of AWS charges ‚Äî especially if resources are tagged by department, or team, or different layers of organizational granularity.\nReserved Instances - commit to specific configurations for one or three years at reduced cost\nSpot Instances - pay significantly lower costs but potential for applications to be interrupted\nSavings Plans\n\nEC2 Instance Savings Plans to reduce compute charges for specific instance types and AWS regions\n\nSavings of up to 72%\n\nCompute Savings Plans to reduce compute costs irrespective of type and region.\n\nSavings up to 66% and extends to ECS Fargate and Lambda functions.\n\n\nImage Management\n\nData Lifecycle Manager - automates the creation, retention, and deletion of images\n\nWill not manage images and snapshots created by other means, and it also excludes instance store-backed images.\nEC2 Recycle Bin - serves as a safety net to avoid the accidental deletion of resources ‚Äî retaining images and snapshots for a configurable time where we may restore them before they are deleted permanently.\n\n\nLambda\n\nCloudwatch - Lambda automatically creates log groups for its functions, unless a group already exists matching the name /aws/lambda/{functionName}. These default groups do not configure a log retention period, leaving logs to accumulate indefinitely and increasing CloudWatch costs.\n\nExplicitly configure groups with matching names and a retention policy to maintain a manageable volume of logs.\n\nMemory Optimization - AWS Lambda Power Tuning can help to identify optimizations, albeit with notable initial costs given the underlying use of AWS Step Functions.\n\nLambda charges based on compute time in GB-seconds, where the duration in seconds is measured from when function code executes until it either returns or otherwise terminates, rounded up to the nearest millisecond. To reduce these times, we desire optimal memory configuration.\n\n\nS3 Lifecycle Configuration\n\nCharged for how much data stored, but also which S3 storage classes are utilized.\n\nStandard (default) class is the most expensive, permitting regular access to objects with high availability and short access times.\nInfrequent Access (IA) classes offer reduced cost for data which requires limited access (usually once per month)\nArchival options via Glacier deliver further cost reductions.\n\nConfiguring the lifecycle allows you to automatically transfer data to different storage classes and thereafter permanently delete it, X and Y days respectively after data creation",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#kaggle",
    "href": "qmd/cloud-services.html#kaggle",
    "title": "4¬† Cloud Services",
    "section": "4.3 Kaggle",
    "text": "4.3 Kaggle\n\nFree\n\n4-core CPU instances w/30 GB RAM\n2-core CPU, 2xT4 GPU w/13GB RAM\n\nT means tensor cores\n1 hour spent using 2xT4‚Äôs takes the same amount of your quota as a P100 (old free gpu offering)\n\nMeans 30-40 hours of free, multi-GPU compute per week",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#saturn-cloud",
    "href": "qmd/cloud-services.html#saturn-cloud",
    "title": "4¬† Cloud Services",
    "section": "4.4 Saturn Cloud",
    "text": "4.4 Saturn Cloud\n\nSaturn Cloud Recipes\n\nJSON files that specify your environment\nGood for keeping track of server dependencies (e.g.¬†linux libraries)\n\nDunno about R packages",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#google-cloud-platform-gcp",
    "href": "qmd/cloud-services.html#google-cloud-platform-gcp",
    "title": "4¬† Cloud Services",
    "section": "4.5 Google Cloud Platform (GCP)",
    "text": "4.5 Google Cloud Platform (GCP)\n\nBigQuery sandbox is Google‚Äôs GCP free tier cloud SQL database. It‚Äôs free but your data only lasts 60 days at a time.\nGCP allows users to run deep learning workloads on TPUs\nSince data expires after 60 days, back-up the model coefficients and performance score tables to Google Sheets. Article suggested this is possible through WebUI.\nAs of Nov.19, regression, logistic regression, and k-nn are the only models available to be run with the sql query editor\nhttps://cloud.google.com/free/\n\n$300 credit for 12 months\nAlways free:\n\n2M requests for containers\n1 GB storage\n\nScalable NoSQL document database.\n50,000 reads, 20,000 writes, 20,000 deletes per day\n\nFunctions\n\n1 f1-micro instance per month (Available only in region: us-west1, Iowa: us-central1, South Carolina: us-east1)\n30 GB-months HDD\n5 GB-months snapshot in select regions\n1 GB network egress from North America to all region destinations per month (excluding China and Australia)\n\nKubernetes\n\nOne-click container orchestration via Kubernetes clusters, managed by Google.\nNo cluster management fee for clusters of all sizes\nEach user node is charged at standard Compute Engine pricing\n\nApp Engine\n\n28 instance hours per day\n5 GB Cloud Storage\nShared memcache\n1,000 search operations per day, 10 MB search indexing\n100 emails per day\n\nBigQuery\n\nFully managed, petabyte scale, analytics data warehouse.\n1 TB of querying per month\n10 GB of storage\n\nOther Stuff\n\nYour free trial credit applies to all GCP resources, with the following exceptions:\n\n* You can‚Äôt have more than 8 cores (or virtual CPUs) running at the same time.\n* You can‚Äôt add GPUs to your VM instances.\n* You can‚Äôt request a quota increase. For an overview of Compute Engine quotas, see Resource quotas.\n* You can‚Äôt create VM instances that are based on Windows Server images.\n\nYou must upgrade to a paid account to use GCP after the free trial ends. To take advantage of the features of a paid account (using GPUs, for example), you can upgrade before the trial ends. When you upgrade, the following conditions apply:\n\n* Any remaining, unexpired free trial credit remains in your account.\n* Your credit card on file is charged for resources you use in excess of what‚Äôs covered by any remaining credit.\nYou can upgrade your account at any time after starting the free trial. The following conditions apply depending on when you upgrade:\n* If you upgrade before the trial is over, your remaining credit is added to your paid account. You can continue to use the resources you created during the free trial without interruption.\n* If you upgrade within 30 days of the end of the trial, you can restore the resources you created during the trial.\n* If you upgrade more than 30 days after the end of the trial, your free trial resources are lost.\n\nSpot Instances (Preemptible VM)\n\nusage capped at 24 hrs\npricing is fixed and not market-driven\n\nGoogle price calculator:¬†https://cloud.google.com/products/calculator/#id=3115f19f-4ff0-4c57-9028-69cb994fe7ca\nExample\n\ncreating a cluster with:\n\n1 x Dataproc cluster node with 30 GB of RAM\n3 x Dataproc worker nodes with 15 GB of RAM\nUsing less than 5 GB of disk space in a bucket\nAnd running the cluster for only 4 hrs\nWould cost only around $5 at the end of the month\n\n\nFree Tier\n\nincludes a 12-month free trial with $300 credit to use with any GCP services and an Always Free benefit, which provides limited access to many common GCP resources\nUse to test out, but KEEP EVERYTHING SMALL (data, hardware, etc). Need to upgrade it to see the true benefit. Free tier resources look like my desktop computer. Whatever cash is leftover should transfer to account.\nhttps://cloud.google.com/free/docs/gcp-free-tier#how-to-upgrade\nupgrade it from the free trial to a paid account through the GCP Console clicking the Upgrade button at the top of the page\n\n\n\nSteps for new project\n\nGo to interface¬†https://console.cloud.google.com/\ncreate a project. ‚Äúselect a project‚Äù on top bar ‚Äì&gt; ‚Äúnew project‚Äù on top right ‚Äì&gt; choose name (optionally a folder/organization if you have one) ‚Äì&gt; create\n(article wasn‚Äôt very reliable and went on talk about a python implementation so I stopped here\n\nTips\n\nApp Engine\n\nDon‚Äôt use App Engine Standard environments ‚Äî big brother G wants you to use rather Flex environments, otherwise, they‚Äôll punish you.\nReview cost analysis regularly to make sure there are no surprising costs.\nMake sure you clean up redundant App Engine application versions to prevent G from robbing you.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#microsoft-azure",
    "href": "qmd/cloud-services.html#microsoft-azure",
    "title": "4¬† Cloud Services",
    "section": "4.6 Microsoft Azure",
    "text": "4.6 Microsoft Azure\n\nhttps://azure.microsoft.com/en-us/free/?WT.mc_id=Revolutions-blog-davidsmi\nhttps://visualstudio.microsoft.com/dev-essentials/\n\nstarts azure trial but gives you free sql server developer edition\n\nWon‚Äôt be charged until you choose to upgrade.\n12 months access to $ services for free\n$200 credit for any service for 30 days\n\nAt the end of the 30 days, I think the remainder goes into your account after you change to a pay-to-play account\n\nAccess to the services that are always free\n\nAzure Kubernetes Service (AKS)\nFunctions\n\n1,000,000¬†requests per month\na solution for easily running small pieces of code in the cloud. You can write just the code you need for the problem at hand, without worrying about a whole application or the infrastructure to run it.\nExample use case:¬†for handling WebAPI requests and sending the different data and results to where it needs to go.\n\nApp Service\n\n10¬†web, mobile, or API apps\n\nActive Directory B2C (identity)\n\n50,000¬†authentications per month\n\nMachine Learning Server\n\nDevelop and run R and Python models on your platform of choice.\n\nSQL Server 2017 Developer Edition\n\nBuild, test, and demostrate applications in a non-production environment.\n\nOther stuff\n\nBlob storage\n\nobject storage solution for the cloud\noptimized for storing massive amounts of unstructured data\n\nSpot Instances (Low Priority VM)\n\nnot time limit on instance usage\nno warning on termination by Azure\n\nTips\n\nIf you can‚Äôt create a service, because Azure servers are under maintenance for more than a couple of minutes ‚Äî check out your permissions and registrations under the ‚ÄúResource providers‚Äù panel.\nIf you see any strange errors on the Azure Portal ‚Äî just change the filters‚Äô values.\nIf you use Azure Machine Learning, and your scoring function cannot locate your source code ‚Äî deliver the code as a Model and add it explicitly to the sys.path in the init function.\nIf you use Azure Machine Learning, don‚Äôt use Batch Endpoints ‚Äî it looks like they are not ready yet ‚Äî just use the regular Published Pipelines. In fact, ‚ÄúBatch endpoint‚Äù is just a wrapper around a published pipeline.\nDon‚Äôt include flask in your Azure conda environment specification.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#aws",
    "href": "qmd/cloud-services.html#aws",
    "title": "4¬† Cloud Services",
    "section": "4.7 AWS",
    "text": "4.7 AWS\n\nInstance types\n\nc-type instances are compute heavy\nr-type instances are RAM heavy\nm-type instances are balanced\n‚ÄúEach thread is represented as a virtual CPU (vCPU) on the instance. An instance has a default number of CPU cores, which varies according to instance type. For example, an m5.xlarge instance type has two CPU cores and two threads per core by default‚Äîfour vCPUs in total.‚Äù\nspot prices from 03/24/2020, all calculations over the previous month\ngen purpose\n\nm6g.8xlarge\n\ngen purpose, 32 vcpu, 128 gb\nnewer graviton, didn‚Äôt see any specs, but supposed to be much better than the xenon 1st gen\n\nm5.8xlarge\n\ngen purpose, 32 vcpu, 128 gb\nolder 3.1 ghz, xenon\non-demand $1.54/hr\n\nm5a.8xlarge\n\ngen purpose 32 vcpu, 128 gb\n2.4 ghz, slower processor speed than m5\n\nm5n.8xlarge\n\ngen purpose 32 vcpu, 128 gb\n3.1 ghz, xenon specialized for neural networks, ML tasks\nn.virg, 71% savings,¬†&lt;5% interruption\nohio,¬†83% savings, &lt;5% interruption\non-demand $1.90/hr\npotential spot price = $0.32\n\nm5dn.8xlarge\n\nsame but with 2 ssd hard drives\n\nm4.10xlarge\n\ngen purpose 40 vcpu, 160 gb\n2.4 ghz\nsmaller write-up, get the sense these are older processors/instances\n\n\ncompute optimized\n\nRequires HVM AMIs that include drivers for ENA (network adaptor) and NVMe (ssd hard drives)\n\nseems standard on a lot of instances (gen purpose and here), shouldn‚Äô t be an issue\n\nc5.9xlarge\n\n36 vcpu, 72 gb\n3.4 ghz\non-demand $1.53/hr\n\nc5d.9xlarge\n\nsame but with ssd\n\nc5n.9xlarge\n\n36 vcpu, 96 gb\n3.0 ghz, built for task needing high throughput for networking\non-demand, $1.94/hr\n\nc4.8xlarge\n\n36 vcpu, 60 gb\n2.9 ghz\n67% savings, &lt;5% interruption\non-demand $1.59/hr\npotential spot price = $0.52\n\n\nmemory optimized\n\nr5.8xlarge\n\n32 vcpu, 256 gb\n3.1 ghz\nn.virg, 72% savings, 5-10% interruption\nn.cal,¬†76% savings, &lt;5% interruption\non-demand $2.02/hr\npotential spot price = $0.48\n\nr5a.8xlarge\n\n32 vpu, 256 gb\n2.5 ghz\n\nr5n.8xlarge\n\n32 vcpu, 256 gb\n3.1 ghz, neural network optimized\nus.west. oregon 76% savings, 5-10% interruption\non-demand $2.38/hr\npotential spot price = $0.57\n\nr4.8xlarge\n\n32 vcpu, 244 gb\n2.3 ghz\n\nz1d.6xlarge\n\n24 vcpu, 192 gb\n4.0 ghz\non-demand $2.23\n\n\naccelerated computing\n\ninf1.6xlarge\n\n24 vcpu, 48 gb\nbuilt for ML\non-demand $1.91/hr\n\n\n\nFree Tier (12 months after sign-up)\n\naws.amazon.com¬†‚Äì pricing (top) ‚Äì free tier (mid) ‚Äì create a free account (mid)\nEC2\n\n750 hrs/mo of t2-micro instance usage\n\nfor Linux, Windows, RHEL, SLES AMIs\n\n\nElastic Block Storage (EBS)\n\n30 GB\ncan be connected to an ec2\n\nElastic Container Registry\n\n500 MB per month\n\nfor storing and retrieving Docker images\nexample in course was a basic nginx image and it was 50MB\n\n\nS3\n\n5 GB of standard storage (high availability/ high durability)\n20,000 Get Requests,¬†2000 Put Requests per month\n\nElastic Load Balancing\n\n750 hrs per month shared between classic and application load balancers\n\nno idea what the differences are between classic and application\n\n\n\nPricing\n\nPrice per GPU as of 29-06-2023\n\n\nExamples\n\nr3.4xlarge 16 CPUs, 122 GB RAM,¬†1 x 320 SSD,¬†Spot Price: $0.1517/h\n\nTrained H2O GBM, RF, XGBoost, DeepLearning. Cluster ran for 2 hr 40 min. Total Cost = around $0.42\nhttps://www.daeconomist.com/post/2019-01-15-partii/\n\n\nStorage\n\nS3\n\ncharged by amount stored\n\n$0.023/GB for standard (for first 50 TB)\n0.004/GB for glacier and 0.00099/GB deep glacier\n\ntakes longer to retrieve and not always available\n\n\nfree inbound transfer\nfree transfer between aws services (e.g.¬†S3 to EC2) within the same region\n\nAurora\n\nstorage + inbound/outbound: $0.20 per million requests\n\n\nConsolidated Biling\n\na separate account. All company individual accounts (marketing, sales, etc.) bills are pooled into this account\nhas no access to services\nhas no permissions to access services in other accounts\npooled bill counted towards potential discount billing\n\nCalculators\n\nTotal Cost of Ownership (TCO) calculator\n\ncompares cost of running a project on-premises to aws cloud\n\naws pricing calculator\n\ncalculates price of running a cloud application\ncalculator.aws.com\nestimates cost per service, per service group, and total infrastructure\nhelps find right ec2 instance and region\n\n\nBilling and Cost Management console\n\ncost explorer\n\nview and analysis costs and usage\n\n\n\n\nSpot Instances\n\nSummary\n\nGo to spot advisor and find instances that fit budget and compute requirements\nPrepare strategy for interruption\nOther services\n\nAs of Jan 01, 2019, cloudyr‚Äôs aws.ec2 PKG didn‚Äôt support all spot instances.\nno time limit on instance usage\nAWS gives a 2 min warning when it decides it needs your spot instance\npricing is market driven depending on capacity levels at the time\nAvailable actions when Amazon ‚Äúinterrupts‚Äù your instance:\n\nHibernation:\n\n‚Äúlike closing your laptop display‚Äù\nsaves data and memory and reboots once instance is available again\nRight before interruption, a daemon on the instance freezes the memory and stores it in Elastic Block Store (EBS) root volume\nYour EC2 will retain this root volume and any other EBS data volumes\nOnce market price falls below bid price, instance resumes with memory restored from disk to RAM\nYou aren‚Äôt charged while instance is in hibernation, but EBS volumes do cost $.\nAvailable for instance types: C3, C4, M4, R3, and R4 with &lt; 100 GB RAM on Amazon‚Äôs Linux, Ubuntu, and Windows\nAll this is done by something called the EC2 Hibernation Agent which sound like its just the name of the program on the servers\n\nStop\n\n‚Äúlike shutting down your computer to be turned on later‚Äù\nlose whatever is in RAM but retain EBS data volumes ($)\nrestores once bid price &lt; market price\n\nTerminate\n\n***default option***\neverything deleted\n\n\nSpot Advisor\n\n**always use this before spinning up spot instances **\nhttps://aws.amazon.com/ec2/spot/instance-advisor\nInput\n\nvCPUs\nMemory size\nPlatform (linux?)\navailability zone (region?)\namount required (number of instances?)\n\noutput\n\ninstance type\nvCPUs\nMemory (GB)\nSavings over On-Demand (%)\nFrequency of termination (%)\n\nliklihood your instance will get terminated\n\n\n\nRunInstance API\n\nFor requesting a spot instance through CLI I think\nLooks like you send something that looks like a python dict with max price, type, region, etc. to this API\n\nSpot Blocks\n\nallows you to set a finite duration that your instance will run for\n\n1 to 6 hrs\nno interruption during that time\n\ntypically 30 to 45% cheaper than on-demand and maybe an additional 5% cheaper during non-peak hours for the region\nrecommended for batch runs\n\nStrategy\n\nUse regions with largest pools of spot instances\n\nLargest pools\n\nus.east.1 (north.virginia)\neu.west.1(ireland)\n\nThese regions have most types/most instances available\nTypically can go uninterrupted for weeks\nless price fluctuation = more certainty\n\n\nSmallest pools\n\neu.central.1 (frankfort)\nap.south.1 (mumbai)\nap.southeast.1 (singapore)\n\ntypically get interrupted within days\n\n\n\nRun groups of instances that come from multiple spot pools\n\nTo used different compute types, jobs/tasks need to be in containers\nspot pools are instances with same region, type, OS, etc.\napplications running on instances from a least 5 different pools can cut interruptions by up to 80%\n\n\nManaging/preparing for interruptions\n\nOnly use for jobs that are short lived\n\ndevelopment and staging environments, short data processing, proof-of-concept, etc.\n\nBuild internal management system that automatically handles interruptions\n\nlook at spot pool historical prices for past 90 days\n\nlooking for least volatile pools\nolder generation (e.g.¬†c-family, m-family) tend to be most stable\n\n\nUse 3rd party platform that manages spot instances and interruptions\n\nSpotinst - uses ML to choose and manage instances that optimizes price and provide continuous activity for apps that are without a single point of failure.\n\nUses on-demand as a fall-back.\nSLA guarantees 99.9% availability.\nSnapshots volumes to migrate data to new instances in case of interruption.\nworks with other services and platforms (kubernetes, codedeploy, etc.)\n\nSpot Fleet - aws service, automanages groups of spot instances according to either of the following strategies:\n\nstrategy options\n\nlowest price - lowest price instances\ndiversified - spread instances across pools\n\nAfter receiving 2 min warning,\n\ntake snapshots of AMI and any attached EBS volumes and use them to launch a new instance.\n\nsnapshot of AMI\n\non EC2 dashboard ‚Äì left panel ‚Äì instances ‚Äì instances\n\nright-click instance ‚Äì image ‚Äì create AMI\n\nimage is in left -panel ‚Äì Images ‚Äì AMIs\n\n\n\nActually both snapshots might be able to taken in left panel ‚Äì spot requests\n\nsee AWS note ‚Äì EC2 for further details\n\n\n\n\n\n\nneed to drain and detach instance from elastic load balancer if one is used\nIf using auto-scaling, need to create an on-demand group and a spot instance group\n\n\nKubernetes\n\nAfter receiving 2 minute interruption warning from AWS:\n\nDetach instance from elastic load balancer (ELB) is one is being used\nMark instance as unschedulable (?)\n\nprevents new pods (group of containers on an instance that performs a job) from being scheduled on that node\nunderlying compute capacity and scheduling of resources of the pods needs to be monitored. Compute capacity and pod resource requirements need to match.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#comparison",
    "href": "qmd/cloud-services.html#comparison",
    "title": "4¬† Cloud Services",
    "section": "4.8 Comparison",
    "text": "4.8 Comparison\n\nMisc\n\nNotes from\n\nThe Top Clouds Evaluated Such That You Don‚Äôt Need to Repeat Our Mistakes\nAWS vs GCP reliability is wildly different\n\nNo services for blockchain development, quantum computing, and graph databases in GCP (May 2022)\nhttps://cloud-gpus.com/ - tool for comparing gpu compute prices across vendors\n\nData centers\n\nCloser the resources are to your business, the less latency\n(May 2022) GCP has caught up and surpassed AWS in the number of data centers and regions that are available\n\nCompute\n\nCheapest vCPU\n\nGCP ‚Äúe2-micro-preemptible‚Äù with 2 vCPU and 1 GB memory.\n\n48% lower than ‚Äút4g.nano‚Äù from AWS\n5 times lower than ‚ÄúA0‚Äù from Azure.\n\nAWS is in-between GCP and Azure in terms of price (i.e.¬†Azure most expensive for cheap vCPUs)\n\nMore performant GCP instances usually cost approximately the same as their analogs from other cloud providers\n\nAzure servers cost the same or slightly less than AWS\n\nGCP: dedicated PostgreSQL server\n\nCheapest instances are 25% lower than the competitors\n\nGPU on-demand availability\n\nConclusion: Assuming you need on-demand boxes to succeed right when you need them, the consensus seems to clearly point to AWS. If you can stand to wait or be redundant to spawn failures, maybe Google‚Äôs hardware acceleration customizability can win the day.\nStats\n\nAWS consistently spawned a new GPU in under 15 seconds (average of 11.4s).\nGCP on the other hand took closer to 45 seconds (average of 42.6s).\nAWS encountered one valid launch error in these two weeks whereas GCP had 84\n\nCaveats\n\nGCP allows you to attach a GPU to an arbitrary VM as a hardware accelerator - you can separately configure quantity of the CPUs as needed.\nAWS only provisions defined VMs that have GPUs attached\n\n\n\nRecommendations\n\nAzure\n\nYou use the Microsoft Office stack (Word, Teams, OneDrive, SharePoint, etc.) and/or C# programming language.\nYou head neither for the cheapest servers nor for the most expensive ones ‚Äî you need something in the middle.\nYou need a memory-optimized solution rather than a general-purpose or a compute-optimized one.\nYou read about the current bugs and inconsistencies in Azure, and it does not scare you.\n\nAWS\n\nYou are rich.\nYou have AWS experts in your team.\nYou build an enterprise-level long-term project.\nOR you just want to rent a cheap virtual machine, and you don‚Äôt care about all the other facilities.\n\nGCP\n\nYou are a start-up company.\nYou can‚Äôt invest much time in learning AWS and dealing with Azure bugs.\nYou don‚Äôt need much flexibility and configuration facilities from the cloud.\nYou are ready to accept the approaches dictated by the platform.\nYou need either a general-purpose or a compute-optimized solution, but not a memory-optimized one.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html",
    "href": "qmd/db-engineering.html",
    "title": "Engineering",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-misc",
    "href": "qmd/db-engineering.html#sec-db-eng-misc",
    "title": "Engineering",
    "section": "",
    "text": "If you‚Äôre developing an application, a good rule of thumb is to write your frequently run queries in such a way that they return a response within 500 ms\nColumn storage files (parquet) are more lightweight, as adequate compression can be made for each column. Row storage doesn‚Äôt work in that way, since a single row can have multiple data types.\n\n\n(See below) Apache Avro is smaller file size than most row format file types (e.g.¬†csv)\n\n{pins}\n\nConvenient storage method\nUse when:\n\nObject is less than a 1 Gb\n\nUsed {butcher} for large model objects\n\nSome model objects store training data\n\n\n\nBenefits\n\nJust need the pins board name and name of pinned object\n\nThink the set-up is supposed to be easy\n\nEasy to share; don‚Äôt need to understand databases",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-terms",
    "href": "qmd/db-engineering.html#sec-db-eng-terms",
    "title": "Engineering",
    "section": "Terms",
    "text": "Terms\n\nACID - A database transaction, by definition, must be atomic, consistent, isolated and durable. These are popularly known as ACID properties.¬† These properties can ensure the concurrent execution of multiple transactions without conflict. Guarantees data validity despite errors and ensure that data does not become corrupt because of a failure of some sort.\n\nCrucial to business use cases that require a high level of data integrity such as transactions happening in banking.\n\nBatch processing - performing an action on data, such as ingesting it or transforming it, at a given time interval.\nBTEQ - Batch Teradata Query (like SQL) is simply a utility and query tool for Teradata which is a relational database system Creating a BTEQ script to load data from a flat-file.\nConcurrency - multiple computations are happening at the same time\nData Dump - A file or a table containing a significant amount of data to be analysed or transferred. A table containing the ‚Äúdata dump‚Äù of all customer addresses.\nData Mart - A subset of a data warehouse, created for a very specific business use case. Finance data mart storing all the relevant financial information required by the Accounting team to process their month-end cycles.\nData Integration - Usually, the hardest part of the project, where multiple sources of data are integrated into a singular application/data warehouse. Integrating finance and customer relationship systems integrating into an MS SQL server database.\nData Lake - A repository for all kinds of structured and unstructured data. Mainly based on Hadoop storage technology. Called a lake as it is flexible enough to store anything from raw data to unstructured email files. Hadoop Data Lake. Storing logs of all customers called into the inbound call centre including call duration.\nData Mesh - Decentralized design where data is owned and managed by teams across the organisation that understands it the most, known as domain-driven ownership. tl;dr - Each department controls they‚Äôre own data from ingestion to ‚Äúdata products.‚Äù This data product is then made a available to the other departments for them to use in their projects. Each department has their own engineers, scientists, and analysts.\n\nEach business unit or domain aims to infuse product thinking to create quality and reusable data products ‚Äî a self-contained and accessible data set treated as a product by the data‚Äôs producers ‚Äî which can then published and shared across the mesh to consumers in other domains and business units ‚Äî called nodes on the mesh.\nEnables teams to work independently with greater autonomy and agility, while still ensuring that data is consistent, reliable and well-governed.\nYou don‚Äôt have to figure out who‚Äôs in charge of what data, who gets to access it, who needs to protect it and what controls and monitoring is in place to ensure things don‚Äôt go wrong.\nExample: Banking\n\nCredit risk domain‚Äôs own data engineers can independently create and manage their data pipelines, without relying on a centralised ingestion team far removed from the business and lacking in credit expertise. This credit team will take pride in building and refining high-quality, strategic, and reusable data products that can be shared to different nodes (business domains) across the mesh.\n\n\nData Models - A way of organising the data in a way that it can be understood in a real-world scenario. Taking a huge amount of data and logically grouping it into customer, product and location data.\nData Quality - A discipline of measuring the quality of the data to improve and cleanse it. Checking Customer data for completeness, accuracy and validity.\nData Replication - There are multiple ways to do this, but mainly it is a practice of replicating data to multiple servers to protect an organisation against data loss. Replicating the customer information across two databases, to make sure their core details are not lost.\nDenormalization - database optimization technique in which we add redundant data to one or more tables. Designers use it to tune the performance of systems to support time-critical operations. Done in order to avoid costly joins. Me: Seems like it‚Äôs kind of like a View except a View might have calculated columns in it.\nDimensions - A data warehousing term for qualitative information. Name of the customer or their country of residence.\nDistributed SQL -¬† a single logical database deployed across multiple physical nodes in a single data center or across many data centers if need be; all of which allow it to deliver elastic scale and resilience. Billions of transactions can be handled in a globally distributed database.\nEDW - The same as a data warehouse except it includes all the data within an organisation. This means that the entire enterprise can rely on this warehouse for their business decisions. Organising sales, customer, marketing and finance data in an enterprise data warehouse to be able to create several key management reports.\nEmbedded aka In-Process\n\nEmbedded database as in a database system particularly designed for the ‚Äúembedded‚Äù space (mobile devices and so on.) This means they perform reasonably in tight environments (memory/CPU wise.)\nEmbedded database as in databases that do not need a server, and are embedded in an application (like SQLite.) This means everything is managed by the application.\n\nFacts - A data warehousing term for quantitative information. The number of orders placed by a customer.\nFlat File - Commonly used to transfer data due to their basic nature; flat files are a single table storing data in a plain text format. All customer order numbers stored in a comma-separated value (.csv) file\nHTAP - Hybrid Transactional Analytical Processing - System that attempts be good at both OLAP and OLTP\nMaster Data - This is data that is the best representation of a particular entity in the business. This gives you a 360 view of that data entity by generally consolidating multiple data sources. Best customer data representation from multiple sources of information.\nMulti-Master - allows data to be stored by a group of computers, and updated by any member of the group. All members are responsive to client data queries. The multi-master replication system is responsible for propagating the data modifications made by each member to the rest of the group and resolving any conflicts that might arise between concurrent changes made by different members.\n\nAdvantages\n\nAvailability: If one master fails, other masters continue to update the database.\nDistributed Access: Masters can be located in several physical sites, i.e.¬†distributed across the network.\n\nDisadvantages\n\nConsistency: Most multi-master replication systems are only loosely consistent, i.e.¬†lazy and asynchronous, violating ACID properties. (mysql‚Äôs multi-master is acid compliant)\nPerformance: Eager replication systems are complex and increase communication latency.\nIntegrity: Issues such as conflict resolution can become intractable as the number of nodes involved rises and latency increases.\n\nCan be contrasted with primary-replica replication, in which a single member of the group is designated as the ‚Äúmaster‚Äù for a given piece of data and is the only node allowed to modify that data item. Other members wishing to modify the data item must first contact the master node. Allowing only a single master makes it easier to achieve consistency among the members of the group, but is less flexible than multi-master replication.\n\nNiFi - It is an open-source extract, transform and load tool (refer to ETL), this allows filter, integrating and joining data. Moving postcode data from a .csv file to HDFS using NiFi.\nNormalization - A method of organizing the data in a granular enough format that it can be utilised for different purposes over time. Organizing according to data attributes reduces or eliminates data redundancy (i.e.¬†having the same data in multiple places). Usually, this is done by normalizing the data into different forms such as 1NF (normal form) or 3NF (3rd normal form) which is the most common. (See DB, Relational &gt;&gt; Normalization)\n\nTaking customer order data and creating granular information model; order in one table, item ordered in another table, customer contact in another table, payment of the order in another table. This allows for the data to be re-used for different purposes over time.\n\nNULL indexes - These are the indexes that contain a high ratio of NULL values\nObject-Relational Mapping (ORM) - Allows you to define your data models in Python classes, which are then used to create and interact with the database. See {{SQLAlchemy}}\nODS - Operational data store generally stores limited and current information to help simple queries. Unable to handle historical or complex data queries. An ODS for daily stock fluctuations in a warehouse help the warehouse manager decide what to prioritise in the next order delivery.\nOLAP - Online Analytical Processing - large chunks of tables are read to create summaries of the stored data\n\nUse chunked-columnar data representation\n\nOLTP - Online Transactional Processing - rows in tables are created, updated and removed concurrently\n\ntraditionally use a row-based data representation\npostgres excels at this type of processing\n\nRDBMS - Relational database management system. All of the above examples are RDBMS, meaning they store data in a structured format using rows and columns.\n\nA Microsoft SQL server database.\n\nReal-Time Processing (aka Event Streaming) - each new piece of data that is picked up triggers an event, which is streamed through the data pipeline continuously\nReverse ETL - Instead of ETL where data is transformed before it‚Äôs stored or ELT where data is stored and transformed while in storage, Reverse ETL performs transformations in the pipeline between Storage and the Data Product.\n\nSCD Type 1‚Äì6 - A method to deal with changes in the data over time in a data warehouse. Type 1 is when history is overwritten whereas Type 2 (most common) is when history is maintained each time a change occurs.\n\nWhen a customer changes their address; SCD Type 1 would overwrite the old address with the new one, whereas Type 2 would store both addresses to maintain history.\n\nSchemas - A term for a collection of database objects. These are generally used to logically separate data within the database and apply access controls.\n\nStoring HR data in HR schema allows logical segregation from other data in the organisation.\n\nSharding - Horizontal Partitioning ‚Äî divides the data horizontally and usually on different database instances, which reduces performance pressure on a single server.\n\nStaging - The name of a storage area that is temporary in nature; to allow for processing of ETL jobs (refer to ETL). Typically data is loaded from a source database into the staging area database where it is transformed. Once transformed, it‚Äôs loaded into the production database where analytics can be performed on it.\n\nA staging area in an ETL routine to allow for data to be cleaned before loading into the final tables.\n\nTransactional Data - This is data that describes an actual event.\n\nOrder placed, a delivery arranged, or a delivery accepted.\n\nUnstructured Data - Data that cannot be nicely organised in a tabular format, like images, PDF files etc.\n\nAn image stored on a data lake cannot be retrieved using common data query languages.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-datqual",
    "href": "qmd/db-engineering.html#sec-db-eng-datqual",
    "title": "Engineering",
    "section": "Data Quality",
    "text": "Data Quality\n\nAlso see Production, Data Validation\nAccuracy - addresses the correctness of data, ensuring it represents real-world situations without errors. For instance, an accurate customer database should contain correct and up-to-date addresses for all customers.\nCompleteness - extent your datasets have all the required information on every record\n\nMonitor: missingness\n\nConsistency - extent that no contradictions in the data received from different sources. Data should be consistent in terms of format, units, and values. For example, a multinational company should report revenue data in a single currency to maintain consistency across its offices in various countries.\nTimeliness - Data should be available at the time it‚Äôs required in the system\nValidity - ensuring that data adheres to the established rules, formats, and standards.\n\nMonitor: variable types/classes, numeric variable: ranges, number of decimal places, categorical variable: valid categories, spelling\n\nUniqueness - no replication of the same information twice or more. They appear in two forms; duplicate records and information duplication in multiple places.\n\nMonitor: duplicate rows, duplicate columns in multiple tables",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-costopt",
    "href": "qmd/db-engineering.html#sec-db-eng-costopt",
    "title": "Engineering",
    "section": "Cost Optimization",
    "text": "Cost Optimization\n\nAlso see\n\npage 53 in notebook\nGoogle, BigQuery &gt;&gt; Optimization\n\nAvoid disk operations, make sure that you look out for hints & information in the EXPLAIN PLAN of your query. (e.g.¬†using SORT without an index)\n\nWhen you see filesort, understand that it will try to fit the whole table in the memory in many chunks.\n\nIf the table is too large to fit in memory, it will create a temporary table on disk.\n\nLook out for a using filesort with or without a combination of using temporary.\n\nLoading data in chunks or streaming it record by record for ETL jobs helps to optimize memory usage.\nSplit tables with many columns Might be efficient to split the less-frequently used data into separate tables with a few columns each, and relate them back to the main table by duplicating the numeric ID column from the main table.\n\nEach small table can have a primary key for fast lookups of its data, and you can query just the set of columns that you need using a join operation.\n\nPrimary keys should be global integers.\n\nIntegers consume less memory than strings, and they are faster to compare and hash\n\nJoins\n\nWith correlated keys\n\nThe query planner won‚Äôt recognize the correlated keys and do nested loop join when a hash join is more efficient\nI don‚Äôt fully understand what correlated keys on a join are, but see SQL &gt;&gt; Terms &gt;&gt; Correlated/Uncorrelated queries\n\nIn the example below, a group of merge_commit_ids will only be from 1 repository id, so the two keys are associated in a sort of traditional statistical sense.\n\nSolutions\n\nUse LEFT_JOIN instead of INNER_JOIN\nUse extended statistics\nCREATE STATISTICS ids_correlation ON repository_id, merge_commit_id FROM pull_requests;\n\n‚Äúrepository_id‚Äù and ‚Äúmerge_commit_id‚Äù are the correlated keys\nI‚Äôm not sure if ‚Äúids_correlation‚Äù is a function or just a user-defined name\nPostgreSQL ‚â•13 will recognize correlation and the query planner will make the correct calculation and perform a hash join\n\n\n\n\nPre-join data before loading it into storage\n\nIf a group of tables is frequently joined and frequently queried, then pre-joining will reduce query costs\ncan be done using an operational transform system such as Spark, Flow, or Flink (dbt can parallelize runs and work w/Spark)\n\nIndexes{#sec-db-eng-costopt-index}\n\nIndexes help in filtering data faster as the data is stored in a predefined order based on some key columns.\n\nIf the query uses those key columns, the index will be used, and the filter will be faster.\n\nSuitable for any combination of columns that are used in filter, group, order, or join\nMySQL Docs\nDon‚Äôt use indexes with LIKE\nCluster a table according to an index\n\nAlso see Google, BigQuery &gt;&gt; Optimization &gt;&gt; Partition and Cluster\nRearranges the rows of a table on the disk\nDoesn‚Äôt stay ‚Äúclustered‚Äù if table is updated\n\nSee pg_repack for a solution\n\nExample\n-- create index\nCREATE INDEX pull_requests_repository_id ON pull_requests (repository_id, number)\n-- cluster table\nCLUSTER pull_requests USING pull_requests_repository_id\n\n\nUseful for queries such as\nSELECT *\nFROM pull_requests\nWHERE repository_id IN (...) AND number &gt; 1000\nBest Pactices\n\nAvoid too many indexes\n\nA copy of the indexed column + the primary key is created on disk\nIndexes add to the cost of inserts, updates, and deletes because each index must be updated\nBefore creating an index, see if you can repurpose an existing index to cater to an additional query\nCreate the least possible number of indexes to cover most of your queries (i.e.¬†Covering Indexes).\n\nMakes effective use of the index-only scan feature\nAdd INCLUDE to the create index expression\nExample\n-- query\nSELECT y FROM tab WHERE x = 'key';\n-- covering index, x\nCREATE INDEX tab_x_y ON tab(x) INCLUDE (y);\n-- if the index, x, is unique\nCREATE UNIQUE INDEX tab_x_y ON tab(x) INCLUDE (y);\n\ny is called a non-payload column\n\nDon‚Äôt add too many non-payload columns to an index. Each one duplicates data from the index‚Äôs table and bloat the size of the index.\n\n\nExample: Query with function\n-- query\nSELECT f(x) FROM tab WHERE f(x) &lt; 1;\n-- covering index, x\nCREATE INDEX tab_f_x ON tab (f(x)) INCLUDE (x);\n\nWhere f() can be MEAN, MEDIAN, etc.\n\n\n\nFix unusable indexes\n\nIssues related to data types, collation (i.e.¬†how it‚Äôs sorted), character set (how the db encodes characters), etc\nSometimes you can make the indexes work by explicitly forcing the optimizer to use them. (?)\n\nRepurpose or delete stale indexes\n\nIndexes are designed to serve an existing or a future load of queries on the database\nWhen queries change, some indexes originally designed to serve those queries might be completely irrelevant now\nAutomate stale index removal. Dbs keep statistics. Write a script to either notify you or just delete the index if it‚Äôs older and not been used past a certain threshold\n\nUse the most cost efficient index type\n\nExample: If your use case only needs a regular expression search, you‚Äôre better off having a simple index than a Full Text index.\n\nFull Text indexes occupy much more space and take much more time to update\n\n\nDon‚Äôt index huge tables (&gt; 100M rows), partition instead\n\nThen prune the partitions (partition pruning) you don‚Äôt need and create indexes for the partitioned tables you do keep.\n\n\nPartitioning\n\nAlso see Google, BigQuery &gt;&gt; Optimization &gt;&gt; Partition and Cluster\nSplits your table into smaller sub-tables under the hood\n\nNot viewable unless you check the table directory to see the multiple files that have been created\n\nThe same goes for indexes on that table.\n\n\nUse on tables with at least 100 million rows (BigQuery recommends &gt; 1 GB) Partitioning helps reduce table size and, in turn, reduces index size, which further speeds up the Data Warehouse (DWH) operations. But, partitioning also introduces complexity in the queries and increases the overhead of managing more data tables, especially backups. So try a few of the other performance techniques before getting to Sharding.\nPartition columns should always be picked based on how you expect to use the data, and not depending on which column would evenly split the data based on size.\n\nExample: partition on county because your analysis or transformations will largely be done by county even though since some counties may be much larger than others and will cause the partitions to be substantially imbalanced.\n\n\nUse ELT (e.g.¬†load data from on-prem server to cloud, then transform) instead of ETL (transform data while on-prem, then load to cloud) for data pipelines\n\nMost of the time you have a lot of joins involved in the transformation step\n\nSQL joins are one of the most resource-intensive commands to run. Joins increase the query‚Äôs runtime exponentially as the number of joins increases.\nExample\n\nRunning 100+ pipelines with some pipelines having over 20 joins in a single query.\nEverything facilitated by airflow (see bkmk for code)\nETL: postgres on-prem server, sql queries with joins, tasks ran 12+ hours, then the transformed data is loaded to google storage\n\n13+ hrs for full pipeline completion\n\nELT: running the queries with the joins, etc. with bigquery sql on the data after it‚Äôs been loaded into google storage.\n\n6+ hrs for full pipeline completion\n\n\n\n\nUse Materialized Views\n\nA smaller data object that contains the subset of data resulting from a specific query\nWhereas a query happens after data is loaded, a materialized view is a precomputation\nThe computation is done once, and changes to the data are incorporated as they occur, making subsequent updates to the view much cheaper and more efficient than querying the entire database from scratch\n\nFetching a large table will be slower if you try to use multiple cores.\n\nYou have to divide up the table and recombine it. Plus setting up parallel network processes takes time.\nThe time used to fetch some data from the internet depends massively on the internet bandwidth available on your router/network.\n\nUse Random Access via http range header + sparse-hilbert index to optimize db for query searches\nCITEXT extension makes it so you don‚Äôt have use lower or upper which are huge hits on performance (at least they are in WHERE expressions) GIN custom indexes for LIKE and ILIKE\nCREATE EXTENSION IF NOT EXISTS btree_gin;\nCREATE EXTENSION IF NOT EXISTS pg_trgm;\nCREATE INDEX index_users_on_email_gin ON users USING gin (email gin_trgm_ops);\n\nCREATE EXTENSION adds btree and pg_trgm extensions\nindex_users_on_email_gin is the name of the index\nusers is the table\nUSING gin (email gin_trgm_ops)\n\ngin specifies that it‚Äôs a gin index\nemail is the field\ngin_trgm_ops is from the pg_trgm extension. It splits the index into trigrams which is necessary for the gin index to work with LIKE or ILIKE\n\nSlower to update than the standard ones. So you should avoid adding them to a frequently updated table.\n\nGiST indexes are very good for dynamic data and fast if the number of unique words (lexemes) is under 100,000, while GIN indexes will handle 100,000+ lexemes better but are slower to update.\n\n\nNULLS LASTputs the NULLS in a field in any sorting operations at the end\n\nThe default behavior of ORDER BY will put the NULLS first, so if you use LIMIT , you might get back a bunch of NULLS.\nUsing NULLS LAST fixes this behavior but its slow even on an indexed column\n\nExample: ORDER BY email DESC NULLS LAST LIMIT 10\n\nInstead use two queries\nSELECT *\nFROM users\nORDER BY email DESC\nWHERE email IS NOT NULL LIMIT 10;\n\nSELECT *\nFROM users\nWHERE email IS NULL LIMIT 10;\n\nThe first one would fetch the sorted non-null values. If the result does not satisfy the LIMIT, another query fetches remaining rows with NULL values.\n\n\nRebuild Null Indexes\nDROP INDEX CONCURRENTLY users_reset_token_ix;\nCREATE INDEX CONCURRENTLY users_reset_token_ix ON users(reset_token)\nWHERE reset_token IS NOT NULL;\n\nDrops and rebuilds an index to only include NOT NULL rows\nusers_reset_token_ix is the name of the index\nusers is the table\nI assume ‚Äúreset_token has to be the field\n\nWrap multiple db update queries into a single transaction\n\nImproves the write performance unless the database update is VERY large.\nA large-scale update performed by a background worker process could potentially timeout web server processes and cause a user-facing app outage\nFor large db updates, add batching\n\nExample: db update has a 100K rows, so update 10K at a time.\nUPDATE messages SET status = 'archived'\n¬† WHERE id IN\n¬† (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 0);\nUPDATE messages SET status = 'archived'\n¬† WHERE id IN\n¬† (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 10000);\nUPDATE messages SET status = 'archived'\n¬† WHERE id IN\n¬† (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 20000);\n\nmessages is the table name\nI guess OFFSET is what‚Äôs key here.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-ets",
    "href": "qmd/db-engineering.html#sec-db-eng-ets",
    "title": "Engineering",
    "section": "Event Tracking Systems",
    "text": "Event Tracking Systems\n\nEvents are queued, then batch inserted into your db.\n\nStreaming events does not scale very well and is not fault tolerant.\n\nCommercial Services\n\nSegment\n\nMost popular option\nVery expensive\nSusceptible to ad blockers\nOnly syncs data once per hour or two\nMissing a few key fields in the schema it generates (specifically, session and page ids).\n\nFreshpaint is a newer commercial alternative that aims to solve some of these issues.\n\nOpen Source (each with a managed offering if you don‚Äôt feel like hosting it yourself)\n\nSnowplow is the oldest and most popular, but it can take a while to setup and configure.\nRudderstack is a full-featured Segment alternative.\nJitsu is a pared down event tracking library that is laser focused on just getting events into your warehouse as quickly as possible.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-stream",
    "href": "qmd/db-engineering.html#sec-db-eng-stream",
    "title": "Engineering",
    "section": "Streaming",
    "text": "Streaming\n\nStreaming or near real-time (i.e.¬†micro-batch) data\nQuestions\n\nWhat would be the data flow rate in that pipeline?\nDo you require real-time analytics, or is near-real-time sufficient?¬†\n\nData Characteristics\n\nIt is ingested near-real-time.\nUsed for real-time reporting and/or calculating near-real-time aggregates. Aggregation queries on it are temporal in nature so any aggregations defined on the data will be changed over time as the data comes.\nIt is append-only data but can have high ingestion rates so needs support for fast writes.\nHistorical trends can be analyzed to forecast future metrics.\n\nRelational databases can‚Äôt handle high ingestion rates and near-real-time aggregates without extensions.\nSteaming is the most expensive way to process the data in the majority of cases. Typically batch ingesting into warehouses is free, but streaming may not be.\nUse Cases: anomaly detection and fraud prevention, real-time personalized marketing and internet of things.\nTools:\n\nApache Kafka - Flexible, connects to app servers, other microservices, databases, sensor networks, financial networks, etc. and can feed the data to same types of systems including analytical tools.\n\nUtilizes a publish-subscribe model where producers (i.e.¬†sources) publish data to topics and consumers (e.g.¬†DBs, BI tools, Processing tools) subscribe to specific topics to receive relevant data.\nHighly scalable due to its distributed architecture, allowing data handling across multiple nodes.\nConfluent‚Äôs Kafka Connect - Open source and Commerical Connectors\n\nApache Flume - Similar to Kafka but easier to manage, more lightweight, and built to output to storage (but not as flexible as Kafka)\n\nLess scalable as data ingestion is handled by individual agents, limiting horizontal scaling.\nIts lightweight agents and simple configuration make it ideal for log collection\nCan also handle Batch workloads\nAble to perform basic preprocessing, e.g.¬†filtering specific log types or converting timestamps to a standard format\n\nAmazon Kinesis - A managed, commercial alternative to Kafka. Charges based on data throughput and storage. Additional features include data firehose for delivery to data stores and Kinesis analytics for real-time analysis.\nApache Flink - Processes streaming data with lower latency than Spark Streaming, especially at high throughputs. Less likely to duplicate data. Uses SQL. Steeper learning curve given its more advanced features.\nApache Spark Streaming - See Apache, Spark &gt;&gt; Streaming\nGoogle Pub/Sub - Uses Apache Beam programming API to construct processing pipelines\n\nGoogle Dataflow can create processing pipelines using streaming data from Pub/Sub. Developers write their pipelines using Beam‚Äôs API, and then Beam translates them into specific instructions for Flink or Spark to execute.\n\n{{temporian}} can interact with Beam to perform various time series preprocessing\n\nIf you have existing workflows around Hadoop or Spark or expertise in those frameworks, then Google Dataproc allows you to reuse that code. It also allows you to used other libraries that aren‚Äôt available in Dataflow. Supports various languages like Java, Python, and Scala.\nFor short-lived batch jobs, Dataproc might be more cost-effective. Although, Dataflow‚Äôs serverless nature avoids idle resource charges while Dataproc clusters incur costs even when idle.\n\n\nArchitectures\n\nNotes from\n\nData Pipeline Design Patterns\n\nETL\n\n\nKinesis collects data from a server (e.g.¬†app) and continuously feeds it to a lambda function for transformation. Transformed data is deposited into a S3 bucket, queried using Athena, and visualized using Quicksight.\n\nHybrid (Streaming and Batch)\n\n\nKinesis streams data to S3 and when a threshold is reached, a lambda trigger activates a transformation/batch load to the BQ warehouse\n\n\nTimeScale DB\n\nOpen source extension for postgresql\nSupport all things postgresql like relational queries, full SQL support(not SQL-like) as well as the support of real-time queries\nSupports an ingestion of 1.5M+ metrics per second per server\nNear-real-time aggregation of tables\nProvides integration with Kafka, kinesis, etc for data ingestion.\nCan be integrated with any real-time visualization tool such as Graphana\n\nPipeline DB\n\nOpen source extension for postgresql\nSimilar features as TimeScale DB\nEfficiency comes from it not storing raw data\n\nUsually, it‚Äôs recommended to store raw data",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-otools",
    "href": "qmd/db-engineering.html#sec-db-eng-otools",
    "title": "Engineering",
    "section": "Other Tools",
    "text": "Other Tools\n\nDataFold monitors your warehouse and alerts you if there are any anomalies (e.g.¬†if checkout conversion rate drops suddenly right after a deploy).\nHightouch lets you sync data from your warehouse to your marketing and sales platforms.\nWhale is an open source tool to document and catalog your data.¬†\nRetool lets you integrate warehouse data into your internal admin tools.\nGrowth Book that plugs into your data warehouse and handles all of the complicated querying and statistics required for robust A/B test analysis.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html",
    "href": "qmd/db-lakes.html",
    "title": "Lakes",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-misc",
    "href": "qmd/db-lakes.html#sec-db-lakes-misc",
    "title": "Lakes",
    "section": "",
    "text": "Data is stored in structured format or in its raw native format without any transformation at any scale.\n\nHandling both types allows all data to be centralized which means it can be better organized and more easily accessed.\n\nOptimal for fit for bulk data types such as server logs, clickstreams, social media, or sensor data.\nIdeal use cases\n\nBackup for logs\nRaw sensor data for your IoT application,\nText files from user interviews\nImages\nTrained machine learning models (with the database simply storing the path to the object)\n\nTools\n\nRclone - A command-line program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors‚Äô web storage interfaces. Over 70 cloud storage products support rclone including S3 object stores\n\nLower storage costs due to their more open-source nature and undefined structure\nOn-Prem set-ups have to manage hardward and environments\n\nIf you wanted to separate stuff like test data from production data, you also probably had to set up new hardware.\nIf you had data in one physical environment that had to be used for analytical purposes in another physical environment, you probably had to copy that data over to the new replica environment.\n\nHave to keep a tie to the source environment to ensure that the stuff in the replica environment is still up-to-date, and your operational source data most likely isn‚Äôt in one single environment. It‚Äôs likely that you have tens ‚Äî if not hundreds ‚Äî of those operational sources where you gather data.\n\nWhere on-prem set-ups focus on isolating data with physical infrastructure, cloud computing shifts to focus on isolating data using security policies.\n\nObject Storage Systems\n\nCloud data lakes provide organizations with additional opportunities to simplify data management by being accessible everywhere to all applications as needed\nOrganized as collections of files within directory structures, often with multiple files in one directory representing a single table.\n\nPros: highly accessible and flexible\nMetadata Catalogs are used to answer these questions:\n\nWhat is the schema of a dataset, including columns and data types\nWhich files comprise the dataset and how are they organized (e.g., partitions)\nHow different applications coordinate changes to the dataset, including both changes to the definition of the dataset and changes to data\n\nHive Metastore (HMS) and AWS Glue Data Catalog are two popular catalog options\n\nContain the schema, table structure and data location for datasets within data lake storage\n\n\nIssues:\n\nDoes not coordinate data changes or schema evolution between applications in a transactionally consistent manner.\n\nCreates the necessity for data staging areas and this extra layer makes project pipelines brittle",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-brands",
    "href": "qmd/db-lakes.html#sec-db-lakes-brands",
    "title": "Lakes",
    "section": "Brands",
    "text": "Brands\n\nHadoop\n\nTraditional format for data lakes\n\nAmazon S3\n\nTry to stay &lt;1000 entries per level of hierarchy when designing the partitioning format. Otherwise there is paging and things get expensive.\nAWS Athena ($5/TB scanned)\n\nAWS Athena is serverless and intended for ad-hoc SQL queries against data on AWS S3\n\n\nMicrosoft Azure Data Lake Storage (ADLS)\nMinio\n\nOpen-Source alternative to AWS S3 storage.\nGiven that S3 often stores customer PII (either inadvertently via screenshots or actual structured JSON files), Minio is a great alternative to companies mindful of who has access to user data.\n\nOf course, AWS claims that AWS personnel doesn‚Äôt have direct access to customer data, but by being closed-source, that statement is just a function of trust.\n\n\nDatabricks Delta Lake -\nGoogle Cloud Storage\n\n5 GB of US regional storage free per month, not charged against your credits.\n\nApache Hudi - A transactional data lake platform that brings database and data warehouse capabilities to the data lake. Hudi reimagines slow old-school batch data processing with a powerful new incremental processing framework for low latency minute-level analytics.",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-iceb",
    "href": "qmd/db-lakes.html#sec-db-lakes-iceb",
    "title": "Lakes",
    "section": "Apache Iceberg",
    "text": "Apache Iceberg\n\nOpen source table format that addresses the performance and usability challenges of using Apache Hive tables in large and demanding data lake environments.\n\nOther currently popular open table formats are Hudi and Delta Lake.\n\nInterfaces\n\nDuckDB can query Iceberg tables in S3 with an extension, docs\nAthena can create Iceberg Tables\nGoogle Cloud Storage has something called BigLake that can create Iceberg tables\n\nFeatures\n\nTransactional consistency between multiple applications where files can be added, removed or modified atomically, with full read isolation and multiple concurrent writes\nFull schema evolution to track changes to a table over time\nTime travel to query historical data and verify changes between updates\nPartition layout and evolution enabling updates to partition schemes as queries and data volumes change without relying on hidden partitions or physical directories\nRollback to prior versions to quickly correct issues and return tables to a known good state\nAdvanced planning and filtering capabilities for high performance on large data volumes\nThe full history is maintained within the Iceberg table format and without storage system dependencies\n\nComponents\n\nIceberg Catalog - Used to map table names to locations and must be able to support atomic operations to update referenced pointers if needed.\nMetadata Layer (with metadata files, manifest lists, and manifest files) - Stores instead all the enriching information about the constituent files for every different snapshot/transaction\n\ne.g.¬†table schema, configurations for the partitioning, etc.\n\nData Layer - Associated with the raw data files\n\nSupports common industry-standard file formats, including Parquet, ORC and Avro\nSupported by major data lake engines including Dremio, Spark, Hive and Presto\nQueries on tables that do not use or save file-level metadata (e.g., Hive) typically involve costly list and scan operations\nAny application that can deal with parquet files can use Iceberg tables and its API in order to query more efficiently\nComparison",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html",
    "href": "qmd/docker-aws.html",
    "title": "AWS",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-misc",
    "href": "qmd/docker-aws.html#sec-docker-aws-misc",
    "title": "AWS",
    "section": "",
    "text": "Notes from Linkedin Learning Docker on AWS\n\nThe example used in this class is for a web server",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-summ",
    "href": "qmd/docker-aws.html#sec-docker-aws-summ",
    "title": "AWS",
    "section": "Summary",
    "text": "Summary\n\nPush docker image to ECR\nPush app code and build instructions (buildspec yaml) to CodeCommit\nCreate CodeBuild project that executes image building instructions\nCreate a Pipeline that triggers CodeBuild (automatic image build when new code is committed)\nChoose a Cluster method (fargate or manual EC2), then create and start cluster instances\n\nonly able to specify number of instances to create with the EC2 method\nfargate handles most of the configuration (cost extra?)\n\nCreate a task or a service\n\nA task is for short running jobs, no load balancer or autoscaling. Its definition details the container configuration; how much of the resources you want your workloads (e.g.¬†app) to be able to use; communication between containers, etc.\nA service is for long running jobs. Creates tasks and autoscales number of instances and load balances traffic¬†\n\nAdd a storage container and update task definition to include a shared volume",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-gloss",
    "href": "qmd/docker-aws.html#sec-docker-aws-gloss",
    "title": "AWS",
    "section": "Glossary of AWS Services Used",
    "text": "Glossary of AWS Services Used\n\nECR stores your images that you build\nCodeCommit (CC) is like a github (code storage)\nCodeBuild sets up the process of using a yaml file in your CC repo as instructions to build the images\nPipeline is the CI/CD part. Triggers an image build every time there‚Äôs a new push to CodeCommit¬†\nRoute 53 takes your domain name (www.store.com/app), creates a DNS ip address and reroutes traffic from that domain to your load balancer.\nContainer Networking Models\n\nHost - direct mapping to host networking (EC2)\n\nuse when performance is prime concern\nonly 1 container per task per port\n\nAwsvpc - ENI per task, required for fargate\n\nmost flexibility\nrecommended for most use cases\n\nBridge - ‚ÄúClassic‚Äù docker networking\n\ndidn‚Äôt get discussed\n\nNone - multi-container localhost and storage\n\nonly local connectivity (i.e.¬†communication between containers within a task)\n\nSecurtiy groups available for Host and AWSvpc\n\nsecurity groups allow for tracking container performance and limiting access",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-meths",
    "href": "qmd/docker-aws.html#sec-docker-aws-meths",
    "title": "AWS",
    "section": "Two Methods For Running Containers on AWS",
    "text": "Two Methods For Running Containers on AWS\n\nManaging the EC2 instances yourself (see section below for set-up instructions)\n\nif you understand the capacity you need and want greater control, this might be better\nYou pay for unused capacity\nBilling is like the standard billing for using an EC2 instance¬†\n\nFargate (see section below for set-up instructions)\n\nManaged by Amazon, less control, less to deal with\nYou don‚Äôt have to deal with starting, stopping, choosing compute sizes, capacities etc. of EC2 instances\nBilled by CPU/Memory and Time that‚Äôs used by your container",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ecr",
    "href": "qmd/docker-aws.html#sec-docker-aws-ecr",
    "title": "AWS",
    "section": "Elastic Container Registry (ECR)",
    "text": "Elastic Container Registry (ECR)\n\nCreate an ECR account\n\nLog into your account\nsearch ecr\nClick create ‚Äúget started‚Äù under Create a Repository (mid right)\n\nSays you pay for the amount of data you store in the repository and data transferred to the internet.\n\nReason for doing this is latency. The repo is regional and you want your image/app to be close to the host\n\n\nAssign a name\n\nfirst part is a hash + region + amazon.com\nyou add a name. whatever you want\n\nmutable/immutable\n\nIf you‚Äôre going to be storing multiple versions of the same image, you should choose mutable.\n\nClick create repository (bottom right)\n\nPush image to ECR repo\n\ncopy the URI for your repo from the ecr console (ecr ‚Äì left panel ‚Äì repositories ‚Äì images)\n\nsave it to registry-tag.txt file in your local image directory\nAlso include it as the tag to your docker image\n\ndocker build . -t &lt;URI&gt;\n\nauto-appends ‚Äú:latest‚Äù\n\n\n\nIn terminal\n\n(aws ecr get-login --no-include-email --region &lt;region&gt;}\n\n*with parentheses\nregion is whatever you have in your profile e.g.¬†us-east-2\ngets login from the aws profile you‚Äôve already set-up\nprints some kind of warning, he didn‚Äôt act like it was meaningful\n\ndocker push &lt;tag&gt;\n\nwhich is your URI:\n\nin the example, the version is ‚Äúlatest‚Äù\n\n\nIf something doesn‚Äôt work, the instructions are at aws\n\nIn repository console\n\nclick repo name\nclick view push commands (top right)\nshows how to connect to repository and ‚Äúpush‚Äù instance\n\n\n\nIn console, hit refresh (mid-right) to see that the image is loaded into the repo",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-codcom",
    "href": "qmd/docker-aws.html#sec-docker-aws-codcom",
    "title": "AWS",
    "section": "CodeCommit",
    "text": "CodeCommit\n\nCreate a CodeCommit git repository - benefit is having (image/app) code live near hosting service, less latency for CI/CD processes\n\nDeveloper tools ‚Äì CodeCommit ‚Äì (maybe left panel ‚Äì Source ‚Äì Repositories)\nClick create repository (top right)\n\nEnter name\n\nDoesn‚Äôt have to match the name of the image repo, but might be worth doing\nalso a box for entering a description\n\nClick create\n\nConnection Steps\n\nhttps or ssh\n\nclick ssh\n\nfollow these directions to gitbash and create SSH keys for windows, enter them into config file, clone repository, etc. etc.\n\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-windows.html\nHe cloned the repo, one directory above the .ssh directory\n\n\n\nPush Container Code to CodeCommit repo\n\ncd to cloned repo directory\ncopy code files to that directory\ngit add *, git commit -m ‚Äúblah blah‚Äù, git push -u origin master\n\n-u is for upstream\n‚Äú-u origin master‚Äù¬† necessary for a first push\n\nFiles should be present in developer tools ‚Äì CodeCommit ‚Äì Source ‚Äì Repositories ‚Äì repo",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-codcomproj",
    "href": "qmd/docker-aws.html#sec-docker-aws-codcomproj",
    "title": "AWS",
    "section": "Create a CodeBuild Project",
    "text": "Create a CodeBuild Project\n\nBuild the CodeCommit repo into a docker container\nbuildspec.yml (see hostname folder in exercise files)\n\nyaml script that automates building docker, logging into ECR, building an image, and pushing it to ECR\ncodebuild version used was 2.0 (which is at the top of the yaml script)\n\ncodebuild must be some aws tool you can use to do this\n\nadd, commit, push to CodeCommit repo\n\ndeveloper tools ‚Äî codecommit ‚Äì left panel ‚Äì build ‚Äì build projects\n\nclick create build project (upper right)\n\nanything not listed below, just used defaults\n\nenter project name\n\nhe gave same name as CC repo\n\nUnder Source, make sure it says CodeCommit, enter repo name in box\nMake sure Manage Image box is ticked\nOperating System\n\nhe used Ubuntu\n\nRuntime\n\nselect Standard\n\nImage\n\nstandard 2.0\n\nPriviledged\n\ntick box ‚ÄúEnable this flag if you want to build Docker images or want your builds to have elevated priviledges‚Äù\n\nLogs\n\nUsing cloudwatch\n\ngroup name - codebuild\nStream Name\n\nhe used the name of the CC repo\n\n\n\nClick Create build project (bottom right)\n\nGoto IAM console ‚Äì left panel ‚Äì Roles\n\nWhen the ‚Äúbuild project‚Äù was created a role was also created\n\nUnder Role name -¬† click ‚Äúcodebuild--service-role‚Äù\nclick attach policy (mid left)\nSearch for AmazonEC2ContainerRegistryPowerUser\ntick box to select it\nclick attach policy (bottom right)\n\n\nGoto Developer tools ‚Äì CodeBuild ‚Äì left panel ‚Äì Build ‚Äì Build Project ‚Äì project name\n\nClick Start Build (top right)\nkeep all defaults, Click Start Build (bottom right)\n\nProject builds and under Build Status, status should say ‚Äúsucceeded‚Äù when it finishes\n\nWhich means there are now two images in the ECR repo\n\noriginal push and image built from this project build process (duplicate)\n\n\nAutomate building container when new code is pushed (CI/CD)\n\ndeveloper tools ‚Äì codebuild ‚Äì left panel ‚Äì pipeline ‚Äì pipelines\nclick create pipeline\n\nenter pipeline name\n\nhe named it the CC repo name, hostname\nclick next\n\nAdd source stage\n\nchoices\n\nCodeCommit\nECR\nS3\nGithub\nChoose codecommit\n\nSelect repo name\n\nexample: ‚Äúhostname‚Äù\n\nSelect branch\n\nexample ‚Äúmaster‚Äù\n\nDetection option\n\nselect CloudWatch\n\nClick next\n\nAdd build stage\n\nCodeBuild or Jenkins\n\nchoose CodeBuild\n\nRegion\n\nexample US East - (Ohio)\n\nProject Name\n\nname of the build project from last section\nexample hostname\n\nClick next\n\nAdd deploy stage\n\nskipped, because something I didn‚Äôt understand. Sound like another level of automation that might be used in the future\nclick skip deploy stage\n\nReview\n\nClick create pipeline (bottom right)\n\n\nOnce created, it will start building the pipeline from the CodeCommit source\n\nProcess takes a few minutes\ndetects the buildspec.yml in CC and executes it\nunder Build Section, there will be a details link, you can right-click and open it in a new tab\n\nShould result in a 3rd image (duplicate images) in the ECR repo\n\nSo anytime a new commit is pushed to CodeCommit, an image will be built and stored in ECR",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ec2user",
    "href": "qmd/docker-aws.html#sec-docker-aws-ec2user",
    "title": "AWS",
    "section": "Create Cluster: EC2 (User-Managed)",
    "text": "Create Cluster: EC2 (User-Managed)\n\nCreate Cluster: Set-up instructions for running containers using EC2 method\n\nSearch for ECS\nleft panel ‚Äì Under Amazon ECS: Clusters\n\nClick create cluster\nChoose Linux + Networking\n\nWindows + Networking and Networking-only (Fargate see below) options also available\nclick next (bottom right)\n\nConfigure Cluster\n\nEnter Cluster name\n\nexample: ecs-ec2\n\nProvisioning\n\nOn demand instance\nspot instance\n\nEC2 instance type (size of compute)\n\nexample: t2.medium\n\nNumber of instances\n\nhe chose 1\n\nEC2 AMI id\n\nLinux-1, linux-2\n\nhe chose linux-2; didn‚Äôt give a reason\n\n\nDefaults kept for Virtual Private Cloud (VPC), Security Group, storage, etc.\nCloudWatch container insights\n\ntick enable container insights\nso you can monitor stats in Cloudwatch and help you tune compute resources in the future\n\nClick create\n\ntakes a minute or two to spin up the instance",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ec2ssh",
    "href": "qmd/docker-aws.html#sec-docker-aws-ec2ssh",
    "title": "AWS",
    "section": "Create Cluster: EC2 with SSH Access",
    "text": "Create Cluster: EC2 with SSH Access\n\nCreate Cluster using EC2 method with SSH access (not possible with a Fargate cluster) and connect to it\nSteps\n\nFind your ssh public key\n\ngo into git bash and type ‚Äúcat ~/.ssh/id_ed25519.pub‚Äù\n\noptions\n\nid_rsa.pub\nid_ecdsa.pub\nid_ed25519.pub\n\nI have 2, rsa that I created when linking rstudio to github and ed25519 when I created gitlab acct\n\nCopy everything (including the ssh-filename beginning part) all the way until your email (don‚Äôt include)\n\nGoto EC2 services page (open new tab)\n\nUnder Resources (mid), click Key Pairs\n\nClick import\npaste key into Public Key Contents box\nenter a name\n\nexample ecs-ec2-key\n\nclick import\n\n\nGo back to the ECS services page and create another cluster\n\nSame as before. (create cluster - EC2 method above) except:\n\ncluster name - ecs-ec2-ssh\nkey pair - chose newly imported key pair\nNetworking\n\nvpc\n\ndrop down\n\nchoose vpc created by prev. cluster (some big long hash)\n\n\nsubnets\n\ndropdown\n\nchoose subnet created by prev. cluster\nspawns another dropdown to add another subnet\n\ndropdown\n\nchoose second subnet created by prev.cluster\n\nShould only be 2, they‚Äôre names should gray-out after you choose them\n\nsecurity group\n\nchoose the one created by the prev. cluster\n\n\nHaving SSH available will allow us to go into the container and view docker ressource\n\n\nCopy public ip address and open SSH port (also see AWS notebook ‚Äì EC2 ‚Äì connect/terminate instance)\n\nClick on Cluster name\nclick on ECS instances tab (mid left)\nright-click EC2 Instance id and open in new tab\n\ncopy IPv4 Public IP (lower right)\nclick security group link (lower left)\n\nclick inbound tab (lower left)\nclick edit\n\nclick add rule\nunder Type, click dropdown and select SSH\n\nautomatically chooses port 22\n\nSource\n\nkept 0.0.0.0¬† (‚Äúv4 address‚Äù, guess 4 is for the 4 numbers in the address)\n\nDescription\n\nkept default\n\nclick save\n\n\n\n\nOpen terminal\n\nssh -i ~/.ssh/id_rsa ec2-user@\n\nasks if you‚Äôre sure, say yes\n\nCheck container status on instance\n\nsudu su -\n\nswitches to being a root user\n\ndocker ps\n\nshows container id and name, image, status, etc.\n\n\nexec into container\n\ndocker exec -it  sh\n\nonly works if linux image has a shell environment\ninstead of sh, can try bash\nor docker run¬† ‚Äìrm ‚Äìname linux -it alpine:latest sh\nctrl + d\n\nleave shell\n\nexit\n\nto exit as root user\n\nexit\n\nleaves instance, closes connection",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-farg",
    "href": "qmd/docker-aws.html#sec-docker-aws-farg",
    "title": "AWS",
    "section": "Create Cluster: Fargate (AWS-Managed)",
    "text": "Create Cluster: Fargate (AWS-Managed)\n\nSet-up instructions for running containers using Fargate method\n\nSearch for ECS\nleft panel ‚Äì Under Amazon ECS: Clusters\n\nClick create cluster\n\nChoose Network-only (amazon fargate)\nEnter Cluster name\n\nexample ecs-fargate\n\ntick box for Enable Container insights (cloudwatch)\nclick create (bottom right)\n\n\nCluster created instantaneously\n\nclick view cluster",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-catd",
    "href": "qmd/docker-aws.html#sec-docker-aws-catd",
    "title": "AWS",
    "section": "Creating a Task Defintion",
    "text": "Creating a Task Defintion\n\nA task definition is a blueprint for your tasks, specifying what container image to use, how much CPU and memory is needed, and other configurations.\nIf using load balancer goto the ‚Äúcreate appplication load balancer‚Äù and¬† ‚Äúadd ecs service and task‚Äù below\ndetails the images to use, the CPU and memory to allocate, environment variables, ports to expose, and how the containers interact.\n\nassociates the cluster created in the previous section with the app or workload\n\n1 task definition can be used in multiple containers\nsearch for ECS\nleft panel ‚Äì Clusters ‚Äì task definitions\n\nclick create new task definition\nselect cluster method (Fargate or EC2)\n\nchoose fargate\nclick next step\n\ncreate task-definition name\n\neg hostname-fargate\n\ntask role\n\nused if workload creates other resources inside aws\nleave blank\nsome kind of warning about the network settings, he ignored it.\n\ntask execution iam role\n\ngives permission for cluster to use task defintion\nkeep default\n\ntask size\n\nmemory size choice effects available choices for cpu\ndepends on your application needs, if running multiple containers with this definition, etc.\n\nhe chose the smallest for each just because this is for illustrative purposes\n\n\ncontainer definitions\n\nassigns which containers will be using this definition\nclick add container\n\ncontainer name\n\nwhatever you want, he chose hostname\n\nimage\n\ngoto services (top left) (open new tab) ‚Äì left panel ‚Äì ecr ‚Äì left panel ‚Äì repositories\n\nclick image repo name\ncopy image uri that you want to associate with the definition\ngo back to the task definitions tab\n\npaste uri into the box\n\nIf you want to always use the latest image and your uri has build version tag, replace the build tag with ‚Äúlatest‚Äù\n\nbuild tag starts at ‚Äúbuild‚Äù and goes to the end of the uri\n\n\n\nauthentication only necessary if ecr repo is private\nsoft memory limit\n\nspecify a memory limit for the container\nleaving blank says only limit will be the memory size of the task definition (see 6.)\n\nport mappings\n\nwhatever is specified in dockerfile/image\nexample nginx image exposes 80 tcp\n\nAdvanced options\n\nhealthcheck\n\nsome cli code that allows you to check if your container is running properly at the container level\nhe already has this in his buildspec.yml code (see create CodeBuild project section)\n\nhealthcheckz is a check at the system level\n\n\nEnvironment, network settings, volumes\n\nseems like a bunch of stuff that would be used in a docker run command or in a docker-compose file\nall left blank\n\n\nclick add\n\n\nvolumes\n\nexternal volume to be shared\nleft blank\n\nclick create\n\nTakes you to launch status\n\nclick view task definition",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-utdtsdvbc",
    "href": "qmd/docker-aws.html#sec-docker-aws-utdtsdvbc",
    "title": "AWS",
    "section": "Update Task Definition to Share Data Volumes Between Containers",
    "text": "Update Task Definition to Share Data Volumes Between Containers\n\nFargate cluster example\n\nAlso see the Data Volumes, Sharing Data between containers, Docker-Compose sections of part 1 of this note\n\nthink a lot of what happens in those sections is automated by using the this task definition\n\nSearch ECS ‚Äì Left panel ‚Äì Task Definitions\n\nclick on fargate task definition\n\nclick on latest revision of the definition\n\ndefinitions are versioned\nclick on create new revision\n\nscroll down to click on add container\n\ncontainer name\n\nwhatever, fargate-storage (he called his hostname-v2)\n\nimage\n\nadd image uri (see creating task definition above)\nhe used the same image as the first container. This becomes a problem because he has two containers using the same port since both nginx containers are using 80. See troubleshooting section below. Also mentioned in part 1 ‚Äì running containers ‚Äì flags ‚Äì p\nThink for data science we‚Äôd use a postgressql, redis, etc. image\n\nEnvironment\n\nThink this was for display purposes. He added one just so when he went to the webpage and it displayed the container names, we could tell the difference. The first container said version 1 and this one says version two.\nenvironment variables\n\nkey\n\nexample VERSION\n\nvalue\n\nexample versionTwo\n\n\n\nclick add\n\nVolumes\n\nclick add volume\n\nname\n\nwhatever\nexample shared-volume\n\nclick add\n\n\nGO BACK to container section\n\nDo this for each container: click on the container name\n\nStorage and Logging\n\nmount points\n\nsource volume\n\nclick dropdown and select volume name\n\nexample from above: shared-volume\n\n\ncontainer path\n\nhe added the shared folder path and it was the same for both containers\nsee part 1 Data Volumes and Sharing Data between containers sections\n\nI think using that example, we‚Äôd specify¬†‚Äú/app/public/‚Äù (no quotes) for the app container. *** This dude said to add a trailing ‚Äú/‚Äù to the paths ***\nfor storage container, example redis, it‚Äôd be ‚Äú/data/‚Äù which is designated by the redis image authors.\n\n\n\n\nclick update\n\n\nClick create\n\nNote the revision number that‚Äôs given\n\n\n\nleft panel ‚Äì Clusters\n\nclick on fargate cluster\n\nclick services tab (mid left)\n\nclick service name (example is hostname) using the task definition\n\nThis was created in the Add ECS service and task section below\nclick update button (top right)\n\nConfigure Service\n\nTask Definition\n\nrevision\n\nselect revision number of the updated definition\n\n\nclick next\n\nclick next all the way to review\n\nclick update service\n\nclick view service\n\n\n\nclick tasks tab (mid left)\n\nrefresh (mid right) and watch new task start up with ‚Äúprovisioning‚Äù status and then ‚Äúrunning‚Äù\n\n\n\ncan go to ip address with volume path appended to the address to see that the volumes are up and running\n\nnot sure if this would work with a database example or not",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ratwaad",
    "href": "qmd/docker-aws.html#sec-docker-aws-ratwaad",
    "title": "AWS",
    "section": "Running a Task with an Available Definition",
    "text": "Running a Task with an Available Definition\n\nleft panel ‚Äì Clusters\n\nClick Cluster your using for the task definition\n\nhe used the fargate one he created\n\nclick tasks tab (mid left)\nclick run new task\n\ntick fargate launch type\ncluster vpc,¬†subnets\n\nclick dropdown boxes\nit‚Äôll show the ones that were made during cluster creation\n\nchoose vpc and both subnets\n\n\nSecurity group\n\ncreates one for you with default rules which you can keep\n\nAlso can manipulate after created by going to EC2 ‚Äì left panel ‚Äì Network and Security ‚Äì Security Groups\n\nOr click edit button to specify ports, choose existing security group, etc\n\nadd additional port\n\ntype\n\nselect custom with tcp protocol\n\nport range\n\n81-90\ncontainer must be configured to be able to listen on the range of ports\n\nSource\n\ncan choose a group that allows you to connect with other tasks in the environment and limit access\nhe kept Anywhere\n\nclick save\n\n\n\nclick run task (bottom right)\n\nclick the task hash under Task column\n\nat the bottom, you can watch the status turn from ‚Äúpending‚Äù to ‚Äúrunning‚Äù\n\nrefresh button (right)\n\ncopy the public ip under Network section\n\npaste into address bar + port\n\nexample 3.15.13.43:80\nexample: for his nginx server, it just displayed the private ip and the image name\n\n\nSimple way for a minor scale up the access to the application is to duplicate the task definition (also see autoscaling section below)\n\nleft panel ‚Äì Clusters\n\nselect the same cluster\nclick tasks tab again\nclick task hash id again\n\nclick ‚Äúrun more like this‚Äù (top right)\n\ntick fargate again\nselect the same vpc and subnets again\nclick run task\n\nget the public ip the same way as before\nnow two ips are available for the app",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-calb",
    "href": "qmd/docker-aws.html#sec-docker-aws-calb",
    "title": "AWS",
    "section": "Create Application Load Balancer (ALB)",
    "text": "Create Application Load Balancer (ALB)\n\nAlso see AWS &gt;&gt; EC2 &gt;&gt; Configure Load Balancer and Application Ports\nsearch ec2\nleft panel ‚Äì load balancing ‚Äî load balancers\n\nclick create load balancer (top left)\n\nConfigure Load Balancer\n\nselect type\n\napplication, network or classic\n\napplication is for http, https\n\nguess this is for internet traffic coming into (and out of?) application\n\nnetwork is for tcp, tls, udp\n\nguess this would be for communication between containers\n\nclassic is for http, https, and tcp\n\nsomething about an app running on an ec2 classic network\n\nhe chose application\n\n\ngive it a name\n\nexample ecs-alb\n\nip address type\n\nipv4 (default)\n\nscheme\n\ninternal or internet facing\n\nkept internet-facing (default)\n\n\nListeners\n\nhttp, port 80\ncan add other ports if you want\n\nfor production should add a https, 80\n\n\nAvailability zones\n\nvpc, subnets\n\nselect those asscociated with the cluster\nsubnets have region specification (us-east-2a, b)\n\n\nclick next: configure security settings (bottom right)\n\nif you haven‚Äôt add https port, it‚Äôll give you a warning\n\nclick next if don‚Äôt care about https\n\n\n\nConfigure Security Settings\n\ntick box that has the name of the security group that was created during the cluster creation\n\nExample: EC2ContainerServic-ecs-ec2-EcsSecurityGroup-somehash\n\ndescription: ECS Allowed Ports\n\n\nclick Next\n\nConfigure Routing\n\ntarget group (backend of load balancer)\n\nnew target group (default)\n\nName\n\n(literally) ‚Äúdefault‚Äù\n\ntarget type\n\nInstance, IP, Lambda\nchose IP\n\nsomething about being able to use on EC2 and Fargate\n\n\nprotocol\n\nkept http\n\nport\n\nkept 80\n\nhealth check\n\nkept defaults\n\nclick next\n\nRegister targets\n\nkeep defaults\ngoing to specify this info through ecs in the next section\nclick next to review\n\nReview\n\nclick create\n\n\n\nEdit the default forwarding target\n\nA listener rule is comprised of a target (or group of targets) and conditions. When the load balancer receives a request, it checks it against the conditions in the listener rules. For whichever condition the request meets, the load balancer then sends the request to the target (e.g.¬†ip address(instance) or lambda function (code scripts)) associated with that condition.\nSearch ec2 ‚Äì left panel ‚Äì load balancing ‚Äì load balancer¬†\n\ntick the load balancer you want\nclick listener tab (mid left)\nFor listener id = http 80 and under the Rules column it will say Default: forwarding to default\n\nclick view/edit rules\n\nunder the IF column (ie the condition) it says, Requests otherwise not routed. Which means any request that doesn‚Äôt meet any of the other conditions\nclick the edit pencil icon (top left) ‚Äì click pencil icon next to http 80: default action\n\nUnder the THEN column ‚Äì click the trash can to delete ‚Äúforward to default‚Äù\nclick add action\nselect return fixed response\n\nkeep response code 503\n\nmeans server had an issue responding\n\nin Response body, type message\n\nexample: sorry, no one is home right now\nclick check mark\nclick update (top right)\n\n\n\nclick back arrow (top left)\n\n\ntest it out by clicking description tab (mid left)\n\nGet DNS (Domain Name Service) name\n\nExample blahblahamazonaws.com\nNormally, you take your domain name (www.store.com/app) and give it the Amazon Route 53 service which translates your domain name into an ip address.\nYou then reroute traffic from your domain ip address to this dns name.\n\npaste it in the browser and the error message displays",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-cesat",
    "href": "qmd/docker-aws.html#sec-docker-aws-cesat",
    "title": "AWS",
    "section": "Create ECS Service and Task",
    "text": "Create ECS Service and Task\n\nAlso see create task definition and run task sections above\nSteps\n\nsearch ecs ‚Äì left panel ‚Äì clusters ‚Äì click fargate cluster\nclick tasks tab (mid left) ‚Äì select task that was created in Create task section ‚Äì click stop button (mid left)\nclick services tab (mid left) ‚Äì click create service\n\nConfigure Service\n\nConfigure Service\n\nselect launch type\n\nchoose fargate\n\nTask Definiton and Cluster\n\nkept the ones created in sections above\n\nenter a service name\n\nexample hostname\n\nnumber of tasks\n\nexample 2\n\nclick next step\n\nDeployments\n\nkeep default, rolling update\n\nallows you to upgrade the task definition from version 1 to version 2 in a rolling fashion¬†\n\ndon‚Äôt know what he‚Äôs talking about here with versions\n\n\n\nclick next step\n\nConfigure Network\n\nService\n\ncluster vpc, subnets\n\nselect the ones that are associated with this fargate cluster\n\n¬†security group\n\nkeep default (allows traffic in)\n\nauto-assign public ip\n\nkeep default ENABLED\nwith a load balancer, we could choose to use only used private ips though\n\n\nHealth check grace period\n\nset to 60 (in seconds)\ngives the container/cluster a chance to get up an running before it tests it to see if everything is working\n\nLoad Balancing\n\ntick application load balancer\ncontainer to load balancer\n\nshows container name port:port\nclick add to load balancer\n\nproduction listener port\n\nclick dropdown ‚Äì select 80 HTTP\n\n80 is our port of the container and we chose http when we created the load balancer\n\n\npath pattern\n\nit was /hostname but he changed it to /* which is every pattern\n\nI think /hostname would that ‚Äú/hostname‚Äù would be ip address pattern associated with this container (i.e.¬†the condition or rule)\nand /* means route any request from  no matter what pattern is attached to it.\n\n\nevaluation order\n\nas soon as the first rule/condition is matched, traffic goes to that target and no other rules are considered. Lower the evaluation order, the sooner the rule is considered\nhe chose 1\n\nhealth check path\n\ndefault was /hostname\nsince he‚Äôs using /*, he changed it to /hostname/ so the healthcheck will get a webpage (code 200) and not a redirect (code 300 error)\n\nService Discovery\n\nEnables Route 53 to create a local network dns address for your container.\nUseful for when you have multiple applications talking to each other\nuntick box for enable service discovery integation since this is only one application\n\n\n\n\nclick next step\n\nSet Autoscaling\n\nSee next section on adding autoscaling\nThis can be added after this service has been created by updating\nclick next step\n\nReview\n\nclick create service\n\nCreates target group, rule/condition\n\nclick view service\nshould see two tasks starting up\ngoto the dns address on a webpage (see end of create load balancer section)\n\nbefore it displayed the error message (default target), now it shows a webpage (like in the Create Task section above)\nrefresh and it shows the second dns address associated with having a second task\n\n2 tasks means it can handle more traffic (just like the end of the create task section above)",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-utstaa",
    "href": "qmd/docker-aws.html#sec-docker-aws-utstaa",
    "title": "AWS",
    "section": "Updating the Service to Add Autoscaling",
    "text": "Updating the Service to Add Autoscaling\n\nsearch ecs ‚Äì left panel ‚Äì clusters\nClick on your cluster that you want to update its service\n\nservices tab (mid left) ‚Äì click service name or id\n\nclick update\n\nClick next until you get to Set Autoscaling\ntick configure service autoscaling\nminimum number of tasks\n\nhe chose 1\n\ndesired number of tasks\n\nhe chose 3\n\nmaximum number of tasks\n\nhe chose 5\n\nIAM role\n\nuse default ecsautoscalerole\nuse create new role if there isn‚Äôt already one available\n\nclick Add scaling policy\n\ntick step scaling\nenter policy name\n\nexample stepUp\n\nexecute policy when\n\ntick create new alarm\nalarm name\n\nexample upAlarm\n\nECS service metric\n\nCPU Utilization\n\nAlarm threshold\n\navg cpu utilization &gt; 10 (%)\nconsecutive period = 1\nperiod = 8 min\n\nhe chose 1 just for illustrative purposes\n\nclick save\n\n\nscaling action\n\nadd 1 task\nwhen cpu utilization &gt; 10\n\ncountdown period\n\namount of time it takes to make a decision\n30 sec\n\nclick save\n\nclick Add scaling policy (again)\n\nsame thing but for scaling down\navg cpu utilization\n\nhe chose &lt;= 10 but I‚Äôm not sure if that‚Äôs what you‚Äôd do in real life. I‚Äôd think you‚Äôd want some separation between the up and down scaling, but maybe not\n\nscaling action\n\nremove 1 task\n\n\nClick next to review\nclick update service\n\n\nClick tasks tab (mid left) to see how many task are currently running\nclick autoscaling tab to see the both upAlarm and downAlarm condition info\nTo see status of the targets (ip addresses of instances/containers)\n\nGoto EC2 ‚Äì left panel ‚Äì load balancing ‚Äì target groups\n\ntick the target group name of the load balancer\nUnder Registered Targets (Bottom)\n\nshows ips, status\n\nstatus == draining when auto-scaling taking a resource offline",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-troub",
    "href": "qmd/docker-aws.html#sec-docker-aws-troub",
    "title": "AWS",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nNotes:\n\nnormal for an active cluster without any running services or tasks to have 1 active container instance. It‚Äôs called the container management instance.\n\nExample: you notice your cluster is running 5 containers when you only desire 3\n\nCan see this in Clusters ‚Äì fargate ‚Äì services tab, under the desired tasks and running tasks columns\n\nits says 5 for desired but he chose that for his max in the autoscaling section, so I don‚Äôt know if he adjusted it for demonstration purposes or if this something confusing that AWS does.\n\nAnswer: he had both containers trying to bind to port 80 (see logs below)\nService level\n\nclick service name\n\ntasks tab\n\ncan see the task ids and definitions that the various active tasks are using\n\nexample: tasks are alternating between running and provisioning. Why are some shutting down and others starting in their place? The container is running for some time and then being stopped for some reason.\n\nclick task id\n\nlogs tab ‚Äì select container\n\nshows errors that have occurred\n\nDetails tab\n\nContainers (bottom)\n\nclick expand-arrow on desired container\n\nclick view logs in CloudWatch\n\ntakes you to CloudWatch console\n\nview the logs of the task\n\nable to filter log by events\n\ngo up one level to see log streams\n\ncan match containers and tasks to see if it might be a task issue\nend hash is the task id\n\n\n\n\n\n\n\n\nDetails tab\n\nLoad Balancing ‚Äì click target group name\n\ntargets tab\n\nshows the individual targets (ip addresses), ports, statuses\nstatus by region (if you have resources in different zones)\n\nexample: there were 2 zones -¬† us.east.2a and 2b and one had all healthy and the other had zero healthy, but he didn‚Äôt mention anything about it. Think that‚Äôs just how nodes are taken on and offline and not that there‚Äôs a regional issue.\n\n\nhealth checks tab\n\nhealthy threshold\n\nnumber of code 200s i.e.¬†healthy responses required in order for a node to be considered healthy\n\nunhealthy threshold\n\nnumber of code 300s i.e.¬†error responses required in order for the node to be considered unhealthy\n\n\n\n\nLogs tab\n\nshows the aggregate of the logs for each container (all tasks included)\nselect a container from the dropdown\n\ntimestamp, message, task id\nexample: shows a ‚Äúbind‚Äù error that says the container can‚Äôt a bind to port 80 because it‚Äôs already in use.¬† In the¬†Update task definition to share data volumes section, the second container he added was a duplicate of the nginx container and both were trying to bind to port 80. Hence the error\n\n\n\n\nCluster metrics\n\nClusters ‚Äì EC2 cluster ‚Äì metrics tab\n\nonly useful for EC2 clusters\ncompute and memory resources being used\n\nshows time series of min, max, and average percent usage\n\n\nClusters ‚Äì fargate cluster ‚Äì services tab\n\nclick service name ‚Äì metrics tab\n\nsame stuff as EC2 metrics tab\ncan click on the different metrics (cpu, memory utilization) and create custom metric functions, change period length, etc.\n\nleft panel has alarms (autoscaling trigger history), events, logs, and settings",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html",
    "href": "qmd/geospatial-processing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-misc",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-misc",
    "title": "Preprocessing",
    "section": "",
    "text": "Beware statistical computations of tibbles/sf_tibbles with geometry columns\n\nCould result in an expensive union operation over identical geometries and an R session crash\n\nExample with 100K rows crashed R.\n\nNotes from thread\nOption 1 (slower): Set do_union = FALSE in summarize\ntx_income_groups &lt;- \n  get_acs(\n    geography = \"tract\",\n    table = \"B19001\",\n    state = \"TX\",\n    year = 2020,\n    geometry = TRUE\n  ) |&gt; \n  filter(variable != \"B19001_001\") |&gt; \n  mutate(bracket = case_when(\n    variable &gt; \"B19001_012\" ~ \"Above $100k\",\n    TRUE ~ \"Below $100k\"\n  )) |&gt; \n  group_by(GEOID, bracket) |&gt; \n  summarize(n_households = sum(estimate, na.rm = TRUE),\n            do_union = FALSE)\nOption 2 (faster): Perform calculation without geometries then join\ntx_tracts &lt;- tracts(\"TX\", cb = TRUE, year = 2020) |&gt; \n  select(GEOID)\n\ntx_income_groups &lt;- \n  get_acs(\n    geography = \"tract\",\n    table = \"B19001\",\n    state = \"TX\",\n    year = 2020,\n    geometry = TRUE\n  ) |&gt; \n  filter(variable != \"B19001_001\") |&gt; \n  mutate(bracket = case_when(\n    variable &gt; \"B19001_012\" ~ \"Above $100k\",\n    TRUE ~ \"Below $100k\"\n  )) |&gt; \n  group_by(GEOID, bracket) |&gt; \n  summarize(n_households = sum(estimate, na.rm = TRUE))\n\ntx_income_groups &lt;- tx_tracts |&gt; \n  left_join(tx_income_groups, by = \"GEOID\")\n\n{tidycensus} has an arg to bypass d/ling the geometries, geometry = FALSE and a separate tracts function to get the census tract geometries",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-filtyp",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-filtyp",
    "title": "Preprocessing",
    "section": "File Types",
    "text": "File Types\n\nPMTiles - A single-file archive format for tiled data. A PMTiles archive can be hosted on a commodity storage platform such as S3, and enables low-cost, zero-maintenance map applications that are ‚Äúserverless‚Äù - free of a custom tile backend or third party provider. (Docs)\n\nRun your interactive, smooth-zooming vector map from any storage like S3 that supports http requests; a Caddy server running on your Wi-Fi router, or even GitHub pages (if tiles &lt; 1GB).\nCloudflare R2 is the recommended storage platform for PMTiles because it does not have bandwidth fees, only per-request fees: see R2 Pricing.\n\nShape Files\n\nD/L and Load a shapefile\nMay need API key from Census Bureau (see {tigris} docs)\nExample: Counties in California\ntbl &lt;- tigris::counties(state = \"CA\") %&gt;%\n¬† ¬† st_set_crs(4326)\n{tigris} - US data\nlibrary(tigris)\n\nus_states &lt;- states(resolution = \"20m\", year = 2022, cb = TRUE)\n\nlower_48 &lt;- us_states %&gt;%\n¬† filter(!(NAME %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")))\n{rnaturalearth} - World data\n# Via URL\n# Medium scale data, 1:50m Admin 0 - Countries\n# Download from https://www.naturalearthdata.com/downloads/50m-cultural-vectors/\nworld_map &lt;- read_sf(\"ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp\") %&gt;%\n¬† filter(iso_a3 != \"ATA\")¬† # Remove Antarctica\n\n# Via Package\nlibrary(rnaturalearth)\n\n# rerturnclass = \"sf\" makes it so the resulting dataframe has the special\n# sf-enabled geometry column\nworld_map &lt;- ne_countries(scale = 50, returnclass = \"sf\") %&gt;%\n¬† filter(iso_a3 != \"ATA\")¬† # Remove Antarctica\n\nGeoJSON\n\nWrite data to geojson\ndata %&gt;%\n¬† ¬† st_write(\"mb_shapes.geojson\")",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-proj",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-proj",
    "title": "Preprocessing",
    "section": "Projections",
    "text": "Projections\n\nWGS 84\n\nGoogle ‚Äúepsg code‚Äù + ‚Äúyour region name‚Äù to find a reasonable projection code to use\n\nStandard projection is 4326 aka WGS84 (required by leaflet)\nTransform shapefile\nmb_shapes &lt;- read_sf(download_folder)\nmb_shapes %&gt;%\n¬† st_transform(4326)\n\n\nTransform latitude and longitude then visualize\nnew_tbl &lt;- old_tbl # contains latitude and longitude variables\n¬† ¬† # convert to simple features object\n¬† ¬† sf::st_as_sf(\n¬† ¬† ¬† ¬† coords = c(\"&lt;longitude_var&gt;\", \"&lt;latitude_var&gt;\"), # order matters\n¬† ¬† ¬† ¬† crs = 4326 # standard crs\n¬† ¬† ) %&gt;%\n¬† ¬† mapviw::mapview()\nWGS 84 projection, which is what Google Maps (and all GPS systems) use\nus_states &lt;- us_states %&gt;% # df with geometries\n¬† sf::st_transform(st_crs(\"EPSG:4326\"))¬† # WGS 84\nNAD83, Albers, Mercator, Robinson\n\nlibrary(patchwork)\n\np1 &lt;- ggplot() +\n¬† geom_sf(data = lower_48, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\n¬† coord_sf(crs = st_crs(\"EPSG:4269\")) +¬† # NAD83\n¬† labs(title = \"NAD83 projection\") +\n¬† theme_void() +\n¬† theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np2 &lt;- ggplot() +\n¬† geom_sf(data = lower_48, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\n¬† coord_sf(crs = st_crs(\"ESRI:102003\")) +¬† # Albers\n¬† labs(title = \"Albers projection\") +\n¬† theme_void() +\n¬† theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np3 &lt;- ggplot() +\n¬† geom_sf(data = world_map, fill = \"#FF4136\", color = \"white\", linewidth = 0.1) +\n¬† coord_sf(crs = st_crs(\"EPSG:3395\")) +¬† # Mercator\n¬† labs(title = \"Mercator projection\") +\n¬† theme_void() +\n¬† theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np4 &lt;- ggplot() +\n¬† geom_sf(data = world_map, fill = \"#FF4136\", color = \"white\", linewidth = 0.1) +\n¬† coord_sf(crs = st_crs(\"ESRI:54030\")) +¬† # Robinson\n¬† labs(title = \"Robinson projection\") +\n¬† theme_void() +\n¬† theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\n(p1 | p2) / (p3 | p4)",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-py",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-py",
    "title": "Preprocessing",
    "section": "Python",
    "text": "Python\n\nExample: Filter Data based on a polygon using latitude and longitude data\n\nGet California‚Äôs polygon\nimport osmnx\nimport geopandas as gpd\n\nplace = \"California, USA\"\ngdf = osmnx.geocode_to_gdf(place)\n# Get the target geometry\ngdf = gdf[[\"geometry\", \"bbox_north\", \"bbox_south\", \"bbox_east\", \"bbox_west\"]]\nFilter data according the polygon geometry\nfrom shapely.geometry import Point\n\n# Convert to a GeoDataFrame with Point geometry\ngeometry = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])]\nearthquake_gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:4326')\n\n# Filter to keep only points within the California bounding box\npoints_within_california = gpd.sjoin(earthquake_gdf, gdf, how='inner', predicate='within')\n\n# Select latitude, longitude etc. columns\ndf = points_within_california[['id', 'Latitude', 'Longitude', 'datetime', 'properties.mag']]\n\nLatitude and longitude are converted to point geometry to match the polygon point geometry\nAn inner join is used on the data and california polygon to get the points that are only in California.",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html",
    "href": "qmd/job-management-leadership.html",
    "title": "29¬† Management / Leadership",
    "section": "",
    "text": "29.1 Misc",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#misc",
    "href": "qmd/job-management-leadership.html#misc",
    "title": "29¬† Management / Leadership",
    "section": "",
    "text": "Try really hard not to send messages outside of work hours\nEmphasize unplugging during vacations\nProvide immediate feedback - positive and negative\nDedicate time to freeform exploration\n\nSometimes the rest of the business doesn‚Äôt know what to ask of your data org. That‚Äôs why you need to give your team time to explore.\nTeam members can come to new and exciting conclusions when they‚Äôre given time to explore the data for fun. They can apply their talents to looking for patterns that no one has requested, and have the space to uncover new discoveries. This freeform exploration can lead to game-changing innovations that no business stakeholder would have imagined were possible.\nHelps keep your most valuable team members engaged and satisfied in their work.\n\nWhen first starting, request documentation\n\nRelevant server locations & descriptions\nLocations of our documentation and dashboards\nA list of tools/software that are available to be used\nA list of relevant stakeholders/gatekeepers that I‚Äôd need to make contact with\n\nRemote Teams\n\nVideo calls too easily become transactional and with little time for the chitchat that builds a proper human relationship. Without those deeper bonds, misunderstandings fester into serious relationship difficulties, and teams can get tangled in situations that would be effectively resolved if everyone were able to talk in person.\nSome organizations may balk at the costs of travel and accommodation for a team assembly like this, but they should think of it as an investment in the team‚Äôs effectiveness. Neglecting these face-to-faces leads to teams getting stuck, heading off in the wrong direction, plagued with conflict, and people losing motivation. Compared to this, saving on airplanes and hotels is a false economy.\nFrequency\n\nGet together for a week every two or three months\nAfter the team has become seasoned they may then decide to reduce the frequency, but I would worry if a team isn‚Äôt having at least two face-to-face meetings a year.\nIf a team is all in the same city, but using a remote-first style to reduce commuting, then they can organize shorter gatherings, and do them more frequently.\n\nSchedule\n\nSet a full day of work, focusing on those tasks that benefit from the low-latency communication that comes from being together. tasks that require lots of input from many people with rapid feedback\nWe should then include what feels like too much time for breaks, informal chatter, and opportunities to step outside the office.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#terms",
    "href": "qmd/job-management-leadership.html#terms",
    "title": "29¬† Management / Leadership",
    "section": "29.2 Terms",
    "text": "29.2 Terms\n\nFocus Time - uninterrupted time, usually refers to a period of time (e.g.¬†2 hrs) where people can work without any distractions\nReport (aka Individual Contributor (IC))- People who report to the manager",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#getting-the-promotion",
    "href": "qmd/job-management-leadership.html#getting-the-promotion",
    "title": "29¬† Management / Leadership",
    "section": "29.3 Getting the Promotion",
    "text": "29.3 Getting the Promotion\n\nYou don‚Äôt get a promotion and THEN start to perform at the next level; you perform at the next level IN ORDER TO get a promotion.\n\nSo when you notice a gap somewhere, even if it doesn‚Äôt necessarily fall into your current role description, don‚Äôt be afraid to bring it up to your manager and discuss whether you can/should take initiative to help plug the gap.\nThe best way to notice gaps is to be a good listener and constantly communicate with your partners & stakeholders about their teams‚Äô work and pain points.\n\nMentor a peer\n\nIf you have new members joining the team, offer to be an onboarding buddy to guide them through their first few weeks.\nbrainstorm with team members when then need help\n\nStep out of your immediate scope\nGet involved in team-level activities\n\nHelp out with things such as sprint planning, quarterly planning, etc.\n\nAllows you to gain knowledge about other team members‚Äô work and other teams‚Äô requests for your team\nGives you some exposure to the manager‚Äôs plan and vision for the team\n\nVolunteering for culture initiatives is a great way to practice thinking about the team as a whole\nTake on projects that help the whole team\n\nproduct design doc for the data product\nSLA agreement with partner teams (?)\nCodify the best practices you use in your own work\n\n\nHave open, timely feedback conversations with your manager\n\nAsk for the leveling guide when you have the initial career development conversation with your manager.\n\nAnd make sure you mention your aspiration to be a manager as soon as possible (don‚Äôt be shy) as well as your aspired timeline that you are working towards.\n\nAsk your manager for candid feedback with regards to their assessment of your readiness to become a manager, and any gaps that they think you need to address.\nIn followup career development check-ins, ask your manager to provide feedback for you against the leveling guide.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#managerial-hats",
    "href": "qmd/job-management-leadership.html#managerial-hats",
    "title": "29¬† Management / Leadership",
    "section": "29.4 Managerial Hats",
    "text": "29.4 Managerial Hats\n\nPeople manager - Learn what makes your direct reports (aka people that you manage) tick, identify their career aspirations, and point out opportunities for progress.\nResource manager - Determine what resources are needed and acquire them. Mostly this means recruiting, hiring, and onboarding, but it also means advocating for money for training and team activities.\nProject manager - Collect and triage projects and project requirements, set timetables and schedules, assigned tasks, and have the final say about when work was ‚Äúdone‚Äù.\nCommunications manager - Make sure the team‚Äôs work was being shared with the rest of the organization, and that everyone on the team knew what was going on outside.\nProcess manager - Help design the team‚Äôs processes to make sure we could identify, allot, do, and communicate work across the team.\nTechnical mentor and coach - A technical expert who reviews code, answers technical questions, and gives work feedback to my team.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#preparation-for-a-managerial-role",
    "href": "qmd/job-management-leadership.html#preparation-for-a-managerial-role",
    "title": "29¬† Management / Leadership",
    "section": "29.5 Preparation for a Managerial Role",
    "text": "29.5 Preparation for a Managerial Role\n\nTake notes on the time needed to do difficult tasks, easy quick-wins, common roadblocks, and their solutions.\n\nThis will help estimate deadlines for new projects\n\nPractice verbal and written communication\nGather information on ‚Äúbig picture‚Äù strategy of your company and that applies to data projects\nTake notes of every data team member‚Äôs strengths and weaknesses\nListen to your colleagues.\n\nComplaints on a day-to-day basis.\nPraise about the workplace in general.\nPay attention to pet projects of your teammates: these are the areas they actively pursue outside their usual work.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#meetings",
    "href": "qmd/job-management-leadership.html#meetings",
    "title": "29¬† Management / Leadership",
    "section": "29.6 Meetings",
    "text": "29.6 Meetings\n\nMeetings are bad when they:\n\nResult in calendar fragmentation.\n\nTry to schedule sometime after a Focus Time\nLimit size and number of meetings\n\n1-1s (1 on 1), team-wide update, or decision-making meetings\nLarge (&gt; 4 ppl) brainstorming meetings don‚Äôt work\n\nBetter to circulate a memo of come-up with options then debate those options during a meeting\n\n\n\nFeel useless to attendees\n\nKeep focus on the meetings agenda\n\nGroup meetings (manager‚Äôs agenda)\n1-1s (1 on 1) (report‚Äôs agenda)",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#tools-for-servant-leadership",
    "href": "qmd/job-management-leadership.html#tools-for-servant-leadership",
    "title": "29¬† Management / Leadership",
    "section": "29.7 Tools for Servant Leadership",
    "text": "29.7 Tools for Servant Leadership\n\nTeaching - As a leader you often have more context and more experience than your team members.\n\nTeach the team which situations different models work in, how those models are perceived in your organization and the red-flags to watch out for during development.\n\nReflecting - Make time to think back to events within your team.\n\nWhat caused success? What led to failure? Are we setting expectations appropriately for our models and analyses?\n\nDebate - Encourage debate.\n\nThe team is trying to use data to understand the world, and as in any form of science, there will be competing hypotheses\nTake advantage of the diversity (all forms) within our teams to minimize the impact of those personal biases\n\nProcess - Leaders will have to deal with ambiguity, but for the wider team we need to ensure there are steps to follow that support consistency across the team and alignment on the team‚Äôs over-arching goals.\nFeedback - To maintain team members‚Äô morale, the balance between negative and positive feedback has to tilt heavily towards the positive\n\nIf you can‚Äôt find that balance, then you need to consider whether the team member should continue on your team. If you want them to remain, then you must figure out how to articulate their positives back to them, otherwise you can expect them to leave.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#nonviolent-communication-nvc",
    "href": "qmd/job-management-leadership.html#nonviolent-communication-nvc",
    "title": "29¬† Management / Leadership",
    "section": "29.8 Nonviolent Communication (NVC)",
    "text": "29.8 Nonviolent Communication (NVC)\n\nMisc\n\nNotes from How to deliver constructive feedback in difficult situations\nOther methods\n\nSBI (Situation-Behavior-Impact) Useful for giving better feedback by removing emotions from it and making it clear and concise\n\nSteps\n\nSituation - Start the feedback with a specific situation that occurred which serves as a common reference point and is specific.\nBehavior - Refer to a specific behavior that you observed and want to talk about. Make sure to not give any judgments and leave the interpretation out of it.\nImpact - Talk about the impact that behavior had and what you think and feel about it. Feel free to address what other people think and how it impacted things.\nIntent - Ask about the person‚Äôs intention and try to uncover if the person is aware of what he did and why he did it. Then, work together with the person to see how things can be made better and how to overcome issues.\n\n\n\n\nAt the core of NVC is a straightforward communication pattern:\n\n‚ÄúWhen ____[observation], I feel ____[emotion] because I‚Äôm needing some ____[universal needs]. Would you be able to ____[request]?‚Äù\nExamples\n\nTo a co-founder: ‚ÄúWhen you said, ‚ÄòI‚Äôm not happy with your work,‚Äô to me in front of the team, I felt embarrassed because it didn‚Äôt meet my need for trust and recognition. Please, could we set up a weekly one-on-one session to share feedback in private?‚Äù\nTo an investor: ‚ÄúI haven‚Äôt received any responses from the last three monthly updates. I‚Äôm feeling concerned because I need input. Please, would you mind getting back to me with responses to my questions in the last update?‚Äù\nTo a teammate: ‚ÄúYou arrived 10 minutes late to the last three team meetings. I am frustrated because, as a team, we have a need for efficiency. Please, could you help me understand what‚Äôs happening?‚Äù\n\n\nObservations (vs evaluations)\n\nExamples\n\nEvaluation: ‚ÄúYou are lazy‚Äù (which is a character attack). Observation: ‚ÄúYou said that you‚Äôd send the document last week, and I haven‚Äôt received it.‚Äù\nEvaluation: ‚ÄúYour work is sloppy‚Äù (which is a criticism). Observation: ‚ÄúThree of the numbers in the report were inaccurate.‚Äù\nEvaluation: ‚ÄúYou‚Äôre always late,‚Äù (which is a generalization). Observation: ‚ÄúYou arrived 10 minutes late to the meeting this morning.‚Äù\nEvaluation: ‚ÄúYou ignored me.‚Äù (which implies intent). Observation: ‚ÄúI sent you two emails, and I haven‚Äôt received a response.‚Äù\n\nCheck\n\nask yourself, ‚ÄúWhat did I actually see or hear?‚Äù\n\n\nEmotions (vs thoughts, vs evaluations)\n\nUsing an evaluation or thought instead of an emotion, can result in a defensive reply\nExamples\n\nEmotion: ‚ÄúI feel frustrated.‚Äù Thought: ‚ÄúI feel that you aren‚Äôt taking this seriously.‚Äù\nEvaluation: ‚ÄúI feel judged.‚Äù Impact: ‚ÄúI feel resentful.‚Äù\n\ndefensive reply: ‚ÄúI didn‚Äôt judge you.‚Äù\n\nEvaluation: ‚ÄúI feel misunderstood.‚Äù Impact: ‚ÄúI feel frustrated.‚Äù\nEvaluation: ‚ÄúI feel rejected.‚Äù Impact: ‚ÄúI feel hurt.‚Äù\n\nCheck\n\nFor thoughs, if you can substitute ‚ÄúI feel‚Äù with ‚ÄúI think‚Äù and the phrase still works ‚Äî because it‚Äôs a thought, not an emotion.\n\n\nUniversal Need (vs strategy for obtaining a need)\n\nExamples\n\nStrategy: ‚ÄúI need you to copy me into every email.‚Äù Universal Need: ‚ÄúI need some transparency.‚Äù\nUniversal: ‚Äú‚ÄúI need support.‚Äù NOT Universal: ‚ÄúI need support from you.‚Äù\n\nNOT Universal is more easily interpreted as a veiled accusation and implication that ‚ÄúYou aren‚Äôt supporting me.‚Äù\n\n\n\nRequests (vs demands)\n\nrequests are invitations for another person to meet our needs ‚Äî but only if it doesn‚Äôt conflict with one of their needs.\nCharacteristics of a good request\n\nMake them specific\n\n‚ÄúI request that you arrive to meetings on time.‚Äù instead of ‚ÄúI request that you be more respectful of everyone‚Äôs time.‚Äù\n\nSay what you want, not what you don‚Äôt want\n\nDon‚Äôt want: ‚ÄúI request that you don‚Äôt dismiss other people‚Äôs ideas straightaway‚Äù\nWant: ‚ÄúI request that when a team member shares an idea, you ask two or three probing questions before sharing your conclusion.‚Äù\n\nStay curious\n\nBe optimistic that everyone‚Äôs needs can be met.\nTreat ‚Äúno‚Äù to a request or a defensive reply as an invitation to explore the needs stopping someone from saying ‚Äúyes.‚Äù\nThink about how the other person is feeling and consider what unmet needs may be stopping them from saying ‚Äúyes.‚Äù\n\nAre you feeling hurt because you need some understanding?\nAre you feeling angry because you need your hard work to be recognized?\nIs there more you‚Äôd like to say?\n\nSimilarly, if you‚Äôre on the receiving end of a request and have to say ‚Äúno,‚Äù state the underlying need that stops you from saying ‚Äúyes.‚Äù\n\n\n\nDiplomatically confirm communication if needed\n\n‚ÄúJust so we know we‚Äôre on the same page, could you play back what I‚Äôm asking of you?‚Äù\n\n40-word rule\n\nDuring difficult conversations, it‚Äôs important to be extremely concise. Aim to describe your observations, feelings, needs, and requests in fewer than 40 words. Using more words suggests you‚Äôre justifying your needs, and that decreases their power.\n\nFace-to-Face is better\n\nNVC loses some of its power when it‚Äôs in an email.\n\nConsequences should be protective, not punitive\n\nAs a manager, you are responsible for the effectiveness of your team ‚Äî and every team needs effectiveness. If deadlines continue to be missed (the boundary), you might have to switch their responsibilities or move them on (the consequence). It‚Äôs not personal, it‚Äôs just what you‚Äôll do to protect your need for effectiveness.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#promoting-your-teams-projects",
    "href": "qmd/job-management-leadership.html#promoting-your-teams-projects",
    "title": "29¬† Management / Leadership",
    "section": "29.9 Promoting Your Team‚Äôs Projects",
    "text": "29.9 Promoting Your Team‚Äôs Projects\n\nAnnouncement emails\n\nUnlike ‚Äúsharing‚Äù emails (brief description, link), announcement emails have more pomp associated with them\nCharacteristics\n\nUse catchy subject lines ‚Äî e.g.¬†‚ÄòRetention Dashboard is here!‚Äô or ‚ÄòIntroducing Retention Dashboard‚Äô\nIn addition to stating what the dashboard contains, tie it to key insights, recommendations and next steps\nUse icons & visuals ‚Äî Adding relevant icons and visuals makes the email easier to consume and provides a nice break from all the heavy text. Caution: Do not overuse!\n\nExample\n\n\nReadouts\n\nAn analysis that is packaged in a way that is easy to read through\nTypically a one-time analysis\n\n(deep dive) e.g.¬†what drives customer retention\n(root cause analysis) e.g.¬†why did top of funnel conversion decline or analyzing an experiment / launch / campaign performance\n\nAlso reoccurring\n\nCould be weekly or monthy, depending on topics important to your stakeholders\nActively sharing summarized findings from dashboards to the stakeholders can change perception that these dashboards are just another source of data\n\nMonthly or Quarterly Business Reviews\n\nPresentations where you review health of business based on trends in key metrics (month over month, quarter over quarter)\n\nAutomate frequent requests from Marketing managers, Product managers, and Operations managers\n\nProduce a readout that covers insights from multiple dashboards that managers are frequently asking about.\n\n\nNewletter\n\nHighlight goals for ongoing work-streams and outcomes for those completed, always connecting to business outcomes or stakeholder needs\nSample Layout\n\nSummary ‚Äî Key Wins & What‚Äôs Coming\nDetailed updates by themes\nNewsletter FAQ‚Äôs ‚Äî\n\nGoals: What is the goal of this newsletter. e.g.¬†Providing visibility and aligning on prioritization\nCadence: Weekly / Bi-weekly / Monthly\nAudience: Sr.¬†Leadership of company\nTeam members\nPOC: Who should they reach out to if they have questions\n\n\nExample",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html",
    "href": "qmd/production-tools.html",
    "title": "38¬† Tools",
    "section": "",
    "text": "38.1 Misc",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-misc",
    "href": "qmd/production-tools.html#sec-prod-tools-misc",
    "title": "38¬† Tools",
    "section": "",
    "text": "Overview of some 2021 tools Descriptions in article \nAWS Batch - Managed service for computational jobs. Alternative to having to maintain a kubernetes cluster\n\nTakes care of keeping a queue of jobs, spinning up EC2 instances, running code and shutting down the instances.\nScales up and down depending on how many jobs submitted.\nAllows you to execute your code in a scalable fashion and to request custom resources for compute-intensive jobs (e.g., instances with many CPUs and large memory) without requiring us to maintain a cluster\nSee bkmks: Hosting &gt;&gt; AWS &gt;&gt; Batch\nPackages:\n\n{crew.aws.batch}",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-stckcomprnk",
    "href": "qmd/production-tools.html#sec-prod-tools-stckcomprnk",
    "title": "38¬† Tools",
    "section": "38.2 Stack Component Rankings",
    "text": "38.2 Stack Component Rankings\n\nDB format\n\narrow files\n\nELT Operations\n\n*dbt\n\nGoogle‚Äôs alternative is Dataform\nAWS‚Äôs alternative is Databrew\n\n*Spark\n*Google Big Query SQL\n*AWS Athena\n\nOrchestration and monitoring\n\n*Targets\n\n+ {cronR} for orchestration + scheduling\n\n*Mage-AI\n*AWS Glue\nPrefect\nAirflow\n\nData Ingestion\n\nAirbyte (data ingestion)\nfivetran (data ingestion)\n\nCan ‚Äúprocess atomic REST APIs to extract data out of SAAS silos and onto your warehouse‚Äù\n\nterraform (multi-cloud management)\n\nTracking/Versioning for Model Building\n\n*DVC\nMLFlow\n\nReporting\n\nblastula (email), xaringan (presentation), RMarkdown (reports), flexdashboard (dashboards),\nRStudio Connect (publishing platform to stakeholders)\n\ndashboards, apps\non-demand and scheduled reports\npresentations\nAPIs (?)\nPublish R and Python\nEnterprise security\nCan stay in RStudio\n\n\nVisualization Platforms\n\nLooker*\nPowerBI, DataStudio",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-depman",
    "href": "qmd/production-tools.html#sec-prod-tools-depman",
    "title": "38¬† Tools",
    "section": "38.3 Dependency Management",
    "text": "38.3 Dependency Management\n\nR\n\nr2u for linux installations\n\n‚Äúfor Ubuntu 20.04 and 22.04 it provides _all_ of CRAN (and portion of BioConductor) as binary #Rstats packages with full, complete and automatic resolution of all dependencies for full system integration. If you use `bspm` along with it you can even use this via `install.packages()` and friends. Everything comes from a well connected mirror‚Äù",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#data-versioning",
    "href": "qmd/production-tools.html#data-versioning",
    "title": "38¬† Tools",
    "section": "38.4 Data versioning",
    "text": "38.4 Data versioning\n\nFlat Table by Github\n\nHas a Github action associated with it\nHas a datetime commit message\nLists as a feature that it tracks differences from one commit to the next, but doesn‚Äôt a normal data commit doe the same thing?\n\nLumberjack R package\n\nAdd functions to your processing script\ntracks using a log file\noptions for changes you want to track",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-dating",
    "href": "qmd/production-tools.html#sec-prod-tools-dating",
    "title": "38¬† Tools",
    "section": "38.5 Data Ingestion",
    "text": "38.5 Data Ingestion\n\nFiveTran\n\nFree-tier\nSync raw data sources\n\nevery 1hr for starter plan, every 15 minutes both standard plans, every 5 min for enterprise plan",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-orch",
    "href": "qmd/production-tools.html#sec-prod-tools-orch",
    "title": "38¬† Tools",
    "section": "38.6 Orchestration",
    "text": "38.6 Orchestration\n\n38.6.1 Airflow\n\nWebpage\nOpen-source platform for authoring, scheduling, and executing data pipelines.\n\nFeatures for managing and monitoring data pipelines, including integration with various data storage and processing technologies. Similar to the Unix cron utility ‚Äî you write scripts and schedule them to run every X minutes.\nAirflow can be used for any sort of scheduling task, but is often used for scheduling data modeling. schedule, run and monitor the refresh of our data warehouse\nMonitoring on-prem checking Airflow logs is not user-friendly (better in AWS MWAA)\n\ndifferent types of logs for task, web server, scheduler, worker, and DAGs\nhave to SSH into the server and run commands which becomes more complicated when you want to use distributed servers for scalability.\n\nRequires you to create a central logging storage and make additional setup to make all servers write logs into that single place\n\n\nServer-based remains active even when not running jobs ‚Äì&gt; continually incurring cost\n\nNo latency since servers are always running\n\nProblems\nLong feedback loop\n\nWhile programming, instant feedback of your DAG becomes crucial when you want a sanity check before your code goes too far.\nTo see the graph view, which is mainly for visualizing dependencies in DAGs, your code needs to be in the folder of an Airflow scheduler that can be picked up. The airflow scheduler also takes time to render and parse your DAG until it shows up.\nMakes debugging difficult during the development cycle, so some engineers write more lines of code and test them all together. If the lines of code become unmanageable on one screen, you might vaguely remember what to validate and what dependencies to check.\n\nDifficult with local development\n\na docker image can be used to inject as much production-related information as possible. But it‚Äôs still not 100% copy, and it takes tremendous effort to develop and maintain that docker image.\nEven if you set up dev, staging, and production environments for running Airflow, they aren‚Äôt totally isolated and developers can end-up interfering with one another. Services/Extensions Astronomer offers a managed Airflow service.\nAmazon Managed Workflows for Apache Airflow (MWAA) - managed Airflow service\n\nOrchestrate jobs in EMR, Athena, S3, or Redshift\n\nGlue\n\nAirflow has the glue operator\n\nCloudFormation can be used to configure and manage\n\nallows for autoscaling which saves on costs by scaling down when usage is low\nstill needs a server running even when not running jobs\nmonitoring much easier since all the logs are written into CloudWatch search certain logs using Logs Insights\n\nhave a dashboard that displays usage of server resources like CPU, memory, and network traffic.\nmonitor numerous other Airflow-specific metrics.\nset up alerts and manage notification recipients programmatically.\n\n\nCost factors\n\nInstance size\nAdditional worker instance\nAdditional scheduler instance\nMeta database storage\n\nPotential Issues:\n\nResources are shared on multiple jobs so performance can suffer if:\n\nDon‚Äôt distribute trigger times evenly\nMisconfigure your maximum worker count\n\n\nOperate through AWS SDK\n\nCan\n\ncreate, update, and delete MWAA environments and retrieve their environment information that includes logging policies, number of workers, schedulers\nrun Airflow‚Äôs internal commands to control DAGs\n\nCan‚Äôt\n\nSome of Airflow‚Äôs native commands like backfill (check this AWS document), dags list, dags list-runs, dags next-execution, and more\n\n\n\n\n\n\n\n38.6.2 AWS Glue\n\nCloud-based data integration service that makes it easy to move data between data stores.\n\nIncludes a data catalog for storing metadata about data sources and targets, as well as a ETL (extract, transform, and load) engine for transforming and moving data.\nIntegrates with other AWS services, such as S3 and Redshift, making it a convenient choice for users of the AWS ecosystem. Serverless (i.e.¬†costs only incurred when triggered by event) Each job triggers separate resources, so if one job overloads resources, it doesn‚Äôt affect other jobs\nJobs experience latency since instances have to spin-up and install packages\nCost Charged by Data Processing Unit (DPU) multiplied by usage hours (Pricing)\n\nJob types:\n\nPython shell: you can choose either 0.0625 or 1 DPU.\nApache Spark: you can use 2 to 100 DPUs.\nSpark Streaming: you can use 2 DPUs to 100 DPUs.\n\n\n\nCan run Spark\nExpensive for longer running ETL tasks. So, setting up your own container and deploying it on ECS with Fargate makes sense, both in terms of efficiency and cost.\nMonitoring\n\nCloudwatch\nGlueStudio within Glue Clicking number sends you to Cloudwatch where you can drill down into jobs\nCloudFormation can be used to configure and manage\nGlue SDK available\n\n\n\n\n38.6.3 Prefect\n\nEasier to manage for smaller data engineer teams or a single data engineer\nmore user friendly than Airflow; Better UI; more easily discover location and time of errors\npurely python\nMisc\n\nadd slack webhook for notifications\nHas slack channel to get immediate help with issues or questions\n\nautomatic versioning for every flow, within every project\n\nalso document the models deployed with each version in the README they provide with every flow\n\nComponents\n\nTasks - individual jobs that do one unit of work\n\ne.g.¬†a step that syncs Fivetran data or runs a dbt model\n\nFlows - functions that consist of a bunch of smaller tasks, or units of work, that depend on one another\n\ne.g.¬†1 flow could be multiple tasks running Fivetran syncs and dbt models\n\nExample:\nfrom prefect import flow, task\n@flow(name=\"Create a Report for Google Trends\")\ndef create_pytrends_report(\n¬† ¬† keyword: str = \"COVID\", start_date: str = \"2020-01-01\", num_countries: int = 10\n):\n\nThese flows are then scheduled and run by whatever types of agents you choose to set up.\n\nSome options include AWS ECS, GCP Vertex, Kubernetes, locally, etc.\n\nDeployments (docs)\n\nAlso see Create Robust Data Pipelines with Prefect, Docker, and GitHub\nDefintions\n\nSpecify the execution environment infrastructure for the flow run\nSpecify how your flow code is stored and retrieved by Prefect agents\nCreate flow runs with custom parameters from the UI\nCreate a schedule to run the flow\n\nSteps\n\nBuild the deployment definition file and optionally upload your flow to the specified remote storage location\nCreate the deployment by applying the deployment definition\n\nSyntax: prefect deployment build [OPTIONS] &lt;path-to-your-flow&gt;:&lt;flow-name&gt;\nExample:\nprefect deployment build src/main.py:create_pytrends_report \\\n¬† -n google-trends-gh-docker \\\n¬† -q test\n\nDeployment for the flow create_pytrends_report (see flow example) from the file, ‚Äúsrc/main.py‚Äù\n-n google-trends-gh-docker specifies the name of the deployment to be google-trends-gh-docker.\n-q test specifies the work queue to be test . A work queue organizes deployments into queues for execution.\nOutput\n\n‚Äúcreate_pytrends_report-deployment.yaml‚Äù file and a ‚Äú.prefectignore‚Äù created in the current directory.\n\n‚Äúcreate_pytrends_report-deployment.yaml‚Äù:¬† specifies where a flow‚Äôs code is stored and how a flow should be run.\n‚Äú.prefectignore‚Äù:¬† prevents certain files or directories from being uploaded to the configured storage location.\n\n\n\n\n\n38.6.4 Azure Data Factory\n\nAllows users to create, schedule, and orchestrate data pipelines for moving and transforming data from various sources to destinations.\nData Factory provides a visual designer for building pipelines, as well as a range of connectors for integrating with various data stores and processing technologies.\nExample: Demand Planning Project\n\n\n\n38.6.5 Mage-AI\n\nEnables users to define DAG regardless of the choice of languages (python/SQL/R)\nWeb-based IDE, so its mobility allows working from different devices, and sharing becomes more straightforward.\n\nUI layout feels like using RStudio. It has many sections divided into different areas.\nOne of the areas is the DAG visualization which provides instant feedback to the user on the task relationship.\n\nDAGs\n\nThe pipeline or DAG is constructed with modular blocks‚Äîa block maps to a single file.\nBlock Options\n\nExecution with upstream blocks: this triggers all upstream blocks to get the data ready for the current block to run\nExecute and run tests defined in the current block: this focuses on the current block to perform testing.\nSet block as dynamic: this changes the block type into the dynamic block, and it fits better to create multiple downstream blocks at runtime.\n\nManipulate dependencies via drag and drop\n\nmage-ai keeps track of the UI changes the user made and automatically builds the dependencies DAG into the YAML file. (./pipelines/{your_awesome_pipeline_name}/metadata.yaml)\n\nVisualize data in each block\n\nHelpful for inspecting your input data and further validating the transformation.\nOnce the chart has been created, it will also be attached to the current block as the downstream_blocks.\n\n\nR\n\nAllows users to write the main ETL (Extraction, Transformation, and Loading) blocks using R.\n\n\n\n\n38.6.6 kestra\n\nPopular orchestration libraries such as Airflow, Prefect, and Dagster require modifications to the Python code to use their functionalities. You may need to modify the data science code to add orchestration logic\nKestra, an open-source library, allows you to develop your Python scripts independently and then ‚Äã‚Äãseamlessly incorporate them into data workflows using YAML files.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-eltetl",
    "href": "qmd/production-tools.html#sec-prod-tools-eltetl",
    "title": "38¬† Tools",
    "section": "38.7 ELT/ETL Operations",
    "text": "38.7 ELT/ETL Operations\n\n38.7.1 Misc\n\ndbt - see DB, dbt\nGoogle Dataform - Docs, Best Practices\n\n\n\n38.7.2 AWS DataBrew\n\nFeatures to clean and transform the data to ready it for further processing or feeding to machine learning models\n\nNo coding; pay for what you use; scales automatically\nover 250 transformations\nAllows you to add custom transformations with lambda functions",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-modexptrk",
    "href": "qmd/production-tools.html#sec-prod-tools-modexptrk",
    "title": "38¬† Tools",
    "section": "38.8 Model Experimentation/Version Tracking",
    "text": "38.8 Model Experimentation/Version Tracking\n\n38.8.1 DVC\n\nTracks data and models while model building\nStore code and track changes in a Git repository while data/models are in AWS/GCP/Azure/etc. storage\nTracking changes\n\nSteps\n\nhashes every file in the directory data,\nadds it to .gitignore and\ncreates a small file data.dvc that is added to Git.\n\nBy comparing hashes, DVC knows when files change and which version to restore.\n\nInitial Steps\n\nGoto project directory -cd &lt;path to local github repo&gt;\nInitialize DVC - dvc init\nAdd a data path/uri - dvc remote add -d remote path/to/remote\n\ncan be Google Drive, Amazon S3, Google Cloud Storage, Azure Storage, or on your local machine\ne.g.¬†Google Drive: dvc remote add -d remote gdrive://&lt;hash&gt;\n\nThe hash will the last part of the URL, e.g.¬†‚Äúhttps://drive.google.com/drive/u/0/folders/1v1cBGN9vS9NT6-t6QhJG‚Äù\n\nConfirm data set-up: dvc config -l\n\nThe config file is located inside ‚Äú.dvc/‚Äù\nTo version your config on github: git add .dvc/config\n\n\nAdd data/ to .gitignore\n\nExample showed adding every file in the repo manually but this seems easier\n\nAdd, commit, and push all files to repo\n\nMain differences to regular project initialization\n\ndata/ directory doesn‚Äôt get pushed to github\ndata.dvc file gets pushed to github\n\n\nSet-up DVC data cache\n\nCan be local directory/s3/gs/gdrive/etc\nExample: S3\n¬† ¬† ¬† ¬† ¬† ¬† dvc remote add -d myremote s3://mybucket/path\n¬† ¬† ¬† ¬† ¬† ¬† git add .dvc/config\n¬† ¬† ¬† ¬† ¬† ¬† git commit -m \"Configure remote storage\"\n¬† ¬† ¬† ¬† ¬† ¬† git push\n¬† ¬† ¬† ¬† ¬† ¬† dvc push\n\n\nI‚Äôm guessing .dvc/config is created with dvc remote add¬† and wasn‚Äôt there before. Otherwise in steps 3 and 4, I need to add the files manually.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-modmon",
    "href": "qmd/production-tools.html#sec-prod-tools-modmon",
    "title": "38¬† Tools",
    "section": "38.9 Model/Data Drift Monitoring",
    "text": "38.9 Model/Data Drift Monitoring\n\nArize AI\n\nDocs\nAccessed through Rest API, Python SDK, or Cloud Storage Bucket\n\nFiddler AI Monitoring: fiddler.ai has a suite of tools that help in making the AI explainable, aid in operating ML models in production, monitor ML models and yes data & model drift detection is one of them\nEvidently: EvidentlyAI is another open-source tool, which helps in evaluating and monitoring models in production. If you are not using Azure ML and looking for a non-commercial tool that is simple to use, evidentlyai is a good place to start.\nAzure ML\n\nMonitors data; uses wasserstein distance\n\nAWS Glue DataBrew\n\nmonitors features\ncalculates full suite of summary stats + entropy\n\nCan be exported to a bucket and then download to measure change over time\n\nAccessed through console or programmatically\nGenerates reports that can be viewed in console or be exported in html, pdf, etc.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-appclustmon",
    "href": "qmd/production-tools.html#sec-prod-tools-appclustmon",
    "title": "38¬† Tools",
    "section": "38.10 App/Cluster Monitoring",
    "text": "38.10 App/Cluster Monitoring\n\n38.10.1 Prometheus\n\nDon‚Äôt use for ML monitoring (from article)(maybe for apps?)\n\nNeed to use multiple Prometheus Metric types for cross-component monitoring\nNeed to define histogram buckets up front for single-component monitoring\nCorrectness of query results depending on scraping interval\nInability to handle sliding windows\nDisgusting-looking PromQL queries\nHigh latency for cross-component metrics (i.e., high-cardinality joins)\n\nMisc\n\nPrometheus is not a time series database (TSDB). It merely leverages a TSDB.\nBecause Prometheus scrapes values periodically, some Metric types (e.g., Gauges) can lose precision if the Metric value changes more frequently than the scraping interval. This problem does not apply to monotonically increasing metrics (e.g., Counters).\nMetrics can be logged with arbitrary identifiers such that at query time, users can filter Metrics by their identifier value.\nPromQL is flexible ‚Äì users can compute many different aggregations (basic arithmetic functions) of Metric values over different window sizes, and these parameters can be specified at query time.\n\nMetric values (Docs):\n\nCounter: a cumulative Metric that monotonically increases. Can be used to track the number of predictions served, for example.\nGauge: a Metric that represents a single numerical value that can arbitrarily change. Can be used to track current memory usage, for example.\nHistogram: a Metric that categorizes observed numerical values into user-predefined buckets. This has a high server-side cost because the server calculates quantiles at query time.\nSummary: a Metric that tracks a user-predefined quantile over a sliding time window. This has a lower server-side cost because quantiles are configured and tracked at logging time. Also, the Summary Metric doesn‚Äôt generally support aggregations in queries.\n\nProcess\n\n\nUsers instrument their application code to log Metric values.\nThose values are scraped and stored in a Prometheus server.\nThe values can be queried using PromQL and exported to a visualization tool like Grafana\n\nThe are R packages that might make querying these metrics easier so you don‚Äôt have to learn PromQL",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-oth",
    "href": "qmd/production-tools.html#sec-prod-tools-oth",
    "title": "38¬† Tools",
    "section": "38.11 Other",
    "text": "38.11 Other\n\nTerraform\n\nProvision infrastructure across 300+ public clouds and services using a single workflow through yaml files\n\nAutomates and makes these workflows reproducible\nArticle on using it with R\n\n\nDatadog - Monitor servers in all cloud hosts in one place ‚Äî alerts, metrics, logs, traces, security incidents, etc.\nPagerDuty - Automated incident management ‚Äî alerts, notifications, diagnostics, logging, etc.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html",
    "href": "qmd/quarto-rmarkdown.html",
    "title": "Quarto",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-misc",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-misc",
    "title": "Quarto",
    "section": "",
    "text": "Packages\n\n{quarto}\n\nResources\n\nDocs\nReference\nTroubleshooting\n\nquarto --version - Must be in RStudio Terminal\nquarto check - Must be in RStudio Terminal - versions and engine checks\n$ quarto check\n[&gt;] Checking versions of quarto binary dependencies...\n¬† ¬† ¬† Pandoc version 3.1.1: OK\n¬† ¬† ¬† Dart Sass version 1.55.0: OK\n[&gt;] Checking versions of quarto dependencies......OK\n[&gt;] Checking Quarto installation......OK\n¬† ¬† ¬† Version: 1.3.340\n¬† ¬† ¬† Path: C:\\Users\\tbats\\AppData\\Local\\Programs\\Quarto\\bin\n¬† ¬† ¬† CodePage: 1252\n[&gt;] Checking basic markdown render....OK\n[&gt;] Checking Python 3 installation....OK\n¬† ¬† ¬† Version: 3.8.1 (Conda)\n¬† ¬† ¬† Path: C:/Users/tbats/Miniconda3/python.exe\n¬† ¬† ¬† Jupyter: 4.9.1\n¬† ¬† ¬† Kernels: python3\n(\\) Checking Jupyter engine render....2023-04-28 10:18:15,018 - traitlets - WARNING - Kernel\nProvisioning: The 'local-provisioner' is not found.¬† This is likely due to the presence of multiple jupyter_client distributions and a        previous distribution is being used as the source for entrypoints - which does not include 'local-provisioner'.¬† That distribution should     be removed such that only the version-appropriate distribution remains (version &gt;= 7).¬† Until then, a 'local-provisioner' entrypoint will     be automatically constructed and used.\nThe candidate distribution locations are: ['C:\\\\Users\\\\tbats\\\\Miniconda3\\\\lib\\\\site-packages\\\\jupyter_client-5.3.4.dist-info',                'C:\\\\Users\\\\tbats\\\\Miniconda3\\\\lib\\\\site-packages\\\\jupyter_client-7.0.6.dist-info']\n[&gt;] Checking Jupyter engine render....OK\n[&gt;] Checking R installation...........OK\n¬† ¬† ¬† Version: 4.2.3\n¬† ¬† ¬† Path: C:/PROGRA~1/R/R-42~1.3\n¬† ¬† ¬† LibPaths:\n¬† ¬† ¬† ¬† - C:/Users/tbats/AppData/Local/R/win-library/4.2\n¬† ¬† ¬† ¬† - C:/Program Files/R/R-4.2.3/library\n¬† ¬† ¬† knitr: 1.42\n¬† ¬† ¬† rmarkdown: 2.20\n[&gt;] Checking Knitr engine render......OK\nCLI\n\nquarto render to compile a document\nquarto preview to render a live preview that automatically updates when the source files are saved\n\nUsing a development verison of Quarto\n\nFirst Usage\n\nChange directories to where you want to store the dev version\nClone repo and change to the cloned directory\ngit clone https://github.com/quarto-dev/quarto-cli\ncd quarto-cli\nDisable Anti-Virus\nRun Configuration Script\n\nWindows Command Prompt\ncmd /k configure.cmd\n\n\\k keeps the window open in case it errors\n\nPowershell\nInvoke-Item configure.cmd\nLinux/MacOS\n./configure.sh\nThis will take a minute or two as it checks versions, installs dependencies like pandoc, etc.\n\nAdd path to quarto.cmd to PATH\n\nAfter the configuration file runs, it will output the path you need to put on PATH, e.g.¬†\"C:\\Users\\erc\\Documents\\Quarto\\quarto-cli\\package\\dist\\bin\"\n\nEnable Anti-Virus\nShould be able to use in RStudio\n\nI was not able to use the RStudio terminal for quarto commands (e.g.¬†quarto check) though.\nTo find the version, I just opened powershell and ran quarto ‚Äìversion just to make sure it was running and on PATH.\n\nNot sure if they use this every time but it was 99.9.9 instead of the verion in the changelog.\n\nI also rendered a qmd file using quarto-cmd from the root directory of quarto-cli to see if it matched the output from RStudio. (cd qmd then quarto preview forecasting-statistical.qmd --to html --no-watch-inputs --no-browse)\n\n\nSubsequent Development Versions\n\nChange directory to quarto-cli and git pull\n\n\nShortcuts\n\nNew R chunk: ctrl + alt + i\nBuild whole book: ctrl+shift b\nRender page and preview book: ctrl+shift k\n\nUsing yaml style for chunk options\n\nConvert Rmd chunk options to Quarto: knitr::convert_chunk_header(\"doc.rmd\", \"doc.qmd\")\nAnchor Link - A link, which allows the users to flow through a website page. It helps to scroll and skim-read easily. A named anchor can be used to link to a different part of the same page (like quickly navigating) or to a specific section of another page.\n\nThis is the ‚Äú#sec-moose‚Äù id that can be added to headers which it allows to be referenced within the document or in other documents.\n\nMathJax commands\n\nFont Size: \\tiny{ }, \\scriptsize{ }, \\small{ }, \\normal{ }, \\large{ }, \\Large{ }, \\LARGE{ }, \\huge{ }, \\Huge{ }\n\nLightbox\n\nDocs\nGrouping images for lightbox carousel: ![A Lovely Image](mv-1.jpg){group=\"my-gallery\"}",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-syntax",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-syntax",
    "title": "Quarto",
    "section": "Syntax",
    "text": "Syntax\n\nInline code\n-   Total number of counties: **`{r} polling_places |&gt; filter(state == \"Alabama\") |&gt; distinct(county_name) |&gt; count()`**\n-   Total number of polling places: **`{r} polling_places |&gt; filter(state == \"Alabama\") |&gt; count()`**\n-   Election Day: **`{r} polling_places |&gt; filter(state == \"Alabama\") |&gt; pull(election_date) |&gt; unique()`**\n\nTo escape backticks in inline code, you have to use double-backticks instead of single backticks\n\nExample: To get \"`\"\\$(\\$file.FullName)`\"\"\n`` \"`\"\\$(\\$file.FullName)`\"\" ``\n\n\nAlign code chunk under bullet and add indented comment below chunk\n-   [Example]{.ribbon-highlight} (using a SQL Query; method 1)\n\n    ``` r\n    # open dataset\n    ds &lt;- arrow::open_dataset(dir_out, partitioning = \"species\")\n    # open connection to DuckDB\n    con &lt;- dbConnect(duckdb::duckdb())\n    # register the dataset as a DuckDB table, and give it a name\n    duckdb::duckdb_register_arrow(con, \"my_table\", ds)\n    # query\n    dbGetQuery(con, \"\n      SELECT sepal_length, COUNT(*) AS n\n      FROM my_table\n      WHERE species = 'species=setosa'\n      GROUP BY sepal_length\n    \")\n\n    # clean up\n    duckdb_unregister(con, \"my_table\")\n    dbDisconnect(con)\n    ```\n\n    -   filtering using a partition, the WHERE format is '\\&lt;partition_variable\\&gt;=\\&lt;partition_value\\&gt;'\n\nSpace between bullet and top ticks\nSpace between bottom ticks and bullet\nNote alignment of text\n\nAdd Code Annotations\n-   [Partition a large file and write to arrow format]{.underline}\n\n    ``` r\n    lrg_file &lt;- open_dataset(&lt;file_path&gt;, format = \"csv\") # &lt;1&gt;\n    lrg_file %&gt;%\n        group_by(var) %&gt;% # &lt;2&gt;\n        write_dataset(&lt;output_dir&gt;, format = \"feather\") # &lt;3&gt;\n    ```\n\n    1.  Pass the file path to `open_dataset()`\n\n    2.  Use `group_by()` to partition the Dataset into manageable chunks\n\n    3.  Use `write_dataset()` to write each chunk to a separate Parquet file---all without needing to read the full CSV file into R\n\n    -   `open_dataset` is fast because it only reads the metadata of the file system to determine how it can construct queries\nFootnote\nwords [^1]\n\n[^1]: Data from https://github.com/rfordatascience/tidytuesday\nFor PDF output, you need pagebreaks:\n{{&lt; pagebreak &gt;}}",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-yaml",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-yaml",
    "title": "Quarto",
    "section": "YAML",
    "text": "YAML\n\nSet global chunk options in yaml\n\n\nFor code cells\nexecute:\n  echo: false\n  message: false\n  warning: false\n\nEnable Margin Notes\n---\n# YAML front matter\nreference-location: margin\n---\n!expr to render code within chunk options\n\ne.g.¬†figure caption: #| fig-cap: !expr glue::glue(\"The mean temperature was {mean(airquality$Temp) |&gt; round()}\")\n\ncolumn: screen-inset yaml markup is used to show a very wide table\nIf you haven‚Äôt set your Quarto document to be self-contained, then the images have also already been saved for you - probably in a folder called documentname_files/figure-html/\nformat: \n  html:\n    embed-resources: true\nDate first published and date modified using the current date:\n---\ndate: 2024-01-01\ndate-modified: today\n---\nYAML Examples\n\nExample",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-chunk",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-chunk",
    "title": "Quarto",
    "section": "Chunk Options",
    "text": "Chunk Options\n\nGraphics\n\nCode Chunk\n#| label: \"fig-statemap\"\n#| dpi: 300\n#| fig.height: 7.2\n#| fig.width: 3.6\n#| dev: \"png\"\n#| echo: false\n#| warning: false\n#| message: false\n\nExample shows settings for a graph for mobile\nfig.height and fig.width are always given in inches\n\nReference Figure\n1 See polling place locations in @fig-statemap.\n\nConditional Code Chunk Evaluation\n\nExample: document output type\n\nSet value in a code chunk\n```{r setup}\n# Include in first chunk of .qmd\n# Get output file type\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\n```\nUse !expr sytax to determine evaluation status\n\nExample: eval chunk based on output type\n```{r}\n#| eval: !expr out_type == \"html\"\n\n# code to create interactive {plotly}\n```\n\n```{r}\n#| eval: !expr out_type == \"docx\"\n\n# code to create static {ggplot2}\n```\n\n\nExample: Use parameterization to set value\n---\ntitle: \"test\"\nformat: html\nparams:\n  my_value: false\n---\n\nmy_value can then be used throughout the document to determine chunk evaluation status\n\n\nKnitr Hooks\n\nNotes from Writing knitr hooks\n\nAlso has a knitr hook example that alters cell output (e.g.¬†only prints 4 lines of a vector)\n\nChunk Hooks\n\nChunk hooks get called twice: once before knitr executes the code in the chunk, and once again afterwards\nThe function can take up to four arguments, all of which are optional:\n\nbefore: A logical value indicating whether the function is being called before or after the code chunk is executed\noptions: The list of chunk options\nenvir: The environment in which the code chunk is executed\nname: The name of the code chunk option that triggered the hook function\n\nThe chunk hook is called for its side effects not the return value. However, if it returns a character output, knitr will add that output to the document output as-is.\nExample: Chunk Timer\n\nCode\ncreate_timer_hook &lt;- function() {\n  start_time &lt;- NULL\n  function(before, options) {\n    if (before) {\n      start_time &lt;&lt;- Sys.time()\n    } else {\n      stop_time &lt;- Sys.time()\n      elapsed &lt;- difftime(stop_time, start_time, units = \"secs\")\n      paste(\n        \"&lt;div style='font-size: 70%; text-align: right'&gt;\",\n        \"Elapsed time:\", \n        round(elapsed, 2), \n        \"secs\",\n        \"&lt;/div&gt;\"\n      )\n    }\n  }\n}\nknitr::knit_hooks$set(timer = create_timer_hook())\n\nThe hook is triggered the first time (with before = TRUE) to record the system time somewhere (e.g., in a variable called start_time). Then, when the hook is triggered the second time (with before = FALSE), it records the system time again (e.g., as stop_time), and computes the difference in time.\n\nUse in a cell\n```{r}\n#| timer: true\nrunif(10000)\n```\nOutput",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-rpy",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-rpy",
    "title": "Quarto",
    "section": "R and Python",
    "text": "R and Python\n\nIf only R or R and Python, the notebook is rendered by {knitr}\nIf only Python, the notebook is rendered by jupyter\nSet-up\n\n{reticulate} automatically comes loaded in Quarto and it knows to use it when it sees a python block, so you don‚Äôt need to load the package\nQuarto will select a version of Python using the Python Launcher on Windows or system PATH on MacOS and Linux. You can override the version of Python used by Quarto by setting the QUARTO_PYTHON environment variable.\n\nIn CLI on Windows, type py is see which version the Python Launcher , and therefore Quarto, is using and py ‚Äìlist to see which versions are installed.\n\n\nR\n```{r}\n#| label: read-data\n#| echo: true\n#| message: false\n#| cache: true\nlemurs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-08-24/lemur_data.csv')\n```\nPython\n```{python}\n#| label: modelling \n#| echo: true \n#| message: false\n\nlemur_data_py = r.lemur_data \nimport statsmodels.api as sm \ny = lemur_data_py[[\"Weight\"]] \nx = lemur_data_py[[\"Age\"]] \nx = sm.add_constant(x) \nmod = sm.OLS(y, x).fit() \nlemur_data_py[\"Predicted\"] = mod.predict(x) \nlemur_data_py[\"Residuals\"] = mod.resid`\n```\n\nUse r. to access the data in the R chunk\nThe first execution of a python cell starts reticulate::repl_python() in the terminal\n\n(back to) R\n```{r}\n#| label: plotting \n#| echo: true \n#| output-location: slide \n#| message: false \n#| fig-align: center \n#| fig-alt: \"Scatter plot of predicted and residual values for the fitted linear model.\" \n\nlibrary(reticulate) \nlibrary(ggplot2) \nlemur_residuals &lt;- py$lemur_data_py \nggplot(data = lemur_residuals, aes(x = Predicted, y = Residuals)) +\n  geom_point(colour = \"#2F4F4F\") +\n  geom_hline(yintercept = 0,\n            colour = \"red\") +\n  theme(panel.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"),\n        plot.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"))\n```\n\nUse py$ to access the data in the Python chunk *\nMust call library(reticulate) in order for Quarto to recognize py$",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-lay",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-lay",
    "title": "Quarto",
    "section": "Layouts",
    "text": "Layouts\n\n2 cols (1 col: text, 1 col: image)\n\n::: {layout=\"[50,50]\"}\n\n::: column\nEvery Quarto project starts with a Quarto file that has the extension `.qmd`.\n\n\nThis particular one analyzes children's early words, but every `.qmd` includes the same three basic elements inside:\n\n\n- A block of metadata at the top, between two fences of `---`s. This is written in [YAML](https://learnxinyminutes.com/docs/yaml/). \n- Narrative text, written in [Markdown](https://commonmark.org/help/tutorial/). \n- Code chunks in gray between two fences of ```` ``` ````, written with R or another programming language.\n\n\nYou can use all three elements to develop your code and ideas in one reproducible document.\n:::\n\n![](img/01-source.png)\n:::\n2 figures, 2 columns (i.e.¬†side-by-side) with captions at the top\n---\nfig-cap-location: top\n---\n\n-   Words\n    -   Predictions of Standard RF vs Oblique RF\n\n        ::: {layout-ncol=\"2\"}\n        ![Standard Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-axpred-1.png){fig-align=\"left\" width=\"432\"}\n\n        ![Oblique Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-oblpred-1.png){fig-align=\"left\" width=\"432\"}\n        :::\n\n        -   Words  \n\nfig-cap-location: bottom is default;\nfig-cap-location: margin is buggy, at least in for project type book. Captions are added to the margins but bullet points mysteriously disappear during rendering to html\n\n2 charts side-by-side extending past body margins\n```{r}\n#| label: my-figure\n#| layout-ncol: 2\n#| column: page\nggplot() + ...\nggplot() + ...\n```\n\n‚Äúlayout-ncol‚Äù says 2 side-by-side columns\n‚Äúcolumn: page‚Äù says extend column width to the width of the page\n\nNested Tabs",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-auto",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-auto",
    "title": "Quarto",
    "section": "Automation",
    "text": "Automation\n\nIteration and Parameterization\n\nNotes from\n\nVel√°squez R-Ladies Nairobi: Code, Slides, Video\n\nIt involves having a ‚Äúchild‚Äù document as a template and running it repeatedly with different parameters\nThe ‚Äúmain‚Äù document includes the output from the child document\nRendering Options\n\nCLI: e.g.¬†quarto render polling-places-report.qmd -P state:'California'\n{quarto}:\nquarto::quarto_render(\n  input = here::here(\"polling-places-report.qmd\"),\n  execute_params = list(state = \"California\")\n)\n\nExample: Create a report for each parameter value. In each report, use the parameter value (e.g.¬†state) to iterate through a template file that makes a tables (1 for each county) based on that value.\nMain Report Document\n---\ntitle: \"Polling Places Report - `r params$state`\"\nparams:\n  state: \"California\"\n---\n\n```{r}\n#| results: hide\n\nlibrary(dplyr)\n\ncounties &lt;- polling_places |&gt; \n  filter(state == params$state) |&gt; \n  distinct(county_name) |&gt; \n  pull()\n\nexpanded_child &lt;- \n  counties |&gt; \n    purrr::map(\\(county) {\n      knitr::knit_expand(\"../_template.qmd\", \n                         current_county = county))\n      }|&gt; \n    purrr::flatten()\n\nparsed_child &lt;- knitr::knit_child(text = unlist(expanded_child))\n```\n\n`{r} parsed_child`\n\nThe document that gets published, emailed, etc.\nparams specified in YAML\n\nValue can also be used in the title of the document via inline R code\n\nEach county is iterated through the child document (_template.qmd) via current_county variable and knit_expand\nparsed_child is a list of the template file outputs.\nThen, parsed_child is converted to a character vector by unlist and all the results are printed in the document by the inline R code\n\nChild Document (i.e.¬†Template)\n### {{current_county}} COUNTY\n\n-   Total Polling Places: `{r} polling_places |&gt; filter(state == params$state, county_name == \"{{current_county}}\") |&gt; count()`\n-   Example Locations:\n\n```{r}\npolling_places |&gt; \n  filter(state == params$state, \n         county_name == \"{{current_county}}\") |&gt; \n  head(6) |&gt; \n  select(name, address.x) |&gt; \n  kbl(format = \"markdown\")\n```\n\nParameter value is used to get county data and create tables for each.\nNo YAML is necessary in child document\n\nparams values are automatically available through knitr::knit_expand that‚Äôs executed in the Main document\n\nThe county variable is utilized by the template file using the double curly braces, {{current_county}}\nkbl outputs in markdown format so the table is correctly rendered in the Main document.\n\nRendering Script\npolling_places &lt;-\n  readr::read_csv(here::here(\"data\", \"geocoded_polling_places.csv\"))\n\n# create quarto::render arguments df\npolling_places_reports &lt;-\n  polling_places |&gt;\n  dplyr::distinct(state) |&gt;\n  dplyr::slice_head(n = 5) |&gt;\n  dplyr::mutate(\n    output_format = \"html\",\n    output_file = paste0(tolower(state),\n                         \"-polling-places\"),\n    execute_params = purrr::map(state,\n                                \\(state) list(state = state))\n  ) |&gt;\n  # default output is html, so that variable not selected\n  dplyr::select(output_file, execute_params) \n\n# iterate through args and create reports\npurrr::pwalk(\n  .l = polling_places_reports,\n  .f = quarto::quarto_render,\n  input = here::here(\"main_report_document.qmd\"),\n  .progress = TRUE\n)\n\nCreates a report for each params value (e.g.¬†state)\nGenerates a dataframe for each set of arguments to be fed to quarto::quarto_render.",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-webr",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-webr",
    "title": "Quarto",
    "section": "WebR",
    "text": "WebR\n\nSet-Up\n\nInstall the extension alongside your blog post by running quarto add coatless/quarto-webr\nAdd the extension to your blog by adding filters: [\"webr\"] to your post‚Äôs frontmatter\nInstead of {r} code chunks, use {webr-r} ones\n\nInstall CRAN packages on page load\nfilters:\n  - \"webr\"\nwebr:\n  packages:\n  - \"dplyr\"\n  - \"tidyr\"\n  - \"purrr\"\n  - \"tibble\"\n  - \"crayon\"\n\nAdd to frontmatter\n\nInstall R-Universe Package\n```{webr-r}\n#| context: setup\nwebr::install(\"collateral\", repos = c(\"https://jimjam-slam.r-universe.dev\"))\n```\n\nR-Universe packages must be installed in code cells",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html",
    "href": "qmd/clustering-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-misc",
    "href": "qmd/clustering-general.html#sec-clust-gen-misc",
    "title": "General",
    "section": "",
    "text": "Also see\n\nNotebook, pgs 57-58\nDiagnostics, Clustering\n\nFor static data, i.e., if the values do not change with time, clustering methods are usually divided into five major categories:\n\nPartitioning (or Partitional)\nHierarchical\nDensity-Based\nGrid-Based\nModel-Based Methods",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-terms",
    "href": "qmd/clustering-general.html#sec-clust-gen-terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nCluster Centroid - The middle of a cluster. A centroid is a vector that contains one number for each variable, where each number is the mean of a variable for the observations in that cluster. The centroid can be thought of as the multi-dimensional average of the cluster.\nHard (or Crisp) Clustering - each point belongs to a single cluster\nSoft (or Fuzzy) Clustering - each point is allowed to belong to multiple clusters",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-clustdesc",
    "href": "qmd/clustering-general.html#sec-clust-gen-clustdesc",
    "title": "General",
    "section": "Cluster Descriptions",
    "text": "Cluster Descriptions\n\nPackages\n\n{parameters} - provides various functions for describing, analyzing, and visualizing clusters for various methods\n{clustereval} - compute the statistical association between the features and the detected cluster labels and whether they are significant.\n\nCategorical: Chi-Square, Fisher‚Äôs Exact, or Hypergeometric tests\nContinuous: Mann-Whitney-U test\n\n\nExamine variable values at the centroids of each cluster\n\nA higher absolute value indicates that a certain variable characteristic is more pronounced within that specific cluster (as compared to other cluster groups with lower absolute mean values).\n\nDistributional statistics for each cluster\n\nNumeric variables: mean and sd for each variable in that cluster\nCategorical variables:\n\nbinary: percent where event = 1\nmultinomial: most prominent category\n\n\nRun a decision tree on clusters\n\n\nEach color (orange, blue, green, purple) represents a cluster\nExplains how clusters were generated\n{treeheatr}\n\n\nRadar charts\n\n\n3 clusters: blue (highlighted), red, green\nGuessing the mean values for each variable are the points\n\nScatter\n\nUse clustering variables of interest for a scatter plot then label the points with cluster id",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-gmm",
    "href": "qmd/clustering-general.html#sec-clust-gen-gmm",
    "title": "General",
    "section": "Gaussian Mixture Models (GMM)",
    "text": "Gaussian Mixture Models (GMM)\n\nMisc\n\nSoft clustering algorithm\nNotes from\n\nSerrano video: https://www.youtube.com/watch?v=q71Niz856KE&ab_channel=LuisSerrano\nPackages\n\n{otrimle}\n\nUses Improper Maximum Likelihood Estimator Clustering (IMLEC) method\nHyperparameters automatically tuned; Outliers removed\nRobust gaussian mixture clustering algorithm\nWebpage has links to paper, Coretto and Hennig, 2016\n\n\n\n\nComponents of the Algorithm\n\n‚ÄúColor‚Äù points according to gaussians (clusters)\n\nThe closer a point is to the center of a gaussian the more intensely it matches the color of that gaussian\nPoints in between gaussians are a mixture or proportion of the colors of each gaussian\n\n\nFitting a Gaussian\n\nFind the center of mass\n\n2-dim: calculate the mean of x and the mean of y and that‚Äôs the coordinates of your center of mass\n\nFind the spread of the points\n\n2-dim: calculate the x-variance, y-variance, and covariance\n\n\nFirst Equation: Height of Gaussian (Multivariate Gaussian distribution equation).\nSecond Equation: 1-D gaussian equation that‚Äôs just being used for reference\n\nPartially ‚Äúcolored‚Äù points affect spread and center of mass calculations\n\nFully colored points ‚Äúweigh‚Äù more than partially colored points and pull the center of mass and change the orientation\n\n\n\n\n\nSteps\n\nStart with random Gaussians\n\nEach gaussian has random means, variances\n\nColor points according to distance to the random gaussians\n\nThe heights in the distributions pic above\n\n(Forget about old gaussians) Calculate new gaussians based on the colored points\n(Forget about old colors) Color points according to distance to the new gaussians\nRepeat until some threshold is reached (i.e.¬†gaussians or colors don‚Äôt change much)\n\nTuning\n\nInitial Conditions (i.e.¬†Good starting points for the random gaussians at the beginning)\nLimits on the mean and variance calculations\nNumber of gaussians, k, can be chosen by minimizing the Davies-Bouldin score\n\nSee Diagnostics, Clustering &gt;&gt; Spherical/Centroid Based &gt;&gt; Davies-Bouldin Index\n\nRunning algorithm multiple times\n\nLike CV grid search algs or bootstrapping",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-lpa",
    "href": "qmd/clustering-general.html#sec-clust-gen-lpa",
    "title": "General",
    "section": "Latent Profile Analysis (LPA)",
    "text": "Latent Profile Analysis (LPA)\n\nSort of like k-means + GMM\nk number of profiles (i.e.¬†clusters) are chosen\nModel outputs probabilities that an observation belongs to any particular cluster\nGOF metrics available\n‚ÄúAs with Exploratory Factor Analysis (EFA )(and other latent-variable models), the assumption of LPA is that the latent (unobserved) factor‚Äùcauses‚Äù (I‚Äôm using the term loosely here) observed scores on the indicator variables. So, to refer back to my initial hypothetical example, a monster being a spell caster (the unobserved class) causes it to have high intelligence, low strength, etc. rather than the inverse. This is a worthwhile distinction to keep in mind, since it has implications for how the model is fit.‚Äù\nBin variables that might dominate the profile. This way the profiles will represent a latent variable and not gradations of the dominate variable (e.g.¬†low, middle, high values of the dominate variable).\nCenter other variable observations according to dominant variable bin those observations are in. (e.g.¬†subtract values in bin1 from bin1‚Äôs mean)\n# From D&D article where challenge_rating is a likely dominant variable\nmons_bin &lt;- mons_df %&gt;%\n¬† mutate(cr_bin = ntile(x = challenge_rating, n = 6))\nab_scores &lt;- c(\"strength\", \"dexterity\", \"constitution\", \"intelligence\", \"wisdom\", \"charisma\")¬†\nmons_bin &lt;- mons_bin %&gt;%\n¬† group_by(cr_bin) %&gt;%\n¬† mutate(across(.cols = ab_scores, .fns = mean, .names = \"{.col}_bin_mean\")) %&gt;%\n¬† ungroup()",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-tsne",
    "href": "qmd/clustering-general.html#sec-clust-gen-tsne",
    "title": "General",
    "section": "tSNE",
    "text": "tSNE\n\nPackages\n\n{Rtsne}\n\nt-Distributed Stochastic Neighbor Embedding\nLooks at the local distances between points in the original data space and tries to reproduce them in the low-dimensional representation\n\nBoth UMAP and tSNE attempt to do this but fails (Lior Pachter paper thread, Doesn‚Äôt preserve local structure, No theorem says that it preserves topology)\n\nResults depend on a random starting point\nTuning parameters: perplexity",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-umap",
    "href": "qmd/clustering-general.html#sec-clust-gen-umap",
    "title": "General",
    "section": "UMAP",
    "text": "UMAP\n\nPackages:\n\n{umap}\n{scDEED} (article) - Detects Dubious t-SNE and UMAP Embeddings and Optimizes Hyperparameters\n\nscDEED assigns a reliability score to each 2D embedding to indicate how much the data point‚Äôs mid-range neighbors change in the 2D space. Observations whose 2D embedding neighbors have been drastically changed through the embedding process are called ‚Äòdubious.‚Äô\n\n\nUniform Manifold Approximation and Projection\nSee tSNE section for Lior Pachter threads on why not to use tSNE or UMAP\nPreprocessing\n\nOnly for numeric variables\nStandardize\n\nProjects variables to a nonlinear space\nVariation of tSNE\n\nRandom starting point has less of an impact\n\nCan be supervised (give it an outcome variable)\nComputationally intensive\nLow-dimensional embedding cannot be interpreted\n\nNo rotation matrix plot like in PCA\n\nTry pca - linear method (fast)\n\nIf successful (good separation between categories), then prediction may be easier\nIf not, umap, tsne needed\n\nUMAP can taking training model and apply it to test data or new data (tSNE can‚Äôt)\nTuning parameter: neighbors\n\nExample used 500 iterations (n_epochs) as limit for convergence",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-kmeans",
    "href": "qmd/clustering-general.html#sec-clust-gen-kmeans",
    "title": "General",
    "section": "K-Means",
    "text": "K-Means\n\nSeeks to assign n points to k clusters and find cluster centers so as to minimize the sum of squared distances from each point to its cluster center.\nFor choosing the number of clusters, elbow method (i.e.¬†WSS) is usually awful if there are more than few clusters. Recommended: Calinski-Harabasz Index and BIC then Silhouette Coefficient or Davies-Bouldin Index (See Diagnostics, Clustering &gt;&gt; Spherical/Centroid Based (article)\nBase R kmeans uses the Hartigan-Wong algorithm\n\nFor large k and larger n, the density of cluster centers should be proportional to the density of the points to the power (d/d+2). In other words the distribution of clusters found by k-means should be more spread out than the distribution of points. This is not in general achieved by commonly used iterative schemes, which stay stuck close to the initial choice of centers.\n\n{tidyclust}\n\nEngines\n\nstats and ClusterR run classical K-means\nlaR¬†runs K-Modes models which are the categorical analog to K-means, meaning that it is intended to be used on only categorical data\nclustMixType¬†to run K-prototypes which are the more general method that works with categorical and numeric data at the same time.\n\nExample: Mixed K-Means\nlibrary(tidymodels)\nlibrary(tidyclust)\n\ndata(\"ames\", package = \"modeldata\")\n\nkproto_spec &lt;- k_means(num_clusters = 3) %&gt;%\n  set_engine(\"clustMixType\")\n\nkproto_fit &lt;- kproto_spec %&gt;%\n  fit(~ ., data = ames)\n\nkproto_fit %&gt;%\n  extract_centroids() %&gt;%\n  select(11:20) %&gt;%\n  glimpse()\n#&gt; Rows: 3\n#&gt; Columns: 10\n#&gt; $ Lot_Config     &lt;fct&gt; Inside, Inside, Inside\n#&gt; $ Land_Slope     &lt;fct&gt; Gtl, Gtl, Gtl\n#&gt; $ Neighborhood   &lt;fct&gt; College_Creek, North_Ames, Northridge_Heights\n#&gt; $ Condition_1    &lt;fct&gt; Norm, Norm, Norm\n#&gt; $ Condition_2    &lt;fct&gt; Norm, Norm, Norm\n#&gt; $ Bldg_Type      &lt;fct&gt; OneFam, OneFam, OneFam\n#&gt; $ House_Style    &lt;fct&gt; Two_Story, One_Story, One_Story\n#&gt; $ Overall_Cond   &lt;fct&gt; Average, Average, Average\n#&gt; $ Year_Built     &lt;dbl&gt; 1989.977, 1953.793, 1998.765\n#&gt; $ Year_Remod_Add &lt;dbl&gt; 1995.934, 1972.973, 2003.035",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-ann",
    "href": "qmd/clustering-general.html#sec-clust-gen-ann",
    "title": "General",
    "section": "Approximate Nearest Neighbor (ANN)",
    "text": "Approximate Nearest Neighbor (ANN)\n\nkNN runs at O(N*K), where N is the number of items and K is the size of each embedding. Approximate nearest neighbor (ANN) algorithms typically drop the complexity of a lookup to O(log(n)).\nMisc\n\nAlso see Maximum inner product search using nearest neighbor search algorithms\n\nIt shows a preprocessing transformation that is performed before kNN to make it more efficient\nIt might already be implemented in ANN algorithms\n\n\nCommonly used in Recommendation algs to cluster user-item embeddings at the end. Also, any NLP task where you need to do a similarity search of one character embedding to other character embeddings.\nGenerally uses one of two main categories of hashing methods: either data-independent methods, such as locality-sensitive hashing (LSH); or data-dependent methods, such as Locality-preserving hashing (LPH)\nLocality-Sensitive Hashing (LSH)\n\nHashes similar input items into the same ‚Äúbuckets‚Äù with high probability.\nThe number of buckets is much smaller than the universe of possible input items\nHash collisions are maximized, not minimized, where a collision is where two distinct data points have the same hash.\n\nSpotify‚Äôs Annoy\n\nUses a type of LSH, Random Projections Method (RPM) (article didn‚Äôt explain this well)\nL RPM hashing functions are chosen. Each data point, p, gets hashed into buckets in each of the L hashing tables. When a new data point, q, is ‚Äúqueried,‚Äù it gets hash into buckets like p did. All the hashes in the same buckets of p are pulled and the hashes within a certain threshold, c*R, are considered nearest neighbors.\n\nWiki article on LSH and RPM clears it up a little, but I‚Äôd probably have to go to Spotify‚Äôs paper to totally make sense of this.\n\nAlso the Spotify alg might bring trees/forests into this somehow\n\nFacebook AI Similarity Search (FAISS)\n\nHierarchical Navigable Small World Graphs (HNSW)\nHNSW has a polylogarithmic time complexity (O(logN))\nTwo approximations available Embeddings are clustered and centroids are calculated. The k nearest centroids are returned.\n\nEmbeddings are clustered into veroni cells. The k nearest embeddings in a veroni cell or a region of veroni cells is returned.\n\nBoth types of approximations have tuning parameters.\n\nInverted File Index + Product Quantization (IVFPQ)(article)",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-dbscan",
    "href": "qmd/clustering-general.html#sec-clust-gen-dbscan",
    "title": "General",
    "section": "DBSCAN",
    "text": "DBSCAN\n\n\nMisc\n\nNotes from:\n\nUnderstanding DBSCAN and Implementation with Python\nClustering with DBSCAN, Clearly Explained video\n\nPackages\n\n{dbscan}\n{parameters}\n\nn_clusters_dbscan - Given a ‚Äúmin_size‚Äù (aka minPts?), the function estimates the optimal ‚Äúeps‚Äù\ncluster_analysis - Shows Sum of Squares metrics and the (standardized) mean value for each variable within each cluster.\n\n\nHDBSCAN is the hierarchical density-based clustering algorithm\nUse Cases\n\nGeospatially Clustering Earthquakes\n\nEvents can occur in irregular shaped clusters (i.e., along faults of different orientations).\nEvents can occur in different densities (i.e.¬†some fault zones are more active than others).\nEvents can occur far away from fault zones (i.e.¬†outliers)\n\n\n\nTuning\n\neps - The maximum distance between two samples for one to be considered to be connected to the other\n\nLarge eps tend to include more points within a cluster,\nToo-large eps will include everything in the same single cluster\nToo-small eps will result in no clustering at all\n\nminPts (or min_samples) - The minimum number of samples in a neighborhood for a point to be considered as a core point\n\nToo-small minPts is not meaningful because it will regard every point as a core point.\nLarger minPts can be better to deal with noisy data\n\n\nAlgorithm\n\nFor each data point, find the points in the neighborhood within eps distance, and define the core points as those with at least minPts neighbors.\n\n\nThe orange circle represents the eps area\nIf minPts = 4, then the top 4 points are core points because they have at least 4 points overlapping the eps area\n\nDefine groups of connected core points as clusters.\n\n\nAll the green points have been labelled as core points\n\nAssign each non-core point to a nearby cluster if it‚Äôs directly reachable from a neighboring core point, otherwise define it as an outlier.\n\n\nThe black points are non-core points but are points that overlap the eps area for the outer-most core points.\nAdding these black points finalizes the first cluster\n\nThis process is repeated for the next group of core points and continues until all that‚Äôs left are outliers.\n\nAdvantages\n\nDoesn‚Äôt require users to specify the number of clusters.\nNot sensitive to outliers.\nClusters formed by DBSCAN can be any shape, which makes it robust to different types of data.\n\nExample: Nested Cluster Structure\n\nK-Means\n\n\nK-Means wants spherical clusters which makes it grab groups of points it shouldn‚Äôt\n\nDBSCAN\n\n\nAble correctly identify the oblong shaped cluster\n\n\n\n\nDisadvantages\n\nIf the data has a very large variation in densities across clusters because you can only use one pair of parameters, eps and MinPts, on one dataset\nIt could be hard to define eps without the domain knowledge of the data\nClusters not totally reproducible. Clusters are defined sequentially so depending on which group of core points the algorithm starts with and hyperparameter values, some non-core points that are within the eps area of multiple clusters may be assigned to different clusters on different runs of the algorithm.",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-lkhs",
    "href": "qmd/db-lakes.html#sec-db-lakes-lkhs",
    "title": "Lakes",
    "section": "Lakehouse",
    "text": "Lakehouse\n\n\nThe key idea behind a Lakehouse is to be able to take the best of a Data Lake and a Data Warehouse.\n\nData Lakes can in fact provide a lot of flexibility (e.g.¬†handle structured and unstructured data) and low storage cost.\nData Warehouses can provide really good query performance and ACID guarantees.",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html",
    "href": "qmd/db-postgres.html",
    "title": "Postgres",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-misc",
    "href": "qmd/db-postgres.html#sec-db-pstgr-misc",
    "title": "Postgres",
    "section": "",
    "text": "Notes from\n\nCourse: linkedin.learning.postgresql.client.applications\nCourse: Linux.Academy.PostgreSQL.Administration.Deep.Dive\nPostgres is eating the database world\n\nResources\n\nDocs - All on one page so you can just ctrl + f\nPostgreSQL is Enough - Links to various applications/extensions resources\n\nEverything is case sensitive, so use lowercase for db and table names\nCheck postgres sql version - psql --version or -V\nSee flag options - psql --help\nIf there‚Äôs a ‚Äú#‚Äù in the prompt after logging into a db, then that signifies you are a super-user\nMeta commands (i.e.¬†commands once you‚Äôre logged into the db)\n\n\\du - list roles (aka users + permissions)\n\\c  - switches databases\n\\password  - assign a password to a user (prompt will ask for the password twice)\n\nCan also use ALTER ROLE for this but the password will then be in the log\n\n\nUnlogged Table - Data written to an unlogged table will not be logged to the write-ahead-log (WAL), making it ideal for intermediate tables and considerably faster. Note that unlogged tables will not be restored in case of a crash, and will not be replicated.\nPigsty\n\nOpen Source RDS alternative\nAims to harness the collective power of PostgreSQL ecosystem extensions and democratize access to high-quality database services.",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-gstgr-ext",
    "href": "qmd/db-postgres.html#sec-db-gstgr-ext",
    "title": "Postgres",
    "section": "Extensions",
    "text": "Extensions\n\npg_analytics\n\nIntro, Repo\nArrow and Datafusion integrated with Postgres\nDelta Lake tables behave like regular Postgres tables but use a column-oriented layout via Apache Arrow and utilize Apache DataFusion, a query engine optimized for column-oriented data\nData is persisted to disk with Parquet\nThe delta-rs library is a Rust-based implementation of Delta Lake. This library adds ACID transactions, updates and deletes, and file compaction to Parquet storage. It also supports querying over data lakes like S3, which introduces the future possibility connecting Postgres tables to cloud data lakes.\n\npg_bm25\n\nIntro, Repo\nRust-based extension that significantly improves Postgres‚Äô full text search capabilities\n\nBuilt to be an Elasticsearch inside of a postgres db\n\nPerformant on large tables, adds support for operations like fuzzy search, relevance tuning, or BM25 relevance scoring (same algo as Elasticsearch), real-time search ‚Äî new data is immediately searchable without manual reindexing\n\nQuery times over 1M rows are 20x faster compared to tsquery and ts_ran (built-in search and sort)\n\nCan be combined with PGVector for semantic fuzzy search\n\nCitus\n\nWebsite\nDistributed Postgres\nTransforms a standalone cluster into a horizontally partitioned distributed database cluster.\nScales Postgres by distributing data & queries. You can start with a single Citus node, then add nodes & rebalance shards when you need to grow.\nCan combine with PostGIS for a distributed geospatial database, PGVector for a distributed vector database, pg_bm25 for a distributed full-text search database, etc.\n\nduckdb_fdw\n\nRepo\nForeign Data Wrapper (FDW) to connect PostgreSQL to DuckDB database file.\nA Foreign Data Wrapper (FDW) in PostgreSQL is a mechanism that allows you to access and query data stored in external data sources (e.g.¬†duckdb) from within a PostgreSQL database.\n\nplprql\n\nRepo\nEnables you to run PRQL queries. PRQL has a syntax that is similar to {dplyr}\nBuilt in Rust so you have to have pgrx installed. Repo has directions.\n\npgrx\n\nRepo\nFramework for developing PostgreSQL extensions in Rust\nTo install extensions built in Rust, you need to have this extension installed\n\npg_sparse\n\nIntro, Repo\nEnables efficient storage and retrieval of sparse vectors using HNSW\n\nSPLADE outputs sparse vectors with over 30,000 entries. Sparse vectors can detect the presence of exact keywords while also capturing semantic similarity between terms.\n\nFork of pgvector with modifications\nCompatible alongside both pg_bm25 and pgvector\n\npgvector\n\nRepo\nAlso see Databases, Vector Databases for alternatives and comparisons\nEnables efficient storage and retrieval of dense vectors using HNSW\n\nOpenAI‚Äôs text-embedding-ada-002 model outputs dense vectors with 1536 entries\n\nExact and Approximate Nearest Neighbor search\nL2 distance, Inner Product, and Cosine Distance\nSupported inside AWS RDS\n\npg_vectorize\n\nRepo\nWorkflows for both vector search and RAG\nIntegrations with OpenAI‚Äôs embeddings and chat-completion endpoints and a self-hosted container for running Hugging Face Sentence-Transformers\nAutomated creation of Postgres triggers to keep your embeddings up to date\nHigh level API - one function to initialize embeddings transformations, and another function to search",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-dock",
    "href": "qmd/db-postgres.html#sec-db-pstgr-dock",
    "title": "Postgres",
    "section": "Docker",
    "text": "Docker\n\nSteps\n\nStart docker desktop\nStart powershell\ndocker run --name pg_database -p 5432:5432 -e POSTGRES_PASSWORD=ericb2022 -d postgres:latest\n\n1st 5432 is local computer port\n2nd 5432 is the required postgres image port\n-e is for defining an environment variable; here its the db password that I set to ericb2022\n-d\n\nRuns the container in the background\nAllows you to run commands in the same terminal window that you used the container run command in\n\n‚Äúpostgres:latest‚Äù is the name of the image to build the container from\n\nClose powershell\nIn docker desktop, the ‚Äúpg_database‚Äù container should be running\n\nConnect to the db\n\nSteps\n\npsql should be in your list of path environment variables\n\nRight-click Start &gt;&gt; System &gt;&gt; advanced settings (right panel) &gt;&gt; environment variables &gt;&gt; highlight path &gt;&gt; edit\n‚ÄúC:\\Program Files\\PostgreSQL\\14\\bin‚Äù\n\n** Note the ‚Äú14‚Äù in the path which is the current version. Therefore, when postgres is updated, this path will have to be updated **\n\n\npsql --host localhost --port 5432 --dbname postgres --username postgres\n\nNote these are all default values, so this is equivalent to psql -U postgres\n‚Äìhost (-h) is the ip address or computer name that you want to connect to\n\nlocalhost is for the docker container that‚Äôs running\n\n5432 is the default ‚Äìport (-p) for a postgres container\n‚Äìdbname (-d) is the name of the database on the server\n\n‚Äúpostgres‚Äù is a db that ships with postgres\n\n‚Äìusername (-U) is a username that has permission to access the db\n\n‚Äúpostgres‚Äù is the default super-user name\n\n\nA prompt will then ask you for that username‚Äôs password\n\nThe container above has the password ericb2022\n\nThis didn‚Äôt work for me, needed to use my postgres password that I set-up when I installed postgres and pgAdmin.\nMy local postgres server and the container are listening on the same port, so maybe if I changed the first port number to something else, it would connect to the container.\n\n\nTo exit db, \\q\n\n\nCreate a db\n\nSteps\n\ncreatedb -h localhost -p 5432 -U postgres -O eric two_trees\n\n-U is the user account used to create the db\n-O is used to assign ownership to another user account\n\n‚Äúrole‚Äù (i.e.¬†user account) must already exist\n\n‚Äútwo_trees‚Äù is the name of the new db\nYou will be prompted for user‚Äôs password\n\nList of dbs on the server\n\npsql -h localhost -p 5432 -U postgres -l\n\n-l lists all dbs on server\nYou will be prompted for user‚Äôs password\n\n\n\n\nRun a sql script\n\npsql -d acweb -f test.sql\n\n-d is for the database name (e.g.¬†acweb)\n-f is for running a file (e.g.¬†test.sql)\n\n\nAdd users\n\nCreate user/role (once inside db)\nCREATE USER &lt;user name1&gt;;\nCREATE ROLE &lt;user name2&gt;;\nALTER ROLE &lt;user name2&gt; LOGIN\n\nCREATE USER will give the user login attribute/permission while CREATE ROLE will not\n\nALTER ROLE gives the user attributes/permissions (e.g.¬†login permission)\n\nCreate user/role (at the CLI) - createuser &lt;user name&gt;",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-pgadm",
    "href": "qmd/db-postgres.html#sec-db-pstgr-pgadm",
    "title": "Postgres",
    "section": "pgAdmin",
    "text": "pgAdmin\n\nCreate a server\n\nRight-click on servers &gt;&gt; create &gt;&gt; server\n\nGeneral tab &gt;&gt; enter name\nConnection tab\n\nHost name/address: computer name or ip address where the server is running\n\nlocal: localhost or 127.0.0.1\n\nPort: default = 5432\nMaintenance database: db you want to connect to\n\nIf you haven‚Äôt created it yet, just use default ‚Äúpostgres‚Äù which autmatically created during installation\n\nusername/password\n\nu: default is postgres\np: installation password\nTick Save password\n\n\nClick Save\n\n\nCreate a db\n\nRight-click databases &gt;&gt; create &gt;&gt; databases &gt;&gt; enter name (lowercase) and click save\n\nCreate a table\n\nVia gui\n\nClick db name &gt;&gt; schema &gt;&gt; public &gt;&gt; right-click tables &gt;&gt; create &gt;&gt; tables\nGeneral tab\n\nEnter the table name (lower case)\n\nColumns tab\n\nEnter name, data type, whether there should be a ‚ÄúNot Null‚Äù constraint, and whether it‚Äôs a primary key\nAdd additional column with ‚Äú+‚Äù icon in upper right\nIf you‚Äôre going to fill the table with a .csv file, make sure the column names match\n\nClick save\nTable will be located at db name &gt;&gt; schema &gt;&gt; public &gt;&gt; tables\n\nVia sql\n\nOpen query tool\n\nRight-click  or Schemas or Tables &gt;&gt; query tool\nClick Tools menu dropdown (navbar) &gt;&gt; query tool\n\nRun CREATE TABLE statement\n\nIf you don‚Äôt include the schema as part of the table name, pgadmin automatically places it into the ‚Äúpublic‚Äù schema directory (e.g.¬†public.table_name)\n\n\n\nImport csv into an empty table\n\nMake sure the column names match\nRight-click table name &gt;&gt; import/export\nOptions tab\n\nMake sure import is selected\nSelect the file\nIf you have column names in your csv, select Yes for Header\nSelect ‚Äú,‚Äù for the Delimiter\n\nColumns tab\n\nCheck to make sure all the column names are there\n\nClick OK\n\nQuery Table\n\nRight-click table &gt;&gt; query editor\nQuery editor tab\n\nType query &gt;&gt; click ‚ñ∂ to run query",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-rds",
    "href": "qmd/db-postgres.html#sec-db-pstgr-rds",
    "title": "Postgres",
    "section": "AWS RDS",
    "text": "AWS RDS\n\nMisc\n\nNotes from Create an RDS Postgres Instance and connect with pgAdmin\n\nSteps\n\nSearch AWS services for ‚ÄúRDS‚Äù (top left navbar)\nCreate Database\n\nClick ‚ÄúCreate Database‚Äù\n\nCreate Database\n\nChoose Standard create or Easy Create\n\nEasy Create - uses ‚Äúbest practices‚Äù settings\n\nSelect postgres\n\nAlso available: Amazon Aurora, MySQL, MariaDB, Oracle, Microsoft SQL Server\n\nTemplates\n\nProduction\n\nMulti-AZ Deployment - Multiple Availability Zones\nProvisioned IOPS Storage - Increased output\n\nDev/Test\nRree tier\n\n750 hrs of Amazon RDS in a Single-AZ db.t2.micro Instance.\n20 GB of General Purpose Storage (SSD).\n20 GB for automated backup storage and any user-initiated DB Snapshots.\n\nRDS pricing page\n\nSettings\n\nDB Instance Identifier - enter name\nSet master username, master username password\n\nDB Instance\n\ndb.t3.micro or db.t4g.micro for free tier\n\ndev/test, production has many other options\n\n\nStorage\n\nDefaults: SSD with 20GB\nAutoscaling can up the storage capacity to a default 1000GB",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-py",
    "href": "qmd/db-postgres.html#sec-db-pstgr-py",
    "title": "Postgres",
    "section": "Python",
    "text": "Python\n\n{{psycopg2}}\n\nMisc\n\nNotes from Fastest Way to Load Data Into PostgreSQL Using Python\ntl;dr\n\nLarge Data: use copy_to\nMedium to Small Data:\n\nTime and memory isn‚Äôt an issue: Use extract_values or maybe copy_to if you don‚Äôt have JSON.\n\n\n\nConnect to db\nimport psycopg2\n\nconnection = psycopg2.connect(\n    host=\"localhost\",\n    database=\"testload\",\n    user=\"haki\",\n    password=None,\n)\nconnection.autocommit = True\nCreate a table\ndef create_staging_table(cursor) -&gt; None:\n    cursor.execute(\"\"\"\n        DROP TABLE IF EXISTS staging_beers;\n        CREATE UNLOGGED TABLE staging_beers (\n            id                  INTEGER,\n            name                TEXT,\n            tagline             TEXT,\n            first_brewed        DATE,\n            description         TEXT,\n            image_url           TEXT,\n            abv                 DECIMAL,\n            ibu                 DECIMAL,\n            target_fg           DECIMAL,\n            target_og           DECIMAL,\n            ebc                 DECIMAL,\n            srm                 DECIMAL,\n            ph                  DECIMAL,\n            attenuation_level   DECIMAL,\n            brewers_tips        TEXT,\n            contributed_by      TEXT,\n            volume              INTEGER\n        );\n    \"\"\")\n\nwith connection.cursor() as cursor:\n  create_staging_table(cursor)\n\nThe function receives a cursor and creates a unlogged table called staging_beers.\n\nInsert many rows at once\n\nNotes from Fastest Way to Load Data Into PostgreSQL Using Python\nThe best way to load data into a database is using the copy command (last method in this section). The issue here is that copy needs a .csv file and not json.\n\nThis might be an issue just because of psycopg2 library doesn‚Äôt support json or that there is a postgres extension that isn‚Äôt supported by the library. This also might not be a problem in the future.\n\nData\nbeers = iter_beers_from_api()\nnext(beers)\n{'id': 1,\n 'name': 'Buzz',\n 'tagline': 'A Real Bitter Experience.',\n 'first_brewed': '09/2007',\n 'description': 'A light, crisp and bitter IPA brewed...',\n 'image_url': 'https://images.punkapi.com/v2/keg.png',\n 'abv': 4.5,\n 'ibu': 60,\n 'target_fg': 1010,\n...\n}\nnext(beers)\n{'id': 2,\n 'name': 'Trashy Blonde',\n 'tagline': \"You Know You Shouldn't\",\n 'first_brewed': '04/2008',\n 'description': 'A titillating, ...',\n 'image_url': 'https://images.punkapi.com/v2/2.png',\n 'abv': 4.1,\n 'ibu': 41.5,\n ...\n }\n\nData is from beers api\niter_beers_from_api is a udf that takes the json from the api and creates a generator object that iterates through each beer.\n\nInsert data in db using execute_values (low memory usage and still pretty fast)\ndef insert_execute_values_iterator(\n    connection,\n    beers: Iterator[Dict[str, Any]],\n    page_size: int = 100,\n) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        psycopg2.extras.execute_values(cursor, \"\"\"\n            INSERT INTO staging_beers VALUES %s;\n        \"\"\", ((\n            beer['id'],\n            beer['name'],\n            beer['tagline'],\n            parse_first_brewed(beer['first_brewed']),\n            beer['description'],\n            beer['image_url'],\n            beer['abv'],\n            beer['ibu'],\n            beer['target_fg'],\n            beer['target_og'],\n            beer['ebc'],\n            beer['srm'],\n            beer['ph'],\n            beer['attenuation_level'],\n            beer['brewers_tips'],\n            beer['contributed_by'],\n            beer['volume']['value'],\n        ) for beer in beers), page_size=page_size)\n\ninsert_execute_values_iterator(page_size=1000)\n\nparse_first_brewed is a udf that transforms a date string to datetime type.\nbeer[‚Äòvolume‚Äô][‚Äòvalue‚Äô]: Data is in json and the value for volume is subsetted from the nested field.\nBenchmark: At page_size = 1000, 1.468s, 0.0MB of RAM used\nThe generator((bear['id'], ‚Ä¶ , bear['volume']['value'], for beer in beers) keeps data from being stored in memory during transformation\npage_size: maximum number of¬†arglist items to include in every statement. If there are more items the function will execute more than one statement.\n\nHere arglist is the data in the form of generator\n\n\nInsert data in db using copy_from (Fast but memory intensive)\nimport io\n\ndef clean_csv_value(value: Optional[Any]) -&gt; str:\n    if value is None:\n        return r'\\N'\n    return str(value).replace('\\n', '\\\\n')\n\ndef copy_stringio(connection, beers: Iterator[Dict[str, Any]]) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        csv_file_like_object = io.StringIO()\n        for beer in beers:\n            csv_file_like_object.write('|'.join(map(clean_csv_value, (\n                beer['id'],\n                beer['name'],\n                beer['tagline'],\n                parse_first_brewed(beer['first_brewed']),\n                beer['description'],\n                beer['image_url'],\n                beer['abv'],\n                beer['ibu'],\n                beer['target_fg'],\n                beer['target_og'],\n                beer['ebc'],\n                beer['srm'],\n                beer['ph'],\n                beer['attenuation_level'],\n                beer['contributed_by'],\n                beer['brewers_tips'],\n                beer['volume']['value'],\n            ))) + '\\n')\n        csv_file_like_object.seek(0)\n        cursor.copy_from(csv_file_like_object, 'staging_beers', sep='|')\n\nclean_csv_value: Transforms a single value\n\nEscape new lines: some of the text fields include newlines, so we escape \\n -&gt; \\\\n.\nEmpty values are transformed to \\N: The string \"\\N\" is the default string used by PostgreSQL to indicate NULL in COPY (this can be changed using the NULL option).\n\ncsv_file_like_object: Generate a file like object using io.StringIO. A StringIO object contains a string which can be used like a file. In our case, a CSV file.\ncsv_file_like_object.write: Transform a beer to a CSV row\n\nTransform the data: transformations on first_brewed and volume are performed here.\nPick a delimiter: Some of the fields in the dataset contain free text with commas. To prevent conflicts, we pick ‚Äú|‚Äù as the delimiter (another option is to use QUOTE).\n\n\nInsert data (streaming) in db using copy_from (Fastest and low memory but complicated, at least with json)\n\nBuffering function\nfrom typing import Iterator, Optional\nimport io\n\nclass StringIteratorIO(io.TextIOBase):\n    def __init__(self, iter: Iterator[str]):\n        self._iter = iter\n        self._buff = ''\n\n    def readable(self) -&gt; bool:\n        return True\n\n    def _read1(self, n: Optional[int] = None) -&gt; str:\n        while not self._buff:\n            try:\n                self._buff = next(self._iter)\n            except StopIteration:\n                break\n        ret = self._buff[:n]\n        self._buff = self._buff[len(ret):]\n        return ret\n\n    def read(self, n: Optional[int] = None) -&gt; str:\n        line = []\n        if n is None or n &lt; 0:\n            while True:\n                m = self._read1()\n                if not m:\n                    break\n                line.append(m)\n        else:\n            while n &gt; 0:\n                m = self._read1(n)\n                if not m:\n                    break\n                n -= len(m)\n                line.append(m)\n        return ''.join(line)\n\nThe regular io.StringIO creates a file-like object but is memory-heavy. This function creates buffer that will feed each line of the file into a buffer, stream it to copy, empty the buffer, and load the next line.\n\nCopy to db\ndef clean_csv_value(value: Optional[Any]) -&gt; str:\n    if value is None:\n        return r'\\N'\n    return str(value).replace('\\n', '\\\\n')\n\ndef copy_string_iterator(connection, beers: Iterator[Dict[str,\nAny]]) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        beers_string_iterator = StringIteratorIO((\n            '|'.join(map(clean_csv_value, (\n                beer['id'],\n                beer['name'],\n                beer['tagline'],\n                parse_first_brewed(beer['first_brewed']).isoformat(),\n                beer['description'],\n                beer['image_url'],\n                beer['abv'],\n                beer['ibu'],\n                beer['target_fg'],\n                beer['target_og'],\n                beer['ebc'],\n                beer['srm'],\n                beer['ph'],\n                beer['attenuation_level'],\n                beer['brewers_tips'],\n                beer['contributed_by'],\n                beer['volume']['value'],\n            ))) + '\\n'\n            for beer in beers\n        ))\n        cursor.copy_from(beers_string_iterator, 'staging_beers', sep='|')\n\nSimilar to other code above",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#distributed-architectures",
    "href": "qmd/db-postgres.html#distributed-architectures",
    "title": "Postgres",
    "section": "Distributed Architectures",
    "text": "Distributed Architectures\n\nMisc\n\nNotes from An Overview of Distributed PostgreSQL Architectures\nFeatures to achieve single node availability, durability, and performance - Replication - Place copies of data on different machines - Distribution - Place partitions of data on different machines - Decentralization - Place different DBMS activities on different machines\nIf transactions take on average 20ms, then a single (interactive) session can only do 50 transactions per second. You then need a lot of concurrent sessions to actually achieve high throughput. Having many sessions is not always practical from the application point-of-view, and each session uses significant resources like memory on the database server. Most PostgreSQL set ups limit the maximum number of sessions in the hundreds or low thousands, which puts a hard limit on achievable transaction throughput when network latency is involved.\n\nNetwork-Attached Block Storage (e.g.¬†EBS)\n\n\nCommon technique in cloud-based architectures\nDatabase server typically runs in a virtual machine in a Hypervisor, which exposes a block device to the VM. Any reads and writes to the block device will result in network calls to a block storage API. The block storage service internally replicates the writes to 2-3 storage nodes.\nPros\n\nHigher durability (replication)\nHigher uptime (replace VM, reattach)\nFast backups and replica creation (snapshots)\nDisk is resizable\n\nCons\n\nHigher disk latency (~20Œºs -&gt; ~1000Œºs)\nLower IOPS (~1M -&gt; ~10k IOPS)\nCrash recovery on restart takes time\nCost can be high\n\nGuideline: The durability and availability benefits of network-attached storage usually outweigh the performance downsides, but it‚Äôs worth keeping in mind that PostgreSQL can be much faster.\n\nRead Replicas\n\n\nThe most common way of using a replica is to set it up as a hot standby that takes over when the primary fails in a high availability set up.\nHelps you scale read throughput when reads are CPU or I/O bottlenecked by load balancing queries across replicas, which achieves linear scalability of reads and also offloads the primary, which speeds up writes!\n\nThe primary usually does not wait for replication when committing a write, which means read replicas are always slightly behind. That can become an issue when your application does a read that, from the user‚Äôs perspective, depends on a write that happened earlier.\nFor example, a user clicks ‚ÄúAdd to cart‚Äù, which adds the item to the shopping cart and immediately sends the user to the shopping cart page. If reading the shopping cart contents happens on the read replica, the shopping cart might then appear empty. Hence, you need to be very careful about which reads use a read replica.\n\nWhen load balancing between different nodes, clients might repeatedly get connected to different replica and see a different state of the database\nPowerful tool for scaling reads, but you should consider whether your workload is really appropriate for it.\nPros\n\nRead throughput scales linearly\nLow latency stale reads if read replica is closer than primary\nLower load on primary\n\nCons\n\nEventual read-your-writes consistency\nNo monotonic read consistency\nPoor cache usage\n\nGuideline: Consider using read replicas when you need &gt;100k reads/sec or observe a CPU bottleneck due to reads, best avoided for dependent transactions and large working sets.\n\nDBMS-Optimized Cloud Storage\n\n\nWhere DBMS is Database Management Software. (e.g.¬†Aurora)\nPostgreSQL is not optimized for this architecture\nWhile the theory behind DBMS-optimized storage is sound. In practice, the performance benefits are often not very pronounced (and can be negative), and the cost can be much higher than regular network-attached block storage. It does offer a greater degree of flexibility to the cloud service provider, for instance in terms of attach/detach times, because storage is controlled in the data plane rather than the hypervisor.\nPros\n\nPotential performance benefits by avoiding page writes from primary\nReplicas can reuse storage, incl.¬†hot standby\nCan do faster reattach, branching than network-attached storage\n\nCons\n\nWrite latency is high by default\nHigh cost / pricing\nPostgreSQL is not designed for it, not OSS\n\nGuideline: Can be beneficial for complex workloads, but important to measure whether price-performance under load is actually better than using a bigger machine.\n\nActive-Active (e.g.¬†BDR)\n\n\nAny node can locally accept writes without coordination with other nodes.\nIt is typically used with replicas in multiple sites, each of which will then see low read and write latency, and can survive failure of other sites.\nActive-active systems do not have a linear history, even at the row level, which makes them very hard to program against.\nPros\n\nVery high read and write availability\nLow read and write latency\nRead throughput scales linearly\n\nCons\n\nEventual read-your-writes consistency\nNo monotonic read consistency\nNo linear history (updates might conflict after commit)\n\nGuideline: Consider only for very simple workloads (e.g.¬†queues) and only if you really need the benefits.\n\nTransparent Sharding (e.g.¬†Citus)\n\n\nTables distributed and/or replicated across multiple primary nodes using a ‚Äúshard key .‚Äù\n\nEach node shows the distributed tables as if they were regular PostgreSQL tables and queries\n\nData are located in ‚Äúshards‚Äù which are regular PostgreSQL tables. Joins and foreign keys that include the shard key can be performed locally.\nScaling out transactional workloads is most effective when queries have a filter on the shard key, such that they can be routed to a single shard group (e.g.¬†single tenant in a multi-tenant app) or compute-heavy analytical queries that can be parallelized across the shards (e.g.¬†time series / IoT).\nWhen loading data, use COPY, instead of INSERT, to avoid waiting for every row.\nPros\n\nScale throughput for reads & writes (CPU & IOPS)\nScale memory for large working sets\nParallelize analytical queries, batch operations\n\nCons\n\nHigh read and write latency\nData model decisions have high impact on performance\nSnapshot isolation concessions\n\nGuideline: Use for multi-tenant apps, otherwise use for large working set (&gt;100GB) or compute heavy queries.\n\nDistributed Key-Value Stores With SQL (e.g.¬†Yugabyte)\n\n\nA bunch of complicated stuff I don‚Äôt understand üòÖ\nTables are stored in the key-value store, with the key being a combination of the table ID and the primary key.\nBetter to use PostgresSQL without this architecture.\nPros\n\nGood read and write availability (shard-level failover)\nSingle table, single key operations scale well\nNo additional data modeling steps or snapshot isolation concessions\n\nCons\n\nMany internal operations incur high latency\nNo local joins in current implementations\nNot actually PostgreSQL, and less mature and optimized\n\nGuideline: Just use PostgreSQL. For simple applications, the availability and scalability benefits can be useful.",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/missingness.html",
    "href": "qmd/missingness.html",
    "title": "Missingness",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-misc",
    "href": "qmd/missingness.html#sec-missing-misc",
    "title": "Missingness",
    "section": "",
    "text": "Also see\n\nEDA &gt;&gt; Missingness\nModel Building, tidymodels &gt;&gt; Recipe &gt;&gt; Imputation\n\nBagging and knn methods for cross-sectional data\nRolling method for time series data\n\nHarrell RMS 3.5 Strategies for Developing an Imputation Model\n\nPackages\n\n{mice} (Multivariate Imputation by Chained Equations) - Imputes mixes of continuous, binary, unordered categorical and ordered categorical data\n\nBased on Fully Conditional Specification, where each incomplete variable is imputed by a separate model.\nImpute continuous two-level data, and maintain consistency between imputations by means of passive imputation.\nMany diagnostic plots are implemented to inspect the quality of the imputations.\n\n{naniar} - Tidyverse compliant methods to summarize, visualize, and manipulate missing data.\n{simputation} - Model-based, multivariate, donar, and simple stat methods available\n{NPBayesImputeCat}: Non-Parametric Bayesian Multiple Imputation for Categorical Data\n\nProvides routines to i) create multiple imputations for missing data and ii) create synthetic data for statistical disclosure control, for multivariate categorical data, with or without structural zeros\nImputations and syntheses are based on Dirichlet process mixtures of multinomial distributions, which is a non-parametric Bayesian modeling approach that allows for flexible joint modeling\nVignette\n\n\n‚ÄúBut more precisely, even having the correct model of the analysis stage does not absolve the analyst of considering the relationship between the imputation stage variables, the causal model, and the missingness mechanism. It turns out that in this simple example, imputing with an analysis-stage collider is innocuous (so long as it is excluded at the analysis stage). But imputation-stage colliders can wreck MI even if they are excluded from the analysis stage.‚Äù\n\nSee Multiple Imputation with Colliders\n\n**Don‚Äôt impute missing values before your training/test split\nImputing Types full-information maximum likelihood\n\nMultiple imputation\nOne-Step Bayesian imputation\n\nMissness Types (MCAR, MAR, and MNAR)\n\nMultivariate Imputation with Chained Equation (MICE) assumes MAR\n\nMethod entails creating multiple imputations for each missing value as opposed to just one. The algorithm addresses statistical uncertainty and enables users to impute values for data of different types.\n\nStochastic Regression Imputation is problematic\n\nPopular among practitioners though\nIssues\n\nStochastic regression imputation might lead to implausible values (e.g.¬†negative incomes).\nStochastic regression imputation has problems with heteroscedastic data\n\nBayesian PMM handles these issues\n\nMissingness in RCT due dropouts (aka loss to follow-up)\n\nNotes from To impute or not: the case of an RCT with baseline and follow-up measurements\n\n{mice} used for imputation\n\nBias in treatment effect due to missingness\n\nIf there are adjustment variables that affect unit dropout then bias increases as variation in treatment effect across units increases (aka hetergeneity)\n\nIn the example, a baseline measurement of the outcome variable, used an explanatory variable, was also causal of missingness. Greater values of this variable resulted in greater bias\nUsing multiple imputation resulted in less bias than just using complete cases, but still underestimated the treatment effect.\n\nIf there are no such variables, then there is no bias due to hetergeneous treatment effects\n\nComplete cases of the data can be used\n\n\nLast observation carried forward\n\nSometimes used in clinical trials because it tends to be conservative, setting a higher bar for showing that a new therapy is significantly better than a traditional therapy.\nMust assume that the previous value (e.g.¬†2008 score) is similar to the ahead value (e.g.¬†2010 score).\nInformation about trajectories over time is thrown away.\n\n\nAssessment of Imputations\n\nSee {naniar} vignette - Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations | Journal of Statistical Software",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-caim",
    "href": "qmd/missingness.html#sec-missing-caim",
    "title": "Missingness",
    "section": "Choosing an Imputation Method",
    "text": "Choosing an Imputation Method\n\n** Don‚Äôt use this. Just putting it here to be aware of **) Standard Procedure for choosing an imputation method\n\nIssues\n\nSome methods will be favored based on the metric used\n\nConditional means methods (RMSE)\nConditional medians methos (MAE) Chosen methods tend to artificially strengthen the association between variables. As a consequence, statistical estimation and inference techniques applied to the so-imputed data set can be invalid.\n\n\nSteps\n\nSelect some observations\nSet their status to missing\nImpute them with different methods\nCompare their imputation accuracy\n\nFor numeric variables, RMSE or MAE typically used\nFor categoricals, percentage of correct predictions (PCP)\n\n\n\nInitial Considerations\n\nIf a dataset‚Äôs feature has missing data in more than 80% of its records, it is probably best to remove that feature altogether.\nIf a feature with missing values is strongly correlated with other missing values, it‚Äôs worth considering using advanced imputation techniques that use information from those other features to derive values to replace the missing data.\nIf a feature‚Äôs values are missing not at random (MNAR), remove methods like MICE from consideration. I-Score {Iscores}, Paper\nA proper scoring rule metric\nConsistent for MCAR, but MAR requires additional assumptions\n\n‚Äúvalid under missing at random (MAR) if we restrict the random projections in variable space to always include all variables, which in turn requires access to some complete observations‚Äù\n\nKinda complicated. I need to read the paper",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-bayes",
    "href": "qmd/missingness.html#sec-missing-bayes",
    "title": "Missingness",
    "section": "Bayesian",
    "text": "Bayesian\n\nPredictive Mean Matching (PMM)\n\nNotes from:\n\nPredictive Mean Matching Imputation (Theory & Example in R)\nPredictive Mean Matching Imputation in R (mice Package Example)\n\nUses a bayesian regression to predict a missing value, then randomly picks a value from a group of observed values that are closest to the predicted value.\nSteps\n\nEstimate a linear regression model:\n\nUse the variable we want to impute as Y.\nUse a set of good predictors as \\(X\\) (Guidelines for the selection of \\(X\\) can be found in van Buuren, 2012, p.¬†128).\nUse only the observed values of \\(X\\) and \\(Y\\) to estimate the model.\n\nDraw randomly from the posterior predictive distribution of \\(\\hat \\beta\\) and produce a new set of coefficients \\(\\beta^*\\).\n\nThis bayesian step is needed for all multiple imputation methods to create some random variability in the imputed values.\n\nCalculate predicted values for observed and missing \\(Y\\).\n\nUse \\(\\hat \\beta\\) to calculate predicted values for observed \\(Y\\).\nUse \\(\\beta^*\\) to calculate predicted values for missing \\(Y\\).\n\nFor each case where \\(Y\\) is missing, find the closest predicted values among cases where \\(Y\\) is observed.\n\nExample:\n\n\\(Y_i\\) is missing. Its predicted value is 10 (based on \\(\\beta^*\\)).\nOur data consists of five observed cases of \\(Y\\) with the values 6, 3, 22, 7, and 12.\nIn step 3, we predicted the values 7, 2, 20, 9, and 13 for these five observed cases (based on \\(\\hat \\beta\\)).\nThe predictive mean matching algorithm selects the closest observed values (typically three cases) to our missing value \\(Y_i\\). Hence, the algorithm selects the values 7, 9, and 13 (the closest values to 10).\n\n\nDraw randomly one of these three close cases and impute the missing value \\(Y_i\\) with the observed value of this close case.\n\nExample: Continued\n\nThe algorithm draws randomly from 6, 7, and 12 (the observed values that correspond to the predicted values 7, 9, and 13).\nThe algorithm chooses 12 and substitutes this value to \\(Y_i\\).\n\n\nIn case of multiple imputation (strongly advised), steps 1-5 are repeated several times.\n\nEach repetition of steps 1-5 creates a new imputed data set.\nWith multiple imputation, missing data is typically imputed 5 times.\n\n\nExample\ndata_imp &lt;- \n  complete(mice(data,\n           m = 5,\n           method = \"pmm\"))\n\nm is the number of times to impute the data\ncomplete formats the data into different shapes according to an action argument\nRunning parmice instead of mice imputes in parallel",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-multimp",
    "href": "qmd/missingness.html#sec-missing-multimp",
    "title": "Missingness",
    "section": "Multiple Imputation Fit",
    "text": "Multiple Imputation Fit\n\nAKA ‚Äúmultiply‚Äù imputed data\nFitting a regression model with multiply imputed data\n\nSee If you fit a model with multiply imputed data, you can still plot the line\nMethods\n\nPredict then Combine (PC)\nCombine then Predict (CP)",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-ts",
    "href": "qmd/missingness.html#sec-missing-ts",
    "title": "Missingness",
    "section": "Time Series",
    "text": "Time Series\n\nIf seasonality is present, mean, median, mode, random assignment, or previous value methods shouldn‚Äôt be used.",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/llms-general.html",
    "href": "qmd/llms-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "LLMs",
      "General"
    ]
  },
  {
    "objectID": "qmd/llms-general.html#misc",
    "href": "qmd/llms-general.html#misc",
    "title": "General",
    "section": "",
    "text": "What chatGPT is:\n\n‚ÄúWhat would a response to this question sound like‚Äù machine Researchers build (train) large language models like GPT-3 and GPT-4 by using a process called ‚Äúunsupervised learning,‚Äù which means the data they use to train the model isn‚Äôt specially annotated or labeled. During this process, the model is fed a large body of text (millions of books, websites, articles, poems, transcripts, and other sources) and repeatedly tries to predict the next word in every sequence of words. If the model‚Äôs prediction is close to the actual next word, the neural network updates its parameters to reinforce the patterns that led to that prediction.\nConversely, if the prediction is incorrect, the model adjusts its parameters to improve its performance and tries again. This process of trial and error, though a technique called ‚Äúbackpropagation,‚Äù allows the model to learn from its mistakes and gradually improve its predictions during the training process. As a result, GPT learns statistical associations between words and related concepts in the data set.\nIn the current wave of GPT models, this core training (now often called ‚Äúpre-training‚Äù) happens only once. After that, people can use the trained neural network in ‚Äúinference mode,‚Äù which lets users feed an input into the trained network and get a result. During inference, the input sequence for the GPT model is always provided by a human, and it‚Äôs called a ‚Äúprompt.‚Äù The prompt determines the model‚Äôs output, and altering the prompt even slightly can dramatically change what the model produces.Iterative prompting is limited by the size of the model‚Äôs ‚Äúcontext window‚Äù since each prompt is appended onto the previous prompt.  ChatGPT is different from vanilla GPT-3 because it has also been trained on transcripts of conversations written by humans. ‚ÄúWe trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sides‚Äîthe user and an AI assistant,‚Äù\nChatGPT has also been tuned more heavily than GPT-3 using a technique called ‚Äúreinforcement learning from human feedback,‚Äù or RLHF, where human raters ranked ChatGPT‚Äôs responses in order of preference, then fed that information back into the model. This has allowed the ChatGPT to produce coherent responses with fewer confabulations than the base model. The prevalence of accurate content in the data set, recognition of factual information in the results by humans, or reinforcement learning guidance from humans that emphasizes certain factual responses.\nTwo major types of falsehoods that LLMs like ChatGPT might produce. The first comes from inaccurate source material in its training data set, such as common misconceptions (e.g., ‚Äúeating turkey makes you drowsy‚Äù). The second arises from making inferences about specific situations that are absent from its training material (data set); this falls under the aforementioned ‚Äúhallucination‚Äù label.\nWhether the GPT model makes a wild guess or not is based on a property that AI researchers call ‚Äútemperature,‚Äù which is often characterized as a ‚Äúcreativity‚Äù setting. If the creativity is set high, the model will guess wildly; if it‚Äôs set low, it will spit out data deterministically based on its data set. If creativity is set low, ‚Äú[It] answers ‚ÄòI don‚Äôt know‚Äô all the time or only reads what is there in the Search results (also sometimes incorrect). What is missing is the tone of voice: it shouldn‚Äôt sound so confident in those situations.‚Äù\nIn some ways, ChatGPT is a mirror: It gives you back what you feed it. If you feed it falsehoods, it will tend to agree with you and ‚Äúthink‚Äù along those lines. That‚Äôs why it‚Äôs important to start fresh with a new prompt when changing subjects or experiencing unwanted responses.\n‚ÄúOne of the most actively researched approaches for increasing factuality in LLMs is retrieval augmentation‚Äîproviding external documents to the model to use as sources and supporting context,‚Äù said Goodside. With that technique, he explained, researchers hope to teach models to use external search engines like Google, ‚Äúciting reliable sources in their answers as a human researcher might, and rely less on the unreliable factual knowledge learned during model training.‚Äù Bing Chat and Google Bard do this already by roping in searches from the web, and soon, a browser-enabled version of ChatGPT will as well. Additionally, ChatGPT plugins aim to supplement GPT-4‚Äôs training data with information it retrieves from external sources, such as the web and purpose-built databases.\nOther things that might help with hallucination include, ‚Äúa more sophisticated data curation and the linking of the training data with ‚Äòtrust‚Äô scores, using a method not unlike PageRank‚Ä¶ It would also be possible to fine-tune the model to hedge when it is less confident in the response.‚Äù (arstechnica article)\n\nOpenAI models\n\ndavinci (e.g.¬†davinci-003) text-generation models are 10x more expensive than their chat counterparts (e.g.¬†gpt-3.5-turbo)\nFor lower usage in the 1000‚Äôs of requests per day range ChatGPT works out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests per day, open-sourced models deployed in AWS work out cheaper. (As of April 24th, 2023.) (article)\n\nUsed AWS Lambda for deployment\n\ndavinci hasn‚Äôt been trained using reinforcement learning from human feedback (RLHF}\nchatgpt 3.5 turbo models\n\nPros\n\nPerforms better on 0 shot classification tasks than Davinci-003\nOutperforms Davinci-003 on sentiment analysis\nSignificantly better than Davinci-003 at math\ncheaper than davinci\n\nCons\n\nTends to produce longer responses than Davinci-003, which may not be ideal for all use cases\nIncluding k-shot examples can lead to inefficient resource usage in multi-turn use cases\n\n\ndavinci-003\n\nPros\n\nPerforms slightly better than GPT-3.5 Turbo with k-shot examples\nProduces more concise responses than GPT-3.5 Turbo, which may be preferable for certain use cases\n\nCons\n\nLess accurate than GPT-3.5 Turbo on 0 shot classification tasks and sentiment analysis\nPerforms significantly worse than GPT-3.5 Turbo on math tasks\n\n\n\nUse Cases\n\nUnderstanding code (Can reduce cognative load)(article)\n\nDuring code reviews or onboarding new programmers\nunder-commented code\n\nGenerating the code scaffold for a problem where you aren‚Äôt sure where or how to start solving it.\nLLMs don‚Äôt require removing stopwords during preprocessing of documents\n\nCost\n\nFor lower usage in the 1000‚Äôs of requests per day range ChatGPT works out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests per day, open-sourced models deployed in AWS work out cheaper. (article, April 24th, 2023.)\n\nMethods for giving chatGPT data\n\nThink you can upload a file\nThrough prompt\n\nSee bizsci video\n\npaste actual data\npaste column names and types (glimse() with no values)\n\nGenerate a string for each row of data that contains the column name and value\n\nExample\n\n‚ÄúThe  is . The  is . ‚Ä¶‚Äù\n‚ÄúThe fico_score is 578.0. The load_amount is 6000.0.¬† The annual income is 57643.54.‚Äù\n\n\n\n\nEvolution of LLMs",
    "crumbs": [
      "LLMs",
      "General"
    ]
  },
  {
    "objectID": "qmd/llms-agent-chains.html",
    "href": "qmd/llms-agent-chains.html",
    "title": "Agent Chains",
    "section": "",
    "text": "LangChain",
    "crumbs": [
      "LLMs",
      "Agent Chains"
    ]
  },
  {
    "objectID": "qmd/llms-agent-chains.html#langchain",
    "href": "qmd/llms-agent-chains.html#langchain",
    "title": "Agent Chains",
    "section": "",
    "text": "Framework for developing applications powered by language models; connect a language model to other sources of data; allow a language model to interact with its environment\nImplementation of the paper¬†ReAct: Synergizing Reasoning and Acting in Language Models which demonstrates a prompting technique to allow the model to ‚Äúreason‚Äù (with a chain-of-thoughts) and ‚Äúact‚Äù (by being able to use a tool from a predefined set of tools, such as being able to search the internet). This combination is shown to drastically improve output text quality and give large language models the ability to correctly solve problems.\nMisc\n\nNotes from:\n\nA Gentle Intro to Chaining LLMs, Agents, and utils via LangChain\n\nSee article for workflow for multi-chains\nAlso shows some diagnostic methods that are included in the library\n\n\nAvailable vector stores for document embeddings\n\nAlso see Databases, Vector Databases\n\nSee article for description and link to code for a manual, more controlled parsing of markdown files to get text and code blocks\n\nComponents\n\nDocument loader: Facilitate the data loading from various sources, including CSV files, SQL databases, and public datasets like Wikipedia.\nAgent: Use the language model as a reasoning engine to determine which actions to take and in which order. It repeats through a continuous cycle of thought-action-observation until the task is completed.\nChain: Different from agents, they consist of predetermined sequences of actions, which are hard coded. It addresses complex and well-defined tasks by guiding multiple tools with high-level directions.\nMemory: Currently the beta version supports accessing windows of past messages, this provides the application with a conversational interface\n\nIn general, chains are what you get by sequentially connecting one or more large language models (LLMs) in a logical way.\nChains can be built using LLMs or ‚ÄúAgents‚Äù.\n\nAgents provide the ability to answer questions that require recent or specialty information the LLM hasn‚Äôt been trained on.\n\ne.g.¬†‚ÄúWhat will the weather be like tomorrow?‚Äù\nAn agent has access to an LLM and a suite of tools for example Google Search, Python REPL, math calculator, weather APIs, etc. (list of supported agents)\nLLMs will use tools to interact with Agents (list of tools)\n\nTool process\n\nUses a LLMChain for building the API URL based on our input instructions and makes the API call.\nUpon receiving the response, it uses another LLMChain that summarizes the response to get the answer to our original question.\n\n\nReAct (Reason + Act) is a popular agent that picks the most usable tool (from a list of tools), based on what the input query is.\n\nOutput Components: observation, a thought, or it takes an action. This is mainly due to the ReAct framework and the associated prompt that the agent is using (See example with ZERO_SHOT_REACT_DESCRIPTION)\n\nserpapi is useful for answering questions about current events.\n\n\nChains can be simple (i.e.¬†Generic) or specialized (i.e.¬†Utility).\n\nGeneric ‚Äî A single LLM is the simplest chain. It takes an input prompt and the name of the LLM and then uses the LLM for text generation (i.e.¬†output for the prompt).\n\nGeneric chains are more often used as building blocks for Utility chains\n\nUtility ‚Äî These are specialized chains, comprised of many LLMs to help solve a specific task. For example,\n\nLangChain supports some end-to-end chains (such as AnalyzeDocumentChain for summarization, QnA, etc) and some specific ones (such as GraphQnAChain for creating, querying, and saving graphs). Programme Aided Language Model reads complex math problems (described in natural language) and generates programs (for solving the math problem) as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.\n2-Chain Examples\n\nChain 1 is used to clean the prompt (remove extra whitespaces, shorten prompt, etc) and chain 2 is used to call an LLM with this clean prompt. (link)\nChain 1 is used to generate a synopsis for a play and chain is used to write a review based on this synopsis. (link)\n\n\n\nDocument Loaders (docs)- various helper functions that take various formats and types of data and produce a document output\n\nFormats like like markdown, word docs, text, PowerPoint, images, HTML, PDF, csvs, AsciiDoc (adoc), etc.\nExamples\n\nGitLoader function clones the repository and load relevant files as documents\nYoutubeLoader - gets subtitles from videos\nDataFrameLoader - converts text columns in panda dfs to documents\n\nAlso, tons of other functions for googledrive or dbs like bigquery, duckdb or cloud storage like s3 or confluence or email or discord, etc.\n\nText Spitters (Docs) - After loading the documents, they‚Äôre usually fed to text splitter to create chunks of text due to LLM context constraints. The chunks of text can then be transformed into embeddings.\n\nFrom ‚ÄúSales and Support Chatbot‚Äù article in example below\n\n\n# Define text chunk strategy\nsplitter = CharacterTextSplitter(\n¬† chunk_size=2000,\n¬† chunk_overlap=50,\n¬† separator=\" \"\n)\n# GDS guides\ngds_loader = GitLoader(\n¬† ¬† clone_url=\"https://github.com/neo4j/graph-data-science\",\n¬† ¬† repo_path=\"./repos/gds/\",\n¬† ¬† branch=\"master\",\n¬† ¬† file_filter=lambda file_path: file_path.endswith(\".adoc\")\n¬† ¬† and \"pages\" in file_path,\n)\ngds_data = gds_loader.load()\n# Split documents into chunks\ngds_data_split = splitter.split_documents(gds_data)\nprint(len(gds_data_split)) #771\n\nEmbeddings\n\nOpenAI‚Äôs text-embedding-ada-002 model is easy to work with, achieves the highest performance out of all of OpenAI‚Äôs embedding models (on the BEIR benchmark), and is also the cheapest ($0.0004/1K tokens).\nHuggingFace‚Äôs sentence-transformers , which reportedly has better performance than OpenAI‚Äôs embeddings, but that involves downloading the model and running it on your own server.\n\nExample: Generic\n\nBuild Prompt\n\n\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n¬† ¬† input_variables=[\"product\"],\n¬† ¬† template=\"What is a good name for a company that makes [{product}]{style='color: #990000'}?\",\n)\nprint(prompt.format(product=\"podcast player\"))\n# OUTPUT\n# What is a good name for a company that makes podcast player?\n\nIf using multiple variables, then you need, e.g.¬†print(prompt.format(product=\"podcast player\", audience=\"children‚Äù), to get the updated prompt.\nCreate LLMChain instance and run\n\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMChain\nllm = OpenAI(\n¬† ¬† ¬† ¬† ¬† model_name=\"text-davinci-003\", # default model\n¬† ¬† ¬† ¬† ¬† temperature=0.9) #temperature dictates how whacky the output should be\nllmchain = LLMChain(llm=llm, prompt=prompt)\nllmchain.run(\"podcast player\")\n\nIf you had more than one input_variables, then you won‚Äôt be able to use run. Instead, you‚Äôll have to pass all the variables as a dict.\n\ne.g., LLMchain({‚Äúproduct‚Äù: ‚Äúpodcast player‚Äù, ‚Äúaudience‚Äù: ‚Äúchildren‚Äù}).\n\nUsing the less expensive chat models: chatopenai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\nExample: Multiple Chains and Multiple Input Variables\n\nGoal: create an age-appropriate gift generator\nChain 1: Find age\n\n\n# Chain1 - solve math problem, get the age\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.agents import load_tools\n\nllm = OpenAI(temperature=0)\ntools = load_tools([\"pal-math\"], llm=llm)\nagent = initialize_agent(tools,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† llm,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† verbose=True)\n\npal-math is a math-solving tool\nReact agent uses the tool to answer the age problem\nChain 2: Recommend a gift\n\ntemplate = \"\"\"You are a gift recommender. Given a person's age,\\n\nit is your job to suggest an appropriate gift for them. If age is under 10,\\n\nthe gift should cost no more than [{budget}]{style='color: #990000'} otherwise it should cost atleast 10 times [{budget}]{style='color: #990000'}.\nPerson Age:\n[{output}]{style='color: #990000'}\nSuggest gift:\"\"\"\nprompt_template = PromptTemplate(input_variables=[\"output\", \"budget\"], template=template)\nchain_two = LLMChain(llm=llm, prompt=prompt_template)\n\n‚Äú{output}‚Äù is the name of the output from the 1st chain\n\nFind the name of the output of a chain: print(agent.agent.llm_chain.output_keys)\n\nThe prompt includes a conditional that transforms {budget} (more below)\nLLMchain is used when there are multiple variable in the template\nCombine Chains and Run\n\noverall_chain = SequentialChain(\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† input_variables=[\"input\"],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† memory=SimpleMemory(memories={\"budget\": \"100 GBP\"}),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† chains=[agent, chain_two],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† verbose=True)\noverall_chain.run(\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\")\n\nThe prompt is only for the 1st chain, and it‚Äôs output, Age, will be input for the second chain.\nSimpleMemory is used pass the variable for the second prompt which adds some additional context to the second chain ‚Äî the {budget} for the gift.\nOutput\n\n#&gt; Entering new SequentialChain chain...\n#&gt; Entering new AgentExecutor chain...\n# I need to figure out my dad's current age and then divide it by two.\n#Action: PAL-MATH\n#Action Input: What is my dad's current age if he is going to be 60 next year?\n#Observation: 59\n#Thought: I now know my dad's current age, so I can divide it by two to get my age.\n#Action: Divide 59 by 2\n#Action Input: 59/2\n#Observation: Divide 59 by 2 is not a valid tool, try another one.\n#Thought: I can use PAL-MATH to divide 59 by 2.\n#Action: PAL-MATH\n#Action Input: Divide 59 by 2\n#Observation: 29.5\n#Thought: I now know the final answer.\n#Final Answer: My current age is 29.5 years old.\n#&gt; Finished chain.\n# For someone of your age, a good gift would be something that is both practical and meaningful. Consider something like a nice watch, a piece of jewelry, a nice leather bag, or a gift card to a favorite store or restaurant.\\nIf you have a larger budget, you could consider something like a weekend getaway, a spa package, or a special experience.'}\n#&gt; Finished chain.\n\nExample: Sales and Support Chatbot (article)\n\nCreate embeddings from text data sources and store in Chroma vector store\n\n\n# Define embedding model\nOPENAI_API_KEY = \"OPENAI_API_KEY\"\nembeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\nsales_data = medium_data_split + yt_data_split\nsales_store = Chroma.from_documents(\n¬† ¬† sales_data, embeddings, collection_name=\"sales\"\n)\nsupport_data = kb_data + gds_data_split + so_data\nsupport_store = Chroma.from_documents(\n¬† ¬† support_data, embeddings, collection_name=\"support\"\n)\n\nSales data is from Medium articles and YouTube subtitles\nSupport data is from docs in a couple github repos and stackoverflow\nInstantiate chatgpt\n\nllm = ChatOpenAI(\n¬† ¬† model_name=\"gpt-3.5-turbo\",\n¬† ¬† temperature=0,\n¬† ¬† openai_api_key=OPENAI_API_KEY,\n¬† ¬† max_tokens=512,\n)\n\nSales prompt template\n\nsales_template = \"\"\"As a Neo4j marketing bot, your goal is to provide accurate¬†\nand helpful information about Neo4j, a powerful graph database used for¬†\nbuilding various applications. You should answer user inquiries based on the¬†\ncontext provided and avoid making up answers. If you don't know the answer,¬†\nsimply state that you don't know. Remember to provide relevant information¬†\nabout Neo4j's features, benefits, and use cases to assist the user in¬†\nunderstanding its value for application development.\n[{context}]{style='color: #990000'}\nQuestion: [{question}]{style='color: #990000'}\"\"\"\nSALES_PROMPT = PromptTemplate(\n¬† ¬† template=sales_template, input_variables=[\"context\", \"question\"]\n)\nsales_qa = RetrievalQA.from_chain_type(\n¬† ¬† llm=llm,\n¬† ¬† chain_type=\"stuff\",\n¬† ¬† retriever=sales_store.as_retriever(),\n¬† ¬† chain_type_kwargs={\"prompt\": SALES_PROMPT},\n)\n\n‚Äú{context}‚Äù in the template is the data stored in the vector store\n‚Äúretriever‚Äù arg points to vector store (e.g.¬†sales_store) and uses the as_retriever method to get the embeddings\nSupport prompt template\n\nsupport_template = \"\"\"\nAs a Neo4j Customer Support bot, you are here to assist with any issues¬†\na user might be facing with their graph database implementation and Cypher statements.\nPlease provide as much detail as possible about the problem, how to solve it, and steps a user should take to fix it.\nIf the provided context doesn't provide enough information, you are allowed to use your knowledge and experience to offer you the best possible assistance.\n[{context}]{style='color: #990000'}\nQuestion: [{question}]{style='color: #990000'}\"\"\"\nSUPPORT_PROMPT = PromptTemplate(\n¬† ¬† template=support_template, input_variables=[\"context\", \"question\"]\n)\nsupport_qa = RetrievalQA.from_chain_type(\n¬† ¬† llm=llm,\n¬† ¬† chain_type=\"stuff\",\n¬† ¬† retriever=support_store.as_retriever(),\n¬† ¬† chain_type_kwargs={\"prompt\": SUPPORT_PROMPT},\n)\n\nSee Sales template\nAdd React agent to determine whether LLM should use Sales or Support templates and contexts\n\ntools = [\n¬† ¬† Tool(\n¬† ¬† ¬† ¬† name=\"sales\",\n¬† ¬† ¬† ¬† func=sales_qa.run,\n¬† ¬† ¬† ¬† description=\"\"\"useful for when a user is interested in various Neo4j information,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† use-cases, or applications. A user is not asking for any debugging, but is only\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† interested in general advice for integrating and using Neo4j.\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Input should be a fully formed question.\"\"\",\n¬† ¬† ),\n¬† ¬† Tool(\n¬† ¬† ¬† ¬† name=\"support\",\n¬† ¬† ¬† ¬† func=support_qa.run,\n¬† ¬† ¬† ¬† description=\"\"\"useful for when when a user asks to optimize or debug a Cypher statement or needs\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† specific instructions how to accomplish a specified task.¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Input should be a fully formed question.\"\"\",\n¬† ¬† ),\n]\n\nagent = initialize_agent(\n¬† ¬† tools,¬†\n¬† ¬† llm,¬†\n¬† ¬† agent=\"zero-shot-react-description\",¬†\n¬† ¬† verbose=True\n)\nagent.run(\"\"\"What are some GPT-4 applications with Neo4j?\"\"\")\n\nIn this example, the tools used by the Agent are custom data sources and prompt templates",
    "crumbs": [
      "LLMs",
      "Agent Chains"
    ]
  },
  {
    "objectID": "qmd/llms-fine-tuning.html",
    "href": "qmd/llms-fine-tuning.html",
    "title": "Fine Tuning",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "LLMs",
      "Fine Tuning"
    ]
  },
  {
    "objectID": "qmd/llms-fine-tuning.html#sec-nlp-fintun-misc",
    "href": "qmd/llms-fine-tuning.html#sec-nlp-fintun-misc",
    "title": "Fine Tuning",
    "section": "",
    "text": "Tuning an LLM\n\nNotes from:\n\nHacker‚Äôs Guide to Language Models (Howard)\n\nStages\n\nLM Pre-Training - Trained on a large corpus (e.g.¬†much of the internet) to predict the next word in a sentence or to fill in a word in a sentence\nLM Fine-Tuning - Trained on a specific task (e.g.¬†solve problems, answer questions). Instruction Tuning is often used. OpenOrca is an example of a Q&A dataset to train a LM to answer questions. Still predicting the next word, like in Pre-Training, but more target-based on a specific task.\nClassifier Fine-Tuning - Reinforcement Learning from Human Feedback (RLHF)is often used. The LLM being trained gives a few answers to a question and then a human or better LLM will pick which one is best.\n\nPre-Trained LLMs are ones that are typically the open source ones being released and available for download\n\nThey will need to be Fine-Tuned, but not necessarily Classifier Fine-Tuned. Often times, LM Fine-Tuning is enough.\n\n\nWorkflow Example (paper)\n\nRaschka - An introduction to the core ideas and approaches to Finetuning Large Language Models - by Sebastian Raschka\nWith ChatGPT, you can have it answer questions from context that contains thousands of documents.\n\n\nStore all these documents as small chunks of text (allowable size of the context window) in a database.\nCreate embeddings of documents and question\nThe documents of relevance can then be found by computing similarities between the question and the document chunks. This is done typically by converting the chunks and question into word embedding vectors, and computing cosine similarities between chunks and question, and finally choosing only those chunks above a certain cosine similarity as relevant context.\nFinally, the question and context can be combined into a prompt as below, and fed into an LLM API like ChatGPT: prompt=f\"Answer the question. Context: {context}\\n Question: {question}\"",
    "crumbs": [
      "LLMs",
      "Fine Tuning"
    ]
  },
  {
    "objectID": "qmd/llms-prompt-engineering.html",
    "href": "qmd/llms-prompt-engineering.html",
    "title": "Prompt Engineering",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "LLMs",
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "qmd/llms-prompt-engineering.html#sec-nlp-prompt-misc",
    "href": "qmd/llms-prompt-engineering.html#sec-nlp-prompt-misc",
    "title": "Prompt Engineering",
    "section": "",
    "text": "Definition\n\nAsking the right question\n‚ÄúPrompt engineering is the process of designing and optimizing prompts to LLMs for a wide variety of applications and research topics. Prompts are short pieces of text that are used to guide the LM‚Äôs output. They can be used to specify the task that the LM is supposed to accomplish, provide additional information that the LM needs to complete the task, or simply make the task easier for the LM to understand.‚Äù\n\nComponents\n\nAsk the question (e.g.¬†‚ÄúWhat‚Äôs 1+1?‚Äù)\nSpecify the type of response you want. (e.g.¬†Only return the numeric answer.)\n\nPersona\n\n‚ÄúExplain this to my like I‚Äôm a fifth grader.‚Äù\n\nStyle\n\n‚ÄúUse a style typical of scientific abstracts to write this.‚Äù\n\nFormat\n\nIf you say ‚ÄúFormat the output as a JSON object with the fields: x, y, z‚Äù you can get better results and easily do error handling.\n\n\n\nLLMs don‚Äôt understand the complexities or nuances of various subjects\n\nIf an industry term is used in multiple ways, the LLM might not understand the meaning just by context alone.\nLLMs can have problems with information in complex formats.\n\nTables sometimes have this same issue, because tables are the mechanism used for layout structure and not a content structure (e.g.¬†sentence)\n\nThe models themselves continue to evolve so if it doesn‚Äôt understand something today doesn‚Äôt mean that it won‚Äôt understand it tomorrow\n\nWhen the output is incomplete, type ‚Äúcontinue‚Äù for the next prompt and it will finish the output.\nDon‚Äôt give LLMs proprietary data\n\nAlternative: slice(0)\ndat |&gt;\n¬† slice(0) |&gt;\n¬† glimpse()\n\nGives the column names and classes\nDepending on the use case, you might want to make the column names unabbreviated and meaningful.\n\n\nSecurity\n\nDon‚Äôt let the user have the last word: When taking a user‚Äôs prompt, incorporating it with your own prompt, and sending it to ChatGPT or some other similar application, always add a line like ‚ÄúIf user input doesn‚Äôt make sense for doing xyz, ask them to repeat the request‚Äù after the user‚Äôs input. This will stop the majority of prompt injections.\nDon‚Äôt just automatically run code or logic that is output from a LLM\n\nTips when used for writing\n\nBe specific on word count and put higher than you need\nDon‚Äôt be afraid to ask it to add more information or expand on a particular point\\\nIt‚Äôs better at creating outlines rather than full pieces of content.\nBe as specific as possible, and use keywords to help ChatGPT understand what you are looking for\nYou can ask it to rephrase its response\nAvoid using complex language or convoluted sentence structures\n**Review the content for accuracy",
    "crumbs": [
      "LLMs",
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "qmd/llms-prompt-engineering.html#sec-nlp-prompt-dsex",
    "href": "qmd/llms-prompt-engineering.html#sec-nlp-prompt-dsex",
    "title": "Prompt Engineering",
    "section": "Templates",
    "text": "Templates\n\nMisc\n\nSpecify language, libraries, and functions\n\nExample: BizSci Lab 82\n\n\nShow the prompt he used in a markdown file. He just copied and pasted it into the prompt.\nSpecify libraries to use; models to use; that you want to tune the models in parallel\nThis is not an ideal prompt. You should iterate prompts and guide gpt through complete ds process\n\ni.e.¬†prompt for collection then a prompt for cleaning, and so on with eda, preprocessing, modelling, cv, model selection, app\nUse the phrases like:\n\n‚ÄúPlease update code to include &lt;new feature&gt;‚Äù\n‚ÄúPlease update feature to be &lt;new thing&gt; instead of &lt;old thing&gt;‚Äù\n\n\n\nExample: Various DS Activities\n\nExample: Student Feedback\n\nFrom Now is the time for grimoires\nComponents\n\nRole: Tell the AI who it is. Context helps the AI produce tailored answers in useful ways, but you don‚Äôt need to go overboard.\nGoal: Tell the AI what you want it to do.\nStep-by-Step Instructions: Research has found that it often works best to give the AI explicit instructions that go step-by-step through what you want.\n\nOne approach, called Chain of Thought prompting, gives the AI an example of how you want it to reason before you make your request\nYou can also give it step-by-step directions the way we do in our prompts.\n\nConsider Examples: Few-shot prompting, where you give the AI examples of the kinds of output you want to see, has also proven very effective in research.\nAdd Personalization: Ask the user for information to help tailor the prompt for them.\nAdd Your Own Constraints: The AI often acts in ways that you may not want. Constraints tell it to avoid behaviors that may come up in your testing.\nFinal Step: Check your prompt by trying it out, giving it good, bad, and neutral input. Take the perspective of your users‚Äì is the AI helpful? Does the process work? How might the AI be more helpful? Does it need more context? Does it need further constraints? You can continue to tweak the prompt until it works for you and until you feel it will work for your audience.\n\nPrompt",
    "crumbs": [
      "LLMs",
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html",
    "href": "qmd/code-snippets.html",
    "title": "Snippets",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-misc",
    "href": "qmd/code-snippets.html#sec-code-snippits-misc",
    "title": "Snippets",
    "section": "",
    "text": "Check whether an environment variable is empty\nnzchar(Sys.getenv(\"blopblopblop\"))\n#&gt; [1] FALSE\nwithr::with_envvar(\n  new = c(\"blopblopblop\" = \"bla\"),\n  nzchar(Sys.getenv(\"blopblopblop\"))\n)\nUse a package for a single instance using {withr::with_package}\n\n\nUsing library() will keep the package loaded during the whole session, with_package() just runs the code snippet with that package temporarily loaded. This can be useful to avoid namespace collisions for example\n\nRead .csv from a zipped file\n# long way\ntmpf &lt;- tempfile()\ntmpd &lt;- tempfile()\ndownload.file('https://website.org/path/to/file.zip', tmpf)\nunzip(tmpf, exdir = tmpd)\ny &lt;- data.table::fread(file.path(tmpd,\n                       grep('csv$',\n                            unzip(tmpf, list = TRUE)$Name,\n                            value = TRUE)))\nunlink(tmpf)\nunlink(tmpd)\n\n# quick way\ny &lt;- data.table::fread('curl https://website.org/path/to/file.zip | funzip')\nLoad all R scripts from a directory: for (file in list.files(\"R\", full.names = TRUE)) source(file)\nView dataframe in View as html table using {kableExtra}\ndf_html &lt;- kableExtra::kbl(rbind(head(df, 5), tail(df, 5)), format = \"html\")\nprint(df_html)",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-opts",
    "href": "qmd/code-snippets.html#sec-code-snippits-opts",
    "title": "Snippets",
    "section": "Options",
    "text": "Options\n\n{readr}\noptions(readr.show_col_types = FALSE)",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-cleaning",
    "href": "qmd/code-snippets.html#sec-code-snippits-cleaning",
    "title": "Snippets",
    "section": "Cleaning",
    "text": "Cleaning\n\nRemove all objects except: rm(list=setdiff(ls(), c(\"train\", \"validate\", \"test\")))\nRemove NAs\n\ndataframes\ndf %&gt;% na.omit\ndf %&gt;% filter(complete.cases(.))\ndf %&gt;% tidyr::drop_na()\nvariables\ndf %&gt;% filter(!is.na(x1))\ndf %&gt;% tidyr::drop_na(x1)\n\nFind duplicate rows\n\n{datawizard} - Extract all duplicates, for visual inspection. Note that it also contains the first occurrence of future duplicates, unlike duplicated or dplyr::distinct. Also contains an additional column reporting the number of missing values for that row, to help in the decision-making when selecting which duplicates to keep.\ndf1 &lt;- data.frame(\n  id = c(1, 2, 3, 1, 3),\n  year = c(2022, 2022, 2022, 2022, 2000),\n  item1 = c(NA, 1, 1, 2, 3),\n  item2 = c(NA, 1, 1, 2, 3),\n  item3 = c(NA, 1, 1, 2, 3)\n)\n\ndata_duplicated(df1, select = \"id\")\n#&gt;   Row id year item1 item2 item3 count_na\n#&gt; 1   1  1 2022    NA    NA    NA        3\n#&gt; 4   4  1 2022     2     2     2        0\n#&gt; 3   3  3 2022     1     1     1        0\n#&gt; 5   5  3 2000     3     3     3        0\n\ndata_duplicated(df1, select = c(\"id\", \"year\"))\n#&gt; 1   1  1 2022    NA    NA    NA        3\n#&gt; 4   4  1 2022     2     2     2        0\ndplyr\ndups &lt;- dat %&gt;%¬†\n¬† group_by(BookingNumber, BookingDate, Charge) %&gt;%¬†\n¬† filter(n() &gt; 1)\nbase r\ndf[duplicated(df[\"ID\"], fromLast = F) | duplicated(df[\"ID\"], fromLast = T), ]\n\n##¬† ¬† ¬† ¬† ID value_1 value_2 value_1_2\n## 2¬† ID-003¬† ¬† ¬† 6¬† ¬† ¬† 5¬† ¬† ¬† 6 5\n## 3¬† ID-006¬† ¬† ¬† 1¬† ¬† ¬† 3¬† ¬† ¬† 1 3\n## 4¬† ID-003¬† ¬† ¬† 1¬† ¬† ¬† 4¬† ¬† ¬† 1 4\n## 5¬† ID-005¬† ¬† ¬† 5¬† ¬† ¬† 5¬† ¬† ¬† 5 5\n## 6¬† ID-003¬† ¬† ¬† 2¬† ¬† ¬† 3¬† ¬† ¬† 2 3\n## 7¬† ID-005¬† ¬† ¬† 2¬† ¬† ¬† 2¬† ¬† ¬† 2 2\n## 9¬† ID-006¬† ¬† ¬† 7¬† ¬† ¬† 2¬† ¬† ¬† 7 2\n## 10 ID-006¬† ¬† ¬† 2¬† ¬† ¬† 3¬† ¬† ¬† 2 3\n\ndf[duplicated(df[\"ID\"], fromLast = F) doesn‚Äôt include the first occurence, so also counting from the opposite direction will include all occurences of the duplicated rows\n\n\nRemove duplicated rows\n\n{datawizard} - From all rows with at least one duplicated ID, keep only one. Methods for selecting the duplicated row are either the first duplicate, the last duplicate, or the ‚Äúbest‚Äù duplicate (default), based on the duplicate with the smallest number of NA. In case of ties, it picks the first duplicate, as it is the one most likely to be valid and authentic, given practice effects.\ndf1 &lt;- data.frame(\n  id = c(1, 2, 3, 1, 3),\n  item1 = c(NA, 1, 1, 2, 3),\n  item2 = c(NA, 1, 1, 2, 3),\n  item3 = c(NA, 1, 1, 2, 3)\n)\n\ndata_unique(df1, select = \"id\")\n#&gt; (2 duplicates removed, with method 'best')\n#&gt;   id item1 item2 item3\n#&gt; 1  1     2     2     2\n#&gt; 2  2     1     1     1\n#&gt; 3  3     1     1     1\nbase R\ndf[!duplicated(df[c(\"col1\")]), ]\ndplyr\ndistinct(df, col1, .keep_all = TRUE)\n\nShowing all combinations present in the data and creating all possible combinations\n\nFuzzy Join (alt to case_when)\nref.df &lt;- data.frame(\n¬† ¬† ¬† ¬† ¬† ¬† bucket = c(‚ÄúHigh‚Äù, ‚ÄúMedium-High‚Äù, ‚ÄúMedium-Low‚Äù, ‚ÄúLow‚Äù),\n¬† ¬† ¬† ¬† ¬† ¬† value.high = c(max(USArrests$Assault), 249, 199, 149),\n¬† ¬† ¬† ¬† ¬† ¬† value.low = c(250, 200, 150, min(USArrests$Assault)))\nUSArrests %&gt;%¬†\n¬† fuzzy_join(ref.df,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† by = c(\"Assault\"=\"value.low\",\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† \"Assault\" = 'value.high'),¬†\n¬† ¬† ¬† ¬† ¬† ¬† match_fun = c(`&gt;=`,`&lt;=`)) %&gt;%¬†\n¬† select(-c(value.high, value.low))\n\nAlso does partial matches\n\n\n\n\nRemove elements of a list by name\npurrr::discard_at(my_list, \"a\")\nlistr::list_remove",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-func",
    "href": "qmd/code-snippets.html#sec-code-snippits-func",
    "title": "Snippets",
    "section": "Functions",
    "text": "Functions\n\nggplot\nviz_monthly &lt;- function(df, y_var, threshhold = NULL) {\n\n¬† ggplot(df) +\n¬† ¬† aes(\n¬† ¬† ¬† x = .data[[\"day\"]],\n¬† ¬† ¬† y = .data[[y_var]]\n¬† ¬† ) +\n¬† ¬† geom_line() +\n¬† ¬† geom_hline(yintercept = threshhold, color = \"red\", linetype = 2) +\n¬† ¬† scale_x_continuous(breaks = seq(1, 29, by = 7)) +\n¬† ¬† theme_minimal()\n}\n\naes is on the outside\n\nThis was a function for a shiny module\nIt‚Äôs peculier. Necessary for function or module?\n\n\nCreate formula from string\nanalysis_formula &lt;- 'Days_Attended ~ W + School'\nestimator_func &lt;-¬† function(data) lm(as.formula(analysis_formula), data = data)\nRecursive Function\n\nExample\n# Replace pkg text with html\nreplace_txt &lt;- function(dat, patterns) {\n  if (length(patterns) == 0) {\n    return(dat)\n  }\n\n  pattern_str &lt;- patterns[[1]]$pattern_str\n  repl_str &lt;- patterns[[1]]$repl_str\n  replaced_txt &lt;- dat |&gt;\n    str_replace_all(pattern = pattern_str, repl_str)\n\n  new_patterns &lt;- patterns[-1]\n  replace_txt(replaced_txt, new_patterns)\n}\n\nArguments include the dataset and the iterable\nTests whether function has iterated through pattern list\nRemoves 1st element of the list\nreplace_text calls itself within the function with the new list and new dataset\n\nExample: Using Recall and tryCatch\nload_page_completely &lt;- function(rd) {\n  # load more content even if it throws an error\n  tryCatch({\n      # call load_more()\n      load_more(rd)\n      # if no error is thrown, call the load_page_completely() function again\n      Recall(rd)\n  }, error = function(e) {\n      # if an error is thrown return nothing / NULL\n  })\n}\n\nload_more is a user defined function\nRecall is a base R function that calls the same function it‚Äôs in.",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-calcs",
    "href": "qmd/code-snippets.html#sec-code-snippits-calcs",
    "title": "Snippets",
    "section": "Calculations",
    "text": "Calculations\n\nCompute the running maximum per group\n(df &lt;- structure(list(var = c(5L, 2L, 3L, 4L, 0L, 3L, 6L, 4L, 8L, 4L),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† group = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† .Label = c(\"a\", \"b\"), class = \"factor\"),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† time = c(1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L)),\n¬† ¬† ¬† ¬† ¬† .Names = c(\"var\", \"group\",\"time\"),\n¬† ¬† ¬† ¬† ¬† class = \"data.frame\", row.names = c(NA, -10L)))\n\ndf[order(df$group, df$time),]\n#¬† ¬† var group time\n# 1¬† ¬† 5¬† ¬† a¬† ¬† 1\n# 2¬† ¬† 2¬† ¬† a¬† ¬† 2\n# 3¬† ¬† 3¬† ¬† a¬† ¬† 3\n# 4¬† ¬† 4¬† ¬† a¬† ¬† 4\n# 5¬† ¬† 0¬† ¬† a¬† ¬† 5\n# 6¬† ¬† 3¬† ¬† b¬† ¬† 1\n# 7¬† ¬† 6¬† ¬† b¬† ¬† 2\n# 8¬† ¬† 4¬† ¬† b¬† ¬† 3\n# 9¬† ¬† 8¬† ¬† b¬† ¬† 4\n# 10¬† 4¬† ¬† b¬† ¬† 5\n\ndf$curMax &lt;- ave(df$var, df$group, FUN=cummax)\ndf\nvar¬† |¬† group¬† |¬† time¬† |¬† curMax\n5¬† ¬† ¬† a¬† ¬† ¬† ¬† 1¬† ¬† ¬† ¬† 5\n2¬† ¬† ¬† a¬† ¬† ¬† ¬† 2¬† ¬† ¬† ¬† 5\n3¬† ¬† ¬† a¬† ¬† ¬† ¬† 3¬† ¬† ¬† ¬† 5\n4¬† ¬† ¬† a¬† ¬† ¬† ¬† 4¬† ¬† ¬† ¬† 5\n0¬† ¬† ¬† a¬† ¬† ¬† ¬† 5¬† ¬† ¬† ¬† 5\n3¬† ¬† ¬† b¬† ¬† ¬† ¬† 1¬† ¬† ¬† ¬† 3\n6¬† ¬† ¬† b¬† ¬† ¬† ¬† 2¬† ¬† ¬† ¬† 6\n4¬† ¬† ¬† b¬† ¬† ¬† ¬† 3¬† ¬† ¬† ¬† 6\n8¬† ¬† ¬† b¬† ¬† ¬† ¬† 4¬† ¬† ¬† ¬† 8\n4¬† ¬† ¬† b¬† ¬† ¬† ¬† 5¬† ¬† ¬† ¬† 8\n\n\nTime Series\n\nBase-R\n\nIntervals\n\nDifference between dates\n# Sample dates\nstart_date &lt;- as.Date(\"2022-01-15\")\nend_date &lt;- as.Date(\"2023-07-20\")\n\n# Calculate time difference in days\ntime_diff_days &lt;- end_date - start_date\n\n# Convert days to months\nmonths_diff_base &lt;- as.numeric(time_diff_days) / 30.44  # average days in a month\n\ncat(\"Number of months using base R:\", round(months_diff_base, 2), \"\\n\")\n#&gt; Number of months using base R: 18.1 \n\n\n\n\n{lubridate}\n\nDocs\nIntervals\n\nLubridate‚Äôs interval functions\nNotes from: Wrangling interval data using lubridate\nDifference between dates\n# Load the lubridate package\nlibrary(lubridate)\n\n# Sample dates\nstart_date &lt;- ymd(\"2022-01-15\")\nend_date &lt;- ymd(\"2023-07-20\")\n\n# Calculate months difference using lubridate\nmonths_diff_lubridate &lt;- interval(start_date, end_date) %/% months(1)\n\ncat(\"Number of months using lubridate:\", months_diff_lubridate, \"\\n\")\n#&gt; Number of months using lubridate: 18 \n\n%/% is used for floor division by months. For decimals, just use /\n\nData\n(house_df &lt;- tibble(\n  person_id  = factor(c(\"A10232\", \"A10232\", \"A10232\", \"A39211\", \"A39211\", \"A28183\", \"A28183\", \"A10124\")),\n  house_id   = factor(c(\"H1200E\", \"H1243D\", \"H3432B\", \"HA7382\", \"H53621\", \"HC39EF\", \"HA3A01\", \"H222BA\")),\n  start_date = ymd(c(\"20200101\", \"20200112\", \"20211120\", \"19800101\", \"19900101\", \"20170303\", \"20190202\", \"19931023\")),\n  end_date   = ymd(c(\"20200112\", \"20211120\", \"20230720\", \"19891231\", \"20170102\", \"20180720\", \"20230720\", \"20230720\"))\n))\n\n#&gt;   A tibble: 8 √ó 4\n#&gt;   person_id house_id start_date end_date  \n#&gt;   &lt;fct&gt;     &lt;fct&gt;    &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 A10232    H1200E   2020-01-01 2020-01-12\n#&gt; 2 A10232    H1243D   2020-01-12 2021-11-20\n#&gt; 3 A10232    H3432B   2021-11-20 2023-07-20\n#&gt; 4 A39211    HA7382   1980-01-01 1989-12-31\n#&gt; 5 A39211    H53621   1990-01-01 2017-01-02\n#&gt; 6 A28183    HC39EF   2017-03-03 2018-07-20\n#&gt; 7 A28183    HA3A01   2019-02-02 2023-07-20\n#&gt; 8 A10124    H222BA   1993-10-23 2023-07-20\nCreate interval column\nhouse_df &lt;- \n  house_df |&gt; \n  mutate(\n    # create the interval\n    int = interval(start_date, end_date), \n    # drop the start/end columns\n    .keep = \"unused\"                      \n  )\n\nhouse_df\n#&gt;   A tibble: 8 √ó 3\n#&gt;   person_id house_id int                           \n#&gt;   &lt;fct&gt;     &lt;fct&gt;    &lt;Interval&gt;                    \n#&gt; 1 A10232    H1200E   2020-01-01 UTC--2020-01-12 UTC\n#&gt; 2 A10232    H1243D   2020-01-12 UTC--2021-11-20 UTC\n#&gt; 3 A10232    H3432B   2021-11-20 UTC--2023-07-20 UTC\n#&gt; 4 A39211    HA7382   1980-01-01 UTC--1989-12-31 UTC\n#&gt; 5 A39211    H53621   1990-01-01 UTC--2017-01-02 UTC\n#&gt; 6 A28183    HC39EF   2017-03-03 UTC--2018-07-20 UTC\n#&gt; 7 A28183    HA3A01   2019-02-02 UTC--2023-07-20 UTC\n#&gt; 8 A10124    H222BA   1993-10-23 UTC--2023-07-20 UTC\nIntersection Function\n\nint_intersect &lt;- function(int, int_limits) {\n  int_start(int) &lt;- pmax(int_start(int), int_start(int_limits))\n  int_end(int)   &lt;- pmin(int_end(int), int_end(int_limits))\n  return(int)\n}\n\nThe red dashed line is the reference interval and the blue solid line is the interval of interest\nThe function creates an interval thats the intersection of both intervals (segment between black parentheses)\n\nProportion of the Reference Interval\n\nint_proportion &lt;- function(dat, reference_interval) {\n\n  # start with the housing data\n  dat |&gt; \n    # only retain overlapping rows, this makes the following\n    # operations more efficient by only computing what we need\n    filter(int_overlaps(int, reference_interval)) |&gt; \n    # then, actually compute the overlap of the intervals\n    mutate(\n      # use our earlier truncate function\n      int_sect = int_intersect(int, reference_interval),\n      # then, it's simple to compute the overlap proportion\n      prop = int_length(int_sect) / int_length(reference_interval)\n    ) |&gt; \n    # combine different intervals per person\n    summarize(prop_in_nl = sum(prop), .by = person_id)\n\n}\n\nExample\nint_2017  &lt;- interval(ymd(\"20170101\"), ymd(\"20171231\"))\nprop_2017 &lt;- \n  int_proportion(dat = house_df, \n                 reference_interval = int_2017)\n\nprop_2017\n\n#&gt; # A tibble: 3 √ó 2\n#&gt;   person_id prop_in_nl\n#&gt;   &lt;fct&gt;          &lt;dbl&gt;\n#&gt; 1 A39211       0.00275\n#&gt; 2 A28183       0.832  \n#&gt; 3 A10124       1",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-paral",
    "href": "qmd/code-snippets.html#sec-code-snippits-paral",
    "title": "Snippets",
    "section": "Parallelization",
    "text": "Parallelization\n\nMaking a cluster out of SSH connected machines (Thread)\n\nBasic\npacman::p_load(parallely, future, furrr)\nnodes = c(\"host1\", \"host2\")\nplan(cluster, workers = nodes)\nfuture_map(...)\nWith {renv}\npacman::p_load(parallely, future, furrr)\nnodes = c(\"host1\", \"host2\")\nplan(cluster, workers = nodes, rscript_libs = .libPaths())\nfuture_map(...)",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html",
    "href": "qmd/bayes-troubleshooting-hmc.html",
    "title": "Troubleshooting HMC",
    "section": "",
    "text": "Divergent Transitions",
    "crumbs": [
      "Bayes",
      "Troubleshooting HMC"
    ]
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-divtrans",
    "href": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-divtrans",
    "title": "Troubleshooting HMC",
    "section": "Divergent Transitions",
    "text": "Divergent Transitions\n\n‚Äúdivergent transitions after warmup‚Äù\nSee Taming Divergences in Stan Models and Identifying non-identifiability\nSee Statistical Rethinking &gt;&gt; Ch.9 MCMC &gt;&gt; Issues\nDivergent Transition - A rejected proposed parameter value in the posterior during the sampling process\n\nToo many DTs could indicate a poor exploration of the posterior by the sampling algorithm and possibly biased estimates.\n\nIf the DTs are happening in the same region of the posterior then that region isn‚Äôt being sampled by the HMC algorithm\n\nIf there are ‚Äústeep‚Äù areas in the posterior, these areas can break the sampling process resulting in a ‚Äúbad‚Äù proposed parameter value.\n\nSolutions\n\nadjust priors from flat to weakly informative\nNeed more data\nIncrease adapt_delta closer to 1 (default: 0.8)\nReparameterize the model",
    "crumbs": [
      "Bayes",
      "Troubleshooting HMC"
    ]
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-chnmix",
    "href": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-chnmix",
    "title": "Troubleshooting HMC",
    "section": "Chains Not Mixing",
    "text": "Chains Not Mixing\n\nMisc\n\nNotes from When MCMC fails: The advice we‚Äôre giving is wrong. Here‚Äôs what we you should be doing instead. (Hint: it‚Äôs all about the folk theorem.)\n\nPotential issues\n\nPriors on some parameters are weak or nonexistent or the data are too weak to identify all the parameters in the model.\n\nSigns: Chains are exploring extreme regions of the parameter space. Check out the y-axis range in trace and see how high or low the values are.\nExamples: elasticity parameters of -20 or people with 8 kg livers\n\nCoding mistake\n\nStan examples:\n\nYou can forget to use a log link or set a prior using variance instead sd\nArray indices and for loops don‚Äôt match\n\n\nMinor modes in the tails of the posterior distribution\n\nYour posterior is multimodal and all but one of the modes have near-zero mass\nSigns: different chains will cluster in different places\nSolutions:\n\nUsing starting values near the main mode (brms init arg; see [Statistical Rethinking &gt;&gt; Ch 4]((https://ercbk.github.io/Statistical-Rethinking-Notebook/qmd/chapter-4.html){style=‚Äúcolor: green‚Äù}\n\nCan also be used in general cases where you‚Äôre getting bad mixing from you chains\n\ne.g.¬†divergent transitions, large numbers of transitions, high R-hat values, and/or very low effective sample size estimates\n\nDiagnostic example\n\nCheck the intercept warm-up (For real-world models, it‚Äôs good to look at the trace plots for all major model parameters)\n\ngeom_trace &lt;- function(subtitle = NULL,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† xlab = \"iteration\",¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† xbreaks = 0:4 * 500) {\nlist(\nannotate(geom = \"rect\",¬†\n¬† ¬† ¬† ¬† xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf,\n¬† ¬† ¬† ¬† fill = fk[16], alpha = 1/2, size = 0),\ngeom_line(size = 1/3),\nscale_color_manual(values = fk[c(3, 8, 27, 31)]),\nscale_x_continuous(xlab, breaks = xbreaks, expand = c(0, 0)),\nlabs(subtitle = subtitle),\ntheme(panel.grid = element_blank())\n)¬† ¬†\n}\np1 &lt;-\nggmcmc::ggs(fit1, burnin = TRUE) %&gt;%\nfilter(Parameter == \"b_Intercept\") %&gt;%¬†\nmutate(chain = factor(Chain),\n¬† ¬† intercept = value) %&gt;%¬† ¬†\nggplot(aes(x = Iteration, y = intercept, color = chain)) +\ngeom_trace(subtitle = \"fit1 (default settings)\") +\nscale_y_continuous(breaks = c(0, 650, 1300), limits = c(NA, 1430))¬†\np1\n\n\n\n\n\nOne of our chains eventually made its way to the posterior, three out of the four stagnated near their starting values (lines near zero).\nSet starting values manually. Same values to all 4 chains.\ninits &lt;- list(\nIntercept = 1300,\nsigma¬† ¬† = 150,\nbeta¬† ¬† ¬† = 520\n)\nlist_of_inits &lt;- list(inits, inits, inits, inits)\nfit2 &lt;- brm(\ndata = dat,\nfamily = exgaussian(),\nformula = rt ~ 1 + (1 | id),\ncores = 4, seed = 1,\ninits = list_of_inits\n)\n\nMuch mo‚Äô better, but not evidence of chain convergence since all started at the same value\nNo practical for large models with many parameters\nSet starting values (somewhat) randomly by function\nset_inits &lt;- function(seed = 1) {¬† ¬†\nset.seed(seed)\nlist(\n# posterior for the intercept often looks gaussian\nIntercept = rnorm(n = 1, mean = 1300, sd = 100),\n# posteriors for sigma and beta need to nonnegative (alt:rgamma)\nsigma¬† ¬† = runif(n = 1, min = 100, max = 200),\nbeta¬† ¬† ¬† = runif(n = 1, min = 450, max = 550)\n)¬† ¬†\n}\n\nlist_of_inits &lt;- list(\n# different seed values will return different results\nset_inits(seed = 1),\nset_inits(seed = 2),\nset_inits(seed = 3),\nset_inits(seed = 4)\n)\n\n\nChains are mixing and evidence of convergence since we started at different starting values\nNeed to also check sigma and beta\n\nFitting multiple models and averaging predictions (stacking). Don‚Äôt fully understand but this is from a section 5.6 of Gelman‚Äôs Bayesian Workflow paper\n\n‚Äúdivide the model into pieces by introducing a strong mixture prior and then fitting the model separately given each of the components of the prior. Other times the problem can be addressed using strong priors that have the effect of ruling out some of the possible modes‚Äù\n\nAlso ‚ÄúSoon we should have Pathfinder implemented and this will get rid of lots of these minor modes automatically‚Äù\nThe model can be reparameterized\n\nThink this had to do with Divergent Transitions\nSigns: You have coefficients like 0.000002\nSolutions:\n\nUse variables that have been log transformed or scaled (e.g.¬†per million). For some reason, itt‚Äôs difficult for the sampler when parameters values are on vastly different scales.\nReparameterize to ‚Äúunit scale‚Äù\n\nI think scale in ‚Äúunit scale‚Äù refers to scale as a distribution parameter like sd is the scale parameter in a Normal distribution, and ‚Äúunit scale‚Äù is scale = 1 (e.g.¬†sd = 1 in standardization). But there‚Äôs more to this, and I haven‚Äôt read S.R. ch 13 yet\n\n\n\nCommon (misguided?) solutions\n\nIncrease iterations\nTweak adapt_delta and max_treedepth parameters to make it explore the space more carefully\n\nOther\n\nSequential Monte Carlo (SMC) is a potential solution multimodal posterior problem (Stan‚Äôs NUTS sampler may already do this to some extent. See Ch. 9 Statistical Rethinking)\n\nBayesTools PKG implements a SMC sampler\nAlgorithm details - https://docs.pymc.io/notebooks/SMC2_gaussians.html?highlight=smc",
    "crumbs": [
      "Bayes",
      "Troubleshooting HMC"
    ]
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-reparam",
    "href": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-reparam",
    "title": "Troubleshooting HMC",
    "section": "Reparameterization",
    "text": "Reparameterization\n\n\nNote: ‚ÄúRT‚Äôs‚Äù in the image is probably refering to a response variable (e.g.¬†Response Time, Reaction Time, etc.)\nMisc\n\nResources\n\nSee Stan User Guide\nSee Ch 13.4 in Statistical Rethinking\n\nNotes from Thread\nIn the paper, Strategies for fitting nonlinear ecological models in R, AD Model Builder, and BUGS, it suggests scaling; eliminating correlation; making contours elliptical.(?)\n\nGamma from {shape,scale} to {log-mean, log-shape} is often good.\n\n\nParameters with distributions such as cauchy, student-t, normal, or any distribution in the location-scale family are good reparameterization candidates\nNeal‚Äôs Funnel\n\nTechnique for efficiently sampling random effects or latent variables in hierarchical Bayesian models.\n\nExample: McElreath\n\\[\nx \\sim \\mathbb{exponential}(\\lambda) \\\\\n\\text{-same as-} \\\\\nz \\sim \\mathbb{exponential(1)} \\\\\nx = \\frac{z}{\\lambda}\n\\]\n\nFactoring out scale parameters",
    "crumbs": [
      "Bayes",
      "Troubleshooting HMC"
    ]
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html#sec-bayes-trblhmc-divtrans",
    "href": "qmd/bayes-troubleshooting-hmc.html#sec-bayes-trblhmc-divtrans",
    "title": "Troubleshooting HMC",
    "section": "",
    "text": "‚Äúdivergent transitions after warmup‚Äù\nSee Taming Divergences in Stan Models and Identifying non-identifiability\nSee Statistical Rethinking &gt;&gt; Ch.9 MCMC &gt;&gt; Issues\nDivergent Transition - A rejected proposed parameter value in the posterior during the sampling process\n\nToo many DTs could indicate a poor exploration of the posterior by the sampling algorithm and possibly biased estimates.\n\nIf the DTs are happening in the same region of the posterior then that region isn‚Äôt being sampled by the HMC algorithm\n\nIf there are ‚Äústeep‚Äù areas in the posterior, these areas can break the sampling process resulting in a ‚Äúbad‚Äù proposed parameter value.\n\nSolutions\n\nAdjust priors from flat to weakly informative\nNeed more data\nIncrease adapt_delta closer to 1 (default: 0.8)\nReparameterize the model",
    "crumbs": [
      "Bayes",
      "Troubleshooting HMC"
    ]
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html#sec-bayes-trblhmc-chnmix",
    "href": "qmd/bayes-troubleshooting-hmc.html#sec-bayes-trblhmc-chnmix",
    "title": "Troubleshooting HMC",
    "section": "Chains Not Mixing",
    "text": "Chains Not Mixing\n\nMisc\n\nNotes from When MCMC fails: The advice we‚Äôre giving is wrong. Here‚Äôs what we you should be doing instead. (Hint: it‚Äôs all about the folk theorem.)\n\n\n\nMinor Issues\n\nPriors on some parameters are weak or nonexistent or the data are too weak to identify all the parameters in the model.\n\nSigns: Chains are exploring extreme regions of the parameter space. Check out the y-axis range in trace and see how high or low the values are.\nExamples: elasticity parameters of -20 or people with 8 kg livers\n\nCoding mistake\n\nStan examples:\n\nYou can forget to use a log link or set a prior using variance instead sd\nArray indices and for loops don‚Äôt match\n\n\n\n\n\nMulti-Modal Posterior\n\nMinor modes in the tails of the posterior distribution\nYour posterior is multimodal and all but one of the modes have near-zero mass\nWhen the posterior distribution is multimodal, it means that there are multiple distinct regions of high probability density, separated by regions of low probability density. This can lead to poor mixing of the MCMC chains, as the chains may get stuck in one of the modes and have difficulty exploring the other modes.\nSigns: different chains will cluster in different places\n\n\nSolutions\n\nPathfinder\n\nPaper, Github\nGets rid of lots of these minor modes automatically\nA Variational Inference (VI) method that locates approximations to the target density along a quasi-Newton optimization path. Starting from a random initialization in the tail of the posterior distribution, the quasi-Newton optimization trajectory can quickly move from the tail, through the body of the distribution, to a mode or pole.\n\nVariational inference searches for a tractable approximate distribution that minimizes Kullback-Leibler (KL) divergence to the posterior and is typically faster than Monte Carlo sampling.\n\n\nUse starting values near the main mode\n\n{brms} init argument (See Statistical Rethinking &gt;&gt; Ch 4)\nCan also be used in general cases where you‚Äôre getting bad mixing from you chains\n\ne.g.¬†divergent transitions, large numbers of transitions, high R-hat values, and/or very low effective sample size estimates\n\nExample:\n\nDiagnose Issue: Check the intercept warm-up (For real-world models, it‚Äôs good to look at the trace plots for all major model parameters)\n\ngeom_trace &lt;- \n  function(subtitle = NULL,¬†\n¬† ¬† ¬† ¬† ¬† ¬†xlab = \"iteration\",¬†\n¬† ¬† ¬† ¬† ¬† ¬†xbreaks = 0:4 * 500) {\n    list(\n      annotate(geom = \"rect\",¬†\n      ¬† ¬† ¬† ¬†  xmin = 0, \n               xmax = 1000, \n               ymin = -Inf, \n               ymax = Inf,\n      ¬† ¬† ¬† ¬†  fill = fk[16], \n               alpha = 1/2, \n               size = 0),\n      geom_line(size = 1/3),\n      scale_color_manual(values = fk[c(3, 8, 27, 31)]),\n      scale_x_continuous(xlab, \n                         breaks = xbreaks, \n                         expand = c(0, 0)),\n      labs(subtitle = subtitle),\n      theme(panel.grid = element_blank())\n    )¬† ¬†\n}\np1 &lt;-\n  ggmcmc::ggs(fit1, burnin = TRUE) %&gt;%\n  filter(Parameter == \"b_Intercept\") %&gt;%¬†\n  mutate(chain = factor(Chain),\n¬† ¬†      intercept = value) %&gt;%¬† ¬†\n  ggplot(aes(x = Iteration, \n             y = intercept, \n             color = chain)) +\n  geom_trace(subtitle = \"fit1 (default settings)\") +\n  scale_y_continuous(breaks = c(0, 650, 1300), \n                     limits = c(NA, 1430))¬†\np1\n\nOne of our chains eventually made its way to the posterior, three out of the four stagnated near their starting values (lines near zero).\n\nSet starting values manually. Same values to all 4 chains\n\ninits &lt;- list(\n  Intercept = 1300,\n  sigma¬† ¬† = 150,\n  beta¬† ¬† ¬† = 520\n)\nlist_of_inits &lt;- list(inits, inits, inits, inits)\nfit2 &lt;- brm(\n  data = dat,\n  family = exgaussian(),\n  formula = rt ~ 1 + (1 | id),\n  cores = 4, seed = 1,\n  inits = list_of_inits\n)\n\nMuch mo‚Äô better, but not evidence of chain convergence since all started at the same value\nNo practical for large models with many parameters\n\nSet starting values (somewhat) randomly by function\n\nset_inits &lt;- function(seed = 1) {\n  set.seed(seed)\n  list(\n    # posterior for the intercept often looks gaussian\n    Intercept = rnorm(n = 1, mean = 1300, sd = 100),\n    # posteriors for sigma and beta need to nonnegative (alt:rgamma)\n    sigma¬† ¬† = runif(n = 1, min = 100, max = 200),\n    beta¬† ¬† ¬† = runif(n = 1, min = 450, max = 550)\n  )\n}\n\nlist_of_inits &lt;- list(\n  # different seed values will return different results\n  set_inits(seed = 1),\n  set_inits(seed = 2),\n  set_inits(seed = 3),\n  set_inits(seed = 4)\n)\n\nChains are mixing and evidence of convergence since we started at different starting values\nNeed to also check sigma and beta\n\n\n\nFit multiple models and average predictions (stacking). Don‚Äôt fully understand but this is from a section 5.6 of Gelman‚Äôs Bayesian Workflow paper\n\n‚ÄúDivide the model into pieces by introducing a strong mixture prior and then fitting the model separately given each of the components of the prior. Other times the problem can be addressed using strong priors that have the effect of ruling out some of the possible modes‚Äù\n\nReparameterize the Model\n\nThink this had to do with Divergent Transitions\nSigns: You have coefficients like 0.000002\nSolutions:\n\nUse variables that have been log transformed or scaled (e.g.¬†per million). For some reason, itt‚Äôs difficult for the sampler when parameters values are on vastly different scales.\nReparameterize to ‚Äúunit scale‚Äù\n\nI think scale in ‚Äúunit scale‚Äù refers to scale as a distribution parameter like sd is the scale parameter in a Normal distribution, and ‚Äúunit scale‚Äù is scale = 1 (e.g.¬†sd = 1 in standardization). But there‚Äôs more to this, and I haven‚Äôt read S.R. ch 13 yet\n\n\n\nCommon (misguided?) Solutions\n\nIncrease iterations\nTweak adapt_delta and max_treedepth parameters to make it explore the space more carefully\n\nOther\n\nSequential Monte Carlo (SMC) is a potential solution multimodal posterior problem (Stan‚Äôs NUTS sampler may already do this to some extent. See Ch. 9 Statistical Rethinking)\n\nAlgorithm details - https://docs.pymc.io/notebooks/SMC2_gaussians.html?highlight=smc",
    "crumbs": [
      "Bayes",
      "Troubleshooting HMC"
    ]
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html#sec-bayes-trblhmc-reparam",
    "href": "qmd/bayes-troubleshooting-hmc.html#sec-bayes-trblhmc-reparam",
    "title": "Troubleshooting HMC",
    "section": "Reparameterization",
    "text": "Reparameterization\n\n\nNote: ‚ÄúRT‚Äôs‚Äù in the image is probably refering to a response variable (e.g.¬†Response Time, Reaction Time, etc.)\nUsed to help solve HMC issues\nMisc\n\nResources\n\nSee Stan User Guide\nSee Ch 13.4 in Statistical Rethinking\n\nAlso see {makemypriors} in Bayes, Priors &gt;&gt; Misc &gt;&gt; Packages - Recommended in the Thread below as method for handling the same issues that reparameterization solves.\nNotes from Thread\nIn the paper, Strategies for fitting nonlinear ecological models in R, AD Model Builder, and BUGS, it suggests scaling; eliminating correlation; making contours elliptical.(?)\n\nGamma from {shape,scale} to {log-mean, log-shape} is often good.\n\n\nParameters with distributions such as cauchy, student-t, normal, or any distribution in the location-scale family are good reparameterization candidates\nNeal‚Äôs Funnel\n\nTechnique for efficiently sampling random effects or latent variables in hierarchical Bayesian models.\nZ-Scores Random Effects\n\nExample: McElreath\n\\[\n\\begin{align}\n&x \\sim \\text{Exponential}(\\lambda) \\\\\n&\\quad\\quad\\text{-same as-} \\\\\n&z \\sim \\text{Exponential(1)} \\\\\n&x = \\frac{z}{\\lambda}\n\\end{align}\n\\]\n\nFactoring out scale parameters",
    "crumbs": [
      "Bayes",
      "Troubleshooting HMC"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html",
    "href": "qmd/bayes-priors.html",
    "title": "Priors",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-misc",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-misc",
    "title": "Priors",
    "section": "",
    "text": "Resources\n\nStan/brms prior distributions (mc-stan/function-ref)\n\nDistributions are towards the bottom of the guide\n{brms} should have all the distributions available in Stan according to their docs (see Details section)\n\n\nPackages\n\n{makemyprior} (paper)- Intuitive construction of Joint priors for variance parameters.\n\nGUI can be used to choose the joint prior, where the user can click through the model and select priors.\nUsing a hierarchical variance decomposition, a joint variance prior is formulated that takes the whole model structure into account. In this way, existing knowledge can intuitively be incorporated at the level it applies to.\n\nAlternatively, independent variance priors can be used for each model component in the latent Gaussian model.\n\n\n\nUsing meta-analyis or previous studies to create informed priors\n\n‚ÄúSystematic use of informed studies leads to more precise, but more biased estimates (due to non-linear information flow in the literature). Critical comparison of informed and skeptical priors can provide more nuanced and solid understanding of our findings.‚Äù (thread + paper)\n\ni.e.¬†try both and compare the result\n\n\nPrior sensitivity analysis\n\n{priorsense}\n\nVideo, Thread\n\n{BayesSenMC}\n\nFor binary exposure and a dichotomous outcome\nGenerates different posterior distributions of adjusted odds ratio under different priors of sensitivity and specificity, and plots the models for comparison. It also provides estimations for the specifications of the models using diagnostics of exposure status with a non-linear mixed effects model.\nVignette\n\n\nStatistical Rethinking\n\nThe ‚Äúflatness‚Äù of a Normal prior is controlled by the size of the s.d. value\n\nnot in logistic regression (see examples below)\n\nFlat priors result in poor frequency properties (i.e.¬†consistently give bad inferences) in realistic settings where studies are noisy and effect sizes are small. (Gelman post)\nWeakly informative priors: they allow some implausibly strong relationships but generally bound the lines to possible ranges of the variables. (fig 5.3, pg 131)\nWe want our priors to be skeptical of large differences [in treatment effects], so that we reduce overfitting. Good priors hurt fit to sample but are expected to improve prediction. (pg 337)\nWe don‚Äôt formulate priors based on the sample data. We want the prior predictive distribution to live in the plausible outcome space, not fit the sample.\nFor logistic regression and poisson regression, a flat prior in the logit space is not a flat prior in the outcome probability space (pg 336)\nAs long as the priors are vague, minimizing the sum of squared deviations to the regression line is equivalent to finding the posterior mean. pg 200\n‚ÄúAs always in rescaling variables, the goals are to create focal points that you might have prior information about, prior to seeing the actual data values. That way we can assign priors that are not obviously crazy, and in thinking about those priors, we might realize that the model makes no sense. But this is only possible if we think about the relationship between measurements and parameters, and the exercise of rescaling and assigning sensible priors helps us along that path. Even when there are enough data that choice of priors is not crucial, this thought exercise is useful.‚Äù pg 258\nComparing the posteriors with the priors",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-preproc",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-preproc",
    "title": "Priors",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nCentering the predictor\n\nMakes the posterior easier to sample\nReduces covariance among the parameter posterior distributions\nMakes it easier to define the prior on average temperature in the center of the time range (instead defining prior for temperature at year 0).\nLinks to Gelman posts about centering your predictors (article)\n\nIf you standardize your predictors, you can use a mean of 0 for the prior on your intercept\n\nWith flat priors, this doesn‚Äôt make much of difference",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-getprecs",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-getprecs",
    "title": "Priors",
    "section": "Get Prior Recommendations",
    "text": "Get Prior Recommendations\n\nExample: Fitting a spline\n# get recommended prior specifications\n# s is the basis function brms imports from mgcv pkg\nbrms::get_prior(data = d2,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† family = gaussian,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† doy ~ 1 + s(year))\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† prior¬† ¬† class¬† ¬† coef group resp dpar nlpar bound¬† ¬† ¬† source¬†\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† (flat)¬† ¬† ¬† ¬† b¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† default¬†\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† (flat)¬† ¬† ¬† ¬† b syear_1¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† (vectorized)¬†\n##¬† student_t(3, 105, 5.9) Intercept¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† default¬†\n##¬† ¬† student_t(3, 0, 5.9)¬† ¬† ¬† sds¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† default¬†\n##¬† ¬† student_t(3, 0, 5.9)¬† ¬† ¬† sds s(year)¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† (vectorized)¬†\n##¬† ¬† student_t(3, 0, 5.9)¬† ¬† sigma¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† default\n\n# applying the recommendations\n# multi-level method for spline fitting\nb4.11 &lt;- brm(data = d2,¬†\n¬† ¬† ¬† ¬† ¬† ¬† family = gaussian,¬†\n¬† ¬† ¬† ¬† ¬† ¬† # k = 19, corresponds to 17 basis functions I guess ::shrugs::¬†\n¬† ¬† ¬† ¬† ¬† ¬† # The default for s() is to use what‚Äôs called a thin plate regression spline¬†\n¬† ¬† ¬† ¬† ¬† ¬† # bs uses a basis spline¬†\n¬† ¬† ¬† ¬† ¬† ¬† temp ~ 1 + s(year, bs = \"bs\", k = 19),¬†\n¬† ¬† ¬† ¬† ¬† ¬† prior = c(prior(normal(100, 10), class = Intercept),¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(normal(0, 10), class = b),¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(student_t(3, 0, 5.9), class = sds),¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(exponential(1), class = sigma)),¬†\n¬† ¬† ¬† ¬† ¬† ¬† iter = 2000, warmup = 1000, chains = 4, cores = 4,¬†\n¬† ¬† ¬† ¬† ¬† ¬† seed = 4,¬†\n¬† ¬† ¬† ¬† ¬† ¬† control = list(adapt_delta = .99))\nExample: Multinomial Logistic Regression\n# Outcome categorical variable has k = 3 levels. We fit k-1 models. Hence the 2 intercept priors\n# intercept model\nget_prior(data = d,¬†\n¬† ¬† ¬† ¬† ¬† # refcat sets the reference category to the 3rd level\n¬† ¬† ¬† ¬† ¬† family = categorical(link = logit, refcat = 3),\n¬† ¬† ¬† ¬† ¬† # just an intercept model\n¬† ¬† ¬† ¬† ¬† career ~ 1)\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† prior¬† ¬† class coef group resp dpar nlpar bound¬† source\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† (flat) Intercept¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† default\n##¬† student_t(3, 3, 2.5) Intercept¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† mu1¬† ¬† ¬† ¬† ¬† ¬† default\n##¬† student_t(3, 3, 2.5) Intercept¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† mu2¬† ¬† ¬† ¬† ¬† ¬† default\n\nb11.13io &lt;-\n¬† brm(data = d,¬†\n¬† ¬† ¬† family = categorical(link = logit, refcat = 3),\n¬† ¬† ¬† career ~ 1,\n¬† ¬† ¬† prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(normal(0, 1), class = Intercept, dpar = mu2)),\n¬† ¬† ¬† iter = 2000, warmup = 1000, cores = 4, chains = 4,\n¬† ¬† ¬† seed = 11,\n¬† ¬† ¬† file = \"fits/b11.13io\")\n\nAs of brms 2.12.0, ‚Äúspecifying global priors for regression coefficients in categorical models is deprecated.‚Äù Meaning ‚Äî if we want to use the same prior for both, we need to use the dpar argument for each",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-finterp",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-finterp",
    "title": "Priors",
    "section": "Formulating an Intercept Prior",
    "text": "Formulating an Intercept Prior\n\nExample SR 6.3.1 pgs 182-83\n\nA thought process on how to set a predictor prior based on its relationship to the outcome and an intercept prior.\n\nExample SR 7.1.1 pg 200\n\nOutcome variable was scaled, outcome/max(outcome)\n\nValues now between 0 and 1\nUseful for when 0 is a meaningful boundary\n\nNow able to center the intercept prior on mean of outcome, Œ± ‚àº Normal(0.5, 1)\n\nSays that the average species with an average body mass (predictor variable) has a brain volume (outcome variable) with an 89% credible interval (¬± 1.5 sd) from about ‚àí1 to 2.\n\nBody mass was centered, so it‚Äôs at its average is when its value is zero.\n\n\n\nExample SR 8.3.2 pg 259\n\nSimilar to 7.1.1 example except there‚Äôs the observation that a sd = 1 for the intercept prior is too large given that the outcome is bdd between 0 and 1 (after scaling)\na &lt;- rnorm( 1e4 , 0.5 , 1 )\nsum( a &lt; 0 | a &gt; 1 ) / length( a )\n[1] 0.6126\n\n61% of the prior is outside the bounds for the outcome which makes no sense\n\nIf it‚Äôs 0.5 units from the mean to zero, then a standard deviation of 0.25 should put only 5% of the mass outside the valid range.\na &lt;- rnorm( 1e4 , 0.5 , 0.25 )\nsum( a &lt; 0 | a &gt; 1 ) / length( a )\n[1] 0.0486\n\nNot sure why you want 5% outside the valid range of the outcome variable\n\n\nExample (Ch 11 pg 335-6)\n\nWith logistic regression, flat Normal priors aren‚Äôt priors with a high sd.\n\n\nThe Normal prior on the logit scale with the large sd says that the probabilty of an event is either 0 or 1 which usually isn‚Äôt reasonable.\n\nlogit(pi) = Œ±\n\nŒ± ~ Normal(0, 1.5) ‚Äî the curve for the probability of an event is very flat, looks like a mesa\nŒ± ~ Normal(0, 1.0) ‚Äîthe curve for the probability of an event is a fat hill shape. A little more skeptical of extreme probabilities\n\n\nExample\n\nAlso have ggplot code in Documents &gt;&gt; R &gt;&gt; Code &gt;&gt; Simulations &gt;&gt; sim-prior-predictive-distr.R\nPoisson regression (pg 356)\nIn poisson regression, flat normal priors aren‚Äôt priors with high s.d.\n\n# prior predictive distribution\ncurve( dlnorm( x , 3 , 0.5 ) , from=0 , to=100 , n=200 )\n\nSince poisson regression uses a log link, the outcome is log-normal. We‚Äôre simulating the effect of a normal prior on a log-normal outcome which is why the simulation code uses dlnorm.\n‚Äúnumber of tools‚Äù is the outcome variable\nThe prior with s.d. 10 has almost all the probability density at zero and huge mean\na &lt;- rnorm(1e4,0,10)\nlambda &lt;- exp(a)\nmean(lambda)\n[1] 9.622994e+12\nThis usually doesn‚Äôt make sense for a prior\nThe prior with s.d. 0.5 has a mean around 20 and a more spread out probability density which makes much more sense given the literature on the subject.",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-fslopp",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-fslopp",
    "title": "Priors",
    "section": "Formulating a Slope Prior",
    "text": "Formulating a Slope Prior\n\nExample SR pg 259\n\nslopes centered on zero, imply no prior information about direction\nHow big could can the slopes be in theory?\n\nAfter centering, range of each predictor is 2‚Äîfrom ‚àí1 to 1 is 2 units.\nTo take us from the theoretical minimum of outcome variable = 0 on one end to the observed maximum of 1‚Äîa range of 1 unit‚Äîon the other would require a slope of 0.5 from either predictor variable‚Äî0.5 √ó 2 = 1.\n\nAssign a standard deviation of 0.25, then 95% of the prior slopes are from ‚àí0.5 to 0.5, so either predictor could in principle account for the entire range, but it would be unlikely\n\nExample SR pg 336-7\n\nWith logistic regression, flat Normal priors aren‚Äôt priors with a high sd.\n\n\nShows difference between two levels of the treatment effect (i.e.¬†2 different treatments) on the 0/1 outcome\nThe prior with large sd has all the probability density massed at 0 and 1\n\nSays that the 2 treatments are completely alike or completely different\n\nThe prior with the small sd (e.g.¬†Normal(0, 0.5) is concentrated from about 0 to about 0.4\n\nAlthough 0 difference in treatments has the highest probability, the mean is at a difference around 0.10\n\nPrior says that large differences between treatments are very unlikely, but if the data contains strong evidence of large differences, they will shine through\n\nPairs nicely with an intercept prior, Œ± ~ Normal(0, 1.5)\nAn example of a weakly informative prior that reduces overfitting the sample data\n\n\n\nExample: pg 357\n\nset.seed(11)\n## TOP ROW\n\n# how many lines would you like?\nn &lt;- 100\n# simulate and wrangle\ntibble(i = 1:n,\n       a = rnorm(n, mean = 3, sd = 0.5)) %&gt;%\n  mutate(`beta%~%Normal(0*', '*10)` = rnorm(n, mean = 0 , sd = 10),\n         `beta%~%Normal(0*', '*0.2)` = rnorm(n, mean = 0 , sd = 0.2)) %&gt;%\n  pivot_longer(contains(\"beta\"),\n               values_to = \"b\",\n               names_to = \"prior\") %&gt;%\n  expand(nesting(i, a, b, prior),\n         x = seq(from = -2, to = 2, length.out = 100)) %&gt;%\n\n  # plot\n  ggplot(aes(x = x, y = exp(a + b * x), group = i)) +\n  geom_line(size = 1/4, alpha = 2/3,\n            color = wes_palette(\"Moonrise2\")[4]) +\n  labs(x = \"log population (std)\",\n       y = \"total tools\") +\n  coord_cartesian(ylim = c(0, 100)) +\n  facet_wrap(~ prior, labeller = label_parsed)\n\n## BOTTOM ROW\nprior &lt;-\n  tibble(i = 1:n,\n         a = rnorm(n, mean = 3, sd = 0.5),\n         b = rnorm(n, mean = 0, sd = 0.2)) %&gt;%¬†\n  expand(nesting(i, a, b),\n         x = seq(from = log(100), to = log(200000), length.out = 100))\n# left\np1 &lt;-\n  prior %&gt;%\n  ggplot(aes(x = x, y = exp(a + b * x), group = i)) +\n  geom_line(size = 1/4, alpha = 2/3,\n            color = wes_palette(\"Moonrise2\")[4]) +\n  labs(subtitle = expression(beta%~%Normal(0*', '*0.2)),\n       x = \"log population\",\n       y = \"total tools\") +\n  coord_cartesian(xlim = c(log(100), log(200000)),\n                  ylim = c(0, 500))\n# right\np2 &lt;-\n  prior %&gt;%\n  ggplot(aes(x = exp(x), y = exp(a + b * x), group = i)) +\n  geom_line(size = 1/4, alpha = 2/3,\n            color = wes_palette(\"Moonrise2\")[4]) +\n  labs(subtitle = expression(beta%~%Normal(0*', '*0.2)),\n       x = \"population\",\n       y = \"total tools\") +\n  coord_cartesian(xlim = c(100, 200000),\n                  ylim = c(0, 500))\n# combine\np1 | p2\n\nWith poisson regression, flat Normal priors aren‚Äôt priors with a high sd.\nOutcome: total_tools, predictor: log_population\nBottom row fig titles have a typo. Should be a ~ dnorm(3, 0.5) since it‚Äôs the Intercept prior\nVariables have been standardized; total_tools simulated with intercept + predictor priors. So the y axis is simulating the potential fitted values.\nTop Left (0 is mean of log_population):\n\nlarge sd: mostly results in explosive growth of tools just after mean of log_population or explosive decline just before mean log_population (unlikely)\n\nTop Right (0 is mean of log_population)\n\nsmall sd (flatter): most results are around the mean of the intercept prior results (see above) but still allows for more extreme estimates. (reasonable)\n\nBottom Left\n\n100 trend lines between total tools and un-standardized log population\n\nViewing prior predictive trends with un-standardized variables is more natural to see what‚Äôs happening\n\n100 total tools is probably the most we expect to ever see in these data\n\nLooks like 80-85% of the trend lines are under 100. still keeps some explosive possibilities.\n\n\nBottom Right\n\n100 trend lines between total tools and un-standardized, un-logged population\n\nViewing prior predictive trends with un-standardized, un-transformed variables is even more natural to see what‚Äôs happening\n\nWhen a predictor variable is logged in a regression with a log-link (i.e.¬†log-log), this means we are assuming diminishing returns for the raw predictor variable.\n\nEach additional person contributes a smaller increase in the expected number of tools\nDiminishing returns as a predictor value continues to increase makes sense in many situations which is why logging predictors is a popular transformation\n\n\nThoughts\n\nBottom-right seems like the right way to visualize the prior to think about the association between the outcome and predictor\nTop row and bottom-left seem to give a better sense of how many explosive possibilities and their patterns that your allowing for with different transformations",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-fsigp",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-fsigp",
    "title": "Priors",
    "section": "Formulating a Sigma Prior",
    "text": "Formulating a Sigma Prior\n\nCommon to start with exponential(1)\nTightening the spread of the Exponential distribution by using a Gamma distribution (Thread)\n\n\nYou can keep ‚Äúmean = 1‚Äù (aka exponential(1) and adjust the ‚Äúsd‚Äù.\n\nSee Distributions &gt;&gt; Gamma for details on the process\n\nAlso allows you to move most of the mass of the prior a littler further away from 0.\nAnother alternative is the Weibull distribution",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-eapfam",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-eapfam",
    "title": "Priors",
    "section": "Extracting a Prior From a Model",
    "text": "Extracting a Prior From a Model\n\nExample: Logistic Regression (SR sect 11.1.1 pg 336)\n\nIntercept\n\n# prior_samples and inv_logit_scaled are brms functions\n# theme is from ggthemes\nprior_samples(b11.1) %&gt;%\n  mutate(p = inv_logit_scaled(Intercept)) %&gt;%\n\n  ggplot(aes(x = p)) +\n  geom_density(fill = wes_palette(\"Moonrise2\")[4],\n               size = 0, adjust = 0.1) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"prior prob pull left\")",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-simppd",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-simppd",
    "title": "Priors",
    "section": "Simulating a Prior Predictive Distribution",
    "text": "Simulating a Prior Predictive Distribution\n\nExample: SR pg 176\n# log-normal prior\nsim_p &lt;- rlnorm( 1e4 , 0 , 0.25 )\n\n# \"this prior expects anything from 40% shrinkage up to 50% growth\"\nrethinking::precis( data.frame(sim_p) )\n# 'data.frame': 10000 obs. of 1 variables:\n#       mean    sd  5.5% 94.5% histogram\n# sim_p 1.03  0.26  0.67  1.48 ‚ñÅ‚ñÉ‚ñá‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n# tidy-way\nsim_p &lt;-\n  tibble(sim_p = rlnorm(1e4, meanlog = 0, sdlog = 0.25))sim_p %&gt;%\n    mutate(`exp(sim_p)` = exp(sim_p)) %&gt;%\n    gather() %&gt;%\n    group_by(key) %&gt;%\n    tidybayes::mean_qi(.width = .89) %&gt;%\n    mutate_if(is.double, round, digits = 2)\n\n## # A tibble: 2 x 7\n##  key         value .lower .upper .width .point .interval\n##  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;chr&gt;     &lt;chr&gt;\n## 1 exp(sim_p)  2.92   1.96   4.49   0.89   mean        qi\n## 2 sim_p       1.03   0.67    1.5   0.89   mean        qi\n\nVisualize with ggplot\n\n# wrangle\nsim_p %&gt;%\n  mutate(`exp(sim_p)` = exp(sim_p)) %&gt;%\n  gather() %&gt;%\n  # plot\n  ggplot(aes(x = value)) +\n  geom_density(fill = \"steelblue\") +\n  scale_x_continuous(breaks = c(0, .5, 1, 1.5, 2, 3, 5)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(0, 6)) +\n  theme(panel.grid.minor.x = element_blank()) +\n  facet_wrap(~key, scale = \"free_y\", ncol = 1)\n\nExample:\n\nPossible Intercept Priors\n\ngrid &lt;- seq(-3, 3, \n             length.out = 1000) # evenly spaced values from -3 to 3\nb0_prior &lt;- \n   map_dfr(.x = c(0.5, 1, 2), # .x represents the three sigmas \n           ~ data.frame(grid = grid,\n                        b0 = dnorm(grid, mean = 0, sd = .x)),\n                        .id = \"sigma_id\")\n# Create Friendlier Labels\nb0_prior &lt;- b0_prior %&gt;%\n  mutate(sigma_id = factor(sigma_id, \n         labels = c(\"normal(0, 0.5)\",\n                    \"normal(0, 1)\",\n                    \"normal(0, 2)\")))\nggplot(b0_prior, aes(x = grid, y = b0)) +\n  geom_area(fill = \"cadetblue4\", color = \"black\", alpha = 0.90) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.85)) +\n  labs(x = NULL,\n       y = \"probability density\",\n       title = latex2exp::TeX(\"Possible $\\\\beta_0$ (intercept) priors\")) +\n       facet_wrap(~sigma_id, nrow = 3)\nSampling of lines from \\(\\beta_0\\) and \\(\\beta_1\\) priors\n\nb0b1 &lt;- \n  map2_df(.x = c(0.5, 1, 2), \n          .y = c(0.25, 0.5, 1), \n          ~ data.frame(\n              b0 = rnorm(100, mean = 0, sd = .x),\n              b1 = rnorm(100, mean = 0, sd = .y)), \n          .id = \"sigma_id\"\n  )\n\n# Create friendlier labels\nb0b1 &lt;- \n  b0b1 %&gt;%\n    mutate(sigma_id = factor(sigma_id, \n                             labels = c(\"b0 ~ normal(0, 0.5); b1 ~ normal(0, 0.25)\",\n                                        \"b0 ~ normal(0, 1); b1 ~ normal(0, 0.50)\",\n                                        \"b0 ~ normal(0, 2); b1 ~ normal(0, 1)\")))\n\nggplot(b0b1) +\n  geom_abline(aes(intercept = b0, slope = b1), color = \"cadetblue4\", alpha = 0.75) +\n  scale_x_continuous(limits = c(-2, 2)) +\n  scale_y_continuous(limits = c(-3, 3)) +\n  labs(x = \"x\",\n       y = \"y\",\n  title = latex2exp::TeX(\"Sampling of lines from $\\\\beta_0$ and $\\\\beta_1$ priors\")) +\n  facet_wrap(~sigma_id, nrow = 3)",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-conjp",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-conjp",
    "title": "Priors",
    "section": "Conjugate Priors",
    "text": "Conjugate Priors\n\nIf the posterior distributions p(Œ∏ | x) are in the same probability distribution family as the prior probability distribution p(Œ∏), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function p(x | Œ∏)\nBenefits\n\nBayesian updates no longer need to compute the product of the likelihood and prior (only addition is needed).\n\nThis product is computationally expensive and sometimes not feasible.\nOtherwise numerical integration may be necessary\n\nMay give intuition, by more transparently showing how a likelihood function updates a prior distribution.\n\nAll members of the exponential family have conjugate priors.\nList\n&lt;Beta posterior&gt;\nBeta prior * Bernoulli likelihood ‚Üí Beta posterior\nBeta prior * Binomial likelihood ‚Üí Beta posterior\nBeta prior * Negative Binomial likelihood ‚Üí Beta posterior\nBeta prior * Geometric likelihood ‚Üí Beta posterior\n\n&lt;Gamma posterior&gt;\nGamma prior * Poisson likelihood ‚Üí Gamma posterior\nGamma prior * Exponential likelihood ‚Üí Gamma posterior\n\n&lt;Normal posterior&gt;¬†\nNormal prior * Normal likelihood (mean) ‚Üí Normal posterior\n\n&lt;Others&gt;\nDirichlet prior * Multinomial likelihood ‚Üí Dirichlet posterior",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-pelic",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-pelic",
    "title": "Priors",
    "section": "Prior Elicitation",
    "text": "Prior Elicitation\n\nTranslating expert opinion to a probability density than you can use as a prior\nMisc\n\nNotes from:\n\nVideo: On using expert information in Bayesian statistics (R &gt;&gt; Videos)\nPaper: Methods for Eliciting Informative Prior Distributions (R &gt;&gt; Documents &gt;&gt; Bayes)\n\nPackages\n\n{SHELF} - shiny apps for eliciting for various distributions and via various methods\n\nwebpage also has links to papers\n\n\nPrior sensitivity analysis should done especially if you‚Äôre using expert-informed priors.\n\nBest Practice\n\nUse domain experts to set constraints for your priors (e.g.¬†upper and lower limits) instead of formulating a prior\n\nBest to use data, previous studies, etc. to formulate priors\n\nUse domain experts to inform you on relationships between variables\n\nElicitation process is resource intensive for what is probably minimal gain in comparison to data\n\nIt takes a lot of your time and their time to get this right\nExperts may not have the statistical knowledge to understand what you need or how to convey the information\n\nIn this case, you‚Äôll need to school them on basic statistical concepts\n\n\nWhen to use expert information to formulate a prior\n\nIf your DAG specifies a model that requires more data than you have.\n\nWhile it might be necessary to augment your data, be aware that expert knowledge has been shown to much less useful in complex models rather than simpler models.\n\nIf the your data is really noisy\nIf experts know something that isn‚Äôt represented by your data or can‚Äôt be captured by the data\nIf domain expertise is required given your research question\n\nInterviews with experts\n\nQuality Control: If your subject matter allows, try to create ‚Äúcalibrating‚Äù questions to weed-out the experts that aren‚Äôt really experts\n\nShould be questions that you are certain of the answer and are things any expert should know\n\nThis can be difficult for some subject matter.\n\nMaybe consult with an expert that you‚Äôre confident is an expert to help come up with some questions.\n\nQuestions should be standardized, so you know that the results from each expert are consistent.\nFace-to-face elicitation produces greater quality results, because the facilitator can clarify the questions from the experts if needed.\nTry to keep experts from biasing the information they give you\n\nDon‚Äôt use experts that have seen the results of your model\n\nIf they‚Äôve seen your raw data that‚Äôs okay. (I dunno about this, even if they‚Äôve seen eda plots?)\n\nDon‚Äôt provide them with any estimates you may have from previous studies or other experts\nDon‚Äôt let them fixate on outlier scenarios they may have encountered\n\nRecord conversations with video and/or audio\n\nIf problems surface when evaluating the expert‚Äôs information, these can be useful to go back over the information collection process\n\nWas there a misunderstanding between you and the expert on what information you wanted\nWas the information biased? (see above)\nIf using mulitiple experts, maybe subgroups have different viewpoints/experiences which is causing a divergence in opinion (e.g nurses vs psychologists treating PTSD)\n\n\nIf problems surface when evaluating the expert‚Äôs information, it can be useful to gather specific experts that differ and have them discuss why they hold their substantially differing opinions. Afterwards, they may adjust their opinions and you‚Äôll have a greater consensus.\nProcess\n\nElicit location parameter (e.g.¬†via Trial Roulette) from the expert\n\nTrial Roulette (see paper in Misc for details)\n\nRequires the expert to have sufficient statistical knowledge to be able to place the blocks to form an appropriate distribution\nUser should be aware that distribution output may be inappropriate for sample data\n\nExample: Algorithm may output a Gamma distribution which is inappropriate for percentage data since the upper bound can be greater than one\n\nParameter space is split into subsections (e.g.¬†quantiles)\nUser assigns blocks to each subsection\nExample From MATCH website which was an earlier implementation of {SHELF}\n\nTop chart is a histogram where each cell is a ‚Äúblock‚Äù (called ‚Äúchips‚Äù at the bottom). The right panel shows the options for setting the axis ranges and number of bins\nBottom chart evidently estimates distribution parameters from the histogram in the top chart which are your prior‚Äôs parameters\n\n\nWith experts with less statistical training, it may be better for you to give them scenarios (e.g.¬†combinations of quantiles of the predictor variables) and have them predict the outcome variable.\n\nCompute the statistics given their answers. Show them the results. Ask them to give an uncertainty range around that statistic.\nExample: From their predictions, you calculate the mean. Then you present them with this average and ask them about their uncertainty?\n\ni.e.¬†What is the range around this value they expect the average to be in?\n\n\nAlso try combinations of methods\nSee paper in Misc for other options\n\nFeedback session\n\nExplain to the expert how you‚Äôre interpreting their information. Do they agree with your interpretation? Refine information based on their feedback.\n\nElicit scale and shape parameters (upper and lower bounds)\nFeedback session\nEvaluate distribution\n\n\nEvaluating Expert Distribution(s)\n\nMisc\n\nMight be better to use another measure instead of K-L divergence (see Inspect the distributions visually section below)\n\ne.g Jensen-Shannon Divergence, Population Stability Index (see Production, ML Monitoring for details)\n\n\nCalculate K-L divergence between the expert distribution and the computed posterior using the expert distribution as a prior\n\nSmaller K-L divergence means the 2 distributions are more similar\nLarger K-L divergence means the 2 distributions are more different\n\nCreate a benchmark distribution\n\nShould be a low information distribution as compared to the sample data distribution\ne.g.¬†uniform distribution\n\nCalculate K-L divergence between the benchmark distribution and the computed posterior using the benchmark distribution as the prior\nCalculate ratio of K-L divergences (expert K-L/benchmark K-L)\n\nGreater than 1 is bad. Indicates a ‚Äúprior data conflict‚Äù and it may be better to drop this expert‚Äôs distribution\nLess than 1 is good. Potentially an informative prior\n\nInspect the distributions visually (Expert prior distributions and computed posterior from benchmark prior)\n\nK-L divergence penalyzes more certain distributions (i.e.¬†skinny, tall) than less certain distribtutions (fatter, shorter) even if they have the same mean/median and mostly the same information\n\nSo, an expert that is more certain may have a disqualifying ratio of K-L difference while a less certain expert with a very similar distribution has a qualifying ratio.\n\nAfter inspecting the distributions, you may determine that distributions really are too different and the expert is far too certain to keep.\n\n\n\n\nAggregate distributions if you‚Äôre eliciting from multiple experts\n\nAverage the distributions (i.e.¬†equal weights for all experts)\nRank experts (e.g.¬†by K-L ratio), weight them, then calculate a weighted average distribution\nUse aggregated distribution(s) as your prior(s)",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/css-general.html",
    "href": "qmd/css-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CSS",
      "General"
    ]
  },
  {
    "objectID": "qmd/css-general.html#sec-css-gen-misc",
    "href": "qmd/css-general.html#sec-css-gen-misc",
    "title": "General",
    "section": "",
    "text": "Resources\n\nOnline Interactive Cheat Sheet\nhttps://css-tip.com/\nWidget testing parameter values for css styling a div box\n\nColumn Widths in CSS Grid\n\nCSS comment - /* comment */\nSelector formats\n\nSyntax: #&lt;class&gt;.&lt;id&gt;&lt;additional-stuff&gt;\nExample:\n\nCSS\n#header.fluid-row::before{\n}\nHTML\n&lt;div class=\"fluid-row\" id=\"header\"&gt; == $0\n::before\n&lt;/div&gt;\n\n\nInclude css styling directly into a html page\n\nExample: Via HTML style tag\n&lt;style&gt;\nbody {\n¬† padding: 50px 25px 0px 25px;\n¬† font-family: 'Roboto', sans-serif;\n¬† font-size: 19px;\n}\n&lt;/style&gt;\nExample: Via R chunk\nhtmltools::tags\\$link(href = \"https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght\\@0,400;0,700;1,400&display=swap\",\n                      rel = \"stylesheet\")\nExample: styling of a legend html div\n&lt;style type='text/css'&gt;\n¬† .my-legend .legend-title {\n¬† ¬† text-align: left;\n¬† ¬† margin-bottom: 8px;\n¬† ¬† font-weight: bold;\n¬† ¬† font-size: 90%;\n¬† ¬† }\n¬† .my-legend .legend-scale ul {\n¬† ¬† margin: 0;\n¬† ¬† padding: 0;\n¬† ¬† float: left;\n¬† ¬† list-style: none;\n¬† ¬† }\n¬† .my-legend .legend-scale ul li {\n¬† ¬† display: block;\n¬† ¬† float: left;\n¬† ¬† width: 50px;\n¬† ¬† margin-bottom: 6px;\n¬† ¬† text-align: center;\n¬† ¬† font-size: 80%;\n¬† ¬† list-style: none;\n¬† ¬† }\n¬† .my-legend ul.legend-labels li span {\n¬† ¬† display: block;\n¬† ¬† float: left;\n¬† ¬† height: 15px;\n¬† ¬† width: 50px;\n¬† ¬† }\n¬† .my-legend .legend-source {\n¬† ¬† font-size: 70%;\n¬† ¬† color: #999;\n¬† ¬† clear: both;\n¬† ¬† }\n¬† .my-legend a {\n¬† ¬† color: #777;\n¬† ¬† }\n&lt;/style&gt;\n\nSee link for details on the legend div element that uses this CSS",
    "crumbs": [
      "CSS",
      "General"
    ]
  },
  {
    "objectID": "qmd/css-general.html#centering",
    "href": "qmd/css-general.html#centering",
    "title": "General",
    "section": "Centering",
    "text": "Centering\n\nThere are also instructions for placing elements in different positions (e.g.¬†right edge)\nNotes from How To Center a Div\n\nThere‚Äôs also code/explainer for centering elements (e.g.¬†images) that have to stacked on top of each other\n\n\n\nElements\n\nCenter Horizontally with auto-margins\n.element {\n  max-width: fit-content;\n  margin-left: auto;\n  margin-right: auto;\n  /* margin-inline: auto*/\n}\n\nUse when you want to horizontally center a single element without disturbing any of its siblings\nmax-width is used because if width is used instead, it would lock it to that size, and the element would overflow when the container is really narrow.\nIncluding only margin-left: auto will force the div flush with the right side and vice verse with margin-right\nmargin-inline: auto can replace both margin-left and margin-right to center the div\n\nCentering Vertically and Horizontally\n.container {\n  align-content: center;\n}\n.element {\n  max-width: fit-content;\n  margin-inline: auto;\n}\nCenter Vertically and Horizontally with Flexbox\n/* single element */\n.container {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n}\n/* multiple elements */\n.container {\n  display: flex;\n  flex-direction: row;\n  justify-content: center;\n  align-items: center;\n  gap: 4px;\n}\n\nThe most versatile method; it can be used to center one or multiple children, horizontally and/or vertically, whether they‚Äôre contained or overflowing.\nflex-direction controls the direction in which the items are aligned, and it can have other values: column, row-reverse, column-reverse\n\nText\ncontainer {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n  text-align: center;\n}\n\nBlocks of text can be treated as one element and can be centered using the previous methods. This code (text-align) is for centering the rows of text within a element‚Äôs block.\n\n\n\n\nViewports\n\nUseful for elements like dialogs, prompts, and GDPR banners need to be centered within the viewport. (Think pop-ups)\nCentering With Known Sizes\n.element {\n  position: fixed;\n  inset: 0px;\n  width: 12rem;\n  height: 5rem;\n  max-width: 100vw;\n  max-height: 100dvh;\n  margin: auto;\n}\n\nComplex and has more settings that depend on the element. See article for details but there are four main concepts:\n\nFixed positioning\nAnchoring to all 4 edges with inset: 0px\nConstrained width and height\nAuto margins\n\nOmitting top: 0px will anchor the element to the bottom\n\nUse calc with max-width to make sure theres a buffer around the element\nmax-width: calc(\n    100vw - 8px * 2\n  );\n\n\nCentering Elements With Unknown Sizes\n.element {\n  position: fixed;\n  inset: 0;\n  width: fit-content;\n  height: fit-content;\n  margin: auto;\n}\n\nfit-content is doing the work",
    "crumbs": [
      "CSS",
      "General"
    ]
  },
  {
    "objectID": "qmd/html.html",
    "href": "qmd/html.html",
    "title": "HTML",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "HTML"
    ]
  },
  {
    "objectID": "qmd/html.html#sec-html-misc",
    "href": "qmd/html.html#sec-html-misc",
    "title": "HTML",
    "section": "",
    "text": "Resources\n\nOnline Interactive Cheat Sheet\n\nSome JS libraries use custom attributes in html tags that have hyphens in their name. R hates hypens, but you can just put the attribute name in quotes and it works (e.g.¬†data-sub-html).\n# see data-sub-html\ntags$a(\n  href = paste0(\"images/gallery/large/\", l),\n  \"data-sub-html\" = \"&lt;h4&gt;Photo by - &lt;a href='https://unsplash.com/@entrycube' &gt;Diego Guzm√°n &lt;/a&gt;\",\n  tags$img(src = paste0(\"images/gallery/thumbnails/\", t))\n  )\nglue(\"&lt;b style='background-color:{color}; font-family:Roboto; font-size:15px'&gt;{value}&lt;/b&gt;\")\n\ncolor and value are variables\n\nglue(\"&lt;b style= 'font-family:Roboto; font-size:15px'&gt;{name}&lt;/br&gt;Combined Indicator&lt;/b&gt;: {value_text}\")\n\nname and value_text are variables\n\nhtml coment - &lt;!-- comment --&gt;\nwithTags - Instead of needing to specify tags each time a tag function is used, as in tags\\(div() and tags\\)p(), code inside withTags is evaluated with tags searched first, so you can simply use div() and p().\ntagList - takes a list of tag objects and combines them into html code\nExample: From my website gallery\n\nhtml\n&lt;!---- withTags part ----&gt;\n&lt;div class=\"row\" id=\"lightgallery\"&gt;\n  &lt;!---- tagsList part ----&gt;\n  &lt;a href=\"_gallery/img/images/gallery/large/excess-death-col.png\"&gt;\n    &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-excess-death-col.png\"/&gt;\n  &lt;/a&gt;\n  &lt;a href=\"_gallery/img/images/gallery/large/pos-policy-one-2021-06-20.png\"&gt;\n    &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-pos-policy-one-2021-06-20.png\"/&gt;\n  &lt;/a&gt;\n&lt;/div&gt;\n{htmltools}\n\nCreate list of tags\n# images_thumb, images_full_size are paths to png files\nmoose &lt;- \n  purrr::map2(images$images_thumb, images$images_full_size, \n    function(t, l) {\n      tags$a(\n        href = paste0(\"_gallery/img/\", l),\n                      tags$img(src = paste0(\"_gallery/img/\", \n                      t))\n      )\n    })\n\n#&gt; [[1]]\n#&gt; &lt;a href=\"_gallery/img/images/gallery/large/excess-death-col.png\"&gt;\n#&gt; &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-excess-death-col.png\"/&gt;\n#&gt; &lt;/a&gt;\n\n#&gt; [[2]]\n#&gt; &lt;a href=\"_gallery/img/images/gallery/large/pos-policy-one-2021-06-20.png\"&gt;\n#&gt; &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-pos-policy-one-2021-06-20.png\"/&gt;\n#&gt; &lt;/a&gt;\nConvert list of tags to code with tagsList\nsquirrel &lt;- tagsList(moose)\n\n#&gt; &lt;a href=\"_gallery/img/images/gallery/large/excess-death-col.png\"&gt;\n#&gt; &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-excess-death-col.png\"/&gt;\n#&gt; &lt;/a&gt;\n#&gt; &lt;a href=\"_gallery/img/images/gallery/large/pos-policy-one-2021-06-20.png\"&gt;\n#&gt; &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-pos-policy-one-2021-06-20.png\"/&gt;\n#&gt; &lt;/a&gt;\nInsert into a div frame with withTags\nwithTags(\n  div(\n    class = \"row\",\n    id = \"lightgallery\",\n    squirrel\n  )\n)\n#&gt; &lt;div class=\"row\" id=\"lightgallery\"&gt;\n#&gt;    &lt;a href=\"_gallery/img/images/gallery/large/excess-death-col.png\"&gt;\n#&gt;        &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-excess-death-col.png\"/&gt;\n#&gt;    &lt;/a&gt;\n#&gt;    &lt;a href=\"_gallery/img/images/gallery/large/pos-policy-one-2021-06-20.png\"&gt;\n#&gt;        &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-pos-policy-one-2021-06-20.png\"/&gt;\n#&gt;    &lt;/a&gt;\n#&gt; &lt;/div&gt;",
    "crumbs": [
      "HTML"
    ]
  },
  {
    "objectID": "qmd/information-theory.html",
    "href": "qmd/information-theory.html",
    "title": "Information Theory",
    "section": "",
    "text": "Kullback-Lieber Divergence",
    "crumbs": [
      "Information Theory"
    ]
  },
  {
    "objectID": "qmd/information-theory.html#sec-infothy-kldiv",
    "href": "qmd/information-theory.html#sec-infothy-kldiv",
    "title": "Information Theory",
    "section": "",
    "text": "Figure by Erik-Jan van Kesteren\n\n\n\nMeasures the similarity between the joint probability density function and the product of the individual density functions\n\nIf they‚Äôre the same, then both variables are independent\n\nAlso see Statistical Rethinking &gt;&gt; Chapter 7\nExample: Measuring Segregation (link)\n\\[\nL_u = \\sum_{g=1}^G p_{g|u} \\log \\frac{p_{g|u}}{p_g}\n\\]\n\n\\(p_{g|u}\\) is the proportion of a racial group, \\(g\\), in a neighborhood, \\(u\\)\n\\(p_g\\) is the overall proportion of that racial group in the metropolitan area\nThis is a sum of scores across all racial groups of a neighborhood, \\(u\\)",
    "crumbs": [
      "Information Theory"
    ]
  },
  {
    "objectID": "qmd/information-theory.html#sec-infothy-mi",
    "href": "qmd/information-theory.html#sec-infothy-mi",
    "title": "Information Theory",
    "section": "Mutual Information",
    "text": "Mutual Information\n\nMeasures how dependent two random variables are on one another\n\nAccounts for linear and non-linear dependence\n\nIf the mutual information is 0, the variables are independent, otherwise there is some dependence.\nExample: Measuring Segregation\n\\[\nM = \\sum_{u=1}^U p_uL_u\n\\]\n\n\\(L_u\\) : See example in Kullback-Lieber Divergence section\n\\(p_u\\) : Described as the ‚Äúsize of the neighborhood‚Äù\n\nNot sure if this is a count or a proportion of the population of the neighborhood to the population of the metropolitan area. Both may end up in the same place.\n\nThis is a sum of scores across all neighborhoods in a metropolitan area\n\nSo the neighborhood scores are weighted by neighborhood population and summed for an overall metropolitan score\n\\(L_u\\) is affected by the smallest racial proportion (see article) for that metropolitan area, so unless these are the same, you can‚Äôt compare metropolitan areas with this number. But you can use these numbers to see how a metro‚Äôs (or neighborhood‚Äôs) diversity has changed over time.\n\n\n\n\n\n\nFigure by Erik-Jan van Kesteren",
    "crumbs": [
      "Information Theory"
    ]
  },
  {
    "objectID": "qmd/teaching.html",
    "href": "qmd/teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-misc",
    "href": "qmd/teaching.html#sec-teach-misc",
    "title": "Teaching",
    "section": "",
    "text": "Also see\n\nBkmks:\n\nScience and Mathematics &gt;&gt; Education\nData Science &gt;&gt; Modeling &gt;&gt; Reporting, Sharing, Publishing &gt;&gt; Concepts &gt;&gt; Teaching\n\n\ntl;dr for writing¬†article or preparing a talk Audience\n\nKnow their level of knowledge\nWhy they should want to know this\nHow does that reason get them closer to their goal\n(Maybe those last 2 are the same?)\n\nBooks\n\nTeaching what you dont know\nThe discussion book -¬†50 great ways to get people talking\n\nOn average, 7 \\(\\pm\\) 2 things can be kept in short term memory\nTeach chunks of your concept map. Each new chunk should be adjacent to the previous chunk.\nSacrifice truth for clarity to give learner actionable concept\nExplaining an unclear concept from the homework or reading (Gelman)\n\nBetter to work through an example than to try to clarify a definition or restate it, etc.\nThen, ask the students to get into pairs and explain to each other the meaning of each of the concepts in question\nThen, if students want to ask questions on the concept, we could do it in the context of this example that they‚Äôve just been talking about. We could also loop back to their homework assignment.",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-steps",
    "href": "qmd/teaching.html#sec-teach-steps",
    "title": "Teaching",
    "section": "Steps",
    "text": "Steps\n\nState goalpost\nSplit goalpost into concepts\nConnect concepts to form map\nIf more than 7 \\(\\pm\\) 2 concepts, then group in chunks\nThe next chunk of concepts you present should be adjacent to previous chunk\nSummarize",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-chtaud",
    "href": "qmd/teaching.html#sec-teach-chtaud",
    "title": "Teaching",
    "section": "Characterize the Audience",
    "text": "Characterize the Audience\n\nGeneral background\nrelevant experience\nperceived needs\nspecial consideration",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-mentmod",
    "href": "qmd/teaching.html#sec-teach-mentmod",
    "title": "Teaching",
    "section": "Mental Model",
    "text": "Mental Model\n\nDraw a concept map. Concepts and connections between them\nExamples:¬†Venn¬†diagrams, flow charts",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-ass",
    "href": "qmd/teaching.html#sec-teach-ass",
    "title": "Teaching",
    "section": "Assessments",
    "text": "Assessments\n\nSummative\n\nSummary of what you want to be learned. Goalpost\nGuide to creating¬†formative assessments by working backwards from the endpoint\n\nFormative\n\nIs the learning working?¬†\nTypes of mistakes or questions are clues to the types of misconceptions that learners are thinking and what you should say next.\nDiagnosing misconceptions by checking in every few minutes with questions. Think about what those answers might be and they mean.\nQuestions should have diagnostic power\nTells you if its okay to move on to the next lesson.",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-forces",
    "href": "qmd/teaching.html#sec-teach-forces",
    "title": "Teaching",
    "section": "Forces for Learning",
    "text": "Forces for Learning\n\nPositive\n\nIntrinsic Motivation - Learner isnt being made to learn something, theyre choosing to learn\nUtility - Moves them towards their goals\nCommunity - Not alone in learning, connection to peers, more comfortable about not knowing someting\n\nNegative\n\nUnpredictability - ‚ÄúWhat i do doesnt seem to affect the outcome‚Äù, learned helplessness\nUnfairness - Teacher Bias\nIndifference - Feeling that the teacher doesnt care about your problem",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-tidyc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-tidyc",
    "title": "Census Data",
    "section": "tidycensus",
    "text": "tidycensus\n\nGet an API key\n\nRequest a key, then activate the key from the link in your email.(https://api.census.gov/data/key_signup.html)\n\nRequired for hitting the census API over 500 times per day which isn‚Äôt as hard as you‚Äôd think.\n\nSet as an environment variable: census_api_key(\"&lt;api key&gt;\", install = TRUE)\n\nOr add this line to .Renviron file, CENSUS_API_KEY=‚Äò&lt;api key‚Äô\n\n\nSearch Variables\n\nColumns\n\nName - ID of the variable (Use this in the survey functions)\nLabel - Detailed description of the variable\nContext - Subject of the table that the variable is located in.\n\nPrefixes (Variables can have combinations of prefixes)\n\nP: i.e.¬†Person; Data available at the census block and larger\nCT: Data available at the census track and larger\nH: Data available at the Housing Unit level\n\nI think housing unit is an alternatve unit. So instead of the unit being a person, which I assume is the typical unit, it‚Äôs a housing unit (~family).\nNot affected by Differential Privacy (i.e.¬†no noise added; true value)\nExample: Total Deleware housing units at census block level\ndp_households &lt;- \n      get_decennial(\n            geography = \"block\",\n            variables = \"H1_001N\",\n            state = \"DE\",\n            sumfile = \"dhc\",\n            year = 2020\n      )\n\n\nExample: DHC data in census for 2020\n\nvars &lt;- load_variables(2020, \"dhc\")\n\nView(vars)\n\nView table, click filter, and then search for parameters (e.g.¬†Age, Median, etc.) with the Label, Context boxes, and overall search box\n\n\nsummary_var - Argument for supplying an additional variable that you need to calculate some kind of summary statistic\n\nExample: Race Percentage per Congressional District\n\nrace_vars &lt;- c(\n  Hispanic = \"P5_010N\", # all races identified as hispanic\n  White = \"P5_003N\", # white not hispanic\n  Black = \"P5_004N\", # black not hispanic\n  Native = \"P5_005N\", # native american not hispanic\n  Asian = \"P5_006N\", # asian not hispanic\n  HIPI = \"P5_007N\" # hawaiian, islander not hispanic\n)\n\ncd_race &lt;- \n  get_decennial(\n    geography = \"congressional district\",\n    variables = race_vars,\n    summary_var = \"P5_001N\", # total population for county\n    year = 2020,\n    sumfile = \"cd118\"\n)\n\ncd_race_percent &lt;- \n  cd_race %&gt;%\n    mutate(percent = 100 * (value / summary_value)) %&gt;% \n    select(NAME, variable, percent)",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-map",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-map",
    "title": "Census Data",
    "section": "Mapping",
    "text": "Mapping\n\nMisc\n\nUse geometry = TRUE for any of the get_* {tidycensus} functions, and it‚Äôll join the shapefile with the census data. Returns a SF (Simple Features) dataframe for mapping.\nIf you only want the shape files without the demographic data, see {tigris}\nFor examples with {tmap}, see Chapter 6.3 of Analyzing US Census Data\n{mapview} along with some other packages gives you some tools for comparing maps (useful for eda or exploratory reports, etc.) (m1 and m2 are mapview objects)\n\nm1 + m2 - Creates layers that allows you click the layers button and cycle through multiple maps. So I assume you could compare more than two maps here.\nm1 | m2 - Creates swipe map (need {leaflet.extras2}). There will be a vertical slider that you can interactively slide horizontally to gradually expose one map or the other.\nsync(m1, m2) - Uses {leafsync} to create side by side maps. Zooming and cursor movement are synced on both maps.\n\n\n\n\nPreprocessing\n\nRemove water from geographies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnyc_income_tiger &lt;- \n  get_acs(\n    geography = \"tract\",\n    variables = \"B19013_001\",\n    state = \"NY\",\n    county = c(\"New York\", \"Kings\", \"Queens\",\n               \"Bronx\", \"Richmond\"),\n    year = 2022,\n    cb = FALSE,\n    geometry = TRUE\n)\n\nlibrary(tigris)\nlibrary(sf)\nsf_use_s2(FALSE)\n\nnyc_erase &lt;- \n  erase_water(\n    nyc_income_tiger,\n    area_threshold = 0.5,\n    year = 2022\n)\n\nmapview(nyc_erase, zcol = \"estimate\")\n\nThe left figure is before and the right figure is after water removal\n\nAt the center-bottom, you can see how a sharp point is exposed where it was squared off before\nAt the top-left, the point along the border which are piers/docks are now exposed.\nAt the upper-middle, extraneous boundary lines have been removed and islands in the waterway are more clearly visible.\n\nWorks only with regular tigris shapefiles from the US census bureau ‚Äî so not OpenStreetMaps, etc. For other shapefiles, you‚Äôd need to do the manual overlay, see Chapter 7.1 in Analyzing US Census Data for details.\nCan take a couple minutes to run\ncb = FALSE says get the regular tigris line files which avoid sliver polygons which are caused by slight misalignment of layers (?)\narea_threshold = 0.5 says that water areas below the 50th percentile in terms of size are removed. Probably a value you‚Äôll have to play around with.\n\n\n\n\nChoropleths\n\nBest for continuous data like rates and percentages, but you can use for discrete variables\n\nYou can create a discrete color palette with the at argument in the mapview function.\n\nExample\n# check min and max of your data to select range of bins\nmin(iowa_over_65, \n    na.rm = TRUE) # 0\nmax(iowa_over_65, \n    na.rm = TRUE) # 38.4\n\nm1 &lt;- \n  mapview(\n    iowa_over_65, \n    zcol = \"value\",\n    layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n    col.regions = inferno(100, direction = -1),\n    at = c(0, 10, 20, 30, 40)\n  )\n\nThis will result in a discrete palette with bins of 0-10, 10-20, etc. Looks like an overlap, so I‚Äôm sure which bin contains the endpoints.\n\n\n\nExample: Over 65 in Iowa by census tract\n\nlibrary(mapviw); library(viridisLite)\n\niowa_over_65 &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"DP1_0024P\",\n    state = \"IA\",\n    geometry = TRUE,\n    sumfile = \"dp\",\n    year = 2020\n  )\nm1 &lt;- \n  mapview(\n    iowa_over_65, zcol = \"value\",\n    layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n    col.regions = inferno(100, \n                          direction = -1))\n\n{mapview} is interactive and great for exploration of data\n\nExport as an HTML file\nhtmlwidgets::saveWidget(m1@map, \"iowa_over_65.html\")\n\nCan embed it elsewhere (html report or website) by adding it as an asset\n\nIn {ggplot}\ntexas_income_sf &lt;- \n  get_acs(\n    geography = \"county\",\n    variables = \"B19013_001\",\n    state = \"TX\",\n    year = 2022,\n    geometry = TRUE\n)\n\nplot(texas_income_sf['estimate'])\n\n\n\nCircle Maps\n\n‚ÄúGraduated Symbol‚Äù maps are better for count data. Even though using a choropleth is not as bad at the census tract level since all tracts have around 4000 people, the sizes of the tracts can be substantially different which can influence the interpretation. Using circles or bubbles, etc. focuses the user on the size of the symbol and less on the size of the geography polygons.\nExample: Hispanic Counts in San Diego County at the Census Tract Level\n\n\nsan_diego_race_counts &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    Hispanic = \"DP05_0073\",\n    White = \"DP05_0079\",\n    Black = \"DP05_0080\",\n    Asian = \"DP05_0082\"\n  ),\n  state = \"CA\",\n  county = \"San Diego\",\n  geometry = TRUE,\n  year = 2022\n)\n\nsan_diego_hispanic &lt;- \n  filter(\n    san_diego_race_counts, \n    variable == \"Hispanic\"\n  )\n\ncentroids &lt;- st_centroid(san_diego_hispanic)\n\ngrad_symbol &lt;- \n  ggplot() + \n  geom_sf(\n    data = san_diego_hispanic, \n    color = \"black\", \n    fill = \"lightgrey\") + \n  geom_sf(\n    data = centroids, \n    aes(size = estimate),\n    alpha = 0.7, \n    color = \"navy\") + \n  theme_void() + \n  labs(\n    title = \"Hispanic population by Census tract\",\n    subtitle = \"2018-2022 ACS, San Diego County, California\",\n    size = \"ACS estimate\") + \n  scale_size_area(max_size = 6)\n\nst_centroid finds the center point of geography polygons which will be the location of the symbols. If you look at the geometry column it will say POINT, which only has a latitude and longitude, instead of POLYGON, which as multiple coordiates.\nscale_size_area scales the size of the circle according to the count value.\n\nmax_size is the maximum diameter of the circle which you‚Äôll want to adjust to be large enough so that you can differentiate the circles but small enough so you have the least amount of overlap between circles in neighboring geographies (although this is probably inevitable).\n\n\n\n\n\nDot Density\n\nUseful to show heterogeneity and mixing between groups versus plotting group facet maps.\nExample: Population by Race in San Diego County\n\nsan_diego_race_dots &lt;- \n  as_dot_density(\n    san_diego_race_counts, # see circle maps example for code\n    value = \"estimate\", # group population\n    values_per_dot = 200,\n    group = \"variable\" # races\n  )\n\ndot_density_map &lt;- \n  ggplot() + \n  annotation_map_tile(type = \"cartolight\", \n                      zoom = 9) + \n  geom_sf(\n    data = san_diego_race_dots, \n    aes(color = variable), \n    size = 0.01) + \n  scale_color_brewer(palette = \"Set1\") + \n  guides(color = guide_legend(override.aes = list(size = 3))) + \n  theme_void() + \n  labs(\n    color = \"Race / ethnicity\",\n    caption = \"2018-2022 ACS | 1 dot = approximately 200 people\")\n\nas_dot_density scatters the dots randomly within a geography. values_per_dotsays each dot is 200 units (e.g.¬†people or households). Without shuffling, ggplot will layer each group‚Äôs dots on top of each other.\nannotation_map_tile from {ggspatial} applies a base map layer as a reference for the user. Base maps have land marks and popular features labeled in the geography and surrounding areas to help the user identify the area being shown.",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html",
    "href": "qmd/causal-inference.html",
    "title": "Causal Inference",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-misc",
    "href": "qmd/causal-inference.html#sec-causinf-misc",
    "title": "Causal Inference",
    "section": "",
    "text": "Notes from\n\nhttps://fabiandablander.com/r/Causal-Inference.html\n\nStatistical models measure associations (e.g.¬†linear, non-linear) which is mutual information among the variables\n\ne.g.¬†wind and leaves moving in a tree (doesn‚Äôt answer whether the leaves moving creates the wind or the wind creates leaving moving)\n\nCausal inference predicts the conseqences after an intervention (i.e.¬†action)\n\nYou must know the direction of causation in order to predict the conseqences of an intervention (unlike measuring associations)\nAnswers the question, ‚ÄúWhat happens if I do this?‚Äù\n\nCausal inference is able to reconstruct unobserved counterfactual outcomes.\n\nAnswers the question, ‚ÄúWhat happens if I had done something else?‚Äù\n\nCausal assumptions are necessary in order to make causal inferences\n\nmultiple regression does not distinguish causes from confounds\np-values are not causal statements\n\nDesigned to control type I error rate\n\nAIC, etc are purely predictive\n\nCausal Experiment Assumptions\n\nsee tlverse workshop notes and ebook for listing of assumptions and definitions,¬† https://tlverse.org/acic2019-workshop/intro.html#identifiability\n\nThe tlverse Project seeks to use ML models to calculate causal effects. Uses Super Learner ensembling and Targeted Maximum Likelihood Estimation (TMLE) which they call Targeted Learning.\n\nIgnorability - By randomly assigning treatment, researchers can ensure that the potential outcomes are independent of treatment assignment, so that the average difference in outcomes between the two groups can only be attributable to treatment\n\nEngineering outcome variables using potential adjustment variables does not automatically adjust for those variables in your model\n\nNotes from There Are No Magic Outcome Variables\nExample\n\n\nP is population density\nX is the variable of interest\nGDP and P have been used to create GDP/P\nP influences X and provides a backdoor path to GDP/P, so P must be adjusted for\nEven if P doesn‚Äôt influence X, the point is that constructing GDP/P using P doens‚Äôt automatically adjust for P\n\n\nRandomized experiments remove all paths from the treatment variable, X\n\n\nAdjusting for Z, B, and C can add precision to measurement of the treatment effect since they are causal to Y, but they aren‚Äôt necessary to get an unbiased estimate of the treatment effect.\n\nTable 2 fallacy (Notes from McElreath video, 2022 SR Lecture 6)\n\n\nThe 2nd table presented in a paper is usually a summary of all the effects of a regression. The fallacy is that the coefficient of each variable is treated as causal.\nExample: The effect of HIV on Stroke\n\nThe model is lm(Stroke ~ HIV + Smoke + Age)\n\nOnly the coefficient of the HIV variable should be treated as causal and none of the other adjustment variables (Smoke, Age)\n\nThe effects for Smoke and Age are only partial.\nThere are likely unobserved confounding variables, U, on the effect of Smoking on Stroke (e.g.¬†other lifestyle variables).\n\nSmoke is confounded so it‚Äôs causal estimate is biased\nAge is also confounded since Smoke is now a collider and has been conditioned upon. This opens the non-causal path, Age-Smoke-U-Stroke.\n\nAge-Smoke is frontdoor, but the backdoor path, Smoke-U, also becomes a backdoor path for Age once Smoke is conditioned upon. (aka sub-backdoor path)\nSo any open path that contains a backdoor path must also be closed\n\n\n\nSolutions\n\nDon‚Äôt include effect estimates of adjustment variables\nExplicitly interpret each effect estimate according to the causal model\n\nSee 2022 SR at the end of Lecture 6 where McElreath breaks down the interpretation of each adjustment variable estimated effect.\n\n\n\nPartial Identification (Handling Unobserved Confounds)\n\nMisc\n\nAlso see\n\nPaper: Hidden yet quantifiable: A lower bound for confounding strength using randomized trials (code)\n\nUsing RCT results and Observational data, this paper proposes a statistical test and a method for determining the lower bound confounder strength.\nIn the context of pharmacuticals, RCT results are evidently often released after FDA approval, but this method can be used in any field where there‚Äôs a combination of RCT and observational studies..\n\n\n\nSometimes the confounding paths of a DAG model can be not be resolved.\n\nFor confounders that influence the treatment and outcome, see:\n\nStructural Causal Models &gt;&gt; Bayesian examples\nIf there‚Äôs a mediator, see Other Articles &gt;&gt; Frontdoor Adjustment\n\nMeasure proxies for the unobserved confound if it‚Äôs not practical/ethical to measure\n\ni.e.¬†If the confound is ability, then test scores, letters of recommendation, etc. could be proxies.\n\nExample: 2022 SR Lecture 10 video, code\n\n\nA: Admitted to Grad School, G: Gender, D: Dept, u: Ability, T1,2,3: 3 Test Scores\n\nAbility is latent variable/unobserved confounder\nTest Scores are proxies for Ability\n\nBoth models are fit simultaneously\nCouldn‚Äôt find a way to use {brms} to code this and Kurz didn‚Äôt included it in his brms SR book.\n\n\n\nA biased estimate is better than no estimate. It can provide an upper bound\nFind a natural experiment or design one\nSensitivity Analysis\n\nAfter the analyis, you should be able to make the statement, ‚ÄúIn order for the confound to be responsible for the entire causal effect, it was have to be .‚Äù\n\n\nPackages\n\n{tipr} - Tools for tipping point sensitivity analyses\n{konfound} (vignette): Utilizes a sensitivity analysis approach that extends from dichotomous outcomes and omitted variable sensitivity of previous approaches to continuous outcomes and evaluates both changes in estimates and standard errors.\n\nITCV (Impact Threshold of a Confounding Variable) - Generates statements about the correlation of an omitted, confounding variable with both a predictor of interest and the outcome. The ITCV index can be calculated for any linear model.\nRIR (Robustness of an Inference to Replacement) - Assesses how replacing a certain percentage of cases with counterfactuals of zero treatment effect could nullify an inference. The RIR index is more general than the ITCV index.\n\n\nSteps for using sensitivity analysis\n\nPerform a sensitivity analysis to determine plausibly how much of the causal effect is due to confounding paths\n\nAssume the confound exists, model it‚Äôs consequences for different strengths/kinds of influence\nExample: 2022 SR Lecture 10 video, code \n\nA: Admitted to Grad School, G: Gender, D: Dept, u: Unobserved Confounder\nBoth models are fit simultaneously\nValues for Œ≤ and Œ≥ are specified and u is estimated as a parameter\nI think Gender (G) is an interaction in both models which I didn‚Äôt think was possible given there are no arrows of influence from gender to u.\n\nSince gender is a moderator it wouldn‚Äôt necessarily have to be an influence arrow, it would only need to be an arrow from G to the effect of u on D (see Moderator Analysis), so maybe this is kosher\nCould also be that I‚Äôm misunderstanding McElreath‚Äôs code he uses to specify his models with {Rethinking}.\n\nCouldn‚Äôt find a way to use {brms} to code this and Kurz didn‚Äôt included it in his brms SR book.\n\n\nUse previous studies that have effect strengths of those potential confounding variables\nCompare the strengths from the previous studies to the strength determined from the sensitivity analysis. The difference is a good guess for the strength of the causal effect of your treatment variable.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-causdes",
    "href": "qmd/causal-inference.html#sec-causinf-causdes",
    "title": "Causal Inference",
    "section": "Causal Design",
    "text": "Causal Design\n\nNotes from McElreath video\nMisc\n\nWhen trying to determine the relationship (e.g.¬†linear, nonlinear) between variables and remove inconsequential variables, the Double Debiased ML procedure might be useful.\n\nDouble Debiased Machine Learning - basic concepts, links to papers, videos\nEconML (Microsoft) and causalml (Uber) has included the method in their libraries\n\n\nWhen trying to infer causal relationships, we should not blindly enter all variables into a regression in order to ‚Äúcontrol‚Äù for them, but think carefully about what the underlying causal DAG could look like. Otherwise, we might induce spurious associations (e.g.¬†confounding such as collider bias).\nOverview\n\nMake a causal model (i.e.¬†DAG)\n\nNeed background information in order to make the causal assumptions represented in the DAG\nDAGs only show whether or not a variable influences another, not how the influence occurs (e.g.¬†DAGs can‚Äôt show interactions between variables or whether the association is non-linear)\n\nUse it to design data collection and statistical procedures\n\nSteps:\n\nDetermine two variables of interest (exposure, outcome) that you want to determine if a causal relationship exists and what effect the exposure has.\nUse domain knowledge or prior scholarship to determine the relevant variable and the likely associations between all variables in data\nCreate the DAG\n\nIdentify the direct causal path between exposure and outcome\nIdentify other explanatory variables and label their directions of influence with each other, the exposure, and the outcome variable\nConsider which variables (especially the exposure and the outcome) have unobserved variables influencing them.\n\nAnalyze the DAG\n\nIdentify colliders and use d-separation to determine conditional independencies\nIdentify additional paths (backdoor paths, sub-backdoor paths) between exposure and outcome\nUse the backdoor criterion to determine the set of variables that need to be adjusted for in order to block all backdoor paths with only the direct causal path remaining open.\nAdd additional adjustment variables that are causal to the outcome variable (but don‚Äôt confound the treatment effect) in order to add precision to the estimate of the treatment effect\n\nCreate simulated data that fits the DAG (i.e.¬†a generative model)\nPerform statistical analysis (i.e.¬†SCMs) on the simulated data¬† to make sure you can measure the causal effect.\nDesign experiment and collect the data\nRun the statistical analysis on the collected data and calculate the average causal effect (ACE) under the assumptions that your DAG and model specifications are correct.\nBased on your results, revise the DAG and SCM as necessary and repeat as necessary\n\nBad Adjustment Variables (Code and more details included in 2022 SR, Lecture 6)\n\nFor all examples, Z is the adjustment variable that‚Äôs being considered; X is the treatment and Y is the outcome\n\nIn each scenario, including Z produces a biased estimate of X, so the correct model is Y ~ X.\n\nM-bias\n\n\nZ doesn‚Äôt have a direct causal influence on the either X or Y, but when it‚Äôs conditioned upon it becomes a collider due to unobserved confounds that have a direct causal influence on X and Y.\nCommon issue in Political Science and network analysis\nExample\n\nY: Health of Person 2\nX: Health of Person 1\nZ: Friendship status\n\nPre-treatment variable (tend to be open to collider paths) since they could be friends before the exposure\n\nU: Hobbies of Person 1\nV: Hobbies of Person 2\n\n\nPost-Treatment Bias\n\n\nZ is a mediator and conditioning upon Z blocks the path from X to Y, but opens the backdoor path through the unobserved confound, U.\nCommon in medical studies¬†¬†\nExample\n\nY: Lifespan\nX: Win Lottery\nZ: Happiness\nU: Contextual Confounds\n\n\nSelection Bias\n\n\nSame as collider bias\n\nThis version adds an unobserved confounder\n\nExample\n\nY: Income\nX: Education\nZ: Values\nU: Family\n\n\nCase-Control Bias\n\n\nZ is a descendent. Since Z has information about Y, conditioning on it will narrow the variation of Y and distort the measured effect of X.\nAlso see Association &gt;&gt; Single Path DAGs &gt;&gt; Descendent\nExample\n\nY: Occupation\nX: Education\nZ: Income\n\n\nPrecision Parasite\n\n\n2 versions: with and without U\n\nWithout U, conditioning on Z removes variation from X and lessens (but doesn‚Äôt bias) the precision of the estimated effect of X on Y (i.e.¬†inflated std.error)\nWith U, the effect of X is biased and that bias is amplified when Z is included.\n\n\nPeer Bias\n\n\nClassic DAG of the Berkley Admission-Race-Department study\nAlso see Structural Causal Models &gt;&gt; Example (Bayesian Peer Bias)\nX is race, E is department, Q is unobserved (e.g.¬†student quality), Y is Admission\nDepartment cannot be conditioned upon because it‚Äôs a collider with Q and would bias the estimate of X through a sub-backdoor path, X-E-Q-Y\nOnly the total effect of X on Y can be estimated (Y ~ X) since E cannot be conditioned upon but that‚Äôs not interesting and maybe not precise",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-terms",
    "href": "qmd/causal-inference.html#sec-causinf-terms",
    "title": "Causal Inference",
    "section": "Terms",
    "text": "Terms\n\nAverage Causal Efffect (ACE) - average population effect that‚Äôs calculated from an intervention (see Counterfactual definition for info on Individual Causal Effects)\n\nIf X is binary, then  ¬†is the average causal effect¬†(see Simpson‚Äôs Paradox example)\n\nCalculated from a contingency table\n\nAlso, \n\nThis looks like the interpretation of the slope in a regression model.\n\n\nBackdoor Criterion - A valid causal estimate is available if it is possible to condition on variables such that all backdoor paths are closed\n\nGiven two nodes, X and Y, an adjustment set,¬†L, fulfills the backdoor criterion if¬†\n\nno member in¬†L¬†is a descendant of X and\nmembers in¬†L¬†block all backdoor paths (‚Äúshutting the backdoor‚Äù) between X and Y.\n\nAdjusting for¬†L¬†thus yields the causal effect of X‚ÜíY.\nAfter executing an intervention, the conditional distribution in the observational DAG (seeing) will correspond to the interventional distribution (doing) when blocking the spurious path. (see Simpson‚Äôs Paradox example)\n\nBackdoor Path - A non-causal path that enters a causal variable in a DAG rather than exits it.\n\ne.g.¬†the path that connects a collider to a causal variable points from the collider to the causal variable\nSub-backdoor Path - this path begins with a frontdoor path but through conditioning on a variable, it opens a connecting backdoor path which biases the treatment effect\n\nsee Misc &gt;&gt; Table 1 Fallacy and Causal Design &gt;&gt; Bad Adjustment Variables &gt;&gt; Peer Bias\n\n\nThe causal effect is the distribution of Y when we change x, averaged over the distributions of the adjustment variables (Z)\nCausal Hierarchy (lowest to highest)\n\nAssociation\n\nassociated action: Seeing - observational; observing the value of Y when X = x\n\n , observational distribution; What values Y would likely take on if X happened to equal x.\n\n\nIntervention\n\nassociated action (do-Calculus): Doing -¬† experimental; observing the value of Y after setting X = x\n\n , interventional distribution; What values Y would likely take on if X would be set to x.\nUsing the do operator allows us to make inferences about the population but not individuals.\ndo(X) means to cut all of the backdoor paths into X, as if we did a manipulative experiment. The do-operator changes the graph, closing the backdoors.\nThe do-operator defines a causal relationship, because Pr(Y|do(X)) tells us the expected result of manipulating X on Y, given a causal graph.\n\nWe might say that some variable X is a cause of Y when Pr(Y|do(X)) &gt; Pr(Y|do(not-X)).\n\n(makes more sense to me with a binary outcome, Pr(Y = 1|do(X), but maybe Y as a continuous variable can be defined a subset. ‚Ä¶I dunno)\n\n\nThe ordinary conditional probability comparison, Pr(Y|X) &gt; Pr(Y|not-X), is not the same. It does not close the backdoor.\nNote that what the do-operator gives you is not just the direct causal effect. It is the total causal effect through all forward paths.\n\nTo get a direct causal effect, you might have to close more backdoors.\n\nThe do-operator can also be used to derive causal inference strategies even when some backdoors cannot be closed.\n\n\nCounterfactual\n\nassociated action: Imagining¬†- what would be the outcome if the alternative would‚Äôve happened.\nIndividual Causal Effects can be calculated but it requires stronger assumptions and deeper understanding of the causal mechanisms\n\nNeed to research this part further.\nIf the underlying SCM is linear then the ICE = ACE.\n\n\n\nA collider along a path blocks that path. However, conditioning on a collider (or any of its descendants) unblocks that path\n\nWhen a collider is conditioned upon, the change in the association between the two nodes it separates is called collider bias.\n\ne.g.¬†if Z is a collider between X and Y, conditioning upon Z will induce an association between X and Y.\n\n\nA conditioning set, \\(L\\), is the set of nodes we condition on (it can be empty).\nConfounding is the situation where a (possibly unobserved) common cause obscures the causal relationship between two or more variables.\n\nThere is more than one causal path between two nodes.\nA causal effect of X on Y is confounded if¬† \nCollider bias is a type of confounding. When a collider is controlled for, a second (or more) path opens, and the effect is confounded\n\nX and Y are d-separated by¬†[L¬†if conditioning on all members in¬†[L¬†blocks all paths between the nodes, X and Y.\n\nTool for checking the conditional independencies which are visualized in DAGs.\n\nA descendant is a node connected to a parent node by that parent node‚Äôs outgoing arrow.\nFrontdoor Adjustment - In a causal chain with three nodes X‚ÜíZ‚ÜíY, we can estimate the effect of X on Y indirectly by combining two distinct quantities: (Useful for when unobserved confounders prevent direct causal estimation)\n\nThe estimate of the effect of X on Z, P(Z|do(X))\nThe estimate of the effect of Z on Y, P(Y|do(Z), X)\n\nFrontdoor Path - a path that exits a causal variable in a DAG rather than enters it.\n\ne.g.¬†the path that connects a causal variable, X, to an outcome variable, Y, has an arrow that points from X to Y.\n\nMarkov Equivalence - A set of DAGs, each with the same conditional independencies\nMediation Analysis -¬†seeks to identify and explain the mechanism or process that underlies an observed relationship between an independent variable and a dependent variable via the inclusion of a third hypothetical variable, known as a mediator variable (z-variable in the DAGs of ‚Äúpipes‚Äù below)\n\nIncluding a mediator and the independent variable in a regression will result in the independent variable not being signficant and the mediator being significant.\n\nModeration Analysis - Like mediation analysis, it allows you to test for the influence of a third variable, Z, on the relationship between variables X and Y, but rather than testing a causal link between these other variables, moderation tests for when or under what conditions an effect occurs.\nA node is a parent of another node if it has an outgoing arrow to that node\nA path from X to Y is a sequence of nodes and edges such that the start and end nodes are X and Y, respectively.\nResidual Confounding occurs when a confounding variable is measured imperfectly or with some error and the adjustment using this imperfect measure does not completely remove the effect of the confounding variable.\n\nExample: Women who smoke during pregnancy have a decreased risk of having a Down syndrome birth.\n\nThis is puzzling, as smoking is not often thought of as a good thing to do. Should we ask women to start smoking during pregnancy?\nIt turns out that there is a relationship between age and smoking during pregnancy, with younger women being more likely to indulge in this bad habit. Younger women are also less likely to give birth to a child with Down syndrome. When you adjust the model relating smoking and Down syndrome for the important covariate of age, then the effect of smoking disappears. But when you make the adjustment using a binary variable (age&lt;35 years, age &gt;=35 years), the protective effect of smoking appears to remain.\n\n\nStructural Causal Models (SCMs) - relate causal and probabilistic statements; each equation is a causal statement\n\n\n\n‚Äú:=‚Äù is the assignment operator\nX is a direct cause of Y which it influences through the function f( )\n\nwhere f is a statistical model\n\nThe noise variables, œµX and œµY, are assumed to be independent.\n\nThere are Stochastic and Deterministic SCMs. Deterministic SCMs presented in article.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-assoc",
    "href": "qmd/causal-inference.html#sec-causinf-assoc",
    "title": "Causal Inference",
    "section": "Association",
    "text": "Association\n\n\n\nFar left: lm(Y ~ X); X and Y show a linear correlation when Z is NOT conditioned upon\nLeft: lm(Y ~ X + Z); X and Y show NO linear correlation when Z is conditioned upon\nRight: lm(Y ~ X); X and Y show NO linear correlation when Z is NOT conditioned upon\nFar Right:¬† lm(Y ~ X + Z); X and Y show a linear correlation when Z is conditioned upon",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-singpath",
    "href": "qmd/causal-inference.html#sec-causinf-singpath",
    "title": "Causal Inference",
    "section": "Single path DAGs",
    "text": "Single path DAGs\n\n\nFor each of these DAGs, Z would be the only member of the conditioning set.\nThe first 3 DAGs represent the scatter plots above\n\nZ only blocks the path between X and Y when it‚Äôs conditioned upon.\n\nX and Y are associated (e.g.¬†linear correlation, mutual information, etc.) when Z is ignored\nConditioning on Z results in X and Y no longer being associated (i.e.¬†conditional independence)\n\nThe first and second DAGs are elemental confounds or relations called ‚ÄúPipes.‚Äù\n\nThe left one\n\nIn general, DO NOT add these variables to your model\n\nThese paths are causal so they shouldn‚Äôt be blocked\nIf your goal isn‚Äôt causal inference, then adding these variables might provide predictive information\ne.g.¬†If there was a causal arrow from X to Y, the far left DAG would NOT have a backdoor path and therefore Z would not¬† be conditioned upon to block the path, X-Z-Y\n\nThe path from X to Z is a frontdoor path since the arrow exits X.\n\n\nSometimes you DO condition on these variables\n\nDuring mediation analysis, you condition on these variables as part of the process to determine how much of the effect goes through Z.\nThe mediation path can have an important interpretation depending on your research question\n\ne.g.¬†indirect descrimination\n\nSee Statistical Rethinking &gt;&gt; Chapter 11 &gt;&gt; Conclusion of Berkeley Admissions example\n\nalso Lecture 9 2022 video\n\n\n\n\n\nThe right one is a backdoor path and should be conditioned on.\nEverything you can learn about Y from X (or vice versa) happens through Z, therefore learning about X separately provides no additional information\nZ is traditionally labelled a mediator\n\nThe third DAG is an elemental confound¬† or relation called a ‚ÄúFork.‚Äù\n\nIn general, add these variables to your model\nThese are backdoor paths and are NOT causal\nX and Y have a common cause in Z and some of the mutual information about Z they each contain, overlaps, and creates an association (when Z isn‚Äôt conditioned upon).\n\n\nThe fourth DAG is an elemental confound or relation called a ‚ÄúCollider.‚Äù\n\n\nIn general, do NOT add these variables to your model\nZ blocks the path between X and Y unless conditioned upon.\nAn association between X and Y is induced¬† by conditioning on Z, lm(Y ~ X + Z)\n\nX and Y are independent causes of Z. Z contains information about both X and Y, but X doesn‚Äôt contain any information about Y and vice versa.\nA small X and a sufficiently large Y (and vice versa) can produce a Z = 1. So X and Y have compensatory relationship in causing Z.\n\ni.e.¬†For a given value of Z, learning something about X tells us what Y might have been.\n\n\n\nThe last elemental confound or relation is called a ‚ÄúDescendent.‚Äù\n\n\nConditioning on a descendent variable, D, is like conditioning on the variable, Z itself, but weaker. A descendent is a variable influenced by another variable.\nControlling for D will also control, to a lesser extent, for Z. The reason is that D has some information about Z. This will (partially) open the path from X to Y, because Z is a collider. The same holds for non-colliders. If you condition on a descendent of Z in the pipe, it‚Äôll still be like (weakly) closing the pipe.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-dualpath",
    "href": "qmd/causal-inference.html#sec-causinf-dualpath",
    "title": "Causal Inference",
    "section": "Dual path DAGs",
    "text": "Dual path DAGs\n\n\nCausal paths do not flow against arrows but associations can.\nTwo examples of DAGs representing confounding\n\nThese are the 2 middle DAGs above with an additional path from X to Y\nIf Z is NOT conditioned on (i.e.¬†top path is not blocked), then the causal effect of X on Y would be confounded.\n\n\n\n\nThe paths from X to Y:\n\nThe path through Z matches the first DAG.\n\nTherefore X and Y are conditionally independent given Z.\n\nThe path through W matches the fourth DAG\n\nTherefore X and Y are conditionally dependent given W.\n\n\nThe path through W (collider) is blocked unless W is conditioned upon\nThe path through Z is open unless Z is conditioned upon\nIf Z and W are conditioned upon, then the path between X and Y is open through W and an association is present.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-interv",
    "href": "qmd/causal-inference.html#sec-causinf-interv",
    "title": "Causal Inference",
    "section": "Intervention",
    "text": "Intervention\n\n\nSince actual interventions are usually unfeasible, we want to be able to determine causality with observational data. This requires two assumptions:\n\nThe intervention occurs locally. Which means that only the variable we target is the one that receives the intervention.\nThe mechanism by which variables interact do not change through interventions; that is, the mechanism by which a cause brings about its effects does not change whether this occurs naturally or by intervention\n\nThe Doing row of DAGs (aka manipulated DAGs) represents setting X = x\n\nFor DAGs 1 and 4, Y is still affected\n\nMoving from seeing to doing didn‚Äôt change anything\n\n\nFor DAGs 2 and 3, Y is now UNaffected\n\nUsing the assumptions and some mathematical manipulation (See article for details):\n\n\n\nThus, the interventional distribution we care about is equal to the (observational) conditional distribution of Y given X when we adjust for Z\n\n\n\n\nThe rule: After an intervention, incoming arrows are cut from the node where the intervention took place.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-confound",
    "href": "qmd/causal-inference.html#sec-causinf-confound",
    "title": "Causal Inference",
    "section": "Confounding",
    "text": "Confounding\n\n\nThe backdoor criterion tells us which variable we need to adjust for in order to for our model to yield a causal relationship between two variables (i.e.¬†graphically, nodes)\n\nBlocks all spurious, that is, non-causal paths between X and Y.\nLeaves all directed paths from X to Y unblocked\nCreates no spurious paths\n\nExample\n\nCausal effect of Z on U is confounded by X¬†because in addition to the legitimate causal path Z‚ÜíY‚ÜíW‚ÜíU, there is also an unblocked path Z‚ÜêX‚ÜíW‚ÜíU which confounds the causal effect\n\nSince X‚Äôs arrow enters the causal variable of interest, Z, it‚Äôs arrow is a backdoor path and needs to be blocked/closed\nThere are some descendant nodes that make the confounding a little difficult to parse out, but this graph is essentially\n\n\nwhich is the same as the second example DAG for confounding in the Association section\n\n\nThe backdoor criterion would have us condition on X, which blocks the spurious path and renders the causal effect of Z on U unconfounded.\n\nThe reduced, confounding DAG above is the same as the third DAG (without the path from Z to U) in the Association section. Conditioning on Z in that example blocked the path between X and Y, so it makes sense that conditioning on X in the reduced DAG would block the Z to X to U path. And therefore, the¬†Z‚ÜêX‚ÜíW‚ÜíU would also be blocked in the complete DAG.\n\nNote that conditioning on W would also block this spurious path; however, it would also block the causal¬†path, Z‚ÜíY‚ÜíW‚ÜíU.\n\n\nIf we breakdown the complete DAG into the modular components involving W, we can see these are the same as the first example DAG in the Association section.\nW is also collider for X and Y, but I don‚Äôt think that has any bearing when discussing the causal effect of Z on U.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-appsimp",
    "href": "qmd/causal-inference.html#sec-causinf-appsimp",
    "title": "Causal Inference",
    "section": "Application: Simpson‚Äôs Paradox Example",
    "text": "Application: Simpson‚Äôs Paradox Example\n\nSex as the adjustment variable¬†¬† ¬†¬†¬† ¬†¬†¬† \n\nPatients CHOOSE whether or not to take a drug to cure some disease.\nMen choosing to take the drug recover at a higher percentage that those that didn‚Äôt\nWomen choosing to take the drug recover at a higher percentage that those that didn‚Äôt\nBut overall, those that chose to take the drug recovered at a lower percentage than those that didn‚Äôt.\nSo should a doctor prescribe the drug or not?\nSuppose we know that women are more likely to take the drug, that being a woman has an effect on recovery more generally, and that the drug has an effect on recovery.¬†\nCreate DAGs\n\n\nS=1 as being female,\nD=1 as having chosen to take the drug\nR=1 as having recovered\nThe right DAG indicates either forcing everyone to either take the drug or not take the drug\nNotice that  ¬†therefore our calculated effect will be confounded.\n\nBackdoor criterion says the manipulated DAG (right) will correspond to the observational DAG (left) if we condition on Sex.\n\n\nUse intervention formula from Intervention section\n\n\nAverage Causal Effect = 0.832 - 0.782 = 0.050. So the drug has a positive effect on average.\n\n\nBlood Pressure as the adjustment variable \n\nBlood Pressure instead of sex is used as the adjustment. Blood Pressure is a post-treatment variable.\nRelatively same observations as before. High or Low Blood Pressure with the drug produces better results than those that chose not to take the drug. Yet overall, those that chose the drug recovered at a lower percentage.\n\nSince Blood Pressure (B) is post-treatment, it has no effect on whether the patient takes the drug or not (D).\nTaking or not taking the drug (D) has an indirect effect on recovery (R) through Blood Pressure (B) along with a direct effect.  ¬†so our calculated effect will be unconfounded.\n\nSo with BP as the adjustment variable, the drug now has a small, negative effect (harmful), 0.78 - 0.83 = -0.05\n\nThe unconfounded, average causal effect for the population is negative, therefore the doctor should NOT prescribe the drug.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-scms",
    "href": "qmd/causal-inference.html#sec-causinf-scms",
    "title": "Causal Inference",
    "section": "Structural Causal Models (SCMs)",
    "text": "Structural Causal Models (SCMs)\n\nYou add additional assumptions to your DAG to derive a causal estimator\n‚ÄúFull Luxury‚Äù Bayesian approach\n\n‚ÄúFull Luxury‚Äù is just a term coined by McElreath; it‚Äôs just a bayesian model but bayesian models can fully model a DAG where standard regression approachs can fail (see examples)\nNo other approach will find something that the bayesian approach doesn‚Äôt\n\nMain disadvantage is that it can be computationally intensive (same with all baysian models)\n\nProvides ways to add ‚Äúcauses‚Äù for missingness and measurement error\n\nExample (2 Moms)\n\nNotes from McElreath video\nHypothesis: a mother‚Äôs family size is causal to her daughter‚Äôs family size\n\nTruth: no relationship\n\nVariables:\n\nM - Mother‚Äôs family size (i.e.¬†number of children the birth)\nD - Daughter‚Äôs family size\nB1 - Mother‚Äôs birth order; binary, first born or not\nB2 - Daughters‚Äô birth order; binary, first born or not\nU - unobserved confounds¬† (shown as curved dotted line)\n\n\n\nUnobserved confounds (economic status, education, cultural background, etc.) are causal to both Mother and Daughter (curved dotted line) which makes regression, D ~ M, impossible\n\nSee Baysian Two Moms example below for results of a typical regression\nStill possible to calculate the effect of M on D with SCMs\n\n\nAssumptions: Relationships are linear (i.e.¬†linear system)\nCausal Effects\n\n\nWe want m which is the causal effect of M on D\nAssumes causal effect of birth order is the same on mother and daughter\nAside: There is no arrow/coefficient from M to B2 because it‚Äôs not germane to the calculation of m\n\nCalculate linear effect (i.e.¬†regression coefficient) without a regression model using a linear system of equations\n\nNote: a regression coefficent, Œ≤ = cov(X,Y) / var(X)\nWe can‚Äôt calculate the covariance of M and D directly because it depends on unobserved confounders but we can calculate the covariance between B1 and D and use that to get m.\nThe covariance for each path is the product of the path coefficients and the variance of the originating causal variable.\nPath B1 ‚Üí M: cov(B1, M) = b*var(B1)\nPath B1 ‚Üí D: cov(B1, D) = b*m*var(B1)\n2 equations and 2 unknowns, m and b\nSolve for b in the first equation, substitute b into the second equation, and solve for m\n\nm = cov(B1, D) / cov(B1, M)\n\nStill need an uncertainty of this value (e.g.¬†bootstrap)\n\n\nExample (Bayesian 2 Moms)\n\nSee previous example for link, hypothesis, and definition of the variables\n\nFunctions (right side)\n\nEach variable‚Äôs function‚Äôs inputs are variables that are causal influences (i.e.¬†have arrows pointing at the particular variable\n\ne.g.¬†M has two arrows pointing at it in the DAG: B1 and u\n\n\nCode\n\nThe assumption is that this is a lineary system, so M and D have Normal distributions for their functions with means as linear regression equations\nB1 and B2 are binary so they get bernoulli distributions\nU gets a standard normal prior\n\nAside: evidently this is a typical prior for latent variables in psychology\n\np, intercepts, sd, k get typical priors for bayesian regressions\n\nResults\n\n\nTruth: no effect\n1st 3 lm models shows how the unobserved confound biases the estimate when using a typical regression model to estimate the causal effect\n\nIncluding B2 adds precision to the biased estimate since it is causal to the outcome D while adding B1 increases the bias\n\nBayesian model isn‚Äôt fooled because U is specified as an input to the functions for M and D\n\nInterpretation: There is no reliable estimate of an effect. The most likely effect is a moderately positive one but it could also be negative.\nAdding more simulated data to this example will move the point estimate towards zero\n\n\n\nExample (Bayesian Peer Bias)\n\nAlso see Causal Design &gt;&gt; Bad Adjustment Variables &gt;&gt; Peer Bias\nHypothesis: racial discrimination in acceptance of applicatioon to Berkeley grad schools\n\nTruth: moderate negative effect, -0.8\n\nVariables:\n\nX is race, E is department, Q is an unobserved confound (latent variable: student quality), Y is binary; Admission/No Admission\nR1 and R2 are proxy variables for Q (e.g.¬†test scores, lab work, extracurriculars, etc.)\n\nAssumptions: System is linear\nDAG and Code\n\n\nXX is the race variable with X as the coefficient in the code\n\nThis code uses his {rethinking} package so some of this syntax is unfamiliar\n\nR1 and R2 are shown in the DAG to be influenced by student quality, Q\nEvery prior is normal except for Q‚Äôs coefficient\n\nResults\n\n\nTruth: -0.8\n1st 3 glm models shows how the unobserved confound, Q, biases the estimate when using a typical logistic regression model to estimate the causal effect\nBayesian model isn‚Äôt fooled because Q is specified as an input to the function for Y\n\nInterpretation: There is a reliably negative effect (no 0 in the CI). The most likely effect is a moderately negative one.\nNot quite equal to the truth but reliably negative and the point estimate is closer than the glms\n\n\n\nExample\n\nAssumptions: Relationships between variables are linear and error terms are independent\nEquations\n\n,¬† \n\n\nDAG 1 (left) shows the association DAG which represents the SCM\nmanipulated DAG 1 (middle) shows intervention where z is set to a constant\n\nincoming causal arrows get cutoff the intervening variable\n\nmanipulated DAG 1 (right) shows intervention where x is set to a constant\n\nSimulation of the SCM (n = 1000) (code in article)\n\n\nZ is more predictive of Y than X\n\nSimulate interventions (code in article)\n\n\nLeft - histogram of SCM for Y without an intervention\nMiddle - Intervention on Z\n\nconfirms the DAG which shows no effect on Y and Z is not causal\n\nRight - intervention on X\n\nconfirms the DAG which shows an intervention on X produces an effect on Y and X is causal\n\nAverage Causal Effect (ACE) can be determined by subtracting the expected values of interventions where¬† X = x +1 and¬† X = x",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-ctrfact",
    "href": "qmd/causal-inference.html#sec-causinf-ctrfact",
    "title": "Causal Inference",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nExample(code in article): Test whether Grandma‚Äôs home remedy can speed recovery time for the common cold\n\nSCM\n\n\nT is 1/0, i.e.¬†whether patient receives Grandma‚Äôs treatment, with p = 0.5; \nR is recovery time\nŒº is the intercept\nŒ≤ is the average causal effect, since\n\n\nwhere¬†\n\n\nFrom fitting the model, we find Œº = 7, Œ≤ = -2, Œ§ = 0, Œµ1 = 0.78\n\nTherefore, the Individual Causal Effect for patient 1\n\n\nJust plug and chug where we substitute T = 1 into the SCM and we already have the T = 0 part from the model\n\n\nIn this case, the SCM is linear, so the ICE = ACE.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-medanal",
    "href": "qmd/causal-inference.html#sec-causinf-medanal",
    "title": "Causal Inference",
    "section": "Mediation Analysis",
    "text": "Mediation Analysis\n\n\nFigure\n\nc‚Äô is the direct effect of X on the outcome after the indirect path has been removed (i.e.¬†conditioned upon, outcome ~ X + mediator)\nc is the to total effect (outcome ~ X)\nc - c‚Äô equals the indirect effect\nSee definitions below\n\nAllows you to test for the influence of a third variable, the mediator, on the relationship between (i.e.¬†effect of) the treatment variable, X, and the Outcome variable, Y.\nMisc\n\nNotes from: Mediation Models\n\nOverview of packages (Aug 2020)\n\n{brms} very flexible in terms of models. You‚Äôll just have to calculate the effects by hand unless some outside package (e.g.¬†sjstats) takes a brms model and does it for you.\n\nSee below for formulas. {mediation} papers should have other formulas for other types of models (e.g.¬†poisson, binomial)\n\n{mediation} handles a lot for you. Method isn‚Äôt bayesian but is very similar to it in a frequentist-bootstrappy-simulation way.\n\nPackage has been substantially updated since that article was written.\n\n\nAlso see\n\nOther Articles &gt;&gt; Frontdoor Adjustment\nStatistical Rethinking &gt;&gt; Chapter 11 &gt;&gt; Conclusion of Berkeley Admissions example\n\nalso Lecture 9 2022 video\n\nebook (w/brms) Introduction to Mediation, Moderation, and Conditional Process Analysis\n\nPackages\n\n{lcmmtp} - Efficient and flexible causal mediation with longitudinal mediators, treatments, and confounders using modified treatment policies\n\nPaper: Identification and estimation of mediational effects of longitudinal modified treatment policies\n\n\nIncluding a mediator and the independent variable in a regression will result in the independent variable not being signficant and the mediator being significant.\n\nExample: Causal effect of education on income\n\nSay occupation is your mediator. Education has a big impact on your occupation, which in turn has a big impact on your income. You don‚Äôt want to control for a mediator if you are interested in the full effect of X on Y! Because a huge part of how X impacts on Y is precisely through the mediation of C, in our case choice of and access to occupation, given a certain level of education. If you ‚Äòcontrol‚Äô for occupation you will be greatly underestimating the importance of education.\n\n\nWhen would you want to only measure the Direct Effect?\n\nExample: Determining the amount of remuneration for discrimination\n\nFrom Simulating confounders, colliders and mediators\nVariables\n\nOutcome: Pay Gap\nTreatment: Gender\n\nIn this case, this variable is actually ‚Äúgender discrimination in the current workplace in making a pay decision‚Äù (for which we use actual, observed Gender as a proxy)\n\nMediators: Occupation and Experience\n\nWhen determining whether a type of descrimination exists, you don‚Äôt want to condtion on the mediators, because the effect of gender will be underestimated. So, you‚Äôd want the total effect. But here, discrimation is already determined and Gender is now a proxy variable. Under Gender‚Äôs new definition, Occupation and Experience might influence the amount of ‚Äúgender discrimiation,‚Äù so they can‚Äôt be definitively labelled mediators any more.\nSo if you want to estimate that final ‚Äúequal pay for equal work‚Äù step of the chain then yes it is legitimate to control for occupation and experience.\n\n\nShould always compare a mediation model to a model without mediation\n\nAn unnecessary mediation model will almost certainly be weaker and probably more confusing than the model you would otherwise have.\n\nAverage Causal Mediation Effect (ACME) (aka Indirect Effect)- the expected difference in the potential outcome when the mediator took the value that it would have under the treatment condition as opposed to the control condition, while the treatment status itself is held constant.\n\nIf this isn‚Äôt significant, there isn‚Äôt a mediation effect\nIt is possible that the ACME takes different values depending on the baseline treatment status. Shown by analyzing the interaction between the treatment variable and the mediator\nŒ¥(t) = E[Y (t, M(t1)) ‚àí Y (t, M(t0))]\n\nwhere\n\nt, t1, t0 are particular values of the treatment T such that t1 ‚â† t0,\nM(t) is the potential mediator\nY (t, m) is the potential outcome variable\n\n\n\nAverage Direct Effect (ADE) - the expected difference in the potential outcome when the treatment is changed but the mediator is held constant at the value that it would have if the treatment equals t.\n\nŒ∂(t) = E[Y (t1, M(t)) ‚àí Y (t0, M(t))]\n\nThe Total Effect of the treatment on the outcome is ACME + ADE.\n\nConditions where you likely do NOT need mediation analysis :\n\nIf you cannot think of your model in temporal or physical terms, such that X necessarily leads to the mediator, which then necessarily leads to the outcome.\nIf you could see the arrows going either direction.\nIf when describing your model, everyone thinks you‚Äôre talking about an interaction (a.k.a. moderation).\nIf there is NO strong correlation between key variables (variables of interest) and mediator, and if there is NO strong correlation between mediator and the outcome.\n\nSobel test - tests whether the suspected mediator‚Äôs influence on the independent variable is significant.\n\nPerforming the test in R via bda::mediation.test - article\n\nMethods\n\nBaron & Kenny‚Äôs (1986) 4-step indirect effect method has low power\nProduct-of-Paths (or difference in coefficients)\n\nc - c‚Äô = a*b (see figure at start of this section) where c - c‚Äô is the indirect effect (aka ACME)\n\nif either a or b are nearly zero, then the indirect effect can only be nearly zero\nFormula only appropriate for the analysis of causal mediation effects when both the mediator and outcome models are linear regressions where treatment (IV) and moderator enter the models additively (e.g.¬†without interaction)\n\nEffect formulas for models with an interaction between treatment and moderator (Paper)\n\nmediator: M = Œ±2 + Œ≤2Ti + ŒæT2Xi + Œµi2(T~i`)\noutcome: Y = Œ±~3 + Œ≤3Ti + Œ≥Mi + Œ∫TiMi + ŒæT3Xi + Œµi3(Ti, Mi)\nACME = Œ≤2(Œ≥ + Œ∫t) where t = 0,1\nADE = Œ≤3 + Œ∫{Œ±2 + Œ≤2t + ŒæT2Œï(Xi)}\nATE = Œ≤2Œ≥ + Œ≤3 +Œ∫{Œ±2 + Œ≤2 + ŒæT2Œï(Xi)}\n\nAlternatively, fit Y = Œ±1 + Œ≤1Ti + ŒæT1Xi + Œ∑TTiXi + Œµi1\n\nThen ATE = Œ≤1 + Œ∑TE(Xi)\n\n\nNotes\n\nVariables\n\nT is treatment, M is mediator, X is a set of adjustment variables\n\nThe exponentiated T in ŒæT is to let you know it can be a set of coefficients for a set of adjustment variables (I guess)\n\n\nCouldn‚Äôt figure out why curly braces are being used\nACME with have two estimates (t=0, t=1)\nATE (average total effect)\nŒï(Xi) is the sample average of each adjustment variable and it‚Äôs multiplied by its associated Œæ2 coefficient\nSee paper for other types of models\n\n\n{lavaan}, {brms}\n\nTingley, Yamamoto, Hirose, Keele, & Imai, 2014\n\nQuasi-bayesian approach (paper ,esp Appendix D, for details)\n\nFits the mediation and outcome models (see 1st example)\nTakes the coefficients and vcov matrices from both models\n\nUses the coefs (means) and vcovs (variances) as inputs to a mvnorm function to simulate distributions for the coefficients.\nI do not understand what these are used for‚Ä¶ would have to look at the code.\n\nSamples predictions of each model K times for treatment = 1, then for treatment = 0\nCalcs difference between predictions for each set of samples, then averages to get the ACME\n\nAssumes Sequential Ignorability\n\nRequires treatment randomization or an equivalent assignment mechanism\nmediator is also ignorable given the observed treatment and pre-treatment confounders. This additional assumption is quite strong because it excludes the existence of (measured or unmeasured) post-treatment confounders as well as that of unmeasured pretreatment confounders. This assumption, therefore, rules out the possibility of multiple mediators that are causally related to each other (see Section 6 for the method that is designed to deal with such a scenario).\nCan‚Äôt be tested but a sensitivity analysis can be conducted using mediation::medsens (see vignette)\n\n{mediation} (vignette)\n\nMultiple types of models for both mediator and outcome\n\nincluding multilevel model functions from {lme4} supported\n\nMethods for:\n\n‚Äòmoderated‚Äô mediation\n\nthe magnitude of the ACME depends on (or is moderated by) a pre-treatment covariate. Such a pre-treatment covariate is called a moderator. (see Moderator Analysis)\nACME can depend on treatment status (i.e.¬†interaction between treatment and mediator), but this situation is talking about a separate variable moderating the effect of the treatment on the mediator.\n\nmultiple mediators (which violates sequential ingnorability but can be handled)\nvarious experimental designs (e.g.¬†parallel, crossover)\ntreatment non-compliance\n\nUses MASS (so may have conflicts with dplyr)\nNo latent variable capabilities\n\n\nEtsy article calculates generalized average causal mediation effect (GACME) and generalized average direct effect (GADE) and uses a known mediator to measure the direct causal effect even when the DAG has multiple unknown mediators (paper, video, R code linked in article)\n\nExample: Tingley, 2014 Method\n\nEquations\n\n\n\nPredictions for ‚Äújob_seek‚Äù in the mediator model (top) are used as predictor values in the outcome model (bottom).\n\nData: data(jobs, package = 'mediation')\n\ndepress2: outcome, numeric: Measure of depressive symptoms post-treatment. The outcome variable.\ntreat: treatment, binary: whether participant was randomly selected for the JOBS II training program.\n\n1 = assignment to participation.\n\njob_seek: mediator, ordinal: measures the level of job-search self-efficacy with values from 1 to 5.\necon_hard: adjustment, ordinal: Level of economic hardship pre-treatment with values from 1 to 5.\nsex: adjustment, binary: 1 = female\nage: adjustment, numeric: Age in years\n\n{mediation}\nmodel_mediator &lt;- lm(job_seek ~ treat + econ_hard + sex + age, data = jobs)\nmodel_outcome¬† &lt;- lm(depress2 ~ treat + econ_hard + sex + age + job_seek, data = jobs)\n\n# Estimation via quasi-Bayesian approximation¬†\nmediation_result &lt;- mediate(\n¬† model_mediator,¬†\n¬† model_outcome,¬†\n¬† sims = 500,\n¬† treat = \"treat\",\n¬† mediator = \"job_seek\"\n)\n\nSummary - summary(mediation_result)\n\n\nerror bar plot also available via plot(mediation_result)\nSays ACME isn‚Äôt significant, therefore no mediation effect detected.\n‚ÄúProp Mediated‚Äù is supposed to be the ratio of the indirect effect to the total.\n\nHowever this is not a proportion, and can even be negative, and so ‚Äúit is mostly a meaningless number.‚Äù\n\n\n\n\nExample: product-of-paths (or difference in coefficients)\n\n{lavaan}\nsem_model = '\n¬† job_seek ~ a*treat + econ_hard + sex + age\n¬† depress2 ~ c*treat + econ_hard + sex + age + b*job_seek\n¬† # direct effect\n¬† direct := c\n¬† # indirect effect\n¬† indirect := a*b\n¬† # total effect\n¬† total := c + (a*b)\n'\nmodel_sem = sem(sem_model, data=jobs, se='boot', bootstrap=500)\nsummary(model_sem, rsq=T)¬† # compare with ACME in mediation\nDefined Parameters:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate¬† Std.Err¬† z-value¬† P(&gt;|z|)\n¬† ¬† direct¬† ¬† ¬† ¬† ¬† -0.040¬† ¬† 0.045¬† -0.904¬† ¬† 0.366\n¬† ¬† indirect¬† ¬† ¬† ¬† -0.016¬† ¬† 0.012¬† -1.324¬† ¬† 0.185\n¬† ¬† total¬† ¬† ¬† ¬† ¬† ¬† -0.056¬† ¬† 0.046¬† -1.224¬† ¬† 0.221\n\nAlso outputs the typical summary regression estimates, std.errors, pvals, R2 etc.\nBootstraps std.errors\nSame results for ‚Äúindirect‚Äù here as with {mediation} ACME estimate\nR2s are poor for both regression models which could be why no mediation effect is detected.\n\n{brms}\nmodel_mediator &lt;- bf(job_seek ~ treat + econ_hard + sex + age)\nmodel_outcome¬† &lt;- bf(depress2 ~ treat + job_seek + econ_hard + sex + age)\nmed_result = brm(\n¬† model_mediator + model_outcome + set_rescor(FALSE),¬†\n¬† data = jobs\n)\nsummary(med_result) # regression results\n# using brms we can calculate the indirect effect as follows\nhypothesis(med_result, 'jobseek_treat*depress2_job_seek = 0')\n\nExact same brms syntax (except priors are specified) as in Statistical Rethinking &gt;&gt; Chapter 5 &gt;&gt; Counterfactual Plots\nExample has a mediator DAG as well.\nhypothesis tests H0: a*b == 0\n\npval &lt; 0.05 says there is a mediation effect.\n\n\n{sjstats}\n\nsjstats::mediation(med_result) %&gt;% kable_df()\n\nmediator (b): the effect of ‚Äújob_seek‚Äù on ‚Äúdepress2‚Äù\nindirect (c-c‚Äô): ACME\ndirect (c‚Äô): ADE\nproportion mediated: See {mediation} example",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-modanal",
    "href": "qmd/causal-inference.html#sec-causinf-modanal",
    "title": "Causal Inference",
    "section": "Moderation Analysis",
    "text": "Moderation Analysis\n\n\nMisc\n\nAlso see Introduction to Mediation, Moderation, and Conditional Process Analysis\n\nLike mediation analysis, it allows you to test for the influence of a third variable, Z (moderator), on the relationship between variables X and Y, but rather than testing a causal link between these other variables, moderation tests for when or under what conditions an effect occurs.\n\nModerators are conceptually different from mediators (‚Äúwhen‚Äù (moderator) vs ‚Äúhow/why‚Äù (mediator)).\n\nThere can be moderated mediation effect though. (see Mediation Analysis &gt;&gt; Methods &gt;&gt; {mediation})\n\nModerators can stengthen, weaken, or reverse the nature of a relationship.\nSome variables may be a moderator or a mediator depending on your question.\n\nAssumption: assumes that there is little to no measurement error in the moderator variable and that the DV did not CAUSE the moderator.\n\nIf moderator error is likely to be high, researchers should collect multiple indicators of the construct and use SEM to estimate latent variables.\nThe safest ways to make sure your moderator is not caused by your DV are to experimentally manipulate the variable or collect the measurement of your moderator before you introduce your IV.\n\nModeration can be tested by interacting variables of interest (moderator x IV) and plotting the simple slopes of the interaction, if present.\n\nSee Regression, Interactions for simple slopes/effects analysis\nMean center both your moderator and your IV to reduce multicolinearity and make interpretation easier. (‚Äúc‚Äù in variable names indicates variable was centered)\n\nExample: academic self-efficacy (moderator)(confidence in own‚Äôs ability to do well in school) moderates the relationship between task importance (independent variable (IV)) and the amount of test anxiety (outcome) a student feels (Nie, Lau, & Liau, 2011).\n\nStudents with high self-efficacy experience less anxiety on important tests (task importance) than students with low self-efficacy while all students feel relatively low anxiety for less important tests.\nSelf-efficacy (Z) is considered a moderator in this case because it interacts with task importance (X), creating a different effect on test anxiety (Y) at different levels of task importance.\n\nExample: What is the relationship between the number of hours of sleep (X, independent variable (IV)) a graduate student receives and the attention that they pay to this tutorial (Y, outcome) and is this relationship influenced by their consumption of coffee (Z, moderator)\nmod &lt;- lm(Y ~ Xc + Zc + Xc*Zc)\nsummary(mod)\n## Coefficients:\n##¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value Pr(&gt;|t|)¬† ¬†\n## (Intercept) 48.54443¬† ¬† 1.17286¬† 41.390¬† &lt; 2e-16 ***\n## Xc¬† ¬† ¬† ¬† ¬† 5.20812¬† ¬† 0.34870¬† 14.936¬† &lt; 2e-16 ***\n## Zc¬† ¬† ¬† ¬† ¬† 1.10443¬† ¬† 0.15537¬† 7.108 2.08e-10 ***\n## Xc:Zc¬† ¬† ¬† ¬† 0.23384¬† ¬† 0.04134¬† 5.656 1.59e-07 ***\n\nSince we have significant interactions in this model, there is no need to interpret the separate main effects of either our IV or our moderator\nPlot the simple slopes (1 SD above and 1 SD below the mean) of the moderating effect\n\n\nFor details on this plot and analysis, see Regression, Interactions &gt;&gt; OLS &gt;&gt; numeric:numeric &gt;&gt; Calculate simple slopes for the IV at 3 representative values for the moderator variable\nInterpretation\n\nThose who drank less coffee (moderator, black line) paid more attention (outcome) with the more sleep (IV) that they got last night but paid less attention overall than average (the red line).\nThose who drank more coffee (moderator, green line) paid more attention (outcome) when they slept more (IV) as well and paid more attention than average.\nThe difference in the slopes for those who drank more or less coffee (moderator) shows that coffee consumption moderates the relationship between hours of sleep and attention paid",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-sr",
    "href": "qmd/causal-inference.html#sec-causinf-sr",
    "title": "Causal Inference",
    "section": "Statistical Rethinking",
    "text": "Statistical Rethinking\n\nMisc\n\nArrows indicate directions of influence\nArrows in DAGs ‚Äúcreate‚Äù correlations\n\ni.e.¬†if arrow, then correlation\nThe direction it points determines whether its association is causal or not.\n\nUnlike a statistical model, a DAG, if it is correct, will tell you the consequences of intervening to change a variable.\n** The data alone can never tell us when a DAG is right. But the data can tell us when a DAG is wrong. **\nMany dynamical systems cannot be usefully represented by DAGs, because they have complex behavior that is sensitive to initial conditions. But these models can still be analyzed and causal interventions designed from them.\nA DAG path means any series of variables you could walk through to get from one variable to another, ignoring the directions of the arrows.\nThe variable, U, in DAGs represents one or more unobserved variables\n\nUsually has circle around the U or is just represented by a dashed line\n\n‚ÄúConditioned upon,‚Äù ‚Äúadjusted for,‚Äù or ‚Äúcontrolled for‚Äù is all the same thing\n‚Äúa‚Äù or ‚ÄúŒ±‚Äù is used in bayesian formulas to represent the intercept\nNotation\n\nX is not independent of Y, i.e \nconditional independence: Y is not associated with some variable X, after conditioning on some other variable Z, i.e.¬†\n\nthey are statements of which variables should be associated with one another (or not) in the data.\nthey are statements of which variables become dis-associated when we condition on some other set of variables.\nThere is no other path of influence from X to Y except through Z\n\n\n(Total ) Causal Effect and Direct Causal Effect\n\n\nWeight (W) is the outcome, Height (H) and Sex (S) are explanatory\n(Total) Causal Effect is simply, W ~ S\nDirect Causal Effect shuts the backdoor paths, W ~ S + H\n\nSometimes we want the total causal effect and not the direct causal effect. (e.g.¬†if H is a post-treatment variable, see SR, Ch.6)\n\n\n\n\n\nTestable Implications\n\nDiffering associations between plausible DAGs that are testable through statistical models\nAny DAG may imply that some variables are independent of others under certain conditions.\nNO conditional independencies ‚Üí NO testable implications\nwww.dagitty.net - Enter DAG and it will give you the Adjustment Set and Testable Implications\nExample\n\nQuestion: What is the causal relationship between Divorce Rate (D), Marriage Rate (M), and Median Age at Marriage (A)\nData:\n\n2 regressions are fit\n\nD ~ Œ± + Œ≤M\n\nShows that M is positively correlated with D\n\nD ~ Œ± + Œ≤A\n\nShows that A is negatively correlated with D\n\n\n\nPlausible DAGs (note: marriage cannot influence your age‚Ä¶ technically)\n\n\n\nA directly influences D\nM directly influences D\nA directly influences M\nReasoning: First, Age can have a direct effect, perhaps because younger people change faster than older people and are therefore more likely to grow incompatible with a partner. Second, it can have an indirect effect by influencing the marriage rate. If people get married earlier, then the marriage rate may rise, because there are more young people. Consider for example if an evil dictator forced everyone to marry at age 65. Since a smaller fraction of the population lives to 65 than to 25, forcing delayed marriage will also reduce the marriage rate. If marriage rate itself has any direct effect on divorce, maybe by making marriage more or less normative, then some of that direct effect could be the indirect effect of age at marriage.\n\n\n\nSimilar to 1 except M does not directly influence D\nReasoning This DAG is plausible even though there‚Äôs a correlation between M and D (regression 1). It could be that M derives it‚Äôs correlation with D through it‚Äôs association with A.\n\nThe direction of influence doesn‚Äôt prevent a correlation between M and D\n\n\n\nTestable implications\n\nDAG 1\n\nThe DAG shows all three are associated to each other, i.e.¬†\nIt would be natural to think about measuring correlation and if a pair shows no correlation you could discard the DAG, but it is NOT a good test since there are many ways two variables can show correlation yet not be directly associated. (see reasoning under DAG 2 above and under DAG2 below)\nDAG1 has NO conditional independencies and therefore, NO testable implications\n\nDAG 2\n\nThis DAG also shows all three variables are associated with each other.\nD and M are associated with one another, because A influences them both. They share a cause, and this leads them to be correlated with one another through that cause. But suppose we condition on A. All of the information in M that is relevant to predicting D is in A. So once we‚Äôve conditioned on A, M tells us nothing more about D\nThe testable implication is that D is independent of M, conditional on A, i.e.¬†\n\n(Conditioning on A does not make D independent of M, because M really influences D all by itself in this model.)\n\ni.e A and M are marginally dependent\n\n\n\nOnly difference between both DAGs is the conditional independence in DAG2.\n\nTest\n\nRun a multiple regression D ~ Œ± + Œ≤MM + Œ≤AA\nIf the effect measured from regression 1 disappears in the multiple regression, then we can discard DAG 1. If the effect remains, then we discard DAG 2.\n\n\nDAGs that are consistent with the data associations (M & N are associated but the causal relationship isn‚Äôt known)\n\nwhere U is an unknown variable. Unobserved variables are circled.\n\nAll three DAGs have no conditional independencies and therefore not testable implications\n\nA set of DAGs, each with the same conditional independencies known as a Markov Equivalence\n\nData cannot eliminate any of these DAGS. Domain knowledge must be used to reduce the number of Markov Equivalent DAGs.\n\n\n\n\n‚ÄúShutting the backdoor‚Äù to potential confounding paths\n\nSection 6.4\nwww.dagitty.net - Enter DAG and it will give you the Adjustment Set and Testable Implications\nRecipe\n\nList all of the paths connecting X (the potential cause of interest) and Y (the outcome).\nClassify each path by whether it is open or closed. A path is open unless it contains a collider.\nClassify each path by whether it is a backdoor path. A backdoor path has an arrow entering X.\nIf there are any backdoor paths that are also open, decide which variable(s) to condition on to close it.\n\nIf you have a choice between two variables where conditioning on either will close a backdoor path and one of them is causal to the outcome variable, then condition on the variable that is causal to the outcome variable. It will add precision to the estimate of the treatment effect.\nAny frontdoor paths that lead to backdoor paths must also be closed (see Misc &gt;&gt; Table 2 fallacy)\n\n\nExamples:\n\n\n\nProblem: We want to measure the causal effect of X ‚Äì&gt; Y\nPotential confounding paths: XUAC, XUBC\n\nXUAC doesn‚Äôt have a collider so a variable needs conditioned on (aka adjusted for)\n\nU is unobserved, so either A or C. C directly influences Y, so it‚Äôs more efficient and will ‚Äúaid in precision.‚Äù\n\nXUBC has a collider, B. So, no need to condition on any variable\n\nSolution: Y ~ a + X + C\n\nlibrary(dagitty)\ndag_6.1 &lt;- dagitty( \"dag {¬†\n¬† ¬† U [unobserved]\n¬† ¬† X -&gt; Y\n¬† ¬† X &lt;- U &lt;- A -&gt; C -&gt; Y\n¬† ¬† U -&gt; B &lt;- C\n}\")\nadjustmentSets( dag_6.1 , exposure=\"X\" , outcome=\"Y\" )\n#&gt; { C }\n#&gt; { A }\n\n\nProblem: We want to measure the causal effect of the number of Waffle Houses, W, on Divorce, D.\nPotential confounding paths: WSM, WSA, WSMA (Also WSAM but McElreath on says there are 3. Maybe a combo of same letters is equivalent?)\n\nWSM doesn‚Äôt have a collider and therefore either S or M needs conditioned on\nWSA doesn‚Äôt have a collider and therefor either S or A needs conditioned on\nWSMA has a collider, M. So that path is blocked\nM is a choice for WSM but it‚Äôs a collider so it‚Äôs out. S is in both WSM and WSA, so conditioning on it kills two birds.\n\nSolution: D ~ a + W + S\n\nlibrary(dagitty)\ndag_6.2 &lt;- dagitty( \"dag {\n¬† ¬† A -&gt; D\n¬† ¬† A -&gt; M -&gt; D\n¬† ¬† A &lt;- S -&gt; M\n¬† ¬† S -&gt; W -&gt; D\n}\")\nadjustmentSets( dag_6.2 , exposure=\"W\" , outcome=\"D\" )\n#&gt; { A, M }\n#&gt; { S }\n\nEvidently conditioning on A and M is also a solution\n\nConditioning on M does close WSM but would then open WSMA. So, by then conditioning on A which is on a fork (or pipe depending on the path) it closes WSMA.\n\nIn his brms ebook, Kurz fits these regressions and a couple others for comparison. There wasn‚Äôt a consensus point estimate for W in the regressions that adjust for S and A + M.\n\nMcElreath mentions, ‚ÄúThis DAG is obviously not satisfactory‚Äìit assumes there are no unobserved confounds, which is very unlikely for this sort of data.‚Äù\nThe inconsistent point estimates are probably do to an omitted variable(s) that is confounding the regression.\n\nConditional independencies:\nimpliedConditionalIndependencies( dag_6.2 )\n#&gt; A _||_ W | S\n#&gt; D _||_ S | A, M, W\n#&gt; M _||_ W | S",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-othart",
    "href": "qmd/causal-inference.html#sec-causinf-othart",
    "title": "Causal Inference",
    "section": "Other Articles",
    "text": "Other Articles\n\nFrontdoor Adjustment\n\nFrom http://arelbundock.com/posts/frontdoor/\nUseful when an unobserved confounder creates a backdoor path that prevents direct causal estimation\nIn a causal chain with three nodes X‚ÜíZ‚ÜíY, we can estimate the effect of X on Y indirectly by combining two distinct quantities:\n\nThe estimate of the effect of X on Z, P(Z|do(X))\nThe estimate of the effect of Z on Y, P(Y|do(Z), X)\n\nAssumptions\n\nFull mediation: there is no direct path from X to Y, except through Z.\nUn-confoundedness 1: There is no open backdoor from X to Z.\nUn-confoundedness 2: All backdoors from Z to Y are blocked by X\n\nExample: 1\n\nOur goal is to estimate P(Y|do(X)). Unfortunately, this relationship between X and Y is confounded by the unobserved variable U, via this backdoor path: X‚ÜêU‚ÜíY. Therefore, we cannot estimate the causal quantity of interest directly.\n\n\ncause X, a mediator Z, an outcome Y, and an unobserved confounder U\n\nlibrary(data.table)\nset.seed(731460)¬†\nN = 1e5\nU = rbinom(N, 1, prob = .2)\nX = rbinom(N, 1, prob = .1 + U * .6)\nZ = rbinom(N, 1, prob = .3 + X * .5)\nY = rbinom(N, 1, prob = .1 + U * .3 + Z * .5)\ndat = data.table(X, Z, Y)\n\n# truth\ncoef(lm(Y ~ X + U))[\"X\"]\n## 0.2549541\nEstimate the effect of X on Z, P(Z|do(X))\nstep1 = lm(Z ~ X, dat)\nEstimate the effect of Z on Y, P(Y|do(Z), X)\nstep2 = lm(Y ~ Z + X, dat)\nCombine both estimates by multiplication\ncoef(step1)[\"X\"] * coef(step2)[\"Z\"]\n## 0.2496002\n\nExample 2\n\nSame as first example but using {dosearch} package\nlibrary('dosearch')\n   data1 &lt;- \"P(X, Y, Z)\"\nquery1 &lt;- \"P(Y | do(X))\"\ngraph1 &lt;- \"U -&gt; X\n¬† ¬† ¬† ¬† ¬† U -&gt; Y\n¬† ¬† ¬† ¬† ¬† X -&gt; Z\n¬† ¬† ¬† ¬† ¬† Z -&gt; Y \"\n   # compute\n   frontdoor &lt;- dosearch(data1, query1, graph1)\n   frontdoor\n\nOutput:\n\nEstimate the causal effect\ndat[, `P(X)`¬† ¬† := fifelse(X == 1, mean(X), 1 - mean(X)) ][\n¬† ¬† , `P(Z|X)`¬† := mean(Z), by = X¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ][\n¬† ¬† , `P(Y|Z,X)` := mean(Y), by = .(Z, X)¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ][\n¬† ¬† , `P(Z|X)`¬† := mean(Z), by = X¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ][\n¬† ¬† , Y := NULL¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ]\ndat = unique(dat)\ndat[, `P(Y|do(Z))` := sum(`P(Y|Z,X)` * `P(X)`), by = Z]\n`P(Y|do(X=0))` = with(dat[X == 0],¬†\n¬† `P(Z|X)`¬† ¬† ¬† ¬† ¬† [Z == 1] *¬†\n¬† `P(Y|do(Z))`¬† ¬† ¬† [Z == 1] +\n¬† (1 - `P(Z|X)`)¬† ¬† [Z == 0] *¬†\n¬† `P(Y|do(Z))`¬† ¬† ¬† [Z == 0]\n)\n`P(Y|do(X=1))` = with(dat[X == 1], {\n¬† `P(Z|X)`¬† ¬† ¬† ¬† ¬† [Z == 1] *¬†\n¬† `P(Y|do(Z))`¬† ¬† ¬† [Z == 1] +\n¬† (1 - `P(Z|X)`)¬† ¬† [Z == 0] *¬†\n¬† `P(Y|do(Z))`¬† ¬† ¬† [Z == 0]\n})\n`P(Y|do(X=1))` - `P(Y|do(X=0))`\n## 0.249766\nComparison\n\nTruth: 0.2549541\nlm: 0.2496002\ndosearch: 0.249766",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html",
    "href": "qmd/model-building-sklearn.html",
    "title": "sklearn",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-misc",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-misc",
    "title": "sklearn",
    "section": "",
    "text": "Extensions\n\n{{DeepChecks}} - For model diagnostics\n{{feature-engine}} - Multiple transformers to engineer and select features to use in machine learning models.\n{{permetrics}} (vignette) - 111 diagnostic metrics for regression, classification, and clustering.\n\nPreventing data leakage in sklearn\n\nUse fit_transform on the train data - this ensures that the transformer learns from the train set only and transforms it simultaneously. Then, call the transformmethod on the test set to transform it based on the information learned from the training data (i.e mean and variance of the training data).\n\nPrevents data leakage\nSomehow the transform parameters calculated on the training data have to saved, so they can be applied to the production data. This can be done with Pipelines (see Pipelines &gt;&gt; Misc) by serializing the Pipeline objects.\n\nu_transf = LabelEncoder()\nitem_transf = LabelEncoder()\n# encoding\ndf['user'] = u_transf.fit_transform(df['user'])\ndf['item'] = item_transf.fit_transform(df['item'])\n# decoding\ndf['item'] = item_transf.inverse_transform(df['item'])\ndf['user'] = u_transf.inverse_transform(df['user'])\n\nA more robust way is to use sklearn‚Äôs Pipelines (see Pipelines below)\n\n\nTuning\n\nPick a metric for GridSearchCV and RandomizedSearchCV\n\nDefault is accuracy for classification which is rarely okay\nUsing metric from sklearn\ngs = GridSearchCV(estimator=svm.SVC(),¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† param_grid={'kernel':('linear', 'rbf'), 'C':[1, 10]},\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† scoring=‚Äòf1_micro‚Äô)\n\nList of metrics available\n\nUsing a custom metric\nfrom sklearn.metrics import fbeta_score, make_scorer\ncustom_metric = make_scorer(fbeta_score, beta=2)\ngs = GridSearchCV(estimator=svm.SVC(),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† param_grid={'kernel':('linear','rbf'), 'C':[1,10]},\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† scoring=custom_metric)\nAlso see Diagnostics, Classification &gt;&gt; Scores &gt;&gt; Lift Score\n\n\nScore model\n\nAlso see Pipelines &gt;&gt; Tuning a Pipeline\nBasic\nmodel = RandomForestClassifier(max_depth=2, random_state=0, warm_start=True, n_estimators=1)\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n\nClassification\nfrom sklearn.metrics import classification_report\nrep = classification_report(y_test, y_pred, output_dict = True)\n\nSee Diagnostics, Classification &gt;&gt; Multinomial for definitions of multinomial scores\n\nMultiple metrics function\nfrom sklearn.metrics import precision_recall_fscore_support\ndef score(y_true, y_pred, index):\n¬† ¬† \"\"\"Calculate precision, recall, and f1 score\"\"\"\n\n¬† ¬† metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n¬† ¬† performance = {'precision': metrics[0], 'recall': metrics[1], 'f1': metrics[2]}\n¬† ¬† return pd.DataFrame(performance, index=[index])\npredict_proba outputs probabilities for classification models",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-opt",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-opt",
    "title": "sklearn",
    "section": "Optimization",
    "text": "Optimization\n\nMisc\n\nFor fast iteration and if you have access to GPUs, it‚Äôs better to use {{XGBoost, LightGBM or CatBoost}} since sklearn is only CPU capable.\n\nFastest to slowest: CaBoost, LightGBM, XGBoost\n\n\n{{sklearnex}}\n\nIntel¬Æ Extension for Scikit-learn that dynamically patches scikit-learn estimators to use Intel(R) oneAPI Data Analytics Library as the underlying solver\n\nRequirements\n\nThe processor must have x86 architecture.\nThe processor must support at least one of SSE2, AVX, AVX2, AVX512 instruction sets.\nARM* architecture is not supported.\nIntel¬Æ processors provide better performance than other CPUs.\n\n\nMisc\n\nDocs\nBenchmarks\nAlgorithms\nCurrently extension does not support multi-output and sparse data for the Random Forest and K-Nearest Neighbors\n\nNon-interactively\npython -m sklearnex my_application.py\nInteractively for all algorithms\nfrom sklearnex import patch_sklearn\npatch_sklearn()\n# then import sklearn estimators\nInteractively for specific algorithms\nfrom sklearnex import patch_sklearn\npatch_sklearn([\n¬† ¬† 'RandomForestRegressor,\n¬† ¬† 'SVC',\n¬† ¬† 'DBSCAN'\n])\nUnpatch unpatch_sklearn()\nNeed to reimport estimators after executing\n\nSpeeding up retraining a model (article)\n\nPotentially useful for slow training models that need to be retrained often.\nNot all models have these methods available\nwarm_start = True permits the use of the existing fitted model attributes to initialize a new model in a subsequent call to fit\n\nDoesn‚Äôt learn new parameters so shouldn‚Äôt be used to fix concept drift\n\ni.e.¬†The new data should have the same distribution as the original data which maintains the same relationship with the output variable\n\nExample\n# original fit\nmodel = RandomForestClassifier(max_depth=2, random_state=0, warm_start=True, n_estimators=1)\nmodel.fit(X_train, y_train)\n\n# fit with new data\nmodel.n_estimators+=1\nmodel.fit(X2, y2)\n\n\nPartial Fit\n\nDoes learn new model parameters\nExample\n# original fit\nmodel = SGDClassifier()¬†\nmodel.partial_fit(X_train, y_train, classes=np.unique(y))\n\n# fit with new data\nmodel.partial_fit(X2, y2)",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-preproc",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-preproc",
    "title": "sklearn",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nMisc\n\nResources\n\nSciKit-Learn Transformation Docs\n\ntrain_test_split doesn‚Äôt choose random rows to be in each partition by default. For example, a 90-10 split has the first 90% of the rows in Train which leaves the last 10% of the rows to be in Test\n\n‚Äúshuffle=True‚Äù will randomly shuffle the rows before it partitions which would be equivalent to randomly selecting rows for each partition\n\n\nStratified train/test splits\nnp.random.seed(2019)\n# Generate stratified split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random.state=42)\n# Train set class weights\n&gt;&gt;&gt; pd.Series(y_train).value_counts(normalize=True)\n1¬† ¬† 0.4\n0¬† ¬† 0.4\n2¬† ¬† 0.2\ndtype: float64\n# Test set class weights\n&gt;&gt;&gt; pd.Series(y_test).value_counts(normalize=True)\n1¬† ¬† 0.4\n0¬† ¬† 0.4\n2¬† ¬† 0.2\n\nOr you can use StratifiedKFold which stratifies automatically.\n\nStratified Train/Validate/Test splits\nX_train, X_, y_train, y_ = train_test_split(\n¬† ¬† df['token'], tags, train_size=0.7, stratify=tags, random_state=RS\n)\n\nX_val, X_test, y_val, y_test = train_test_split(\n¬† ¬† X_, y_, train_size=0.5, stratify=y_, random_state=RS\n)\n\nprint(f'train: {len(X_train)} ({len(X_train)/len(df):.0%})\\n'\n¬† ¬† ¬† f'val: {len(X_val)} ({len(X_val)/len(df):.0%})\\n'\n¬† ¬† ¬† f'test: {len(X_test)} ({len(X_test)/len(df):.0%})')\nCheck proportions of stratification variable (e.g.¬†‚Äútags‚Äù) in splits\nsplit = pd.DataFrame({\n¬† ¬† 'y_train': Counter(', '.join(i for i in row) for row in mlb.inverse_transform(y_train)),\n¬† ¬† 'y_val': Counter(', '.join(i for i in row) for row in mlb.inverse_transform(y_val)),\n¬† ¬† 'y_test': Counter(', '.join(i for i in row) for row in mlb.inverse_transform(y_test))\n}).reindex(tag_dis.index)\n\nsplit = split / split.sum(axis=0)\n\nsplit.plot(\n¬† ¬† kind='bar',¬†\n¬† ¬† figsize=(10,4),¬†\n¬† ¬† title='Tag Distribution per Split',¬†\n¬† ¬† ylabel='Proportion of observations'\n);\nBin numerics: KBinsDiscretizer(n_bins=4)\nImpute Nulls/Nas\n\nSimpleImputer (Docs)\ncol_transformer = ColumnTransformer(\n¬† ¬† # Transformer name, Transformer Object and columns\n¬† ¬† [(\"ImputPrice\", SimpleImputer(strategy=\"median\"), [\"price\"])],\n¬† ¬† # Any other columns are ignored\n¬† ¬† remainder= SimpleImputer(strategy=\"constant\", fill_value=-1)\n¬† )\n\nTakes ‚Äúprice‚Äù and replaces Nulls with median; all other columns get constant, -1, to replace their Nulls\n‚Äúmost frequent‚Äù also available\n\nIterativeImputer (Docs) - Multivariate imputer that estimates values to impute for each feature with missing values from all the others.\nKNNImputer (Docs) - Multivariate imputer that estimates missing features using nearest samples.\n\nLog: FunctionTransformer(lambda value: np.log1p(value))\nStandardize\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nstandardized_data = scaler.fit_transform(df)\nstandardized_df = pd.DataFrame(standardized_data, columns=df.columns)\n\nCheck: standardized_df.apply([\"mean\", \"std\"]\n\nScaling\n\nMin/Max\nfrom sklearn.preprocessing import MinMaxScaler\n# Create a scaler object\n\nmm_scaler = MinMaxScaler()\n\n# Transform the feature values\nchurn[[\"Customer_Age\", \"Total_Trans_Amt\"]] = mm_scaler.fit_transform(churn[[\"Customer_Age\", \"Total_Trans_Amt\"]])\n\n# check the feature value range after transformation\nchurn[\"Customer_Age\"].apply([\"min\", \"max\"])\n\n‚Äúfeature_range=(0,1)‚Äù parameter allows you to change the range\n\ndefault range for the MinMaxScaler is [0,1]\n\n\n\nOrdinal\nencode_cat = ColumnTransformer(\n¬† [('cat', OrdinalEncoder(), make_column_selector(dtype_include='category'))],\n¬† remainder='passthrough'\n)\nCategorical DType (like R factor type)\n\nAssign predefined unordered values so that whenever some value for some column does not exist in the training set, receiving such a value in the test set would not lead to incorrectly assigned labels or any other logical error.\n\nWith multiple categorical columns, it can be difficult to stratify them in the training/test splits\nMust know all possible categories for each categorical variable\n\nimport pandas as pd\nfrom pandas import CategoricalDtype\n\ndef transform_data(df: pd.DataFrame, target: pd.Series, frac: float = 0.07,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† random_state: int = 42) -&gt; pd.DataFrame:\n\n¬† ¬† \"Transform non-numeric columns into categorical type and clean data.\"\n\n¬† ¬† categories_map = {\n¬† ¬† ¬† ¬† ¬† 'gender': {1: 'male', 2: 'female'},\n¬† ¬† ¬† ¬† ¬† 'education': {1: 'graduate', 2: 'university', 3: 'high_school',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 4: 'others', 5: 'others', 6: 'others', 0: 'others'},\n¬† ¬† ¬† ¬† ¬† 'marital_status': {1: 'married', 2: 'single', 3: 'others', 0: 'others'}\n¬† ¬† ¬† ¬† }\n\n¬† ¬† for col_id, col_map in categories_map.items():\n¬† ¬† ¬† ¬† ¬† ¬† df[col_id] = df[col_id].map(col_map).astype(\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† CategoricalDtype(categories=list(set(col_map.values())))\n¬† ¬† ¬† ¬† ¬† ¬† )\n\nOne-hot encoding\n\n** Don‚Äôt use pandas get_dummies because it doesn‚Äôt handle categories that aren‚Äôt in your test/production set **\n\nWonder if get_dummies creates 1 less dummy than the number of categories like it should\n\nBasic\n# Create a one-hot encoder\nonehot = OneHotEncoder()\n\n# Create an encoded feature\nencoded_features = onehot.fit_transform(churn[[\"Marital_Status\"]]).toarray()\n\n# Create DataFrame with the encoded features\nencoded_df = pd.DataFrame(encoded_features, columns=onehot.categories_)\n\nPipeline\nfrom sklearn.preprocessing import OneHotEncoder\n# one hot encode categorical features\nohe = OneHotEncoder(handle_unknown='ignore')\n\nfrom sklearn.pipeline import Pipeline\n# store one hot encoder in a pipeline\ncategorical_processing = Pipeline(steps=[('ohe', ohe)])\n\nfrom sklearn.compose import ColumnTransformer\n# create the ColumnTransormer object\npreprocessing = ColumnTransformer(transformers=[('categorical', categorical_processing, ['gender', 'job'])],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† remainder='passthrough')\nClass Imbalance\n\nSMOTE with {{imblearn}}\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE\nX_train, y_train = make_classification(n_samples=500, n_features=5, n_informative=3)\nX_res, y_res = SMOTE().fit_resample(X_train, y_train)",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-pip",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-pip",
    "title": "sklearn",
    "section": "Pipelines",
    "text": "Pipelines\n\nMisc\n\nHelps by creating more maintainable and clearly written code. Reduces mistakes by transforming train/test sets automatically.\nPipeline objects are estimators and can be serialized and saved like any other estimator\nimport joblib\n\n#saving the pipeline into a binary file\njoblib.dump(pipe, 'wine_pipeline.bin')\n\n#loading the saved pipeline from a binary file\npipe = joblib.load('wine_pipeline.bin')\n\n\n\nBasic Feature Transforming and Model Fitting Pipeline\n\nformat: Pipeline(steps = [(‚Äò&lt;step1_name&gt;‚Äô, function), (‚Äò&lt;step2_name&gt;‚Äô, function)])\nExample\nnp.random.seed(2019)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# creating the pipeline with its different steps\n# fitting the pipeline with data\n# making predictions\npipe = Pipeline([\n¬† ¬† ('feature_selection', VarianceThreshold(threshold=0.1)),\n¬† ¬† ('scaler', StandardScaler()),\n¬† ¬† ('model', KNeighborsClassifier())\n])\npipe.fit(X_train, y_train)\npredictions = pipe.predict(X_test)\n\n#checking the accuracy\naccuracy_score(y_test, predictions) #sklearn function; multi-class: accuracy, binary: jaccard index (similarity)\n\nPipeline transforms according to the sequence of the steps inserted into the list of tuples\n\n\n\n\nColumn Transformers\n\nExample: transform by column type\n# creating pipeline for numerical features\nnumerical_pipe = Pipeline([\n¬† ¬† ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n¬† ¬† ('scaler', StandardScaler())\n])\n\n# creating pipeline for categorical features\ncategorical_pipe = Pipeline([\n¬† ¬† ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n¬† ¬† ('one_hot', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer([\n¬† ¬† ('numerical', numerical_pipe, make_column_selector(dtype_include=['int', 'float'])),\n¬† ¬† ('categorical', categorical_pipe, make_column_selector(dtype_include=['object', 'category'])),\n])\npipe = Pipeline([\n¬† ¬† ('column_transformer', preprocessor),\n¬† ¬† ('model', KNeighborsClassifier())\n])\n\npipe.fit(X_train, y_train)\npredictions = pipe.predict(X_test)\n\nColumnTransformertakes list of tuples: name, transformer, columns\n\nn_jobs, verbose args also available\nremainder=‚Äúpassthrough‚Äù says all other columns not listed are ignored (might be a default)\n\nCan also provide a transformer object\n\nmethods: fit_transform, get_feature_names_out, get_params, etc\n\nmake_column_selector allows your to select the type of column to apply the transformer (docs)\n\nExample: Apply sequence of transformations to a column\ncol_transformer = ColumnTransformer(\n¬† ¬† [\n¬† ¬† ¬† (\n¬† ¬† ¬† ¬† ¬† \"PriceTransformerPipeline\",\n¬† ¬† ¬† ¬† ¬† # Pipeline -&gt; multiple transformation steps\n¬† ¬† ¬† ¬† ¬† Pipeline([\n¬† ¬† ¬† ¬† ¬† ¬† (\"MeanImputer\"¬† ¬† ¬† , SimpleImputer(strategy=\"median\")),\n¬† ¬† ¬† ¬† ¬† ¬† (\"LogTransformation\", FunctionTransformer(lambda value: np.log1p(value)) ),\n¬† ¬† ¬† ¬† ¬† ¬† (\"StdScaler\",¬† ¬† ¬† ¬† StandardScaler() ),\n¬† ¬† ¬† ¬† ¬† ]),\n¬† ¬† ¬† ¬† ¬† [\"price\"]\n¬† ¬† ¬† ¬† ),\n¬† ¬† ],\n¬† ¬† remainder=SimpleImputer(strategy=\"constant\", fill_value=-1)\n¬† )\n\nFor the ‚Äúprice‚Äù columns, it replaces Nulls with median, then log transforms, then standardizes. All other columns get their Nulls replaces with -1.\n\n\n\n\nFunction Transformers\n\nMisc\n\nIf using a method in the sklearn.preprocessing module, then able to use fit, transform, and fit_transform methods (I think)\nData must be the first argument of the function\ninverse_func argument for FunctionTransformer allows you to include a back-transform function\n\nSteps\n\nCreate function that transforms data\nCreate FunctionTransformer object using function as the argument\nAdd function-tranformer to the pipeline by including it as an argument to make_pipeline\n\nExample: Make numerics 32-bit instead 64-bit to save memory\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\ndef reduce_memory(X: pd.DataFrame, y=None):\n¬† ¬† \"\"\"Simple function to reduce memory usage by casting numeric columns to float32.\"\"\"\n¬† ¬† num_cols = X.select_dtypes(incluce=np.number).columns\n¬† ¬† for col in num_cols:\n¬† ¬† ¬† ¬† X[col] = X.astype(\"float32\")\n¬† ¬† return X, y\n\nReduceMemoryTransformer = FunctionTransformer(func = reduce_memory)\n\n# Plug into a pipeline\n&gt;&gt;&gt; make_pipeline(SimpleImputer(), ReduceMemoryTransformer)\n\nData goes through the SimpleImputer first then the ReduceMemoryTransformer\n\n\n\n\nCustom Transformers Classes\n\nMisc\n\nFor more complex transforming tasks\n\nSteps\n\nCreate a class that inherits from BaseEstimator and TransformerMixin classes of sklearn.base\n\nInheriting from these classes allows Sklearn pipelines to recognize our classes as custom estimators and automatically adds fit_transform to your class\n\nAdd transforming methods to Class\n\nClass Skeleton\nclass CustomTransformer(BaseEstimator, TransformerMixin):\n¬† ¬† def __init__(self):\n¬† ¬† ¬† ¬† pass\n¬† ¬† def fit(self):\n¬† ¬† ¬† ¬† pass\n¬† ¬† def transform(self):\n¬† ¬† ¬† ¬† pass\n¬† ¬† def inverse_transform(self):\n¬† ¬† ¬† ¬† pass\nExample: Log transforming outcome variable\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import PowerTransformer\n\nclass CustomLogTransformer(BaseEstimator, TransformerMixin):\n\n¬† ¬† def __init__(self):\n¬† ¬† ¬† ¬† self._estimator = PowerTransformer()¬† # init a transformer\n\n¬† ¬† def fit(self, X, y=None):\n¬† ¬† ¬† ¬† X_copy = np.copy(X) + 1¬† # add one in case of zeroes\n¬† ¬† ¬† ¬† self._estimator.fit(X_copy)\n¬† ¬† ¬† ¬† return self\n\n¬† ¬† def transform(self, X):\n¬† ¬† ¬† ¬† X_copy = np.copy(X) + 1\n¬† ¬† ¬† ¬† return self._estimator.transform(X_copy)¬† # perform scaling\n\n¬† ¬† def inverse_transform(self, X):\n¬† ¬† ¬† ¬† X_reversed = self._estimator.inverse_transform(np.copy(X))\n¬† ¬† ¬† ¬† return X_reversed - 1¬† # return subtracting 1 after inverse transform\n\nreg_lgbm = lgbm.LGBMRegressor()\nfinal_estimator = TransformedTargetRegressor(\n¬† ¬† regressor=reg_lgbm, transformer=CustomLogTransformer()\n)\nfinal_estimator.fit(X_train, y_train)\n\nfit returns the tranformer itself since it returns self\n\nEstimates the optimal parameter lambda for each feature\n\ntransform returns transformed features\n\nApplies the power transform to each feature using the estimated lambdas in fit\n\nfit_transform does both at once\ncustom_log.fit(tps_df)¬†\ntransformed_tps = custom_log.transform(tps_df)\n# or\ntransformed_tps = custom_log.fit_transform(tps_df)\ninverse_transform returns the back-transformed features\nTransformedTargetRegressor transforms the targets y before fitting a regression model. The predictions are mapped back to the original space via an inverse transform. It takes as an argument the regressor that will be used for prediction, and the transformer that will be applied to the target variable\n\nThe regressor parameter accepts both regressors or pipelines that end with regressors\nIf the transformer is a function, like np.log, you can pass it to func argument\n\n\nExample: Dummy transformer + FeatureUnion (article)\n\nThis classes (below) that inherit this class with get these methods along with fit_transform\nclass DummyTransformer(BaseEstimator, TransformerMixin):\n  \"\"\"\n  Dummy class that allows us to modify only the methods that interest us,\n  avoiding redudancy.\n  \"\"\"\n  def __init__(self):\n  ¬† ¬† return None\n\n  def fit(self, X=None, y=None):\n  ¬† ¬† return self\n\n  def transform(self, X=None):\n  ¬† ¬† return self\nTransformer classes\nclass Preprocessor(DummyTransformer):\n\"\"\"\n  Class used to preprocess text\n\"\"\"\ndef __init__(self, remove_stopwords: bool):\n¬† ¬† self.remove_stopwords = remove_stopwords\n¬† ¬† return None\n\ndef transform(self, X=None):\n¬† ¬† preprocessed = X.apply(lambda x: preprocess_text(x, self.remove_stopwords)).values\n¬† ¬† return preprocessed\n\nclass SentimentAnalysis(DummyTransformer):\n\"\"\"\n¬† ¬† Class used to generate sentiment\n¬† ¬† \"\"\"\n¬† ¬† def transform(self, X=None):\n     ¬† ¬† sentiment = X.apply(lambda x: get_sentiment(x)).values\n     ¬† ¬† return sentiment.reshape(-1, 1) # &lt;-- note the reshape to transfor\n\nEach class inherits the dummy transformer and its methods\npreprocess_text and get_sentiment are user functions that are defined earlier in the article\n\nCreate pipeline\nvectorization_pipeline = Pipeline(steps=[\n  ('preprocess', Preprocessor(remove_stopwords=True)), # the first step is to preprocess the text\n  ('tfidf_vectorization', TfidfVectorizer()), # the second step applies vectorization on the preprocessed text\n  ('arr', FromSparseToArray()), # the third step converts a sparse matrix into a numpy array in order to show it in a            dataframe\n])\n\nTfidVectorizer and FromSparseArray are other classes in the article that I didn‚Äôt include in the Transformer classes chunk to save space\n\nCombine transformed features\n# vectorization_pipeline is a pipeline within a pipeline\nfeatures = [\n  ('vectorization', vectorization_pipeline),\n  ('sentiment', SentimentAnalysis()),\n  ('n_chars', NChars()),\n  ('n_sentences', NSententences())\n]\ncombined = FeatureUnion(features) # this is where we merge the features together\n\n# Get col names: subsets the second step of the second object in the vectorization_pipeline to retrieve¬†\n# the terms generated by the tf-idf then adds the other three column names to it\ncols = vectorization_pipeline.steps[1][1].get_feature_names() + [\"sentiment\", \"n_chars\", \"n_sentences\"]\nfeatures_df = pd.DataFrame(combined.transform(df['corpus']), columns=cols)\n\nPipelines are combined with FeatureUnion, features are transformed, and coerced into a pandas df which can be used to train a model\nNChars and NSentences are other classes in the article that I didn‚Äôt include in the Transformer classes chunk to save space\n\n\n\n\n\nTuning a Pipeline\n\nExample\nparameters = {\n¬† ¬† 'column_transformer__numerical__imputer__strategy': ['mean', 'median'],\n¬† ¬† 'column_transformer__numerical__scaler': [StandardScaler(), MinMaxScaler()],\n¬† ¬† 'model__n_neighbors': [3, 6, 10, 15],\n¬† ¬† 'model__weights': ['uniform', 'distance'],\n¬† ¬† 'model__leaf_size': [30, 40]\n}\n\n# defining a scorer and a GridSearchCV instance\nmy_scorer = make_scorer(accuracy_score, greater_is_better=True)\nsearch = GridSearchCV(pipe, parameters, cv=3, scoring=my_scorer, n_jobs=-1, verbose=1)\n\n# search for the best hyperparameter combination within our defined hyperparameter space\nsearch.fit(X_train, y_train)\n\n# changing pipeline parameters to gridsearch results\npipe.set_params(**search.best_params_)\n\n# making predictions\npredictions = pipe.predict(X_test)\n\n# checking accuracy\naccuracy_score(y_test, predictions)\n\nSee Column Transformer section example for details on this pipeline\nNote the double underscore used in the keys of the parameter dict\n\nDouble underscores separate names of steps inside a nested pipeline with last name being the argument of the transformer function being tuned\nExample: ‚Äòcolumn_transformer__numerical__imputer__strategy‚Äô\n\ncolumn_transformer (pipeline step name) &gt;&gt; numerical (pipeline step name) &gt;&gt; imputer (pipeline step name) &gt;&gt; strategy (arg of SimpleImputer function)\n\n\nView tuning results\nbest_params = search.best_params_\nprint(best_params)\n\n# Stores the optimum model in best_pipe\nbest_pipe = search.best_estimator_\nprint(best_pipe)\n\nresult_df = DataFrame.from_dict(search.cv_results_, orient='columns')\nprint(result_df.columns)\n\n\n\n\nDisplay Pipelines in Jupyter\n\nfrom sklearn import set_config¬†\nset_config(display=\"diagram\")\ngiant_pipeline",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-algs",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-algs",
    "title": "sklearn",
    "section": "Algorithms",
    "text": "Algorithms\n\nMisc\nHistogram-based Gradient Boosting Regression Tree\n\nHistogram-based models are more efficient since they bin the continuous features\nInspired by LightGBM. Much faster than GradientBoostingRegressor for big datasets (n_samples &gt;= 10 000).\n\nsklearn.ensemble.HistGradientBoostingRegressor\n\nuser guide\n\nStochastic Gradient Descent (SGD)\n\nalgorithm\nNot a class of models, just merely an optimization technique\n\nSGDClassifier(loss='log') results in logistic regression, i.e.¬†a model equivalent to LogisticRegression which is fitted via SGD instead of being fitted by one of the other solvers in LogisticRegression.\nSGDRegressor(loss='squared_error', penalty='l2') and Ridge solve the same optimization problem, via different means.\n\nPenalyzed regression hyperparameters are labelled different than in R\n\nlambda (R) is alpha (py)\nalpha (R) is 1 - L1_ratio (py)\n\nCan be successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing Advantages:\n\nEfficiency.\nEase of implementation (lots of opportunities for code tuning). Disadvantages:\nSGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\nSGD is sensitive to feature scaling.\n\nProcessing\n\nMake sure you permute (shuffle) your training data before fitting the model or use shuffle=True to shuffle after each iteration (used by default).\nFeatures should be standardized using e.g.¬†make_pipeline(StandardScaler(), SGDClassifier())\n\n\nBisectingKMeans\n\nCentroid is picked progressively (instead of simultaneously) based on the previous cluster. We would split the cluster each time until the number of K is achieved\nAdvantages\n\nIt would be more efficient with a large number of clusters\nCheaper computational costs\nIt does not produce empty clusters\nThe clustering result was well ordered and would create a visible hierarchy.\n\n\nXGBoost with GPU\ngbm = xgb.XGBClassifier(\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† n_estimators=100000,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† max_depth=6,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† objective=\"binary:logistic\",\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† learning_rate=.1,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† subsample=1,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† scale_pos_weight=99,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† min_child_weight=1,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† colsample_bytree=1,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† tree_method='gpu_hist',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† use_label_encoder=False\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† )\neval_set=[(X_train,y_train),(X_val,y_val)]¬†\nfit_model = gbm.fit(¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† X_train, y_train,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† eval_set=eval_set,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† eval_metric='auc',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† early_stopping_rounds=20,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† verbose=True¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† )\n\ntree_method='gpu_hist' specifies the use of GPU\nPlot importance\nfig,ax2 = plt.subplots(figsize=(8,11))\nxgb.plot_importance(gbm, importance_type='gain', ax=ax2)\n\nGBM with Quantile Loss (PIs)\ngbm_lower = GradientBoostingRegressor(\n¬† ¬† loss=\"quantile\", alpha = alpha/2, random_state=0\n)\ngbm_upper = GradientBoostingRegressor(\n¬† ¬† loss=\"quantile\", alpha = 1-alpha/2, random_state=0\n)\ngbm_lower.fit(X_train, y_train)\ngbm_upper.fit(X_train, y_train)\ntest['gbm_lower'] = gbm_lower.predict(X_test)\ntest['gbm_upper'] = gbm_upper.predict(X_test)",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-cv",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-cv",
    "title": "sklearn",
    "section": "CV/Splits",
    "text": "CV/Splits\n\nK-Fold\n\nfrom sklearn.model_selection import KFold\ncv = KFold(n_splits=7, shuffle=True)\n\nShuffling: minimizes the risk of overfitting by breaking the original order of the samples\n\n\nStratifiedKFold\n\nfrom sklearn.model_selection import StratifiedKFold\ncv = StratifiedKFold(n_splits=7, shuffle=True, random_state=1121218)\n\nFor classification, class ratios are held to the same ratios in both the training and test sets.\n\nclass ratios are preserved across all folds and iterations.\n\n\nLeavePOut: from sklearn.model_selection import LeaveOneOut, LeavePOut\n\nData is so limited that you have to perform a CV where you set aside only a few rows of data in each iteration\nLeaveOneOut is P = 1 for LeavePOut\n\nShuffleSplit, StratifiedShuffleSplit\n\nfrom sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\ncv = ShuffleSplit(n_splits=7, train_size=0.75, test_size=0.25)\ncv = StratifiedShuffleSplit(n_splits=7, test_size=0.5)\n\nNot a CV, just repeats the train/test split process multiple times\nUsing different random seeds should resemble a robust CV process if done for enough iterations\n\nTimeSeriesSplit\n\nfrom sklearn.model_selection import TimeSeriesSplit\ncv = TimeSeriesSplit(n_splits=7)\n\nWith time series data, the ordering of samples matters.\n\nGroup Data\n\nData is not iid (e.g.¬†multi-level data)\nOptions\n\nGroupKFold\nStratifiedGroupKFold\nLeaveOneGroupOut\nLeavePGroupsOut\nGroupShuffleSplit\n\nWorks just like the non-group methods but with a group arg for the grouping variable",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/outliers.html",
    "href": "qmd/outliers.html",
    "title": "Outliers",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-eda",
    "href": "qmd/outliers.html#sec-outliers-eda",
    "title": "Outliers",
    "section": "",
    "text": "Also see Anomaly Detection for ML methods\nPackages\n\n{robustmatrix} (vignette) - Robust covariance estimation for matrix-valued data and data with Kronecker-covariance structure using the Matrix Minimum Covariance Determinant (MMCD) estimators and outlier explanation using and Shapley values.\n\nExamples of matrix data would be image resolution and repeated measueres (e.g different time points, different spatial locations, different experimental conditions, etc)\n\n\nResources\n\nNeed to examine this article more closely, Taking Outlier Treatment to the Next Level\n\nDiscusses detailed approach to diagnosing outliers , eda, diagnostics, robust regression, winsorizing, nonlinear approaches for nonrandom outliers.\n\nFor Time Series, see bkmks, pkgs in time series &gt;&gt; cleaning/processing &gt;&gt; outliers",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/outliers.html#eda",
    "href": "qmd/outliers.html#eda",
    "title": "Outliers",
    "section": "EDA",
    "text": "EDA\n\nIQR\n\nObservations above \\(q_{0.75} + (1.5 \\times \\text{IQR})\\) are considered outliers\nObservations below \\(q_{0.25} - (1.5 \\times \\text{IQR})\\) are considered outliers\nWhere \\(q_{0.25}\\) and \\(q_{0.75}\\) correspond to first and third quartile respectively, and IQR is the difference between the third and first quartile\n\nHampel Filter\n\nObservations above \\(\\text{median} + (3 \\times \\text{MAD})\\) are considered outliers\nObservations below \\(\\text{median} - (3 \\times \\text{MAD})\\) are considered outliers\nUse mad(vec, constant = 1)¬† for the MAD",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-tests",
    "href": "qmd/outliers.html#sec-outliers-tests",
    "title": "Outliers",
    "section": "Tests",
    "text": "Tests\n\n** All tests assume data is from a Normal distribution **\nSee the EDA section for ways to find potential outliers to test\nGrubbs‚Äôs Test\n\nTest either a maximum or minimum point\n\nIf you suspect multiple points, you have remove the max/min points above/below the suspect point. Then test the subsetted data. Repeat as necessary\n\nH0: There is no outlier in the data.\nHa: There is an outlier in the data.\nTest statistics\n\\[\nG = \\frac {\\bar{Y} - Y_{\\text{min}}}{s}G = \\frac {Y_{\\text{max}} - \\bar{Y}}{s}\n\\]\n\nStatistics for whether the minimum or maximum sample value is an outlier\n\nThe maximum value is outlier if\n\\[\nG &gt; \\frac {N-1}{\\sqrt{N}} \\sqrt{\\frac {t^2_{\\alpha/(2N),N-2}}{N-2+t^2_{\\alpha/(2N),N-2}}}\n\\]\n\n‚Äú&lt;‚Äù for minimum\nt is denotes the critical value of the t distribution with (N-2) degrees of freedom and a significance level of Œ±/(2N).\nFor testing either the maximum or minimum value, use a significance level of level of Œ±/N\n\nRequirements\n\nNormally distributed\nMore than 7 observations\n\noutliers::grubbs.test(x, type = 10, opposite = FALSE, two.sided = FALSE)\n\nx: a numeric vector of data values\ntype=10: check if the maximum value is an outlier, 11 = check if both the minimum and maximum values are outliers, 20 = check if one tail has two outliers.\nopposite:\n\nFALSE (default): check value at maximum distance from mean\nTRUE: check value at minimum distance from the mean\n\ntwo-sided: If this test is to be treated as two-sided, this logical value indicates that.\n\nsee bkmk for examples\n\nDixon‚Äôs Test\n\nTest either a maximum or minimum point\n\nIf you suspect multiple points, you have remove the max/min points above/below the suspect point. Then test the subsetted data. Repeat as necessary.\n\nMost useful for small sample size (usually n‚â§25)\nH0: There is no outlier in the data.\nHa: There is an outlier in the data.\noutliers::dixon.test\n\nWill only accept a vector between 3 and 30 observations\n‚Äúopposite=TRUE‚Äù to test the maximum value\n\n\nRosner‚Äôs Test (aka generalized (extreme Studentized deviate) ESD test) Tests multiple points\n\nAvoids the problem of masking, where an outlier that is close in value to another outlier can go undetected.\nMost appropriate when n‚â•20\nH0: There are no outliers in the data set\nHa: There are up to k outliers in the data set\nres &lt;- EnvStats::rosnerTest(x,k)\n\nx: numeric vector\nk: upper limit of suspected outliers\nalpha: 0.05 default\nThe results of the test, res , is a list that contains a number of objects\n\nres$all.stats shows all the calculated statistics used in the outlier determination and the results\n\n‚ÄúValue‚Äù shows the data point values being evaluated\n‚ÄúOutlier‚Äù is True/False on whether the point is determined to be an outlier by the test\nRs are the test statistics\nŒªs are the critical values",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-meth",
    "href": "qmd/outliers.html#sec-outliers-meth",
    "title": "Outliers",
    "section": "Methods",
    "text": "Methods\n\nRemoval\n\nAn option if there‚Äôs sound reasoning (e.g.¬†data entry error, etc.)\n\nWinsorization\n\nA typical strategy is to set all outliers (values beyond a certain threshold) to a specified percentile of the data\nExample: A 90% winsorization would see all data below the 5th percentile set to the 5th percentile, and data above the 95th percentile set to the 95th percentile.\nPackages\n\n({DescTools::Winsorize})\n({datawizard::winsorize})",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-mod",
    "href": "qmd/outliers.html#sec-outliers-mod",
    "title": "Outliers",
    "section": "Models",
    "text": "Models\n\nBayes has different distributions for increasing uncertainty\nIsolation Forests - See Anomaly Detection &gt;&gt; Isolation Forests\nSupport Vector Regression (SVR) - See Algorithms, ML &gt;&gt; Support Vector Machines &gt;&gt; Regression\nExtreme Value Theory approaches\n\nfat tail stuff (need to finish those videos)\n\nRobust Regression (see bkmks &gt;&gt; Regression &gt;&gt; Other &gt;&gt; Robust Regression)\n\n{robustbase}\nCRAN Task View\n\nHuber Regression\n\nSee Loss Functions &gt;&gt; Huber Loss\nSee bkmks, Regression &gt;&gt; Generalized &gt;&gt; Huber\n\nTheil-Sen estimator\n\n{mblm}",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html",
    "href": "qmd/mathematics-statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-",
    "title": "Statistics",
    "section": "",
    "text": "Age Adjustment of per 100K rate\n\n\nAllows communities with different age structures to be compared\nThe crude (unadjusted)1994 cancer mortality rate in New York State is 229.8 deaths per 100,000 men. The age-adjusted rate is 214.7 deaths per 100,000 men.\n\nNotice that 214.7 isn‚Äôt 229.8*1 of course but the sum of all the individual age-group weighted rates.\n\nProcess (Formulas in column headers)\n\nCalculate the disease‚Äôs rate per 100K for each age group\nMultiply the age-specific rates of disease by age-specific weights\n\nThe weights are the proportion of the US population within each age group. (e.g.¬†0-14 year olds are 28.4% of the 1970 US population)\n\nThe weighted rates are then summed over the age groups to give the (total) age-adjusted rate",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-terms",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-terms",
    "title": "Statistics",
    "section": "Terms",
    "text": "Terms\n\nCoefficient of Variation (CV) - aka Relative Standard Deviation (RSD) - aka Dispersion Parameter - Measure of the relative dispersion of data points in a data series around the mean. Usually expressed as a percentage.\n\\[\nCV = \\frac{\\sigma}{\\mu}\n\\]\n\nWhile most often used to analyze dispersion around the mean, a quartile, quintile, or decile CV can also be used to understand variation around the median or 10th percentile, for example.\nShould only be used with variables that have minimum at zero (e.g.¬†counts, prices) and not interval data (e.g celsius or fahrenheit)\nWhen the mean value is close to zero, the coefficient of variation will approach infinity and is therefore sensitive to small changes in the mean. This is often the case if the values do not originate from a ratio scale.\nA more robust possibility is the quartile coefficient of dispersion, half the interquartile range divided by the average of the quartiles (the midhinge)\n\\[\nCV_q = \\frac{0.5(Q_3 - Q_1)}{0.5(Q_3 + Q_1)}\n\\]\nFor small samples (Normal Distribution)\n\\[\nCV_*= (1 + \\frac{1}{4n}) CV\n\\]\nFor log-normal distribution\n\\[\nCV_{LN} = \\sqrt{e^{\\sigma^2_{ln}} - 1}\n\\]\n\nWhere \\(\\sigma_{ln}\\) is the standard deviation after a \\(\\ln\\) transformation of the data\n\n\nCovariance - Between two random variables is a measure of how correlated are their variations around their respective means\nGrand Mean - The mean of group means\n\nUse Cases\n\nHierarchical Group Comparison: Allows you to see how the average of each group relates to the overall average across all groups.\n\nExample: Imagine you collect data on the growth of various species of plants in different types of soil. Calculating the grand mean for plant height of each species allows you to see if any specific soil type consistently produces taller plants compared to the overall average.\n\nIdentifying Outliers: If the mean of a specific group deviates significantly from the grand mean, it might indicate the presence of outliers in that group.\nANOVA: It provides a baseline against which the individual group means are compared to assess if there are statistically significant differences between them.\n\nIt‚Äôs the intercept value when you use Sum-to-Zero (used in ANOVA) or Deviation contrasts (See Regression, Linear &gt;&gt; Contrasts &gt;&gt; Sum-to-Zero, Deviation)\n\nKernel Smoothing - Essence is the simple concept of a local average around a point, x; that is, a weighted average of some observable quantities, those of which closest to x being given the highest weights\nMargin of Error (MoE) - The range of values below and above the sample statistic in a confidence interval.\n\\[\n\\text{MOE}_\\gamma = z_\\gamma \\sqrt{\\frac{\\sigma^2}{n}}\n\\]\n\nZ-Score with confidence level Œ≥ ‚®Ø Standard Error\nIn general, for small sample sizes (under 30) or when you don‚Äôt know the population standard deviation, use a t-score to get the critical value. Otherwise, use a z-score.\n\nSee Null Hypothesis Significance Testing (NHST) &gt;&gt; Misc &gt;&gt; Z-Statistic Table for an example\n\nExample: a 95% confidence interval with a 4 percent margin of error means that your statistic will be within 4 percentage points of the real population value 95% of the time.\nExample: a Gallup poll in 2012 (incorrectly) stated that Romney would win the 2012 election with Romney at 49% and Obama at 48%. The stated confidence level was 95% with a margin of error of ¬± 2. We can conclude that the results were calculated to be accurate to within 2 percentages points 95% of the time.\n\nThe real results from the election were: Obama 51%, Romney 47%. So the Obama result was outside the range of the Gallup poll‚Äôs margin of error (2 percent).\n\n48 ‚Äì 2 = 46 percent\n48 + 2 = 50 percent\n\n\n\nNormalization - Rescales the values into a specified range, typically [0,1]. This might be useful in some cases where all parameters need to have the same positive scale. However, the outliers from the data set are lost.\n\\[\n\\tilde{X} = \\frac{X-X_{\\text{min}}}{X_{\\text{max}}-X_{\\text{min}}}\n\\]\n\nSome functions have the option to normalize with range [-1, 1]\nBest option if the distribution of your data is unknown.\n\nParameter - Describes an entire population (also see statistic)\nP-Value - \\(\\text{p-value}(y) = \\text{Pr}(T(y_{\\text{future}}) &gt;= T(y) | H)\\)\n\n\\(H\\) is a ‚Äúhypothesis,‚Äù a generative probability model\n\\(y\\) is the observed data\n\\(y_{\\text{future}}\\) are future data under the model\n\\(T\\) is a ‚Äútest statistic,‚Äù some pre-specified specified function of data\n\nSampling Error - The difference between population parameter and the statistic that is calculated from the sample (such as the difference between the population mean and sample mean).\nStandard Error of the Mean (SEM) - Measures how far the sample mean (average) of the data is likely to be from the true population mean (Also see Fundamentals &gt;&gt; Interpreting s.d., s.e.m, and CI Bars)\n\\[\n\\text{SEM} = \\frac{\\text{SD}}{\\sqrt{n}}\n\\]\n\nAssumes a simple random sample with replacement from an infinite population\n\nStandardization rescales data to fit the Standard Normal Distribution which has a mean (Œº) of 0 and standard deviation (œÉ) of 1 (unit variance).\n\\[\n\\tilde X = \\frac{X-\\mu}{\\sigma}\n\\]\n\nRecommended for PCA and if your data is known to come from a Gaussian distribution.\n\nStatistic - Describes a sample (also see parameter)\nVariance (\\(\\sigma^2\\))- Measures variation of a random variable around its mean.\nVariance-Covariance Matrix - Square matrix containing variances of the fitted model‚Äôs coefficient estimates and the pair-wise covariances between coefficient estimates.\n\\[\n\\begin{align}\n&\\text{Cov}(\\hat\\beta) = (X^TX)^{-1} \\cdot \\text{MSE}\\\\\n&\\text{where}\\;\\; \\text{MSE} = \\frac{\\text{SSE}}{\\text{DSE}} = \\frac{\\text{SSE}}{n-p}\n\\end{align}\n\\]\n\nDiagnonal is the variances, and the rest of the values are covariances\nThere‚Äôs also a variance/covariance matrix for error terms\nExample\nx &lt;- sin(1:100)\ny &lt;- 1 + x + rnorm(100)\nMSE &lt;- sum(residuals(lm(y ~ x))^2)/98 # where 98 is n-2\nvcov_mat &lt;- MSE * solve(crossprod(cbind(1, x)))\n\nvcov_mat is the same as vcov(lm(y ~ x))",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-nhst",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-nhst",
    "title": "Statistics",
    "section": "Null Hypothesis Significance Testing (NHST)",
    "text": "Null Hypothesis Significance Testing (NHST)\n\nMisc\n\nWhy would we not always use a non-parametric test so we do not have to bother about testing for normality? The reason is that non-parametric tests are usually less powerful than corresponding parametric tests when the normality assumption holds. Therefore, all else being equal, with a non-parametric test you are less likely to reject the null hypothesis when it is false if the data follows a normal distribution. It is thus preferred to use the parametric version of a statistical test when the assumptions are met.\nIf your statistic value is greater than the critical value, then it‚Äôs significant and you reject H0\n\n\nThink in terms of a distribution with statistic values on the x-axis, and greater than the critical value means you‚Äôre in the tail (one-sided)\n\nT-Statistic Table\nZ-Statistic Table\n\nExample: 95% CI ‚Üí Œ± = 100% - 95% = 0.05 ‚Üí Œ±/2 (1-tail) = 0.025\n\n1 - 0.025 = 0.975 (subtract from 1 because the z-score table cells are for the area left of the critical value\nThe z-score is 1.96 for a 95% CI\n\n\nZ-score comes from adding the row value with the column value that has the cell value of our area (e.g.¬†0.975) left of the critical value\nIf the area was between 0.97441 and 0.97500, then the z-score would be the row value, 1.9, added to the column value that‚Äôs half way between 0.05 and 0.06, which results in a z-score of 1.955\n\n\n\n\nType I Error - False-Positive; occurs if an investigator rejects a null hypothesis that is actually true in the population\n\nThe models perform equally well, but the A/B test still produces a statistically significant result. As a consequence, you may roll out a new model that doesn‚Äôt really perform better.\nYou can control the prevalence of this type of error with the p-value threshold. If your p-value threshold is 0.05, then you can expect a Type I error in about 1 in 20 experiments, but if it‚Äôs 0.01, then you only expect a Type I error in only about 1 in 100 experiments. The lower your p-value threshold, the fewer Type I errors you can expect.\n\nType II Error - False-Negative; occurs if the investigator fails to reject a null hypothesis that is actually false in the population\n\nThe new model is in fact better, but the A/B test result is not statistically significant.\nYour test is underpowered, and you should either collect more data, choose a more sensitive metric, or test on a population that‚Äôs more sensitive to the change.\n\nType S Error (Sign Error): The A/B test shows that the new model is significantly better than the existing model, but in fact the new model is worse, and the test result is just a statistical fluke. This is the worst kind of error, as you may roll out a worse model into production which may hurt the business metrics.\n\n{retrodesign} - Provides tools for working with Type S (Sign) and Type M (Magnitude) errors. (Vignette)\n\nType M error (Magnitude Error): The A/B test shows a much bigger performance boost from the new model than it can really provide, so you‚Äôll over-estimate the impact that your new model will have on your business metrics.\n\n{retrodesign} - Provides tools for working with Type S (Sign) and Type M (Magnitude) errors. (Vignette)\n\nFalse Positive Rate (FPR)(\\(\\alpha\\)) - ; Probability of a type I error; Pr(measured effect is significant | true effect is ‚Äúnull‚Äù)\n\\[\n\\text{FPR} = \\frac{v}{m_0}\n\\]\n\n\\(v\\): Number of times there‚Äôs a false positive\n\\(m_0\\): Number of non-significant variables\n\nFalse Discovery Rate (FDR) - Pr(measured effect is null | true effect is significant)\n\n\\[\n\\text{FDR} = \\frac{\\alpha \\pi_0}{\\alpha \\pi_0 + (1-\\beta)(1-\\pi_0)}\n\\]\n\n\\(\\alpha\\) - Type I error rate (False Positive Rate)\n\\(\\beta\\) - Type II error rate (False Negative Rate)\n\\(1-\\beta\\) - Power\n\\(\\pi_0\\) - Count of true null effects\n\\(1‚àí\\pi_0\\) - Count of true non-null effects\n\nPower - 1-Œ≤ where beta is the Probability of a type II error\nFamily-Wise Error Rate (FWER) - the risk of at least one false positive in a family of S hypotheses.\n\\[\nFWER = \\frac{v}{R}\n\\]\n\n\\(v\\): Number of times there‚Äôs a false positive\n\\(R\\): Number of times we claim Œ≤ ‚â† 0\ne.g.¬†Using the same data and variables to fit multiple models with different outcome variables (i.e.¬†different hypotheses)\n\nRomano and Wolf‚Äôs Ccorrection\n\nAccounting for the dependence structure of the p-values (or of the individual test statistics) produces more powerful procedures than Bonferroi and Holms. This can be achieved by applying resampling methods, such as bootstrapping and permutations methods.\n\nPermutation tests of regression coefficients can result in rates of Type I error which exceed the nominal size, and so these methods are likely not ideal for such applications\n\nSee Stata docs of the procedure\nPackages\n\n{wildrwolf}: Implements Romano-Wolf multiple-hypothesis-adjusted p-values for objects of type fixest and fixest_multi from the fixest package via a wild cluster bootstrap.",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-boot",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-boot",
    "title": "Statistics",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nMisc\n\nPost-Hoc Analysis, General &gt;&gt; Frequentist &gt;&gt; Bootstrap\nDo NOT bootstrap the standard deviation (article)\n\nBootstrap is ‚Äúbased on a weak convergence of moments‚Äù\nif you use an estimate based standard deviation of the bootstrap, you are being overly conservative (i.e.¬†you‚Äôre overestimating the sd and CIs are too wide)\n\nBootstrapping uses the original, initial sample as the population from which to resample, whereas Monte Carlo simulation is based on setting up a data generation process (with known values of the parameters of a known distribution). Where Monte Carlo is used to test drive estimators, bootstrap methods can be used to estimate the variability of a statistic and the shape of its sampling distribution\nUse bias-corrected bootstrapped CIs (article)\n\n‚Äúpercentile and BCa methods were the only ones considered here that were guaranteed to return a confidence interval that respected the statistic‚Äôs sampling space. It turns out that there are theoretical grounds to prefer BCa in general. It is‚Äùsecond-order accurate‚Äù, meaning that it converges faster to the correct coverage. Unless you have a reason to do otherwise, make sure to perform a sufficient number of bootstrap replicates (a few thousand is usually not too computationally intensive) and go with reporting BCa intervals.‚Äù\n\nPackages\n\n{rsample}\n{DescTools::BootCI}\nboot and boot.ci\n{ebtools::get_boot_ci}\n\n\nSteps\n\nResample with replacement\nCalculate statistic of resample\nStore statistic\nRepeat 10K or so times\nCalculate mean, sd, and quantiles for CIs across all collected statistics\n\nBayesian Bootstrapping (aka Fractional Weighted Bootstrap)\n\nMisc\n\nNotes from\n\nThe Bayesian Bootstrap\nThread\n\nPackages\n\n{fwb}\n\n\nDescription\n\nDoesn‚Äôt resample the dataset, but samples a set of weights from the Uniform Dirichlet distribution and computes weighted averages (or whatever statistic)\nWeights sum to ‚Äòn‚Äô but may be non-integers\nEach row gets a frequency weight based on the number of times they appear\nIn this way, every row is included in the analysis but given a fractional weight that represents its contribution to the statistic\n\nIn a traditional bootstrap, some rows of data may not be sampled and therefore excluded from the calculation of the statistic\n\nParticularly useful with rare events, where a row excluded from a traditional bootstrap sample might cause the whole estimation to explode (e.g., in a rare-events logistic regression where one sample has no events!)\n\n\n\nShould be faster and consume less RAM\nPython implementation\ndef classic_boot(df, estimator, seed=1):\n¬† ¬† df_boot = df.sample(n=len(df), replace=True, random_state=seed)\n¬† ¬† estimate = estimator(df_boot)\n¬† ¬† return estimate\n\ndef bayes_boot(df, estimator, seed=1):\n¬† ¬† np.random.seed(seed)\n¬† ¬† w = np.random.dirichlet(np.ones(len(df)), 1)[0]\n¬† ¬† result = estimator(df, weights=w)\n¬† ¬† return result\n\nfrom joblib import Parallel, delayed\ndef bootstrap(boot_method, df, estimator, K):\n¬† ¬† r = Parallel(n_jobs=8)(delayed(boot_method)(df, estimator, seed=i) for i in range(K))\n¬† ¬† return r\n\ns1 = bootstrap(bayes_boot, dat, np.average, K = 1000)",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-desc",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-desc",
    "title": "Statistics",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nMeans\n\nGeometric Mean\nsummarize_revenue &lt;- function(tbl) {\n¬† ¬† tbl %&gt;%\n¬† ¬† ¬† ¬† summarize(geom_mean_revenue = exp(mean(revenue)),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† n = n())\n}\n\n\n\nProportions\n\nVariance of a proportion\n\nAssume that p applies equally to all n subjects\n\\[\n\\sigma^2_p = \\frac{p(1-p)}{n}\n\\]\n\nExample\n\nSample of 100 subjects where there are 40 females and 60 males\n10 of the females and 30 of the males have the disease\n\nMarginal estimate of the probability of disease is (30+10)/100 = 0.4\n\nVariance of the estimator assuming constant risk (i.e.¬†assuming risk for females = risk for males)\n\n(prob_of_disease √ó prob_not_disease) / n = (0.4 √ó 0.6) / 100 = 0.0024\n\np = (10 + 30) / 100 = 0.40\n\n\n\n\nAssume p depends on a variable (e.g.¬†sex)\n\\[\n\\sigma^2_p = \\frac{p^2_{1,n} \\cdot p_1(1-p_1)}{n_1} + \\frac{p^2_{2,n} \\cdot p_2(1-p_2)}{n_2}\n\\]\n\nExample\n\nDescription same as above\nAdjusted marginal estimate of the probability of disease is\n\n(prop_female √ó prop_disease_female) + (prop_male √ó prop_disease_male)\n(0.4 √ó 0.25) + (0.6 √ó 0.5) = 0.4\nSame marginal estimate as before\n\nVariance of the estimator assuming varying risk (i.e.¬†assumes risk for females \\(\\neq\\) risk for males)\n\n1st half of equation:\n\nprop_female2 √ó (prop_disease_female √ó prop_not_disease_female) / n_female = [0.42 √ó (0.25 √ó 0.74) / 40]\n\n2nd half of equation\n\nprop_male2 √ó (prop_disease_male √ó prop_not_disease_male) / n_male = [0.62 √ó (0.5 √ó 0.5) / 60]\n\n1st half + 2nd half = 0.00224\n\nVariance is smaller than before\n\n\n\n\n\nCIs\n\nPackages:\n\n{binomCI} - 12 confidence intervals for one binomial proportion or a vector of binomial proportions are computed\n\nJeffrey‚Äôs Interval\n# probability of event\n# n_rain in the number of events (rainy days)\n# n is the number of trials (total days)\nmutate(pct_rain = n_rain / n, \n          # jeffreys interval\n          # bayesian CI for binomial proportions\n          low = qbeta(.025, n_rain + .5, n - n_rain + .5), \n          high = qbeta(.975, n_rain + .5, n - n_rain + .5))\n\n\n\n\nSkewness\n\nPackages:\n\n{moments} - Standard algorithm\n{e1071} - 3 alg options\n{DescTools::Skew} - Same algs but with bootstraps CIs\n\nFrom the paper referenced in e1071, b1 (type 3) is better for non-normal population distributions and G1 (type 2) is better for normal population distributions\nSymmetric: Values between -0.5 to 0.5\nModerated Skewed data: Values between¬†-1 and -0.5¬†or between¬†0.5 and 1\nHighly Skewed data: Values¬†less than -1¬†or¬†greater than 1\nRelationship between Mean and Median under different skewness\n\n\n\n\nKurtosis\n\nA high kurtosis distribution has a sharper peak and longer fatter tails, while a low kurtosis distribution has a more rounded peak and shorter thinner tails.\nTypes\n\n\nMesokurtic: kurtosis = ~3\n\nExamples: normal distribution. Also binomial distribution when p = 1/2 +/- sqrt(1/12)\n\nLeptokurtic: This distribution has fatter tails and a sharper peak. Excess kurtosis &gt; 3\n\nExamples: Student‚Äôs t-distribution, Rayleigh distribution, Laplace distribution, exponential distribution, Poisson distribution and the logistic distribution\n\nPlatykurtic: The distribution has a lower and wider peak and thinner tails. Excess kurtosis &lt; 3\n\nExamples: continuous and discrete uniform distributions, raised cosine distribution, and especially the Bernoulli distribution\n\nExcess Kurtosis is the kurtosis value - 3\n\n\n\n\nUnderstanding CI, SD, and SEM Bars\n\narticle\nP-values test whether the sample means are different from each other\nsd bars: Show the population spread around each sample mean. Useful as predictors of the range of new sample.\n\nNever seen these and it seems odd to mix a sample statistic with a population parameter and that the range is centered on the sample mean (unless the sample size is large I guess).\n\ns.e.m. is the ‚Äústandard error of the mean‚Äù (See Terms)\n\nIn large samples, the s.e.m. bar can be interpreted as a 67% CI.\n95% CI ‚âà 2 √ó s.e.m. (n &gt; 15)\n\nFigure 1\n\n\nEach plot shows 2 points representing 2 sample means\nPlot a: bars of both samples touch and are the same length\n\nsem bars intepretation: Commonly held view that ‚Äúif the s.e.m. bars do not overlap, the difference between the values is statistically significant‚Äù is NOT correct. Bars touch here but don‚Äôt overlap and the difference in sample means is NOT significant.\n\nPlot b: p-value = 0.05 is fixed\n\nsd bar interpretation: Although the means differ, and this can be detected with a sufficiently large sample size, there is considerable overlap in the data from the two populations.\nsem bar intepretation: For there to be a significant difference in sample means, sem bars have to much further away from each other than there just being a recognizable space between the bars.\n\n\nFigure 2\n\n\nPlot a: shows how a\n\n95% CI captures the population mean 95% of the time but as seen here, only 18 out of 20 sample CIs (90%) contained the population mean (i.e.¬†this is an asymptotic claim)\nA common misconception about CIs is an expectation that a CI captures the mean of a second sample drawn from the same population with a CI% chance. Because CI position and size vary with each sample, this chance is actually lower.\n\nPlot b:\n\nHard to see at first but the outer black bars are the 95% CI and the inner gray bars are the sem.\nBoth the CI and sem shrink as n increases and the sem is always encompassed by the CI\n\n\nFigure 3\n\n\nsem bars must be separated by about 1 sem (which is half a bar) for a significant difference to be reached at p-value = 0.05\n95% CI bars can overlap by as much as 50% and still indicate a significant difference at p-value = 0.05\n\nIf 95% CI bars just touch, the result is highly significant (P = 0.005)",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-desc-pvfun",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-desc-pvfun",
    "title": "Statistics",
    "section": "P-Value Function",
    "text": "P-Value Function\n\nNotes from https://ebrary.net/72024/health/value_confidence_interval_functions\n{concurve} creates these curves\nGives a more complete picture than just stating the p-value (strength and precision of the estimate)\n\nShows level of precision of the point estimate via shape of the curve\n\nnarrow-based, spikey curves = more precise\n\nVisualizes strength of the effect along the x-axis. Helps in showing ‚Äúsignificant‚Äù effect is not necessarily a meaningful effect.\n\nShows other estimate(s) that are also consistent with that p-value\nShows p-values associated with other estimates for the Null Hypothesis\n\nsee the end of the article for discussion on using this fact in an interpretation context\n\nThe P-value function is closely related to the set of all confidence intervals for a given estimate. (see example 3)\nExample 1\n\n\np-value of the point estimate is (always?) 1 which says, ‚Äúgiven a null hypothesis =  is true (i.e.¬†the true risk ratio = ),¬† the probability of seeing data produce this estimate or this estimate with more strength (ie smaller std error) is 100%.‚Äù\n\nI.e. the pt est is the estimate most compatible with the data.\nThis pval language is mine. The whole ‚Äúthis data or data more extreme‚Äù has never sit right with me. I think this is more correct if my understanding is right.\n\nThe pval for the data in this example is at 0.08 for a H0 of 1. So unlikely, but typically not unlikely enough in order to reject the null hypothesis.\nA pval of 0.08 is identical for a pt est = 1 or pt est = 10.5\nWide base of the curve indicates the estimate is imprecise. There‚Äôs potentially a large effect or little or no effect.\n\nExample 2\n\n\nmore data used for the second curve which indicates a precise point estimate.\npt est very close to H0\npval = 0.04 (not shown in plot)\n\nso the arbitrary pval = 0.05 threshold is passed and says a small effect is probably present\nIs that small of an effect meaningful even if it‚Äôs been deemed statistically present?\n\nIn this case a plot with the second curve helps show that ‚Äústatistically significant‚Äù doesn‚Äôt necessarily translate to meaningful effect\n\nExample 3\n\n\nThe different confidence intervals reflect the same degree of precision (i.e.¬†the curve width doesn‚Äôt change when moving from one CI to another).\nThe three confidence intervals are described as nested confidence intervals. The P-value function is a graph of all possible nested confidence intervals for a given estimate, reflecting all possible levels of confidence between 0% and 100%.",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-fun",
    "href": "qmd/cli-linux.html#sec-cli-lin-fun",
    "title": "Linux",
    "section": "Functions",
    "text": "Functions\n\nBasic\nsay_hello() {\n  echo \"hello\"\n}\nsay_hello\nUsing Return\nfailing_func () {\n  return 1\n}\n\nreturn cannot take strings ‚Äî only numbers 1 to 255\n\nWith arguments\nsay_hello() {\n  echo \"Hello $1 and $2\"\n}\nsay_hello \"Ahmed\" \"Layla\"\nDeclaring local and global variables\nsay_hello() {\n  local x\n  x=$(date)\n  y=$(date)\n}\n\nlocal is a keyword\nx is local and y is global\n\nSuppress errors\nlocal x=$(moose)\n\nWhen local is used in the same line as the variable declaration, then the variable never errors. e.g.¬†Even if moose doesn‚Äôt exist, this line won‚Äôt trigger an error",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html",
    "href": "qmd/spreadsheets.html",
    "title": "Spreadsheets",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-misc",
    "href": "qmd/spreadsheets.html#sec-spdsht-misc",
    "title": "Spreadsheets",
    "section": "",
    "text": "Some Excel files are binaries and in order to use download.file, you must set mode = ‚Äúwb‚Äù\ndownload.file(url, \n              destfile = glue(\"{rprojroot::find_rstudio_root_file()}/data/cases-age.xlsx\"), \n              mode = \"wb\")\nIndustry studies show that 90 percent of spreadsheets containing more than 150 rows have at least one major mistake.",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-cats",
    "href": "qmd/spreadsheets.html#sec-spdsht-cats",
    "title": "Spreadsheets",
    "section": "Catastrophes",
    "text": "Catastrophes\n\nReleasing confidential information\n\nIrish police accidently handed out officers private information when sharing sheets with statistics due to a freedom of information request. (link)\n\nErrors when combining sheets\n\nWales dismissed anaesthesiologists after mistakenly deeming them ‚Äúunappointable.‚Äù Spreadsheets from different areas lacked standardization in formatting, naming conventions, and overall structure. To make matters worse, data was manually copied and pasted between various spreadsheets, a time-consuming and error-prone process. (link)\nWhen consolidating assets from different spreadsheets, the spreadsheet data was not ‚Äúcleaned‚Äù and formatted properly. The Icelandic bank‚Äôs shares were subsequently undervalued by as much as ¬£16 million. (link)\n\nData entry errors\n\nCryto.com accidentally transferred $10.5 million instead of $100 into the account of an Australian customer due to an incorrect number being entered on a spreadsheet. (link)\nNorway‚Äôs $1.5tn sovereign wealth fund lost $92M, on an error relating to how it calculated its mandated benchmark. A person used the wrong date, December 1st instead of November 1st. (link)",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-bprac",
    "href": "qmd/spreadsheets.html#sec-spdsht-bprac",
    "title": "Spreadsheets",
    "section": "Best Practices",
    "text": "Best Practices\n\nNotes from Data organization in spreadsheets\n\nBe consistent\nWrite dates like YYYY-MM-DD\nDon‚Äôt leave any cells empty\nPut just one thing in a cell\nOrganize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row)\nCreate a data dictionary\nDon‚Äôt include calculations in the raw data files\nDon‚Äôt use font color or highlighting as data\nChoose good names for things\nMake backups\nUse data validation to avoid data entry errors\nSave the data in plain text files.",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-transspr",
    "href": "qmd/spreadsheets.html#sec-spdsht-transspr",
    "title": "Spreadsheets",
    "section": "Transitioning from Spreadsheet to DB",
    "text": "Transitioning from Spreadsheet to DB\n\nMisc\n\nWhen you start to have multiple datasets or when you want to make use of several columns in one table and other columns in another table you should consider going the local database route.\nUse db ‚Äúnormalization‚Äù to figure out a schema\nAlso see\n\nDatabases, Normalization\nDatabases, Warehouses &gt;&gt; Design a Warehouse\n\n\nDB advantages over spreadsheets:\n\nEfficient analysis: Relational databases allow information to be retrieved quicker to then be analyzed with SQL (Structured Query Language), to then run queries.\n\nOnce spreadsheets get large, they can lag or freeze when opening, editing, or performing simple analyses in them.\n\nCentralized data management: Since relational databases often require a certain type or format of data to be input into each column of a table, it‚Äôs less likely that you‚Äôll end up with duplicate or inconsistent data.\nScalability: If your business is experiencing high growth, this means that the database will expand, and a relational database can accommodate an increased volume of data.\n\nStart documenting the spreadsheets\n\nFile Names, File Paths\nUnderstand where values are coming from\n\nSource (e.g.¬†department, store, sensor), Owner\n\nHow rows of data are being generated\n\nWho/What is inputting the data\n\nHow does each spreadsheet/notebooks/set of spreadsheets fit in the company‚Äôs business model\n\nHow are they being used and by whom\n\nMap the spreadsheets relationships to one another\n\nSee Databases, Warehouses &gt;&gt; Design a Warehouse",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html",
    "href": "qmd/econometrics-fixed-effects.html",
    "title": "Fixed Effects",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-misc",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-misc",
    "title": "Fixed Effects",
    "section": "",
    "text": "Model with independent intercepts for each time point and/or case, which are called ‚Äúfixed effects‚Äù\n\nThe effects the omitted variables have on the subject at one time, they will also have the same effect at a later time; hence their effects will be constant, or ‚Äúfixed.‚Äù\nA ‚Äúfixed effect‚Äù in statistics is a non-random regression term, while a ‚Äúfixed effect‚Äù in econometrics means that the coefficients in a regression model are time-invariant\n\nNotes from\n\nhttps://www.econometrics-with-r.org/10-rwpd.html\nhttps://www.robertkubinec.com/post/fixed_effects/\n\nPackages\n\n{plm}\n\nFunctions for model estimation, testing, robust covariance matrix estimation, panel data manipulation and information.\n\n{fixest}\n\nFast estimation, has parallel option, glm option and many other features\n\n{estimatr}\n\nProviding a range of commonly-used linear estimators, designed for speed and for ease-of-use. Users can easily recover robust, cluster-robust, and other design appropriate estimates.\nUsers can choose an estimator to reflect cluster-randomized, block-randomized, and block-and-cluster-randomized designs.\n\n{{panelsplit}} (article)- CV for panel data prediction. It seems to take an expanded window approach. Also see Cross-Validation &gt;&gt; Time Series.\n\nIf you used {plm} + {coeftest} and want stata errors, then vcov = vcovCL",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-terms",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-terms",
    "title": "Fixed Effects",
    "section": "Terms",
    "text": "Terms\n\nCoarse Clustering - Grouping the data into larger clusters or units. Each cluster represents a broader and more aggregated subset of observations (as compared to Fine Clustering).\n\nCan lead to lower variance in the estimated standard errors because it captures less of the within-cluster variation.\nMay be used when there is less within-cluster heteroscedasticity or correlation, or when computational efficiency is a concern.\n\nFine Clustering - Grouping the data into small clusters or units. Each cluster represents a relatively small and specific subset of observations in the dataset.\n\nCan lead to higher variance in the estimated standard errors because it captures more of the within-cluster variation.\nAppropriate when there is a substantial degree of heteroscedasticity or correlation within these small clusters.\n\nFixed Panel - When the same set of units/people/cases is tracked throughout the study\nHomogeneous (or Pooled) - Panel data models that assume the model parameters are common across individuals.\nHeterogeneous - Panel models allow for any or all of the model parameters to vary across individuals.\n\nFixed effects and random effects models are both examples of heterogeneous panel data models.\n\nRotating Panel - When the units/people/cases change during the study",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-consid",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-consid",
    "title": "Fixed Effects",
    "section": "Considerations",
    "text": "Considerations\n\nFixed Effects or Random Effects (aka mixed effects model)?\n\nIf there‚Äôs likely correlation between unobserved group/cases variables (e.g.¬†individual talent) and treatment variable (i.e.¬†E(Œ±|x) != 0) AND there‚Äôs substantial variance between group units, then FE is a better choice (see 1-way assumptions or Econometrics, Mixed Effects, Frequentist &gt;&gt; Assumptions for more details)\nIf cases units change little, or not at all, across time, a fixed effects model may not work very well or even at all (SEs for a FE model will be large)\n\nThe FE model is for analyzing within-units variance\n\nDo we wish to estimate the effects of variables whose values do not change across time, or do we merely wish to control for them?\n\nFE: these effects aren‚Äôt estimated but adjusted for by explicitly including a separate intercept term for each individual (Œ±i) in the regression equation\nRE: estimates these effects (might be biased if RE assumptions violated)\nThe RE model is for analyzing between-units variance\n\nThe amount of within-unit variation relative to between-unit variation has important implications for these two approaches\n\nArticle with simulated data showed that within variation around sd &lt; 0.5 didn‚Äôt detect the effect of explanatory variable but ymmv (depends on # of units, observations per unit, N)\n\nDurbin‚ÄìWu‚ÄìHausman test (plm::phtest)\n\nIf H0 is not rejected, then both FE and RE are consistent but only RE is efficient. ‚Äì&gt; use RE but if you have a lot of data, then FE is also fine.\nIf H0 is rejected, then only FE is consistent ‚Äì&gt; use FE\n\n\nValid research questions for using a fixed effect for:\n\nCases/Units (e.g.¬†State, school, individuals, stores) - ‚ÄúHow much does a case unit change relative to other case units?‚Äù\nTime (e.g.¬†Year) - ‚ÄúHow much does a case change in relation to itself over time?‚Äù\n\nHow much each case varies around its average. The larger this coefficient the more cases fluctuate in their outcomes\nExample: Survey data with individual incomes over time\n\nHow the measure is different in a particular year compared to the individual average (e.g., do they have a lower or higher income compared to their normal income).\n\n\nExamples\n\nWhether obtaining more education leads to higher earnings.\nWhether wealthier countries tend to be more democratic than poorer countries\n\n\nFixed Effects or First Difference Estimator (FD)?\n\nTaking the first difference is an alternative to the demeaning step in the FE model\nIf the error terms are homoskedastic with no serial correlation, the fixed effects estimator is more efficient than the first difference estimator.\nIf the error follows a random walk, however, the first difference estimator is more efficient. If T=2, then they are numerically equivalent, and for T &gt; 2, they are not.\n\nIs the panel data balanced?\n\nplm::is.pbalanced(&lt;data&gt;, index = c(\"&lt;id_var&gt;\", \"&lt;time_var&gt;\"))\nBalanced - Has the same number of observations for all groups/units at each time point\nUnbalanced - At least one group/unit is not observed every period\n\ne.g.¬†Have missing values at some time observations for some of the groups/units.\n\nCertain panel data models are only valid for balanced datasets.\n\nFor such models, data will need to be condensed to include only the consecutive periods for which there are observations for all individuals in the cross section.\n\n\nOmitted variable bias\n\nMultiple regression can correct for observable omitted variable bias, however, this cannot account for omitted unobservable factors that differ (e.g.¬†from state to state)¬†\n\nThis refers to doing two multivariable regression models - one for each time period\n\nFE models control for any omitted variables that are constant over time but vary between individuals by explicitly including a separate intercept term for each individual (\\(\\alpha_i\\)) in the regression equation\nYou can difference the outcome and difference predictor variables from period 1 to period 2 in order to remove the effects of unobserved omitted variables that are constant between the time periods\nFrom Kubinec differs regarding omitted variables\n\nAny statistical model should have, as its first requirement, that it match the researcher‚Äôs question. Problems of omitted variables are important, but necessarily secondary.\nFixed effects models do not control for omitted variables. What fixed effect models do is isolate one dimension of variance in the model. As a result, any variables that don‚Äôt vary on that dimension are by definition removed from the model. This side-effect is trumpeted as the great inferential benefit of fixed effect models, but it has nothing to do with inference. Fixed effects (or their cousin, random effects/hierarchical models) are simply about selecting which part of the panel dataset is most germane to the analysis.",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-pit",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-pit",
    "title": "Fixed Effects",
    "section": "Pitfalls",
    "text": "Pitfalls\n\nBickel: If you performed matching on your sample, don‚Äôt condition on any of the matching variables\n\nCan result in collider bias and opening up a previously closed backdoor\nMe: Matching makes sense because FE model has ‚ÄúCommon Trends‚Äù assumption\n\nKubinec says\n\n2-way fixed models have big problems\n\nSlope Interpretation\n\nCases and time points are nested and we end up making comparisons across both dimensions simultaneously. There is no clear research question that matches this model.\nThe one known use of the model is for difference-in-difference estimation, but only with two time points. Says to read his paper for more details.\n\nIs this what the eor book is describing for unobserved omitted variables? (see above)\n\n\nSlope Value Unreliable\n\nOnly identifiable if there‚Äôs a different effect of¬†x¬†on¬†y¬†for each time point/case\n\nI think he‚Äôs saying if there is no variation in one of your fixed effects and you fit a two-way model anyways, the calculated effect is unreliable. He says the data looks normal and you wouldn‚Äôt recognize what happened necessarily.\n\nWhen this model is unidentifiable, R fixes the problem by deleting the last dummy variable (created by factor(fixed_effect_var)) and spits out the estimate.\n\nThe coefficient estimate for the removed dummy variable shows-up as an NA in the summary\n\n\n\nIt‚Äôs best to choose whether within-case or between-case effect is more important and fit the 1-way model.\n\ni.e.¬†It is important to think about which dimension is more relevant, and then go with that dimension.\nAssumptions for a model with just an cases fixed effect\n\nResiduals have mean = 0 (i.e.¬†errors uncorrelated with X)\n\nif violated, then omitted variable bias\n\nX (variable of interest) is i.i.d\n\nwithin-cases, autocorrelation is allowed (e.g.¬†states)\n\nlarge outliers unlikely\nno perfect multicollinearity between variables\n\n\n\nPotential danger of biased effects when treatment is assigned during different periods for each group\n\nExample: group 1 is untreated at periods 1 and 2 and treated at period 3, while group 2 is untreated at period 1 and treated both at periods 2 and 3\nWhen the treatment effect is constant across groups and over time, FE regressions estimate that effect under the standard ‚Äúcommon trends‚Äù assumption.\n\nRequires that the expectation of the outcome without treatment follow the same evolution over time in every group\n\nEstimates can be severely biased ‚Äì and may even be incorrectly signed ‚Äì when treatment effects change over time within treated units (aka hetergeneous treatment effects)\nFundamentally, the main reason TWFE estimates get weird and biased with differently-timed treatments is because of issues with weights‚Äîin TWFE settings, treated observations often get negative weights and vice versa\nAccording to Jakiela (2021, 5), negative weights in treated observations are more likely in (1) early adopter countries, since the country-level treatment mean is high, and (2) later years, since the year-level treatment mean is higher.\n\n\nSo, in general, the bias comes from entity variable categories that received the treatment early and the biased weight estimates occur on observations with later time values. This is because of the extreme treatment imbalance during these ranges/intervals, and its effect on the outcome variable.\n\nHaving negative weights on treated observations isn‚Äôt necessarily bad! It‚Äôs often just a mathematical artefact, and if you have (1) enough never-treated observations and (2) enough pre-treatment data, and if (3) the treatment effects are homogenous across all countries, it won‚Äôt be a problem. But if you don‚Äôt have enough data, your results will be biased and distorted for later years and for early adopters.\nDiagnostics\n\nDo any treated units get negative weight when calculating¬†Œ≤TWFE? Check this by looking at the weights\nCan we reject the hypothesis that the treatment effects are homogenous? Check this by looking at the relationship between¬†Yit and¬†Dit. The slope shouldn‚Äôt be different.\n\ntreatment effect homogeneity implies a linear relationship between residualized outcomes and residualized treatment after removing the fixed effects\n\n\nComments\n\nShe states that she‚Äôs only looking for linearity between the two sets of residuals, but actually breaks it down further by checking whether the relationship varies by treatment. This whole procedure is computing a partial correlation except instead of the last step of measuring the correlation between the two sets of residuals (e.g.¬†cor.test(treatment_resid, out_resid) and getting the p-value, she looks at an interaction.\nI don‚Äôt understand the homogeneity check in 3.2 though. She says that if the linearity relationship varies by treatment then this breaks assumptions for TWFE models. I‚Äôve only looked at her paper and the Chaisemartin paper, and the only assumptions I saw for TWFE models in general was the ‚Äúcommon trends‚Äù and the ‚Äústrong exogeneity‚Äù assumption. I think this is more likely to be about the ‚Äúcommon trends‚Äù assumption, and my understanding of that one is that it pertains to the effect across time for a particular group. I‚Äôm guessing there‚Äôs a connection between those two concepts, but I‚Äôm not seeing it.",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-clus",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-clus",
    "title": "Fixed Effects",
    "section": "Clusters",
    "text": "Clusters\n\nMisc\n\nNotes from Cluster-robust inference: A guide to empirical practice (Paper)\n\nSections: 4.2 (level of clustering), 4.3 (level of clustering), 4.3.2 (influential clusters), 8.1 (infl clusters), 8.2 (placebo regression)\n\nAlso see Econometrics, General &gt;&gt; Standard Errors &gt;&gt; HC and HAC vcov estimators\n\nCluster-Robust Variance Estimators (CRVE)\n\n‚ÄúRandom-effects model is the only model within the class of factor models for which including cluster fixed effects can remove all intra-cluster dependence‚Äù\n\nThink this says that HC or HAC ({sandwich}) should be used for 2FE but not RE models\n\n‚ÄúEven very small intra-cluster correlations can have a large effect on standard errors when the clusters are large‚Äù\n‚ÄúIt has become quite standard in modern empirical practice both to include cluster fixed effects (and perhaps other fixed effects as well) and also to employ cluster-robust inference.‚Äù\n\nLevel of Clustering\n\n‚Äúone or more fine clusters nested within each of the coarse clusters‚Äù\n‚ÄúClustering at too fine a level generally leads to serious over-rejection, which becomes worse as the sample size increases with the numbers of clusters at all levels held constant‚Äù\n‚ÄúClustering at too coarse a level also leads to both some over-rejection and some loss of power, especially when the number of clusters is small.‚Äù\nIssues for Certain Rules of Thumb\n\nJust cluster at the coarsest feasible level\n\nMay be attractive when the number of coarse clusters G is reasonably large, but it can be dangerous when G is small, or when the clusters are heterogeneous in size or other features\n\nCluster at whatever level yields the largest standard error(s) for the coefficient(s) of interest\n\nWill often lead to the same outcome as the first one, but not always. When the number of clusters, G, is small, cluster-robust standard errors tend to be too small, sometimes much too small. Hence, the second rule of thumb is considerably less likely to lead to severe over-rejection than the first one. However, because it is conservative, it can lead to loss of power (or, equivalently, confidence intervals that are unnecessarily long).\n\n\nRecommended: Cluster at the treatment level\n\ne.g.¬†If the treatment is assigned by classroom then cluster by classroom\nBut if there‚Äôs concern of significant spillover effects, then cluster at a coarser level than the treatment level (e.g.¬†schools)\n\n\nDiagnostics\n\nStatistical testing for the correct level of clustering\n\nHard to tell but I don‚Äôt think any of the tests were recommended in the paper\n\nChecking for influential clusters\n\nInfluential Cluster - Estimates change a lot when it‚Äôs deleted.\n‚ÄúIn a few extreme cases, there may be a cluster \\(h\\) for which it is impossible to compute \\(Œ≤_j^{(h)}\\). If so, then the original estimates should probably not be believed.\n\nThis will happen, for example, when cluster \\(h\\) is the only treated one. Inference is extremely unreliable in that case.‚Äù\n\n\nPlacebo Regressions\n\nProcess\n\nAdd a random dummy variable to the model\nfit model check if dummy variable is significant\nrepeat many times\n\nBecause a placebo regressor is artificial, we would expect valid significance tests at level Œ± to reject the null close to Œ±% of the time when the experiment is repeated many times.\nExample:\n\nClustering at levels below state-level leads to rejection rates far greater than Œ±\nUsing a state-level CRVE is important for survey data that samples individuals from multiple states. If we fail to do so, we will find, with probability much higher than Œ±, that nonsense regressors apparently belong in the model.\n\ni.e.¬†placebo regressors are significant &gt; 5% of the time\n\n\nA placebo-regressor experiment should lead to over-rejection whenever both the regressor and the residuals display intra-cluster correlation at a coarser level than the one at which the standard errors are clustered. (e.g.¬†&lt; 5%)\nIf the placebo regressor is clustered at the coarse level, we would expect significance tests based on heteroskedasticity-robust standard errors to over-reject whenever the residuals are clustered at either level. Similarly, we would expect significance tests based on finely-clustered standard errors to over-reject whenever the residuals are clustered at the coarse level. Table 4 in Section 8.2 displays both of these phenomena.",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-owfe",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-owfe",
    "title": "Fixed Effects",
    "section": "One-Way Fixed Effects",
    "text": "One-Way Fixed Effects\n\nOnly compares different periods within the same cases category and discards the between-cases variance Steps\nSteps\n\nRemove endogeneity (resulting from omitted variable bias)\n\nFirst, the error is broken into 2 parts\n\\[\n\\begin{align}\n&y_{it} = \\beta x_{it}+ \\nu_{it} \\\\\n&\\text{where}\\: \\nu_{it} = \\alpha_i + \\epsilon_{it} = 0\n\\end{align}\n\\]\n\n\\(\\alpha\\) is the cases/units-specific or between part of the error\n\nunit-specific heterogeneity, the error component that is constant over time\nIt‚Äôs the unit fixed effect, the unit-specific intercept.\n\n\\(\\epsilon\\) is time-varying or within part of the error\n\nIdiosyncratic, varying both over units and over time\n\n\nThen, each group (aka cases) is centered by each group‚Äôs mean\n\\[\n\\begin{align}\ny_{it}-\\bar y_i &= \\beta(x_{it} - \\bar x_i) + (\\alpha_i - \\alpha_i) + (\\epsilon_{it} - \\bar \\epsilon_i) \\\\\n\\tilde y_{it} &= \\beta \\tilde x_{it} + \\tilde \\epsilon_{it}\n\\end{align}\n\\]\n\nThe centering eliminates all between-group variance, including the person-specific part of the error term (\\(\\alpha_i\\)), and leaves only the within-group variability to analyze\n\n\\(\\alpha_i\\) is a constant so it‚Äôs mean is equal to itself\n\n\n\nOLS is performed after the endogeneity is removed.\n\nAssumptions\n\nFunctional Form\n\nAdditive fixed effect\nConstant and contemporaneous treatment effect (aka homogeneous treatment effects)\nLinearity in covariates\n\nTime-constant unobserved heterogeneity is allowed (not the case for Mixed Effects models)\n\ni.e.¬†\\(\\mathbb{E}(\\alpha|x) \\neq 0\\) or correlation between unobserved unit variables that are constant across time and \\(x\\) is allowed\n\nThis correlation is seen in the figure at the top of section\n\nEach group‚Äôs \\(x\\) values get larger from left to right as each group‚Äôs \\(\\alpha\\) (aka \\(y\\)-intercepts) for each unit get larger time-constant, unobserved variablesexplain variation between cases units\n\n\n\nStrong (strict) Exogeneity\n\n\\(\\mathbb{E}(\\epsilon|x,\\alpha)=0\\)\nTime-varying unobserved heterogeneity biases the estimator\nAlso see Pitfalls &gt;&gt; Kubinec\n\n\nExample\ne2 &lt;- plm(wage ~ marriage, data = df2,\n¬† ¬† ¬† ¬† ¬† index = c(\"id\", \"wave\"),\n¬† ¬† ¬† ¬† ¬† effect = \"individual\", model = \"within\")\n\nWhere marriage is the variable of interest, id is the cases variable and wave is the time variable\nUsing effect = ‚Äúindividual‚Äù, model = ‚Äúwithin‚Äù specifies a one-way fixed effects model",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-twfe",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-twfe",
    "title": "Fixed Effects",
    "section": "Two-Way Fixed Effects (TWFE)",
    "text": "Two-Way Fixed Effects (TWFE)\n\nAdds a time variable that is constant (fixed effect) across cases but varies over time\n\\[\ny_{it} = \\beta x_{it} + \\alpha_i + \\zeta_t + \\epsilon_{it}\n\\]\n\nWhere \\(\\zeta\\) is the time fixed effect\n\nSteps\n\nRemove endogeneity (resulting from omitted variable bias)\n\nFirst, the error is broken into 2 parts: \\(\\nu_{it} = \\alpha_i + \\epsilon_{it} = 0\\)\n\nWhere \\(\\alpha\\) is the cases-specific or between part of the error and \\(\\epsilon\\) is time-varying or within part of the error\n\nThen,\n\\[\n(y_{it} - \\bar y_i -\\bar y_t + \\bar y) = \\beta(x_{it} - \\bar x_i - \\bar x_t + \\bar x) +(\\epsilon_{it} = \\bar \\epsilon_i - \\bar \\epsilon_t + \\bar \\epsilon)\n\\]\n\nFor each group/case, variables are centered by that group‚Äôs mean\nFor each period, variables are centered by that time period‚Äôs mean\nThe grand mean is added back\n\n\nOLS is performed after the endogeneity is removed.\n\nAssumptions\n\nTime-constant unobserved heterogeneity is allowed (See 1-way FE assumptions)\nFunctional Form\n\nAdditive fixed effect\nConstant and contemporaneous treatment effect\nLinearity in covariates\n\nStrong (strict) Exogeneity (also see 1-way FE assumptions)\n\n\\(Œµ \\perp D_{is}, X_{i}, \\alpha_i, \\zeta_t\\)\n\nThis implies the below statement\n\nTreatment assignment, \\(D_i\\), for a given unit, \\(i\\), in time, \\(s\\), is independent of the potential outcomes for that unit in that time period\n\\[\n{Y_{it}(0), Y_{it}(1)} \\perp D_{is}\\;|\\; \\boldsymbol{X}_i^{1:T}, \\alpha_i, \\boldsymbol{f}^{1:T} \\quad \\quad \\forall\\; i, t, s\n\\]\n\ne.g A policy (i.e.¬†treatment) doesn‚Äôt get enacted in region because it experiences negative economic shocks and we‚Äôre measuring some economic metric\nAs a result, if we only had observed outcomes (which of course is all we have), we can substitute either \\(Y_{it}(0)\\) or \\(Y_{it}(1)\\) depending on whether we observe \\(D_{is}= 1\\) or \\(D_{is}= 0\\) and we can still, at least theoretically, get an unbiased estimate of the treatment effect.\n\n\\(D\\) is the treatment variable so it‚Äôs \\(x_{it}\\) in the other equations above and here, \\(X\\) is probably other adjustment variables\n\\(f\\) is the time fixed effect\n\n\nImplies treatment status is assigned randomly or at one shot, not sequentially\n\n\nCommon Trends\n\nSee Fixed Effects with Individual Slopes (FEIS) section for models that relax this assumption\nFor \\(t \\geq 2, \\mathbb{E}(Y_{g,t}(0) ‚àí Y_{g,t‚àí1}(0))\\) does not vary across group, \\(g\\)\n\n\\(Y_{g,t}(0)\\) denotes average potential outcomes without treatment in group \\(g\\) at period \\(t\\).\n\\(Y_{g,t}(1)\\) would denote average potential outcomes with treatment in group \\(g\\) at period \\(t\\).\ni.e.¬†For each period after the first period, the expected change in outcome doesn‚Äôt vary across group \\(g\\)\n\nExample\n\n\nBefore treatment (getting married), wages for the treatment group (top 2 lines) were growing at a substantially faster rate than the control group (bottom two lines). This violates the Common Trends assumption\n\n\n\nExample:\nfe3 &lt;- \n  plm(wage ~ marriage, data = df2,\n¬† ¬† ¬† index = c(\"id\", \"wave\"),\n¬† ¬† ¬† effect = \"twoways\", \n¬† ¬† ¬† model = \"within\")\n\nWhere marriage is the variable of interest, id is the cases variable and wave is the time variable\nUsing effect = ‚Äútwoways‚Äù, model = ‚Äúwithin‚Äù specifies a two-way effects model\n\nExample:\n\nModel\n\\[\n\\begin{align}\nY_{it} &= \\beta_0 + \\beta_1 X_{it} + \\gamma_2 D2_i + \\cdots + \\gamma_n DT_i + \\delta_2B2_t + \\cdots + \\delta_n BT_t + u_{it} \\\\\n\\text{FatilityRate}_{it} &= \\beta_1 \\text{BeerTax}_{it} + \\text{StateEffects} + \\text{TimeFixedEffect} + u_{it}\n\\end{align}\n\\]\n\nIncluding the intercept would allow for a change in the mean fatality rate in the time between the years 1982 and 1988 in the absence of a change in the beer tax.\nThe variable of interest is Beer Tax and it‚Äôs effect on Fatality Rate\n\nBeer Tax is a continuous variable with a value for each unit and for each year\n\nThe state and time fixed effects are the dummy variables in the formal equation\nTheir coefficients start at 2 because the intercept coefficient is considered the first coefficient\n\nCode\n# Two ways to fit the model\n\nlm(fatal_rate ~ beertax + state + year - 1, data = Fatalities)\n\nfatal_tefe_mod &lt;- \n  plm::plm(fatal_rate ~ beertax,\n¬† ¬† ¬† ¬† ¬† ¬†data = Fatalities,\n¬† ¬† ¬† ¬† ¬† ¬†index = c(\"state\", \"year\"),\n¬† ¬† ¬† ¬† ¬† ¬†# fixed effects estimator is also called the 'within' estimator\n¬† ¬† ¬† ¬† ¬† ¬†model = \"within\",\n¬† ¬† ¬† ¬† ¬† ¬†effect = \"twoways\") # twoways required for \"entities\" and \"time\" fixed effects\n\n# only calcs for variable of interest\n# if needed, dof = nrow(dat) - 1\ncoeftest(fatal_tefe_mod, vcov = vcovHC, type = \"HC1\")\n#&gt; t test of coefficients:\n#&gt;¬†\n#&gt;¬† ¬† ¬† ¬† Estimate Std. Error t value Pr(&gt;|t|)¬†\n#&gt; beertax -0.63998¬† ¬† 0.35015 -1.8277¬† 0.06865 .\n\n# moar adjustment vars\nfatalities_mod6 &lt;- \n  plm::plm(fatal_rate ~ beertax + year + drinkage\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† + punish + miles + unemp + log(income),\n¬† ¬† ¬† ¬† ¬† ¬†index = c(\"state\", \"year\"),\n¬† ¬† ¬† ¬† ¬† ¬†model = \"within\",\n¬† ¬† ¬† ¬† ¬† ¬†effect = \"twoways\",\n¬† ¬† ¬† ¬† ¬† ¬†data = Fatalities)\n\nstate and year variables need to be factors\nIntercept removed because it has no meaning in this context\n\n\nExample: {estimatr}\nmodel_lm_robust &lt;- \n    estimatr::lm_robust(primary ~ treatment,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† fixed_effects = ~ country + year,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† data = fpe_primary,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† clusters = country, se_type = \"stata\")\n\ntidy(model_lm_robust)\n##¬† ¬† ¬† ¬† term estimate std.error statistic p.value conf.low conf.high df outcome\n## 1 treatment¬† ¬† 20.4¬† ¬† ¬† 9.12¬† ¬† ¬† 2.24¬† 0.0418¬† ¬† 0.867¬† ¬† ¬† ¬† 40 14 primary\n\nglance(model_lm_robust)\n##¬† r.squared adj.r.squared statistic p.value df.residual nobs se_type\n## 1¬† ¬† 0.768¬† ¬† ¬† ¬† 0.742¬† ¬† ¬† ¬† NA¬† ¬† ¬† NA¬† ¬† ¬† ¬† ¬† 14¬† 490¬† stata\nExample: {fixest}\nmodel_feols &lt;- \n  fixest::feols(primary ~ treatment | country + year,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† data = fpe_primary,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† cluster = ~ country,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† dof = dof(fixef.K = \"full\"))\n\ntidy(model_feols)\n## # A tibble: 1 √ó 5\n##¬† term¬† ¬† ¬† estimate std.error statistic p.value\n##¬† &lt;chr&gt;¬† ¬† ¬† ¬† &lt;dbl&gt;¬† ¬† &lt;dbl&gt;¬† ¬† &lt;dbl&gt;¬† &lt;dbl&gt;\n## 1 treatment¬† ¬† 20.4¬† ¬† ¬† 9.12¬† ¬† ¬† 2.24¬† 0.0418\n\nglance(model_feols)\n## # A tibble: 1 √ó 9\n##¬† r.squared adj.r.squared within.r.squared pseudo.r.squared sigma¬† nobs¬† AIC¬† BIC logLik\n##¬† ¬† ¬† &lt;dbl&gt;¬† ¬† ¬† ¬† &lt;dbl&gt;¬† ¬† ¬† ¬† ¬† ¬† &lt;dbl&gt;¬† ¬† ¬† ¬† ¬† ¬† &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;¬† &lt;dbl&gt;\n## 1¬† ¬† 0.768¬† ¬† ¬† ¬† 0.742¬† ¬† ¬† ¬† ¬† ¬† 0.111¬† ¬† ¬† ¬† ¬† ¬† ¬† NA¬† 14.7¬† 490 4071. 4280. -1985.\n\n# Standard print,summary output from a fixest model (from vignette)\nprint(fixest_pois_mod)\n#&gt; Poisson estimation, Dep. Var.: Euros\n#&gt; Observations: 38,325¬†\n#&gt; Fixed-effects: Origin: 15,¬† Destination: 15,¬† Product: 20,¬† Year: 10\n#&gt; Standard-errors: Clustered (Origin)¬†\n#&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value¬† Pr(&gt;|t|)¬† ¬†\n#&gt; log(dist_km) -1.52787¬† 0.115678 -13.208 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:¬† 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; Log-Likelihood: -7.025e+11¬† Adj. Pseudo R2: 0.764032\n#&gt;¬† ¬† ¬† ¬† ¬† ¬† BIC:¬† 1.405e+12¬† ¬† Squared Cor.: 0.612021\n\n# With clustered SEs\nsummary(fixest_pois_mod, vcov = \"twoway\")\n#&gt; Poisson estimation, Dep. Var.: Euros\n#&gt; Observations: 38,325¬†\n#&gt; Fixed-effects: Origin: 15,¬† Destination: 15,¬† Product: 20,¬† Year: 10\n#&gt; Standard-errors: Clustered (Origin & Destination)¬†\n#&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error¬† t value¬† Pr(&gt;|t|)¬† ¬†\n#&gt; log(dist_km) -1.52787¬† 0.130734 -11.6869 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:¬† 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; Log-Likelihood: -7.025e+11¬† Adj. Pseudo R2: 0.764032\n#&gt;¬† ¬† ¬† ¬† ¬† ¬† BIC:¬† 1.405e+12¬† ¬† Squared Cor.: 0.612021",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-feis",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-feis",
    "title": "Fixed Effects",
    "section": "Fixed Effects with Individual Slopes (FEIS)",
    "text": "Fixed Effects with Individual Slopes (FEIS)\n\nFixed effects model that relaxes the Common Trends assumption (see 2-way FE assumptions above)\n\nGives each case (e.g.¬†State, school, individual, store) it‚Äôs own intercept and slope\nData are not cases ‚Äúdemeaned‚Äù like with a FE estimator, but ‚Äúdetrended‚Äù by the predicted individual slope of each cases unit\n\nMisc\n\nNotes from https://ruettenauer.github.io/Panel-Data-Analysis/Panel_part2.html#Fixed_Effects_Individual_Slopes\n{feisr}\n** Each additional slope variable requires more observations per cases category **\n\nEach cases unit needs at least q+1 observations to contribute to the model. If not, they are dropped.\n\nWhere q number of slope parameters (including a constant)\n\nMost likely this refers to the number of slope variables + constant\nExample: Slope variables are exp + I(exp^2)\n\nq = number_of_slope_vars + constant = 2 + 1 = 3 observations for each unit are required.\n\n\n(Probably not this) Example (Based on the feisr vignette): Slope variables are exp + I(exp^2)\n\nq = number_of_cases_units * (number_of_slope_vars + constant)\nq = number_of_ids * 3\nThis is the actual number of slope parameters estimated but this could be huge, so I doubt it‚Äôs this.\n\n\n\n\nModel Equation: \\(y_i = \\beta X_i + \\alpha_i W_i + \\epsilon_i\\)\n\n\\(W\\) is a matrix of slope variables\n\\(\\alpha\\) is a vector of estimated parameters for the slope variables\n\nProcess\n\nIt‚Äôs equivalent to a typical lm model except with dummies of your cases variable (e.g.¬†‚Äúid‚Äù below) and 2-way interaction terms for all combinations of dummies \\(\\times\\) each slope variable\nActual process (more efficient) (see article for more mathematical detail)\n\nEstimate the individual-specific predicted values for the dependent variable and each covariate based on an individual intercept and the additional slope variables of \\(W_i\\)\nDetrend the original data by these individual-specific predicted values\nRun an OLS model on the residual (‚Äòdetrended‚Äô) data\n\n\nExample: Does marrying increase (log) wages\nwages.feis &lt;- \n  feis(lnw ~ marry + enrol + yeduc + as.factor(yeargr)\n¬† ¬† ¬† ¬†| exp + I(exp^2), \n¬† ¬† ¬† ¬†data = mwp, \n¬† ¬† ¬† ¬†id = \"id\",\n¬† ¬† ¬† ¬†robust = TRUE)\n\nsummary(wages.feis)\n## Coefficients:\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t-value¬† Pr(&gt;|t|)¬† ¬†\n## marry¬† ¬† ¬† ¬† ¬† ¬† ¬† 0.0134582¬† 0.0292771¬† 0.4597¬† 0.64579¬† ¬†\n## enrol¬† ¬† ¬† ¬† ¬† ¬† ¬† -0.1181725¬† 0.0235003 -5.0286 5.325e-07 ***\n## yeduc¬† ¬† ¬† ¬† ¬† ¬† ¬† -0.0020607¬† 0.0175059 -0.1177¬† 0.90630¬† ¬†\n## as.factor(yeargr)2 -0.0464504¬† 0.0378675 -1.2267¬† 0.22008¬† ¬†\n## as.factor(yeargr)3 -0.0189333¬† 0.0524265 -0.3611¬† 0.71803¬† ¬†\n## as.factor(yeargr)4 -0.1361305¬† 0.0615033 -2.2134¬† 0.02697 *¬†\n## as.factor(yeargr)5 -0.1868589¬† 0.0742904 -2.5152¬† 0.01196 *¬†\n## ---\n## Signif. codes:¬† 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##¬†\n## Cluster robust standard errors\n## Slope parameters:¬† exp, I(exp^2)\n\nExperience (exp) is used for the slope variables.\nTo estimate the slope parameters, the relationship with wage (lnw) is assumed to be non-linear (exp + I(exp^2))\nInterpretation: Marrying doesn‚Äôt reliably affect wages (p-value = 0.64579)",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html",
    "href": "qmd/gnu-make.html",
    "title": "GNU Make",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-misc",
    "href": "qmd/gnu-make.html#sec-make-misc",
    "title": "GNU Make",
    "section": "",
    "text": "Notes from\n\narticle\nproject\n\nGood example of an advanced makefile for a practical data science project\n\nVideo\n\nGoes through the differect components of executing a python project with make (e.g.¬†testing, clean-up, defining variables, setting up the virtual environment, etc.)\n\n\nResources\n\nDocs - All on one page so you can just ctrl + f\nNice little tutorial\nAnother tutorial\n\nHaven‚Äôt read it, but looks pretty thorough\n\nDocs for variable types\nDocs for functions\n\nProject orchestration system that only builds steps that have changed\n\n{drake}/{targets} are based on this system\n\nAssuming that you‚Äôve named the file ‚Äúmakefile‚Äù or ‚ÄúMakefile‚Äù or something like that, simply typing make at the command line while inside your project‚Äôs directory will execute the build process.\n\nmake -B --recon shows all the commands used to build the project (i.e.¬†kind of like a DAG)\nmake -B rebuilds the entire project even if no targets have changed",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-gen",
    "href": "qmd/gnu-make.html#sec-make-gen",
    "title": "GNU Make",
    "section": "General",
    "text": "General\n\nSyntax\ntargets: prerequisites\ncommand\ncommand\ncommand\nThe prerequisites are also file names, separated by spaces. These files need to exist before the commands for the target are run. These are also called dependencies.\n.PHONY - Helpful to avoid conflicts between target names and file names\n\nConsidered best practice to use\nExample\n.PHONY: install\ninstall:\n        python3.9 -m venv venv && source venv/bin/activat && pip install -r requirements-dev.txt\n\n.PHONY: dbsetup\ndbsetup:\n        source venv/bin/activate && python -m youtube.db\n\n.PHONY: lint\nlint:\n        flake8 emojisearcher tests",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-rul",
    "href": "qmd/gnu-make.html#sec-make-rul",
    "title": "GNU Make",
    "section": "Rules",
    "text": "Rules\n\nMakefiles are made-up of rules. Each rule is a code chunk.\nExample\n# Inside Makefile\ndata/raw/NIPS_1987-2015.csv:\ncurl -o $@ https://archive.ics.uci.edu/ml/machine-learning-databases/00371/NIPS_1987-2015.csv\n\nDownloads a file\n‚Äúdata/raw/NIPS_1987-2015.csv‚Äù is the file path and target for this rule.\n$@¬† is a Make automatic variable that fills in the target name for the file name arg in the curl command.\nThere is no prerequisite required for this command. So, this syntax is just target : command.",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-tar",
    "href": "qmd/gnu-make.html#sec-make-tar",
    "title": "GNU Make",
    "section": "Targets",
    "text": "Targets\n\nThe targets are file names, separated by spaces. Typically, there is only one per rule.\nDummy Targets - A target with no commands directly associated with it (it is sort of a meta-target).\n\nUseful if you want to only rebuild part of the project\nExample: if you have a couple of scripts that involve data acquisition and cleaning, another few that involve data analysis, and a few that involve the presentation of results (paper, plot), then you might define a dummy for each of them.\nall: data model paper\ndata: raw.csv\nmodel: model.rds\npaper: plot.png paper.pdf\n\nExecuting make paper in the CLI and in the project directory will call the commands that built ‚Äúplot.png‚Äù and ‚Äúpaper.pdf‚Äù",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-var",
    "href": "qmd/gnu-make.html#sec-make-var",
    "title": "GNU Make",
    "section": "Variables",
    "text": "Variables\n\nExpanded Variables\n\nValues are accessed using $(x) or ${x})\n‚ÄúRecursively Expanded‚Äù Variables are defined using = operator\nx = hello\ny = $(x)\n# Both $(x) and $(y) will now yield \"hello\"\nx = world\n# Both $(x) and $(y) will now yield \"world\"\n\nAny functions referenced in the definition will be executed every time the variable is expanded\nCan cause infitinite loops\n\n‚ÄúSimply Expanded‚Äù Variables are defined using the := or ::= operator\nx := hello\ny := $(x)\n# Both $(x) and $(y) will now yield \"hello\"\nx := world\n# $(x) will now yield \"world\", and $(y) will yield \"hello\"\n\nAutomatic Variables\n\n$@ is a Make variable that ‚Äúexpands‚Äù into the (first?) target name\n$^ is a Make variable that ‚Äúexpands‚Äù into all of the prerequisites\n$&lt; is a Make variable that ‚Äúexpands‚Äù into the first prerequisite\n$? is a Make variable that ‚Äúexpands‚Äù into any prerequisites which have a time stamp more recent than the target\n% is a wildcard; looks for any targets in the makefile that matches it‚Äôs pattern or files in the project directory (also see abstraction section below)\nfoo%.o: %.c\n    $(CC) $(CFLAGS) -c $&lt; -o $@\n\nWill match target lib/foobar.o, with:\n\nStem ($*): lib/bar\nTarget name ($@): lib/foobar.o\nPrerequisites ($&lt;, $^): lib/foobar.c\n\n\n$*is a Make variable that ‚Äúexpands‚Äù the ‚Äústem‚Äù (i.e.¬†value) of wildcard",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-com",
    "href": "qmd/gnu-make.html#sec-make-com",
    "title": "GNU Make",
    "section": "Commands",
    "text": "Commands\n\nThe commands are a series of steps typically used to make the target(s). These need to start with a tab character, not spaces.\nSee command used to generate a target\n# CLI\n&gt;&gt; make --recon &lt;target&gt;\nUpdate a specific target\n# CLI\n&gt;&gt; make data/raw/NIPS_1987-2015.csv\n\nThis will re-run the rule that created the file. In this case, it‚Äôs the curl command in the ‚ÄúDownload a file‚Äù section\nIf you run this command again, you‚Äôll receive this message: make:data/raw/NIPS_1987-2015.csv‚Äô is up to date.`",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-exe",
    "href": "qmd/gnu-make.html#sec-make-exe",
    "title": "GNU Make",
    "section": "Execute a script",
    "text": "Execute a script\n\nExample\n\nMakefile\ndata/processed/NIPS_1987-2015.csv : src/data/transpose.py data/raw/NIPS_1987-2015.csv\n    $(PYTHON_INTERPRETER) $^ $@\n\n‚Äú$(PYTHON_INTERPRETER)‚Äù is an environment variable set in the Make file for python3 interpreter\nThe function in this example has 2 args: input file path and output file path\n$^ fills in the prerequisites which takes care of &lt;script&gt; &lt;arg1&gt;\n$@ fills in the target name for &lt;arg2&gt;\n\nCLI\n&gt;&gt; make --recon data/processed/NIPS_1987-2015.csv\npython3 src/data/transpose.py data/raw/NIPS_1987-2015.csv data/processed/NIPS_1987-2015.csv\n\nmake --recon shows us the translation of command line in the Make file\nBasic format for executing a python script in the cli is python3 &lt;script&gt; &lt;arg1&gt; &lt;arg2&gt; ... &lt;argn&gt;",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-abs",
    "href": "qmd/gnu-make.html#sec-make-abs",
    "title": "GNU Make",
    "section": "Abstraction",
    "text": "Abstraction\n\nExample\nall: models/10_topics.png models/20_topics.png\n\nmodels/%_topics.png : src/models/fit_lda.py data/processed/NIPS_1987-2015.csv src/models/prodlda.py\n    $(PYTHON_INTERPRETER) $&lt; $(word 2, $^) $@ --topics $*\n\n% matches both targets in the dummy target, ‚Äúall‚Äù and takes the stem 10 and 20\n\nSo this rule runs twice: once with the value 10 then with the value 20.\n\n$&lt; is an autmatic variable that expands into ‚Äúsrc/models/fit_lda.py‚Äù\nBuilt-in Make text function, $(word n,text) , returns the nth word of text. (see Misc &gt;&gt; Resources for function docs)\n\nThe legitimate values of n start from 1. If n is bigger than the number of words in text, the value is empty\nIn this example, it‚Äôs used to return the 2nd prerequisite to become the 1st argument of the fit_lda.py script\n\n1st arg is the input file path\n\n\n$@ is an automatic variable that expands into the target name which becomes the 2nd argument of the fit_lda.py script\n\n2nd arg is the output file path\n\n--topics is a function option for fit_ldy.py which is defined in the script using decorators from {{click}}\n\n$* is the stem of the wildcard which is a numeric in this case and provides the value the topics flag\n\n\nExample Clean text files in data directory\ndata/processed/%.txt: data/raw/%.txt\nsed 1,20d $^ &gt; $@\n\nTakes all text files in the raw directory, removes some rows (sed 1,20d), outputs (&gt;) the processed file into target with the same file name ($@)",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-vim",
    "href": "qmd/cli-general.html#sec-cli-gen-vim",
    "title": "General",
    "section": "Vim",
    "text": "Vim\n\nCommand-line based text editor\nCommon Usage\n\nEdit text files while in CLI\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you‚Äôll want to know how to write, save, and close a file.\n\nResources\n\nThe minimum vi(m) you need to know\nVim Visual Cheat Sheet\nVim Cheatsheet\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you‚Äôre in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html",
    "href": "qmd/feature-engineering-splines.html",
    "title": "Splines",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-misc",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-misc",
    "title": "Splines",
    "section": "",
    "text": "Knots are placed at several places within the data range with (usually) low-order polynomials that are chosen to fit the data between two consecutive knots.\n\nChoices\n\nNumber of knots\nTheir positions\nDegree of polynomial to be used between the knots (a straight line is a polynomial of degree 1)\n\nThe type of polynomial and the number and placement of knots is what defines the type of spline.\n\ne.g.¬†cubic splines are created by using a cubic polynomial in an interval between two successive knots.\n\nIncreasing the number of knots may overfit the data and increase the variance, whilst decreasing the number of knots may result in a rigid and restrictive function that has more bias.\n\nNotes from A review of spline function procedures in R (paper)\nAlso see:\n\nFeature Engineering, General &gt;&gt; Continuous &gt;&gt; Binning &gt;&gt; Harrell on the benefits of using splines vs binning\nFeature Engineering, Time Series &gt;&gt; Engineering &gt;&gt; Calendar features\nStatistical Rethinking &gt;&gt; (end of ) Ch 4\nFeature Engineering, Geospatial &gt;&gt; Cyclic Smoothing Spline\nHarrell‚Äôs RMS\nModel Building, tidymodels &gt;&gt; Recipe &gt;&gt; Transformations &gt;&gt; Splines\n\nCommon variables: trend, calendar features, age, cardinal directions (N, S, E, W, etc.)\nPackage Comparison\n\nDefault types: {mgcv} uses thin plate splines (see smoothing splines) as a default for it‚Äôs s() which makes it‚Äôs spline more flexible (i.e.¬†curvy) than the default splines for {gam}, {VGAM}, and {gamlss} which use cubic smoothing splines.\n\n{gamlss} doesn‚Äôt use s but instead has specific functions for specific types of splines\n\nP-Splines: {mgcv} and {gamlss} are very similar, and the differences can be attributed to the different way that two packages optimize the penalty weight, Œª.\n\n{mgcv}: option, ‚Äúps‚Äù within s will create a cubic p-spline basis on a default of 10 knots, with a third order difference penalty.\n\nThe penalty weight, Œª, is optimized with generalized cross validation.\n\n{gamlss}: pb defines cubic p-splines functions with 20 interior knots and a second order difference penalty.\n\nThe smoothing parameter is estimated using local maximum likelihood method, but there are also other options based on likelihood methods, AIC, generalized cross validation and more.\nMultiple other functions available for p-splines with various attributes.\n\n\nDependencies: {mgcv} creates its own spline functions while {gam}, {VGAM}, and {gamlss} use the base R package, {splines}.\n\n{gam} and {VGAM} call the base R function smooth.spline (smoothing spline) with four degrees of freedom as default and give identical results",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-terms",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-terms",
    "title": "Splines",
    "section": "Terms",
    "text": "Terms\n\nSmoothly Joined -¬† Means that for polynomials of degree n, both the spline function and its first n-1 derivatives are continuous at the knots.",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-tune",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-tune",
    "title": "Splines",
    "section": "Tuning Parameters",
    "text": "Tuning Parameters\n\nB: Basis functions (e.g.¬†B-Spline)\nd: The degree of the underlying polynomials in the basis\n\nTypically d = 3 (cubic) is used (&gt;3 usuallly indistinguishable)\n\nK: Number of knots for Regression Splines\n\nUsually k = 3, 4, 5. Often k = 4\n\nHarrell (uses natural splines): ‚ÄúFor many datasets, k = 4 offers an adequate fit of the model and is a good compromise between flexibility and loss of precision caused by overfitting‚Äù\n\nIf the sample size is small, three knots should be used in order to have enough observations in between the knots to be able to fit each polynomial.\nIf the sample size is large and if there is reason to believe that the relationship being studied changes quickly, more than five knots can be used.\n\n\nThere should be at least 10‚Äì20 events per degree of freedom (Harrell, RMS)\nVariables that are thought to be more influential on the outcome or more likely to have non-linear associations are assigned more degrees of freedom (i.e.¬†more knots)\nFlexibility of fit vs.¬†n and variance\n\nLarge n (e.g.¬†n ‚â• 100): k = 5\nSmall n (e.g.¬†n &lt; 30): k = 3\n\nCan use Akaike‚Äôs information criterion (AIC) to choose k\n\nThis chooses k to maximize model likelihood ratio of œá2 ‚àí 2k.\nCross-Validation is also valid\n\nAlso option for knot positions\n\nLocations not important in most situations\nPlace knots where data exist e.g.¬†fixed quantiles of predictor‚Äôs marginal distribution (See Regression Splines &gt;&gt; B-Splines for examples)\n\nFrom Harrell‚Äôs RMS\n\n\n\n\nŒª: Penalty weight for Smoothing Splines\n\n\nCalculated by generalized cross-validation in {mgcv} which is an approximation of LOO-CV\n\nSee article or Wood‚Äôs GAM book or Elements of Statistical Learning (~pg 244) for details",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-interp",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-interp",
    "title": "Splines",
    "section": "Interpretation",
    "text": "Interpretation\n\nA regression fit will result in estimated coefficients for each parameter used in the splines.\nOther than including them in technical appendices, in almost all cases, one does not present these estimated coefficients ‚Äì their interpretation is essentially meaningless.\nVisual interpretations of the predicted response vs the splined variable are useful in discovering trends or patterns.\nPredicted responses, given representative values, outlier values, or any values of interest of the splined variable, are useful in calculating various contrasts.\nEffective Coefficient\n\nIt shows how the effect of the variable on the response varies over its range\nThink this is only possible for a natural spline\nExample: Age on Survival in Titanic dataset (link)\n\nmodel_02 &lt;- \n  glm(Survived ~ SibSp + ns(Age, df = 3) + Pclass + Parch + Fare,\n      data = titanic,\n      family = binomial)\n#\n# Create a data frame for prediction: only `Age` will vary.\n#\nN &lt;- 101\nx &lt;- titanic[which.max(complete.cases(titanic)), ]\ndf &lt;- do.call(rbind, lapply(1:N, function(i) x))\ndf$Age &lt;- with(titanic, seq(min(Age, na.rm=TRUE), max(Age, na.rm=TRUE), length.out=N))\n#\n# Predict and plot.\n#\ndf$Survived.hat &lt;- predict(model_02, newdata=df) # The predicted *link,* by default\nwith(df, plot(Age, Survived.hat, type=\"l\", lwd=2, ylab=\"\", main=\"Relative spline term\"))\nmtext(\"Spline contribution\\nto the link function\", side=2, line=2)\n#\n# Plot numerical derivatives.\n#\ndAge &lt;- diff(df$Age[1:2])\ndelta &lt;- diff(df$Survived.hat)/dAge\nage &lt;- (df$Age[-N] + df$Age[-1]) / 2\nplot(age, delta, type=\"l\", lwd=2, ylab=\"Change per year\", xlab=\"Age\",\n     main=\"Spline Slope (Effective Coefficient)\")\n\nThe varying coefficient is computed by calculating the first derivatives numerically: divide the successive differences in predicted values by the successive differences in age.\nAt Age near 35 the effective slope is nearly zero, meaning small changes of Age in this range have no effect on the predicted response. Near ages of zero, the effective slope is near ‚àí0.15, indicating each additional year of Age reduces the value of the link function by about 0.15. At the oldest ages, the effective slopes are settling down to a value near ‚àí0.09, indicating each additional year of age in this age group decreases the link function by ‚àí0.09.",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-reg",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-reg",
    "title": "Splines",
    "section": "Regression Splines",
    "text": "Regression Splines\n\nNo penalty function added\n\nSplined variable is just added to the regression model like any other predictor\n\nTypes\n\nTruncated Power Basis\n\nIssue: Basis functions are not supported locally but over the whole range of the data\n\nCould lead to high correlations between some basis splines, implying numerical instabilities in spline estimation\n\nExample: d = 3 (cubic) with 5 equidistant knots\n\nExample: d = 3 with 3 knots (œÑ1, œÑ2, œÑ3)\n\n\\[\nf(X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\beta_4 (X - \\tau_1)^3 + \\beta_5 (X - \\tau_2)^3 + \\beta_5 (X - \\tau_3)^3\n\\]\n\n7 dof\n\n\nB-splines\n\n\nBased on a special parameterization of a cubic spline\nSee Statistical Rethinking Notebook &gt;&gt; (end of) Chapter 4\nBasis functions supported locally which leads to high numerical stability, and also in an efficient algorithm for the construction of the basis functions.\nIssue: can be erratic at the boundaries of the data (boundary knots)\nDegrees of freedom (dof) = d + K\nbs(x) will create a cubic B-spline basis with two boundary knots and one interior knot placed at the median of the observed data values\n\nBounded by the range of the data\nlm(y ~ bs(x))\n\nExample: bs(x, degree=2, knots=c(0,.5,1))\n\ndegree specifies d\nknots specifies the number of knots and their locations\n\nExample: bs(x, knots = median(x))\n\n1 interior knot created at the median\n4 dof since d + K = 3 + 1\n\nd = 3 (default)\n\n\nExample: bs(x, knots = c(min(x), median(x), max(x)))\n\n1 interior knot specified at the median and 2 boundary knots at the min and max.\n6 dof since d + K = 3 + 3\n\nd = 3 (default)\n\n\n\nNatural Cubic and Cardinal Splines\n\n\nStable at boundaries of data because of additional constraints that they are linear in the tails of the boundary knots\nDegrees of freedom (dof) = K + 1\nns(x) returns a straight line within the boundary knots\n\nlm(y ~ ns(x))\n\nExample: ns(x,df=3)\n\n‚Äúdf‚Äù specifies degrees of freedom\n‚Äúknots‚Äù: alternatively to specifying df, you can specify the knots (# and positions) like in bs\n\nCardinal splines\n\nHave an additional constraint that leads to the interpretation that each coefficient \\(\\beta_k\\) is equal to the value of the spline function at the knot \\(\\tau_k\\)",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-smth",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-smth",
    "title": "Splines",
    "section": "Smoothing Splines (aka Penalized Splines)",
    "text": "Smoothing Splines (aka Penalized Splines)\n\nAutomatically handles the number of knots and knot positions by using a large number of knots and letting Œª control the amount of smoothness\n\nDifferent packages usually produce similar results. Penalties are very powerful in controlling the fit, given that enough knots are supplied into the function\n\nRequires modification of the fitting routine in order to accommodate it\n\nProbably need a GAM package to use.\n\nA special case of the more general class of thin plate splines\nFunction\n\\[\n\\hat{\\beta} = \\arg\\max_{\\beta} [l_\\beta (x_1, y_1, \\ldots, x_n, y_n) - \\lambda J_\\beta]\n\\]\n\nThe maximization of this function implies a trade-off between smoothness and model fit that is controlled by the tuning parameter Œª\nTerms\n\nlŒ≤ is the likelihood\nJŒ≤ (penalty function) is the roughness penalty (expresses the smoothness of the spline function)\n\nFor a gaussian regression this is the integrated second derivative of the spline function (see paper for more details)\n\nExample:\n\\[\n||y-f||^2 + \\lambda \\int \\left(\\frac {\\partial^2 f(\\text{log[baseline profit]})}{\\partial \\; \\text{log[baseline profit]}^2}\\right)^2 \\partial x\n\\]\n\n\nŒª is a tuning parameter that‚Äôs ‚â•0\n\n\nB-Spline basis is typically used\nNot easy to specify the degrees of freedom, since they will vary depending on the size of the penalty\n\nUsually can be restricted to a maximum number of degrees of freedom or desired degrees of freedom\n\nPenalized Regression Splines\n\nApproximation of a smoothing spline\nBest used when n is large and the variable range is covered densely by the observed data\nP-Spline\n\nBased on the cubic B-spline basis and on a ‚Äòlarge‚Äô set of equidistant knots (usually, 10‚Äì40)\nSimplifies the calculation of JŒ≤ (see paper for more details)\nPackages: {mgcv}, {gamlss} (See above, Misc &gt;&gt; Package Comparison)",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-inter",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-inter",
    "title": "Splines",
    "section": "Interactions",
    "text": "Interactions\n\nNumeric spline varying by indicator\ns(log_profit_rug_business_b, by = treatment)\n\nCoefficient is a conditional average treatment effect (CATE)\nCreates the main effect and the interaction",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/job-reports.html",
    "href": "qmd/job-reports.html",
    "title": "Reports",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-misc",
    "href": "qmd/job-reports.html#sec-job-reports-misc",
    "title": "Reports",
    "section": "",
    "text": "Packages\n\n{narrator} - Creates a text summarization of descriptive statistics. The outputted text can be enhanced with ChatGPT. Available in both R and Python.\n\nChicago manual of style for citations\nReread after 3 days to make sure it makes sense before publishing\n\nDancho does his labs on Wed afternoons, so maybe that‚Äôs a good time to release articles.\n\nPrint out article and highlight topic sentences\n\nDoes each topic sentence describe the paragraph. Do all the other sentences in the paragraph support the topic sentence.\nDo the topic sentences produce a good outline about the subject you wanted to discuss. Do they follow a logical data storytelling sequence.\n\nPrimary interests of business people: business question, budget, whether research is conclusive or not conclusive, and value the research or product provides.\nKeep color schemes for categoricals, metrics, etc.\n\nIf you used a color palette for male/female in an earlier section/slide, keep that same palette throughout.\n\nKeep date and other variable formats consistent throughout\nNo more than 3 dimensions on a chart\nPick the chart, graph or table that best fits with the paragraph and move on to the next point. Don‚Äôt use multiple charts that show the same thing.\nNever introduce something into the conclusion that was not analyzed or discussed earlier in the report.\nDo not include more information than is necessary to support you report objectives\nPhrases for communicating uncertainty\n\n‚ÄôHere‚Äôs something we expect to see a lot,‚Äù\n‚ÄúHere‚Äôs something we expect to see sometimes‚Äù\n‚ÄúHere‚Äôs something that could happen on rare occasions, but which is worth considering because of the high stakes.‚Äù",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-concepts",
    "href": "qmd/job-reports.html#sec-job-reports-concepts",
    "title": "Reports",
    "section": "Concepts",
    "text": "Concepts\n\nWhat is the problem we are solving\n\ne.g.¬†Why are we losing so many customers?\n\nUnderstand the kind of story you want to tell -\n\nA one-time story: What caused the last month‚Äôs shortage?\nUpdated, ongoing story: Weekly rise and fall of sales, fraud detection\n\nKnow your audience\n\nWhat knowledge your audience brings to the story. What kind of preconceptions does the audience have.\n\nInclude the critical elements of a traditional story structure\n\nPoint of View: Someone has to ask the question that‚Äôs answered with data.\nEmpathy: Need to have human protagonist who‚Äôs solving the problem.\nAn Antagonist: Confusion or misunderstanding that makes achievement of the solution difficult.\nAn Explicit Narrative: This happened, then this happened, and then‚Ä¶\n\nDevelop the Right Hook\n\nWhat helps grab the attention of the managers? e.g.¬†newspaper lead opening, startling statistics, teaser\n\nA picture is priceless\n\nPeople like visuals but good ones are really difficult to create\n\nWhat‚Äôs your point?\n\nResolve and close what does your story advise to do? e.g A Call to Action\n\nIterate\n\nSome stories need to be retold continuously when new data arrives, good stories live on",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-genguid",
    "href": "qmd/job-reports.html#sec-job-reports-genguid",
    "title": "Reports",
    "section": "General Guidelines",
    "text": "General Guidelines\n\nKnow your audience Don‚Äôt use technical terms when talking to non-technical people.\n\nFast-track the conversation to the technical stuff when talking to fellow data scientists.\nThe more senior the person you‚Äôre talking to, the more to the point your message has to be.\nSmall talk with long-term clients is always essential to maintain a strong relationship.\nThe CEO only wants to know the result of your analysis and what it means for their company.\n\nSimplified\n\nIntent: This is your overall goal for the project, the reason you are doing the analysis and should signal how you expect the analysis to contribute to decision-making.\nObjectives: The objectives are the specific steps you are taking or have taken to achieve your above goal. These should likely form the report‚Äôs table of contents.\nImpact: The result of your analysis has to be usable ‚Äî it must offer solutions to the problems that led to the analysis, to impact business actions. Any information which won‚Äôt trigger action, no matter how small, is useless.\n\nWhat is the¬†business question?\nWhy is it important?\n\nDoes your model enable us to better select our target audience?\nHow much better are we using your model?\nWhat will the expected response on our campaign be?\nWhat is the financial impact of using your model?\n\nWhat is the data science question?\nWhat is the data science answer?\nWhat is the business answer?\nShow a general form of the equation, definition of terms, before explaining how to fill it with values of particular to your problem\nHow would you recommend your model/results be used?\nBe direct. Communicate your thoughts in a forthright manner, otherwise the reader may begin to tune out.\nStart with an outline\n\nState your objective\nList out your main points\nNumber and underline your main points to guide the reader\nEnd with a summary.\n\nOpen with short paragraphs and short sentences\nUse short words. The goal is to reduce friction.\n\nUse adjectives and adverbs for precision, not exclamation points.\n\nCut lazy words like very, great, awfully, and basically. These do nothing for you.\n\nUse down-to-earth language and avoid jargon. Like explaining to a 6th grader\nDon‚Äôt use generalities (e.g.¬†‚ÄúOur campaign was a great success and we came in under budget‚Äù).\n\nBe specific (e.g.¬†‚ÄúWe increased click-through rates by 21% while spending 19% less than expected.‚Äù)\n\nTake the time to build down what you have to say. Then, express it confidently in simple, declarative sentences.\n\nEspecially in memos and emails, put your declaration in the subject line or as the first line",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-narr",
    "href": "qmd/job-reports.html#sec-job-reports-narr",
    "title": "Reports",
    "section": "Narrative Structures",
    "text": "Narrative Structures\n\nDeveloping a narrative when presenting results is imperative in order for recommendations to gain traction with stakeholders\n\nBudget at least 50% of time in the project plan for insight generation, and structuring a narrative (seems a bit large)\nWith each iteration (potentially dozens) of improving your presentation, you are looking to address any insight gaps, and improve the effectiveness in conveying the insight and recommendations\nAnticipate potential follow up questions they might ask and preemptively address them\nEliminate any distractions to the key message such as ambiguous statements, or erroneous facts that can derail the presentation\nIf possible find someone with tenure in the organization, or has expertise in the business area you are analyzing to lend a critical eye to your presentation.\n\nAlso may provide insight on how best to win the trust of key decision makers and potential areas that can derail the effort\n\n\nExample 1\n\nExecutive Summary\n\nBrief Description of Problem\nApproach Taken\nModels Used\nResults\nConclusion\nRecommendations\n\nDescribe the status quo\n\nMaybe describe what each proceeding section will entail\n\nWhat‚Äôsthe problem that needs fixing or improved upon\nProposed solution\nIssues that arose during process, maybe a new path discovered not previously thought of\nSolution\n\nDescription of data\n\nRecommendations or next steps\n\nThe stakeholder must understand the expected outcome, and the levers that need to be pulled to achieve that outcome.\nAn effective analysis owner will take on the responsibility for the stakeholder‚Äôs understanding, through communicating both specific predictions and the supporting evidence in a consumable way.\n\n\nExample 2\n\nExecutive Summary\n\nBrief Description of problem\nApproach Taken\nModels Used\nResults\nConclusion\nRecommendations\n\nIntroduction\n\nQuestion\nBackground\nWhy Important\nDescribe Structure of the Report\n\nMaybe a Table of Contents\n\n\nMethodology (EDA and Models)\n\nDescribe the data you are using\nThe types of analyses you have conducted & why\n\nResults\n\nMain body of the report split into sections according to the various business questions the report attempts to answer\nThe results generated for each question.\n\nDiscussion\n\nBring together patterns seen in EDA, model interpretations\nCompare with your prior beliefs and/or other papers results\nObjective recommendations for business actions to be taken\n\nConclusion/Summary\n\nRestate Question\nSteps Taken\nAnswers to Auestions\nIssues Faced\nNext Steps",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-lay",
    "href": "qmd/job-reports.html#sec-job-reports-lay",
    "title": "Reports",
    "section": "Layouts",
    "text": "Layouts\n\nNotes from: How I create an Analyst Style Guide\nMost important details (i.e.¬†the conclusion) always come first\n\ne.g.¬†Executive summaries at the beginning of reports; Conclusions/useful sentences for titles of sections and slide titles\nThe goal is to reduce the time required by the reader to understand what you‚Äôre trying to tell them. If they want further details, they can read on further.\n\nUse consistent layouts so your audience can get used to where different types of information will be located\n\nExample: Driver layout\n\nPlot the trend of the Goal KPI on the left side with a text description in the same box.\nUse the larger space on the right side to plot the trends of the Driver KPIs that can explain the development of the Goal KPI\n\n\nThe Goal KPI is Sales Revenue and the Driver KPIs are Leads (#), Conversion Rate (%) and Order Value (EUR)\n\n\nExample: Contrast layout\n\nUseful to highlight the difference in two or more KPIs given the same segmentation\nDivide the space equally depending on the number of the metrics I want to compare with.\n\n\nThe contrast is between the metrics\nThe segmentation is gender and age groups\nTakeaway: Females generate most revenue and cost the least to obtain",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-eym",
    "href": "qmd/job-reports.html#sec-job-reports-eym",
    "title": "Reports",
    "section": "Explaining Your Model",
    "text": "Explaining Your Model\n\nMisc\n\nFor ML models use feature importance to pick predictors to use for partial dependence plots (with standardized predictors, these can also advise on feature importance) and go back to do descriptive/aggregated statistical explorations (box plots, bars, etc.). Explain what‚Äôs happening in the plots, potential reasons why it‚Äôs happening, and potential solutions.\n\nTypes\n\nWhen talking to a colleague or regulator you may need to give more technical explanations. In comparison, customers would expect simpler explanations. It is also unlikely that you would need to give a global explanation to a customer. This is because they would typically only be concerned with decisions that affect them personally.\nGlobal: Explain what trends are being captured by the model in general\n\n‚ÄúWhich features are the most important?‚Äù or ‚ÄúWhat relationship does feature X have with the target variable?‚Äù\n\nLocal: explain individual model predictions\n\nTypically needed to explain a decision that has resulted from a model prediction\n‚ÄúWhy did we reject this loan application?‚Äù or ‚ÄúWhy was I given this movie recommendation?‚Äù\n\n\nCharacteristics\n\n\nTrue: Include uncertainty in your explanations of your model predictions\nCorrect level: Use the language of your audience instead of DS or statistical terminology\nNo.¬†of Reasons & Significant: Only give the top features that are responsible for a prediction or trend, and those features should be responsible for a substantial contribution\nGeneral: Explain features that are important to large portion of predictions (e.g.¬†feature importance, mean SHAP)\nAbnormal: Explain features that are important to extreme predictions or a representative prediction\n\nMight be a feature that isn‚Äôt globally important but important for an individual prediction or an outlier prediction\n\nContrasting: Explain contrasting decisions made by your model\n\n‚ÄúWhy was my application rejected and theirs accepted?‚Äù\nUse important features (ranges/levels of those features) that aren‚Äôt common to both decisions",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-bizpres",
    "href": "qmd/job-reports.html#sec-job-reports-bizpres",
    "title": "Reports",
    "section": "Business Presentation",
    "text": "Business Presentation\n\nThey‚Äôre only interested in the story the data tells and the actions it influences\nPrep\n\nCreate an outline\nSituation-Complication-Resolution Framework\n\nSituation: Facts about the current state.\nComplication: Action is required based on the situation.\nResolution: The action is taken or recommended to solve the complication.\nExample\n\n\nOne minute per slide rule\n\nIf you have a 20-minute presentation, aim for 20 slides with content\n\nTry to stick to 3 bullet points\n\nOr if you need to include more information, structure the slide with some sort of ‚Äú3‚Äù framework\n\nExample: 3 columns\n\n\nEach column has 3 bullets\n\n\n\nFocus audience attention to important words\n\nBold, italics, a different color, or size for words you want to emphasize\n\n\nUse emotional elements as hooks to grab attention before starting the introduction. They generate these emotions but also curiosity about what comes next.\n\nGreed - ‚Äúthis has the potential to double revenue‚Äù\nFear - ‚Äúlayoffs may be coming‚Äù\nPride - ‚Äúwe can do this!‚Äù\nAnger - ‚ÄúIt‚Äôs the competition‚Äôs fault!‚Äù\nSympathy - ‚Äúthey‚Äôre counting on us to help‚Äù\nSurprise - ‚Äúyou won‚Äôt believe what we found‚Äù\n\nUse meaningful sentences as slide titles.\n\nExamples\n\nInstead of ‚ÄúSales outlook‚Äù, use ‚ÄúSales outlook is promising in the next 12 months‚Äù.\nInstead of ‚ÄúAnnual Sales‚Äù, use ‚ÄúSales Up 22% In 2022‚Äù\nInstead of ‚ÄúAlgorithm Training and Validation‚Äù use ‚ÄúPredict Customer Churn with 92% Accuracy‚Äù\nInstead of ‚ÄúQ1 Conversation Rates‚Äù use ‚ÄúAccounts With Direct Contact are 5x More Likely to Purchase‚Äù\nInstead of ‚ÄúUtilizing XGBoost to Classify Accounts‚Äù use ‚ÄúMachine Learning Improves Close Rates by 22%‚Äù\n\n\nRead (only) slide titles aloud\n\nBy reading just the tile and title only as you start each slide, the audience will be able to process the message much more easily than reading the written words and listening to you simultaneously.\nFor the rest of the slide, do not read the content, especially if you use a lot of bulleted or ordered lists. Reading all of your content can be monotonous\n\nIntroduction:\n\nProblem: ‚Äúflat 4th quarter sales‚Äù and maybe a why? it happened\nGoal: ‚Äúrestore previous year‚Äôs growth‚Äù\nDescribe the presentation to come: ‚ÄúBy analyzing blah blah, we can forecast blah, blah‚Äù and maybe a teaser on how it will be solved.\nDesired outcome: ‚ÄúOur goal here today is to leave with a budget, schedule, and brainstorm some potential advertising approaches that might be more successful‚Äù\nIf analysis is negative, it‚Äôs important to frame the story or somebody else will. Could become an investigation or witchhunt. Include something about the way the forward, so keep the focus positive and about teamwork.\nInclude disclaimers/assumptions but only those that directly pertain to the specific subject matter of the presentation\nLayout Q&A ground rules (questions only after or also during the presentation?)\n\nBody\n\nInterpret all visuals. Don‚Äôt let the audience reach their own conclusions.\nBullets\n\nShould only cover key concepts so don‚Äôt read\nYour narration should add more\n\nMore context\nMore interpretation\nMore content\nMore feeling\n\n\nPresentation Pattern: Present visual \\(\\rightarrow\\) interpret visual\n\nStart with a visual that illustrates the problem \\(\\rightarrow\\) discuss problem \\(\\rightarrow\\) present hypothesis that explains a cause of the problem\nPresent visual that is evidence for your hypothesis \\(\\rightarrow\\) interpret visual\n\nRepeat\nVisuals act as a chain of evidence\n\nProvide recommendation for a course of action \\(\\rightarrow\\) present visuals or data that support this action\n\ne.g.¬†Historical results from previous instances of taking this action\n\nHow this situation mimics the successful instances\n\nForecasts that support the recommendation\n\nTalk about the uncertainty, consequences of lower and upper bounds\n\nSurvey Data\n\nInvite questions and comments about the data and visuals you shown if you have no recommendations or courses of action\n\nTake notes (yourself or assistent)\nIf you don‚Äôt have an answer:\n\n‚ÄúI don‚Äôt have an answer for that offhand but I‚Äôll get back to you after we look into that.‚Äù\n‚ÄúI don‚Äôt have the answer to that. I can reanalyze the data and see if they support that idea.‚Äù\n\n\n\n\nSatisfying Conclusion\n\nSummarize (especially if a lot was covered)\n\nConsiderations\n\nWhat does your audience care about?\nWhat are the implications of your results?\n\nHow does these results affect the business or solve the problem or clarify the problem, etc.?\n\nWhich insights from your analysis will have the biggest impact?\n\n\nIf you asked for questions or comments above, summarize them and any conclusions from the discussion, which ones require further study, etc.\nIf you provided recommendations, review them and include the rationale for them ideally tied to the data, and the expected results of such actions\n\nExample: ‚ÄúThe price reduction on  has resulted in a strong rebound in sales figures that analysis shows will increase further with additional marketing support. We recommend increasing the advertising budget for this line by 25% next quarter and would like the art department to take on design of a new campaign as their immediate action item.‚Äù\n\nDefine success metrics and what values would require a rethink of the strategy.\nDefine a timeframe\n\n‚ÄúIt is our hope that the additional 25% marketing investment in the  will result in Q4 revenue that is 50% over last year‚Äôs Q4 revenue for that line. We will review the results next January and meet again to discuss them and determine any changes in course going forward.‚Äù\n\nPotentially include consequences of not following recommendations\n\n‚Äú‚Ä¶ it is unlikely sales will recover and we‚Äôll continue to lose market share.‚Äù\n\nIf anyone made any commitments to other actions, note those.\nBring back emotional hook that you used in the intro\n\n‚Äù our analysis shows that blah, blah will justify the further investment and eliminate the need for layoffs.‚Äù\n‚Äú‚Ä¶ should lead to a return to robust sales and profitability, along with stronger profit sharing.\nIf you used greed, conclude with how rewarding the action will be\nif you used fear, end with how the action will alleviate that fear\n\n\nQ&A\n\nPlant questions with collegues about info you wanted to include but the topic didn‚Äôt fit into the presentation\nPrepare for likely questions will have tables or other slides that answer those questions\nDisagreements or questions you don‚Äôt have an answer to:\n\nDON‚ÄôT BE DISMISSIVE\n\nDon‚Äôt respone with any variant of, ‚Äúyou don‚Äôt trust data?‚Äù or blaming difficulties on someone‚Äôs lack of ‚Äúdata literacy.‚Äù\nWith so many potential sources of error or misunderstanding, it seems sensible for the data scientist to listen to concerns.\nClient questions provide an important counterweight against over-trust in data products.\n\nGive non-defensive responses\n\nA non-defensive response is helpful when you‚Äôre wrong, but pure gold when you are right (and both things will happen from time to time). If you are right, but are argumentative or dismissive, the client is likely to be upset. if you take a client‚Äôs concerns seriously and are thoughtful about addressing the situation, then turn out to be correct on top of that, you‚Äôre likely to make a very positive impression.\nPhrases\n\nThat‚Äôs a great question. We need to collect more data before we‚Äôll be able to answer that.\nThank you for bringing that to my attention\nI need to think about that\nI‚Äôm not prepared to give that the consideration it deserves, but can we make an appointment to discuss it later?\nI hadn‚Äôt thought of it that way\nAnything is possible\n\n\nAnswer a question with a question (to clarify)\n\nA great many disagreements arise due to mismatched interpretation of goals and definitions. It‚Äôs important to fully understand the nature of the concern.\nReports sometimes are outdated or refer to a different product, department, etc.\nThey can speed up finding the root cause of your own error.\n\nUse email\n\nFollowing-up emails summarizing an issue, outlining plans, and suggesting timelines for investigations, are nearly always appreciated\n\nBe careful about taking lifelines from the audience\n\nDuring a disagreement, a helpful bystander will often offer a suggestion. Their ideas are usually generous, imagining a way that the data scientist might be correct. It might be tempting to agree, but be careful! Thoughtlessly taking a lifeline is a fast way to lose credibility.\nPhrases\n\n‚ÄúThat‚Äôs a possibility, John, thanks for the suggestion!‚Äù\n‚ÄúGreat idea, Sally, but I need more time to look at the data to be sure!‚Äù\n\n\n\nDon‚Äôt let anyone hijack q&a and turn it into a one and one conversations. Cut off or defer answering a follow up question.\n\n‚ÄúThanks for your great question, but we do need to let others ask their questions. Please follow-up with me afterwards.‚Äù\n\nThank everyone for attending and leave the front of the room.\n\nFollow-up\n\nKeep Promises\n\nAnswer questions to promised to look into\nPost slide deck if you said you would\nSchedule and attend a meeting if you said you would\n\nSend summary email to participants if any actions resulted from the meeting\nSet up monitoring of success metrics. Someone could want an interim report before the settled upon timeframe has been reached.",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-instrart",
    "href": "qmd/job-reports.html#sec-job-reports-instrart",
    "title": "Reports",
    "section": "Instructional Articles",
    "text": "Instructional Articles\n\nWhat?\n\nGiven a short description of the subject matter\n\nWhy?\n\nWhy is the subject matter important\nWhy is the subject matter useful\nWhy do it this way and not another\nState what each section will entail.\n\nBackground\n\nSome history\nContext surrounding the problem\nBusiness and Data Science interpretations of the problem or subject matter\n\nExample\n\nFramework\n\nDescribe the variables\nDescribe the model\nPotential issues/assumptions with approach\n\nAnalysis\nResults\n\nConclusion",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-domspec",
    "href": "qmd/job-reports.html#sec-job-reports-domspec",
    "title": "Reports",
    "section": "Domain Specific",
    "text": "Domain Specific\n\nTime Series\n\nNotes from Why Should I Trust Your Forecasts?\nIn Goodwin et al.¬†(paper yet to be published, July 2021), people trusted forecasts more when they were presented as ‚Äúbest case‚Äù and ‚Äúworst case‚Äù values rather than as ‚Äúbounds of a 90% prediction interval.‚Äù\n\nWtf is ‚Äúworst case‚Äù? Outside an 80% CI? If so that has a 20% chance of happening.\n\nIn some situations, managers who are not mathematically inclined may be suspicious of forecasts presented using technical terminology and obscure statistical notation (Taylor and Thomas, 1982).\n\nSuch a manager may respect the forecast provider‚Äôs quantitative skills, but simultaneously perceive that the provider has no understanding of managers‚Äô forecasting needs ‚Äì hence the manager distrusts the provider‚Äôs forecasts\n\nI don‚Äôt understand this one either. What could possibly be the different ‚Äúforecasting need‚Äù that the manager needs?\n\n\nExplanations (i.e.¬†justifications, rationale, etc.) of the forecast can improve people‚Äôs perceptions of a forecast. The higher the perceived value of the explanations, the higher the level of acceptance of the forecast. (G√∂n√ºl et al, 2006)\n\nPeople enjoy the ‚Äústories‚Äù and it makes the forecasts more believable.\n\nProvide cues for how to evaluate the forecast in the report\nProvide accuracy metrics in relation to a reasonable benchmark\n\nExample: rolling average, naive, average for these days over the previous 5 yrs, whatever the current method is, etc.\nIn very unpredictable situations, this will help to show that relatively high forecast errors are unavoidable and not a result of the forecaster‚Äôs lack of competence.\n\nBeing transparent about assumptions, and even presenting multiple forecasts based on different assumptions, will most likely reassure the user about the integrity of the provider.",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html",
    "href": "qmd/visualization-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-misc",
    "href": "qmd/visualization-general.html#sec-vis-gen-misc",
    "title": "General",
    "section": "",
    "text": "Notes from\n\nFriends Don‚Äôt Let Friends\n\nMicrosoft Paint 3D\n\nLocation: Start &gt;&gt; All Programs &gt;&gt; Paint 3D\nHightlight Text\n\nClick 2D Shapes (navbar) &gt;&gt; Select square (side panel)\nLeft click and hold &gt;&gt; Extend area around text you want to highlight &gt;&gt; Release\nChoose Line Type color and Sticker Opacity level (37%)\nOn area surrrounding text\n\nIf needed, make area size adjustment dragging little box-shaped icons that are along the outside\nOn the right side, click the check mark icon to finalize\n\nClick Menu (left-side on navbar) &gt;&gt; save as &gt;&gt; Image\n\nIt adds a png extension, but you just need to type the name.\n\n\n\nAlt Text\n\nThe guiding principle is to write alt text that gives disabled readers as close to the same experience as nondisabled readers as possible.\n\nggplot2\n\nDon‚Äôt use stat calculating geoms and set axis limits with scale_y_continuous\n\n\nSee examples of the behavior in this thread\n\nDefaults for any {ggplot2} geom using the default_aes field (i.e.¬†GeomBlah$default_aes )\n\nFractional Data\n\nUse Stacked Bars instead of Pie or Circular or Donut\n\nHumans are better at judging lengths than angles (article)\n\n\nFactorial Experiments\n\nDon‚Äôt use bars factorial experiments\n\nCheck outcome ranges by group when facetting",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-concepts",
    "href": "qmd/visualization-general.html#sec-vis-gen-concepts",
    "title": "General",
    "section": "Concepts",
    "text": "Concepts\n\nExploration and Analysis\n\nGoal: explore a new dataset, gertan overview, find answers to specific questions\nFast iteration of many generic charts, don‚Äôt customize or worry about color schemes, etc.\n\nExplanation\n\nGoal: help others understand a relationship in the data\nUse as few charts as possible, carefully chosen\nSequence so that they are easy to understand\nAdd interaction to help people get a better understanding\n\nPresentation\n\nGoal: walk your audience through an argument, help them come to a decision\nFocus on polishing charts: colors, legends, titles, etc.\nHighlighting of key elements (which might be considered biasing in Exploration)\nPossibly use of unusual charts for memorability\nSequence to make a specific point",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-svg",
    "href": "qmd/visualization-general.html#sec-vis-gen-svg",
    "title": "General",
    "section": "SVG",
    "text": "SVG\n\nBetter for doing post-processing in Inkscape and gimp\nSVGs won‚Äôt be pixelated when you zoom in like PNGs are\nD3 outputs SVG\nsvglite PKG\n\nusing svglite instead of base::svg( ) allows you alter text in Inkscape or Illustrator\nrequires the used fonts to be present on the system it is viewed on.\n\nThe vast majority of interactive data visualizations on the web are now based on D3.js which often renders to SVG and it all seems to behave. Still, this is something to be mindful of, and a reason to use svg() if exactness of the rendered text is of prime importance\n\nFile size will be dramatically smaller",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-layout",
    "href": "qmd/visualization-general.html#sec-vis-gen-layout",
    "title": "General",
    "section": "Layout",
    "text": "Layout\n\nFacetting vs Single Graph\n\nLayout based on experiement design\n\nAlign title ALL the way to the left (ggplot: plot.title.position = ‚Äúplot‚Äù)\nremove legends\n\nuse colored text in title (ggtext)\nlabel points or lines\nlast resort: place legend underneath title/subtitle\n\ngrid lines\n\nremove if possible\nsparse and faint if needed\n\naxis labels\n\nremove if obvious (e.g brands of cars)\ncreate a title that informs about the axis labels\nshould always be horizontal\n\nflip axis, don‚Äôt angle them 45 degrees\n\n\ntext\n\nleft-align most text\ncan center a subtitle if it helps with making the graph more symmetrical\nsome labels can be right-aligned\n\nRemove all borders\nMaximize white space\n\ndon‚Äôt cram visuals together\n\nWorking memory. A cognitive limitation that affects plot comprehension is the limit on working memory. Typically, working memory is limited to approximately seven (plus or minus two) items, or chunks. In practice, this means that categorical scales with more than seven categories decrease readability, increase comprehension time, and require significant attentional resources, because it is not possible to hold the legend mapping in working memory.\nThe use of redundant aesthetics that activate the same gestalt principles (such as color and shape in a scatter plot, which both activate similarity) results in higher identification of corresponding data features. In addition, dual encoding increases the accessibility of a chart to individuals who have impaired color vision or perceptual processing (e.g., dyslexia, dysgraphia). This experimental evidence directly contradicts the guidelines popularized by Tufte (1991), which suggest the elimination of any feature that is not dedicated to representing the core data, including redundant encoding and other unnecessary graphical elements.\nggplot themes\n\nCedric Sherer (article)\ntheme_set(theme_minimal(base_size = 15, base_family = \"Anybody\"))\ntheme_update(\n  axis.title.x = element_text(margin = margin(12, 0, 0, 0), color = \"grey30\"),\n  axis.title.y = element_text(margin = margin(0, 12, 0, 0), color = \"grey30\"),\n  panel.grid.minor = element_blank(),\n  panel.border = element_rect(color = \"grey45\", fill = NA, linewidth = 1.5),\n  panel.spacing = unit(.9, \"lines\"),\n  strip.text = element_text(size = rel(1)),\n  plot.title = element_text(size = rel(1.4), face = \"bold\", hjust = .5),\n  plot.title.position = \"plot\"\n)",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-ar",
    "href": "qmd/visualization-general.html#sec-vis-gen-ar",
    "title": "General",
    "section": "Aspect Ratio",
    "text": "Aspect Ratio\n\nMisc\n\nGolden Rectangle\n```{{{r, fig.width = 6, fig.asp = 1.618}}}\n```\nGet consistent outputs\n\nRStudio pane displays in 72dpi which can mislead you on what your output looks like.\nThink {ragg} is supposed to have taken care of the inconsistency in terms of printing on different OSes\nUsing {camcorder}\n\nStart ‚Äúrecording‚Äù plots\ncamcorder::gg_record(\n  dir = \"imgs\",\n  width = 12,\n  height = 12*9/16,\n  dpi = 300,\n  bg = \"white\"  # Makes sure background is actually white an not transparent\n)\n\nAll plots will immediately be exported as a .png-file to the directory specified\nAll plots will be displayed in the viewer with dimensions and resolution that you specified and not in the plots pane in RStudio\n300 dpi is pretty standard and default of ggsave\n\nDo work. Export final png file in directory when done and delete the rest\nRegarding Fonts\n\nIf using {ragg}, then all is fine.\nIf using {showtext}, then you have to set resolution in options, showtext_opts(dpi = 300)\n\n\n\n\nTwitter\n\nVideo: 1105 x 1920\n\nLine Charts\n\nMatters most if two different line charts are being compared\n\nThe core idea of ‚Äúbanking‚Äù is that the slopes in a line chart are most readable if they average to 45¬∞.\nUse ggthemes::bank_slopes(x, y, method = c(\"ms\", \"as\"))\n\n2 methods (that req. no optimization) from Jeer, Maneesh who followed Cleveland‚Äôs 45¬∞ guideline\ndocs\n\n‚ÄúThe problem with banking is that sometimes you need the chart in a certain aspect ratio to fit into a page layout. Especially if banking produces portrait sized charts. But why not let the optimal chart ratio define your layout? For instance, you can put the additional information to the side of the chart. Remember that the main goal of banking is to increase the readability of the line slopes. In the following example, the slopes for Nuclear and Renewables would have been much more difficult to see, if the chart would have been ‚Äòsqueezed‚Äô to a landscape aspect.‚Äù (article)",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-typo",
    "href": "qmd/visualization-general.html#sec-vis-gen-typo",
    "title": "General",
    "section": "Typography",
    "text": "Typography\n\nCSS Length Units\n\nAbsolute Lengths\n\n* Pixels (px) are relative to the viewing device. For low-dpi devices, 1px is one device pixel (dot) of the display. For printers and high resolution screens 1px implies multiple device pixels.\n\n\ncm\ncentimeters\n\n\nmm\nmillimeters\n\n\nin\ninches (1in = 96px = 2.54cm)\n\n\npx*\npixels (1px = 1/96th of 1in)\n\n\npt\npoints (1pt = 1/72 of 1in)\n\n\npc\npicas (1pc = 12 pt)\n\n\n\nRelative Lengths\n\nThe em and rem units are practical in creating perfectly scalable layout! * Viewport = the browser window size. If the viewport is 50cm wide, 1vw = 0.5cm.\n\n\n\n\n\n\nem\nRelative to the font-size of the element (2em means 2 times the size of the current font)\n\n\nex\nRelative to the x-height of the current font (rarely used)\n\n\nch\nRelative to the width of the ‚Äú0‚Äù (zero)\n\n\nrem\nRelative to font-size of the root element\n\n\nvw\nRelative to 1% of the width of the viewport*\n\n\nvh\nRelative to 1% of the height of the viewport*\n\n\nvmin\nRelative to 1% of viewport‚Äôs* smaller dimension\n\n\nvmax\nRelative to 1% of viewport‚Äôs* larger dimension\n\n\n%\nRelative to the parent element\n\n\n\n\nCSS formula to make font size responsive to screen size (article)\n:root {\n  font-size: calc(1rem + 0.25vw);\n}\nFont Weight\n\n400 is the same as normal, and 700 is the same as bold\n\nFonts\n\nAdelle\n\nA serif font that doesn‚Äôt go overboard. Good for short paragraphs.\n\nAlegreya\nBarlow\n\nSlender font\n\nFira Code Retina\n\ncode syntax highlighting\n@import url(‚Äúhttps://cdn.rawgit.com/tonsky/FiraCode/1.205/distr/fira_code.css‚Äù);\n\nLora\n\nbody\nUsed in COVID-19 project &gt;&gt; Static Charts, Hospitals\n@import url(‚Äòhttps://fonts.googleapis.com/css2?family=Lora&display=swap‚Äô);\n\nMerriweather\n\nSimilar to Adelle, but has a bit more pronounced hooks\n\nMontserrat\n\nSimple design that can handle long lines of text. I like it for minimal plots.\n\nPrata\n\nheader\nUsed in ericbook-distill\n@import url(‚Äòhttps://fonts.googleapis.com/css2?family=Cinzel&display=swap‚Äô);\n\nReforma family\n\nonly one I have is Roboto, need to import and load the rest using extrafont pkg\n\nRoboto family\n\nDancho shiny apps\n\np, body: 100 wt\nHeaders, (h1, h2, etc.): 400 wt\n\nRoboto Slab\n\nNot sure if this is exact font used but it‚Äôs very similar. Only difference I spotted was the ‚Äú3.‚Äù\n\n\nTitillium Web Bold\n\nheaders\nUsed in ebtools\n@import url(‚Äòhttps://fonts.googleapis.com/css?family=Titillium+Web&display=swap‚Äô);\n\n\nNumbers\n\nshould all have the same height (Lining)\nshould all have the same width (Tabular)\n\nUsing {showtext}\nlibrary(showtext)\n#load font\nfont_add_google(name = \"Metal Mania\", family = \"metal\")\nfont_add_google(name = \"Montserrat\", family = \"montserrat\")\nshowtext_auto()",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-annot",
    "href": "qmd/visualization-general.html#sec-vis-gen-annot",
    "title": "General",
    "section": "Annotation",
    "text": "Annotation\n\nPeople love annotations (thread, paper). More text, the better.\n\nTheir takeaway from the chart is more likely to resemble the annotation if it takes the form of L2 and/or L4 and is close to the data\n\nExample: Financial Times\n\n\nTitle (L2) is used for part of the takaway message\n\nSubtitle used to describe the Y-Axis\n\nChart annotation paragraph (L4) gives contextual information\n\n\nWhen to annotate\n\na design element in your visualization that needs explaining\na data point or series that you want readers to see, like an outlier\nreaders should know something to better understand why certain data points look the way they do\n\nRemove the color key/legend and directly label your categories\n\nIf the screen is small (e.g.¬†mobile), then it‚Äôs better to keep the legend\n\nMake it obvious which units your data uses.\n\nDon‚Äôt just put units in the description, but also in axis labels, tooltips, and annotations\n\nFor large numbers (e.g.¬†20 million), try to use B, M, K instead of an annotation somewhere that says something like ‚Äúin thousand‚Äù\nTooltips\n\nConsider not just stating the numbers in tooltips, but also the category\n\ne.g.¬†‚Äú3.4% unemployed‚Äù instead of ‚Äú3.4%,‚Äù or ‚Äú+16% revenue‚Äù instead of ‚Äú+16%‚Äù\n\nUse a transparent background by setting the alpha channel of CSS background-colorto a number less than 1\n\ne.g.¬†0.3 using rgba(255, 255, 255, 0.3)\n\nWith a transparent background, text behind the tooltip can interfere with the text in the tooltip, so also apply backdrop-filter\n\nExample:\n.tooltip {\n¬† background-color: rgba(255, 255, 255, 0.3);\n¬† -webkit-backdrop-filter: blur(2px);\n¬† backdrop-filter: blur(2px);\n}\n@media (prefers-contrast: more) {\n¬† .tooltip {\n¬† ¬† background-color: white;\n¬† ¬† -webkit-backdrop-filter: none;\n¬† ¬† backdrop-filter: none;\n¬† }\n}\n\nExample shows a tooltip that has an HTML class of ‚Äútooltip‚Äù.\nblur is measured in pixels and the image size varies with screen width, so the optimal blur size here may vary for you depending on the dimensions of your browser window.\n\nApplies a Gaussian blur to the target element‚Äôs background with the standard deviation specified as the argument (e.g.¬†two pixels).\n\nAs of Mar 2023, doesn‚Äôt work on Safari, so adding -webkit-backdrop-filter allows it to work on Safari\n@media (prefers-contrast: more)checks if your user has informed their operating system or browser that they prefer increased contrast. When they do, this chunk then overrides the applied styles.\n\n\n\nTransparent backgrounds might work better with thematic maps and less with scatter plots\nDon‚Äôt center-align your text\nUse straightforward phrasings\nMove axis labels nearest the most important chart objects (e.g.¬†bars)\n\n\nIf the higher bars are what‚Äôs most important and they‚Äôre on the right, then usea right-side axis\n\nFonts for annotation\n\n\nUse what readers are most used to (e.g.¬†sans-serif regular, &gt;12px, (almost) black text\nIf you need to need a lot of words and they don‚Äôt fit, don‚Äôt use smaller font, use a tooltip instead\n\nOn mobile screens you can also hide the least important annotations, or move them below the visualization\n\n\nLead the eye with font sizes, styles, and colors\n\n\nThe biggest and boldest text with the highest contrast against the background should be reserved for the most important information.\n\nDon‚Äôt overdo it though\n\nUse only two levels of hierarchy that are clearly different from each other ‚Äî like a 12px gray and a 14px black\nEmphasize within the annotations using boldness\n\nKeep labels horizontal\n\n\nUse a text outline\n\n\nSet the stroke around your letters, using the background color of your chart.\n\nBe conversational first and precise later",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-color",
    "href": "qmd/visualization-general.html#sec-vis-gen-color",
    "title": "General",
    "section": "Color",
    "text": "Color\n\nMisc\n\nWhen choosing bg and fg colors, keep in mind that it‚Äôs generally a good idea to pick colors with a similar hue but a large difference in their luminance.\nDatawrapper guide\nWhen using several subplots together to tell a story and they each have their own color scheme. Blend a color into each color scheme to produce a more unified look\n\nExample: Blending blue into a plot with green color scheme.\n\n\nBreakpoints for scales\n\nHow to choose an interpolation for your color scale\n\nCharts (see prismatic PKG to do this manipulation within ggplot)\nPalette composition methods\n\nComplimentary\n\nopposite sides of the color wheel (2 colors)\ncontrast\n\nAnalogous\n\nsame side of the color wheel (multiple)\ngradient\n\nTriadic\n\nforms triangle on the color wheel\nvibrant, contrast\n\nOthers\n\nsplit complimentary (popular)\n\nComprised of one color and two colors symmetrically placed around it. This strategy adds more variety than complementary color schemes by including three hues without being too jarring or bold. Using this method, we end up with combinations that include warm and cool hues that are more easily balanced than the complementary color schemes\n\nquadratic\n\n\nAdjustments once you chosen a color (hue) to create variations\n\nMove brightness up for lighter variations and down for darker variations\nThen, move saturation in the opposite way you moved brightness\n\nSave colors you find attractive\n\ninstant eyedropper (windows)\nThen use HSL (hue, saturation, lightness) slider for adjustments\n\nBackgrounds\n\nWhite\n\nbright, used a lot\ntry ivory or a light gray\nshades of eggshell, link\n\nAvoid black (or REALLY dark) unless situation calls for it\n\ndark is fine\n\n\nLightest and darkest colors should have meaning (e.g.¬†min, max, mean, zero) and not just some arbitrary numbers\n\nWhat to do when you have a lot of categories\n\nSimply don‚Äôt show different colors Does your chart work without colors?\n\n1 color and a discrete axis with the categories\n\nShow shades, not hues Can you make the chart less confetti-like?\n\nAlthough, consider not using shades when the parts are as or more important than the totals\n\nEmphasize Can you only use color for your most important categories?\nLabel directly Can you use the same or similar colors but label them?\nMerge categories Can you put categories together?\nGroup categories, but keep showing them Can strokes help to tell categories apart?\n\nChange the chart type Will another chart type rely less on colors?\n‚ÄúSmall multiply‚Äù it Can you split the categories into multiple charts? (i.e.¬†facet by category)\nAdd other indicators Can you add symbols, patterns, line widths, or dashes?\n\n\nDoesn‚Äôt use any color ‚Äî just opacity, thickness, and dotted lines.\n\nUse tooltips and hover effects Can smaller categories be hidden with them?\n\nColor scales should be chosen to best match the data values and plot type: If the goal is to show magnitude, a univariate color scheme is typically preferable, while a double-ended color scale is typically more effective when showing data that differ in sign and magnitude. Where possible, color scales should use a minimal number of hues, varying intensity or lightness of the color to show magnitude, and transitioning through neutral colors (white, light yellow) when utilizing a gradient. Cognitive load can also be reduced by selecting colors with cultural associations that match the data display, such as the use of blue for men and red (or pink) for women, or the use of blue for cold temperatures and red/orange for warm temperatures.\n\nIt is also important to consider the human perceptual system, which does not perceive hues uniformly: We can distinguish more shades of green than any other hue, and fewer shades of yellow, so green univariate color schemes will provide finer discriminability than other colors because the human perceptual system evolved to work in the natural world, where shades of green are plentiful.\n\n\nFigure above shows the International Commission on Illumination (CIE) 1931 color space, which maps the wavelength of a color to a physiologically based perceptual space; a significant portion of the color space is dedicated to greens and blues, while much smaller regions are dedicated to violet, red, orange, and yellow colors. This unevenness in mapping color is one reason that the multi-hued rainbow color scheme is suboptimal‚Äîthe distance between points in a given color space may not be the same as the distance between points in perceptual space. As a result of the uneven mapping between color space and perceptual space, multi-hued color schemes are not recommended.",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-types",
    "href": "qmd/visualization-general.html#sec-vis-gen-types",
    "title": "General",
    "section": "Chart Types",
    "text": "Chart Types\n\nBar Graphs\n\nDon‚Äôt use bar graphs for anything except counts. Audiences have trouble with the abstraction.\nFor averages, used errorbar charts or use median + raincloud.\nGuide\n\nDesign Examples (link)\n\n\nStacked Bar\n\nReplacement for pie charts et al when dealing with fractional data\nAlways reorder stacks\n\n\nRmd tutorial for reordering optimation\n\n\nBox Plots\n\n\nSmall data - emphasize the points\nLarge data - emphasize the box\n\nLine Charts\n\nSometimes it‚Äôs appropriate not to use zero as the baseline\nHaving the y-axis not intersect the x-axis can minimize the risk of confusing the readers with a non-zero baseline chart\n\nTime Series of ordinal discrete data by category\n\n\nordinal data has 3 levels\n\n\nHeatmaps\n\nReorder rows and columns to produce a more meaningful visualization\n\n\nGuide on reordering heatmaps\nIf order is important, then this may not be possible\n\nReorder by clustering\n\n\nNetwork Graphs\n\nAlways try different multiple layout methodologies",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-maps",
    "href": "qmd/visualization-general.html#sec-vis-gen-maps",
    "title": "General",
    "section": "Maps",
    "text": "Maps\n\nAbove rules also apply\nRemove as many extraneous elements as possible\n\nHard because maps have so many necessary elements\n\nBorders, Labels, etc.\n\nIn cloropleths, remove unnecessary borders (e.g.¬†along coastlines)\n\n\n‚ÄúBorders as lines‚Äù is much less cluttered\nArticle, rmapshaper::ms_innerlines() keeps only the necessary inner borders in the ‚Äúgeometry‚Äù column of the spatial dataset.\n\n\nPay close attention to typography hierarchy\n\nBold, Font size, etc\n\nUse iconography to help users identify what you want them to see\nNumeric values (thread)\n\nPalettes: use a sequential (top row) or diverging (bottom row)\n\n\nFor diverging palettes\n\n\nThe middle value should be light on a light background (top left) or dark on a dark background (bottom left)\n\n\nBackgrounds:\n\n\nLight background: darker color on the value of interest (usually the higher value) (top left)\nDark background: lighter color on the value of interest (usually the higher value) (bottom left)\n\n\nTry not to use Rainbow palettes, because they are misleading\n\nThe rainbow and jet colors are problematic as the change in color is not perceptually uniform, leading to distinct ‚Äòbands‚Äô of certain colors. This causes misleading jumps and emphasizes certain values, most likely without the intention to highlight them. (Cedric Scherer)\n(acceptable) rainbow called ‚ÄúTurbo‚Äù if you need one (article)\n\n\nCode - see comments for links to R scripts and improved versions of Turbo\n\nOther Alternatives",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-area",
    "href": "qmd/visualization-general.html#sec-vis-gen-area",
    "title": "General",
    "section": "Area",
    "text": "Area\n\nIn general, these charts aren‚Äôt good for noisy data and data with many categories\n\nHave issues when values increase sharply (see video. around 50:13)\n\nExperiment with the order of the groups\n\nEvents that you‚Äôre looking for are probably only visable when there‚Äôs a particular order\nMost of the time, putting the most stable groups at the bottom produces the best results",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-ts",
    "href": "qmd/visualization-general.html#sec-vis-gen-ts",
    "title": "General",
    "section": "Time Series",
    "text": "Time Series\n\nHorizon Charts\n\nSee Anomaly Detection &gt;&gt; Charts\nEspecially useful for showing data with large amplitudes in a short vertical space",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-uncert",
    "href": "qmd/visualization-general.html#sec-vis-gen-uncert",
    "title": "General",
    "section": "Uncertainty",
    "text": "Uncertainty\n\nVisualizing only inferential uncertainty can lead to significant overestimates of treatment effects\n\n\nWhen possible, plot individual data points alongside statistical estimates\n\nTranslate percentages into counts (e.g.¬†‚Äúa 1 out of 5 chance‚Äù rather than ‚Äúa 20% chance‚Äù)\n\n\n{riskyr} - icon arrays and less sophisticated viz for the above chart\nicon arrays\n\nExamples\n\nbase rates and error rates (paper)\nrelative risks (paper)\n\n\nWaffle plots are similar to icon arrays\n\nquantile dotplots\n\n{ggdist} (many examples and flavors)\n\nhypothetical outcome plots\n\nConsists of multiple individual plots (frames), each of which depicts one draw from a distribution (use case for animation)\nBest suited for multivariate judgments like how reliable a perceived difference between two random variables is\nIllustration of the process\n\n\nYou create a distribution to sample from or using known distribution and parameters or bootstrapping the sample and sample from each bootstrap.\nEach sample/draw is presented on the right side of the distribution plot (fig 1) (final product)\n\nI think it would be better if after each draw the previous draw remained but was de-emphasized (i.e.¬†turned light gray)\nAnother example would McElreath‚Äôs lecture video on posterior prediction distribution.\n\nFigs 2 and 3 show a sequence of draws from a joint distribution of uncorrelated variables (fig 2) and correlated variables (fig 3)\n\nExample: NYT on interpreting jobs reports\n\n\n\n2 facets: accelerating job growth (left), steady job growth (right)\nFor each facet,\n\nthe left plot is static, and the right plot is animated showing different noisy samples of the same underlying dgp\nthe left plot shows what normals perceive the distribution to look like for the given interpretation (e.g.¬†accelerating job growth), and the right plot shows what real (i.e.¬†noisy) data with the same interpretion looks like.\n\n\n\n\nFan charts\n\n\nshows a 90% interval broken divided into 30% increments (left) or 10% increments (right)\n\nShow previous forecasts\n\n\nTruth is in dark blue with light blue branches showing previous forecasts",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-mob",
    "href": "qmd/visualization-general.html#sec-vis-gen-mob",
    "title": "General",
    "section": "Mobile",
    "text": "Mobile\n\nMisc\n\nRStudio plots are displayed in 96 dpi and ggsave uses 300 dpi as default\n\ni.e.¬†viewed plots won‚Äôt look the same as the saved plots using default settings\n\n\nUse sharp color contrasts when highlighting\nMinimal readable size is 16, but 22 is recommended\nAspect ratio of 4:3 or 1024 x 768 pixels\n\nAnother article say 1:2\n\nBar Charts should be horizontal to make charts with many categories readable\n\nMobile screens are more tall than wide so labels on the y-axis makes more sense than on the x-axis\n\nR\n\nSet-up external window with aspect ratio (e.g.¬†1:2)\ndev.new(width=1080, height=2160, unit=\"px\", noRStudioGD = TRUE)\n\nnoRStudioGD = TRUE says any new plots appear in the new graphics window rather than the RStudio graphics device\nCan also use windows(), x11(), or png() from {ragg}\n\n\nUse Quarto (or Rmd) for developement\n#| dpi: 300     \n#| fig.height: 7.2     \n#| fig.width: 3.6     \n#| dev: \"png\"     \n#| echo: false     \n#| warning: false     \n#| message: false`\n\nThis way your dpi and aspect ratio are set and you can view the final output without having to save the png and viewing it separately to see how it looks\nfig.height and fig.width are always given in inches\n\nIf you haven‚Äôt set your Quarto document to be self-contained, then the images have also already been saved for you - probably in a folder called documentname_files/figure-html/",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html",
    "href": "qmd/feature-engineering-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-misc",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-misc",
    "title": "General",
    "section": "",
    "text": "Tree-based Models\n\nFrom Uber, ‚ÄúTree-based models are performing piecewise linear functional approximation, which is not good at capturing complex, non-linear interaction effects.‚Äù\n\nWith regression models, you have to be careful about encoding categoricals as ordinal (i.e.¬†integers) which means one-hot encoding is better.\n\nFor example, the raw numerical encoding (0-24) of the ‚Äúhour‚Äù feature prevents the linear model from recognizing that an increase of hour in the morning from 6 to 8 should have a strong positive impact on the number of bike rentals while a increase of similar magnitude in the evening from 18 to 20 should have a strong negative impact on the predicted number of bike rentals.\n\nModels with large numbers (100s) of features increases the opportunity for feature drift\nZero-Inflated Predictors/Features\n\nFor ML, transformations probably not necessary\nFor regression\n\nlog(x + 0.05)\n\nlarger effect on skew than sqrt\n\narcsinh(x) (see Continuous &gt;&gt; Transformations &gt;&gt; Logging)\n\napproximates a log but handles 0s\n\nsqrt maybe Yeo-Johnson (?)\n\n&gt;60% of values = 0, consider binning or binary",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-cont",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-cont",
    "title": "General",
    "section": "Continuous",
    "text": "Continuous\n\nBinning\n\nBenefits\n\nReduces Noise\n\nContinuous variables tend to store information with minute fluctuations that provide no added value for the machine learning task of interest\n\nMakes the feature more intuitive\n\nIs there an important threshold value?\n\n1 value ‚Äì&gt; split into a binary\nmultiple values ‚Äì&gt; multinomial\n\n\nMinimizes outlier influence\n\nBin and Embed\n\nSteps\n\nFind bin ranges\n\nIf sufficient data, calculate quantiles of the numeric vector to find the bin ranges\nsklearn.preprocessing.KBinsDiscretizer has a few different methods\nUse some other method to find the number/ranges of bins (see R packages)\n\nUse the indices of the bins (i.e.¬†leftmost bin is 1, 2nd leftmost bin is 2) to discretize each value of the numeric\n\nMight need to be one-hot coded\n\nCreate an embedding of the discretized vector and use the embedding as features.\n\n\nDichotomizing is bad (post, list of papers)\n\nTypical arguments for splitting (even when there‚Äôs no underlying reason to do so) include: simplifies the statistical analysis and leads to easy interpretation and presentation of results\n\nExample: splitting at the median‚Äîleads to a comparison of groups of individuals with high or low values of the measurement, leading in the simplest case to a t test or œá2 test and an estimate of the difference between the groups (with its confidence interval) on another variable.\n\nUsing multiple categories (to create an ‚Äúordinal‚Äù variable) is generally preferable , and using four or five groups the loss of information can be quite small\nIssues:\n\nInformation is lost, so the statistical power to detect a relation between the variable and patient outcome is reduced.\n\nDichotomising a variable at the median reduces power by the same amount as would discarding a third of the data\n\nMay increase the risk of a positive result being a false positive\nMay seriously underestimate the extent of variation in outcome between groups, such as the risk of some event, and considerable variability may be subsumed within each group.\nIndividuals close to but on opposite sides of the cutpoint are characterised as being very different rather than very similar.\nConceals any non-linearity in the relation between the variable and outcome\nUsing a stat like median for a cutpoint means studies will have different cutpoints, therefore results cannot easily be compared, seriously hampering meta-analysis of observational studies\nAn ‚Äúoptimal‚Äù cutpoint (usually that giving the minimum P value) runs a high risk of a spuriously significant result. Effect will be overestimated and the CI too narrow\nAdjusting for the effect of a confounding variable, dichotomisation will run the risk that a substantial part of the confounding remains\n\n\nHarrell\n\nThink most of these issues are related to inference models like types of logistic regression\nA better approach that maximizes power and that only assumes a smooth relationship is to use a restricted cubic spline (regression spline; piecewise cubic polynomial) function for predictors that are not known to predict linearly. Use of flexible parametric approaches such as this allows standard inference techniques (P -values, confidence limits) to be used (See Feature Engineering, Splines)\nIssues with binning continuous variables\n\nIf cutpoints are chosen by trial and error in a way that utilizes the response, even informally, ordinary P -values will be too small and confidence intervals will not have the claimed coverage probabilities.\n\nThe correct Monte-Carlo simulations must take into account both multiple tests and uncertainty in the choice of cutpoints.\n\nUsing the ‚Äúminimum p-value approach‚Äù often results in multiple cutpoints so ¬Ø\\_(„ÉÑ)_/¬Ø plus multiple testing p-value adjustments need to be used.\n\nThis approach involves testing multiple cutpoints and choosing one that minimizes the p-value below a threshold.\n\nOptimal cutpoints often change from sample to sample\nThe optimal cutpoint for a predictor would necessarily be a function of the continuous values of all the other predictors\nYou‚Äôre losing variation (information) which causes a loss of power and precision\nAssumes that the relationship between the predictor and the response is flat within each interval\n\nthis assumption is far less reasonable than a linearity assumption in most cases\n\nPercentiles\n\nUsually estimated from the data at hand, are estimated with sampling error, and do not relate to percentiles of the same variable in a population\nValue of binned variable potentially takes on a different relationship with the outcome\n\ne.g.¬†Body Mass Index has a smooth relationship with every outcome studied, and relates to outcome according to anatomy and physiology. Binning may change that relationship to being how many subjects have a similar BMI.\n\n\nMany bins usually required to make it worth it. Therefore, many dummy variables will end up being created resulting in a loss of power and precision. (i.e.¬†more bins = more variables = more dof used)\nPoor predictive performance with Cox regression models\n\n\nThey might help with prediction using ML or DL models though\n\n‚ÄúInstead of directly using marketplace health as a continuous feature, we decided to use a form of target-encoding by splitting up the metric into buckets and taking the average historical delivery duration within that bucket as the new feature. With this approach, we directly helped the model learn that very supply-constrained market conditions are correlated with very high delivery times ‚Äî rather than relying on the model to learn those patterns from the relatively sparse data available.‚Äù\n\nImproving ETA Prediction Accuracy for Long Tail Events\nHelps to ‚Äúrepresent features in a way that makes it easy for the model to learn sparse patterns.‚Äù\n\nThis article was about modeling tail events, so maybe this is most useful for features that have an association with the tail values in the outcome variable\n\n\nXGBoost seems to like numerics much more than dummies\n\nTrees may prefer larger cardinalities. So if you do bin, you‚Äôd probably want quite a few bins\nNever really seen a binned age variable do well, so guessing more than 10 at least. Though maybe Age just wasn‚Äôt important enough.\n\n\nExamples\n\nBinary\n\nWhether a user spent more than $50 or didn‚Äôt\nIf user had activity on the weekend or not\n\nMultinomial or Discrete\n\nTimestamp to morning/afternoon/ night,\nOrder values into buckets of $10‚Äì20, $20‚Äì30, $30+\nHeight, age\n\nExample: step_discretize\ndata(ames, package = \"modeldata\")\n\nrecipe(~ Lot_Frontage + Lot_Area, data = ames) |&gt;\n  step_discretize(all_numeric_predictors(), num_breaks = 5) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 √ó 2\n#&gt;    Lot_Frontage Lot_Area\n#&gt;    &lt;fct&gt;        &lt;fct&gt;   \n#&gt;  1 bin5         bin5    \n#&gt;  2 bin4         bin4    \n#&gt;  3 bin5         bin5    \n#&gt;  4 bin5         bin4    \n#&gt;  5 bin4         bin5    \n#&gt;  6 bin4         bin3    \n#&gt;  7 bin2         bin1    \n#&gt;  8 bin2         bin1    \n#&gt;  9 bin2         bin1    \n#&gt; 10 bin2         bin2    \n#&gt; # ‚Ñπ 2,920 more rows\n\n\n\n\nTransformations\n\nMisc\n\nAlso see:\n\nRegression, Linear &gt;&gt; Transformations\nFeature Engineering, Splines\n\nCentering\n\nNo matter how a variable is centered (e.g.¬†around the mean, median, or other number), its linear regression coefficient will not change - only the intercept will change.\n\nGuide for choosing a scaling method for classification modeling\n\nNotes from The Mystery of Feature Scaling is Finally Solved (narrator: it wasn‚Äôt)\n\nOnly used a SVM model for experimentation so who knows if this carries over to other classifiers\n\ntldr\n\nGot time and compute resources? ‚Äì&gt; Ensemble different standardization methods using averaging\nNo time and limited compute resources ‚Äì&gt; standardization\n\nModels that are distribution independent or distance sensitive (e.g.¬†SVM, kNN, ANNs) should use standardization\n\nModels that are distribution dependent (e.g.¬†regularized linear regression, regularized logistic regression, or linear discriminant analysis) weren‚Äôt tested\n\nNo evidence that data-centric rules (e.g.¬†normal or non-normal distributed variables, outliers present)\nFeature scaling that is aligned with the data or model can be responsible for overfitting\nEnsembling by averaging (instead of using a model to ensemble) different standarization methods\n\nExperiment used robust scaler (see below) and z-score standardization\n\nWhen they added a 3rd method it created more biased results\n\nRequires predictions to be probabilities\n\nFor ML models, this takes longer because an extra CV has to be run\n\n\n\n\n\n\nStandardization\n\nThe standard method transforms feature to have mean = 0, and standard deviation = 1\n\nNot robust to outliers\n\nFeature will be skewed\n\n\nUsing the median to center and the MAD to scale makes the transformation robust to outliers\nScaling by 2 sd/MAD instead of 1 sd/MAD can be useful to obtain model coefficients of continuous parameters comparable to coefficients related to binary predictors, when applied to the predictors (not the outcome)\nNotes from\n\nWhen conducting multiple regression, when should you center your predictor variables & when should you standardize them?\n\nReasons to standardize\n\nMost ML/DL models require it\n\nMany elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n\nMost Clustering methods require it\nPCA can only be interpreted as the singular value decomposition of a data matrix when the columns have centered\nInterpreting the intercept as the mean of the outcome when all predictors are held at their means\nPredictors with large values (country populations) can have really small regression coefficients. Standardization makes the coefficients have a more managable scale.\nSome types of models are more numerically stable with the predictors have been standardized\nEasier to set priors in Bayesian modeling\nCentering fixes collinearity issues when creating powers and interaction terms\n\nCollinearity between the created terms and the main effects\n\n\nOther Reasons why you might want to:\n\nCreating a composite score\n\nWhen you‚Äôre trying to sum or average variables that are on different scales, perhaps to create a composite score of some kind. Without scaling, it may be the case that one variable has a larger impact on the sum due purely to its scale, which may be undesirable.\nOther Examples:\n\nResearch into children‚Äôs behavioral disorders - researchers might get ratings from both parents & teachers, & then want to combine them into a single measure of maladjustment.\nStudy on the activity level at a nursing home w/ self-ratings by residents & the number of signatures on sign-up sheets for activities\n\n\nTo simplify calculations and notation.\n\nA sample covariance matrix of values that has been centered by their sample means is simply X‚Ä≤X (correlation matrix)\nIf a univariate random variable, X, has been mean centered, then var(X)=E(X2) and the variance can be estimated from a sample by looking at the sample mean of the squares of the observed values.\n\n\nReasons NOT to standardize\n\nWe don‚Äôt want to standardize when the value of 0 is meaningful.\n\nstep_best_normalize\n\nRequires {bestNormalize} and has bestNormalize for use outside of {tidymodels}\nChooses the best standardization method using repeated cross-validation to estimate the Pearson‚Äôs P statistic divided by its degrees of freedom (from {nortest}) which indicates closness to the Gaussian distribution.\nPackage features the method, Ordered Quantile normalization (orderNorm, or ORQ). ORQ transforms the data based off of a rank mapping to the normal distribution.\nAlso includes: Lambert W\\(\\times\\)F, Box Cox, Yeo-Johnson, arcsinh, exponential, log, square root, and has a method to add your own.\nExample\nlibrary(bestNormalize)\n\ndata(ames, package = \"modeldata\")\n\nrecipe(Sale_Price ~ Lot_Frontage + Lot_Area, data = ames) |&gt;\n  step_best_normalize(all_numeric_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 √ó 3\n#&gt;    Lot_Frontage Lot_Area Sale_Price\n#&gt;           &lt;dbl&gt;    &lt;dbl&gt;      &lt;int&gt;\n#&gt;  1        2.48     2.29      215000\n#&gt;  2        0.789    0.689     105000\n#&gt;  3        0.883    1.28      172000\n#&gt;  4        1.33     0.574     244000\n#&gt;  5        0.468    1.19      189900\n#&gt;  6        0.656    0.201     195500\n#&gt;  7       -0.702   -1.27      213500\n#&gt;  8       -0.669   -1.24      191500\n#&gt;  9       -0.735   -1.19      236500\n#&gt; 10       -0.170   -0.654     189000\n#&gt; # ‚Ñπ 2,920 more rows\n\nscale(var or matrix)\n\nDefault args: center = T, scale = T\nStandardizes each column of a matrix separately\nFYI scale(var) == scale(scale(var))\n\n{datawizard::standardize} - Can center by median and scale by MAD (robust), can scale by 2sd (Gelman)\n{{sklearn::RobustScaler}}\n\nStandardize by median and IQR instead of mean and sd\n\n(value ‚àí median) / IQR\n\nThe resulting variable has a zero mean and median and a standard deviation of 1, although not skewed by outliers and the outliers are still present with the same relative relationships to other values.\nstep_normalize has means, sd args, so it might be able to do this\n\nHarrell recommends substituting the gini mean difference for the standard deviation\n\nGini‚Äôs mean difference - the mean absolute difference between any two distinct elements of a vector.\n\n\nHmisc::GiniMd(x, na.rm = F) (doc)\nsjstats::gmd(x or df, ...) (doc)\n\nIf ‚Äúdf‚Äù then it will compute gmd for all vectors in the df\n‚Äú‚Ä¶‚Äù allows for use of tidy selectors\n\nManual\ngmd &lt;- function(x) {\n¬† n &lt;- length(x)\n¬† sum(outer(x, x, function(a, b) abs(a - b))) / n / (n - 1)\n¬† }\n\n\n\n\n\nRescaling/Normalization\n\nMisc\n\nIf the values of the feature get rescaled between 0 and 1, i.e.¬†[0,1], then it‚Äôs called normalization\nExcept in min/max, all values of the scaling variable should be &gt; 0 since you can‚Äôt divide by 0\n{datawizard::rescale} - Scales variable to a specified range\n\nMin/Max\n\nRange: [0, 1]\n\n\nMake sure the min max value are NOT outliers. If they are outliers, then the range of your data will be more constricted that it needs to be.\n\ne.g.¬†if values are in between 100 and 500 with an exceptional value of 25000, then 25000 is scaled as 1 and all the other values become very close to the lower bound of zero\n\nExample: Age is the predictor and Happiness is the outcome. Imagine a very strong relationship between age and happiness, such that happiness is at its maximum at age 18 and its minimum at age 65. It‚Äôll be easier if we rescale age so that the range from 18 to 65 is one unit. Now this new variable A ranges from 0 to 1, where 0 is age 18 and 1 is age 65. (from Statistical Rethinking section 6.3.1 pg 182)\nd2 &lt;- d[ d$age&gt;17 , ] # only adults\nd2$A &lt;- ( d2$age - 18 ) / ( 65 - 18 )\n\nRange: [a, b]\n\nAlso see notebook for code to transform more than 1 variable at a time.\n\nBy max\nscaled_var = var/max(var)\n\nExample: From Statistical Rethinking, pg 246\n\n‚Äú‚Ä¶ zero ruggedness is meaningful. So instead terrain ruggedness is divided by the maximum value observed. This means it ends up scaled from totally flat (zero) to the maximum in the sample at 1 (Lesotho, a very rugged and beautiful place).‚Äù\n\nExample: From Statistical Rethinking, pg 258\n\n‚ÄúI‚Äôve scaled blooms by its maximum observed value, for three reasons. First, the large values on the raw scale will make optimization difficult. Second, it will be easier to assign a reasonable prior this way. Third, we don‚Äôt want to standardize blooms, because zero is a meaningful boundary we want to preserve.‚Äù\n\nblooms is bloom size. So there can‚Äôt be a negative but zero makes sense.\nblooms is 2 magnitudes larger than both its predictors.\n\n\n\nBy mean\nscaled_var = var/mean(var)\n\nExample: From Statistical Rethinking, pg 246\n\n‚Äúlog GDP is divided by the average value. So it is rescaled as a proportion of the international average. 1 means average, 0.8 means 80% of the average, and 1.1 means 10% more than average.‚Äù\n\n\n\n\n\nLogging\n\nUseful for skewed variables\nIf you have zeros, then its common to add 1 to the predictor values\n\nTo backtransform: exp(logged_predictor) - 1\narcsinh(x): approximates a log (at large values of x) but handles 0s:\n\nBacktransform: log(x + sqrt(1+x^2))\n\n* Don‚Äôt use these for outcome variables (See Regression, Other &gt;&gt; Zero-Inflated/Truncated &gt;&gt; Continuous for methods, Thread for discussion and link to a paper on the alternatives)\n\nThe scale of the outcome matters. The thread links to a discussion of a paper on log transforms.\nProposals in the paper are in Section 4.1. One of the recommendations is log(E[Y(0)] + Y) where (I think) E[Y(0)] is the average value of Y when Treatment = 0 but I‚Äôm not sure. Need to read the paper.\n\n\nRemember to back-transform predictions if you transformed the target variable\n# log 10 transformed target variable\npreds_intervals &lt;- predict(\n¬† workflows::pull_workflow_fit(lm_wf),\n¬† workflows::pull_workflow_prepped_recipe(lm_wf) %&gt;% bake(ames_holdout),\n¬† type = \"pred_int\",\n¬† level = 0.90\n) %&gt;%¬†\n¬† mutate(across(contains(\".pred\"), ~10^.x))\nCombos\n\nLog + scale by mean\n\nExample From Statistical Rethinking Ch8 pg 246\n\n‚ÄúRaw magnitudes of GDP aren‚Äôt meaningful to humans. Since wealth generates wealth, it tends to be exponentially related to anything that increases it (earlier in chapter). This is like saying that the absolute distances in wealth grow increasingly large, as nations become wealthier. So when we work with logarithms instead, we can work on a more evenly spaced scale of magnitudes‚Äù\n‚ÄúLog GDP is divided by the average value. So it is rescaled as a proportion of the international average. 1 means average, 0.8 means 80% of the average, and 1.1 means 10% more than average.‚Äù",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-disc",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-disc",
    "title": "General",
    "section": "Discrete",
    "text": "Discrete\n\nQuantitative variables that are countable with no in-between the values. (e.g.¬†integer value variables)\n\ne.g.¬†Age, Height (depending on your scale), Year of Birth, Counts of things\n\nMany variables can be either discrete or continuous depending on whether they are ‚Äúexact‚Äù or have been rounded (i.e.¬†their scale).\n\nTime since event, distance from location\nA zip code would not be a discrete variable since it is not quantitative (i.e.¬†don‚Äôt represent amounts of anything). The values just represent geographical locations and could just as easily be names instead of numbers. There is no inherent meaning to arithmetic operations performed on them (e.g.¬†zip_code1 - 5 has no obvious meaning)\n\nBinning\n\nSee Binning\n\nRange to Average\n\nSo numerical range variables like Age can have greater predictive power in ML/DL algorithms by just using the average value of the range\ne.g.¬†Age == 21 to 30 ‚Äì&gt; (21+30)/2 = 25.5\n\nRates/Ratios\n\nSee Domain Specific\n\nMin/Max Rescaling\n\nSee Continuous &gt;&gt; Transformations &gt;&gt; Rescaling/Normalization",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feats-general-cats",
    "href": "qmd/feature-engineering-general.html#sec-feats-general-cats",
    "title": "General",
    "section": "Categoricals",
    "text": "Categoricals\n\nMisc\n\nSee Feature Engineering, Embeddings &gt;&gt; Engineering\nOne-Hot Encode Issues:\n\nWith high cardinality, the feature space explodes ‚Äì&gt;\n\nLess power\nLikely to encounter memory problems\n\nUsing a sparse matrix is memory efficient which might make the one-hot encode feasible\n\nSparse data sets don‚Äôt work well with highly efficient tree-based algorithms like Random Forest or Gradient Boosting.\n\nModel can‚Äôt determine similarity between categories (embedding does)\nEvery kind of encoding and embedding outperforms it by a lot, especially in tree models\n\n\n\n\nCombine/Lump/Collapse\n\nCollapse categories with similar characteristics to reduce dimensionality\n\nstates to regions (midwest, northwest, etc.)\n\nLump\n\nCat vars with levels with too few counts ‚Äì&gt; lump together into an ‚ÄúOther‚Äù category\nstep_other(cat_var, threshold = 0.01) # see\n\nFor details see Model Building, tidymodels &gt;&gt; Recipe\nLevels with too few data will have large uncertainties about the effect and the bloated std.devs can cause some models to throw errors\n\n\nCombine\n\nThe feature reduction can help when data size is a concern\n\nThink this is equivalent to a cat-cat interaction.¬† ML models usually algorithmically create interactions but I guess this way you get the interaction but with fewer features.\nAlso might be useful to use the same considerations that you use to choose interactions to choose which cat variables to combine.\n\nSteps\n\nCombine var1 and var2 (e.g.¬†‚Äúdog‚Äù, ‚Äúminnesota‚Äù) to create a new feature called var3 (‚Äúdog_minnesota‚Äù).\nRemove individual features (var1 and var2) from the dataset.\nencode (one-hot, dummy, etc.) var 3\n\n\n\n\n\nEncode/Hashing\n\nCat vars with high numbers of levels need encoded\nCan‚Äôt dummy var because it creates too many additional variables ‚Äì&gt; reduces power\nNumeric: as.numeric(as.factor(char_var))\nTarget Encoding\n\n{collinear}\n\ntl;dr; I don‚Äôt see a method that stands out as theoretically better or worse than the others. The rnorm method would probably produce the most variance within the predictor.\ntarget_encoding_lab takes a df and encodes all categoricals using all or some of the methods\nRank (target_encoding_rank): Returns the rank of the group as a integer, starting with 1 as the rank of the group with the lower mean of the response variable\n\nwhite_noise argument might be able to used.\n\nMean (target_encoding_mean): Replaces each value of the categorical variable with the mean of the response across the category the given value belongs to.\n\nThe argument, white_noise, limits potential overfitting. Must be a value betwee 0 and 1. The value added depends on the magnitude of the response. If response is within 0 and 1, a white_noise of 0.25 will add to every value of the encoded variable a random number selected from a normal distribution between -0.25 and 0.25\n\nrnorm (target_encoding_rnorm): Computes the mean and standard deviation of the response for each group of the categorical variable, and uses rnorm() to generate random values from a normal distribution with these parameters.\n\nThe argument rnorm_sd_multiplier is used as a multiplier of the standard deviation to control the range of values produced by rnorm() for each group of the categorical predictor. Values smaller than 1 reduce the spread in the results, while values larger than 1 have the opposite effect.\n\nLOO (target_encoding_loo): Replaces each categorical value with the mean of the response variable across the other cases within the same group.\n\nThe argument, white_noise, limits potential overfitting.\n\n\n{{category_encoders}}\npip install category_encoders\nimport category_encoders as ce\ntarget_encoder = ce.TargetEncoder(cols=['cat_col_1', 'cat_col_2'])\ntarget_encoder.fit(X, y)\nX_transformed = target_encoder.transform(X_pre_encoded)\n\nCatboost Encoder\npip install category_encoders\nimport category_encoders as ce\ntarget_encoder = ce.CatBoostEncoder(cols=['cat_col_1', 'cat_col_2'])\ntarget_encoder.fit(X, y)\nX_transformed = target_encoder.transform(X_pre_encoded)\nBinary Encoding\n\nBenchmarks for decision trees:\n\nNumeric best (&lt; 1000 categories)\nBinary best (&gt; 1000 categories)\n\nStore N cardinalities using ceil(log(N+1)/log(2)) features\n\n\nHashing\n\nI‚Äôm not sure about this method. I‚Äôve seen examples where hashes look very different when 1 small thing changes ‚Äî but maybe it was the algorithm. Also seems like it would be poor security if things that looked similar had similar hashes.\nSee Privacy for hashing packages\nBeyond security and fast look-ups, hashing is used for similarity search.\n\ne.g.¬†Different pictures of the same thing should have similar hashes\nSo, if these hashes are being binned, you‚Äôd want something a hashing algorithm thinks is similar to actually be similar in order for this to be most effective.\n\nzip codes, postal codes, lat + long would be good\nNot countries or counties since I‚Äôd think the hashing similarity would be related to how similar they are alphabetically or maybe phonetically\nMaybe something like latin species names since those have similar roots, etc. would work. (e.g.¬†dogs are canis-whatever)\n\n\nCan‚Äôt be reversed to the original values\n\nAlthough since you have the original, it seems like you could see which cat levels are in a particular hash and maybe glean some latent variable\n\nCreates dummies for each cat but fewer of them.\n\nIt is likely that multiple levels of the column will map to the same hashed columns (even with small data sets). Similarly, it is likely that some columns will have all zeros.\n\nA zero-variance filter (via recipes::step_zv) is recommended for any recipe that uses hashed columns\n\n\ntextrecipes::step_dummy_hash - Dimension Reduction. Create dummy variables, but instead of giving each level its own column, you run the level through a hashing function (MurmurHash3) to determine the column.\n\nnum_terms: Tuning parameter tha controls the number of indices that the hashing function will map to. Since the hashing function can map two different tokens to the same index, will a higher value of num_terms result in a lower chance of collision.\nExample\ndata(ames, package = \"modeldata\")\n\nrecipe(Sale_Price ~ Neighborhood, data = ames) |&gt;\n  step_dummy_hash(Neighborhood, num_terms = 4) |&gt; # Low for example\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 √ó 5\n#&gt;    Sale_Price dummyhash_Neighborhood_1 dummyhash_Neighborhood_2\n#&gt;         &lt;int&gt;                    &lt;int&gt;                    &lt;int&gt;\n#&gt;  1     215000                        0                       -1\n#&gt;  2     105000                        0                       -1\n#&gt;  3     172000                        0                       -1\n#&gt;  4     244000                        0                       -1\n#&gt;  5     189900                        0                        0\n#&gt;  6     195500                        0                        0\n#&gt;  7     213500                        0                        0\n#&gt;  8     191500                        0                        0\n#&gt;  9     236500                        0                        0\n#&gt; 10     189000                        0                        0\n#&gt; # ‚Ñπ 2,920 more rows\n#&gt; # ‚Ñπ 2 more variables: dummyhash_Neighborhood_3 &lt;int&gt;,\n#&gt; #   dummyhash_Neighborhood_4 &lt;int&gt;\n\n\nLikelihood Encodings\n\nEstimate the effect of each of the factor levels on the outcome and these estimates are used as the new encoding. The estimates are estimated by a generalized linear model. This step can be executed without pooling (via glm) or with partial pooling (stan_glm or lmer). Currently implemented for numeric and two-class outcomes.\n{embed}\n\nstep_lencode_glm, step_lencode_bayes , and step_lencode_mixed\n\n\n\n\n\nOrdinal\n\nMisc\n\nIf there are NAs or Unknowns, etc.,\n\nAfter coercing into a numeric/integer, you can convert Unknowns to NA and then impute the variable\n\nAll these encodings will produce the same results for a tree model, since tree-based models rely on variable ranks rather than exact values.\n0 = ‚Äú0 Children‚Äù\n1 = ‚Äú1 Child‚Äù\n2 = ‚Äú2 Children‚Äù\n3 = ‚Äú3 Children‚Äù\n4 = ‚Äú4 or more Children‚Äù\n\n1 = ‚Äú0 Children‚Äù\n2 = ‚Äú1 Child‚Äù\n3 = ‚Äú2 Children‚Äù\n4 = ‚Äú3 Children‚Äù\n5 = ‚Äú4 or more Children‚Äù\n\n-100 = ‚Äú0 Children‚Äù\n-85¬† = ‚Äú1 Child‚Äù\n0¬† ¬† = ‚Äú2 Children‚Äù\n10¬† = ‚Äú3 Children‚Äù\n44¬† = ‚Äú4 or more Children‚Äù\n\nVia {tidymodels}\nstep_mutate(ordinal_factor_var = as_integer(ordinal_factor_var))\n# think this uses as_numeric\nstep_ordinalscore(ordinal_factor_var)\nPolynomial Contrasts\n\nSee the section Kuhn‚Äôs book\n\nRainbow Method (article)\n\nMIsc\n\nCreates an artifical ordinal variable from a nominal variable (i.e.¬†ordering colors according the rainbow, roy.g.biv)\nAt worst, it maintains the signal of a one-hot encode, but with tree models, it results in less splits and therefore a simpler, more efficient, and less overfit model.\nTest psuedo ordinal method by constructing a simple bayesian model with response ~ 0 + ordinal. Then, you extract the posterior for each constructed ordinal level. Pass these posteriors through a constraint that labels draws for that level that are less (or not) than the draws of the previous level. Lastly calculate the proportion of those that were less than in order to get a probability that the predictor is ordered (article &gt;&gt; ‚ÄúThe Model‚Äù section)\n\nCode\ngrid &lt;- data.frame(\n  Layer = c(\"B\", \"C\", \"E\", \"G\", \"I\"),\n  error = 0\n)\n\ngrid_with_mu &lt;- tidybayes::add_linpred_rvars(grid, simple_mod, value = \".mu\")\n\nis_stratified &lt;- with(grid_with_mu, {\n  .mu[Layer == \"B\"] &gt; .mu[Layer == \"C\"] &\n  .mu[Layer == \"C\"] &gt; .mu[Layer == \"E\"] &\n  .mu[Layer == \"E\"] &gt; .mu[Layer == \"G\"] &\n  .mu[Layer == \"G\"] &gt; .mu[Layer == \"I\"]\n})\n\nPr(is_stratified)\n#&gt; [1] 0.78725\n‚ÄúLayer‚Äù is the ordinal variable being tested\nadd_linpred_rvars extracts the mean response posteriors for each level of the variable\nResults strongly suggest that the levels of the variable (‚ÄúLayer‚Äù) are ordered, with a 0.79 posterior probability.\n\n\nMethods:\n\nDomain Knowledge\nVariable Attribute (see examples)\nOthers - Best to compute these on a hold out set, so as not cause data leakage\n\nAssociation with the target variable where the value of association is used to rank the categories\nProportion of the event for a binary target variable where the value of the proportion is used to rank the categories\n\n\nIf it‚Äôs possible, use domain knowledge according the project‚Äôs context to help choose the ranking of the categories.\nThere are always multiple ways to rank the categories, so it may be worthwhile to try multiple versions of the artificial ordinal variable\n\nNot recommended to use more than log‚ÇÇ(K) versions, so as to not surpass the number of variables creating using One-hot (where k is the number of categories)\n\nExample: Vehicle Type\n\nCategories\n\nC: ‚ÄúCompact Car‚Äù\nF: ‚ÄúFull-size Car‚Äù\nL: ‚ÄúLuxury Car‚Äù\nM: ‚ÄúMid-Size Car‚Äù\nP: ‚ÄúPickup Truck‚Äù\nS: ‚ÄúSports Car‚Äù\nU: ‚ÄúSUV‚Äù\nV: ‚ÄúVan‚Äù\n\nPotential attributes to order by: vehicle size, capacity, price category, average speed, fuel economy, costs of ownership, motor features, etc.\n\nExample: Occupation\n\nCategories\n\n1: ‚ÄúProfessional/Technical‚Äù\n2: ‚ÄúAdministration/Managerial‚Äù\n3: ‚ÄúSales/Service‚Äù\n4: ‚ÄúClerical/White Collar‚Äù\n5: ‚ÄúCraftsman/Blue Collar‚Äù\n6: ‚ÄúStudent‚Äù\n7: ‚ÄúHomemaker‚Äù\n8: ‚ÄúRetired‚Äù\n9: ‚ÄúFarmer‚Äù\nA: ‚ÄúMilitary‚Äù\nB: ‚ÄúReligious‚Äù\nC: ‚ÄúSelf Employed‚Äù\nD: ‚ÄúOther‚Äù\n\nPotential attributes to order by: average annual salary, by their prevalence in the geographic area of interest, or variables in a Census dataset or some other data source\n\n\n\n\n\nWeight of Evidence\n\nembed::step_woe",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-inter",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-inter",
    "title": "General",
    "section": "Interactions",
    "text": "Interactions\n\nManually\n\nNumeric ‚®Ø Cat\n\nDummy the cat, then multiply the numeric times each of the dummies.",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-date",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-date",
    "title": "General",
    "section": "Date",
    "text": "Date\n\nDuration\n\nDays since last purchase per customer\n\nExample: (max(invoice_date) - max_date_overall) / lubridate::ddays(1)\n\nThink ddays converts this value to a numeric\n\n\nCustomer Tenure\n\nExample: (min(invoice_date) - max_date_overall) / lubridate::ddays(1)",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-dom",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-dom",
    "title": "General",
    "section": "Domain Specific",
    "text": "Domain Specific\n\nRates/Ratios\n\nPurchase per Customer\n\nTotal Spent\n\nExample: sum(total_per_invoice, na.rm = TRUE)\n\nAverage Spent\n\nExample: mean(total_per_invoice, na.rm = TRUE)\n\n\nLet the effect of Cost vary by the person‚Äôs income\n\nmutate(cost_income = cost_of_product/persons_income)\nIntuition being that the more money you have the less effect cost will have on whether purchase something.\nDividing the feature by income is equivalent to dividing the \\(\\beta\\) by income.\n\n\nPre-Treatment Baseline\n\nExample: From Modeling Treatment Effects and Nonlinearities in A/B Tests with GAMS\n\noutcome = log(profit), treatment = exposure to internation markets, group = store\nBaseline variable is log(profit) before experiment is conducted\n\nShould center this variable",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/experiments-rct.html",
    "href": "qmd/experiments-rct.html",
    "title": "RCT",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Experiments",
      "RCT"
    ]
  },
  {
    "objectID": "qmd/experiments-rct.html#sec-exp-rct-misc",
    "href": "qmd/experiments-rct.html#sec-exp-rct-misc",
    "title": "RCT",
    "section": "",
    "text": "Reasons for not running a RCT\n\nIt‚Äôs just not technically feasible to have individual-level randomization of users as we would in a classical A/B test\n\ne.g.¬†randomizing which individuals see a billboard ad is not possible\n\nWe can randomize but expect interference between users assigned to different experiences, either through word-of-mouth, mass media, or even our own ranking systems; in short, the stable unit treatment value assumption (SUTVA) would be violated, biasing the results\n\nA gold-standard design is a 6-period 2-treatment randomized crossover study; the patient actually receives both treatments and her responses can be compared (Harrell)\nATE for RCT:\n\nNon-theoretical ATE (i.e.¬†calculated from actual data) is sample-averaged; population sampling weights are unavailable for RCT subject groups. So this ATE applies to a replication of the study with similar sampling patterns. ATE does not apply to the population and in fact may apply to no one due to lack of conditioning on patient characteristics. The ATE used in 99% of papers has nothing to do with population but uses only convenience sample weighting.¬† Some papers even blatantly call it population-averaged.‚Äù\n‚ÄúThey test causal hypotheses about a group of patients with symptoms & other ‚Äòdiagnostic‚Äô findings that form entry criteria for the RCT & may only be available in sufficient numbers in specialist centres.‚Äù\nGelman\n\n‚Äúthe drug works on some people and not others‚Äîor in some comorbidity scenarios and not others‚Äîwe realize that‚Äùthe treatment effect‚Äù in any given study will depend entirely on the patient mix. There is no underlying number representing the effect of the drug. Ideally one would like to know what sorts of patients the treatment would help, but in a clinical trial it is enough to show that there is some clear average effect. My point is that if we consider the treatment effect in the context of variation between patients, this can be the first step in a more grounded understanding of effect size.\n\nGelman regarding a 0.1 ATE for a treatment in an education study\n\n‚ÄúActually, though, an effect of 0.1 GPA is a lot. One way to think about this is that it‚Äôs equivalent to a treatment that raises GPA by 1 point for 10% of people and has no effect on the other 90%. That‚Äôs a bit of an oversimplification, but the point is that this sort of intervention might well have little or no effect on most people. In education and other fields, we try lots of things to try to help students, with the understanding that any particular thing we try will not make a difference most of the time. If mindset intervention can make a difference for 10% of students, that‚Äôs a big deal. It would be naive to think that it would make a difference for everybody: after all, many students have a growth mindset already and won‚Äôt need to be told about it.\n‚ÄúMaybe in some fields of medicine this is cleaner because you can really isolate the group of patients who will be helped by a particular treatment. But in social science this seems much harder.‚Äù\n\nMe: So, a 0.1 effect wouldn‚Äôt be large if there was no variation (i.e.¬†same size effect for everyone), but that‚Äôs very unlikely to be the case.\n\n\n\nCalculation of standard-errors is different depending on the RCT type in order that variation within arms could be validly used to estimate variation between.\nRandom Sampling vs Random Treatment Allocation (source)\n\nRandom Sampling: licenses the use of measures of uncertainty¬† for (sub)groups of sampled patients.\nRandom Treatment Allocation: licenses the use of measures of uncertainty for the differences between the allocated groups.\n\nRe RCTs:\n\nlicenses the use of measures of uncertainty for hazard ratios, odds ratios, risk ratios, median/mean survival difference, absolute risk reduction etc that measure differences between groups.\nBecause there is no random sampling, measures of uncertainty are not licensed by the randomization procedure for cohort-specific estimates such as the median survival observed in each treatment cohort.\n\nFor those, we can use descriptive measures such as standard deviation (SD), interquartile range etc. Measures of uncertainty will require further assumptions to be considered valid. Further discussion here‚Äù\n\n\n\n\nBalance - Balanced allocations are more efficient in that they lead to lower variances\n\nVariance of the Mean difference (e.g between treatment and control groups) for unbalanced design\n\\[\n\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\n\\]\n\nFollow-Up\n\nIn-Trial Follow-Up (ITFU)\n\nWithout ITFU, the unbiased ascertainment of outcomes may be compromised and statistical power considerably reduced\nStrategies\n\nFace-to-face follow-up is widely used during the initial ‚Äúin-trial‚Äù period, but is costly if employed longer term.\nTelephone-based approaches are more practical, with the ability to contact many participants coordinated by a central trial office\nPostal follow-up has been shown to be effective.\nWeb-based techniques may become more widespread as technological advances develop.\n\n\nPost-Trial Follow-Up (PTFU)\n\nRCTs are costly and usually involve a relatively brief treatment period with limited follow-up. A treatment response restricted to this brief ‚Äúin-trial‚Äù period can potentially underestimate the long-term benefits of treatment and also may fail to detect delayed hazards.\nStrategies\n\nSee ITFU strategies\nUse of routine health records can provide detailed information relatively inexpensively, but the availability of such data and rules governing access to it varies across countries.",
    "crumbs": [
      "Experiments",
      "RCT"
    ]
  },
  {
    "objectID": "qmd/experiments-rct.html#sec-exp-rct-feats",
    "href": "qmd/experiments-rct.html#sec-exp-rct-feats",
    "title": "RCT",
    "section": "RCT Features",
    "text": "RCT Features\n\nThree valuable design features of clinical trials are concurrent control, randomization and blinding.\n\nBlinding is weak at best without Randomization\nRandomization is impossible without Concurrent Control,\nConcurrent Control is necessary for the other two, so it can be regarded as the most important of the three.\n\nBlinding or Masking - patients are unaware of the treatment they are receiving and treating physicians are unaware of the treatment they are administering\n\nPrevents differential care during follow-up, accounts for nonspecific effects associated with receiving an intervention (placebo effects), may facilitate blinding of outcome assessors, and may improve adherence.\n\nConcurrent Control - the effect of a treatment should be assessed by comparing the results of subjects treated with the intervention being studied with the results of subjects treated concurrently (i.e.¬†at the ‚Äòsame‚Äô places at the ‚Äòsame‚Äô times) with a control treatment, for example, placebo.\n\nIn reality\n\nRe ‚Äòsame time‚Äô: the idea behind concurrent control is that the times at which they are recruited will vary randomly within treatment arms in the same way as between, so that variation in outcomes arising as a result of the former can be used to judge variation in outcomes as a result of the latter.\n\nTiming matters: The time at which a patient is recruited into the trial matters, but should not be biasing if patients are randomized throughout the trial to intervention and control. It will tend to increase the variance of the treatment effect, and rightly so, but it is a component that may be possible to eliminate (partially) by modelling a trend effect.\n\nExample: 1990s AIDS studies found survival of patients who were recruited later into trials tended to be better than those recruited earlier\n\n\nRe ‚Äòsame place‚Äô: The vast majority of randomised clinical trials are run in many centers. The variations in design around this many-centers aspect is the primary difference between various types of RCTs (see below). All the types will be regarded as employing concurrent control, but have their standard errors calculated differently.\n\nConsequence of violations\n\nIf variation from center to center is ignored and patients have not been randomized concurrently, then Fisher‚Äôs exact test, Pearson‚Äôs chi-square and Student‚Äôs t will underestimate the variation\n\n\nRandomized assignment means that eligible units are randomly assigned to a treatment or comparison group. Each eligible unit has an equal chance of being selected. This tends to generate internally valid impact estimates under the weakest assumptions.\n\nRandomization also allows us to achieve statistical independence, which eliminates omitted variable bias. Statistical independence implies that the treatment variable is not correlated with the other variables. The key assumption is that randomization effectively produces two groups that are statistically identical with respect to observed and unobserved characteristics. In other words, the treatment group is the same as the control group on average.\n\ni.e.¬†randomization process renders the experimental groups largely comparable. Thus, we can attribute any differences in the final metrics between the experimental groups to the intervention.\n\nIn the absence of randomization, we might fall victim to this omitted variable bias because our treatment variable will probably be endogenous. That is, it will be probably correlated with other variables excluded from the model (omitted variable bias).",
    "crumbs": [
      "Experiments",
      "RCT"
    ]
  },
  {
    "objectID": "qmd/experiments-rct.html#sec-exp-rct-assopts",
    "href": "qmd/experiments-rct.html#sec-exp-rct-assopts",
    "title": "RCT",
    "section": "Assignment Options",
    "text": "Assignment Options\n\nRandomization\n\nIndividual Randomization - The individual or patient that is allocated to an intervention (may be more than one intervention group) or control group, and simple statistical analyses on participant outcomes are used to evaluate if the intervention was effective.\n\nThese analyses assume that all participants are completely independent (ie. unlike each other, do not influence each other, and any outcomes measured on them are influenced by the intervention or usual care in the same way)\nStratified Randomization - Study participants are first divided into strata or subgroups based on one or more relevant characteristics or variables that are thought to influence response to treatment (i.e.¬†confounders). Then, within each stratum, participants are randomly assigned to the treatment or control groups.\n\nThe purpose is to increase statistical precision and ensure balance across known confounding factors.\nStrafiication variables could be age, gender, severity of disease, or any other potentially confounding factors.\nThe purpose of stratification is to ensure that the treatment and control groups are balanced concerning these potentially confounding variables, which can improve the precision and statistical power of the study.\nTo obtain valid inference following stratified randomisation, treatment effects should be estimated with adjustment for stratification variables.\n\nIf stratification variables are originally continuous but have been discretized for purposes of stratification (e.g.¬†Age), then the continuous form of these variables should be used in the model. (Paper)\n\n\nBlock Randomization\n\nThe purpose is to control for known sources of variability and remove their effects from the experimental error\nTwitter thread presenting an example of the procedure for a block randomized RCT\nSimilar to Stratified Randomization but blocks are formed based on practical considerations, such as time, location, or specific characteristics of participants or experimental units. (Models seem to be the same though)\n\nExample: These hospitals volunteered to participate in my study and I wish to randomize individuals within groups. I have no reason to believe that a particular hospital envirnoment will affect the treatment‚Äôs efficacy on the outcome variable (i.e hospital is not a confounder), but I wish to account for it in order to reduce error.\n\nDesigns with unequal sized blocks are known as Incomplete Block Designs\n\nThe assumption of homogeneous variances across blocks is likely to be violated when block sizes differ greatly. This violates one of the standard ANOVA assumptions.\nTypically modelled by Weighted Least Squares or Mixed Effects. Random effects for blocks to account for the variability between blocks, and fixed effects for treatment effects.\n\nBlock Designs\n\nLatin Square\n\nIt‚Äôs commonly used in situations where there are two sources of variability that need to be controlled, such as when testing multiple treatments in different orders or sequences while accounting for variations in time or space.\nAppropriate when complete randomization of treatments is feasible and desirable. This design requires no natural structure or hierarchy among experimental units (see Split-Plot for comparison), and treatments can be applied independently to each unit.\nEach treatment appears once in each row and each column of the Latin square, ensuring that each treatment occurs in every possible position relative to the other treatments.\nUseful for reducing noise and increasing efficiency.\nVariations of the Latin square design, such as the Graeco-Latin square design, can be used to extend the concept to accommodate groups or blocks of experimental units. In these designs, the basic principles of the Latin square are applied within each block or group, allowing for more flexibility in experimental design while still controlling for sources of variation\nExample: Does Music Genre Affect Work Productivity\n\n\n\n\nLocation 1\nLocation 2\nLocation 3\nLocation 4\n\n\n\n\nTime 1\nGenre A\nGenre B\nGenre C\nGenre D\n\n\nTime 2\nGenre D\nGenre A\nGenre B\nGenre C\n\n\nTime 3\nGenre C\nGenre D\nGenre A\nGenre B\n\n\nTime 4\nGenre B\nGenre C\nGenre D\nGenre A\n\n\n\n\nDetermine if there are significant differences in task performance across the different music genres while controlling for potential confounding factors (time of day and office location).\nEach cell is a different study participant which receives a particular treatment at a particular location and particular time.\n\n\nSplit-Plot\n\nParticularly useful when there is a hierarchical structure to the experimental units\nHandles hard-to-randomize factors such as logistical factors\nThe whole plots are for hard-to-randomize factors and subplots are for easy-to-randomize factors. It controls variability due to the whole plot factor.\nExample: Effects of Learning Environment and Study Technique on Memory Recall\n\nMain Plots (Groups of Participatnts)\n\n\n\nPlot 1\nPlot 2\n\n\nPlot 3\nPlot 4\n\n\n\n\nEach plot consists of 20 participants\nPlot 1: Participants from Psychology Department\nPlot 2: Participants from Biology Department\nPlot 3: Participants from English Department\nPlot 4: Participants from Mathematics Department\nNote how it would be impossible to randomly assign students to each department.\n\nSubplots within each main plot\n\n\n\n\nRepeated Reading\nMind Mapping\n\n\nQuiet Room\nsubgroup 1\nsubgroup 2\n\n\nNoisy Caf√©\nsubgroup 3\nsubgroup 4\n\n\n\n\nEach subgroup has 5 participants and are randomly assigned to a combination of two treatments: Environment and Study Technique.\n\n\n\nFamily Block - Naturally occurring groups or families (e.g.¬†litters, plots with similar soil conditions, etc.).\n\n\n\nCluster Randomization - One in which intact social units, or clusters of individuals rather than individuals themselves, are randomized to different intervention groups\n\nAll participants recruited from the practice, school or workplace are allocated to either the intervention or the control group\nThe outcomes of the intervention are still measured at the individual level, but the level at which the comparison is made is the practice, school or workplace\nAdvantages\n\nMembers from intervention and control groups are less likely to have direct contact with each other and are less likely to pass on components of the intervention to the control group. (i.e.¬†contamination)\nThere may also be increased compliance due to group participation.\nClusters typically consistent in their management.\n\n\n\n\n\nTreatment Strategy\n\nParallel Group - Subjects are randomized to one or more study arms (aka treatment groups) and each study arm will be allocated a different intervention. After randomization each participant will stay in their assigned treatment arm for the duration of the study\n\nThink this is just typical randomization into treatment/control groups but can be extended to include multiple treatment arms.\n‚ÄúChange from Baseline‚Äù (aka Change Scores) should never be the outcome variable\nCentral Question: For two patients with the same premeasurement value of x, one given treatment A and the other treatment B, will the patients tend to have different post-treatment values of y?\n\nCrossover Group - Subjects are randomly allocated to study arms where each arm consists of a sequence of two or more treatments given consecutively.\n\ni.e.¬†each subject receives more than one treatment and each treatment occurs sequentially over the duration of the study.\nExample: AB/BA study - Subjects allocated to the AB study arm receive treatment A first, followed by treatment B, and vice versa in the BA arm.\nAllows the response of a subject to treatment A to be contrasted with the same subject‚Äôs response to treatment B.\n\nRemoving patient variation in this way makes crossover trials potentially more efficient than similar sized, parallel group trials in which each subject is exposed to only one treatment.\n\nIn theory treatment effects can be estimated with greater precision given the same number of subjects.\nMisc\n\nBest practice is to avoid this design if there is a reasonable chance of Carry Over\nAlso see\n\nSenn SJ. Crossover trials in clinical research. Chichester: John Wiley; 1993, 2002.\n\n‚ÄúReadable approach to the problems of designing and analysing crossover trials‚Äù\nSee R &gt;&gt; Documents &gt;&gt; Experimental Design\nKurz notes on Chapters 3 and 4 with updated R code\n\n\n\nIssue: Carry Over\n\nEffects of one treatment may ‚Äúcarry over‚Äù and alter the response to subsequent treatments.\n(Pre-experiment) Solution: introduce a washout (no treatment) period between consecutive treatments which is long enough to allow the effects of a treatment to wear off.\n\nA variation is to restrict outcome measurement to the latter part of each treatment period. Investigators then need to understand the likely duration of action of a given treatment and its potential for interaction with other treatments.\n\nTesting for Carry Over\n\nIf carry over is present the outcome on a given treatment will vary according to its position in the sequence of treatments.\nExample: Concluding that there was no carry over when an analysis of variance found no statistically significant interaction between treatment sequence and outcome.1\nHowever such tests have limited power and cannot rule out a type II error (wrongly concluding there is no carry over effect).\n\n(Post-experiment) Solution: If Carry Over is detected:\n\nOption 1: Treat the study as though it were a parallel group trial and confine analysis to the first period alone.\n\nThe advantages of the crossover are lost, with the wasted expense of discarding the data from the second period.\nMore importantly, the significance test comparing the first periods may be invalid\n\nOption 2 (applicable only to studies with at least three treatment periods, e.g.¬†ABB/BAA)\n\nModel the carry over effect and use it to adjust the treatment estimate.\nSuch approaches, while statistically elegant, are based on assumptions which can rarely be justified in practice.\nSee Senn paper above\n\n\n\n\nRandomized designs are classified as completely randomized design, complete block design, randomized block design, Latin square design, split pot design, crossover design, family block design, stepped-wedge cluster design, etc.\n\nCompletely Randomized Parallel Group trial - Any given center will have some patients randomly allocated to intervention and some to control. Randomization includes centers (i.e.¬†a patient is randomly selected either treatment/control and which center they will receive the treatment)\n\nParallel Group Blocked by Center - Randomization happens within each center (i.e.¬†each center handles their own randomization). Treatment/Control ratio is the same for each center.\n\n‚ÄúCenter‚Äù should be included as a variable in the model.\n\n\nCluster-Randomized trial - Randomly allocate some centers to dispense the intervention and some the control\n\nFundamental unit of inference becomes the center and patients are regarded as repeated measures on it\n\n\nExamples\n\nThe effects of a leading mindfulness meditation app (Headspace) on mental health, productivity, and decision-making (Paper)\n\nRCT with 2,384 US adults recruited via social media ads.\nFour-week experiment\n\nfirst group is given free access to the app (worth $13)\nsecond group receives, in addition, a $10 incentive to use the app at least four or ten separate days during the first two weeks\nthird group serves as a (waitlist) control group",
    "crumbs": [
      "Experiments",
      "RCT"
    ]
  },
  {
    "objectID": "qmd/experiments-rct.html#sec-exp-rct-sbias",
    "href": "qmd/experiments-rct.html#sec-exp-rct-sbias",
    "title": "RCT",
    "section": "Sources of Bias",
    "text": "Sources of Bias\n\nMisc\n\nNotes from\n\nBiases in randomized trials: a conversation between trialists and epidemiologists\nAlso see\n\nExperiments, A/B Testing &gt;&gt; Potential Biases\nExperiments, Planning &gt;&gt; Misc\n\n\n\nSelection Bias - Occurs when there are systematic differences between baseline characteristics of groups.\n\nIf the assignment that was not properly randomized or the randomized assignment was not sufficiently concealed (i.e.¬†allocation concealment), and so the person enrolling participants was aware of allocation sequence and influenced which patients were assigned to each group based on their prognostic factors\nExample: if groups are not comparable on key demographic factors, then between-group differences in treatment outcomes cannot necessarily be attributed solely to the study intervention.\nExample: The assignment of patients to a group is influenced by knowledge of which treatment they will receive\nSolutions:\n\nRandomized Assignment - RCTs attempt to address selection bias by randomly assigning participants to groups ‚Äì but it is still important to assess whether randomization was done well enough to eliminate the influence of confounding variables.\nBlinding - participants and investigators should remain unaware of which group participants are assigned to.\n\n\nPerformance Bias - Refers to systematic differences between groups that occur during the study. Leads to overestimated treatment effects, because of the physical component of interventions\n\nExample: if participants know that they are in the active treatment rather than the control condition, this could create positive expectations that have an impact on treatment outcome beyond that of the intervention itself.\nSolution: Blinding - participants and investigators should remain unaware of which group participants are assigned to.\n\nMore easily achieved in medication trials than in surgical trials\n\n\nDetection Bias - Refers to systematic differences in the way outcomes are determined.\n\nExample: if providers in a psychotherapy trial are aware of the investigators‚Äô hypotheses, this knowledge could unconsciously influence the way they rate participants‚Äô progress.\nSolution: Attention to conflicts of interest and Blinding (also see Performance Bias) - RCTs address this by utilizing independent outcome assessors who are blind to participants‚Äô assigned treatment groups and investigators‚Äô expectations.\n\nAttrition Bias - occurs when there are systematic differences between groups in withdrawals from a study.\n\nIt‚Äôs common for participants to drop out of a trial before or in the middle of treatment, and researchers who only include those who completed the protocol in their final analyses are not presenting the full picture.\nSolution: Intention to Treat analysis - Analyses should include all participants who were randomized into the study, and not only participants who completed some or all of the intervention.\n\nReporting Bias - Refers to systematic differences between reported and unreported data.\n\nExample: publication bias - occurs because studies with positive results are more likely to be published, and tend to be published more quickly, than studies with findings supporting the null hypothesis.\nExample: outcome reporting bias - occurs when researchers only write about study outcomes that were in line with their hypotheses.\nSolution: Requirements that RCT protocols be published in journals or on trial registry websites, which allows for confirmation that all primary outcomes are reported in study publications.\n\nOther Bias - A catch-all category that includes specific situations not covered by the above domains.\n\nIncludes bias that can occur when study interventions are not delivered with fidelity, or when there is ‚Äúcontamination‚Äù between experimental and control interventions within a study (for example, participants in different treatment conditions discussing the interventions they are receiving with each other).",
    "crumbs": [
      "Experiments",
      "RCT"
    ]
  },
  {
    "objectID": "qmd/classification.html",
    "href": "qmd/classification.html",
    "title": "Classification",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/classification.html#sec-class-misc",
    "href": "qmd/classification.html#sec-class-misc",
    "title": "Classification",
    "section": "",
    "text": "Also see Regression, Logistic\nGuide for suitable baseline models: link\nIf you have mislabelled target classes, try AdaSampling to correct the labels (article)\nSample size requirements\n\nLogistic Regression: (Harrell, link)\n\nThese are conservative estimates. Sample size estimates assume an event probability of 0.50.\nFor just estimating the intercept and a margin of error for predicted probabilities of 0.1\n\nWith no covariates (i.e.¬†population is homogeneous), n = 96\nWith 1 categorical covariate, n = 96 for each level of the covariate\n\ne.g.¬†For gender, you need 96 males and 96 females\n\n\nFor just estimating the intercept and a margin of error for predicted probabilities of 0.05\n\nWith no covariates (i.e.¬†population is homogeneous), n = 384\nIf true probabilities of event (and non-event) are known to be extreme, i.e.¬†\\(p \\notin [0.2, 0.8]\\), n = 246\n\nFor estimating predicted probabilities with 1 continuous predictor\n\nFor a margin of error of 0.1, n = 150\nFor a margin of error of 0.07, n = 300\n\n\nRF: 200 events per candidate feature (Harrell, link)\n\nUndersampling non-events(0s) is the popular way to balance the target variable in data sets but other ma be worth exploring while building the model.\nSpline ‚Äî don‚Äôt bin continuous, baseline, adjustment variables, where ‚Äúbaseline‚Äù means the patients measurements before treatment. Lack of fit will then come only from omitted interaction effects. (Harrell)\n\ne.g.: if older males are much more likely to receive treatment B than treatment A than what would be expected from the effects of age and sex alone, adjustment for the additive propensity would not adequately balance for age and sex.\nAlso see\n\nFeature Engineering, General &gt;&gt; Continuous &gt;&gt; Binning\nFeature Engineering, General &gt;&gt; Continuous &gt;&gt; Splines\n\n\nThe best information to present to the patient is the estimated individualized risk of the bad outcome separately under all treatment alternatives. That is because patients tend to think in terms of absolute risk, and differences in risks don‚Äôt tell the whole story (Harrell)\n\nA risk difference (RD, also called absolute risk reduction) often means different things to patients depending on whether the base risk is very small, in the middle, or very large.\n\nRecommended metrics to be reported for medical studies (Harrell). This is perhaps generalizable to any RCT with a binary outcome.\n\nThe distribution of Risk Difference (RD)\nCovariate-Adjusted OR\nAdjusted marginal RD (mean personalized predicted risk as if all patients were on treatment A minus mean predicted risk as if all patients were on treatment B) (emmeans?)\nMedian RD",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/classification.html#sec-class-diag",
    "href": "qmd/classification.html#sec-class-diag",
    "title": "Classification",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nAlso see\n\nDiagnostics, Classification\nRegression, Logistic &gt;&gt; Diagnostics\n\n‚ÄúDuring the initial phase of model building, a good strategy for data sets with two classes is to focus on the AUC statistics from these curves instead of metrics based on hard class predictions. Once a reasonable model is found, the ROC or precision-recall curves can be carefully examined to find a reasonable cutoff for the data and then qualitative prediction metrics can be used.‚Äù ‚Äì 3.2.2¬†Classification Metrics‚Äù Kuhn and Kjell\n‚ÄúStable‚Äù AUC requirements for 0/1 outcome:\n\nPaper: Modern modelling techniques are data hungry: a simulation study for predicting dichotomous endpoints | BMC Medical Research Methodology | Full Text\nLogistic Regression: 20 to 50 events per predictor variable\nRandom Forest and SVM: greater than 200 to 500 events per predictor variable",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/classification.html#sec-class-imbal",
    "href": "qmd/classification.html#sec-class-imbal",
    "title": "Classification",
    "section": "Class Imbalance",
    "text": "Class Imbalance\n\nMisc\n\nAlso see\n\nModel Building, tidymodel &gt;&gt; Recipe &gt;&gt; up/down-sampling\nModel Building, sklearn &gt;&gt; Preprocessing &gt;&gt; Class Imbalance\nSurveys, Analysis &gt;&gt; Modeling &gt;&gt; Tidymodels &gt;&gt; Importance weights\nPaper: Subsampling without calibration will likely be more harmful than without subsampling\n\nPackages\n\n{themis} - Extra steps for the {recipes} for dealing with unbalanced data.\n{ebmc} - Four ensemble-based methods (SMOTEBoost, RUSBoost, UnderBagging, and SMOTEBagging) for class imbalance problem are implemented for binary classification.\n{imbalance} - Novel oversampling algorithms, filtering of instances and evaluation of synthetic instances. Methods: MWMOTE (Majority Weighted Minority Oversampling Technique), RACOG (Rapidly Converging Gibbs), wRACOG (wrapper-based RACOG), RWO (Random Walk Oversampling), and PDFOS (Probability Distribution density Function estimation based Oversampling).\n{{imbalanced-learn}} - Tools when dealing with {{sklearn}} classification with imbalanced classes\n\nSubsampling can refer to up/oversampling or down/undersampling.\nIt is perfectly ok to train a model on 1M negatives and 10K positives (i.e.¬†plenty of events), as long as you avoid using accuracy as a metric\n\n1K to 10K events might be enough for the ML algorithm to learn from\nFor a RF model: 200 events per candidate feature (Harrell, link)\n\nUnless recalibration is applied, applying subsampling to correct class imbalance will lead to overpredicting the minority class (discrimination) and worse calibration when using logistic regression or ridge regression (paper)\n\nPaper used random undersampling (RUS), random oversampling (ROS), and SMOTE (Synthetic Minority Oversampling Technique)\nEvent Fractions: 1%, 10%, 30%\nN: 2500, 5000; p: 3, 6, 12, 24\n‚ÄúWe anticipate that risk miscalibration will remain present regardless of type of model or imbalance correction technique, unless the models are recalibrated. However, class imbalance correction followed by recalibration is only worth the effort if imbalance correction leads to better discrimination of the resulting models.‚Äù\n\nThey used what looked to be Platt Scaling for recalibration\n\nAlso see Model Building, tidymodels &gt;&gt;¬†Tune &gt;&gt; Tune Model with Multiple Recipes for an example of how downsampling (w/o calibration) + glmnet affects class prediction and GOF metrics\n\n\nIssues\n\nUsing Accuracy as a metric\n\nIf the positivity rate is just 1%, then a naive classifier labeling everything as negative has 99% accuracy by definition\n\nIf you‚Äôve used subsampling, then the training data is not the same as the data used in production\nLow event rate\n\nIf you only have 10 to 100 positive samples, the model may easily memorize these samples, leading to an overfit model that generalized poorly\nMay result in large CIs for your effect estimate (see Gelman post)\n\n\nCheck Imbalance\ndata %&gt;%\n¬† count(class) %&gt;%\n¬† mutate(prop = n / sum(n)) %&gt;%\n¬† pretty_print()\nDownsampling\n\nUse cases for downsampling the majority class\n\nwhen the training data doesn‚Äôt fit into memory (and your ML training pipeline requires it to be in memory), or\nwhen model training takes unreasonably long (days to weeks), causing too long iteration cycles, and preventing you from iterating quickly.\n\nUsing a domain knowledge filter for downsampling\n\na simple heuristic rule that cuts down most of the majority class, while keeping nearly all of the minority class.\n\ne.g.¬†if a rule can retain 99% of positives but only 1% of the negatives, this would make a great domain filter\n\nExamples\n\ncredit card fraud prediction: filter for new credit cards, i.e.¬†those without a purchase history.\nspam detection: filter for Emails from addresses that haven‚Äôt been seen before.\ne-commerce product classification: filter for products that contain a certain keyword, or combination of keywords.\nads conversion prediction: filter for a certain demographic segment of the user population.\n\n\n\nCV\n\nIt is extremely important that subsampling occurs inside of resampling. Otherwise, the resampling process can produce poor estimates of model performance.\n\ndata leakage: if you first upsample the data and then split the data into training (aka analysis set) and validation (aka assessment set) folds, your model can simply memorize the positives from the training data and achieve artificially strong performance on the validation data, causing you to think that the model is much better than it actually is.\nThe subsampling process should only be applied to the analysis (aka training) set. The assessment (aka validation) set should reflect the event rates seen ‚Äúin the wild.‚Äù\n\nDoes {recipe} handle this correctly?\n\n\nProcess\n\nSubsample the data only in the analysis set\nPerform CV algorithm selection and tuning using a suitable metric for class imbalance\nUse same metric to get score on the test set\n\n\nML Methods\n\nSynthetic Data Approaches\n\nTabDDPM: Modeling Tabular Data with Diffusion Models (Raschka Thread)\n\nCategorical and Binary Features: adds uniform noise via multinomial diffusion\nNumerical Features: adds Gaussian noise using Gaussian diffusion\n\nSynthetic Data Vault (Docs)\n\nData generated with variational autoencoders adapted for tabular data (TVAE) (paper)\nCan Synthetic Data Boost Machine Learning Performance?\n\n\nImbalanced Classification via Layered Learning (ICLL)\n\nFor code, see article\nA hierarchical model composed of two levels:\n\nLevel 1: A model is built to split easy-to-predict instances from difficult-to-predict ones.\n\nThe goal is to predict if an input instance belongs to a cluster with at least one observation from the minority class.\n\nLevel 2: We discard the easy-to-predict cases. Then, we build a model to solve the original classification task with the remaining data.\n\nThe first level affects the second one by removing easy-to-predict instances from the training set.\n\n\nIn both levels, the imbalanced problem is reduced, which makes the modeling task simpler.\n\n\nTuning parameters (last resort)\n\nXGBoost and LightGBM have a parameter called scale_pos_weight, which up-weighs positive samples when computing the gradient at each boosting iteration\nUnlikely to have a major effect and probably won‚Äôt generalize well.",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/classification.html#sec-class-calib",
    "href": "qmd/classification.html#sec-class-calib",
    "title": "Classification",
    "section": "Calibration",
    "text": "Calibration\n\nCalibrated- When the predicted probabilities from a model match the observed distribution of probabilities for each class.\n\nCalibration Plots visualize this comparison of distributions\n\nCalibration mesasure are important for validating a predictive model.\n\nLogistic Regression models are usually well-calibrated, but most ML and DL model predicted probabilities aren‚Äôt directly produced by the algorithms and aren‚Äôt usually calibrated\nRF models can also benefit from calibration although given enough data, they are already pretty well calibrated\nSVM, Naive Bayes, boosted tree algorithms, and DL models benefit most from calibration\n\nAn unintended consequence of applying calibration modeling can be the worsening of calibration for models that are already well calibrated\nIf a model isn‚Äôt calibrated, then the magnitude of the predicted probability cannot be interpreted as the likelihood of an event\n\ne.g.¬†If 0.66 and 0.67 are two predicted probabilities from an uncalibrated xgboost model, the 0.67 prediction cannot be interpreted as more likely to be an event than the 0.66 prediction.\nMiscalibrated predictions don‚Äôt allow you to have more confidence in a label with a higher probability than a label with a lower probability.\nSee (below) the introduction in the paper, ‚ÄúA tutorial on calibration measurements ‚Ä¶‚Äù for examples of scenarios where calibration of risk scoring model is essential. If you can‚Äôt trust the predicted probabilities (i.e.¬†risk) then decision-making is impossible.\n\nalso this paper which also explains some sources of miscalibration (e.g.¬†dataset from region with low incidence, measurement error, etc.)\n\n\nEven if a model isn‚Äôt caibrated and depending on the metric, it can still be more accurate than a calibrated model.\n\nPoor calibration may make an algorithm less clinically useful than a competitor algorithm that has a lower AUC but is well calibrated (paper)\nCalibration doesn‚Äôt affect the AUROC (does not rely on predicted probabilities) but does affect the Brier Score (does rely on predicted probabilities) (paper)(Harrell)\nSince thresholds are based on probabilities, I don‚Äôt see how a valid, optimized threshold can be established for an uncalibrated model\nWonder how this affects model-agnostic metrics, feature importance, shap, etc.\n\n\n\nMisc\n\nAlso see Diagnostics, Classification &gt;&gt; Calibration\nPackages\n\n{probably} - tidymodels calibration package\n\nCalibrating Binary Probabilities - Nice tutorial\n\n\nNotes from:\n\nHow and When to Use a Calibrated Classification Model with scikit-learn\nCan I Trust My Model‚Äôs Probabilities? A Deep Dive into Probability Calibration\n\nPython, multiclass example\n\nA tutorial on calibration measurements and calibration models for clinical prediction models (paper)\n\nCalibration curves for nested cv (post)\nFor calibrated models, sample size affects the how well they‚Äôre calibrated\n\n\nEven for a logistic model, N &gt; 1000 is desired for good calibration\nFor a rf, closer to N = 10,000 is probably needed.\n\nDistributions of predicted probabilities\n\n\nRandom Forest pushes the probabilities towards 0.0 and 1.0, while the probabilities from the logistic regression are less skewed.\nDecision Trees are even more skewed than RF\nSays how rare a prediction is.\n\nIn a rf, really low or high probability predictions aren‚Äôt rare. So, if the model gives you a 0.93, you shouldn‚Äôt interpet it the way you normally would such a high probability (i.e.¬†high certainty), because a rf inherently pushes its probabilities towards 0 and 1.\n\n\n\n\n\nBasic Workflow\n\nMisc\n\nIdeally you‚Äôd want each model (i.e.¬†tuning parameter combination) being scored in a CV or a nested CV to have its own calibration model, but it‚Äôs not practical. But also, it‚Äôs unlikely an algorithm with a slightly different parameter value combination with have a substantially different predicted probability distribution, and it‚Äôs the algorithm itself that‚Äôs the salient factor. Therefore, for now, I‚Äôd go with 1 calibration model per algorithm.\n\nProcess\n\nSplit data into Training, Calibration, and Test\nFor each algorithm, train the algorithm on the Training set, and create the calibration model using it and the Calibration set.\n\nEach algorithm with have it‚Äôs own calibration model\nSee below, Example: AdaBoost in Py using CV calibrator\n\nI like sklearn‚Äôs ideas for training a calibration model\n\n\nUse the Training set for CV or Nested CV\nFor each split during CV or Nested CV (outer loop)\n\nTrain the algorithm on the training fold, predict on the validation fold, calibrate predictions with calibration model, score the calibrated predictions for that fold\n\nThe calibration model could be used in the tuning process in the inner loop of Nested CV as well\n\nScores should include calibration metrics (See Diagnostics, Classification &gt;&gt; Calibration &gt;&gt; Basic Workflow)\n\nThe rest is normal CV/Nested CV procedure\n\ni.e.¬†average scores across the splits then select the algorithm with the best score. Predict and score algorithm on the Test set\nSee Calibration curves for nested cv for details and code on averaging calibration curves\n\n\n\n\n\nMethods\n\n\nMisc\n\nTLDR; Both Platt Scaling and Isotonic Regression methods are the essentially the same except:\n\nPlatt Scaling uses a logistic regression model as the calibration model\nIsotonic Regression uses an isotonic regression model on ordered data as the calibration model\n\n\nPlatt Scaling\n\nMisc\n\nSuitable for smaller data and for calibration curves with the S-shape.\nMay fail when model is already well calibrated (e.g.¬†logistic regression models)\nPerforms best under the assumption that the predicted probabilities are close to the midpoint and away from the extremes\n\nSo, might be bad for tree models but okay for SVM, Naive Bayes, etc.\n\n\nProcess\n\nSplit data into Training, Calibration, and Test Sets\nTrain your model on the Training Set\nGet the predicted probabilities from your model on the Test Set\nFit a logistic regression using your model‚Äôs predicted probabilities for the Calibration Set as the predictor and the outcome variable in the Calibration Set as the outcome\nCalibrated probabilities are the predicted probabilities of the LR model using your model‚Äôs predicted probabilites on the Test set as new data.\n\nExample: SVM in R\nsvm_platt_recal = svm_platt_recal_func(model_fit, calib_dat, test_preds, test_dat)\n\nsvm_platt_recal_func = function(model_fit, calib_dat, test_preds, test_dat){\n\n¬† # Predict on Calibration Set¬†\n¬† cal_preds_obj &lt;- predict(model_fit,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† calib_dat[, -which(names(calib_dat) == 'outcome_var')],¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† probability = TRUE)¬†\n¬† # e1071 has a funky predict output; just getting probabilities¬†\n¬† cal_preds &lt;- as.data.frame(attr(cal_preds_obj, 'probabilities')[, 2])\n¬† # Create calibration model\n¬† cal_obs_preds_df = data.frame(y = calib_dat$outcome_var, yhat = cal_preds[, 1])\n¬† calib_model &lt;- glm(y ~ yhat, data = cal_obs_preds_df, family = binomial)¬†\n\n¬† # Recalibrate classifiers predicted probabilities on the test set¬† ¬†\n¬† colnames(test_preds) &lt;- c(\"yhat\")\n¬† recal_preds = predict(calib.model, test_preds, type='response')¬† ¬†\n\n¬† return(recal_preds)\n\n}\n\nSee Istotonic Regression for ‚Äúmodel_fit‚Äù, ‚Äúcalib_dat‚Äù, ‚Äútest_preds‚Äù, and ‚Äútest_dat‚Äù\n\nExample: AdaBoost in py\nX_, X_test, y_, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_train, X_calib, y_train, y_calib = train_test_split(X_, y_, test_size=0.4, random_state=42)\n\n# uncalibrated model\nclf = AdaBoostClassifier(n_estimators=50)\ny_proba = clf.fit(X_train, y_train).predict_proba(X_test)\n\n# calibrated model\ncalibrator = LogisticRegression()\ncalibrator.fit(clf.predict_proba(X_calib), y_calib)\ny_proba_calib = calibrator.predict_proba(y_proba)\n\nIsotonic Regression\n\nMore complex, requires a lot more data (otherwise it may overfit), but can support reliability diagrams with different shapes (is nonparametric).\n\nTried on palmer penguins (n = 332) and almost all the probabilites were pushed to the edges\nTried on mlbench::PimaIndiansDiabetes (n = 768). Probabilites were mostly clumped into 3 modes.\nPaper used a simulated (n = 5000) dataset. Probabilities moved a little more towards the center but the movement was much less dramatic that the other two datasets.\nI didn‚Äôt calculate brier scores but I‚Äôd guess you‚Äôd need over a 1000 or so observations for this method to have a significantly positive effect.\n\nLacks continuousness, because the fitted regression function is a piecewise function.\n\nSo, a slight change in the uncalibrated predictions can result in dramatic difference in the calibrated predictions (i.e.¬†a change in step)\n\nProcess\n\nSplits: train (50%), Calibration (25%), Test (25%)\nTrain classifier model on train data\nGet predictions from the classifier on the test data\nCreate calibration model dataset from the calibration data\n\nGet predictions from the classifier on the calibration data\nCreate df with observed outcome of calibration data and predictions on calibration data\nOrder df rows according the predictions column\n\nLowest to largest probabilities in the paper but I‚Äôm not sure it matters\n\n\nFIt calibration model\n\nFit isotonic regression model on calibration model dataset\nCreate a step function using the isotonic regression fitted values and the predictions on the calibration data\n\nCalibrate the classifier‚Äôs predictions on the test set with the step function\n\nExample: SVM in R\nlibrary(dplyr)\nlibrary(e1071)\ndata(PimaIndiansDiabetes, package = \"mlbench\")\n\n# isoreg doesn't handle NAs\n# e1071::svm doesn't identify the event correctly in non-0/1 factor variables\ndat_clean &lt;-¬†\n¬† PimaIndiansDiabetes %&gt;%¬†\n¬† mutate(diabetes = ifelse(as.numeric(diabetes) == 2, 1, 0)) %&gt;%¬†\n¬† rename(outcome_var = diabetes) %&gt;%¬†\n¬† tidyr::drop_na()\n\n# Data splits Training, Calibration, Test (50% - 25% - 25%)\nsmp_size &lt;- floor(0.50 * nrow(dat_clean))\nval_smp_size = floor(0.25 * nrow(dat_clean))\ntrain_idx &lt;- sample(seq_len(nrow(dat_clean)), size = smp_size)\ntrain_dat &lt;- dat_clean[train_idx, ]\ntest_val_dat &lt;- dat_clean[-train_idx, ]\nval_idx &lt;- sample(seq_len(nrow(test_val_dat)), size = val_smp_size)\ncalib_dat &lt;- test_val_dat[val_idx, ]\ntest_dat &lt;- test_val_dat[-val_idx, ]\nrm(list=setdiff(ls(), c(\"train_dat\", \"calib_dat\", \"test_dat\")))\n\n# Fit classifier; predict on Test Set\n# e1071::svm needs a factor outcome, probability=T to output probabilities\nmodel_fit &lt;- svm(factor(outcome_var) ~ .,\n¬† ¬† ¬† ¬† ¬† ¬† data = train_dat,¬†\n¬† ¬† ¬† ¬† ¬† ¬† kernel = \"linear\", cost = 10, scale = FALSE, probability = TRUE)\ntest_preds_obj &lt;- predict(model_fit,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† test_dat[, -which(names(test_dat) == 'outcome_var')],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† probability = TRUE)\n# e1071 has a funky predict output; just getting probabilities\ntest_preds &lt;- as.data.frame(attr(test_preds_obj, 'probabilities')[, 2])\n\n# Predict on Calibration Set\ncal_preds_obj &lt;- predict(model_fit,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† calib_dat[, -which(names(calib_dat) == 'outcome_var')],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† probability = TRUE)\n# e1071 has a funky predict output; just getting probabilities\ncal_preds &lt;- as.data.frame(attr(cal_preds_obj, 'probabilities')[, 2])\n\n# Create Calibration Model dataset\ncal_obs_preds_mtx = cbind(y = calib_dat$outcome_var, yhat = cal_preds[, 1])\n# order training data by predicted probabilities\niso_train_mtx &lt;- cal_obs_preds_mtx[order(cal_obs_preds_mtx[, 2]), ]\n\n# Fit Calibration Model\n# (predicted probabilities, observed outcome)\ncalib_model &lt;- isoreg(iso_train_mtx[, 2], iso_train_mtx[, 1])¬†\n# yf are the fitted values of the outcome variable\nstepf_data &lt;- cbind(calib_model$x, calib_model$yf)¬†\nstep_func &lt;- stepfun(stepf_data[, 1], c(0, stepf_data[, 2]))¬†\n# recalibrate classifiers predicted probabilities on the test set\nrecal_preds &lt;- step_func(test_preds[, 1])\n\nhead(recal_preds, n = 20)\nhist(recal_preds)\nhist(test_preds[, 1])\n\nisoreg doesn‚Äôt handle NAs\nFor a binary classification model that outputs probabilities, e1071::svm needs:\n\nfactored 0/1 outcome variable\nprobability = TRUE\n\n\nExample: AdaBoost in Py using CV calibrator\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn_evaluation import plot\nX, y = datasets.make_classification(10000, 10, n_informative=8,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† class_sep=0.5, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nclf = AdaBoostClassifier(n_estimators=100)\nclf_calib = CalibratedClassifierCV(base_estimator=clf,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† cv=3,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ensemble=False,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† method='isotonic')\nclf_calib.fit(X_train, y_train)\ny_proba = clf_calib.predict_proba(X_test)\ny_proba_base = clf.fit(X_train, y_train).predict_proba(X_test)\nfig, ax = plt.subplots()\nplot.calibration_curve(y_test,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† [y_proba_base[:, 1], y_proba[:, 1]],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† clf_names=[\"Uncalibrated\", \"Calibrated\"],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† n_bins=20)\nfig.set_figheight(4)\nfig.set_figwidth(6)\nfig.set_dpi(150)\n\nCalibratedClassifierCV has both platt scaling (‚Äúsigmoid‚Äù)(default) and isotonic (‚Äúisotonic‚Äù) calibration methods (Docs)\nCalibration model is built using the test fold (aka validation fold)\n\nensemble=True\n\nFor each cv split, the base estimator is fit on the training fold, and the calibration model is built using the ‚Äútest‚Äù fold (aka validation fold)\n\nThe test (aka validation) fold is the calibration data described in the isotonic and platt scaling process sections above\n\nFor prediction, predicted probabilities are averaged across these individual calibrated classifiers.\n\nensemble=False\n\nLOO CV is performed using cross_val_predict, and those predictions are used to train the calibration model.\nThe base estimator is trained using all the data (i.e.¬†training and test (aka validation)).\nFor prediction, there is only one classifier and calibration model combo.\nThe benefit is that it‚Äôs faster, and since there‚Äôs only one combo, it‚Äôs smaller in size as compared to ensemble = True which is k combos. (not as accurate or as well-calibrated though)\n\n\n\nOther forms\n\nFor logistic regression models, adjustment using the Calibration Intercept (and Calibration Slope)\n\nSeems similar to Platt Scaling, but has strong overfitting vibes so caveat emptor\nFor calculating the values, see Diagnostics, Classification &gt;&gt; Calibration &gt;&gt; Evaluation of Calibration Levels &gt;&gt; Weak &gt;&gt; Intercept, Slope\npaper, see supplemental material\nProcedure\n\nThe calibration intercept is added to the fitted model‚Äôs intercept\nThe calibration slope is multiplied times all (nonexponentiated) coefficients of the fitted model (including interactions)\nPredictions are then calculated using the new formula\n\n\nExample\n\nFrom How We Built Our COVID-19 Estimator\nTarget: Risk of Death (probabilities)\nPredictors: Age, Gender, Comorbidities\nModel: XGBoost\n‚ÄúEvery time our model makes a prediction, we compare the result to what the model would have returned for an individual of the same age and sex and only one of the listed comorbidities, or none at all. If the predicted risk is lower than the risk for a comorbidity taken on its own‚Äîsuch as, say, the estimated risk for heart disease alone being greater than the risk for heart disease and hypertension, or the risk for metabolic disorders being lower than the risk of someone with no listed comorbidities‚Äîour tool delivers the higher number instead. We also smoothed our estimates and confidence intervals, using five-year moving averages by age and gender.‚Äù",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/classification.html#sec-class-featimp",
    "href": "qmd/classification.html#sec-class-featimp",
    "title": "Classification",
    "section": "Feature Importance",
    "text": "Feature Importance\n\nMisc\n\nSee notebook and bookmarks for proper algorithms (e.g.¬†permutation importance)¬†to specify for variable importance plots\nPermutation Importance\n\nPermutation importance is generally considered as a relatively efficient technique that works well in practice\nImportance of correlated features may be overestimated\n\nIf you have highly correlated features, use partykit::varimp(conditional = TRUE)\n\nProcess\n\nTake a model that was fit to the training dataset\nEstimate the predictive performance of the model on an independent dataset (e.g., validation dataset) and record it as the baseline performance\nFor each feature i:\n\nrandomly permute feature column i in the original dataset\nrecord the predictive performance of the model on the dataset with the permuted column\ncompute the feature importance as the difference between the baseline performance (step 2) and the performance on the permuted dataset\n\n\n\nVariable Importance plots can be useful for explaining the model to clients. They should never be interpreted as causal. Ex. Real estate: variables that are most influential in determining housing price by this model are aspects of a house that could be emphasized by Realtors to their clients.\nMake sure to use ‚ÄúLossFunctionChange‚Äù importance type in Catboost.\n\nLooks at how much the loss function changes when a feature is excluded from the model.\nDefault method capable of giving importance to random noise\nRequires evaluation on a testing set\n\n\nxgboost\nxg_wf_best &lt;- xg_workflow_obj %&gt;%\nfinalize_workflow(select_best(xg_tune_obj))\nxg_fit_best &lt;- xg_wf_best %&gt;%\nfit(train)\nimportances &lt;- xgboost::xgb.importance(model = extract_fit_engine(xg_fit_best))\nimportances %&gt;%\nmutate(Feature = fct_reorder(Feature, Gain)) %&gt;%\nggplot(aes(Gain, Feature)) +\ngeom_point() +\nlabs(title = \"Importance of each term in xgboost\",\n¬† subtitle = \"Even after turning direction numeric, still not *that* important\")\nUsing vip package\nlibrary(vip)\n\n# xg_wf = xgboost workflow object\nfit(xg_wf, whole_training_set) %&gt;%¬†\npull_workflow_fit() %&gt;%¬†\nvip(num_features = 20)\nglmnet\n\nlin_trained &lt;- lin_wf %&gt;%\n¬† ¬† finalize_workflow(select_best(lin_tune)) %&gt;%\n¬† ¬† fit(train) # or split_obj, test_dat, etc.\n\nlin_trained$fit$fit %&gt;%\n¬† ¬† broom::tidy %&gt;%\n¬† ¬† top_n(50, abs(estimate)) %&gt;%\n¬† ¬† filter(term != \"(Intercept)\") %&gt;%\n¬† ¬† mutate(ter = fct_reorder(term, estimate)) %&gt;%\n¬† ¬† ggplot(aes(estimate, term, fill = estimate &gt; 0)) +\n¬† ¬† geom_col() +\n¬† ¬† theme(legend.position = \"none\")",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html",
    "href": "qmd/db-duckdb.html",
    "title": "DuckDB",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-misc",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-misc",
    "title": "DuckDB",
    "section": "",
    "text": "High performance embedded database for analytics which provides a few enhancements over SQLite such as increased speed and allowing a larger number of columns\n\nFaster than sqlite for most analytics queries (sums, aggregates etc).\n\nVectorizes query executions (columnar-oriented), while other DBMSs (SQLite, PostgreSQL‚Ä¶) process each row sequentially\n\n\nUnlike some other big data tools it is entirely self-contained. (aka embedded, in-process)\n\nNo external dependencies, or server software to install, update, or maintain\n\nCan directly run queries on Parquet files, CSV files, SQLite files, postgres files, Pandas, R and Julia data frames as well as Apache Arrow sources\nResources\n\nAwesome DuckDB - Curated list of libraries, tools, and resources.\n\nExtensions\n\nDocs, List of Official Extensions\n\nTools\n\nSQL Workbench - Query parquet files locally or remotely. Can also produce charts of results. Uses DuckDB-WASM so browser based.\n\nTutorial - Along with explaining the features of the tool, it has complete normalization example and analysis.\nFor visualizations, click the configure button on the right side of the Results sections (bottom main), click Data Grid, choose a chart type, drag column names from the bottom to various areas (similar to Tableau). Click the Reset button in the toolbar close to the configure button to return to Table mode.\nFor tables, if you right-click their name in the Schema pane (far-left), you get a list of options including Summarize which gives summary stats along with uniques and null % for missing data.\nIf tables have foreign keys, data models can be visualized in a mermaid diagram by clicking Data Modes in the bottom-left of the schema panel",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-setup",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-setup",
    "title": "DuckDB",
    "section": "Set-up",
    "text": "Set-up\n\nInstallation: install.packages(\"duckdb\")\nCreate db and populate table from csv\n\nExample \nExample\n# includes filename/id\nwithr::with_dir(\"data-raw/files/\", {\n  dbSendQuery(\n    con, \"\n    CREATE TABLE files AS\n    SELECT *, regexp_extract(filename, '\\\\d{7}') AS file_number\n    FROM read_csv_auto('*Control*File-*.txt', FILENAME = TRUE);\"\n  )\n})",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-dbplyr",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-dbplyr",
    "title": "DuckDB",
    "section": "d/dbplyr",
    "text": "d/dbplyr\n\nExample: Connect, Read in Parallel, and Summarize\ncon &lt;- \n  dbConnect(duckdb(), \n            \":memory:\")\ndf &lt;- \n  dplyr::tbl(con, \n             paste0(\"read_csv('\",\n                    file_name,\n                    \"',\n                    parallel = true,\n                    delim = ',',\n                    header = true,\n                    columns = {\n                        'measurement': 'DOUBLE',\n                        'state': 'VARCHAR'\n                    })\"), \n             check_from = FALSE)\ndf &lt;- df |&gt;\n  summarize(\n    .by = state,\n    mean = mean(measurement),\n    min = min(measurement),\n    max = max(measurement)\n  ) |&gt;\n  collect()\ndf &lt;- NULL\ndbDisconnect(con, shutdown = TRUE)\ngc()\n\nCompetative with running the operation in SQL\n\nExample Connect to db; Write a df to table; Query it\nlibrary(dbplyr)\n\nduck = DBI::dbConnect(duckdb::duckdb(), dbdir=\"duck.db\", read_only=FALSE)\nDBI::dbWriteTable(duck, name = \"sales\", value = sales)\nsales_duck &lt;- tbl(duck, \"sales\")\n\nsales_duck %&gt;%\n  group_by(year, SKU) %&gt;%\n  mutate(pos_sales = case_when(\n          sales_units &gt; 0 ~ sales_units,\n          TRUE ~ 0)) %&gt;%\n  summarize(total_revenue = sum(sales_units * item_price_eur),\n            max_order_price = max(pos_sales * item_price_eur),\n            avg_price_SKU = mean(item_price_eur),\n            items_sold = n())\n\nDBI::dbDisconnect(duck)",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-arrow",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-arrow",
    "title": "DuckDB",
    "section": "Apache Arrow",
    "text": "Apache Arrow\n\nto_duckdb() and to_arrow(): Converts between using {arrow} engine and {duckdb} engieg in workflow without paying any cost to (re)serialize the data when you pass it back and forth\n\nUseful in cases where something is supported in one of Arrow or DuckDB but not the other\n\nBenefits\n\nUtilization of a parallel vectorized execution engine without requiring any extra data copying\nLarger Than Memory Analysis: Since both libraries support streaming query results, we are capable of executing on data without fully loading it from disk. Instead, we can execute one batch at a time. This allows us to execute queries on data that is bigger than memory.\nComplex Data Types: DuckDB can efficiently process complex data types that can be stored in Arrow vectors, including arbitrarily nested structs, lists, and maps.\nAdvanced Optimizer: DuckDB‚Äôs state-of-the-art optimizer can push down filters and projections directly into Arrow scans. As a result, only relevant columns and partitions will be read, allowing the system to e.g., take advantage of partition elimination in Parquet files. This significantly accelerates query execution.\n\nExample (using a SQL Query; method 1)\n# open dataset\nds &lt;- arrow::open_dataset(dir_out, partitioning = \"species\")\n# open connection to DuckDB\ncon &lt;- dbConnect(duckdb::duckdb())\n# register the dataset as a DuckDB table, and give it a name\nduckdb::duckdb_register_arrow(con, \"my_table\", ds)\n# query\ndbGetQuery(con, \"\n  SELECT sepal_length, COUNT(*) AS n\n  FROM my_table\n  WHERE species = 'species=setosa'\n  GROUP BY sepal_length\n\")\n\n# clean up\nduckdb_unregister(con, \"my_table\")\ndbDisconnect(con)\n\nfiltering using a partition, the WHERE format is ‚Äò&lt;partition_variable&gt;=&lt;partition_value&gt;‚Äô\n\nExample (using SQL Query; method 2)\nlibrary(duckdb)\nlibrary(arrow)\nlibrary(dplyr)\n\n# Reads Parquet File to an Arrow Table\narrow_table &lt;- arrow::read_parquet(\"integers.parquet\", as_data_frame = FALSE)\n\n# Gets Database Connection\ncon &lt;- dbConnect(duckdb::duckdb())\n\n# Registers arrow table as a DuckDB view\narrow::to_duckdb(arrow_table, table_name = \"arrow_table\", con = con)\n\n# we can run a SQL query on this and print the result\nprint(dbGetQuery(con, \"SELECT SUM(data) FROM arrow_table WHERE data &gt; 50\"))\n\n# Transforms Query Result from DuckDB to Arrow Table\nresult &lt;- dbSendQuery(con, \"SELECT * FROM arrow_table\")\nExample (using dplyr)\nlibrary(duckdb)\nlibrary(arrow)\nlibrary(dplyr)\n\n# Open dataset using year,month folder partition\nds &lt;- arrow::open_dataset(\"nyc-taxi\", partitioning = c(\"year\", \"month\"))\n\nds %&gt;%\n  # Look only at 2015 on, where the number of passenger is positive, the trip distance is\n  # greater than a quarter mile, and where the fare amount is positive\n  filter(year &gt; 2014 & passenger_count &gt; 0 & trip_distance &gt; 0.25 & fare_amount &gt; 0) %&gt;%\n  # Pass off to DuckDB\n  to_duckdb() %&gt;%\n  group_by(passenger_count) %&gt;%\n  mutate(tip_pct = tip_amount / fare_amount) %&gt;%\n  summarize(\n    fare_amount = mean(fare_amount, na.rm = TRUE),\n    tip_amount = mean(tip_amount, na.rm = TRUE),\n    tip_pct = mean(tip_pct, na.rm = TRUE)\n  ) %&gt;%\n  arrange(passenger_count) %&gt;%\n  collect()\n\nIn the docs, the example has to_duckdb after the group_by. Not sure if that makes a difference in speed.¬†\n\nExample (Streaming Data)\n# Reads dataset partitioning it in year/month folder\nnyc_dataset = open_dataset(\"nyc-taxi/\", partitioning = c(\"year\", \"month\"))\n\n# Gets Database Connection\ncon &lt;- dbConnect(duckdb::duckdb())\n\n# We can use the same function as before to register our arrow dataset\nduckdb::duckdb_register_arrow(con, \"nyc\", nyc_dataset)\n\nres &lt;- dbSendQuery(con, \"SELECT * FROM nyc\", arrow = TRUE)\n# DuckDB's queries can now produce a Record Batch Reader\nrecord_batch_reader &lt;- duckdb::duckdb_fetch_record_batch(res)\n\n# Which means we can stream the whole query per batch.\n# This retrieves the first batch\ncur_batch &lt;- record_batch_reader$read_next_batch()",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-sql",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-sql",
    "title": "DuckDB",
    "section": "SQL",
    "text": "SQL\n\nMisc\n\nDocs\n\nExample: Connect, Read in Parallel, and Query\nsqltxt &lt;- paste0(\n  \"select\n        state, min(measurement) as min_m,\n        max(measurement) as max_m,\n        avg(measurement) as mean_m\n  from read_csv('\", file_name, \"',\n        parallel = true,\n        delim = ',',\n        header = true,\n        columns = {\n            'measurement': 'DOUBLE',\n            'state': 'VARCHAR'\n        }\n  )\n  group by state\"\n)\ncon &lt;- \n  dbConnect(duckdb(), \n            dbdir = \":memory:\")\ndbGetQuery(con, \n           sqltxt)\ndbDisconnect(con, \n             shutdown = TRUE)\ngc()\n\nFastest method besides polars for running this operation in this benchmark\n\nStar Expressions\n\nAllows you dynamically select columns\n-- select all columns present in the FROM clause\nSELECT * FROM table_name;\n-- select all columns from the table called \"table_name\"\nSELECT table_name.* FROM table_name JOIN other_table_name USING (id);\n-- select all columns except the city column from the addresses table\nSELECT * EXCLUDE (city) FROM addresses;\n-- select all columns from the addresses table, but replace city with LOWER(city)\nSELECT * REPLACE (LOWER(city) AS city) FROM addresses;\n-- select all columns matching the given expression\nSELECT COLUMNS(c -&gt; c LIKE '%num%') FROM addresses;\n-- select all columns matching the given regex from the table\nSELECT COLUMNS('number\\d+') FROM addresses;",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-remcon",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-remcon",
    "title": "DuckDB",
    "section": "Remote Connections",
    "text": "Remote Connections\n\nMisc\n\nNotes from\n\nQuery Remote Parquet Files with DuckDB\n\n\nhttpfs Extension\n\nCreate a db in memory since the data is stored remotely.\nconn &lt;- \n  DBI::dbConnect(\n    duckdb::duckdb(),\n    dbdir = \":memory:\"\n  )\nInstall and Load httpfs extension\nDBI::dbExecute(conn, \"INSTALL httpfs;\")\nDBI::dbExecute(conn, \"LOAD httpfs;\")\n\nCurrently not available for Windows\n\nQuery\nparquet_url &lt;- \"url_to_parquet_files\"\nres &lt;- DBI::dbGetQuery(\n  conn, \n  glue::glue(\"SELECT carrier, flight, tailnum, year FROM '{parquet_url}' WHERE year = 2013 LIMIT 100\")\n)\n\nQueries that needs more data and return more rows takes longer to run, especially transmitting data over the Internet. Craft carefully your queries with this in mind.\n\nTo use {dplyr}, a View must first be created\nDBI::dbExecute(conn, \n               glue::glue(\"CREATE VIEW flights AS SELECT * FROM PARQUET_SCAN('{parquet_url}')\"))\nDBI::dbListTables(conn)\n#&gt; [1] \"flights\"\n\ntbl(conn, \"flights\") %&gt;%\n  group_by(month) %&gt;%\n  summarise(freq = n()) %&gt;%\n  ungroup() %&gt;%\n  collect()\nClose connection: DBI::dbDisconnect(conn, shutdown = TRUE)\n\n{duckdbfs}\n\nCreate dataset object\nparquet_url &lt;- \"url_to_parquet_files\" #e.g. AWS S3\nds &lt;- duckdbfs::open_dataset(parquet_url)\nQuery\nds %&gt;%\n  group_by(month) %&gt;%\n  summarise(freq = n()) %&gt;%\n  ungroup() %&gt;%\n  collect()",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-ext",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-ext",
    "title": "DuckDB",
    "section": "Extensions",
    "text": "Extensions\n\nVS Code extension\n\nConnect to a local DuckDB instance\nCreate new in-memory DuckDB instance\nView DuckDB tables, columns, and views\nRun SQL queries on open DuckDB connections\nAttach SQLite database files to in-memory DuckDB instances\nQuery remote CSV and Parquet data files with DuckDB HTTPFS extension\nCreate in-memory DuckDB tables from remote data sources and query results\nManage DuckDB connections in SQLTools Database Explorer\nAutocomplete SQL keywords, table names, column names, and view names on open database connections in VSCode SQL editor\nSave named SQL query Bookmarks\nUse SQL Query History\nExport SQL query results in CSV and JSON data formats\nintegrate with the equally spiffy SQL Tools extension\n\nJSON extension\n\nExample: From hrbrmstr drop\nINSTALL 'json';\nLOAD 'json';\n\nCOPY (\n  SELECT * FROM (\n    SELECT DISTINCT\n      cve_id,\n      unnest(\n        regexp_split_to_array(\n          concat_ws(\n            ',',\n            regexp_extract(case when cweId1 IS NOT NULL THEN cweId1 ELSE regexp_replace(json_extract_string(problem1, '$.description'), '[: ].*$', '') END, '^(CWE-[0-9]+)', 0),\n            regexp_extract(case when cweId2 IS NOT NULL THEN cweId2 ELSE regexp_replace(json_extract_string(problem2, '$.description'), '[: ].*$', '') END, '^(CWE-[0-9]+)', 0)\n          ),\n          ','\n        )\n      ) AS cwe_id\n    FROM (\n      SELECT \n        json_extract_string(cveMetadata, '$.cveId') AS cve_id, \n        json_extract(containers, '$.cna.problemTypes[0].descriptions[0]') AS problem1,\n        json_extract(containers, '$.cna.problemTypes[0].descriptions[1]') AS problem2,\n        json_extract_string(containers, '$.cna.problemTypes[0].cweId[0]') AS cweId1,\n        json_extract_string(containers, '$.cna.problemTypes[0].cweId[1]') AS cweId2\n      FROM \n        read_json_auto(\"/data/cvelistV5/cves/*/*/*.json\", ignore_errors = true) \n    )\n    WHERE \n      (json_extract_string(problem1, '$.type') = 'CWE' OR\n       json_extract_string(problem2, '$.type') = 'CWE')\n    )\n  WHERE cwe_id LIKE 'CWE-%'\n) TO '/data/summaries/cve-to-cwe.csv' (HEADER, DELIMETER ',')\n\nProcesses a nested json\nClones the CVE list repo, modify the directory paths and run it. It burns through nearly 220K hideous JSON files in mere seconds, even with some complex JSON operations.\n\nDBs\n\nMySQL, Postgres, SQLite\n\nMight need to use FORCE INSTALL postgres\n\nAllows DuckDB to connect to those systems and operate on them in the same way that it operates on its own native storage engine.\nUse Cases\n\nExport data from SQLite to JSON\nRead data from Parquet into Postgres\nMove data from MySQL to Postgres\nDeleting rows, updating values, or altering the schema of a table in another DB\n\nNotes from\n\nMulti-Database Support in DuckDB\n\nHas other examples including transaction operations\n\n\nExample: Open SQLite db file\nATTACH 'sakila.db' AS sakila (TYPE sqlite);\nSELECT title, release_year, length FROM sakila.film LIMIT 5;\n\nATTACH opens the db file and TYPE says that it‚Äôs a SQLite db file\nMultiple dbs without using TYPE\nATTACH 'sqlite:sakila.db' AS sqlite;\nATTACH 'postgres:dbname=postgresscanner' AS postgres;\nATTACH 'mysql:user=root database=mysqlscanner' AS mysql;\nIn python\nimport duckdb\ncon = duckdb.connect('sqlite:file.db')\n\nExample: Switch between attached dbs\nUSE sakila;\nSELECT first_name, last_name FROM actor LIMIT 5;\n\nUSE switches from the previous db to the ‚Äúsakila‚Äù db\n\nExample: View all attached dbs\nSELECT database_name, path, type FROM duckdb_databases;\nExample: Copy table from one db type to another\nCREATE TABLE mysql.film AS FROM sqlite.film;\nCREATE TABLE postgres.actor AS FROM sqlite.actor;\nExample: Joins\nSELECT first_name, last_name\nFROM mysql.film\nJOIN sqlite.film_actor ON (film.film_id = film_actor.film_id)\nJOIN postgres.actor ON (actor.actor_id = film_actor.actor_id)\nWHERE title = 'ACE GOLDFINGER';",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-relational.html",
    "href": "qmd/db-relational.html",
    "title": "Relational",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Relational"
    ]
  },
  {
    "objectID": "qmd/db-relational.html#sec-db-rel-misc",
    "href": "qmd/db-relational.html#sec-db-rel-misc",
    "title": "Relational",
    "section": "",
    "text": "Packages\n\n{dplyr}\n\ncompute stores results in a remote temporary table\ncollect retrieves data into a local tibble.\ncollapse doesn‚Äôt force computation, but instead forces generation of the SQL query.\n\nsometimes needed to work around bugs in dplyr‚Äôs SQL generation.\n\n\n{dm}\n\nCan join multiple tables from a db, but keeps the meta info such as table names, primary and foreign keys, size of original tables etc.\n\n\nBenchmarks\n\nClickBench ‚Äî a Benchmark For Analytical DBMS\n\nRelational databases do not keep all data together but split it into multiple smaller tables. That separation into sub-tables has several advantages:\n\nAll information is stored only once, avoiding repetition and conserving memory\nAll information is updated only once and in one place, improving consistency and avoiding errors that may result from updating the same value in multiple locations\nAll information is organized by topic and segmented into smaller tables that are easier to handle\n\nOptimized for a mix of read and write queries that insert/select a small number of rows at a time and can handle up to 1TB of data reasonably well.\nThe main difference between a ‚Äúrelational database‚Äù and a ‚Äúdata warehouse‚Äù is that the former is created and optimized to ‚Äúrecord‚Äù data, whilst the latter is created and built to ‚Äúreact to analytics‚Äù.\nTypes\n\nEmbedded aka In-Process (see Databases, Engineering &gt;&gt; Terms): DuckDB (analytics) and SQLite (transactional)\nServer-based: postgres, mysql, SQL Server\n\nMix of transactional and analytical\nDistributed SQL (database replicants across regions or hybrid (on-prem + cloud)\n\nmysql, postgres available for both in AWS Aurora (See below)\npostgres available using yugabytedb\nSQL Server on Azure SQL Database\nCloud Spanner on GCP\n\n\n\nApache Avro\n\nRow storage file format unlike parquet\nA single Avro file contains a JSON-like schema for data types and the data itself in binary format\n4x slower reading than csv but 1.5x faster writing than csv\n1.7x smaller file size than csv\n\nWrapper for db connections (e.g.¬†con_depA &lt;- connect_databaseA(username = ..., password = ...) )\n# ... other stuff including code for \"connect_odbc\" function\n\n# connection attempt loop\nwhile(try &lt; retries) {\n¬† ¬† con &lt;- connect_odbc(source_db = \"&lt;database name&gt;\"\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† username = username,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† password = password)\n¬† ¬† if(class(con) == \"NetexxaSQL\") {\n¬† ¬† ¬† ¬† try &lt;- retries + 1\n¬† ¬† } else if (!\"NetezzaSQL\" %in% class(con) & try &lt; retries {\n¬† ¬† ¬† ¬† warning(\"&lt;database name&gt; connection failed. Retrying...\")\n¬† ¬† ¬† ¬† try &lt;- try + 1\n¬† ¬† ¬† ¬† Sys.sleep(retry_wait)\n¬† ¬† } else {\n¬† ¬† ¬† ¬† try &lt;- try + 1\n¬† ¬† ¬† ¬† warning(\"&lt;database name&gt; connection failed\")\n¬† ¬† }\n}\n\nGuessing ‚ÄúNetezzaSQL‚Äù is some kind of error code for a failed connection to the db\n\nBenchmarks\n\nExample\n\nData\n\n~54,000,000 rows and 6 columns\n10 .rds files with gz compression is 220MB total,\n\nIf they were .csv, 1.5 GB\n\nSQLite file is 3 GB\nDuckDB file is 2.5 GB\nArrow creates a structure of directories, 477 MB total\n\nOperation: read, filter, group_by, summarize\nResults\n##¬† format¬† ¬† ¬† ¬† ¬† median_time mem_alloc\n##¬† &lt;chr&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† &lt;bch:tm&gt; &lt;bch:byt&gt;\n## 1 R (RDS)¬† ¬† ¬† ¬† ¬† ¬† ¬† 1.34m¬† ¬† 4.08GB\n## 2 SQL (SQLite)¬† ¬† ¬† ¬† ¬† 5.48s¬† ¬† 6.17MB\n## 3 SQL (DuckDB)¬† ¬† ¬† ¬† ¬† 1.76s¬† 104.66KB\n## 4 Arrow (Parquet)¬† ¬† ¬† 1.36s¬† 453.89MB\n\nTradional relational db solutions balloon up the file size\n\nSQLite 2x, DuckDB 1.66x (using csv size)",
    "crumbs": [
      "Databases",
      "Relational"
    ]
  },
  {
    "objectID": "qmd/db-relational.html#sec-db-rel-brands",
    "href": "qmd/db-relational.html#sec-db-rel-brands",
    "title": "Relational",
    "section": "Brands",
    "text": "Brands\n\nSQLite vs MySQL as transactional dbs (article)\n\nSQLite:\n\nEmbedded, size ~600KB\nLimited data types\nBeing self-contained, other clients on a network would not have access to the database (no multi-users) unlike with MySQL\nNo built-in authentication that is supported\nMultiple processes are able to access the database at the same time, but making changes at the same time is not something supported\nUse Cases\n\nData being confined in the files of the device is not a problem\nNetwork access to the db is not needed\nApplications that will minimally access the database and not require heavy calculations\n\n\nMySQL:\n\nopposites of the sqlite stuff\nSize ~600MB\nsupports replication and scalability\nSecurity is a large; built-in features to keep unwanted people from easily accessing data\nUse cases\n\ntransactions are more frequent like on web or desktop applications\nif network capabilities are a must\nmulti-user access and therefore security and authentication\nlarge amounts of data\n\n\n\nMySQL\n\nInstallation docs\nBasic intro\nSee SQL notebook\n\nSQLite\n\n{RSQLite}\n\nCloud SQL - Google service to provide hosting services for relational dbs (see Google, BigQuery &gt;&gt; Misc). Can use postgres, mysql, etc. on their machines.\n\nCloud SQL Insights - good query optimization tool\n\nAWS RDS for db instances (see Database, postgres &gt;&gt; AWS RDS)\n\nAvailable: Amazon Aurora, MySQL, MariaDB, postgres, Oracle, Microsoft SQL Server\nRDS (Relational Database Service)\n\nBenefits over hosting db on EC2: AWS handles scaling, availability, backups, and software and operating system updates\n\n\nAWS Aurora - MySQL- and PostgreSQL-compatible enterprise-class database\n\nStarting at &lt;$1/day.\nSupports up to 64TB of auto-scaling storage capacity, 6-way replication across three availability zones, and 15 low-latency read replicas.\nCreate MySQL and Postgres instances using AWS Cloudformation",
    "crumbs": [
      "Databases",
      "Relational"
    ]
  },
  {
    "objectID": "qmd/docker-fundamentals.html",
    "href": "qmd/docker-fundamentals.html",
    "title": "14¬† Docker, Fundamentals",
    "section": "",
    "text": "TOC\n\n‚ÄúA Cloud Guru: Docker Fundamentals‚Äù\nMisc\nCreating a Dockerfile\nImages\nDocker Hub\nRunning Containers\nNetworking Containers\nData Volumes\nOptimizing container file size\nRunning scripts when a container starts\nDocker utility functions\nDocker Compose\nManaging a web app with Docker-Compose\n\nMisc\n\n\n\nMisc\n\nNotes from the course, ‚ÄúA Cloud Guru: Docker Fundamentals‚Äù\nResources\n\nThe Ultimate Docker Cheatsheet\nCourse: Container Essentials: Build, Deploy, Scale\n\nMore advanced topics: swarm config, multi-container, app deployment, security, alt docker runtime\n\n\nTo start a Docker project, run docker init\n\nIntro\nExecute the command in the target project folder and it will generate docker files according to the programming language. Currently, the only DS language supported is Python.\nSets up the essential scaffolding for your project.\n\nA .dockerignore¬†to keep unwanted files out,\nA Dockerfile tailored to your project‚Äôs needs,\nA compose.yaml¬†for managing multi-container setups\nA README.Docker.md¬†for documentation.¬†\n\n\nDocker file linting tools\n\nhadolint\nSynk\nTrivy\nClaire\nAnchore\n\nDocker help commands (every command preceded by ‚Äúdocker  ‚Äù)\n\nEvery management command has its own subset of commands associated with it\nmanagement\n\ncontainer\nimage\nnetwork\nnode\nplugin\nsecret\nservice\nstack\nswarm\nsystem\nvolume\nExample docker image --help\n\nshows sub-commands for management command image and short descriptions\nFor even more information on a sub-command, add ‚Äìhelp after the sub-command\n\nExample: docker image build --help\n\n\n\n\nLinux OS stuff\n\nDebian is a very stable Linux distribution\n\nDebian-Jessie is the latest version of Debian\n\n(As of July 6 2019, it‚Äôs called buster)\nSlim is a light-weight Debian-Jessie\n\nDebian-Wheezy is the version before Jessie\n\nAlpine is a very small Linux distribution (much smaller than even Slim)\n\nAn instance of an image or the result of running an image is called container\n\nAny changes made while running the container is lost once that container has been stopped.\n\nIf you create or add a file while in the container, stop the container, rerun the container, then it will no longer be there.\n\n\nAn image is the setup of the virtual computer. A combination of a file system and parameters. A package that rolls up everything you need to run an application.\n\nComposed of stacked layers where the layers are self-contained files\nYou can download, build, and run image, but they cannot be changed (immutable)\n\nYou can have many running containers of the same image.\nDocker Hub is a registry for docker repositories. It‚Äôs like a github for images. Each repo has 1 image but the repo can store many tagged versions of that image (version control)\nPull and run a docker image from Docker Hub from local cli\n\ndocker run url/repo/image\n\nexample: docker run docker.io/library/hello-world\n\ndocker.io = docker hub\nlibrary = repo name for all ‚Äúofficial‚Äù images\nhello-world = name of image\n\n\n\nTwo ways to create a image\n\nWhile inside a container, make changes. Then use commit command to create the new image with the changes\n\nNever used. Creating a dockerfile is superior.\n\nUsing a dockerfile\n\nimages on docker hub\n\nmdancho/h2o-verse\n\nh20, tidyverse, tensorflow, keras¬†\n~2 GB\n\n\n\nCreating a dockerfile (example of a toy python flask app)\n\nArchitecture¬†\n\nstart with FROM base:tag\n\nEssentially copy-pastes the base image\nusually good to start with a base image\n\nFROM python:2.7-alpine\n\npython is the base image and 2.7-alpine is the tag\n\n\n\nRUN executes commands or scripts as if you were in the container‚Äôs OS\n\nRUN mkdir /app\n\nmakes a directory called ‚Äúapp‚Äù\n\n\nWORKDIR sets the working directory for everything that happens after this command\n\nWORKDIR /app\n\nCOPY¬†has source path+file (local) and a destination path+file (container) as args\n\nCOPY requirements.txt requirements.txt\n\nthe first requirements.txt is in the same dir as the docker file so no ‚Äú/other-dir/‚Äù required\nthe second ‚Äúrequirements.txt‚Äù says the file is to be placed into the /app dir\n\nequivalent to ‚Äú/app/requirements.txt‚Äù because /app is our working directory\n\n\nCannot use .. to move above the dockerfile directory. Every path must be below it.\n\nInstall packages\n\nRUN pip -install¬† -r requirements.txt\n\ninside requirements.txt says Flask==0.12\n\n\nCopy entire local directory to the working directory\n\nCOPY . .\n\nfirst period says copy everything is the current local directory\nsecond period says put everything in the working dir\n\n\nLABELs have key-value pairs. can be useful in production settings. The can be retrieved with a command later on.¬†\n\nSome uses:\n\nfilter a container‚Äôs output based on a label ¬†\ncan include scripts for automatic load balancing (also see aws load balancer section below)\n\nLABEL maintainer=‚ÄúEric Book ericbook@email.com‚Äù\\\n\n\n\n\n¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†version=‚Äú1.0‚Äù¬†¬† ¬†\n\nOften changed or added to, so they should be close to the end of the dockerfile (but not the last line)\nCMD gives the default instruction when the image gets ran which is different from RUN commands, which are executed when the image is built.\n\nCMD flask run ‚Äìhost=0.0.0.0 ‚Äìport=5000\nUnder-the-hood: CMD instructions are arguments to an entrypoint script and get run through a default docker entrypoint (see waaaaay below for info about entrypoints)\n\nA \\\\ is Linux operator that chains together instructions so they can be in separate lines of code for easier reading. Think a space also does the same thing, but you can‚Äôt see it in the code.\nThe ordering of the inputs in the dockerfile will affect its size and efficiency\n\nThe source code is much more likely to change in the future than the package dependencies. Therefore even though there would be fewer COPY commands, it‚Äôs best to install the dependencies before copying the source code.\nWhenever changes to the source code occur Docker has a caching mechanism, so that it doesn‚Äôt rebuild everything above the layer where the changes occur. COPY . . executes in millisecs so the rebuild happens almost instantly while installing dependencies could take minutes.\n\nAttaching packages and libraries\n\nDownload and install R packages\n\nRUN R -e ‚Äúinstall.packages(c(‚Äòshinydashboard‚Äô, ‚Äòreticulate‚Äô, ‚Äòshiny‚Äô))‚Äù\n\nDownload, install Python libraries (ubuntu image)\n\nRUN ln -s /usr/bin/python3 /usr/bin/python && \\\n\n\n\n¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬† ¬†¬†¬† ¬†¬†¬†¬†¬† ¬†¬†¬† ¬†¬†¬†ln -s /usr/bin/pip3 /usr/bin/pip¬†¬† ¬†\n\nRUN¬†apt-get update\nRUN apt-get install -y libpython-dev\nRUN apt-get install -y libpython3-dev\nImages\n\nBuild image from dockerfile (** don‚Äôt forget the dot at the end **)\n\ndocker image build -t web1 .\n\nimage is the management command\nbuild is the sub-command of image\n-t is the flag for ‚Äútag‚Äù\nweb1 is the tag. This allows us to to refer to this image as ‚Äúweb1‚Äù instead of a hash\n. says build the image and place in current directory\n\nEnd of build shows hash id ‚Äúsuccessfully built ‚Äù\ndocker image build -t web1:1.0\n\nversioning the image\nfor just ‚Äúweb1‚Äù the version will be web1:latest\n\n\nInspect the image\n\ndocker image inspect web1\ninfo in json format\nAt the top complete hash id\nshows the versions of the image under ‚Äúrepotag‚Äù\nvarious info about how the layers were created\nthe number of layers created is shown at the bottom by the lines preceded by a ‚Äúsha‚Äù and a hash\n\nList of images in local docker host\n\ndocker image ls\n\nbase images loaded from dockerfiles listed alongside the images we create\n images are ‚Äúdangling images.‚Äù They are failed image builds or images that were built on top of already existing images\n\nsafe to delete; frees up disk space\n\n\n\nDelete¬†local image\n\nUsing name and version tag\n\ndocker image rm web1:1.0\n\nUsing id\n\ndocker image rm -f 633f\n\n-f is the ‚Äúforce‚Äù flag\n\nnecessary when image has been tagged/copied (like when pushing to docker hub) and you want to remove both images.\n\n633f - only need the first four characters of the hash id\n\nids in docker image ls\n\n\n\n\n\nDocker Hub\n\nLogin to Docker Hub\n\nHave to do it once then a config file is created so you don‚Äôt have to do it again\nInput your username and password for your Docker Hub acct\ndocker login\n\nPush image to Docker Hub\n\nTag image with docker hub username\n\ndocker image tag web1 ercbk/web1:latest\n\nweb1 is the image we want to push\nercbk/web1:latest\n\nercbk is the username\nweb1 is the repo\nlatest is the tag\n\n\ndocker image ls will show the newly tagged image\n\nPush image to docker hub\n\ndocker image push ercbk/web1:latest\n\n\n\nRunning Containers\n\n** Unless you include -d in the run command, you will need to open a separate terminal (i.e.¬†different from the one you ran the run command in) in order run more docker cli commands while an app is running **\n** container names and ports need to be unique for each running container **\ndocker container ls\n\nlists all running containers, container_id, image, creation time, runtime, name\n-a\n\nshows all stopped containers\n\n\ndocker container rm &lt;name&gt;\n\ndeletes a stopped container\ncan also use 1st four characters of &lt;container_id&gt;\n\nshown in ls (see above)\n\n\ndocker container run\n\nthe basic run command\nhardly ever want to use just the basic commmand\n\nsee flags and examples below\n\n\ndocker container stop &lt;name&gt;\n\nstops a container\nadd more s to stop more than one container\n\nwith spaces between the names\n\nalso, ctrl + c\n\nuse it in the same terminal you used the run command in.\nonly works if you included the -it flag when you started it.\n\nalso see ‚Äìrm below\ndocker container stop $( docker container ls -q)\n\nStops all running containers\n-q flag says list them quietly, so it doesn‚Äôt print them out\nif you a ‚Äústop requires at least one argument‚Äù error, then there aren‚Äôt any containers running\n\n\ndocker container logs &lt;name&gt;\n\ncan also use 1st four characters of &lt;container_id&gt;\nfor active or inactive containers\nshows times container was accessed\n-f\n\nruns log in the foreground (i.e.¬†you can view accesses in real-time)\nctrl + c to kill it\n\n\ndocker container stats\n\nreal-time monitoring of active containers\nshows cpu and memory usage for each container\nnetwork input/output\n\nflags\n\n** Don‚Äôt think order matters except for that the name of the image needs to be last **\n-d\n\nruns the container in the background\nallows you to run commands in the same terminal window that you used the container run command in\n\n-e\n\nallows you to pass an environment variable into the container\nNot always necessary\n\nFlask requires one (e.g.¬†-e FLASK_APP=app.py)\n\napp.py is the name of the app script\n\n\nmultiple -e flags are allowed\n\n-e FLASK_DEBUG=1\n\nturns debug mode on when you run the container\nuse along with -v to make real-time changes to the app while the container is running\n\n\n\n-it\n\nallows for unix commands such as ctrl c to kill a process (such as a running container)\nmakes the docker container interactive. Which allows you to go into the container, navigate the file system, and make changes.\n\n‚Äìname\n\ndocker automatically provide a name for a container but this flag allows you to provide your own\n‚Äìname moose\n\n-p¬†\n\nports\n\nmap ports from local machine to ports within docker\n\nexpected to supply two ports separated by a colon\n-p 5000:5000\n\nThe first 5000 is the host port. The port you use to interact with the app in your browser (e.g.¬†localhost:5000)\n\nAll containers running on your docker host need to have unique ports to run on. (e.g.¬†-p 5001:8000)\nSo, I think this can be any port you want as long something else isn‚Äôt already using it.\n\nThe second 5000 is the container port that was specified in the dockerfile\n\nTried 5000:5000 and the app didn‚Äôt run in the browser even when the dockerfile had 5000 specified, but after specifying 800 in the dockerfile, -p 5000:8000 did work\n\nThis was run on the default docker network\n\nRunning -p 5000:5000 DID work on a custom network.\n\nAlso these are 2 different apps. The 1st one was stand-alone (03-lecture cloud guru docker fundamentals), and the 2nd ran in conjunction with a redis server (see custom network section below)(09-lecture in cloud guru docker fundamentals). Not sure if that makes a difference\n\n\n\nWhen running the container, if it‚Äôs an app, it can be viewed in a browser at localhost:5000 (i.e.¬†the first port specified)\n-p 5000\n\ndocker will attach a random port number\n\nYou can specify more than one port mapping e.g.¬†-p 5000:5000 -p 3838:3838\nports (second port specified) for common images\n\nredis (storage) - 6379\nnginx (web server) - 80\n\nopen source, handles a lot of connections efficiently, used to host a lot of websites\n\nRStudio: -p 3838:3838\nShiny apps launched from within RStudio: -p 8787:8787\n\n\n‚Äìrestart\n\nwith value, on-failure\n\nsays restart the container if there‚Äôs some catastrophic failure (e.g.¬†docker daemon dies and restarts)\nuseful in production\n\ncannot be included if ‚Äìrm is also used\n\n‚Äìrm\n\ndeletes the container once it has been stopped\ncannot be included if ‚Äìrestart is also used\n\n-v\n\nvolumes\nUse cases\n\nwhile developing. Makes changes inside the container in real-time.\nstore data in a db\nMapping to a folder on your local machine allows you to work on projects stored within the container\n\n(1st half) requires address (local machine) to the directory with the script (e.g.¬†app script not dockerfile) where you‚Äôre making changes  (2nd half) address inside the container where this directory should be ‚Äúmounted‚Äù (i.e.¬†where the script you‚Äôre making changes to is located inside the container)\n\n1st half\n\nFor Linux, you can used the shortcut, ‚Äú$PWD‚Äù which is a linux environment variable that stores the path to the working directory. It has the same value as the command ‚Äúpwd -L‚Äù which Prints Working Directory.\n\nfyi the -L has something to do with ‚Äúsymlinks‚Äù which are thinks that can be created that point to another directory. If you‚Äôre in in the symlink directory -L will print that directory and -P will print the directory that the symlink points to.\nsame thing for Mac, except the quotes might need to be included\n\nFor windows, ‚Äú/c/users/&lt;user_name&gt;/path/to/directory‚Äù\n\n2nd half\n\n‚Äú/app‚Äù which is where the app script is (also the working directory specified in the dockerfile for the example)\n\nExample in Linux, docker container run -it -p 5000:5000 -e FLASK\\_APP=app.py --rm -name web1 -e FLASK\\_DEBUG=1 -v $PWD:/app web1\nExample in Windows,¬†docker container run --rm -itd -p 5000:5000 -e FLASK\\_APP=1 -e FLASK\\_DEBUG=1 --name web2 -v /C/Users/tbats/Documents/R/Projects/Docker/Courses/cloud-guru-docker-fund/09-linking-containers-with-docker-networks:/app web2\n\nWhile running, if you open a new terminal and docker container inspect &lt;name&gt;, then there should be a ‚Äúmount‚Äù section with type = ‚Äúbind‚Äù\nTroubleshooting if changes don‚Äôt show up in the container in real-time\n\nit‚Äôs probably the path specifications for the value of the -v tag. Docker is picky, esp w/WIndows.\nMake sure code script and docker are on same drive\nNext try replacing Alpine Linux with Slim Linux in the dockerfile ‚Äì&gt; rebuild image ‚Äì&gt; run container with volume flag + the addresses like stated above\n\nSomething about inotify in Alpine not doing something\n\nsee exec command below\n\n\n\nExamples:\n\ndocker container run -it -p 5000:8000 -e FLASK\\_APP=app.py web1\n\nweb1 is the name of the image.\nThis is probably least number of flags necessary to run an app in a container\n\ndocker container run -it --rm --name web2 -p 5000:8000 -e FLASK\\_APP=app.py -d --restart on-failure web2\n\nexec\n\nexecutes an interactive bash session in the container\nContainer must be running, so open a new terminal window and type the code line:\n\ndocker container exec -it web1 bash\n\nwhere web1 in the name of the container\nbash is for the Slim distribution of Linux; use sh for Alpine\n\nOr run a bash command detached in the background docker exec -t -d web1 bash \"ls -al\"\n\nyou‚Äôll be logged in under ‚Äúroot‚Äù and dropped into wherever you designated the working directory in the dockerfile\nType ls -la to view the files\nExample of a debug\n\nPython app isn‚Äôt showing changes after running it with a volume flag (-v)\nyou exec a bash session inside the container\ndelete some .pyc files (might be corrupted somehow) that are created when python runs flask\n\nrm *.pyc\nls -la to confirm\n\ngo to local script and make changes, save\ngo to terminal window where container is running and see changes to scripts detected in the terminal\ngo to browser where app is running and refresh\nwait a few secs and changes show up. Yay.\ngo back to bash terminal window, ctrl + d to kill it\n\n\n\nNetworking Containers\n\nInternal networks (LANs), external networks (WAN)\n\nWAN is a wide area network. Can be public or private. Stretches across city or industrial park, etc\n\nAccess addresses\n\nservers bound to 0.0.0.0: give access to any computers on your LAN or WAN\nif localhost:, then only laptop running the server can connect to it\nif : then any computer on your LAN, WAN, or on the internet can connect\n\ne.g.¬†192.168.1.4:5000\n\n\nList of Docker networks\n\ndocker network ls\nipconfig (windows) ifconfig (Linux, Mac)\n\nshows info about networks\n\nbridge network is docker0 which is the docker supplied network\n\nfyi docker0 didn‚Äôt show up on Windows for me\n\n\nPing from one container to another\n\nNote: ping and ifconfig removed from Alpine and Slim image\n\nto reinstall, add this to 2nd line of dockerfile\n\nAlpine\n\nRUN apk update && apk add iputils\n\nSlim\n\nRUN apt-get update && apt-get install -y net-tools iputils-ping\n\n\n\ndocker exec web2 ping 172.17.0.2\n\nwhere 172.17.0.2 is the other container‚Äôs eth0 inet address\n\nfound by docker exec &lt;container name&gt; ifconfig\n\n\nctrl + c to stop the pinging\n\nView etc file\n\ndocker exec &lt;container name&gt; cat /etc/hosts\nshows eth0 inet address is mapped to container id\n\nCreate custom network\n\nallows us to connect containers by name which means if the addresses change, the apps won‚Äôt break.\ndocker network create --driver bridge &lt;name&gt;\n\nthe bridge driver is used for networking containers on the same docker host\nfor networking across multiple docker hosts, the overlay driver is used. (would need to research this further)\n\nInspect network\n\ndocker network inspect &lt;name&gt;\n\nadd container to custom network\n\nadd ‚Äìnet  to the run-container instruction\n\nexample:¬†docker container run -it --rm name web2 -p 5000:5000 -e FLASK\\_APP=app.py -d --net firstnetwork web2\nexample:¬†docker container run -it --rm name redis -p 6379:6379 -d --net firstnetwork redis:3.2-alpine\n\nThis containers only linked up with the app running on the browser when they were run on the custom network and not the default docker bridge network\nin debug mode:¬†docker container run -it --name web2 -p 5000:5000 -e FLASK\\_APP=app.py -e FLASK\\_DEBUG=1 -v /C/Users/tbats/Documents/R/Projects/Docker/Courses/cloud-guru-docker-fund/09-linking-containers-with-docker-networks:/app -d --rm --net firstnetwork web2\n\ncontainers will show up when you inspect firstnetwork\ncan now ping using container names (assuming both containers have been added to the network)\n\ndocker exec web2 ping redis\n\nweb2 is the container doing the pinging\nredis being pinged¬†\n\n\n\n\n\n\nData Volumes\n\nallows data to persist on docker host after the container is stopped\n\nShould save on the host for apps because they should be portable\nFor databases, not so bad\n\nA volume is nothing more than a folder on your computer that is linked to a folder inside the Docker container.\nDefault volume path on host, ‚Äú/var/lib/docker/volume/‚Äù\ndocker volume create web2\\_redis\n\nweb2_redis is the name of the volume\n\ngood idea to pick a name that‚Äôs relevant to job\n\n\ndocker volume ls\n\nlist of volumes\n\ndocker volume inspect web2\\_redis\n\nshows info about volume\n‚ÄúMountpoint‚Äù shows where the volume will be stored on the host machine\n\ndocker volume rm &lt;volume1 name&gt; &lt;volume2 name&gt;\n\nremove specific volumes by name\n\ndocker volume prune\n\nremoves all volumes\n\ndocker rm -v &lt;container name&gt;\n\nremoves container and anonymous volume\n\nWill not remove a named volume\n\n-v required else a ‚Äúdangling‚Äù is created\n\ndocker volume ls -qf dangling=true docker volume rm $(docker volume ls -qf dangling=true)\n\nremoves dangling volumes\n\nAdd volume flag, data volume name, and destination to container\n\ndocker container run -it --rm name redis -p 6379:6379 -d --net firstnetwork -v web2\\_redis:/data¬† redis:3.2-alpine\n\nweb2_redis is the name we gave to the data volume\n/data is designated by the redis people\n\nThis technique works for mysql, postgres, elasticsearch, etc. You just have to figure out the WHERE they decided that they want you to store your data (i.e.¬†the /data part)\n\ngo to their docker hub image page ‚Äì&gt; view readme ‚Äì&gt; look for section on persistent storage\nExample I went to postgres page and did an ctrl+f ‚Äúpersistent‚Äù and found a section describing when to use /data or /pgdata\n\n\n\n\nSaving the data\n\nredis does it automatically every 30 sec\nManual save if you need to save right away:\n\ndocker exec redis redis-cli SAVE\n\n\nExample: Named Volume\n\n\n¬† ¬† ¬† ¬† ¬† ¬† version: '3.8'\n¬† ¬† ¬† ¬† ¬† ¬† services:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† db:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† image: mysql\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† restart: always\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† environment:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† MYSQL_ROOT_PASSWORD: root\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† MYSQL_DATABASE: test_db\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ports:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - \"3306:3306\"\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† volumes:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - db_data:/var/lib/mysql\n¬† ¬† ¬† ¬† ¬† ¬† volumes:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† db_data:\n\n‚Äúdb_data‚Äù is the name\n‚Äú/var/lib/mysql‚Äù is the path inside the container\nAdvantages\n\nData persists after we restart or remove a container\nAccessible by other containers\n\nExample: Unnamed (aka Anonymous) Volume\n\n¬† ¬† ¬† ¬† ¬† ¬† version: '3.8'\n¬† ¬† ¬† ¬† ¬† ¬† services:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† db:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† image: mysql\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† restart: always\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† environment:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† MYSQL_ROOT_PASSWORD: root\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† MYSQL_DATABASE: test_db\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ports:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - \"3306:3306\"\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† volumes:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - /var/lib/mysql\n\nNo volume name here but still has the path inside the container\nData will persist on restart of the container, but not after the container is stopped and removed\nNot accessible by other containers\nActually‚Ä¶ even without the ‚Äúvolumes‚Äù instruction, the ‚Äúmysql‚Äù image/dockerfile has a ‚ÄúVOLUME‚Äù instruction, so an anonymous volume would still be created.\nExample (Bind Mounts)\n\n¬† ¬† ¬† ¬† ¬† ¬† version: '3.8'\n¬† ¬† ¬† ¬† ¬† ¬† services:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† db:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† image: mysql\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† restart: always\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† environment:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† MYSQL_ROOT_PASSWORD: root\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† MYSQL_DATABASE: test_db\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ports:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - \"3306:3306\"\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† volumes:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - $PWD/data:/var/lib/mysql\n\nInstead of using the default host directory for the volume, you can specify a location yourself\nfirst half (before colon) : where on the host machine to mount (i.e.¬†create) the volume\n\nIn the example, it‚Äôs located in the working directory + /data\nFor Linux, you can used the shortcut, ‚Äú$PWD‚Äù which is a linux environment variable that stores the path to the working directory. It has the same value as the command ‚Äúpwd -L‚Äù which Prints Working Directory.\n\nfyi the -L has something to do with ‚Äúsymlinks‚Äù which are thinks that can be created that point to another directory. If you‚Äôre in in the symlink directory -L will print that directory and -P will print the directory that the symlink points to.\nsame thing for Mac, except the quotes might need to be included\n\nFor windows, ‚Äú/c/users/&lt;user_name&gt;/path/to/directory‚Äù\n\nsecond half (after colon): specify the path inside the container you want to use for the volume. Usually specified by the db software.\n\nIn the example, it‚Äôs ‚Äú/var/lib/mysql‚Äù which has been specified by the mysql image\n\nSharing data between containers\n\nContainers sharing and receiving need to be on the same docker host\nAdd to app dockerfile\n\nVOLUME [‚Äú/app/public‚Äù]\n\nLine location in the dockerfile wasn‚Äôt specified but he put his right before the CMD line\nlocal directory with app has a folder named ‚Äúpublic‚Äù that will be shared with other containers.\n\n\nSteps\n\nbuild app image\nrun app image\nrun redis image with flags, ‚Äìvolume -from ¬†\n\ndocker container run -it --rm name redis -p 6379:6379 -d --net firstnetwork -v web2\\_redis:/data --volume -from web2¬† redis:3.2-alpine\n\n\nFiles in app‚Äôs public folder will be in redis‚Äôs container\n\nSteps\n\ngo into redis container\n\ndocker container exec -it redis sh\n\nchange directory to the public folder\n\ncd /app/public\n\ncontents of app‚Äôs public folder are in this folder too\n\nexamine contents by printing to the terminal\n\ncat main.css\n\n\n\n\nWhile containers are running, you can exec into volume container (e.g.¬†web2) add files, make changes to files, etc., and the files in the other containers will be updated in real time.\nAlternate method (*not recommended for production*)\n\nDon‚Äôt put the VOLUME instruction in the dockerfile\nAdd -v /app/public to the app run command\nAdd the ‚Äìvolume -from flags to the redis container run command like before\n\n\nOptimizing container file size\n\n.dockerignore\n\nContains file paths to files in the local project directory that you don‚Äôt want on the image (e.g.¬†files with private data, .git files can be huge)\nDuring the image build, when docker runs the COPY/ADD instructions it will bypass the files in the .dockerignore file\nFile paths in the .dockerignore file begin wherever you‚Äôve designated the working directory in WORKDIR in the dockerfile\nExamples:\n\n.git/\n\nadding a trailing ‚Äú/‚Äù isn‚Äôt necessary but lets people know it‚Äôs a directory and not a file\n\nmay want to include the .dockerignore itself\nfolder/*\n\nThe folder itself will be added to the image but all the content will be ignored\n\n**/*.txt\n\nsays ignore all files with the .txt extension\n\n!name.txt\n\nthis grants an exception to the name.txt file. It will be added to the image even with the **/*.txt\n\nYou can negate the .dockerignore file\n\nJust a * on line 1\nbegin each file path with a ‚Äú!‚Äù to specify which files to include\n\n\n\nRemoving the build dependency files of the system dependency files (** only for Alpine Linux **)\n\nIn other words the files used to build the system dependency files\nHe uses the example of a postgres dependency in the video, but no dependencies are actually listed in the dockerfile in the files folder.\n\n2 hypotheses on what he‚Äôs talking about:\n\nThe package you use in python to interact with the sql db has dependencies and files are needed to build those dependencies (build dependencies). After the dependencies are built, the build dependencies get deleted.\nThis image with include the sql db and that db has build dependencies. They get deleted\n\nI think it‚Äôs 1., but I dunno. In the Installing packages section above, there are examples of python-dev files being installed, but I‚Äôm not sure which hypothesis that favors.\n\n\n\nIt‚Äôs a bear of a bash script, so see dockerfile for details in cloud guru docker fund course, 012 lecture files folder on optimizing\n\nscript stays above the COPY . . instruction\nIf you use this script, thoroughly check everything and make sure the packages are working as intended.\n\nSteps\n\nfirst line (indent of the lines is the same in the file)\n\n\n\n\nRUN apk add --no-cache --virtual .build-deps \\\n\nsecond line, add dependencies\n\n¬† ¬† postgressql-dev dependency2 dependency3 \\\n\nThen add the rest of the bash gunk\n\n¬† ¬† && pip install -r requirements.txt \\\n¬† ¬† && find /usr/local \\\n¬† ¬† ¬† ¬† \\( -type d -a -name test -o -name tests \\) \\\n¬† ¬† ¬† ¬† -o \\( -type f -a -name '*.pyc' -o -name '*.pyo' \\) \\\n¬† ¬† ¬† ¬† -exec rm -rf '{}' + \\\n¬† ¬† && runDeps=\"$( \\\n¬† ¬† ¬† ¬† scanelf --needed --nobanner --recursive /usr/local \\\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† | awk '{ gsub(/,/, \"\\nso:\", $2); print \"so:\" $2 }' \\\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† | sort -u \\\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† | xargs -r apk info --installed \\\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† | sort -u \\\n¬† ¬† )\" \\\n¬† ¬† && apk add --virtual .rundeps $runDeps \\\n¬† ¬† && apk del .build-deps\n\nRunning scripts when a container starts\n\nInstead of making multiple similar dockerfiles/images, you can use entry points¬†into one dockerfile/image\n\nExamples:\n\nFor a postgressql image, you pass a run instruction inside the dockerfile that sets up your user authorization and password as an environment variable. It also has an ENTRYPOINT, so that if you have other projects that use a postgres sql db, they can gain access to that information through the entry point.\nRunning a db migration after a container starts\n\nrun_db_migration1 as environment variable but with a default value set to off. You can control that action through entrypoint and a script\n\ncontrol stuff in a nginx config to set an external ip after the container is running\n\n\nThe docker_entrypoint scripts aren‚Äôt in your dockerfile so they don‚Äôt add layers to an image\nSteps\n\nAdd lines to dockerfile\n\n\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† COPY docker-entrypoint.sh /\n\ndocker-entrypoint.sh should be a file in the root project directory\n\nNot in app directory, because it‚Äôs best practice to keep entrypoint files separate.\nFor details see section below\n\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† RUN chmod +x /docker-entrypoint.sh\n\ntells Linux to give the entrypoint script permission to run\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ENTRYPOINT [\"/docker-entrypoint.sh\"]\n\nentry point instruction that points to where the script is located.\nRun redis container as before\nBuild app image as before (gave it the name, ‚Äúwebentrypoint‚Äù)\nRun app container\n\ndocker container run -it --name webentrypoint -p 5000:5000 -e FLASK_APP=app.py -e FLASK_DEBUG=1 --rm --net firstnetwork webentrypoint\n\nname and image values changed to webentrypoint\nremoved -d flag because we want it to run in the foreground\nIf you go to localhost:5000 it has some message printed from the docker-entrypoint.sh script\nStop the app container\nRe-run the app container\n\ndocker container run -it --name webentrypoint -p 5000:5000 -e FLASK_APP=app.py -e FLASK_DEBUG=1 -e WEB2_COUNTER_MSG=\"Docker fans will have visited this page\" --rm --net firstnetwork webentrypoint\n\nWEB2_COUNTER_MSG is an environment variable that was given a default value inside the docker-entrypoint.sh script.\nSo we were able to change the environment variable without having to run the container, exec into the container, then change it.\nDetails on the dockerentrypoint.sh file\n\nWhen you run the container, this script gets run before the dockerfile\nExports an environment variable that gets accessed by the app.py script\n\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† #!/bin/sh\n\nComment that tells us that we‚Äôre running a shell script\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† set -e\n\nsays abort if there‚Äôs an error in the script\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† echo \"The Dockerfile ENTRYPOINT has been executed!\"\n\nLine that prints in the terminal when we run the container\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† export WEB2_COUNTER_MSG=\"${WEB2_COUNTER_MSG:-carbon based life forms have sensed this website}\"\n\nWhere the custom scripting takes place\nIn this case, WEB2_COUNTER_MSG environment variable is created and a default value set\n\nI think the syntax after the = has something to do with making the value a default value\n\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† exec \"$@\"\n\nSays that after running everything in this script, then run the CMD stuff in the dockerfile\nDocker utility functions (can be run in any directory)\n\ndocker system df\n\nshows resources being used by docker\nadding -v tag at the end, produces a verbose output all the info is broken down further\n\ndocker system info\n\ninfo about your docker installation\n\nuseful when reporting bugs, creating issues\nverify docker installation\n\n\ndocker system prune\n\ndeletes all the crud\n\nstopped containers that you forgot to include ‚Äìrm\nvolumes not used by at least one container\nnetworks not used by at least one container\nall dangling images\n\nadd -f flag to execute the prune without the confirmation message\n\nuseful for automating through cron jobs, etc.\n\nadd -a flag to remove ALL unused images\n\n**only run if want every image not being used by a running container to be deleted**\n\n\n\nDocker Compose (note dash between docker and compose in commands)\n\ndocker-compose.yml properties\n\nyaml files do not need to be in the same directory as your dockerfiles. You just have to give the path in the build property (see below)\n\n\n\nversion: '3'\n\n\n# pound sign is for comments\nservices:\n¬† redis:\n¬† ¬† image: 'redis:3.2-alpine'\n¬† ¬† ports:\n¬† ¬† ¬† - '6379:6379'\n¬† ¬† volumes:\n¬† ¬† ¬† - 'redis:/data'\n\n¬† web:\n¬† ¬† build: '.'\n¬† ¬† depends_on:\n¬† ¬† ¬† - 'redis'\n¬† ¬† env_file:\n¬† ¬† ¬† - '.env'\n¬† ¬† ports:\n¬† ¬† ¬† - '5000:5000'\n¬† ¬† volumes:\n¬† ¬† ¬† - '.:/app'\n\n¬† worker:\n¬† ¬† build: '.'\n¬† ¬† command: celery &lt;command&gt;\n¬† ¬† depends_on:\n¬† ¬† ¬† - 'redis'\n¬† ¬† env_file:\n¬† ¬† ¬† - '.env'\n¬† ¬† volumes:\n¬† ¬† ¬† - '.:/app'\n\nvolumes:\n¬† redis: {}\n\nversion is the version of the compose api\nservices are the containers we‚Äôre building\n\nservice names (e.g.¬†redis, web) will end up being the container and image names (see below for the exception)\nimage property\n\nuses the base:tag format like the value for the FROM instruction in the dockerfile\n\nbuild property\n\n‚Äò.‚Äô says build an image from the current directory\nIf your app has it‚Äôs own folder then you need to specify the path\n\nexample ‚Äò./web‚Äô¬†\n\n\nimage and build property in the same service\n\ndocker will build 2 of the same image\n\none with project name (build) and the other with the image value as the name for the image\n\nexample build: ‚Äò.‚Äô and image: ‚Äôercbk/web:1.0\n\nsays build image using dockerfile in current directory and name it ercbk/web:1.0\nuseful if the image is going to be pushed to the docker hub\n\n\nports\n\nports to be used for the container (see -p flag in running containers section for more details)\n\nthe ‚Äúbind‚Äù port (right side) supplied here needs to match the dockerfile\n\na  indicates a list in yamls.\n\nexample: forward 2 sets of ports\n\n\n\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - '6379:6379'\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - '5348:5348'\n\nnetwork\n\ntakes list inputs\nused when multiple networks are created\n\nexample: databases/workers communicate on one network while apps communicate on both networks\n\n\n\n¬† result:\n¬† ¬† build: ./result\n¬† ¬† command: nodemon --debug server.js\n¬† ¬† volumes:\n¬† ¬† ¬† - ./result:/app\n¬† ¬† ports:\n¬† ¬† ¬† - \"5001:80\"\n¬† ¬† networks:\n¬† ¬† ¬† - front-tier\n¬† ¬† ¬† - back-tier\n\n¬† worker:\n¬† ¬† build: ./worker\n¬† ¬† networks:\n¬† ¬† ¬† - back-tier\n\n¬† db:\n¬† ¬† image: postgres:9.4\n¬† ¬† volumes:\n¬† ¬† ¬† - \"db-data:/var/lib/postgresql/data\"\n¬† ¬† networks:\n¬† ¬† ¬† - back-tier\n\nvolumes\n\nsee data volumes section for more details\n means, just like for ports, values for this property are in list format\nIn the app service,\n\nspecify which directory to share\n\n‚Äô.:/app‚Äù says share the current local directory which will be the app directory in the container\n\n\nin the redis service,¬†\n\nweb2_redis is the name we give to the data volume\n/data is designated by the redis people\n\n\ndepends_on\n\nnecessary if one service depends on another. Indicates if one container needs to start before another.\n\nexample: web (app) depends on redis (db)\n\ntakes a value in list format\n\nenvironment\n\nmethod 1 for setting environment variables\nname: value pair\n\nexample:\n\n\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† environment:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† FLASK_DEBUG: 'true'\n\nenv_file\n\nmethod 2 takes a list of environment files to load\nloads from top to bottom\n\nif any variables in a lower listed file match those in a file listed higher up, then the values in the earlier file get overwritten to the values in the later file\nuseful if you have a stock env file, decide to put the containers into production, then you can just add a production env file to the directory and the yaml\n\nexample: .env is the name of the file in the current directory ( is in the actual name)\nfile example\n\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† COMPOSE_PROJECT_NAME=web2\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† PYTHONBUFFERED=true¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† FLASK_APP=app.py\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† FLASK_DEBUG=1\n\nThe flask variables are the ones we passed in the run containers section\nCan configure Docker Compose options here\n\nexample: supply a project name. Otherwise Docker would just use the current directory for the name of the project. Also gets added as a prefix for networks, etc. that Compose will create.\n\nThe Python buffer variable is necessary if you want to see the output from the terminal through Compose\nworker (service)\n\nUse the same dockerfile for both services\n\neg worker uses the same dockerfile as web in the example docker-compose.yml\n\nuseful for background services for your app\n\ncelery is a python library used this task\n\ndifferences\n\nno need for port to be exposed (therefore overriding the CMD instruction in the dockerfile)\nuses command property which replaces the CMD instruction in the dockerfile\n\n\nvolumes\n\nthe name given here needs to match the name of the volume given in the services property\n\nexample: for this yaml the name of the volume created is ‚Äúredis‚Äù\n\n{} - curly brackets are for adding options to the volume, such as being read-only (see docker docs for more options)\n\nin this example no options are supplied so it‚Äôs empty (still necessary though)\n\n\nManaging a web app with Docker-Compose¬†(note dash between docker and compose in commands)\n\nCurrent directory need to have the docker-compose.yml file\n\nunless you use the -f flag\n\ndocker-compose --help\n\ninfo on commands\n\ndocker-compose build\n\nbuilds an image of any service in the yaml file with a build property\ndocker image ls will show the built images with the project name as the prefix\n\nproject name set in the env file (see above)\n\n\ndocker-compose pull\n\npulls any other images specified with the image property in the yaml file\n\n¬†not just local but also pulls from docker hub I think\n\n\ndocker-compose up\n\nruns the project (everything created with project name prefix)\n\ncreates network\n\neg web2_defaults\n\ncreates volume\n\neg web2_redis\n\nstarts containers\n\neg web2_redis_1, web2_web_1\n\nthe ‚Äú_1‚Äù suffix is in case the project calls for multiple instances of the same container\n\nreminder: multiple apps require different ports (binding?) (see -p in running-a-container sections). Would also require setting up a load balancer. (see aws load balancer section below)\n\n\n\n\ndocker-compose up &lt;service&gt;\n\nstarts a specific service\nif you start a service with a dependency (‚Äúdepends_on‚Äù specified in yaml), it will also start that dependency service.\ndocker compose web\n\nstarts web but also redis, because web has a specified redis dependency\n\n\n\ndocker-compose stop\n\nstops all containers\n\ncan probably specify a container\n\neg docker-compose stop web\n\n\ncan also use ctrl+c, but (as of 2017) there‚Äôs a bug that throws an error and aborts instead\n\ndocker-compose up --build -d\n\n¬†runs both build and up commands at once\n-d says run in the background\n\ndocker-compose ps\n\nsimilar info as docker container ls but presented slightly differently\n\ndocker-compose logs -f\n\nsince containers running in the background -f needed to logs of the container activity\nIt‚Äôs a realtime log of all containers running in the project, so will require ctrl+c to exit\n\ndocker-compose restart\n\nrestarts all containers\n¬†can also just specify one container\n\neg docker-compose restart redis\n\n\ndocker-compose exec web\n\nfor a running container, you can execute commands inside the container from the outside\n\ndocker-compose exec web ls -la\n\nshows file contents inside container\n\n\nopening a shell inside the container\n\ndocker-compose exec web sh\n\nno -it flag necessary like for docker container exec (see running-a-container section)\n\n\nexit\n\nexits the shell\n\n\ndocker-compose run &lt;service&gt; &lt;command&gt;\n\nallows you to instantly run the container, execute a command inside the container, and exit the shell\nequivalent to the command sequence: up ‚Äì&gt; exec ‚Äì&gt; exit\nexample: docker-compose run redis redis-server ‚Äìversion\n\ndocker-compose rm\n\ndeletes all containers (only stopped ones I assume)\nThese will also be removed using the docker system prune from the docker-utility-functions section",
    "crumbs": [
      "Docker",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Docker, Fundamentals</span>"
    ]
  },
  {
    "objectID": "qmd/docker-misc.html",
    "href": "qmd/docker-misc.html",
    "title": "15¬† Docker, Misc",
    "section": "",
    "text": "15.1 BuildKit",
    "crumbs": [
      "Docker",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Docker, Misc</span>"
    ]
  },
  {
    "objectID": "qmd/docker-misc.html#buildkit",
    "href": "qmd/docker-misc.html#buildkit",
    "title": "15¬† Docker, Misc",
    "section": "",
    "text": "Allows you to use external caching sources and build mounts to speed up image builds through caching (requires Docker version ‚â•18.09)\n\nAble to supply a previously built image in your registry where Docker will check the manifest of the image, and pull any layers that can be used as local cache.\n\nNotes from Fast Docker Builds With Caching\nMust have the environment variable, DOCKER_BUILDKIT=1\nExternal Cache\n\nDocs\nExample (single stage build)\n\n\nDOCKER_BUILDKIT=1 docker build \\\n¬† --cache-from my-repo.com/my-image \\\n¬† --build-arg BUILDKIT_INLINE_CACHE=1 \\\n\nUse --build-arg BUILDKIT_INLINE_CACHE=1 and --cache-from arguments when building the image\n\n‚Äúmy-repo.com/my-image‚Äù is the url of the image you want Docker to pull dependencies (aka layers) that can be used as a local cache\n\nExample (multi-stage build)\n\nexport DOCKER_BUILDKIT=1\nIMAGE=my-repo.com/my-image\n# Build image for the build stage\ndocker build \\\n¬† --target build-stage \\\n¬† --cache-from \"$[{IMAGE}]{style='color: #990000'}:build-stage\" \\\n¬† --tag \"$[{IMAGE}]{style='color: #990000'}:build-stage\" \\\n¬† --build-arg BUILDKIT_INLINE_CACHE=1 \\\n¬† .\n# Build the final image\ndocker build \\\n¬† --cache-from \"${IMAGE_NAME}:build-stage\" \\\n¬† --cache-from \"${IMAGE_NAME}:latest\" \\\n¬† --tag \"${IMAGE_NAME}:latest\" \\\n¬† --build-arg BUILDKIT_INLINE_CACHE=1 \\\n¬† .\n\n# Push the build-stage image too so that it can be reused for cache\ndocker push \"${IMAGE_NAME}:build-stage\"\ndocker push \"${IMAGE_NAME}:latest\"\n\nThis shell script that gets referenced in the docker file (another example in this note; search for ‚Äúshell script‚Äù)\n‚Äúexport‚Äù creates the environment variable; IMAGE is a variable storing the URL of the externally cached image\n--target¬† in the first build command to stop at the build-stage stage, and that\nThe second build command referenced both the build-stage and latest images as cache sources\nBuild Mounts\n\nThis type of caching is only available:\n\nlocally and cannot be reused across machines\nduring a single RUN instruction, so you need to either:\n\ncopy the files to a different location in the image before the RUN instruction finishes (e.g., with cp) or\nCOPY the cache directory from another image, e.g., a previously built build-stage image.\n\n\nSee the article for an example\n\nCredentials\n\nSteps\n\nPrepare an auth.toml file with your credentials\n\nExample (poetry LIB credentials for installing deps from a private repo)\n\n\n\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† [http-basic]\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† [http-basic.my_repo]\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† username = \"my_username\"\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† password = \"my_ephemeral_password\"\n\nPlace it outside of your Docker context or exclude it in .dockerignore (the cache would still get invalidated otherwise).\nUpdate your Dockerfile to include ‚Äú# syntax=docker/dockerfile:1.3‚Äù as the very first line, and\nAdjust install commands that require the credentials (e.g.¬†poetry install command becomes:)\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† RUN --mount=type=secret,id=auth,target=/root/.config/pypoetry/auth.toml \\\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† poetry install --no-dev --no-interaction --remove-untracked\n\nbuild the image with docker build --secret id=auth,src=auth.toml ...",
    "crumbs": [
      "Docker",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Docker, Misc</span>"
    ]
  },
  {
    "objectID": "qmd/docker-misc.html#multi-stage-dockerfile",
    "href": "qmd/docker-misc.html#multi-stage-dockerfile",
    "title": "15¬† Docker, Misc",
    "section": "15.2 Multi-Stage Dockerfile",
    "text": "15.2 Multi-Stage Dockerfile\n\nMisc\n\nWith multi-stage builds, you compile and build everything in an initial stage, and then, in a separate stage, you copy over just the necessary artifacts. This results in a much leaner, more efficient final image. It‚Äôs not only good practice for keeping image sizes down, but it also means quicker deployments and reduced storage costs.\nNotes from\n\nUsing multi-stage builds to make your docker image 10x smaller\n\n\nExample: 2-Stage\n\nFROM ubuntu:20.04 AS final\nFROM ubuntu:20.04 as build\n# BUNDLE LAYERS\nRUN apt-get update -y && apt install -y --no-install-recommends \\\n¬† curl \\\n¬† osmium-tool \\\n&& rm -rf /var/lib/apt/lists/*\nRUN mkdir /osmfiles \\\n&& mkdir /merged \\\n&& curl http://download.geofabrik.de/europe/monaco-latest.osm.pbf -o /osmfiles/monaco.osm.pbf \\\n&& curl http://download.geofabrik.de/europe/andorra-latest.osm.pbf -o /osmfiles/andorra.osm.pbf \\\n&& osmium merge /osmfiles/monaco.osm.pbf /osmfiles/andorra.osm.pbf -o /merged/merged.osm.pbf\n\nFROM final\nRUN mkdir /merged\nCOPY --from=build /merged /merged\n\nStage 1: build\n\nStarts at FROM ubuntu:20.04 as build\nDownloads a couple geospatial files, then merges them and stores them in the ‚Äúmerged‚Äù folder\n\nStage 2: final\n\nStarts at FROM final\nCreates a ‚Äúmerged‚Äù dir and copies merged file from stage 1 (build) to the ‚Äúmerged‚Äù dir\nThe curl and osmium-tool dependencies that are installed in Stage 1 are not included in Stage 2 which reduces the size of the final image.\n\nI‚Äôm not sure if FROM ubuntu:20.04 AS final being the first line (instead of replacing the FROM final line) is necessary or not. It looks kind of redundant.\nIf a slimmer ubuntu image is used in the last stage, the size of the image can reduced further",
    "crumbs": [
      "Docker",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Docker, Misc</span>"
    ]
  },
  {
    "objectID": "qmd/docker-misc.html#optimizations",
    "href": "qmd/docker-misc.html#optimizations",
    "title": "15¬† Docker, Misc",
    "section": "15.3 Optimizations",
    "text": "15.3 Optimizations\n\nResources\n\nVideo: Dockerfile: From Start to Optimized (DockerCon 2023) - Best practices for setting up Dockerfile and optimizing the build.\n\nOptimize the Dockerfile\nMulti-stage build\nSetting testing\nBuilding for multiple platforms",
    "crumbs": [
      "Docker",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Docker, Misc</span>"
    ]
  },
  {
    "objectID": "qmd/python-classes.html",
    "href": "qmd/python-classes.html",
    "title": "Classes",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-misc",
    "href": "qmd/python-classes.html#sec-py-class-misc",
    "title": "Classes",
    "section": "",
    "text": "When to use classes:\n\nWhen you have a group of functions and they have many of the same arguments, this indicates a class might be helpful. Also, if one or more of the functions is used in the other functions, this is also an indication that creating a class would be better.\n\nSee Python, Snippets &gt;&gt; Refactor a group of functions into a class\n\nWhen you have code and data that go together and need to keep track of the current state\n\ne.g.¬†managing a bunch of students and grades\n\nWhen you see hierarchies, using classes leads to better code organization, less duplication, and reusable code.\n\nYou can make a single change in the base class (parent) and all child classes pick up the change\nExample: Report class\n\nYou can have a base class with shared attributes like report name, location and rows. But when you go into specifics like formats (xml, json, html), you could override a generate_report method in the subclass.\n\n\nEncapsulation\n\nWhen you want to separate external and internal interfaces in order to (ostensibly) hide internal code from the user.\nKeeps excess complexity from the user\n\n\nBest Practices\n\nUse camel case for class names\nUse snake case for methods and attributes\nAlways use self as the first argument of a method\nWrite docstrings for your classes so that your code is more understandable to potential collaborators and future you.\n\nCreate a Class that allows method chaining\n\n\nreturn self is what allows the chaining to happen",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-terms",
    "href": "qmd/python-classes.html#sec-py-class-terms",
    "title": "Classes",
    "section": "Terms",
    "text": "Terms\n\nClass inheritance - mechanism by which one class takes on the attributes and methods of another\nclass Employee:\n¬† ¬† def __init__(self, name, salary=0):\n¬† ¬† ¬† ¬† self.name = name\n¬† ¬† ¬† ¬† self.salary = salary\n__init__() is a constructor method. It assigns (or initializes) attributes that every object (aka instance) for this class must have.\nself is the 1st argument in any method definition. It refers to a particular instance.\nself.salary¬† = salary creates an attribute called¬†salary¬†and assigns to it the value of the¬†salary¬†parameter (default set to 0)\nClass attributes are attributes that have the same value for all class instances.\n\nAccessing a class attribute\n# access first employee's name attribute\ne1.name\n# access second employee's salary attribute\ne2.salary\n\nInstance Attributes - Values for these attribute depend on the instance (e.g.¬†they vary depending on each employee)\nInstantiate - Creating a new object from a class\ne1 = Employee(\"yyy\", 5000)¬† # name, salary\ne2 = Employee(\"zzz\", 8000)",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-meth",
    "href": "qmd/python-classes.html#sec-py-class-meth",
    "title": "Classes",
    "section": "Methods",
    "text": "Methods\n\nMisc\n\nIn most classes, best practice to at least include __init__ and __repr__ methods\n\n\n\nInstance Method\n\nFunctions that can only be called by an object from this class\n\nSimilar to regular functions with the difference of having ‚Äúself‚Äù as the first parameter\n\nclass Employee:\n¬† ¬† def __init__(self, name, salary=0):\n¬† ¬† ¬† ¬† self.name = name\n¬† ¬† ¬† ¬† self.salary = salary\n\n#Instance method\n¬† ¬† def give_raise(self, amount):\n¬† ¬† ¬† ¬† self.salary += amount\n¬† ¬† ¬† ¬† return f\"{self.name} has been given a {amount} raise\"\n\n# calling an instance method\n# instantiate object first\nobject = MyClass()¬†\nobject.method()\n\n\n\nDunder Methods\n\naka Magic or Special Methods\nResources\n\nEvery dunder method in Python\n\nAren‚Äôt meant to be called, usually invoked by an operation\n\nExamples\n\n__add__ invoked by myclass() + myclass()\n__str__ invoked by str(myclass())\n\nExample\nclass Address:\n¬† ¬† def __init__(self, street, city, state, zipcode, street2=''):\n¬† ¬† ¬† ¬† self.street = street\n¬† ¬† ¬† ¬† self.street2 = street2\n¬† ¬† ¬† ¬† self.city = city\n¬† ¬† ¬† ¬† self.state = state\n¬† ¬† ¬† ¬† self.zipcode = zipcode\n¬† ¬† def __str__(self):\n¬† ¬† ¬† ¬† lines = [self.street]\n¬† ¬† ¬† ¬† if self.street2:\n¬† ¬† ¬† ¬† ¬† ¬† lines.append(self.street2)\n¬† ¬† ¬† ¬† lines.append(f'{self.city}, {self.state} {self.zipcode}')\n¬† ¬† ¬† ¬† return '\\n'.join(lines)\n\n&gt;&gt;&gt; address = Address('55 Main St.', 'Concord', 'NH', '03301')\n&gt;&gt;&gt; print(address)\n55 Main St.\nConcord, NH 03301\n\nCan be an instance or class type of method\nfrom datetime import datetime, timedelta\nfrom typing import Iterable\nfrom math import ceil\nclass DateTimeRange:\n¬† ¬† def __init__(self, start: datetime, end_:datetime, step:timedelta = timedelta(seconds=1)):\n¬† ¬† ¬† ¬† self._start = start\n¬† ¬† ¬† ¬† self._end = end_\n¬† ¬† ¬† ¬† self._step = step\n\n¬† ¬† def __iter__(self) -&gt; Iterable[datetime]:\n¬† ¬† ¬† ¬† point = self._start\n¬† ¬† ¬† ¬† while point &lt; self._end:\n¬† ¬† ¬† ¬† ¬† ¬† yield point\n¬† ¬† ¬† ¬† ¬† ¬† point += self._step\n\n¬† ¬† def __len__(self) -&gt; int:\n¬† ¬† ¬† ¬† ¬† ¬† return ceil((self._end - self._start) / self._step)\n\n¬† ¬† def __contains__(self, item: datetime) -&gt; bool:\n¬† ¬† ¬† ¬† ¬† ¬† mod = divmod(item - self._start, self._step)\n¬† ¬† ¬† ¬† ¬† ¬† return item &gt;= self._start and item &lt; self._end and mod[1] == timedelta(0)\n\n¬† ¬† def __getitem__(self, item: int) -&gt; datetime:\n¬† ¬† ¬† ¬† n_steps = item if item &gt;= 0 else len(self) + item\n¬† ¬† ¬† ¬† return_value = self._start + n_steps * self._step\n¬† ¬† ¬† ¬† if return_value not in self:\n¬† ¬† ¬† ¬† ¬† ¬† raise IndexError()\n¬† ¬† ¬† ¬† return return_value¬†\n\n¬† ¬† def __str__(self):\n¬† ¬† ¬† ¬† return f\"Datetime Range [{self._start}, {self._end}) with step {self._step}\"\n\nClass DateTimeRange has methods that allows you to treat a date-range object like a list\n\nJust for illustration. Think methods in pandas can do this stuff\n\n__iter__ method - generator function that creates one element at a time, yields it to the caller, and allows the caller to process it\n\nExample creates datetime ranges instead of numeric ranges\n\n__len__ - find out the number of elements that are part of your range\n__getitem__- uses indexing syntax to retrieve entries from your objects\n__contains__- checks if an element is part of your range. T/F\n\ndivmod returns quotient and remainder.\n\nUsing these magic methods\nmy_range = DateTimeRange(datetime(2021,1,1), datetime(2021,12,1), timedelta(days=12)) #instantiate\nprint(my_range)¬† ¬† ¬† ¬† ¬† ¬† ¬† # __init__ or maybe __str__\nfor r in my_range:¬† ¬† ¬† ¬† ¬† # __iter__\n¬† ¬† do_something(r)\nlen(my_range)¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # __len__\nmy_range[-2] in my_range¬† ¬† # __getitem__ (neg indexing), __contains__\n\nOthers\n\n__repr__ - Creates a string representation of the class object\n\nExample\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def __repr__(self):\n        return f\"Person(name='{self.name}', age={self.age})\"\n\nperson = Person(\"John\", 25)\nprint(person)\n#&gt; Person(name='John', age=25)\n\n__eq__ - Provides a method for comparing two class objects by their values.\n\nExample\nclass Person:\n  def __init__(self, age):\n    self.age = age\n\n  def __eq__(self, other):\n    return self.age == other.age\n\nalice = Person(18)\nbob = Person(19)\ncarl = Person(18)\n\nprint(alice == bob)\n#&gt; False\n\nprint(alice == carl)\n#&gt; True",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-inher",
    "href": "qmd/python-classes.html#sec-py-class-inher",
    "title": "Classes",
    "section": "Inheritance",
    "text": "Inheritance\n\n\nSome notes from this much more detailed example (article)\n\nShows how to combine different scripts¬† (aka modules) and diagram the hierarchies\nSome good debugging too\n\nRunning &lt;class&gt;.__mro__will show you the order of inheritance.\n\n\nInheritance models what is called an ‚Äúis a‚Äù relationship. This means that when you have a Derived (aka subclass, child) class that inherits from a Base (aka super, parent) class, you created a relationship where Derived is a specialized version of Base.\nChild classes inherit all of their parent‚Äôs attributes and methods, but they can also define their own attributes and methods.\nCan override or extend parent class attributes and methods\nclass Manager(Employee):\n¬† ¬† pass\n\nm1 = Manager(\"aaa\", 13000)\n\nManager is the child class and Employee is the parent class (see top)\nChild classes don‚Äôt require a constructor method for an object to be created\n\nExtending the instance attributes of the parent class\nclass Manager(Employee):\n¬† ¬† def __init__(self, name, salary=0, department):\n¬† ¬† ¬† ¬† Employee.__init__(self, name, salary=0)\n¬† ¬† ¬† ¬† self.department = department\n\nContructor method for the child class with the new attribute, ‚Äúdepartment,‚Äù in the arguments.\nParent class (Employee) constructor method is called and a new attribute, department, is defined.\n\n\n\nsuper()\n\nAlternative way of extending instance attributes through inheritance\nExample\nclass Rectangle:\n¬† ¬† def __init__(self, length, width):\n¬† ¬† ¬† ¬† self.length = length\n¬† ¬† ¬† ¬† self.width = width\n¬† ¬† def area(self):\n¬† ¬† ¬† ¬† return self.length * self.width\n¬† ¬† def perimeter(self):\n¬† ¬† ¬† ¬† return 2 * self.length + 2 * self.width\nclass Square(Rectangle):\n¬† ¬† def __init__(self, length):\n¬† ¬† ¬† ¬† super().__init__(length, length)\nclass Cube(Square):\n¬† ¬† def surface_area(self):\n¬† ¬† ¬† ¬† face_area = super().area()\n¬† ¬† ¬† ¬† return face_area * 6\n¬† ¬† def volume(self):\n¬† ¬† ¬† ¬† face_area = super().area()\n¬† ¬† ¬† ¬† return face_area * self.length\n\n&gt;&gt;&gt; cube = Cube(3)\n&gt;&gt;&gt; cube.surface_area()\n54\n&gt;&gt;&gt; cube.volume()\n27\n\nClass Cube inherits from Square and extends the functionality of .area() (inherited from the Rectangle class through Square) to calculate the surface area and volume of a Cube instance Also notice that the Cube class definition does not have an .__init__(). Because Cube inherits from Square and .__init__() doesn‚Äôt really do anything differently for Cube than it already does for Square, you can skip defining it, and the .__init__() of the other child class (Square) will be called automatically.\nsuper(Square, self).__init__(length, length) is equivalent to calling super without parameters (see above example)\nUsing super(Square, self).area() in class Cube. Setting the 1st parameter to Square instead of Cube causes super() to start searching for a matching method (in this case, .area()) at one level above Square in the instance hierarchy, in this case Rectangle.\n\nIf Square had an .area method, but you wanted to use Rectangle‚Äôs instead, this would be a way to do that.\n\nNote another difference between using super() and using the class name (e.g.¬†the first example) ‚Äî ‚Äúself‚Äù is NOT one of the args in super()\nExample: Child class of two separate hierarchies\n# Super class\nclass Rectangle:\n¬† ¬† def __init__(self, length, width, **kwargs):\n¬† ¬† ¬† ¬† self.length = length\n¬† ¬† ¬† ¬† self.width = width\n¬† ¬† ¬† ¬† super().__init__(**kwargs)\n¬† ¬† def area(self):\n¬† ¬† ¬† ¬† return self.length * self.width\n¬† ¬† def perimeter(self):\n¬† ¬† ¬† ¬† return 2 * self.length + 2 * self.width\n# Child class: Square class inherits from the Rectangle class\nclass Square(Rectangle):\n¬† ¬† def __init__(self, length, **kwargs):\n¬† ¬† ¬† ¬† super().__init__(length=length, width=length, **kwargs)\n# Child class: Cube class inherits from Square and also from Rectangle classes\nclass Cube(Square):\n¬† ¬† def surface_area(self):\n¬† ¬† ¬† ¬† face_area = super().area()\n¬† ¬† ¬† ¬† return face_area * 6\n¬† ¬† def volume(self):\n¬† ¬† ¬† ¬† face_area = super().area()\n¬† ¬† ¬† ¬† return face_area * self.length\n\n# Class (separate)\nclass Triangle:¬†\n¬† ¬† def __init__(self, base, height, **kwargs):¬†\n¬† ¬† ¬† ¬† self.base = base¬†\n¬† ¬† ¬† ¬† self.height = height¬†\n¬† ¬† ¬† ¬† super().__init__(**kwargs)¬†\n¬† ¬† def tri_area(self):¬†\n¬† ¬† ¬† ¬† return 0.5 * self.base * self.height\n\n# Inherits from a child class (and super class) and a class\nclass RightPyramid(Square, Triangle):\n¬† ¬† def __init__(self, base, slant_height, **kwargs):\n¬† ¬† ¬† ¬† self.base = base\n¬† ¬† ¬† ¬† self.slant_height = slant_height\n¬† ¬† ¬† ¬† kwargs[\"height\"] = slant_height\n¬† ¬† ¬† ¬† kwargs[\"length\"] = base\n¬† ¬† ¬† ¬† super().__init__(base=base, **kwargs)\n¬† ¬† def area(self):\n¬† ¬† ¬† ¬† base_area = super().area()\n¬† ¬† ¬† ¬† perimeter = super().perimeter()\n¬† ¬† ¬† ¬† return 0.5 * perimeter * self.slant_height + base_area\n¬† ¬† def area_2(self):\n¬† ¬† ¬† ¬† base_area = super().area()\n¬† ¬† ¬† ¬† triangle_area = super().tri_area()\n¬† ¬† ¬† ¬† return triangle_area * 4 + base_area\n\n&gt;&gt;&gt; pyramid = RightPyramid(base=2, slant_height=4)\n&gt;&gt;&gt; pyramid.area()\n20.0\n&gt;&gt;&gt; pyramid.area_2()\n20.0\n\nRightPyramid inherits from a child class (Square) and a class Triangle\n\nSince Square inherits from Rectangle, so does RightPyramid\nTriangle is a separate class not part of any hierarchy\n\nIf there‚Äôs an .area method in either of the classes, then super will search hierarchy of the class listed first (Square), then the hierarchy of the class listed second (Triangle)\n\nBest practice to make sure each class has different method names.\n\nEach class with an __init__ constructor gets a super().__init__() and **kwargs added to its args\n\nWhich is every class sans Cube. If Cube was inherited by a class, I think it would require an __init__ constructor and the super().__init__(**kwargs) expression.\nWithout doing this, calling .area_2() in RightPyramid will give us an AttributeError since .base and .height don‚Äôt have any values. (don‚Äôt completely understand this explanation)\n\nkwarg flow through super().__init__()\n\nIn RightPyramid __init__,¬† slant_height and base values are assigned to height and length keys in the kwargs dict\nsuper() passes base and the kwargs dict up to Square and Triangle\n\nTriangle uses height from kwargs and base\nSquare uses length from kwargs\n\nSquare passes length to Rectangle as values for both width and length\n\n\nAll classes now have the argument values necessary for their functions to work.\n\nNow RightPyramid can call those other classes‚Äô methods (e.g.¬†.area and .perimeter from Rectangle and tri_area from Triangle)\n\nI believe since every class has **kwargs arg in their super().__init__, each has every value in the kwarg dict even if they don‚Äôt need it.\n\nSo probably possible to add functions to those classes that would use those values\n\n\n\nExample: Using super with method other than __init__\nclass SalaryPolicy:¬†\n¬† ¬† def __init__(self, weekly_salary):¬†\n¬† ¬† ¬† ¬† self.weekly_salary = weekly_salary¬†\n¬† ¬† def calculate_payroll(self):¬†\n¬† ¬† ¬† ¬† return self.weekly_salary\n\nclass CommissionPolicy(SalaryPolicy):¬†\n¬† ¬† def __init__(self, weekly_salary, commission):¬†\n¬† ¬† ¬† ¬† super().__init__(weekly_salary)¬†\n¬† ¬† ¬† ¬† self.commission = commission¬†\n¬† ¬† def calculate_payroll(self):¬†\n¬† ¬† ¬† ¬† fixed = super().calculate_payroll()¬†\n¬† ¬† ¬† ¬† return fixed + self.commission\n\nIn CommissionPolicy‚Äôs calculate_payroll, super() accesses SalaryPolicy‚Äôs calculate_payroll method to get the weekly_salary value\n\n\n\n\n\nDiamond Problem\n\n\nAppears when you‚Äôre using multiple inheritance and deriving from two classes that have a common base class.\n\nThis can cause the wrong version of a method to be called.\ne.g.¬†TemporarySecretary uses multiple inheritance to derive from two classes that ultimately also derive from Employee. This causes two paths to reach the Employee base class, which is something you want to avoid in your designs.\n\n\n\n\nMixin Class\n\nOperates the same as Inheritance, but since it only provides simple behavior(s), it is easy to reuse with other classes without causing problems\nExample: Take certain class attributes and create a dict\n# In representations.py\nclass AsDictionaryMixin:\n¬† ¬† def to_dict(self):\n¬† ¬† ¬† ¬† return {\n¬† ¬† ¬† ¬† ¬† ¬† prop: self._represent(value)\n¬† ¬† ¬† ¬† ¬† ¬† for prop, value in self.__dict__.items()\n¬† ¬† ¬† ¬† ¬† ¬† if not self._is_internal(prop)\n¬† ¬† ¬† ¬† }\n¬† ¬† def _represent(self, value):\n¬† ¬† ¬† ¬† if isinstance(value, object):\n¬† ¬† ¬† ¬† ¬† ¬† if hasattr(value, 'to_dict'):\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† return value.to_dict()\n¬† ¬† ¬† ¬† ¬† ¬† else:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† return str(value)\n¬† ¬† ¬† ¬† else:\n¬† ¬† ¬† ¬† ¬† ¬† return value\n¬† ¬† def _is_internal(self, prop):\n¬† ¬† ¬† ¬† return prop.startswith('_')\n\nto_dict is a dictionary comprehension\n\nmydict = {key:val for key, val in dict}\nReturns a dict with key:value (e.g.¬†property (aka attribute):value) pairs from a class‚Äôs __dict__ if the property (prop) doesn‚Äôt have an underscore\n\n_represent makes sure the ‚Äúvalue‚Äù is a value and not object\n_is_interna1 checks whether the attribute has an underscore in the name\n\nApply the Mixin class to any class the same way as using Inheritance\nclass Employee(AsDictionaryMixin):\n¬† ¬† def __init__(self, id, name, address, role, payroll):\n¬† ¬† ¬† ¬† self.id = id\n¬† ¬† ¬† ¬† self.name = name\n¬† ¬† ¬† ¬† self.address = address\n¬† ¬† ¬† ¬† self._role = role\n¬† ¬† ¬† ¬† self._payroll = payroll\n\nAsDicitionaryMixin is used as an arg to the class\n‚Äúself._role‚Äù and ‚Äúself._payroll‚Äù have underscores which tells to_dict not to include them in the resulting dictionary\nNot important to using a mixin class but note that ‚Äúaddress‚Äù is from the Address class via composition (see below). Therefore the Address class would also need to inherit AsDictionaryMixin for this to work\n\nUtilize\nimport json\n\ndef print_dict(d):\n¬† ¬† print(json.dumps(d, indent=2))\n\nfor employee in EmployeeDatabase().employees:\n¬† ¬† print_dict(employee.to_dict())\n\nprint_dict takes the dict output of employee._to_dict and converts it to a json format",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-comp",
    "href": "qmd/python-classes.html#sec-py-class-comp",
    "title": "Classes",
    "section": "Composition",
    "text": "Composition\n\n\nNotes from Inheritance and Composition: A Python OOP Guide\nComposition models a ‚Äúhas a‚Äù relationship. In composition, a class known as composite contains an object of another class known to as component.\nComposition design is typically more flexible than inheritance and is preferable to Inheritance\n\nPrevents ‚Äúclass explosion‚Äù\n\nFor complex projects, too many classes can lead to conflicts and errors because of the inevitable complex network of classes that are connected to each other.\nYou change your program‚Äôs behavior by providing new components that implement those behaviors instead of adding new classes to your hierarchy.\n\nOnly loose class connections in composition\n\nChanges to the component class rarely affect the composite class, and changes to the composite class never affect the component class\n\n\ntl;dr\n\nClasses are written in different py scripts and imported as ‚Äúmodules‚Äù in another script.\nAttribute(s) from a component class (e.g.¬†Address) are used in the composite class (e.g.¬†Employee)\nThen a composite class attribute object is assigned to the instantiated composite class‚Äôs empty attribute\n\nThis is the magic. One class‚Äôs attribute can be used as input into another class‚Äôs attribute without being tightly coupled to that other class (aka inheritance).\n\nThat input isn‚Äôt a value. It‚Äôs class type object.\nSee Utilize code block below\n\n\n\n\n\nComposition Through __init__ Attributes\n\nExample:\n# In contacts.py\n# Component class\nclass Address:\n¬† ¬† def __init__(self, street, city, state, zipcode, street2=''):\n¬† ¬† ¬† ¬† self.street = street\n¬† ¬† ¬† ¬† self.street2 = street2\n¬† ¬† ¬† ¬† self.city = city\n¬† ¬† ¬† ¬† self.state = state\n¬† ¬† ¬† ¬† self.zipcode = zipcode\n\n# In employees.py\n# Composite class\nclass Employee:\n¬† ¬† def __init__(self, id, name):\n¬† ¬† ¬† ¬† self.id = id\n¬† ¬† ¬† ¬† self.name = name\n¬† ¬† ¬† ¬† self.address = None\n\n# ManagerRole and SalaryPolicy are classes from different modules\nclass Manager(Employee, ManagerRole, SalaryPolicy):\n¬† ¬† def __init__(self, id, name, weekly_salary):\n¬† ¬† ¬† ¬† SalaryPolicy.__init__(self, weekly_salary)\n¬† ¬† ¬† ¬† super().__init__(id, name)\n\nYou would import these two modules into a third script and do stuff (see next code block)\nYou initialize the Address.address attribute to ‚ÄúNone‚Äù for now to make it optional, but by doing that, you can now assign an Address to an Employee.\n\ni.e.¬†the attributes of an Address instance from its __init__ are now available to be assigned to a Employee instance.\n\nManager is a child class of multiple other classes (inheritance) including Employee and therefore gets an .address attribute\n\nAside: ManagerRole doesn‚Äôt have an __init__ (i.e.¬†no attributes), so I‚Äôm not sure why super() is used here\n\nwhy not just use Employee.__init__?\nI would‚Äôve thought that ManagerRole would‚Äôve required the same inputs as Employee, so super() is used here to cover both at the same time.\n\nBut that‚Äôs not the case, MangerRole is there just for it‚Äôs method and not it‚Äôs attributes\n\nDoes being able to use ManageRole methods require super() (i.e.¬†necessary for Inheritance)?\n\n\nUtilize\nmanager = employees.Manager(1, 'Mary Poppins', 3000)\nmanager.address = contacts.Address(\n¬† ¬† '121 Admin Rd',¬†\n¬† ¬† 'Concord',¬†\n¬† ¬† 'NH',¬†\n¬† ¬† '03301'\n)\n# ... create other intances of different jobs in the company\n\n# guess this would be like a json\nemployees = [\n¬† ¬† manager,\n¬† ¬† secretary,\n¬† ¬† sales_guy,\n¬† ¬† factory_worker,\n¬† ¬† temporary_secretary,\n]\n\n# do work with the list class objs\nproductivity_system = productivity.ProductivitySystem()\nproductivity_system.track(employees, 40)\n\nThe Address class instance is assigned to the .address attribute of the Manager instance (which gets its .address attribute from Employee)\n\n\n\n\n\nComposition Through a Function Argument\n\nExample:\n# In hr.py\nclass PayrollSystem:\n¬† ¬† def calculate_payroll(self, employees):\n¬† ¬† ¬† ¬† print('Calculating Payroll')\n¬† ¬† ¬† ¬† print('===================')\n¬† ¬† ¬† ¬† for employee in employees:\n¬† ¬† ¬† ¬† ¬† ¬† print(f'Payroll for: {employee.id} - {employee.name}')\n¬† ¬† ¬† ¬† ¬† ¬† print(f'- Check amount: {employee.calculate_payroll()}')\n¬† ¬† ¬† ¬† ¬† ¬† if employee.address:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print('- Sent to:')\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print(employee.address)\n¬† ¬† ¬† ¬† ¬† ¬† print('')\n\npayroll_system = hr.PayrollSystem()\npayroll_system.calculate_payroll(employees)\n\nThe input for calculate_payroll, employees, is a list of instantiated Employee class objects in the previous code chunk.\nName of the class is iterated and represents each instance\n\nHas attributes and methods available\n\n\n\n\n\nOther Module Classes Used as Attributes\n\nExample:\n# In employees.py\nfrom productivity import ProductivitySystem\nfrom hr import PayrollSystem\nfrom contacts import AddressBook\nclass EmployeeDatabase:\n¬† ¬† def __init__(self):\n¬† ¬† ¬† ¬† self._employees = [\n¬† ¬† ¬† ¬† ¬† ¬† {\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'id': 1,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'name': 'Mary Poppins',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'role': 'manager'\n¬† ¬† ¬† ¬† ¬† ¬† },\n¬† ¬† ¬† ¬† ¬† ¬† {\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'id': 2,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'name': 'John Smith',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'role': 'secretary'\n¬† ¬† ¬† ¬† ¬† ¬† }\n¬† ¬† ¬† ¬† ]\n¬† ¬† ¬† ¬† self.productivity = ProductivitySystem()\n¬† ¬† ¬† ¬† self.payroll = PayrollSystem()\n¬† ¬† ¬† ¬† self.employee_addresses = AddressBook()\n¬† ¬† @property\n¬† ¬† def employees(self):\n¬† ¬† ¬† ¬† return [self._create_employee(**data) for data in self._employees]\n¬† ¬† def _create_employee(self, id, name, role):\n¬† ¬† ¬† ¬† address = self.employee_addresses.get_employee_address(id)\n¬† ¬† ¬† ¬† employee_role = self.productivity.get_role(role)\n¬† ¬† ¬† ¬† payroll_policy = self.payroll.get_policy(id)\n¬† ¬† ¬† ¬† return Employee(id, name, address, employee_role, payroll_policy)\n\nProductivitySystem, PayrollSystem, AddressBook are classes imported from various modules\nAs attributes, these classes‚Äô methods are used in the _create_employee function\n\nReturn invokes the Employee class with values obtained by the various class methods\nEmployee class (not shown in this block) is already present in this module so it doesn‚Äôt have to be imported.",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-dec",
    "href": "qmd/python-classes.html#sec-py-class-dec",
    "title": "Classes",
    "section": "Decorators",
    "text": "Decorators\n\nThey can add additional features to a function\n\nUseful because you don‚Äôt have to refactor downstream code\n\nFunctions that take a function as input\n\nSee use cases throughout note and check bkmks\n\n@ is placed above a ‚Äúdecorated‚Äù function\n\nExample\n@decorator_1\ndef temperature():\nreturn temp\n\nCalling temperature() is actually calling decorator_1(temperature())\n\n\nExample: add additional features to a function\n\n\nAdds a timer to a function\n\nExample: Multiple decorators for a function\n@log_execution\n@timing\ndef my_function(x, y):\n¬† ¬† time.sleep(1)\n¬† ¬† return x + y\nSee Custom Examples for the ‚Äúlog_execution‚Äù decorator\n\n\nProperty\n\nArguments\nproperty(fget=None, fset=None, fdel=None, doc=None)\nBuilt-in decorator\nConstitutes a family of decorators\n\n@property: Declares the method as a property.\n\nfget - function to get value of the attribute\n\n@.setter: Specifies the setter method for a property that sets the value to a property.\n\nfset - function to set value of the attribute\nmust have the value argument that can be used to assign to the underlying private attribute\n\n@.deleter: Specifies the delete method as a property that deletes a property.\n\nfdel - function to delete the attribute\nmust have the value argument that can be used to assign to the underlying private attribute\n\n\nExample\n# Using @property decorator\nclass Celsius:\ndef __init__(self, temperature=0):\n¬† ¬† self._temperature = temperature\ndef to_fahrenheit(self):\n¬† ¬† return (self._temperature * 1.8) + 32\n\n\n# decorators\n# attribute getter\n@property\ndef temperature(self):\n¬† ¬† print(\"Getting value...\")\n¬† ¬† return self._temperature\n\n# also adds constraint to the temperature input\n@temperature.setter\ndef temperature(self, value):\n¬† ¬† print(\"Setting value...\")\n¬† ¬† if value &lt; -273.15:\n¬† ¬† ¬† ¬† raise ValueError(\"Temperature below -273 is not possible\")\n¬† ¬† self._temperature = value\n\n@temperature.deleter\ndef temperature(self, value):\n¬† ¬† print(\"Deleting value...\")\n¬† ¬† del self._temperature\n\n&gt;&gt; human = Celsius(37)\nSetting value...\n&gt;&gt; print(human.temperature)\nGetting value...\n37\n&gt;&gt; print(human.to_fahrenheit())\nGetting value...\n98.60000000000001\n&gt;&gt; del human.temperature\nDeleting value...\n&gt;&gt; coldest_thing = Celsius(-300)\nSetting value...\nTraceback (most recent call last):\nFile \"\", line 29, in\nFile \"\", line 4, in __init__\nFile \"\", line 18, in temperature\nValueError: Temperature below -273 is not possible\n\n.deleter didn‚Äôt work for me and neither did the conditional. Don‚Äôt my python version or what\n\n\n\n\nClass Method\n\nMethod that is bound to the class and not the object (aka instance) of the class.\nInstance attributes cannot be referred to with this method\nCan modify the class state that applies across all instances of the class\nUse Cases\n\nTo create new instances of the class without going trhough its normal __init__\nTo create a class instance that requires some async calls when instantiated, since __init__ cannot be async.\n\nStarts with a ‚Äúclassmethod‚Äù decorator\nclass MyClass:¬† ¬†\n@classmethod\ndef classmethod(cls):\n¬† ¬† return 'class method called', cls\nCalling a class method vs an instance method\n# calling a class method\n# no instantiation\nMyClass.classmethod()\n\n# calling an instance method\n# instantiates object first\nobject = MyClass()\nobject.method()\n\n\n\nStatic Method\n\nMethod bound to the class instance, not the class itself.\nDoes not take the class as a parameter.\nIt cannot access or modify the class at all.\nStarts with a ‚Äústaticmethod‚Äù decorator\nDoesn‚Äôt have any access to what the class is‚Äîit‚Äôs basically just a function, called syntactically like a method, but without access to the object and its internals (fields and other methods), which classmethod does have.\nSee SO thread for discussion on the differences between the static and class decorators and their uses\nclass Person:\n¬† def __init__(self, name, age):\n¬† ¬† self.name = name\n¬† ¬† self.age = age\n\n¬† @staticmethod\n¬† def isAdult(age):\n¬† ¬† return age &gt; 18\n\nisAdult(age) function doesn‚Äôt require the usual self argument, so it couldn‚Äôt reference the class even if it wanted to.\nMost often used as utility functions that are completely independent of a class‚Äôs state\nSee classmethod decorator for details on calling this method\n\n\n\n\nCustom Examples\n\nAlso see Code, Optimization &gt;&gt; Python &gt;&gt; Profile decorator\nUsing functools and decorators\nfrom functools import singledispatch\n\n@singledispatch\ndef process_data(data):\nraise NotImplementedError(f\"Type {type(data)} is unsupported\")\n\n@process_data.register\ndef process_dict(data: dict):\nprint(\"Dict is processed\")\n\n@process_data.register\ndef process_list(data: list):\nprint(\"List is processed\")\nMultiprocessing Function Execution Time Limiter\nimport multiprocessing\nfrom functools import wraps\n\nclass TimeExceededException(Exception):\n¬† ¬† pass\n## PART 1\n¬† ¬† def function_runner(*args, **kwargs):\n¬† ¬† ¬† ¬† \"\"\"Used as a wrapper function to handle\n¬† ¬† ¬† ¬† returning results on the multiprocessing side\"\"\"\n\n¬† ¬† ¬† ¬† send_end = kwargs.pop(\"__send_end\")\n¬† ¬† ¬† ¬† function = kwargs.pop(\"__function\")\n¬† ¬† ¬† ¬† try:\n¬† ¬† ¬† ¬† ¬† ¬† result = function(*args, **kwargs)\n¬† ¬† ¬† ¬† except Exception as e:\n¬† ¬† ¬† ¬† ¬† ¬† send_end.send(e)\n¬† ¬† ¬† ¬† ¬† ¬† return\n¬† ¬† ¬† ¬† send_end.send(result)\n\n¬† ¬† @parametrized\n¬† ¬† def run_with_timer(func, max_execution_time):\n¬† ¬† ¬† ¬† @wraps(func)\n¬† ¬† ¬† ¬† def wrapper(*args, **kwargs):\n¬† ¬† ¬† ¬† ¬† ¬† recv_end, send_end = multiprocessing.Pipe(False)\n¬† ¬† ¬† ¬† ¬† ¬† kwargs[\"__send_end\"] = send_end\n¬† ¬† ¬† ¬† ¬† ¬† kwargs[\"__function\"] = func\n\n¬† ¬† ¬† ¬† ¬† ¬† ## PART 2\n¬† ¬† ¬† ¬† ¬† ¬† p = multiprocessing.Process(target=function_runner, args=args, kwargs=kwargs)\n¬† ¬† ¬† ¬† ¬† ¬† p.start()\n¬† ¬† ¬† ¬† ¬† ¬† p.join(max_execution_time)\n¬† ¬† ¬† ¬† ¬† ¬† if p.is_alive():\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† p.terminate()\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† p.join()\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† raise TimeExceededException(\"Exceeded Execution Time\")\n¬† ¬† ¬† ¬† ¬† ¬† result = recv_end.recv()\n\n¬† ¬† ¬† ¬† ¬† ¬† if isinstance(result, Exception):\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† raise result\n\n¬† ¬† ¬† ¬† ¬† ¬† return result\n\n¬† ¬† ¬† ¬† return wrapper\n\nFrom Limiting Python Function Execution Time with a Parameterized Decorator via Multiprocessing\n\nRetry (e.g.¬†for an API)\nimport time\nfrom functools import wraps\n\ndef retry(max_tries=3, delay_seconds=1):\n¬† ¬† def decorator_retry(func):\n¬† ¬† ¬† ¬† @wraps(func)\n¬† ¬† ¬† ¬† def wrapper_retry(*args, **kwargs):\n¬† ¬† ¬† ¬† ¬† ¬† tries = 0\n¬† ¬† ¬† ¬† ¬† ¬† while tries &lt; max_tries:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† try:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† return func(*args, **kwargs)\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† except Exception as e:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† tries += 1\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† if tries == max_tries:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† raise e\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† time.sleep(delay_seconds)\n¬† ¬† ¬† ¬† return wrapper_retry\n¬† ¬† return decorator_retry\n\n@retry(max_tries=5, delay_seconds=2)\ndef call_dummy_api():\n¬† ¬† response = requests.get(\"https://jsonplaceholder.typicode.com/todos/1\")\n¬† ¬† return response\n\nTries to get an API response. If it fails, we retry the same task 5 times. Between each retry, we wait for 2 seconds.\n\nCache Function Results\ndef memoize(func):\n¬† ¬† cache = {}\n¬† ¬† def wrapper(*args):\n¬† ¬† ¬† ¬† if args in cache:\n¬† ¬† ¬† ¬† ¬† ¬† return cache[args]\n¬† ¬† ¬† ¬† else:\n¬† ¬† ¬† ¬† ¬† ¬† result = func(*args)\n¬† ¬† ¬† ¬† ¬† ¬† cache[args] = result\n¬† ¬† ¬† ¬† ¬† ¬† return result\n¬† ¬† return wrapper\n\n@memoize\ndef fibonacci(n):\n¬† ¬† if n &lt;= 1:\n¬† ¬† ¬† ¬† return n\n¬† ¬† else:\n¬† ¬† ¬† ¬† return fibonacci(n-1) + fibonacci(n-2)\n\nUses a dictionary, stores the function args, and returns values. When we execute this function, the decorated will check the dictionary for prior results. The actual function is called only when there‚Äôs no stored value before.\nUsing a dictionary to hold previous execution data is a straightforward approach. However, there is a more sophisticated way to store caching data. You can use an in-memory database, such as Redis.\n\nLogging (e.g.¬†ETL pipeline)\nimport logging\nimport functools\n\nlogging.basicConfig(level=logging.INFO)\n\ndef log_execution(func):\n¬† ¬† @functools.wraps(func)\n¬† ¬† def wrapper(*args, **kwargs):\n¬† ¬† ¬† ¬† logging.info(f\"Executing {func.__name__}\")\n¬† ¬† ¬† ¬† result = func(*args, **kwargs)\n¬† ¬† ¬† ¬† logging.info(f\"Finished executing {func.__name__}\")\n¬† ¬† ¬† ¬† return result\n¬† ¬† return wrapper\n\n@log_execution\ndef extract_data(source):\n¬† ¬† # extract data from source\n¬† ¬† data = ...\n¬† ¬† return data\n@log_execution\ndef transform_data(data):\n¬† ¬† # transform data\n¬† ¬† transformed_data = ...\n¬† ¬† return transformed_data\n@log_execution\ndef load_data(data, target):\n¬† ¬† # load data into target\n¬† ¬† ...\n\ndef main():\n¬† ¬† # extract data\n¬† ¬† data = extract_data(source)\n¬† ¬† # transform data\n¬† ¬† transformed_data = transform_data(data)\n¬† ¬† # load data\n¬† ¬† load_data(transformed_data, target)\n\noutput\nINFO:root:Executing extract_data\nINFO:root:Finished executing extract_data\nINFO:root:Executing transform_data\nINFO:root:Finished executing transform_data\nINFO:root:Executing load_data\nINFO:root:Finished executing load_data\n\nEmail Notification\nimport smtplib\nimport traceback\nfrom email.mime.text import MIMEText\ndef email_on_failure(sender_email, password, recipient_email):\n¬† ¬† def decorator(func):\n¬† ¬† ¬† ¬† def wrapper(*args, **kwargs):\n¬† ¬† ¬† ¬† ¬† ¬† try:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† return func(*args, **kwargs)\n¬† ¬† ¬† ¬† ¬† ¬† except Exception as e:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # format the error message and traceback\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† err_msg = f\"Error: {str(e)}\\n\\nTraceback:\\n{traceback.format_exc()}\"\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # create the email message\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† message = MIMEText(err_msg)\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† message['Subject'] = f\"{func.__name__} failed\"\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† message['From'] = sender_email\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† message['To'] = recipient_email\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # send the email\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† with smtplib.SMTP_SSL('smtp.gmail.com', 465) as smtp:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† smtp.login(sender_email, password)\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† smtp.sendmail(sender_email, recipient_email, message.as_string())\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # re-raise the exception\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† raise\n\n¬† ¬† ¬† ¬† return wrapper\n\n¬† ¬† return decorator\n\n@email_on_failure(sender_email='your_email@gmail.com', password='your_password', recipient_email='recipient_email@gmail.com')\ndef my_function():\n¬† ¬† # code that might fail",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-opt",
    "href": "qmd/python-classes.html#sec-py-class-opt",
    "title": "Classes",
    "section": "Optimizations",
    "text": "Optimizations\n\nMisc\n\nNotes from How to Write Memory-Efficient Classes in Python\n\nUse __slots__ when creating large numbers of instances\n\n\nBy default, Python classes store their instance attributes in a private dictionary (__dict__), which dictionary allows you to add, modify, or delete the class attributes at runtime, but it creates a large memory burden when substantial numbers of instances are created.\nSlots reserves only a fixed amount of space for the specified attributes directly in each instance, instead of using the default dictionary.\nAny attempt to assign an attribute that is not listed in¬†__slots__¬†will raise an¬†AttributeError.\n\nThis can help prevent creating accidental attributes due to typos, but it can also be restrictive if you need to add additional attributes later in development.\n\nExample:\nclass Ant:\n  __slots__ = ['worker_id', 'role', 'colony']\n\n  def __init__(self, worker_id, role, colony):\n      self.worker_id = worker_id\n      self.role = role\n      self.colony = colony\n\nLazy Initialization for memory intensive operations\n\nScenario: Data Loading in an app\n\nUser wants to look at map or examine features before analyzing data.\nWithout Lazy Loading, the entire dataset is loaded upfront, leading to slower startup and potentially exceeding memory limits.\n\nCaching is also a good idea if you‚Äôre performing the same intensive computation more than once.\nExample: Lazy Loader\nfrom functools import cached_property\n\nclass DataLoader:\n\n    def __init__(self, path):\n        self.path = path\n\n    @cached_property\n    def dataset(self):\n        # load the dataset here\n        # this will only be executed once when the dataset property is first accessed\n        return self._load_dataset()\n\n    def _load_dataset(self):\n        print(\"Loading the dataset...\")\n\n        # load a big dataset here\n        df = pd.read_csv(self.path)\n        return df\n\nclass DataProcessor:\n    def __init__(self, path):\n          self.path = path\n        self.data_loader = DataLoader(self.path)\n\n    def process_data(self):\n        dataset = self.data_loader.dataset\n        print(\"Processing the dataset...\")\n        # Perform complex data processing steps on the loaded dataset\n        ...\n\n# instantiate the DataLoader class\npath = \"/[path_to_dataset]/mnist.csv\"\n\n# instantiate the DataProcessor class with the data file path\n# üëâ no data will be loaded at this stage! ‚úÖ\nprocessor = DataProcessor(path)\n\n# trigger the processing\nprocessor.process_data()  # The dataset will be loaded and processed when needed\n\nUse generators to reduce memory usage of loops\n\nSee Python, General &gt;&gt; Loops &gt;&gt; Generators\n\nAlso, in Loops &gt;&gt; Misc, there‚Äôs a chart that shows the memory benefits to using a generator vs list comprehension.",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-examp",
    "href": "qmd/python-classes.html#sec-py-class-examp",
    "title": "Classes",
    "section": "Examples",
    "text": "Examples\n\nRead file chunks, process, and write to parquet\nimport pandas as pd\n\nclass PandasChunkProcessor:\n    def __init__(self, filepath, chunk_size, verbose=True):\n        self.filepath = filepath\n        self.chunk_size = chunk_size\n        self.verbose = verbose\n\n    def process_data(self):\n        for chunk_id, chunk in enumerate(pd.read_csv(self.filepath, chunksize=self.chunk_size)):\n            processed_chunk = self.process_chunk(chunk)\n            self.save_chunk(processed_chunk, chunk_id)\n\n    def process_chunk(self, chunk):\n        # process each chunk of data\n        processed_chunk = processing_function(chunk)\n        return processed_chunk\n\n    def save_chunk(self, chunk, chunk_id):\n        # save each processed chunk to a parquet file\n        chunk_filepath = f\"./output_chunk_{chunk_id}.parquet\"\n        chunk.to_parquet(chunk_filepath)\n        if self.verbose:\n            print(f\"saved {chunk_filepath}\")\nTransforms variables by logging, can add 1 if necessary, back-transform\nfrom sklearn.base import BaseEstimator, TransformerMixin¬†\nfrom sklearn.preprocessing import PowerTransformer¬†\n\nclass CustomLogTransformer(BaseEstimator, TransformerMixin):¬†\n¬† ¬† def __init__(self):¬†\n¬† ¬† ¬† ¬† self._estimator = PowerTransformer()¬† # init a transformer¬†\n¬† ¬† def fit(self, X, y=None):¬†\n¬† ¬† ¬† ¬† X_copy = np.copy(X) + 1¬† # add one in case of zeroes¬†\n¬† ¬† ¬† ¬† self._estimator.fit(X_copy)¬†\n¬† ¬† ¬† ¬† return self¬†\n¬† ¬† def transform(self, X):¬†\n¬† ¬† ¬† ¬† X_copy = np.copy(X) + 1¬†\n¬† ¬† ¬† ¬† return self._estimator.transform(X_copy)¬† # perform scaling¬†\n¬† ¬† def inverse_transform(self, X):¬†\n¬† ¬† ¬† ¬† X_reversed = self._estimator.inverse_transform(np.copy(X))¬†\n¬† ¬† ¬† ¬† return X_reversed - 1¬† # return subtracting 1 after inverse transform\nPredictions for a Huggingface classifer\nimport sys\nfrom transformers import pipeline\nfrom typing import List\nimport numpy as np\nfrom time import perf_counter\nimport logging\n\n# Set up logger\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlog = logging.getLogger(__name__)\n\nclass ZeroShotTextClassifier:\n¬† ¬† \"\"\"Class with only class methods\"\"\"\n¬† ¬† # Class variable for the model pipeline\n¬† ¬† classifier = None\n¬† ¬† @classmethod\n¬† ¬† def load(cls):\n¬† ¬† ¬† ¬† # Only load one instance of the model\n¬† ¬† ¬† ¬† if cls.classifier is None:\n¬† ¬† ¬† ¬† ¬† ¬† # Load the model pipeline.\n¬† ¬† ¬† ¬† ¬† ¬† # Note: Usually, this would also download the model.\n¬† ¬† ¬† ¬† ¬† ¬† # But, we download the model into the container in the Dockerfile\n¬† ¬† ¬† ¬† ¬† ¬† # so that it's built into the container and there's no download at\n¬† ¬† ¬† ¬† ¬† ¬† # run time (otherwise, each time we'll download a 1.5GB model).\n¬† ¬† ¬† ¬† ¬† ¬† # Loading still takes time, though. So, we do that here.\n¬† ¬† ¬† ¬† ¬† ¬† # Note: You can use a GPU here if needed.\n¬† ¬† ¬† ¬† ¬† ¬† t0 = perf_counter()\n¬† ¬† ¬† ¬† ¬† ¬† cls.classifier = pipeline(\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† \"zero-shot-classification\", model=\"facebook/bart-large-mnli\"\n¬† ¬† ¬† ¬† ¬† ¬† )\n¬† ¬† ¬† ¬† ¬† ¬† elapsed = 1000 * (perf_counter() - t0)\n¬† ¬† ¬† ¬† ¬† ¬† log.info(\"Model warm-up time: %d ms.\", elapsed)\n¬† ¬† @classmethod\n¬† ¬† def predict(cls, text: str, candidate_labels: List[str]):\n¬† ¬† ¬† ¬† assert len(candidate_labels) &gt; 0\n¬† ¬† ¬† ¬† # Make sure the model is loaded\n¬† ¬† ¬† ¬† cls.load()\n¬† ¬† ¬† ¬† # For the tutorial, let's create\n¬† ¬† ¬† ¬† # a custom object from the huggingface prediction.\n¬† ¬† ¬† ¬† # Our prediction object will include the label and score\n¬† ¬† ¬† ¬† t0 = perf_counter()\n¬† ¬† ¬† ¬† # pylint: disable-next=not-callable\n¬† ¬† ¬† ¬† huggingface_predictions = cls.classifier(text, candidate_labels)\n¬† ¬† ¬† ¬† elapsed = 1000 * (perf_counter() - t0)\n¬† ¬† ¬† ¬† log.info(\"Model prediction time: %d ms.\", elapsed)\n¬† ¬† ¬† ¬† # Create the custom prediction object.\n¬† ¬† ¬† ¬† max_index = np.argmax(huggingface_predictions[\"scores\"])\n¬† ¬† ¬† ¬† label = huggingface_predictions[\"labels\"][max_index]\n¬† ¬† ¬† ¬† score = huggingface_predictions[\"scores\"][max_index]\n¬† ¬† ¬† ¬† return {\"label\": label, \"score\": score}\nPayroll System\n\nEmployees\n# In employees.py\nfrom hr import (\n¬† ¬† SalaryPolicy,\n¬† ¬† CommissionPolicy,\n¬† ¬† HourlyPolicy\n)\nfrom productivity import (\n¬† ¬† ManagerRole,\n¬† ¬† SecretaryRole,\n¬† ¬† SalesRole,\n¬† ¬† FactoryRole\n)\nclass Employee:\n¬† ¬† def __init__(self, id, name):\n¬† ¬† ¬† ¬† self.id = id\n¬† ¬† ¬† ¬† self.name = name\nclass Manager(Employee, ManagerRole, SalaryPolicy):\n¬† ¬† def __init__(self, id, name, weekly_salary):\n¬† ¬† ¬† ¬† SalaryPolicy.__init__(self, weekly_salary)\n¬† ¬† ¬† ¬† super().__init__(id, name)\nclass Secretary(Employee, SecretaryRole, SalaryPolicy):\n¬† ¬† def __init__(self, id, name, weekly_salary):\n¬† ¬† ¬† ¬† SalaryPolicy.__init__(self, weekly_salary)\n¬† ¬† ¬† ¬† super().__init__(id, name)\nclass SalesPerson(Employee, SalesRole, CommissionPolicy):\n¬† ¬† def __init__(self, id, name, weekly_salary, commission):\n¬† ¬† ¬† ¬† CommissionPolicy.__init__(self, weekly_salary, commission)\n¬† ¬† ¬† ¬† super().__init__(id, name)\nclass FactoryWorker(Employee, FactoryRole, HourlyPolicy):\n¬† ¬† def __init__(self, id, name, hours_worked, hour_rate):\n¬† ¬† ¬† ¬† HourlyPolicy.__init__(self, hours_worked, hour_rate)\n¬† ¬† ¬† ¬† super().__init__(id, name)\nclass TemporarySecretary(Employee, SecretaryRole, HourlyPolicy):\n¬† ¬† def __init__(self, id, name, hours_worked, hour_rate):\n¬† ¬† ¬† ¬† HourlyPolicy.__init__(self, hours_worked, hour_rate)\n¬† ¬† ¬† ¬† super().__init__(id, name)\nProductivity\n# In productivity.py\nclass ProductivitySystem:\n¬† ¬† def track(self, employees, hours):\n¬† ¬† ¬† ¬† print('Tracking Employee Productivity')\n¬† ¬† ¬† ¬† print('==============================')\n¬† ¬† ¬† ¬† for employee in employees:\n¬† ¬† ¬† ¬† ¬† ¬† result = employee.work(hours)\n¬† ¬† ¬† ¬† ¬† ¬† print(f'{employee.name}: [{result}]')\n¬† ¬† ¬† ¬† print('')\nclass ManagerRole:\n¬† ¬† def work(self, hours):\n¬† ¬† ¬† ¬† return f'screams and yells for [{hours}] hours.'\nclass SecretaryRole:\n¬† ¬† def work(self, hours):\n¬† ¬† ¬† ¬† return f'expends [{hours}] hours doing office paperwork.'\nclass SalesRole:\n¬† ¬† def work(self, hours):\n¬† ¬† ¬† ¬† return f'expends [{hours}] hours on the phone.'\nclass FactoryRole:\n¬† ¬† def work(self, hours):\n¬† ¬† ¬† ¬† return f'manufactures gadgets for [{hours}] hours.'\nHR\n# In hr.py\nclass PayrollSystem:\n¬† ¬† def calculate_payroll(self, employees):\n¬† ¬† ¬† ¬† print('Calculating Payroll')\n¬† ¬† ¬† ¬† print('===================')\n¬† ¬† ¬† ¬† for employee in employees:\n¬† ¬† ¬† ¬† ¬† ¬† print(f'Payroll for: {employee.id} - {employee.name}')\n¬† ¬† ¬† ¬† ¬† ¬† print(f'- Check amount: {employee.calculate_payroll()}')\n¬† ¬† ¬† ¬† ¬† ¬† print('')\nclass SalaryPolicy:\n¬† ¬† def __init__(self, weekly_salary):\n¬† ¬† ¬† ¬† self.weekly_salary = weekly_salary\n¬† ¬† def calculate_payroll(self):\n¬† ¬† ¬† ¬† return self.weekly_salary\nclass HourlyPolicy:\n¬† ¬† def __init__(self, hours_worked, hour_rate):\n¬† ¬† ¬† ¬† self.hours_worked = hours_worked\n¬† ¬† ¬† ¬† self.hour_rate = hour_rate\n¬† ¬† def calculate_payroll(self):\n¬† ¬† ¬† ¬† return self.hours_worked * self.hour_rate\nclass CommissionPolicy(SalaryPolicy):\n¬† ¬† def __init__(self, weekly_salary, commission):\n¬† ¬† ¬† ¬† super().__init__(weekly_salary)\n¬† ¬† ¬† ¬† self.commission = commission\n¬† ¬† def calculate_payroll(self):\n¬† ¬† ¬† ¬† fixed = super().calculate_payroll()\n¬† ¬† ¬† ¬† return fixed + self.commission",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html",
    "href": "qmd/regression-regularized.html",
    "title": "Regularized",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-misc",
    "href": "qmd/regression-regularized.html#sec-reg-reg-misc",
    "title": "Regularized",
    "section": "",
    "text": "Regularized Logistic Regression is most necessary when the number of candidate predictors is large in relationship to the effective sample size 3np(1‚àíp) where p is the proportion of Y=1 Harrell\nIf using sparse matrix, then you don‚Äôt need to normalize predictors\nPreprocessing\n\nStandardize numerics\nDummy or factor categoricals\nRemove NAs, na.omit\n\nPackages\n\n{glmnet} - handles families: Gaussian, binomial, Poisson, probit, quasi-poisson, and negative binomial GLMs, along with a few other special cases: the Cox model, multinomial regression, and multi-response Gaussian.\n{robustHD}: Robust methods for high-dimensional data, in particular linear model selection techniques based on least angle regression and sparse regression\nIn {{sklearn}} (see Model building, sklearn &gt;&gt; Algorithms &gt;&gt; Stochaistic Gradient Descent (SGD)), the hyperparameters are different than in R\n\nlambda (R) is alpha (py)\nalpha (R) is 1 - L1_ratio (py)\n\n{SLOPE} - Lasso regression that handles correlated predictors by clustering them\n{{Multi-Layer-Kernel-Machine}} - Multi-Layer Kernel Machine (MLKM) is a Python package for multi-scale nonparametric regression and confidence bands. The method integrates random feature projections with a multi-layer structur\n\nA fast implementation of Kernel Ridge Regression (KRR) (sklearn) which is used for non-parametric regularized regression.\nPaper: Multi-Layer Kernel Machines: Fast and Optimal Nonparametric Regression with Uncertainty Quantification\n\n\nVariable Selection\n\nFor Inference, only Adaptive LASSO is capable of handling block and time series dependence structures in data\n\nSee A Critical Review of LASSO and Its Derivatives for Variable Selection Under Dependence Among Covariates\n\n‚ÄúWe found that one version of the adaptive LASSO of Zou (2006) (AdapL.1se) and the distance correlation algorithm of Febrero-Bande et al.¬†(2019) (DC.VS) are the only ones quite competent in all these scenarios, regarding to different types of dependence.‚Äù\nThere‚Äôs a deeper description of the model in the supplemental materials of the paper. I think the ‚Äú.1se‚Äù means it‚Äôs using the lambda.1se from cv.\n\nlambda.1se : largest value of Œª such that error is within 1 standard error of the cross-validated errors for lambda.min.\n\nsee lambda.min, lambda.1se and Cross Validation in Lasso : Binomial Response for code to access this value.\n\n\n\nRe the distance correlation algorithm (it‚Äôs a feature selection alg used in this paper as benchmark vs LASSO variants)\n\n‚Äúthe distance correlation algorithm for variable selection (DC.VS) of Febrero-Bande et al.¬†(2019). This makes use of the correlation distance (Sz√©kely et al., 2007; Szekely & Rizzo, 2017) to implement an iterative procedure (forward) deciding in each step which covariate enters the regression model.‚Äù\nStarting from the null model, the distance correlation function, dcor.xy, in {fda.usc} is used to choose the next covariate\n\nguessing you want large distances and not sure what the stopping criteria is\n\nalgorithm discussed in this paper, Variable selection in Functional Additive Regression Models\n\nHarrell is skeptical. ‚ÄúI‚Äôd be surprised if the probability that adaptive lasso selects the‚Äùright‚Äù variables is more than 0.1 for N &lt; 500,000.‚Äù",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-conc",
    "href": "qmd/regression-regularized.html#sec-reg-reg-conc",
    "title": "Regularized",
    "section": "Concepts",
    "text": "Concepts\n\nShrinking effect estimates turns out to always be best\n\nOLS is the Best Linear Unbiased Estimator (BLUE), but being unbiased means the variance of the estimated effects is large from sample to sample and therefore outcome variable predictions using OLS don‚Äôt generalize well.\nIf you predicted y using the sample mean times some coefficient, it‚Äôs always(?) the case that you‚Äôll have a better generalization error with a coefficient less than 1 (shrinkage).\n\nRegularized Regression vs OLS\n\nAs N ‚Üë, standard errors ‚Üì\n\nregularized regression and OLS regression produce similar predictions and coefficient estimates.\n\nAs the number of covariates ‚Üë (relative to the sample size), variance of estimates ‚Üë\n\nregularized regression and OLS regression produce much different predictions and coefficient estimates\nTherefore OLS predictions are usually fine in a low dimension world (not usually the case)",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-ridge",
    "href": "qmd/regression-regularized.html#sec-reg-reg-ridge",
    "title": "Regularized",
    "section": "Ridge",
    "text": "Ridge\n\nThe regularization reduces the influence of correlated variables on the model because the weight is shared between the two predictive variables, so neither alone would have strong weights. This is unlike Lasso which just drops one of the variables (which one gets dropped isn‚Äôt consistent).\nLinear transformations in the design matrix will affect the predictions made by ridge regression.",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-lasso",
    "href": "qmd/regression-regularized.html#sec-reg-reg-lasso",
    "title": "Regularized",
    "section": "Lasso",
    "text": "Lasso\n\nWhen lasso drops a variable, it doesn‚Äôt mean that the variable wasn‚Äôt important.\n\nThe variable, \\(x_1\\), could‚Äôve been correlated with another variable, \\(x_2\\), and lasso happens to drop \\(x_1\\) because in this sample, \\(x_2\\), predicted the outcome just a tad better.\n\n\n\nAdaptive LASSO\n\n\nPurple dot indicates that it‚Äôs a weighted (\\(w_j\\)) version of LASSO\nGreen checkmark indicates it‚Äôs optimization is a convex problem\nBetter Selection, Bias Reduction are attributes that it has that are better than standard LASSO\nWeighted versions of the LASSO attach the particular importance of each covariate for a suitable selection of the weights. Joint with iteration, this modification allows for a reduction of the bias.\n\nZhou (2006) say that you should choose your weights so the adaptive Lasso estimates have the Oracle Property:\n\nYou will always identify the set of nonzero coefficients‚Ä¶when the sample size is infinite\nThe estimates are unbiased, normally distributed, and the correct variance (Zhou (2006) has the technical definition)‚Ä¶when the sample size is infinite.\n\nTo have these properties, \\(w_j = \\frac{1}{|\\hat\\beta_j|^q}\\), where \\(q &gt; 0\\) and \\(\\hat\\beta_j\\) is an unbiased estimate of the true parameter, \\(\\beta\\)\n\nGenerally, people choose the Ordinary Least Squares (OLS) estimate of \\(\\beta\\) because it will be unbiased. Ridge regression produces coefficient estimates that are biased, so you cannot guarantee the Oracle Property holds.\n\nIn practice, this probably doesn‚Äôt matter. The Oracle Property is an asymptotic guarantee (when \\(n \\rightarrow \\infty\\)), so it doesn‚Äôt necessary apply to your data with a finite number of observations. There may be scenarios where using Ridge estimates for weights performs really well. Zhou (2006) recommends using Ridge regression over OLS when your variables are highly correlated.\n\n\n\nSee article, Adaptive LASSO, for examples with a continuous, binary, and multinomial outcome",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-firth",
    "href": "qmd/regression-regularized.html#sec-reg-reg-firth",
    "title": "Regularized",
    "section": "Firth‚Äôs Estimator",
    "text": "Firth‚Äôs Estimator\n\nPenalized Logistic Regression estimator\nFor sample sizes less than around n = 1000 or sparse data, using Firth Estimator is recommended\nMisc\n\nNotes from\n\nThread\n\nPackages\n\n{brglm2} -\n{logistf} - Includes FLIC and FLAC extensions; uses profile penalized likelihood confidence intervals which outperform Wald intervals; includes a function that performs a penalized likelihood ratio test on some (or all) selected factors\n\nemmeans::emmeans is supported\n\n\nInvariant to linear transformations of the design matrix (i.e.¬†predictor variables) unlike Ridge Regression\nWhile the standard Firth correction leads to shrinkage in all parameters, including the intercept, and hence produces predictions which are biased towards 0.5, FLIC and FLAC are able to exclude the intercept from shrinkage while maintaining the desirable properties of the Firth correction and ensure that the sum of the predicted probabilities equals the number of events.\n\nPenalized Likelihood\n\\[\nL^*(\\beta\\;|\\;y) = L(\\beta\\;|\\;y)\\;|I(\\beta)|^{\\frac{1}{2}}\n\\]\n\nEquivalent to penalization of the log-likelihood by the Jeffreys prior\n\\(I(\\beta)\\) is the Fisher information matrix, i. e. minus the second derivative of the log likelihood\n\nMaximum Likelihood vs Firth‚Äôs Correction\n\nBias\n\nVariance\n\nCoefficient and CI bar comparison on a small dataset (n = 35, k = 7)\n\n\nLimitations\n\nRelies on maximum likelihood estimation, which can be sensitive to datasets with large random sampling variation. In such cases, Ridge Regression may be a better choice as it provides some shrinkage and can stabilize the estimates by pulling them towards the observed event rate.\nLess effective than ridge regression in datasets with highly correlated covariates\nFor the Firth Estimator, the Wald Test can perform poorly in data sets with extremely rare events.",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html",
    "href": "qmd/surveys-sampling-methods.html",
    "title": "Sampling Methods",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Surveys",
      "Sampling Methods"
    ]
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-misc",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-misc",
    "title": "Sampling Methods",
    "section": "",
    "text": "Notes from:\n\nSurvey data in the field of economy and finance (ebook)",
    "crumbs": [
      "Surveys",
      "Sampling Methods"
    ]
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-terms",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-terms",
    "title": "Sampling Methods",
    "section": "Terms",
    "text": "Terms\n\nIn a survey setting,\n\nU denotes a finite population (i.e.¬†[target population) of N units\nA sample s of n units (n‚â§N) is taken from U\n\nDesign Weights - The average number of units in the population that each sampled unit represents. This weight is determined by the sampling method and is an important part of the estimation process.\nEmpirical Design - When the inclusion probabilities (see below) are unknown\n\nSee Non-Probabilistic Sampling Methods\nExamples\n\nQuota Sampling - Units are selected so to reflect known structures for the population\nExpert Sampling - Units are selected according to expert advice\nNetwork Sampling - Existing sample units recruit future units from among their ‚Äònetwork‚Äô.\n\n\nEstimator of the parameter, Œ∏, is a function of sample observations\n\nExample: Sample Mean\n\\[\n\\hat{\\theta} = \\frac {\\sum_{i \\in S}y_i}{S} = \\bar{y}_S\n\\]\n\nPopulation mean of the study variable can be estimated by the mean value over the sample observations\n\n\nInclusion Probability - The probability for a unit to appear in the sample\nProbabilistic Design - When every element in the population has a fixed, known-in-advance inclusion probabilities\nSampling Bias - The probability distribution in the collected dataset deviates from its true natural distribution one would actually observe in the wilderness.\nSampling Frame - An exhaustive list of all the individuals which comprise the target population (Also see Surveys, Design &gt;&gt; Sources of Error &gt;&gt; Coverage or Frame Error)\n\nStudy Parameter (Œ∏) - Linear parameter of the study variable, such as a mean, a total or a proportion, or a more complex one such as a ratio between two population means, a correlation or a regression coefficient, a quantile (e.g.¬†median, quartile, quintile or decile) or an inequality measure such as the Gini or the Theil coefficient. (also see estimator)\nStudy Variable (y)\n\nQuantitative - Numerical information (e.g.¬†the total disposable income or the total food consumption)\nQualitative - Categorical information (e.g.¬†gender, citizenship, country of birth, marital status, occupation or activity status)",
    "crumbs": [
      "Surveys",
      "Sampling Methods"
    ]
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp",
    "title": "Sampling Methods",
    "section": "Probabilistic Sampling Methods",
    "text": "Probabilistic Sampling Methods\n\nSimple Random Sampling (SRS)\n\nA method of selecting n units out of N such that every sample s of size n has the same probability of selection\nSimple Inclusion Probability - the probability for a unit to appear in the sample\n\\[\n\\pi_i = \\mbox{Pr}(i \\in S) = \\sum \\limits_{i \\in s \\in S} \\mbox{Pr}(S = s) = \\frac {\\binom{N-1}{n-1}}{\\binom{N}{n}} = \\frac {n}{N}\n\\]\n\n\\(n\\) is the size of sample, \\(s\\), and \\(N\\) is the target population size\n\nDouble Inclusion Probability - the probability for 2 units to appear in the sample\n\\[\n\\pi_{ij} = \\mbox{Pr}(i,j \\in S) = \\sum \\limits_{i,j \\in s \\in S} \\mbox{Pr}(S = s) = \\frac {\\binom{N-2}{n-2}}{\\binom{N}{n}} = \\frac {n}{N} \\frac {n-1}{N-1}\n\\]\n\nWhere \\(i \\neq j\\)\n\\(n\\) is the size of sample, \\(s\\), and \\(N\\) is the target population size\n\nWithout Replacement (most common)\n\nAt the first extraction, each one of the population units will have an equal probability of selection, \\(1/N\\).\nAt the second extraction, the remaining N-1 units will have a selection probability equal to \\(1/(N-1)\\). Etc.\n\nWith Replacement - all the units of the population will have all the same probability of being selected \\(1/N\\) Advantages:\n\nIt‚Äôs simple and doesn‚Äôt use auxiliary information on the population\nThe selection is random and, then, any unit is favoured\nThe sample is representative Disadvantages:\nThe choice of the element is completely random\nA complete list of the population units is necessary\nIt‚Äôs time and cost consuming\n\nEstimated Total of the study variable, \\(\\hat{Y}\\)\n\\[\n\\hat{Y}_{SRS} = N \\bar{y}\n\\]\n\nWhere \\(N\\) is the target population size\n\nEstimated Mean of the study variable, \\(\\bar{Y}\\)\n\\[\n\\hat{\\bar{Y}}_{SRS} = \\bar{y}\n\\]\n\nWhere \\(\\bar{y}\\) is the sample mean\n\nVariance for Estimated Total\n\\[\nV(\\hat{Y}_{SRS}) = N^2 (1-f) \\frac {S^2_y}{n}\n\\]\n\n\\(S^2_y\\) is the dispersion of the study variable, \\(y\\), over the population \\(U\\)\n\\[\nS^2_y = \\frac {1}{N-1} \\sum_{i \\in U} (y_i - \\bar{Y})^2\n\\]\nSampling Rate or Sampling Fraction: \\(\\mbox{f} = n/N\\)\nFinite Population Correction Factor: \\(1-\\mbox{f}\\)\n\nVariance for Estimated Mean\n\\[\n\\hat{V}(\\bar{y}) = (1-\\mbox{f}) \\frac {s^2_y}{n}\n\\]\n\nSample Dispersion\n\\[\ns^2_y = \\frac{1}{n-1}\\sum_{i \\in s} (y_i - \\bar{y})^2\n\\]\n\nEstimated size of subpopulation, \\(A\\)\n\\[\n\\hat{N}_A = Np_A\n\\]\n\n\\(p_A\\) is the sample proportion of units from target subpopulation, \\(U_A\\)\n\ni.e.¬†(I think) \\(n_A / N_A\\)\n\nExamples: Subpopulations\n\nTotal number of males or females in the population\nTotal number of elderly people aged more than 65 in the population\nTotal number of establishments having more than 50 employees in a certain geographical region or in a sector of activity.\n\n\nVariance of sample proportion of subpopulation, \\(A\\)\n\\[\n\\hat{V}(p_A) = \\frac{p_A(1-p_A)}{n}\n\\]\nDomain Parameter Estimation\n\nRefers to estimating population parameters for sub-populations of interest, called domains. For instance, one may wish to estimate the mean household disposable income broken down by personal characteristics such as age, gender or citizenship\n\nI think this is different from ‚ÄúEstimated size of subpopulation, A‚Äù (above) because we‚Äôre estimating a study variable of subpopulation vs the size of the subpopulation\n\nEstimated Total of the study variable\n\\[\n\\hat{Y}_D = \\frac{N \\cdot n_D}{n} \\; \\bar{y}_D\n\\]\n\n\\(\\bar{y}_D\\) - The sample mean of study variable, \\(y\\), within the domain, \\(D\\)\n\\(n_D\\) - The total number of sample units from the sample \\(s\\) which fall into domain, \\(D\\)\n\nSample size \\(n_D\\) is a random variable of mean \\(\\bar{n}D = nP_D\\) where \\(P_D = N_D / N\\)\n\nI guess this is a random variable because this is strictly SRS, so you aren‚Äôt stratifying by \\(D\\) when you sample the target population. Therefore, the number of samples from \\(D\\) you happen to get will be random and have a distribution.\n\n\nAlternative: When the size of the domain, \\(N_D\\), of \\(U_D\\) is known\n\n\\(\\hat{Y}_{D,\\mbox{alt}} = N_D \\cdot \\bar{y}D\\)\nThis formula has a provably (see ebook in Misc) lower variance than the original formula\n\n\nVariance for Estimated Total\n\\[\nV(\\hat{Y}_D) \\approx N^2_D \\left(\\frac{1}{\\bar{n}_D} - \\frac{1}{N_D}\\right)S^2_D \\left(1 + \\frac{1-P_D}{CV^2_D} \\right)\n\\]\n\nWhere\n\\[\n\\begin{align}\n&S^2_D = \\sum_{k \\in U_D} \\frac{(y_k - \\bar{Y}_D)^2}{N_D - 1}\\\\\n&CV_D = \\frac{S_D}{\\bar{Y}_D}\n\\end{align}\n\\]\nAssumes the population sizes, \\(N\\) and \\(N_D\\), are ‚Äúlarge enough.‚Äù\nFor the Alternative Estimated Total formula (see above)\n\\[\nV(\\hat{Y}_{D,alt}) \\approx N^2_D \\left(\\frac{1}{\\bar{n}_D} - \\frac{1}{N_D} \\right)S^2_D\n\\]\n\nAssumes the sample size, \\(n_D\\), is ‚Äúlarge enough.‚Äù\nA provably lower variance (see ebook in Misc)\n\n\n\n\n\n\nUnequal Probability Sampling\n\nDifferent units in the population will have different probabilities of being included in a sample.\n\nUnlike SRS, where each unit has an equal probability of being included in the sample\n\nUnequal probability sampling can result in estimators having higher precision than when simple random sampling or other equal probability designs are used.\n\nEmphasizes the importance of utilizing so-called ‚Äúauxiliary‚Äù information as a way to boost sampling precision. (see œÄk below)\n\nHorvitz-Thompson estimator (without replacement selection)\n\nEstimated Total, \\(\\hat{Y}\\), for the study variable\n\\[\n\\hat{Y}_{HT}= \\sum_{k \\in S} \\frac{y_k}{\\pi_k} = \\sum_{k \\in s} d_ky_k\n\\]\n\\(d_k = 1/\\pi_k\\) is the design weight of unit, \\(k\\), of sample, \\(s\\)\n\\(\\pi_k\\) is the inclusion probability for unit, \\(k\\), of sample, \\(s\\)\n\nIn practice, as the study variable \\(y\\) is unknown, the inclusion probabilities should be taken proportional to an auxiliary variable \\(x\\) assumed to have a linear relationship with \\(y: œÄ \\propto x\\) (probability proportional to size sampling)\nAn inclusion probability that is optimal with respect to one study variable may be far from optimal with other study variables. In case of multi-purpose surveys, this is a major problem which generally prevents from using unequal probability sampling.\n\nAlternatively, survey statisticians use stratification as we know it always make accuracy better no matter the study variable.\n\n\n\nHansen-Hurwitz estimator has been proposed in case of sampling with replacement.\n\n\n\nCluster Sampling\n\nAssumes population has natural clusters (e.g.¬†family unit). Different from Stratified Sampling in that the clustering characteristic(s) is the same for all clusters (between cluster variation = 0) and the within cluster variation is heterogeneous (i.e.¬†within cluster variation != 0).\n\nExample: Family units in NYC are chosen randomly chosen. The variation between family members is whats studied.\n\nAdvantages:\n\nit‚Äôs efficient when the clusters constitute naturally formed subgroups, for which we don‚Äôt possess the list of the population\nStudying clusters can be less expensive than simple random sampling.\n\nDisadvantages:\n\nThe conditions of the clusters aren‚Äôt always respected. The clusters may contain similar elements.\n\n\n\n\nStratified Sampling\n\n\nMisc\n\nNotes from Chapter 3 Stratification\nThe population is classified into subpopulations, called strata, based on some categorical characteristics, such as age, gender, education\nStratified sampling buckets the population into k strata (e.g., countries), and then the experiment random samples individuals from each stratum independently.\nAssumes between group variation is not 0 (i.e.¬†heterogeneous) and within-group variation is 0 (i.e.¬†homogeneous)\nReasons for stratification\n\nBaseline for group A different from group B\nReason to believe the effect for group A will be different from group B\n\nAdvantages\n\nIt can be more efficient than simple random sampling\nThere is less risk of obtaining non-representative samples\n\nDisadvantages\n\nIt needs the availability of auxiliary information on the population.\nThere are strict conditions for the strata\n\n\nEstimated Total, \\(\\hat{Y}\\), for the study variable and the\nEstimated Mean of the study variable, \\(\\bar{Y}\\) (respectively)\n\\[\n\\begin{align}\n&\\hat{Y}_\\mbox{STSRS}=\\sum_{h=1}^H N_h \\bar{y}_h \\\\\n&\\hat{\\bar{Y}}_\\mbox{STSRS} = \\sum_{h=1}^H W_h \\bar{y}_h\n\\end{align}\n\\]\n\nAssumes SRS within each strata\n\\(N\\) is the population size\n\\(N_h\\) is the population strata size for strata, \\(h\\)\n\\(W_h\\) is the frequency weight where \\(W_h = N_h / N\\)\n\\(\\bar{y}_h\\) is the sample mean of strata, \\(h\\)\n\nVariance for Estimated Total (assuming SRS within strata)\n\\[\nV(\\hat{Y}_\\mbox{STSRS}) = \\sum_{h=1}^H N_h^2 (1-\\mbox{f}_h)\\frac{S_h^2}{n_h}\n\\]\n\n\\(n_h\\) is the sample size for stratum, \\(h\\)\nStratum Sampling Fraction: \\(\\mbox{f}_h = n_h / N_h = n/N\\) (which is just \\(\\mbox{f}\\))\n\nAssumes \\(\\mbox{f}_h\\) is the same for each strata\n\nStratum Dispersion: \\(S_h^2\\) should be similar to the sample dispersion for SRS below, except the domain of the variables is within stratum, \\(h\\) (e.g.¬†\\(n ‚Üí n_h,\\) \\(»≥ ‚Üí »≥_h\\), etc.)\n\nSample Dispersion for SRS\n\\[\ns_y^2 = \\frac{1}{n-1}\\sum_{i \\in s} (y_i - \\bar{y})^2\n\\]\n\n\nVariance for Estimated Mean (assuming SRS within strata)\n\\[\nV(\\bar{Y}_\\mbox{STSRS}) = (1 - \\mbox{f}) \\frac{s^2_h}{n_h}\n\\]\n\n\\(n_h\\) will be the same for all \\(h\\), so it‚Äôs constant in this case\nSampling Fraction: \\(\\mbox{f} = N / n\\)\n\n\\(n\\) is the overall sample size\n\nWithin-Stratum Dispersion\n\\[\nS_w^2 = \\sum_{h=1}^H W_hS_h^2\n\\]\n\n\\(S^2_h\\): See above\n\\(N\\) is the population size and \\(N_h\\) is the population strata size for strata, \\(h\\)\n\\(W_h\\) is the frequency weight where \\(W_h = N_h / N\\)\n\n\nDesign Weights: \\(d_i = N_h / n_h \\;\\;\\forall \\in s_h\\)\n\nFor SRS, design weights are equal within each stratum\n\\(s_h\\) is the set of samples within stratum, \\(h\\)\n\nStratum sample size allocation methods\n\nLet assume the overall sample size, \\(n\\), has been fixed (generally out of budgetary considerations). We seek to determine which sample size, \\(n_h\\), is to be drawn out of each stratum in order to achieve statistical optimality under cost considerations.\nEqual Allocation\n\n\\(n^\\mbox{eq}_h = n / H\\)\n\\(H\\) is the number of strata\nPerforms poorly when the dispersions, \\(S^2_h\\), are different from one stratum to another\n\nProportional Allocation\n\nConsists of selecting samples in each stratum in proportion to the size, \\(N_h\\), of the stratum population\n\\(n^\\mbox{prop}_h = (n \\cdot N_h) / N = n \\cdot W_h\\)\nVariance\n\\[\n\\begin{align}\nV(\\hat{\\bar{Y}}_\\mbox{prop}) &= (1-\\mbox{f})\\frac{S^2_w}{n} \\\\\n&=\\frac{\\sum_{h=1}^h W_h S^2_h}{n} - \\frac{\\sum_{h=1}^h W_h S^2_h}{N}\n\\end{align}\n\\]\n\nOptimal or Neyman Allocation\n\nSeeks to minimize the variance under the cost constraint\n\\[\n\\sum_{h=1}^H c_h n_h = C_0\n\\]\n\n\\(C_0\\) is the overall budget available and \\(c_h\\) the average survey cost for an individual in stratum \\(h\\).\n\nStrata Sample Size with Cost Constraint\n\\[\n\\forall h \\;\\; n_h^\\mbox{opt} = \\frac{N_hS_h}{\\sqrt{c_h}} \\frac{C_0}{\\sum_{h=1}^H N_h S_h \\sqrt{c_h}}\n\\]\nStrata Sample Size without Cost Constraint\n\\[\nn_h^\\mbox{opt} = n \\frac{N_hS_h}{\\sum_{h=1}^H N_hS_h}\n\\]\nVariance\n\\[\nV(\\hat{\\bar{Y}}_\\mbox{SRS}) = \\frac{1}{n}\\sum_h W_h(\\bar{Y}_h - \\bar{Y})^2 - \\frac{1}{n}\\sum_h W_h(S_h - \\bar{S})^2\n\\]\n\n\\(\\bar{S}\\) must be the mean sqrt dispersion across all stratum\n\nContrary to proportional allocation, the Neyman allocation is variable-specific: optimality is defined with respect to one study variable, and what is optimal with respect to one variable may be far from optimal with respect to another.\nThe gain in accuracy as compared to proportional allocation is pretty small. That‚Äôs why in practice proportional allocation is often preferred to optimal allocation.\n\nBalanced Allocation\n\\[\n\\forall h \\;\\; n_h^\\mbox{bal} = \\frac{\\tilde{n}}{H} + (n - \\tilde{n})W_h\n\\]\n\n\\(\\tilde{n}\\) is a subsample of \\(n\\) that is equally allocated (see above) among the strata which insures minimal precision within the strata (i.e.¬†locally)\nThe rest of the sample (\\(n-\\tilde{n}\\)) can be allocated using either proportional or optimal allocations (see above) in order to optimize accuracy for the overall sample (i.e.¬†globally)\nBoth proportional and Neyman allocations increase sample accuracy at global level, but may happen to perform very poorly when it comes to strata (e.g.¬†regional) level estimates.\n\n\n\n\n\nMulti-Stage Sampling\n\nMisc\n\nUseful when no sampling frame is available\nStages\n\nAt first-stage sampling, a sample of Primary Sampling Units (PSU) is selected using a probabilistic design (e.g.¬†simple random sampling or other, with or without stratification)\nAt second-stage sampling, a sub-sample of Secondary Sampling Units (SSU) is selected within each PSU selected at first-stage. The selection of SSU is supposed to be independent from one PSU to another.\nAt third-stage sampling a sample of Tertiary Sampling Units can be selected with each of the SSU selected at second stage.\netc.\n\nExample: (given an absence of any frame of individuals)\n\nSelect a sample of municipalities (first-stage sampling),\nSelect a sample of neighbourhoods (second-stage sampling) within each selected municipality,\nSelect a sample of households (third-stage sampling) within each of the neighbourhoods selected a second stage\nSelect a sample of individuals (fourth-stage sampling) within each household.\n\nAdvantages:\n\nCan be more efficient than using only 1 of the sampling strategies\nCan decrease sample size if there are numerous units within strata or clusters\n\nDisadvantages:\n\nIf sampling assumptions aren‚Äôt valid, multi-stage sampling results to be less efficient than simple random sampling.\n\n\nExample: 2-stage cluster sampling\n\nAdds a second stage to cluster sampling. After clusters are chosen, units within those clusters are randomly sampled.\n\nExample: 2-Stage Stratified Sampling\n\nNotes from Two Stage Stratified Random Sampling ‚Äî Clearly Explained\nUseful for when you have hierarchical strata (e.g.¬†towns/blocks and households)\nExample: An education study of students where:\n\nSchools (first stage sampling units) may be selected with probabilities proportional to school size\nStudents (second stage units) within selected schools may be selected by stratified random sampling\n\nStage 1\n\n\n(Random?) Sample from group of First Stage Units (FSU)\n\nEach FSU usually has a population within a range\ne.g.¬†census geographies (census block, metropolitan statistical area, etc.)\n\n\nStage 2\n\n\nAll Second Stage Units (SSU) within each FSU are pooled together to create a population\n\nSSUs are the base geography unit you want to measure\ne.g.¬†households\n\nThen each SSU is binned into Second Stage Strata (SSS) according to a characteristic or set of characteristics\n\ne.g.¬†race, age, income level, education, etc.\nThe SSS are stratified sampled\n\n\n\n\n\n\nSystematic Sampling\n\n\nSteps\n\nAfter choosing a sample size, n, calculate the sampling interval k = N/n, where N is the population size\n\nIn the example, we have 9 smiles and we want to obtain a sample of 3 units, then N = 9, n = 3 and k = 9/3 =3.\n\nSelect a random starting point, r, which is a random integer between 1 and k: 1‚â§r‚â§k.\n\nIn the example, r = 2, where 1‚â§r‚â§3.\n\nOnce the first unit is selected, we take every following kth item to build the sample: r, r+k, r+2k , ‚Ä¶ , r+(n-1)k.\n\nAdvantages:\n\nThe random selection is applied only on the first item, while the rest of the items selected depend on the position of the first item and a fixed interval at which items are picked.\n\nDisadvantages:\n\nIf the list of the population elements presents a determined order, there is the risk of obtaining a non-representative sample",
    "crumbs": [
      "Surveys",
      "Sampling Methods"
    ]
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp-nonprob",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp-nonprob",
    "title": "Sampling Methods",
    "section": "Non-Probabilistic Sampling Methods",
    "text": "Non-Probabilistic Sampling Methods\n\nMisc\n\nMostly used when probabilistic methods aren‚Äôt possible due to rarity or difficulty in obtaining a representative sample of the population being studied or cost constraints of the experiment\nPackages\n\n{nonprobsvy} - Inference Based on Non-Probability Samples and {jointCalib} - A Joint Calibration of Totals and Quantiles\n\nPaper: Inference for non-probability samples using the calibration approach for quantiles\nUtilizes a method of joint calibration for totals and quantiles to extend existing inference methods for non-probability samples, such as inverse probability weighting, mass imputation and doubly robust estimators which produces results that are more robust against model mis-specification and helps to reduce bias and improve estimation efficiency.\n\n\n\nQuota Sampling\n\nSimilar to Stratified Sampling (see Probabilistic Sampling Methods) except:\n\nEach stratum‚Äôs sample size is called its quota\nEach stratum‚Äôs sample size takes into account its distribution in the whole population.\n\nExample: If 80% of the population are males, then 80% of the sample should be males.\n\nWithin each stratum‚Äôs quota, the interviewer is free to choose the participants to interview.\n\nThis seems to be the main difference\n\n\nAdvantages:\n\nIt‚Äôs time and cost-effective, in particular with respect to the stratified sampling.\n\nDisadvantages:\n\nThe results can be distorted due to the discretion of the interviewers or the non-response bias\nThe quota sample can produce a selection bias\n\n\nJudgemental Sampling (aka Purposive Sampling)\n\nThe researcher selects the participants because he believes they are representative of the population\n\nUseful when there is only a limited number of people with specific traits\n\nAdvantages:\n\nIt‚Äôs time and cost-effective\nIt‚Äôs suitable to study a certain cultural domain, where the knowledge of an expert is needed\n\nDisadvantages:\n\nIt can lead to a high selection bias the bigger is the gap between the researcher‚Äôs knowledge and the actual situation of the population\n\n\nConvenience Sampling\n\nThe researcher chooses anyone that is ‚Äúconvenient‚Äù to him, i.e.¬†people that are immediately available to answer the questions, without any specific criteria\n\nUsually volunteers\n\nAdvantages:\n\nIt‚Äôs very cheap and fast\n\nDisadvantages:\n\nIt leads to a non-representative sample\n\n\nSnowball Sampling\n\nThe researcher asks already recruited people to identify other potential participants, and so on\n\nUseful for rare populations, for which it‚Äôs not possible to have the list of the population or it‚Äôs difficult to locate the population.\n\ne.g.¬†illegal immigrants\n\n\nAdvantages:\n\nIt‚Äôs useful for market studies or researches about delicate topics.\n\nDisadvantages:\n\nThe sample may be non-representative since it‚Äôs not random, but depends on the people contacted directly or indirectly by the researcher\nIt‚Äôs time-consuming",
    "crumbs": [
      "Surveys",
      "Sampling Methods"
    ]
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-stat",
    "href": "qmd/outliers.html#sec-outliers-stat",
    "title": "Outliers",
    "section": "Statistics",
    "text": "Statistics\n\nFor a skewed distribution, a Winsorized Mean (percentage of points replaced) often has less bias than a Trimmed Mean\nFor a symmetric distribution, a Trimmed Mean (percentage of points removed) often has less variance than a Winsorized Mean.\nHodges‚ÄìLehmann Estimator\n\nPackages: {DescTools::HodgesLehmann}\nA robust and nonparametric estimator of a population‚Äôs location parameter.\nFor populations that are symmetric about one median, such as the Gaussian or normal distribution or the Student t-distribution, the Hodges‚ÄìLehmann estimator is a consistent and median-unbiased estimate of the population median.\n\nHas a Breakdown Point of 0.29, which means that the statistic remains bounded even if nearly 30 percent of the data have been contaminated.\n\nSample Median is more robust with breakdown point of 0.50 for symmetric distributions, but is less efficient (i.e.¬†needs more data).\n\n\nFor non-symmetric populations, the Hodges‚ÄìLehmann estimator estimates the ‚Äúpseudo‚Äìmedian‚Äù, which is closely related to the population median (relatively small difference).\n\nThe psuedo-median is defined for heavy-tailed distributions that lack a finite mean.\n\nFor two-samples, it‚Äôs the median of the difference between a sample from x and a sample from y.\nOne-Variable Procedure\n\nFind all possible two-element subsets of the vector.\nCalculate the mean of each two-element subset.\nCalculate the median of all the subset means.\n\nTwo-Variable Procedure\n\nFind all possible two-element subsets between the two vectors (i.e.¬†cartesian product)\nCalculate difference between subsets\nCalculate median of differences",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-misc",
    "href": "qmd/distributions.html#sec-distr-misc",
    "title": "Distributions",
    "section": "",
    "text": "For a skewed distribution, a Winsorized Mean (percentage of points replaced) often has less bias than a Trimmed Mean\nFor a symmetric distribution, a Trimmed Mean (percentage of points removed) often has less variance than a Winsorized Mean.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-cauchy",
    "href": "qmd/distributions.html#sec-distr-cauchy",
    "title": "Distributions",
    "section": "Cauchy",
    "text": "Cauchy\n\nIt‚Äôs a Student t-distribution with one degree of freedom\nThe Hodges-Lehmann estimate is an efficient estimator of the population median (See Outliers &gt;&gt; Statistics &gt;&gt; Hodges-Lehmann Estimator)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-preproc",
    "href": "qmd/outliers.html#sec-outliers-preproc",
    "title": "Outliers",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nRemoval\n\nAn option if there‚Äôs sound reasoning (e.g.¬†data entry error, etc.)\n\nWinsorization\n\nA typical strategy is to set all outliers (values beyond a certain threshold) to a specified percentile of the data\nExample: A 90% winsorization would see all data below the 5th percentile set to the 5th percentile, and data above the 95th percentile set to the 95th percentile.\nPackages\n\n({DescTools::Winsorize})\n({datawizard::winsorize})",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html",
    "href": "qmd/project-analyses.html",
    "title": "Analyses",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-misc",
    "href": "qmd/project-analyses.html#sec-proj-anal-misc",
    "title": "Analyses",
    "section": "",
    "text": "Also see\n\nLogistics, Demand Planning &gt;&gt; Stakeholder Questions\n\nThese are questions for the stakeholder(s) when preparing to create a forecasting model, but many apply to other types of projects including Analysis.\n\nProject, Development &gt;&gt; CRISP-DM\nProject, Development &gt;&gt; Agile &gt;&gt; Data Science Lifecycle\n\nSee Thread on an analysis workflow using {targets}",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-analy-gen",
    "href": "qmd/project-analyses.html#sec-proj-analy-gen",
    "title": "Analyses",
    "section": "General",
    "text": "General\n\nGeneral Questions\n\n‚ÄúWhat variables are relevant to the problem I‚Äôm trying to solve?‚Äù\n‚ÄúWhat are the key components of this data set?‚Äù\n‚ÄúCan this data be categorized?‚Äù\n‚ÄúIs this analysis result out of the ordinary?‚Äù\n‚ÄúWhat are the key relationships?‚Äù\n‚ÄúIs this the best way this company could be carrying out this task?‚Äù\n‚ÄúWhat will happen under new conditions?‚Äù\n‚ÄúWhat factors are best used to determine or predict this eventuality?‚Äù\n\nBreak down the problem into parts and focus on those during EDA\n\nAlso see Decison Intelligence &gt;&gt; Mental Models for details on methods to break down components\nExample: Why are sales down?\n\nHow are sales calculated?\n\ne.g.¬†Total Sales = # of Orders * Average Order Value\n\nBreakdown # of orders and average order value\n\nnumber of orders = number of walk-ins * % conversion\n\nHas walk-ins or conversion declined?\n\nAverage Order Value\n\nBin avg order value by quantiles, plot and facet or group by binned groups. Is one group more responsible for the decline than others?\n\n\nIs there regional or store or brand variability? (grouping variables)\n\n\nDrill down into each component until the data doesn‚Äôt allow you to go any farther.\nSegment data by groups\n\nColor or facet by cat vars\nPay attention to counts of each category (may need to collapse categories)\nCommon segments in product analytics\n\nFree vs Paid users\nDevice Type (desktop web vs mobile web vs native app)\nTraffic Source (people coming from search engines, paid marketing, people directly typing in your company‚Äôs URL into their browser, etc.)\nDay of the Week.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-tropf",
    "href": "qmd/project-analyses.html#sec-proj-anal-tropf",
    "title": "Analyses",
    "section": "TROPICS framework",
    "text": "TROPICS framework\n\nMisc\n\nFor analyzing changes in key performance metrics\nFrom https://towardsdatascience.com/answering-the-data-science-metric-change-interview-question-the-ultimate-guide-5e18d62d0dc6\nComponents: Time, Region, Other Internal Products, Platform, Industry and Competitors, Cannibalization, Segmentation\n\nTime\n\nWhat to explore\n\nHow has our performance been trending over the last few weeks (or months)?\n\nExample: If we saw a 10% increase in the last week, was the percentage change in the weeks before also 10%? In which case the 10% may actually be pretty normal? Or was the change lower? Higher?\n\nIs this change seasonal? Do we see the same spike around this time each year?\n\nExample: Does WhatsApp see a spike in messages sent during the holiday season?\n\nWas the change sudden or gradual? Did we see a sudden spike or drop overnight? Or has the metric gradually been moving in this direction over time?\n\nExample: If product usage jumps by 50% overnight could there be a bug in our logging systems?\n\nAre there specific times during the day or week where this change is more pronounced?\n\nSolution examples\n\nIf the change is seasonal then there may not necessarily be anything you need to ‚Äòsolve‚Äô for. But, you can leverage this to your advantage.\n\nExample: Amazon sales may jump up on Black Friday so they would want to make sure they have the proper infrastructure in place so the site doesn‚Äôt crash. They may also see if there are certain types of products that are popular purchases and increase their inventory accordingly.\n\nIf there is a sudden decline, there may be a bug in the logging or a new feature or update recently launched that‚Äôs creating problems that you may need to roll back.\nIf there‚Äôs a gradual decline, it may indicate a change in user behavior.\n\nExample: If the time spent listening to music is declining because people prefer to listen to podcasts then Spotify may want to focus more of their content inventory on podcasts.\n\n\n\nRegion\n\nWhat to explore\n\nIs this change concentrated in a specific region or do we see a similar change across the board?\n\nSolution examples\n\nThere may be newly enforced regulations in countries that are affecting your product metrics. You would need to do further research to assess the impacts of these regulations and potential workarounds.\n\nExample: Uber was temporarily banned in London in 2019 for repeated safety failures which resulted in a series of lawsuits and court cases.\n\nPopular local events may also be potential explanations. While these may not be areas to ‚Äòsolve‚Äô for they can be opportunities to take advantage of.\n\nExample: Coachella season means a jump in the number of Airbnb bookings in Southern California that are capitalized on by surge pricing.\n\n\n\nOther Internal Products\n\nWhat to explore\n\nIs this change specific to one product or is it company-wide? How does this metric vary across our other product offerings?\n\nExample: If the Fundraising feature on Facebook is seeing increased usage, is the swipe up to donate feature on Instagram (which Facebook owns) also seeing a similar uptick?\n\nAre there other metrics that have also changed in addition to the one in question?\n\nExample: If the time spent on Uber is going down, is the number of cancellations by drivers also declining (implying people are spending less time on the app because they‚Äôre having a more reliable experience)?\n\n\nSolution examples\n\nIf there is a metric change across our other features and products, it‚Äôs likely a larger problem we should address with multiple teams and may need a Public Relations consultant.\n\nExample: Elon + Twitter.\n\n\n\nPlatform\n\nWhat to explore\n\nMobile vs Desktop?\nMac vs Windows?\nAndroid vs iOS?\n\nSolution examples\n\nIf there was a positive change in our metric on a specific platform (e.g.¬†iOS) and coincides with an (iOS) update we released, we would want to do a retrospective to determine what about that update was favorable so we can double down on it. Alternatively, if the metric change was negative, we may want to reconsider and even roll back the update.\nIf the change was due to a change in the platform experience (e.g.¬†app store placement, ratings) we may want to seek advice from our marketing team since this is a top of the funnel problem\nIf users are showing astrong preference for a specific platform, we want to make sure that the experience of the preferred platform is up to par. We also need to make sure our platform-specific monetization strategies are switching to follow the trend.\n\nExample: Facebook‚Äôs ad model was initially tied to the desktop app only and had to be expanded as mobile became the platform of preference.\n\n\n\nIndustry & Competitors\n\nWhat to explore\n\nWhen our decline began, was there a new competitor or category that emerged?\n\nExample: Did the number of users listening to Apple podcasts go down when Clubhouse came on to the scene?\n\nHave competitors changed their offering lately?\nIs the category as a whole declining?\n\nSolution examples\n\nIf the category is shifting as a whole, we should begin looking at larger-scale changes to the app.\n\nExample: What Kodak should have done.\n\nIf there‚Äôs a new competitor taking our market share, we can begin with reactivation campaigns on churned users. We may also want to conduct user research to understand the gap between our offering and those of our competitors\n\n\nCannibalization\n\nWhat to explore\n\nAre other products or features in our offering experiencing growth in the face of our decline or vice versa?\nHave we released a new feature that is drawing users away from our old features? If so, can we fully attribute the release of the new feature with the decline in the metric of our feature in question?\n\nExample: When Facebook released reactions, did the number of comments on a post go down because people found it easier to press a react button instead of writing a comment?\n\n\nSolution examples\n\nCannibalization may not necessarily be a bad thing. We need to determine whether this shift in user interest across our features is favorable by determining whether the new features align better with the goals of the business.\nCannibalization may also be an indication of but it is indicative of a change in user behavior. In which case we may want to consider if perhaps our core metrics need to change as user behaviors change.\n\nExample: If users care more about watching Instagram stories than engaging with the Instagram feed we may want to optimize for retention (because the ephemeral nature of stories is more likely to motivate users to keep coming back to the platform) instead of time spent on the app.\n\nWe can also look at ways to bridge the two features together to create a more unified platform.\n\n\nSegmentation\n\nWhat to explore\n\nHow does this metric vary by¬†user type:\n\nAge, sex, education\nPower users versus casual users * New users versus existing users\n\nHow does this metric vary by different attributes of the product:\n\nExample: If the time spent watching YouTube videos is going down, is it across longer videos or shorter clips? Is it only for DIY videos or interview tutorial content? Is the same number of people that started watching a video the same but a large chunk of them stop watching it halfway through?\n\n\nSolution examples\n\nIf the metric varies between new and existing users then maybe there is a overcrowding effect.\n\nExample: Reddit forums could hit a critical mass where new users feel lost and less likely to engage than existing users resulting in a drop in engagements per user\n\nIf users are dropping off at certain parts of the funnel then maybe the experience at that funnel step is broken.\n\nExample: While the same number of people are starting carts on Amazon there may be a drop in purchases if the payment verification system isn‚Äôt working.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-actanl",
    "href": "qmd/project-analyses.html#sec-proj-anal-actanl",
    "title": "Analyses",
    "section": "Actionable Analyses",
    "text": "Actionable Analyses\n\nNotes from: Driving Product Impact With Actionable Analyses\nActionable insights do not only provide a specific data point that might be interesting, but lay out a clear narrative how this insight is connected to the problem at hand, what the ramifications are, as well as possible options and next steps to take with the associated benefits/risks of (not) acting upon these.\nNot Actionable: Users under the age of 25 hardly use audiobooks.\n\nIs this good, bad? Should they be listening to audiobooks and is there anything we should do about it?\n\nActionable: Users under the age of 25 hardly use audiobooks because they never explore the feature in the app. However users who listen to audiobooks have a 20% higher retention rate.\n\nThis information tells us that audiobooks represent a potential opportunity to increase retention amongst younger users, however there seems to be more work to be done to encourage users exploring this feature.\n\nSteps\n\nProblem Statement: High-level business problem to solve (e.g.¬†Increasing Retention, Conversion Rate, Average Order Value)\n\nCan also be in regards to a metric that‚Äôs believed to be highly associated with a North Star metric like a Primary metric (See KPIs)\n\nOpportunity Areas: Areas or problems with a strong connection to the problem at hand\n\nInvestigate behaviors of users with the behavior that you‚Äôre interested in (i.e.¬†high or low values of the desired metric).\nDiscovering the characteristics of these users can help to figure out ways to encourage other users to act similarily or gain insight into the type of users you want to attract.\n\nLevers: Different ways to work on the opportunity areas\n\nA lever should be data-based and able to be validated on whether working to increase or decrease the lever will lead to a positive solution to the problem statement.\nThere are typically multiple levers for a given opportunity area\n\nThese should be ordered in terms of priority, and priority should be given to the lever that is believed to result in the greatest impact on the opportunity area that will result in the greatest impact on the solution to the problem statement.\n\n\nExperiments [Optional]: Concrete implementation of a specific lever that can help prove/disprove our hypotheses.\n\nOptional but always helpful to convey recommendations and suggestions with concrete ideas for what the team could or should be building.\n\n\nExample\n\n\nProblem Statement: How can we increase daily listening time for premium users in the Spotify app?\n\nHypothesis: Daily Listening Time is strongly connected to retention for premium users and hensce to monthly revenue.\n\nOpportunity Areas:\n\nUsers who use auto-generated playlists have a x% higher daily listening time\nUsers who subscribed to at least 3 podcasts have a x% higher listening time per day than those who did not subscribe to any.\nUsers who listen to audiobooks have a x% higher daily listening time.\n\nLevers:\n\nOpportunity Area: Increase the percentage of users under 25 using audiobooks from x% to y%.\nQuestions:\n\nDo users not see the feature?\nDo users see the feature but don‚Äôt engage with the feature?\nDo users engage with the feature but drop off after a short amount of time?\n\nFinding: Users under 25 engage less with the Home Screen, the only screen where Audiobooks are promoted, and hence don‚Äôt see this feature in the App. This is likely leading low usage and engagement.\nLever: Increase prominence of Audiobooks within the app\nPrioritzation Table for Report\n\n\nExperiments:\n\n\n‚ÄúWe predict that adding a banner promoting Audiobooks when the App opens [Experiment Change] will increase younger users‚Äô daily listening time [Problem] because more younger users will see and listen to Audiobooks [Lever]. We will know this is true when we see an increase in young users using Audiobooks [Lever], followed by an increase in the daily listening time for younger users [Validation Metrics].‚Äù\nIf there is no significant increase in audiobook usage, then there many other ways to increase the visibility of a feature which can be the hypotheses of further experiments.\nIf , however, there is a significant increase in users using Audiobooks (lever) but no effect on daily listening time (main problem), then the lever is invalidated and we can move on to the next one.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-edap",
    "href": "qmd/project-analyses.html#sec-proj-anal-edap",
    "title": "Analyses",
    "section": "Exploratory Data Analysis Research Plan",
    "text": "Exploratory Data Analysis Research Plan\n\nNotes from Pluralsight Designing an Exploratory Data Analysis Research Plan\n\nSee code &gt;&gt; rmarkdown &gt;&gt; reports &gt;&gt; edarp-demo.Rmd\n\nDuring the development of the EDARP, all stakeholders can align their expectiations. Buy-in from the aligned stakeholders can help sell the project to the organization.\nEach section should have an introduction with a description about whats in it\nMock Schedule\n\nWeek 1: Data request by a department\nWeek 2: Data Scientist and department meet to formalize the research questions\n\nWorking backwards from the desired output can help frame the right questions to ask during this period\n\nWeek 3: Clear metrics are established. The use case of the product is defined (i.e.¬†who‚Äôs using it and what decisions are to be made). Sponsorship is set. Budgets are allocated.\nWeek 4: EDARP is finalized with everyone understanding the objectves, budget, product design, and product usage\nWeek 6: Data Scientist delivers the product to the department.\n\nSections of the Report\n\nAbstract\n\nHighlights the research questions\nWho the stakeholders are\nMetrics of success\nExample:\n\n‚ÄúThe foundational task was to develop sales insights across stores. Through the identification and inclusion of various business groups, data were gathered and questions were formed. The business groups included are Marketing, IT, Sales and Data Science. From this process we defined the primary goal of this research. This research adds understanding to how sales are driven across stores and develops a predictive model of sales across stores. These outcomes fit within budget and offer an expected ROI of 10%.‚Äù\n\n\nFigures and Tables\n\nOptional depending on audience\nSection where all viz is at\n\nIntroduction\n\nDetailed description of metrics of success\n\nExample\n\nROI 8%\nR2 75%\nInterpretability\n\n\n\nStakeholders\n\nMarketing\n\nList of people\n\nIT\nSales\nData Science\n\nBudget and Financial Impact\n\nNot always known, but this section is valuable if you‚Äôre able to include it.\nPotential vendor costs\nInfrastructure costs\nApplication developement\nFinancial impact, completed by finance team, result in an expected ROI of blah%\n\nMethods\n\nData description\nData wrangling\n\nWhat were the variables of interest and why (‚Äúdata wrangling involved looking at trends in sales across stores, store types, and states‚Äù)\n\nAutocorrelation\n\n‚ÄúTesting for autocorrelation was completed leading to insights in seasonality across the stores. We examined by the ACF an PACF metrics in the assessment of autocorrelation‚Äù\n\nClustering\nOutliers\n\nDescription of algorithm comparison and model selection\n\nWords not code or results\nExample\n\nInvolved training and testing regression, random forest,‚Ä¶\nRegression model served as a benchmark comparison across 5 models\nA discussion of interpretability and expected ROI guided the choice of the final model\n\n\n\n\nResults and Discussion\n\n‚ÄúThis section highlights the thought process that went into wrangling the data and building the models. A few of the insights gained in observation of the data are shared. Also, the assessment of the model is discussed at the end of the section.‚Äù\nVisualizing the Data (i.e.¬†EDA viz - descriptive, outliers, clusters)\n\nFigures\nInsights\nRepeat as needed\n\nVariable Importance\nFinal Model\n\nModel Assessment\n\nAlgorithm comparison metrics\nDynamic visual of model output\n\nSimple shiny graph with a user input and a graph\n\ne.g.¬†Choose store number - graph of sales forecast\n\n\n\n\nConclusion\n\nExample:\n\n‚ÄúThe research explored the possibility of building a predictive model to aid in forecasting sales across stores. We found that, given the established metrics of ROI greater than 8%, R-square of greater than .75 and interpretability in the models, this reasearch has resulted in a viable model for the business. Additionally, it was discovered the presence of some outlier phenomena in the data which has been identified by the stakeholders as acceptable noise. Further we discovered that there is a latent grouping to the stores across sales, store type and assortment. This insight will be used to guide marketings action in the future.‚Äù\n\n\nAppendix\n\nSchedule of Maintenance\nFuture Research",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-datmet",
    "href": "qmd/project-analyses.html#sec-proj-anal-datmet",
    "title": "Analyses",
    "section": "Data Meta-Metrics",
    "text": "Data Meta-Metrics\n\nNotes from Data Meta Metrics\nMetrics for categorizing the quality of data being used in your analysis\nYou can be very confident about the methodologies you‚Äôre using to analyze data, but if there are issues with the underlying dataset, you might not be so confident in the results of an analysis or your ability to repeat the analysis.\n\nIdeally, we should be passing this information ‚Äî our confidences and our doubts ‚Äî on to stakeholders alongside any results or reports we share.\n\nUse Cases\n\nConvey the quality of the data and its collection process to technical and non-technical audience\nHelpful for diagnosing the strengths and weaknesses of data storage and collection across multiple departments.\nDevelop a data improvement process with an understanding of what data you do and don‚Äôt have and what you can and can‚Äôt collect.\n\nGood data: You know how and when it‚Äôs collected, it lives in a familiar database, and represents exactly what you expect it to represent.\nLess-Than-Stellar data: Data that comes with an ‚Äúoral history‚Äù and lots of caveats and exceptions when it comes to using it in practice.\n\ne.g.¬†When you ask a department for data and their responses are ‚ÄúAnna needs to download a report with very specific filters from a proprietary system and give you the data‚Äù or ‚ÄúCall Matt and see if he remembers‚Äù\n\nPotential Metrics - The type of metrics you use can depend on the analysis your doing\n\n\nRelevance: Ability to answer the question we were asking of it\nTrustworthiness: Will the data be accurate based on how it was collected, stored, and managed?\nRepeatability: How accessible is this data? Can the ETL process be faithfully reproduced?\n\nSlide Report Examples\n\nGood Data\n\nBad Data\n\n\nWith bad data, notes on why the data is bad are included in the slide.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html",
    "href": "qmd/post-hoc-analysis-anova.html",
    "title": "ANOVA",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-misc",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-misc",
    "title": "ANOVA",
    "section": "",
    "text": "Packages\n\n{car} - Anova function that computes all 3 types of ANOVA table\n\nCan also be applied to glm models to produce Analysis of Deviance tables (e.g.¬†logistic, poisson, etc.)\nThink the other packages wrap this function, so they can be used instead in order to advantage of their plotting, testing conveniences.\n\n{grafify} - ANOVA wrappers, plotting, wrappers for {emmeans}\n{afex} - Analysis of Factorial EXperiments\n\nANOVA helper functions that fit the lm, center, apply contrasts, etc. in one line of code\n\nExample: afex::aov_car(Y ~ group * condition + Error(id), data = d)\nType III used, Factor variables created, Sum-to-Zero contrast is applied\n\nEffect plotting functions\n\n\nNotes from\n\nEverything You Always Wanted to Know About ANOVA\n\nANOVA vs.¬†Regression (GPT-3.5)\n\nDifferent Research Questions:\n\nANOVA is typically used when you want to compare the means of three or more groups to determine if there are statistically significant differences among them. It‚Äôs suited for situations where you‚Äôre interested in group-level comparisons (e.g., comparing the average test scores of students from different schools).\nRegression, on the other hand, is used to model the relationship between one or more independent variables and a dependent variable. It‚Äôs suitable for predicting or explaining a continuous outcome variable.\n\nData Type:\n\nANOVA is traditionally used with categorical independent variables and a continuous dependent variable. It helps assess whether the categorical variable has a significant impact on the continuous variable.\n\nThere are other variants such as ANCOVA (categorical and continuous IVs) and Analysis of Deviance (discrete outcome)\n\nRegression can be used with both categorical and continuous independent variables to predict a continuous dependent variable or to examine the relationship between variables.\n\nMultiple Factors:\n\nANOVA is designed to handle situations with multiple categorical independent variables (factors) and their interactions. It is useful when you are interested in understanding the combined effects of several factors.\nRegression can accommodate multiple independent variables as well, but it focuses on predicting the value of the dependent variable rather than comparing groups.\n\nHypothesis Testing:\n\nANOVA tests for differences in means among groups and provides p-values to determine whether those differences are statistically significant.\nRegression can be used for hypothesis testing, but it‚Äôs more often used for estimating the effect size and making predictions.\n\nAssumptions:\n\nANOVA assumes that the groups are independent and that the residuals (the differences between observed values and group means) are normally distributed and have equal variances.\nRegression makes similar assumptions about residuals but also assumes a linear relationship between independent and dependent variables.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-gen",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-gen",
    "title": "ANOVA",
    "section": "General",
    "text": "General\n\nFamily of procedures which summarizes the relationship between the underlying model and the outcome by partitioning the variation in the outcome into components which can be uniquely attributable to different sources according to the law of total variance.\nEssentially, each of the model‚Äôs terms is represented in a line in the ANOVA table which answers the question how much of the variation in Y can be attributed to the variation in X?\n\nWhere applicable, each source of variance has an accompanying test statistic (oftenF), sometimes called the omnibus test, which indicates the significance of the variance attributable to that term, often accompanied by some measure of effect size.\n\nOne-Way ANOVA - 1 categorical, independent variable\n\nDetermines whether there is a statistically significant difference in the means of the dependent variable across the different levels of the independent variable.\nExample: A researcher wants to compare the average plant height grown using three different types of fertilizer. They would use a one-way ANOVA to test if there is a significant difference in height between the groups fertilized with each type.\n\nTwo-Way ANOVA - 2 categorical, independent variables\n\nExample: 3 treatments are given to subjects and the researcher thinks that females and males will have different responses in general.\n\nTest whether there are treatment differences after accounting for sex effects\nTest whether there are sex differences after accounting for treatment effects\nTest whether the treatment effect is different for females and males if you allow the treatment \\(\\times\\) sex interaction to be in the model\n\n\nTypes\n\nTL;DR;\n\nI don‚Äôt see a reason not to run type III every time.\nType I: Sequential Attribution of Variation\nType II: Simultaneous Attribution of Variation\n\nFor interactions: Sequential-Simultaneous Attribution of Variation\n\nType III: Simultaneous Attribution of Variation for Main Effects and Interactions\nIf the categorical explanatory variables in the analysis are balanced, then all 3 types will give the same results. The results for each variable will be it‚Äôs unique contribution.\n\nExample:\n# balanced\ntable(d$Rx, d$condition)\n#&gt;           Ca Cb\n#&gt;   Placebo  5  5\n#&gt;   Dose100  5  5\n#&gt;   Dose250  5  5\n\n# imbalanced\ntable(d$group, d$condition)\n#&gt;      Ca Cb\n#&gt;   Gb  6  6\n#&gt;   Ga  5  6\n#&gt;   Gc  4  3\n\n\nType I: Sequential Sum of Squares\n\nVariance attribution is calculated sequentially so the order of variables in the model matters. Each term is attributed with a portion of the variation (represented by its SS) that has not yet been attributed to any of the previous terms.\nRarely used in practice because the order in which variation is attributed isn‚Äôt usually important\nExample: Order of terms matters\nanova(lm(Y ~ group + X, data = d))\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Response: Y\n#&gt;           Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \n#&gt; group      2    8783    4391  0.0918 0.912617   \n#&gt; X          1  380471  380471  7.9503 0.009077 **\n#&gt; Residuals 26 1244265   47856                    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm(Y ~ X + group, data = d))\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Response: Y\n#&gt;           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \n#&gt; X          1  325745  325745  6.8067 0.01486 *\n#&gt; group      2   63509   31754  0.6635 0.52353  \n#&gt; Residuals 26 1244265   47856                  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSum of Squares values change based on the order of the terms in the model\nIn the first model,\n\nThe effect of group does not represent its unique contribution to Y‚Äôs variance, but instead its total contribution.\n\nThis reminds me of a dual path DAG where group is influenced by X. Here X‚Äôs variance contribution is included in group‚Äôs contribution since X is not conditioned upon. (See Causal Inference &gt;&gt; Dual Path DAGs)\n\nThe effect of X represents only what X explains after removing the contribution of group ‚Äî the variance attributed to X is strictly the variance that can be uniquely attributed to X, controlling for group\n\n\n\nType II: Simultaneous Sum of Squares\n\nThe variance attributed to each variable is its unique contribution ‚Äî variance after controlling for the other variables. Order of terms does not matter.\nExample\ncar::Anova(m, type = 2)\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: Y\n#&gt;            Sum Sq Df F value   Pr(&gt;F)   \n#&gt; group       63509  2  0.6635 0.523533   \n#&gt; X          380471  1  7.9503 0.009077 **\n#&gt; Residuals 1244265 26                    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSum of Squares values are equal to values of the Type 1 results when each variable is last.\nNote that factor variables, e.g.¬†group, are treated as 1 term and not broken down into dummy variables for each level.\n\nWith interactions, the method of calculation could be called, Sequential-Simultaneous.\n\nTerms are evaluated simultaneously in groups based on type of term, e.g.¬†main effects, 2-way interactions, 3-way interactions, etc., but sequentially according to the order of that term where the order of main effects &lt; 2-way interactions &lt; 3-way interactions, etc.\nAll main effects (1st order) are tested simultaneously (accounting for one another), then all 2-way interactions (2nd order) are tested simultaneously (accounting for the main effects and one another), and finally the 3-way interaction is tested (accounting for all main effects and 2-way interactions).\nSo, if you use this way to test a model with interactions, only the highest order term‚Äôs Sum of Squares represents a unique variance contribution.\n\n\nType III: Simultaneous-Simultaneous Sum of Squares\n\nThe Sum-of-Squares for each main effect and interaction is calculated as its unique contribution (i.e.¬†takes into account all other terms of the model).\nUnlike Type II, it allows you compare variance contributions for every term in your model.\nWithout centering continuous variables and applying sum-to-zero contrasts to categorical variables, tests results can change depending on the categorical level of the moderator. (Also see Regression, Linear &gt;&gt; Contrasts &gt;&gt; Sum-to-Zero)\n\nExample\n\nNo Centering, No Sum-to-Zero Contrasts\nm_int &lt;- lm(Y ~ group * X, data = d)\n\nd$group &lt;- relevel(d$group, ref = \"Gb\")\nm_int2 &lt;- lm(Y ~ group * X, data = d)\n\ncar::Anova(m_int, type = 3)\n#&gt;             Sum Sq Df F value    Pr(&gt;F)    \n#&gt; (Intercept) 538630  1 22.9922 6.994e-05 ***\n#&gt; group       738108  2 15.7536 4.269e-05 ***\n#&gt; X           101495  1  4.3325   0.04823 *  \n#&gt; group:X     682026  2 14.5566 7.246e-05 ***\n#&gt; Residuals   562240 24     \n\ncar::Anova(m_int2, type = 3)\n#&gt;             Sum Sq Df F value    Pr(&gt;F)    \n#&gt; (Intercept) 219106  1  9.3528  0.005402 ** \n#&gt; group       738108  2 15.7536 4.269e-05 ***\n#&gt; X           910646  1 38.8722 1.918e-06 ***\n#&gt; group:X     682026  2 14.5566 7.246e-05 ***\n#&gt; Residuals   562240 24  \n\nThe sum of squares and p-value change for X when the categorical variable‚Äôs reference level changed which shouldn‚Äôt matter given this is an omnibus test (i.e.¬†the categorical variable is treated as 1 entity and not set of dummy variables).\n\nCentered, Sum-to-Zero Contrasts Applied\n# center, contr.sum\nd_contr_sum &lt;- d |&gt; \n  mutate(X_c = scale(X, scale = FALSE))\ncontrasts(d_contr_sum$group) &lt;- contr.sum\nm_int_cont_sum &lt;- lm(Y ~ group * X_c, data = d_contr_sum)\ncar::Anova(m_int_cont_sum, type = 3)\n#&gt;              Sum Sq Df  F value    Pr(&gt;F)    \n#&gt; (Intercept) 4743668  1 202.4902 3.401e-13 ***\n#&gt; group         19640  2   0.4192   0.66231    \n#&gt; X_c          143772  1   6.1371   0.02067 *  \n#&gt; group:X_c    682026  2  14.5566 7.246e-05 ***\n#&gt; Residuals    562240 24\n\n# change reference level\nd_rl &lt;- d_contr_sum |&gt; \n  mutate(group_rl = relevel(group, ref = \"Gb\"))\ncontrasts(d_rl$group_rl) &lt;- contr.sum\ncar::Anova(lm(Y ~ group_rl * X_c, data = d_rl),\n           type = 3)\n#&gt;               Sum Sq Df  F value    Pr(&gt;F)    \n#&gt; (Intercept)  4743668  1 202.4902 3.401e-13 ***\n#&gt; group_rl       19640  2   0.4192   0.66231    \n#&gt; X_c           143772  1   6.1371   0.02067 *  \n#&gt; group_rl:X_c  682026  2  14.5566 7.246e-05 ***\n#&gt; Residuals     562240 24   \n\nNow when the reference level is changed, the sum-of-squares and p-value for X remain the same.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ass",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ass",
    "title": "ANOVA",
    "section": "Assumptions",
    "text": "Assumptions\n\nEach group category has a normal distribution.\nEach group category is independent of each other and identically distributed (iid)\nGroup categories have of similar variance (i.e.¬†homoskedastic variance)\n\nIf this is violated\n\nIf the ratio of the largest variance to the smallest variance is less than 4, then proceed with one-way ANOVA (robust to small differences)\nIf the ratio of the largest variance to the smallest variance is greater than 4, perform a Kruskal-Wallis test. This is considered the non-parametric equivalent to the one-way ANOVA. (example)\n\nEDA\ndata %&gt;%\n¬† group_by(program) %&gt;%\n¬† summarize(var=var(weight_loss))\n#&gt; A tibble: 3 x 2\n#&gt; ¬† program¬† var¬† ¬†\n#&gt; 1 A¬† ¬† ¬† 0.819\n#&gt; 2 B¬† ¬† ¬† 1.53¬†\n#&gt; 3 C¬† ¬† ¬† 2.46\nPerform a statisical test to see if these variables are statistically significant (See Post-Hoc Analysis, Difference-in-Means &gt;&gt; EDA &gt;&gt; Tests for Equal Variances)",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-math",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-math",
    "title": "ANOVA",
    "section": "Mathematics",
    "text": "Mathematics\n\nAsides:\n\nThis lookd like the variance formula except for not dividing by the sample size to get the ‚Äúaverage‚Äù squared distance\nSSA formula - the second summation just translates to multiplying by ni, the group category sample size, since there is no j in that formula\n\nCalculate SSA and SSE\n\\[\n\\begin{align}\n\\text{SST} &= \\text{SSA} + \\text{SSE} \\\\\n&= \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (x_{i,j} - \\mu)^2 \\\\\n&= \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (\\bar x_i - \\mu)^2 + \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (x_{i,j} - \\bar x_i)^2\n\\end{align}\n\\]\n\n\\(\\text{SST}\\): Sum of Squares Total\n\\(\\text{SSA}\\): Sum of Squares between categories, treatments, or factors\n\n‚ÄúA‚Äù stands for attributes (i.e.¬†categories)\n\n\\(\\text{SSE}\\): Sum of Squares of Errors; randomness within categories, treatments, or factors\n\\(x_{ij}\\): The jth observation of the ith category\n\\(\\bar x_i\\): The sample mean of category i\n\\(\\mu\\): The overall sample mean\n\\(n_i\\): The group category sample size\n\\(a\\): The number of group categories\n\nCalculate MSA and MSE\n\\[\n\\begin{align}\n\\text{MSE} &= \\frac{\\text{SSE}}{N-a} \\\\\n\\text{MSA} &= \\frac{\\text{SSA}}{a-1}\n\\end{align}\n\\]\n\nWhere N is the total sample size\n\nCalculate the F statistic and P-Value\n\\[\nF = \\frac{\\text{MSA}}{\\text{MSE}}\n\\]\n\nFind the p-value (need a table to look it up)\nIf our F statistic is less than the critical value F statistic for a \\(\\alpha = 0.05\\) than we cannot reject the null hypothesis (no statistical difference between categories)\n\nDiscussion\n\nIf there is a group category that has more variance than the others‚Äô attribute error (SSA), we should then pick that up when we compare it to the random error (SSE)\n\nIf a group is further away from the overall mean, then it will increase SSA and thus influence the overall variance but might not always increase random error",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-diag",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-diag",
    "title": "ANOVA",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nEta Squared\n\nMetric to describe the effect size of a variable\nRange: [0, 1]; values closer to 1 indicating that a specific variable in the model can explain a greater fraction of the variation\nlsr::etaSquared(anova_model) (use first column of output)\nGuidelines\n\n0.01: Effect size is small.\n0.06: Effect size is medium.\nLarge effect size if the number is 0.14 or above\n\n\nPost-ANOVA Tests\n\nAssume approximately Normal distributions\nFor links to more details about each test, https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/post-hoc/\nDuncan‚Äôs new multiple range test (MRT)\n\nWhen you run Analysis of Variance (ANOVA), the results will tell you if there is a difference in means. However, it won‚Äôt pinpoint the pairs of means that are different. Duncan‚Äôs Multiple Range Test will identify the pairs of means (from at least three) that differ. The MRT is similar to the LSD, but instead of a t-value, a Q Value is used.\n\nFisher‚Äôs Least Significant Difference (LSD)\n\nA tool to identify which pairs of means are statistically different. Essentially the same as Duncan‚Äôs MRT, but with t-values instead of Q values.\n\nNewman-Keuls\n\nLike Tukey‚Äôs, this post-hoc test identifies sample means that are different from each other. Newman-Keuls uses different critical values for comparing pairs of means. Therefore, it is more likely to find significant differences.\n\nRodger‚Äôs Method\n\nConsidered by some to be the most powerful post-hoc test for detecting differences among groups. This test protects against loss of statistical power as the degrees of freedom increase.\n\nScheff√©‚Äôs Method\n\nUsed when you want to look at post-hoc comparisons in general (as opposed to just pairwise comparisons). Scheffe‚Äôs controls for the overall confidence level. It is customarily used with unequal sample sizes.\n\nTukey‚Äôs Test\n\nThe purpose of Tukey‚Äôs test is to figure out which groups in your sample differ. It uses the ‚ÄúHonest Significant Difference,‚Äù a number that represents the distance between groups, to compare every mean with every other mean.\n\nDunnett‚Äôs Test\n\nLike Tukey‚Äôs this post-hoc test is used to compare means. Unlike Tukey‚Äôs, it compares every mean to a control mean.\n{DescTools::DunnettTest}\n\nBenjamin-Hochberg (BH) Procedure\n\nIf you perform a very large amount of tests, one or more of the tests will have a significant result purely by chance alone. This post-hoc test accounts for that false discovery rate.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-oneway",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-oneway",
    "title": "ANOVA",
    "section": "One-Way",
    "text": "One-Way\n\nMeasures if there‚Äôs a difference in means between any group category\nExample: 1 control, 2 Test groups\n\nData\ndata &lt;- data.frame(Group = rep(c(\"control\", \"Test1\", \"Test2\"), each = 10),\nvalue = c(rnorm(10), rnorm(10),rnorm(10)))\ndata$Group&lt;-as.factor(data$Group)\nhead(data)\n#&gt; ¬† Group¬† ¬† ¬† value\n#&gt; 1 control¬† 0.1932123\n#&gt; 2 control -0.4346821\n#&gt; 3 control¬† 0.9132671\n#&gt; 4 control¬† 1.7933881\n#&gt; 5 control¬† 0.9966051\n#&gt; 6 control¬† 1.1074905\nFit model\nmodel &lt;- aov(value ~ Group, data = data)\nsummary(model)\n#&gt; ¬† ¬† ¬† ¬† ¬† ¬† Df¬† ¬† Sum Sq¬†  Mean Sq¬† F value¬† Pr(&gt;F)¬†\n#&gt; Group¬† ¬† ¬† ¬† 2¬† ¬†  4.407¬† ¬† 2.2036¬† ¬†  3.71¬† 0.0377 *\n#&gt; Residuals¬†  27¬† ¬† 16.035¬† ¬† 0.5939\n\n# or\nlm_mod &lt;- lm(value ~ Group, data = data)\nanova(lm_mod)\n\nP-Value &lt; 0.05 says at least 1 group category has a statistically significant different mean from another category\n\nDunnett‚Äôs Test\nDescTools::DunnettTest(x=data$value, g=data$Group)\n\n#&gt; Dunnett's test for comparing several treatments with a control :¬†\n#&gt; ¬† ¬† 95% family-wise confidence level\n#&gt; $control\n#&gt; ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† diff¬† ¬† lwr.ci¬† ¬† ¬† upr.ci¬† pval¬† ¬†\n#&gt; Test1-control -0.8742469 -1.678514 -0.06998022 0.0320 *¬†\n#&gt; Test2-control -0.7335283 -1.537795¬† 0.07073836 0.0768 .\n\nMeasures if there is any difference between treatments and the control\nThe mean score of the test1 group was significantly higher than the control group. The mean score of the test2 group was not significantly higher than the control group.\n\nTukey‚Äôs HSD\nstats::TukeyHSD(model, conf.level=.95)\n\nMeasures difference in means between all categories and each other",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ancova",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ancova",
    "title": "ANOVA",
    "section": "ANCOVA",
    "text": "ANCOVA\n\nAnalysis of Covariance is used to measure the main effect and interaction effects of categorical variables on a continuous dependent variable while controlling the effects of selected other continuous variables which co-vary with the dependent variable.\nMisc\n\nAnalysis of covariance is classical terminology for linear models but we often use the term for nonlinear models (Harrell)\nSee also\n\nHarrell - Biostatistics for Biomedical Research Ch. 13\n\n\nAssumptions\n\nIndependent observations (i.e.¬†random assignment, avoid is having known relationships among participants in the study)\nLinearity: the relation between the covariate(s) and the dependent variable must be linear.\nNormality: the dependent variable must be normally distributed within each subpopulation. (only needed for small samples of n &lt; 20 or so)\nHomogeneity of regression slopes: the beta-coefficient(s) for the covariate(s) must be equal among all subpopulations. (regression lines for these individual groups are assumed to be parallel)\n\nFailure to meet this assumption implies that there is an interaction between the covariate and the treatment.\nThis assumption can be checked with an F test on the interaction of the independent variable(s) with the covariate(s).\n\nIf the F test is significant (i.e., significant interaction) then this assumption has been violated and the covariate should not be used as is.\nA possible solution is converting the continuous scale of the covariate to a categorical (discrete) variable and making it a subsequent independent variable, and then use a factorial ANOVA to analyze the data.\n\n\nThe covariate (adjustment variable) and the treatment are independent\nmodel &lt;- aov(grade ~ technique, data = data)\nsummary(model)\n\n#&gt; ¬† ¬† ¬† ¬† ¬† ¬† Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; technique¬† ¬† 2¬† ¬† 9.8¬† ¬† 4.92¬† ¬† 0.14¬† 0.869\n#&gt; Residuals¬† 87 3047.7¬† 35.03\n\nH0: variables are independent\n\n\nHomogeneity of variance: variance of the dependent variable must be equal over all subpopulations (only needed for sharply unequal sample sizes)\n# response ~ treatment\nleveneTest(exam ~ technique, data = data)\n\n#&gt; ¬† ¬† ¬† Df F value¬† ¬† Pr(&gt;F)¬† ¬†\n#&gt; group¬† 2¬† 13.752 6.464e-06 ***\n#&gt; ¬† ¬† ¬† 87\n\n# alt test\nfligner.test(size ~ location, my.dataframe)\n\nH0: Homogeneous variance\nThis one fails\n\nFit\nancova_model &lt;- aov(exam ~ technique + grade, data = data)\ncar::Anova(ancova_model, type=\"III\")\n\n#&gt; ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Sum Sq Df F value¬† ¬† Pr(&gt;F)¬† ¬†\n#&gt; ¬† ¬† (Intercept) 3492.4¬† 1 57.1325 4.096e-11 ***\n#&gt; ¬† ¬† technique¬† 1085.8¬† 2¬† 8.8814 0.0003116 ***\n#&gt; ¬† ¬† grade¬† ¬† ¬† ¬† ¬† 4.0¬† 1¬† 0.0657 0.7982685¬† ¬†\n#&gt; ¬† ¬† Residuals¬† 5257.0 86\n\nWhen adjusting for current grade (covariate), study technique (treatment) has a significant effect on the final exam score (response).\n\nDoes the effect differ by treatment\npostHocs &lt;- multicomp::glht(ancova_model, linfct = mcp(technique = \"Tukey\"))\nsummary(postHocs)\n\n#&gt; ¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value Pr(&gt;|t|)¬† ¬†\n#&gt; B - A == 0¬†  -5.279¬† ¬† ¬† 2.021¬† -2.613¬† 0.0284 *¬†\n#&gt; C - A == 0¬† ¬† 3.138¬† ¬† ¬† 2.022¬†  1.552¬† 0.2719¬† ¬†\n#&gt; C - B == 0¬† ¬† 8.418¬† ¬† ¬† 2.019¬†  4.170¬† &lt;0.001 ***\n\nAlso see Post-Hoc Analysis, Multilevel &gt;&gt; Tukey‚Äôs Test\n\\(A\\), \\(B\\), and \\(C\\) are the study techniques (treatment)\nSignificant differences between \\(B\\) and \\(A\\) and a pretty large difference between \\(B\\) and \\(C\\).\n\nExample: RCT\n\\[\n\\begin{align}\n\\text{post}_i &\\sim \\mathcal{N}(\\mu_i, \\sigma_\\epsilon)\\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{tx}_i + \\beta_2 \\text{pre}_i\n\\end{align}\n\\]\nw2 &lt;- glm(\n¬† data = dw,\n¬† family = gaussian,\n¬† post ~ 1 + tx + pre)\n\nSpecification\n\npost, pre: The post-treatment and pre-treatment measurement of the outcome variable\ntx: The treatment indicator variable\n\\(\\beta_0\\): Population mean for the outcome variable in the control group\n\\(\\beta_1\\): Parameter is the population level difference in pre/post change in the treatment group, compared to the control group.\n\nAlso a causal estimate for the average treatment effect (ATE) in the population, œÑ\n\nBecause pre is added as a covariate, both \\(\\beta_0\\) and \\(\\beta_1\\) are conditional on the outcome variable, as collected at baseline before random assignment.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/project-development.html",
    "href": "qmd/project-development.html",
    "title": "Development",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Projects",
      "Development"
    ]
  },
  {
    "objectID": "qmd/project-development.html#sec-proj-dev-misc",
    "href": "qmd/project-development.html#sec-proj-dev-misc",
    "title": "Development",
    "section": "",
    "text": "Notes from\n\nAdapting Project Management Methodologies to Data Science\n\nAlong with overview of various methodologies, provides list of Agile foundational values and key principles\n\n\nLoose implementation of CRISP-DM with agile-based practices is recommended\nWaterfall or the newer variation with feedback loops between adjoining stages should not be used\n\nDesigned for manufacturing and construction where the progressive movement of a project is sequential. DS typically requires a lot of experimentation and a modification of requirements.\nAlthough can be useful for certain stages of the data science project such as planning, resource management, scope, and validation\n\nPrior to full deployment, run a pilot deployment\n\nOnly a few groups are given permission to use the product\nReceive feedback (e.g.¬†weekly meetings), fix bugs, and make changes\n\nAfter full deployment\n\nHave an education and training session for users\n\nNote problem areas. These may be potential next steps to improving the product\n\nCheck-in periodically with users to get feedback\n\nProtyping and Testing\n\nSee Lean Data Science\n\nThe idea is to build things that deliver value quickly\n\nIterative Building Steps\n\nBuild ‚Äògood enough‚Äô versions of the tool or project (MVPs)\nGive these to stakeholders to use and get feedback\nIncorporate feedback\nReturn to stakeholders to use and get more feedback\nIterate until project the stakeholder and you feel it has reached production-level\n\nBreak each project down into a set of smaller projects\n\nExample:\n\nMVP to test if the idea is feasible\nMore functional version of the MVP\nProductionized version of the product.\n\nTrack the impact of each of the sub-projects that comprise the larger projects\nAt each of these milestones, decide on whether to progress further on a project by using taking the impact score of the subproject into account\n\nExample: rule-based chatbot manages to\n\nChatbot: successfully helps 10,000 customers a month\n\n10,000 customers ‚®Ø 3 min average call = 30,000 mins = 500 hours.\n\nCall Center Agent\n\nCall-center agent‚Äôs time costs $200/hr in terms of salary and infrastructure,\n\nConclusion: MVP chatbot saves $100K a month and you could likely save even more with a more sophisticated chatbot.",
    "crumbs": [
      "Projects",
      "Development"
    ]
  },
  {
    "objectID": "qmd/project-development.html#sec-proj-dev-comms",
    "href": "qmd/project-development.html#sec-proj-dev-comms",
    "title": "Development",
    "section": "Communication",
    "text": "Communication\n\nRemind stakeholders of what it is we agreed in last meeting you‚Äôd do, what you did and how to interpret the results\nState what it is you need from the stakeholder.\nState whether the project progress in the middle, at it‚Äôs end, you‚Äôre wrapping up or what‚Äôs going on?\nA summary slide or results peppered with comments leading me through what it is I am looking at\nThe Minto Pyramid\n\nOrganize the message so that it starts with a conclusion which leads to the arguments that support it and ends in detailed information.\nProcess\n\nWrite conclusion (2-3 sentences max)\nSupporting arguments: Try to make them concise bullet points\nLink to a more detailed explanation at the bottom if need be\n\n\nMight be useful to time the arrival when the stakeholder is most likely able to read it.\n\ne.g.¬†If a stakeholder has a meeting at 9:30 every morning, it may be better to time the sending of the report to before or after that meeting.",
    "crumbs": [
      "Projects",
      "Development"
    ]
  },
  {
    "objectID": "qmd/project-development.html#sec-proj-dev-kanban",
    "href": "qmd/project-development.html#sec-proj-dev-kanban",
    "title": "Development",
    "section": "Kanban",
    "text": "Kanban\n\n\nPhysical or digital board where tasks are then outlined as story cards.\nEvery card will be extracted from left to right until it is completed.\nflexibility to execute tasks without getting constant deadlines\nMisc\n\nSeems like this could be used within a sprint (columns would have to be defined according to the sprint plan)\n\nAdvantages\n\nbottlenecks, overworked steps, etc. easily identified\neffective at communicating the work in progress for stakeholders and team members\noriented towards individual tasks instead of batches like in scrums\n\nDisadvantages\n\nlack of deadlines can lead to longer project times\nchallenging to define the columns for a data science Kanban board\nCustomer interaction is undefined. As such, customers may not feel dedicated to the process without the structured cadence of sprint reviews",
    "crumbs": [
      "Projects",
      "Development"
    ]
  },
  {
    "objectID": "qmd/project-development.html#sec-proj-dev-crisp",
    "href": "qmd/project-development.html#sec-proj-dev-crisp",
    "title": "Development",
    "section": "CRISP-DM",
    "text": "CRISP-DM\n\n\nCross-Industry Standard Process for Data Mining\nDefacto standard for data mining\nSupports replication, generalizable to any DS project\nPhases (not all are sequential, some phases are iterative):\n\n\n\nBusiness-Science Version\n\n\nProcess\n\nView Business as a Machine\n\nIsolating business units\n\nInternal: Sales, Manufacturing, Accounting, etc\nExternal: customers, suppliers\nVisualizing the connections\n\nDefining objectives\nCollecting outcomes\n\nUnderstand The Drivers\n\nInvestigate if objectives are being met\nSynthesize outcomes\nHypothesize drivers\n\nAt this stage, it‚Äôs critical to meet with subject-matter experts (SMEs). These are people in the organization that are close to process and customers. We need to understand what are the potential drivers of lead time. Form a general equation that they help create.\n\n\nMeasure Drivers\n\nCollect Data\n\nCollect data related to the high level drivers. This data could be stored in databases or it may need to be collected. We could collect competitor data, supplier data, sales data (Enterprise Resource Planning or ERP data), personnel data, and more.\nMay require effort to set up processes to collect it, but developing strategic data sources becomes a competitive advantage over time.\n\nDevelop KPIs\n\nRequires knowledge of customers and industry. Realize that a wealth of data is available outside of your organization. Learn where this data resides, and it becomes a tremendous asset.\n\n\nUncover Problems And Opportunities\n\nEvaluate performance vs KPIs\nHighlight potential problem areas\nReview the our project for what could have been missed\n\nTalk with SME‚Äôs to make sure they agree with your findings so far.\n\n\nEncode Decision Making Algorithms\n\nDevelop algorithms to predict and explain the problem\nOptimize decisions to maximize profit\n\ne.g.¬†For classification, threshold optimization using a custom cost function to optimize resources, costs, precision, and recall (See Diagnostics, Classification &gt;&gt; Scores &gt;&gt; Custom Cost Functions\n\nUse systematic decision-making algorithms to improve decision making\n\nExample: Employee Churn\n\n\nApp uses LIME to get prediction-level feature importance\n\n\nMeasure The Results\n\nCapture outcomes\nSynthesize results\nVisualize outcomes over time\n\nWe are looking for progress. If we have experienced good outcomes, then we need to recognize what contributed to those good outcomes.\nQuestions\n\nWere the decision makers using the tools?\nDid they follow the systematic recommendation?\nDid the model accurately predict risk?\nWere the results poor? Same questions apply.\n\n\n\nReport Financial Impact\n\nMeasure actual results\nTie to financial benefits\nReport financial benefit to key stakeholders\n\nIt‚Äôs insufficient to say that we saved 75 employees or 75 customers. Rather, we need to say that the average cost of a lost employee or lost customer is $100,000 per year, so we just saved the organization $7.5M/year. Always report as a financial value.\n\n\n\nExample: Customer Churn\n\nView business as a machine\n\nIsolating business units: The interaction occurs between Sales and the Customer\nDefining objectives: Make customers happy\nCollecting outcomes: We are slowly losing customers. It‚Äôs lowering revenue for the organization $500K per year.\n\nUnderstand The Drivers\n\nInvestigate if objectives are being met\n\nCustomer Satisfaction: Loss of customers generally indicates low satisfaction. This could be related to availability of products, poor customer service, or competition offering lower prices and/or better service or quality.\n\nSynthesize outcomes\n\nCustomers are leaving for a competitor. In speaking with Sales, several customers have stated ‚ÄúCompetition has faster delivery‚Äù. This is an indicator that lead time, or the ability to quickly service customers, is not competitive.\n\nHypothesize Drivers\n\nLead time is related to supplier delivery, inventory availability, personnel, and the scheduling process.\n\n\nMeasure Drivers\n\nAverage Lead Time: The level is 2-weeks, which is based on customer feedback on competitors.\nSupplier Average Lead Time: The level is 3 weeks, which is based on feedback related to our competitor‚Äôs suppliers.\nInventory Availability Percentage: The level of 90% is related based on where customers are experiencing unmet demand. This data comes from the ERP data comparing sale requests to product availability.\nPersonnel Turnover: The level of 15% is based on the industry averages.\n\nUncover Problems and Opportunities\n\nOur average lead time is 6 weeks compared to the competitor average lead time of 2 weeks, which is the first order cause for the customer churn\nOur supplier average lead time is on par with our competitor‚Äôs, which does not necessitate a concern.\nOur inventory percentage availability is 80%, which is too low to maintain a high customer satisfaction level. This could be a reason that churn is increasing.\nOur personnel turnover in key areas is zero over the past 12 months, so no cause for concern.",
    "crumbs": [
      "Projects",
      "Development"
    ]
  },
  {
    "objectID": "qmd/project-development.html#sec-proj-dev-agile",
    "href": "qmd/project-development.html#sec-proj-dev-agile",
    "title": "Development",
    "section": "Agile",
    "text": "Agile\n\nMisc\n\nResources\n\nhttps://www.atlassian.com/agile/project-management/overview\n\nFeatures adaptability, continuous delivery, iteration, and short time frames\n\n\n\nTerms\n\nEpic - Collection of high level tasks that may represent several user stories\n\nHelps to map the model outcome and define the correct stakeholders for the project\nA helpful way to organize your work and to create a hierarchy.\nThe idea is to break work down into shippable pieces so that large projects can actually get done and you can continue to ship value to your customers on a regular basis\nDelivered over a set of sprints\n\nInitiatives - Collections of epics that drive toward a common goal\nProduct Roadmap - Plan of action for how a product or solution will evolve over time\n\nExpressed and visualized as a set of initiatives plotted along a timeline\n\nScrum - A framework that‚Äôs objective is to fulfill customer needs through transparent communication, continuous progress, and collective responsibility\n\nData-Driven Scrum (DDS) - Scrums, as defined, have fixed lengths which can be an issue with DS projects\n\nSprints - Short periodic blocks that make up a scrum\n\nEach usually ranges from 2-4 weeks\nEach sprint is an entity that delivers the full result.\nComposed of a starting point and requirements that complete the project plan\nTheme - an organization goal that drive the creation of epics and initiatives\n\nUser Story - Smallest unit of work or a task; an informal, general explanation of a software feature written from the perspective of the end user. Its purpose is to articulate how a software feature will provide value to the customer.\n\nAfter reading a user story, the team knows why they are building, what they‚Äôre building, and what value it creates.\n\n\n\n\nValues for Data Analysis\n\nDecisions over dashboards: By focusing on what people want to do with data, we can move past the first set of questions they ask, focus on the valuable iteration and follow-up questions, build trust, cultivate curiosity and drive action.\nFunctional analysis over perfect outputs: To enable quick iterations, we‚Äôre going to have to spend less time crafting perfect outputs and focus on moving from one question to the next as quickly as possible.\nSharing data over gatekeeping data: We‚Äôre going to have to share responsibility for our data and data ‚Äúproducts‚Äù with our business partners. This will help build trust, and keep us all accountable for cultivating great data products and data-driven cultures.\nIndividuals and interactions over processes and tools: When in doubt, we need to rely on the relationships we‚Äôve built with the business over the tools we‚Äôve put in to help guide those relationships.\n\n\n\nData Science Lifecycle\n\n\nNotes from TDSP: When Agile Meets Data Science\nIf at any point we are not satisfied with our results or faced with changing requirements we can return to a previous step since this methodology is focused on iterative development\nSteps\n\nBusiness Understanding\n\nDefine objectives: Work with customers/stakeholders to identify the business problem we are trying to solve.\nIdentify data sources: Identify the data sources that we will need to solve it.\n\nData Acquisition and Understanding\n\nIngest the data: Bring the data into our environment that we are using for analytics.\nExplore the data: Exploratory data analysis (EDA) and determinine if it is adequate for model development.\nSet up a data pipeline: Build a process to ingest new data. A data pipeline can either be batch-based, real-time or a hybrid of the previous options.\nNote: While the data scientists on the team are working on EDA, the data engineers may be working on setting up a data pipeline, which allows us to complete this stage quicker\n\nModeling\n\nFeature engineering: Creat data features from raw data for model training.\n\nEnhanced by having a good understanding of the data.\n\nModel training: Split the data into training, validation, and testing sets. Train models\nModel evaluation: Evaluate those models by answering the following questions:\n\nWhat are the metrics that the model achieves on the validation/testing set?\nDoes the model solve the business problem and fit the constraints of the problem?\nIs this model suitable for production?\n\nNote: Could train one model and find that the results are not satisfactory and return to the feature engineering and model training stages to craft better features and try different modeling approaches.\n\nDeployment (Options)\n\nExposing the model through an API that can be consumed by other applications.\nCreating a microservice or containerized application that runs the model.\nIntegrating the model into a web application with a dashboard that displays the results of the predictions.\nCreating a batch process that runs the model and writes the predictions to a data source that can be consumed.\n\nStakeholder/customer acceptance\n\nSystem validation: Confirm that the data pipeline, model, and deployment satisfy the business use case and meet the needs of the customers.\nProject hand-off: Transfer the project to the group that will manage it in production. l\n\n\n\n\n\nProduct Roadmap Examples\n\nExample\n\nInitiative: build a forecast system to predict sales for an ice cream company\nEpics:\n\n‚ÄúAs a Sales Manager, I need to understand which regions I need to focus my outbound effort based on the sales forecast‚Äù\n‚ÄúAs a Logistics Manager, I need to estimate demand so that I can prepare our production accordingly‚Äù\n\nUser Story:\n\n‚ÄúAs a Logistics Manager, I need to see the forecast on my Production Dashboard‚Äù;\n‚ÄúAs a Logistics Manager, I need to have simulations around how weather predictions can change the forecast‚Äù;\n\n\n\n\n\nSprint Workflow\n\n\nBad flow chart, should be a circle where review loops back to planning\nSprint Review - The scrum team and stakeholders review what was accomplished in the sprint, what has changed in their environment, and collaborate on what to do next\n\nThese are necessary to avoiding issues that might destroy a project. (see below)\n\nData scientist participation will help with their communication skills and increase transparency in what they‚Äôre doing\nStakeholders might think a feature or ML result is feasible with the current data and tech stack. These are important to opportunities to explain why they aren‚Äôt feasible.\nRoles often bleed together. The planning portion is a good way to converge on a strategy of what to do next.\n\n\nSprint Planning (~15 minutes every two weeks)\n\nDevelop the next sprint‚Äôs goals\n\nDo the next sprint‚Äôs goals align with our goals in 3 months\nDo the next sprint‚Äôs goals align with our annual team goals/strategic vision\nRevise the next sprint‚Äôs goals to align with these goals if necessary\n\nBreak the sprint goals into tasks and sub tasks\nAssign the tasks/subtasks to members and estimate time to completion of these tasks\nExtended sprint planning (Every 3 months to roughly plan the next 3 months)\nStrategic meetings (6 months)\n\nSome technical details to starting a project\n\nNotes from The Technical Blockers to Turning Data Science Teams Agile\nStart a repo\n\nin the organization acct not under a personal acct\nuse readme as onboarding document\n\nlast person to join is in-charge of it\n\nThe last person will be best suited to edit/add details that clear up any confusion that they had when they were onboarded\nInclude ‚ÄúThis document is maintained by the most recent person to join the team. That person is currently: ____‚Äù\n\nExplicitly state that anyone can review code in your README. If someone isn‚Äôt familiar with a part of the code, they become so by reviewing it.\n\nEdit the settings of your repo. Make the main branch protected, don‚Äôt allow anyone to push directly to the main branch, and only allow PRs that have passed unit-tests (more about this later) and have undergone a code review.\n\nUpdate the team‚Äôs skills related to Agile\n\nIn the beginning, may not have a lot of tasks to assign as there may be design/requirements discussions\nMake sure everyone knows git and how to write unit tests\n\nCheck team members‚Äô personal accts to see how many commits they have, ‚Äúhttps://github.com/search?q=author:‚Äù\nCheck team members‚Äô projects for unit tests\nIf it doesn‚Äôt look like they don‚Äôt have much experience, assign a udemy, etc. course on the subject and require a certificate in order to be assigned tasks\n\n\nAssign tasks through Agile tools like ZenHub, Jira, or Trello\nSet-up a CI tool\n\nexamples: Github Actions, TravisCI, CircleCI, or Jenkins\nadd learning this tool as part of your ZenHub task boards and don‚Äôt allow people to move on until they‚Äôve learned it.\nrun your unit tests every time someone makes a PR\n\nDaily Stand-ups\n\nUsed to discuss what your daily work will be, and it should be short\nProject strategy meeting should be immediately after the stand-up\nEach team member answers only 3 questions:\n\nWhat will you do today?\nWhat did you do yesterday?\n\nRather than a simple verbal status update. It can be better the show what you did.\n\ne.g.¬†show your coding screen and walk everyone through you code\n\nBenefits\n\nSomeone else on the team will have an idea for a better, faster, or simpler way to solve the problem\nEasier to catch a flaw after a few lines of code than after 1000 during a code review\nIf you find out that someone on my team is doing something very similar, and you can save time by reusing code.\nCool to see incremental progress every day instead of just the final product\n\n\nWhat are you blocked by?\nScreen-share these three questions written out on a PowerPoint slide.¬†\n\nCongratulate people on finishing the courses\nAssign a weekly changing role of scrum master\n\nThe scrum master makes sure the 3 questions above are answered by everyone.\nThis person doesn‚Äôt have to be the boss or most senior person.",
    "crumbs": [
      "Projects",
      "Development"
    ]
  },
  {
    "objectID": "qmd/project-planning.html",
    "href": "qmd/project-planning.html",
    "title": "37¬† Planning",
    "section": "",
    "text": "37.1 Misc",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-misc",
    "href": "qmd/project-planning.html#sec-proj-plan-misc",
    "title": "37¬† Planning",
    "section": "",
    "text": "See Case Study: Applying a Data Science Process Model to a Real-World Scenario A VERY detailed article on the execution of a data science project within a manufacturing company, but can be generalized to other industries.\n  Goes through a scenario of step-by-step planning and execution of changing a manual stock replenishment process to an automated one\n\n  Notes on this article are in [Logistics](Logistics) &gt;&gt; Demand Forecasting &gt;&gt; Misc\nDL model cost calculator (github) (article)\nA clearly defined business problem and targeted success metrics that‚Äôs agreed upon by data scientists and stakeholders are essential before starting a project.\n\nIt should be measurable, clear, and actionable.\nUse the ‚ÄúChallenge Framework‚Äù to solve difficult problems\n\nEvery situation is a function of:\n\nIncentives\nPersonalities\nPerspectives\nConstraints\nResources\n\nIn most ‚Äútough‚Äù situations, 2+ are misaligned. Figure out which and hone in on them.\n\n\nBum, Buy, then Build\n\nBum free solutions while also relaxing quality thresholds.\nLook at buyable options, especially from large, mature organizations that offer low-cost, stable products (with potential discounts if the project is for a non-profit).\nResort to building only if:\n\nIt is far too inefficient to adapt workflows to existing solutions and/or\nThere is an opportunity for reuse by other nonprofits.\n\n\nAdd a buffer\n\nif the business goal is a precision of 95%, you can try tuning your model to an operating point of 96‚Äì97% on the offline evaluation set in order to account for the expected model degradation from data drift.\n\nContracts\n\nOnly promise what is in your power to deliver\n\nExample: A contract with the business stakeholders was to guarantee X% recall on known (historic) data.\n\nIt doesn‚Äôt try to make guarantees about something that the ML team doesn‚Äôt have complete control over: the actual recall in production depends on the amount of data drift, and is not predictable.\n\n\n\nDeliver a Minimally Viable Product (MVP) first.\n\nShould be a product with only the primary features required to get the job done\nThis process with help decide:\n\nhow to implement a more fully fledged product\nwhich additional features might be infeasible or not worth the time and effort to get working\n\n\nFor details on Project/DS Team ROI, see Organizational and Team Development &gt;&gt; Determining a Data Team‚Äôs ROI\nSources of data\n\nInternal resources: Existing historical datasets could be repurposed for new insights.\n\nConsiderations for collecting data\n\nWhether you want to collect qualitative or quantitative data\nThe method for collecting (e.g., surveys, using other reports)\nThe timeframe for the data\nSample size\nOwners of the data\nData sensitivity\nData storage and reporting method (e.g., Salesforce)\nPotential pitfalls or biases in the data (e.g., sample bias, confirmation bias)\n\n\nExternal resources: Governmental organizations, nonprofits, and research institutions have free, accessible datasources that span all different sectors (e.g., agriculture, healthcare, education).\n\nExamples Data.gov\n  _[10 Great Nonprofit Research Resources](https://topnonprofits.com/10-great-nonprofit-research-resources/)_\n\n  _[Forbes ‚Äî 35 Brilliant and Free Datasources](https://www.forbes.com/sites/bernardmarr/2016/02/12/big-data-35-brilliant-and-free-data-sources-for-2016/#5807bf00b54d)_\n\n  _[Springboard ‚Äî Free Public Datasets](https://www.springboard.com/blog/free-public-data-sets-data-science-project/)_\n\n\nOutputs vs Outcomes\n\nLogic model\n\nIt focuses on how inputs and activities translate to outputs and eventually\n\nExample",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-gensteps",
    "href": "qmd/project-planning.html#sec-proj-plan-gensteps",
    "title": "37¬† Planning",
    "section": "37.2 General Steps for Starting a Project",
    "text": "37.2 General Steps for Starting a Project\n\nFraming a data question\n\nGuidelines\n\nPrecision: Be as detailed as possible in how you frame your questions. Avoid generic words like ‚Äòimprove‚Äô or ‚Äòsuccess.‚Äô If you want to improve something, specify by how much. If you want to achieve something, specify by when.\n\nDecide before starting what the minimum project performance is to productionize (i.e.¬†build a fully functional project) and launch (i.e.¬†deliver to all your customers)\n\nSetting these thresholds at the beginning will help to prevent you from bargaining with yourself or stakeholders to deliver the project that might harm your business\n\nAfter working hard on a project and pouring resources into it, it can be difficult to end it.\n\n\n\nData-Centric: Consider the role of data in your organization. Can data help you answer this question? Is it clear what data you will need to collect to answer this question? Can progress on the task be codified into a metric that can be measured? If the answer to any of these questions is ambiguous or negative, investing in additional data resources may be an inefficient allocation of resources.\n\nExample\n\nFigure Out What Data You Need\n\nGuidelines\n\nNecessity: Are the data you are collecting necessary? Avoid data bloat, which is over-collecting data points for a ‚Äújust in case‚Äù scenario. This makes sustaining long-term data collection of those fields far more burdensome.\nAvailability: Are there external, publicly available data sources like government data that you can leverage? If the data needs to be collected, how easy is it to collect? If it is hard to collect, do you have a plan and resources in place as to how you can ensure it is collected at regular intervals over time? One-off data collection is rarely helpful as there is no reference point to measure the impact of interventions over time.\nMaintainability: Can you maintain and easily update this data over time? Is the cost of doing so sustainable? This is critical because longitudinal data collection with standard fields is one of the most valuable resources for a nonprofit. Avoid constantly changing field names, a moving target of data collection objectives, and costly data collection procedures (like purchasing third-party data) that are not sustainable given your overall budget.\nReliability: If you are using a third party data source, do you trust the quality of the data? What are the ways this data may be biased, incomplete, or inaccurate?\n\nExample\n\nA nonprofit with a mission to find long-term housing for the unhoused. This organization may want to answer the question: ‚Äôwhat percentage of the unhoused have we been able to successfully rehabilitate in the area we serve?‚Äô\n\n\nOrganizational Buy-in\n\nProject manager tries to examine whether the project can fundamentally be classified as feasible and whether the requirements can be carried out with the available resources.\n\nExpert Interviews: Is the problem in general is very well suited for the deployment of data science and are there corresponding projects that have already been undertaken externally and also published?\nData science team: Are there a sufficient number of potentially suitable methods for this project and are the required data sources are available?\nIT department: check the available infrastructure and the expertise of the involved employees.\n\nMake sure everyone on the team agrees on what data that you want to collect and measure and who owns the data collection process.\nHaving someone of authority or that‚Äôs respected in each department involved in the development of the product will go a long way to building trust with users when it‚Äôs fully deployed\n\nExample: Demand Forecasting Automation\n\nTeam should consist of Supply Chain department and close collaboration with Sales and IT\n\n\nExample\n\nSuppose teachers at a school are interested in fielding quantitative surveys to track student outcomes, but there exists little incentive for teachers to collect this data on top of regular work. As a result, only one teacher in the school volunteers to design and administer the survey to their class. However, the survey results will now be limited to the students‚Äô experiences and outcomes for just the one class. The measured outcomes will be biased because they will not capture any variance across students from different classes in the school.\n\n\nCalculate How Much Data You Need to Collect\nMake sure you‚Äôve answered these questions\n\nIs the problem definition clear and specific? Are there measurable criteria that define success or failure?\nIs it technically feasible to address the defined problem within the designated timeframe? Is the data required for the envisioned solution approach available?\nDo all relevant stakeholders agree with the problem definition, performance indicators, and selection criteria?\nDoes the intended technical solution resolve the initial business problem?",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-wkshp",
    "href": "qmd/project-planning.html#sec-proj-plan-wkshp",
    "title": "37¬† Planning",
    "section": "37.3 Workshopping",
    "text": "37.3 Workshopping\n\nHelps data scientists understand where their energy is most needed\nMisc\n\nNotes from: Successfully Combining Design Thinking and Data Science\nUsually last 1 hr\n\nSupplies\n\nDifferent coloured sticky notes ‚Äî enough so that everyone has at least 15‚Äì20\nWhiteboard markers for the sticky notes. Permanent markers will likely cause some unintended permanent damage and thin, ballpoint pens are difficult to read unless you‚Äôre right up close to them\n1 white board per group, or alternatively, 2 x large A2 pieces of paper\n\nParticipants\n\nKey business stakeholders involved in the area you‚Äôre working on ‚Äî you need management buy-in if anything is going to happen\nTwo or three people who will actually use the tools or insights you‚Äôll be delivering\nA facilitator (probably you) and a few members of your data science or analytics team\n\nChallenge Identification Workshop\n\nSuited for a situation when the business stakeholders know what their challenges are, but don‚Äôt know what you can do for them.\nGoals:\n\nDS: Try to understand the customer pain points as well as you can.\n\nDo not try to develop solutions in this workshop\n\nStakeholders: get as many ideas out as possible in the time allotted.\n\nAdditional Supplies\n\nPoster or slide with user journey, or something similar, depending on the business\n\nHelps to guide participants to home-in on customer pain points\nNeeds to be done before the workshop\nExample\n\n\nProcess\n\nIntroduction\n\nBrief introduction of your team\nBrief 5-minute ‚ÄòBest In Class‚Äô slide or two where I look at companies who are currently doing amazing things, preferably in the data science domain, in the area that our stakeholders work\nAddress goal of the workshop (see above for stakeholders)\n\nSplit into two or three groups\n\nIdeally between four to five people per group\nPut senior managers or executives in separate groups\n1 DS or analyst in each group\n\nWrite as many customer pain points as possible (aka Ideation)\n\nDuration: 25‚Äì30 minutes\nEverybody writes ideas on sticky note and puts on their board\n\nNo need for whole group to approve of the idea.\n\n‚ÄúBad‚Äù ideas are weeded out later\n\n\nIf you see themes popping up, go ahead and group similar sticky-notes together\nDS or analyst needs to pay attention to each proposed idea, so as to be able to write a fuller description of the idea later on\n\nIf possible, note the author of each idea so as to be able to ask them questions later if needed.\n\n(You) Walk around to each group\n\nRemind them of the rules (get down as many ideas as possible)\nPrompt them with questions to get more ideation happening within the groups\n\n\nOne member from each of the groups presents their group‚Äôs key ideas back to the rest of the room\n\nDuration: 3-5 min per group\n\nPlace similar stickies into groups or themes\nVote on the best ideas\n\nDuration: 3 min\nEveryone gets three dot stickers and is allowed to vote for any ideas\nThey can put all their stickies on one idea or divide them up however they like\nNo discussions\nIf it‚Äôs the case that one sticky within a theme of stickies gets all the votes, or even if the votes are all shared equally, consider all of those as votes towards the theme.\n\n\n\nPredefined Problem Sculpting Workshop\n\nThe difference between this approach and the first one is that here the business stakeholders already have an idea about what they need.\n\ne.g a metric of some sort, some kind of customer segmentation or some kind of tool\n\nExample\n\nDevelop ‚Äòaffluency score‚Äô for each banking customer.\n\nGoal: Answer 2 questions\n\nWhat is it that we‚Äôre trying to do?\n\nDefining what it is you‚Äôre trying to do will help to define what is meant by the metric/segment/measure you‚Äôre developing.\nFrom example (see Goal above), it‚Äôs vital that everyone in the room understands what is meant by ‚Äúaffluence.‚Äù\n\nDoes it mean:\n\nhow much money someone currently has?\nIt is a measure of their potential spend?\nDoes it refer to their lifestyle and how much debt they may take on and can realistically pay?\n\n\n\nWhy do we want to do this?\n\nThe answer to why has design implications\nExamples\n\nIs it something we‚Äôll use to set strategic goals?\nDo we want to use it to identify cross or upsell opportunities?\n\n\n\nAdditional Supplies:\n\nPoster or slide with these 2 questions\n\nProcess\n\nIntroduction\n\nBrief introduction of your team\nAddress goals of the workshop\n\nSplit into two or three groups\n\nIdeally between four to five people per group\nPut senior managers or executives in separate groups\n1 DS or analyst in each group\n\nHave groups answer the ‚ÄúWhat‚Äù question\n\nDuration: 15 min\n\nFeedback session with whole workshop\n\nDuration: 3 min\nGroup answers are presented and discussed\n\nHave groups answer the ‚ÄúWhy‚Äù question\n\nDuration: 15 min\n\nFeedback session with whole workshop\n\nDuration: 3 min\nGroup answers are presented and discussed\n\n\nCompare and address gaps\n\nDuration: 4 min\nCompare all group answers against the current solution\n\ne.g.¬†does the current metric represent these answers to ‚Äúwhat‚Äù and ‚Äúwhy‚Äù?\n\nIf there are gaps between the group answers and the current solution, try to figure out how best the fill those gaps.\n\n\nPost-Workshop Debrief\n\nShould occur the same day as the workshop\nDocument sticky notes and key ideas\n\nThe data scientists/analysts embedded in the groups should be able to expand on the ideas on the sticky notes\nIf the author of the idea was recorded, that should be included as well\n\nAs you‚Äôre starting to think of solutions to the pain points you discussed, reach out to authors/stakeholders to get their opinions on your thoughts and to understand where you might be able to get the data from.\n\n\nHave data team ideate on solutions to these painpoints, etc.",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-thgscndr",
    "href": "qmd/project-planning.html#sec-proj-plan-thgscndr",
    "title": "37¬† Planning",
    "section": "37.4 Things to consider when choosing a project",
    "text": "37.4 Things to consider when choosing a project\n\nMisc\n\nAlso see Optimization, general &gt;&gt; Budget Allocation\n\nConsiderations\n\nBalancing the goals of your department with the desires of stakeholders\n\nBefore handling projects that are possible with the available data, address projects of immediate need to the business according to stakeholders.\n\nNeed to feel they‚Äôre getting what they think is value from the data department.\n\nAlso, have to ask, ‚Äúhow are we going to make money from this output?‚Äù and metrics ‚ÄúWhat organizational KPIs are tied to these metrics and how?‚Äù.\n\nIt is reasonable to (tactfully) say that the ‚Äúwins‚Äù column of your self-evaluation needs some big victories this year, and the ‚Äútop 10 products shipped this hour‚Äù dashboard isn‚Äôt going to get us where we want to be as an organization. Some fluff is acceptable to keep the peace.\nNo one will rush to the defense of the data team come budget season, regardless of how many low-value asks you successfully delivered, as requested, in the prior year (this is called executive amnesia).\n\n\nBeing useful\n\nSituations\n\nImproving upon a metric score (business or model)\n\n‚Äúanalyze actual failure cases and have a hard and honest think about what will fix the largest number of them. (My prediction: improving the data will by and large trump a more complex method.) The point is, you have to have a real think about what will actually improve the number, and that might involve work that‚Äôs stupid and boring. Don‚Äôt just reach for a smart new shiny thing and hope for the best.‚Äù\n\na lot of product people running around saying ‚Äúwe want to do this thing asap, but we don‚Äôt know if it‚Äôs possible\n\nJust find a product person who seems sane, buddy up with them, and work on what they want. They should be experts in their product, have an understanding of what the potential market for it is, and so really know what will be useful and what won‚Äôt.\n\n\n\n\nProcess\n\nScore Potential Projects\n\nCreate a score for ‚Äúimpact‚Äù\n\nPossible impact score features\n\nBack of the envelope monetary value\nCustomer experience score / Net Promoter Score (NPS)\n\n\nCreate a score for ‚Äúdifficulty‚Äù\n\nPossible difficulty score features\n\nhow long you believe the project will take to build\nresource constraints into consideration\nhow difficult the data may be to acquire and clean\n\n\nCategorize each project as Low Hanging Fruit, Quick Wins, Long Term Projects, and Distractions based the Impact and Difficulty Scores\n\nSee chart above\n\nPrioritize Low Hanging Fruit\nConsult with stakeholders and decide which Quick Wins and Long Term Projects to pursue",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-elevptch",
    "href": "qmd/project-planning.html#sec-proj-plan-elevptch",
    "title": "37¬† Planning",
    "section": "37.5 Elevator Pitch",
    "text": "37.5 Elevator Pitch\n\nComponents\n\nValue Proposition:\n\nDescribe the problem\n\nWho is dissatisfied with the current environment\nThe targeted segment that your project is addressing\n\nDescribe the product/service that you are developing and what specific problem that it is solving\n\nYour Differentiation:\n\nDescribe the alternative (perhaps your competition or currently available product/service)\nDescribe a specific (not all) functionality or feature that is different from the currently available product/service.\n\n\nDelivery Formula\n\nFor [targeted segment of customers/users],\nWho are dissatisfied with [currently available product/service]\nOur product/service is a [new product category]\nThat provides a [key problem-solving capability]\nUnlike [product/service alternative],\nWe have assembled [Key product/service features for your specific application]\n\nExample For political election campaign managers,\n  **Who are dissatisfied with** the traditional polling products,\n\n  **Our application is a** new type of polling product\n\n  **That provides** the ability to design, implement, and get results within 24 hours.\n\n  **Unlike** the other traditional polling products that take over 5 days to complete,\n\n  **We have assembled** a quick and more economic yet comparably accurate polling product.\nExample For front line criminal investigators,\n  **Who are dissatisfied with** generic dashboards that display too much unnecessary information,\n\n  **Our application is a** new type of Intelligence product\n\n  **That provides** a highly customized and machine learning-enabled risk assessment tool that allows the investigator to uncover a hidden network of potentially criminals.\n\n  **Unlike** the current dashboard that provides information that is often not very useful,\n\n  **We have assembled** an intelligence product that allows them to make associations between known and unknown actors of interest.",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-edarp",
    "href": "qmd/project-planning.html#sec-proj-plan-edarp",
    "title": "37¬† Planning",
    "section": "37.6 Exploratory Data Analysis Research Plan (EDARP)",
    "text": "37.6 Exploratory Data Analysis Research Plan (EDARP)\n\nImperative that this plan be formulated before work on the project begins\n\n\nDevelop a research question and objectives to be met\n\nA concise research question and objective allows the stakeholders to be informed on how their departments will be affected.\nSoft skills are important in being able to make a stakeholder comfortable with answering questions.\nIt helps if each stakeholder feels like their input is necessary to the project. More likely to get invested in its success.¬†May require salesmanship to convince them that the knowledge of self has future benefits\nComing up with a concise question often involves drilling down into the mechanics of an organization.\n\nThis process can lead to discovering the importance of the research. Also, can lead to other research questions.\n\nIf stakeholder is ambiguous about what they want. Useful to use a backwards path by starting with what the stakeholder envisions as the final output\n\n‚ÄúCan you tell me about the decisions you are going to make based on the output of this request? Understanding those decisions will help us ensure that the (report/data/model) we build for you will fulfill your needs.‚Äù\n\nIf a concise question cannot be obtained or the objective is open-ended, you should gather all the information obtained and use it determine what you think the objective of the project should be. Then, develop a prototype\n\nAfter a portion of the project is completed, it will be necessary to share the progress with the stakeholders and revise aspects of the project as needed. More of an iterative process.\nThe iteration can increase buy-in from stakeholders and develop a more concise question\n\nAfter the question is decided upon, what does success look like? What are the metrics of evaluation?¬†¬†\n\nDefine methods to address the question\n\ntype of method depends on the type of question\n\ndescriptive\n\nUsually answered with historical data, e.g historical trends. Often presented in dashboards\nmean, min, max, etc.\nplots for understanding relationships\noutlier detection\n\npredictive\n\npredictive models and forecasting\nregression, classification, clustering\n\nprescriptive\n\npredictive models + proposing actions\n\n\n\nDetermine which data is on-hand and which need to be acquired\n\nAlso see pipeline section below\nNote data sources\n\nStructured\nUnstructured\nInternal\nExternal\n\nWrangle data\n\nDetermine the budget and adjust prior steps to conform within this budget\n\nDetermined required tools\n\ncompute capacity\nend user requirements\nbusiness interests\n\nMake effort to minimize cost as much as possible. Makes dialogue with stakeholders easier\n\nStakeholders\n\nA departments that will make use of the output or be effected by should be involved\n\nConvincing them to be involved is important if any issues with budgets, etc. pop-up. Having more people on your side will help.\n\nUsing a proof-of-concept or information of past successes of such projects within or outside the organization can help.\nNeed to proof on how this research will increase their ROI. Prototypes with datascience outputs can go a long way convincing or increasing buy-in from skeptical stakeholders.\n\n\nTheir concerns should be noted and addressed.\nOften times they are the end-user, so having them involved can lead to a better output. A greater probability of the project being involved in decision making.\n\nHow will the end-users make use of the output?\n\nShould the output be a report or app?\n\nDevelop a plan of action for upkeep or maintenance of the project\n\nMonitoring model drift\nChanges in the business\nResolve any lingering user concerns\nDetermine schedule for updating project¬†\nSchedule stakeholder meeting to discuss the project and determine if further steps needed",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-decmod",
    "href": "qmd/project-planning.html#sec-proj-plan-decmod",
    "title": "37¬† Planning",
    "section": "37.7 Decision Models",
    "text": "37.7 Decision Models\n\n\nNotes from https://towardsdatascience.com/learn-how-to-financially-evaluate-your-analytics-projects-with-a-robust-decision-model-6655a5229872\nAlso see\n\nAlgorithms, Business &gt;&gt; Cost-Benefit Analysis\nOptimization, general &gt;&gt; Budget Allocation\nVideo Bizsci Learning Lab 68\n\n{tidyquant} has excel functions for Net Present Value (NPV), Future Value of Cashflow (FV), and Present Value of Future Cashflow (PV)\n** A lot of this stuff wasn‚Äôt used in the examples**\n\nFocus on fixed and variable costs, investment, depreciation, cost of capital, tax rate, revenue, qualitative factors\n\nNet Present Value (NPV) is used as the final quantitative metric (Value in flow chart)\n\nShort running projects can just use the orange boxes to calculate value\nLong running projects would use orange, blue, and gray boxes to calculate value\n\nThe values for each variable need to calculated for each year (or whatever time unit) of the project.\nDifferent scenarios (pessimistic, optimistic) can be created by modifying the parameters of the model like the life span, the initial investment, or the costs, to see the potential financial impact on Value.\nMaking a decision\n\nCompare your NPV and the Internal Rate of Return (IRR) with alternative analytics projects that the organization is considering to see which one is more attractive.\nBalance the quantitative and qualitative factors while considering the potential risks of the project.\n\nExample\n\n‚ÄúIn quantitative factors, we used the cash flow model where we calculated a net present value of USD $7,187,955. Related to qualitative factors and potential risks, we can conclude that if the project is back up by an influential group of leaders, then the positive aspects exceed the negative ones. On the balance, we would advise a GO decision for this project.‚Äù\n\n\n\nExecutive Presentation\n\nThe conclusion should be short, precise, and convincing format\nCrucial to persuade key stakeholders to invest in your project\nUse a good action title, writing down the most relevant requirements of your project, use a chart to visualize your results, and have some key takeaways on the bottom with a recommendation.\n\nProposal\n\nSections 1-3 should be put into a concise executive summary\nTell a story\n\nEvery item should be consistent with every other part\n\ne.g.¬†a deliverable shouldn‚Äôt be mentioned if it‚Äôs not part of solving the issue in the problem statement\n\nShould flow from more abstract ideas to more and more detail\n\nFormat\n\nProblem Statement\n\nPaint a bleak picture and explain how the problem could‚Äôve been overcome if your project were in production when the problem occurred\nList examples where risk and costs occurred, opportunities were missed, etc. and how your project would‚Äôve prevented or taken advantage of these things if it had been in production\n\nskill upgrades or efficiency gains are NOT good answers\n\n\nVision\n\nTies your project to the companies long range strategy\n\nBenefits\n\ndetails¬† about the specific new capabilities provided or costs avoided\nneed to be quantifiable\n\nDeliverables\n\ndetails about other non-primary benefits\n\nSuccess Criteria\n\nCharacteristics\n\nspecific, measurable, bounded by a time period, realistic\n\n\nThe plan\n\noverview of the steps the project will take\n\nincludes deadlines, waterfall or agile approach, etc.\n\n\nCost/Budget\n\n\nTerms\n\nCost/Investment\n\nFixed\n\nPersonnel salaries\n\nIf you‚Äôre using outside data consultants, Glassdoor has average salaries, https://www.glassdoor.com/Salaries/data-engineer-salary-SRCH_KO0,13.htm\n\nexample: ~$100K each\n\n\nAdditional investments could be access to external data sources, research, and software licenses\n\nTableau can cost around $48K per year\n\nTraining for any additional or current personnel on tools or subject matter\n\nworkshops, online learning\ntax-reclamation example: ~$12K total\n\n\nVariable\n\nCloud storage\n\ntax-reclamation example: AWS (pay-as-you-go) $75K-$95K per year\n\n\n\nTax rate\n\ntax-reclamation example: 28%\n\nRevenue\n\nForecasts of revenue\n\nCorrection Factors\n\nExplains the difference between profit after tax and cashflow\ne.g.¬†depreciation of investment, changing working capital, investment, change in financing or borrowing\n\ncost of sales = (revenue*some%) - 1\n\nrefers to what the seller has to pay in order to create the product and get it into the hands of a paying customer.\n\nMaybe this is a catch-all for anything else not covered in the Cost/Investment section\n\naka cost of revenue (sales/service) or cost of goods (manufacturing)\n30% used in example\nBegins after year 1\n\ndepreciation per year = (investment - salvage_value)/(num_years - 1)\n\nFor an datascience project, the salvage_value is 0 most of the time.\ndepreciation value is listed for each year after the first year of the project\ngets subtracted from revenue in calculation of profit_before_tax calculation\ngets added to profit_after_tax in cashflow equation\n\nChange in working capital = (expected_yearly_revenue/365) * customer_credit_period\n\ncustomer_credit_period (days): number of days that a¬†customer¬†is allowed to wait before paying an invoice. The concept is important because it indicates the amount of working capital that a business is willing to invest in its accounts receivable in order to generate sales.\n\n60 days used in example\n\nOnly a cost in the 1st yr and then it is recouped in the last year (somehow)\n\nsounds like this whole thing is just an accounting procedure\n\nDunno how this relates to a datascience project\n\n\n\nCost of Capital\n\nUsed to characterize risk here. Think 5-10% is used for back-of-the-envelope calculations.\n\nIn the tax-reclamation example, 8.3% was used\n\nAlso see\n\nAlgorithms, Product &gt;&gt; Cost Benefit Analysis (CBA)\nMorgan Stanley‚Äôs Guide to Cost of Capital (Also in R &gt;&gt; Documents &gt;&gt; Business)\n\nThread about the guide\n\n\nCost of Capital is typically calculated as Weighted Average Cost of Capital (WACC)\n\nSee Finance, Glossary &gt;&gt; Weighted Average Cost of Capital (WACC)\n\nAs of January 2019, transportation in railroads has the highest cost of capital at 11.17%. The lowest cost of capital can be claimed by non-bank and insurance financial services companies at 2.79%.\nHigh CoCs - Biotech and pharmaceutical drug companies, steel manufacturers, Internet (software) companies, and integrated oil and gas companies. Those industries tend to require significant capital investment in research, development, equipment, and factories.\nLow CoCs - Money center banks, power companies, real estate investment trusts (REITs), retail grocery and food companies, and utilities (both general and water). Such companies may require less equipment or benefit from very steady cash flows.\n\nNet Present Value (NPV)\n\nUses cashflow (excludes year 1) and cost of capital as inputs\n\nExcel has some sort of NPV wizard that does the calculation (see tidyquant for npv function)\nExample in article provided link to his googlesheet so the formula is there or maybe googlesheets has a wizard too.\n\n\nInternal Rate of Return (IRR)\n\nIt‚Äôs the value that the cost of capital (aka discount rate) would have to be for NPV and cost to zero out each other.\nThe interest rate at which a company borrows against itself as a proxy for opportunity cost. Typically, large and/or public organizations have a budgetary IRR of 10% to 15% depending on the industry and financial situation.\nHigher is better\nOnly valid in very limited circumstances. MIRR is much better\n\nSee Finance, Glossary &gt;&gt; IRR, MIRR\n\n\nQualitative Factors\n\nOther things that have value but are difficult to quantify. Might change a quantitatively negative value project into a positive value project\n\nUses columns negative, positive, impact\n\nnegative and positive are indicators\nimpact has values low, medium, and high\n\n\nExamples\n\nProject has flexibility (+) (calculated using real options analysis(?))\nStrategic Fit (+) - supports strategy of the company (?)\n\nincreases trust with public\nmarketing boosted because they can use buzzwords like ‚ÄúAI‚Äù or ‚Äúdata-driven‚Äù in ads\n\nIncreases competency (+) - may help the company down the road\n\nmore agile, data-driven, familiarity with newer technologies\n\nred tape or bureaucracy or politics causing delays (-)\nImplementation (-)\n\ncan be a negative, if the leadership that‚Äôs needed is tied up with other projects\nChallenges",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-capf",
    "href": "qmd/project-planning.html#sec-proj-plan-capf",
    "title": "37¬† Planning",
    "section": "37.8 Choosing a Pipeline Framework",
    "text": "37.8 Choosing a Pipeline Framework\n - Kind of like working backwards in the data analysis workflow (design driven development) - Quantify the problems and develop KPIs that can inform the direction of the business - 10-15 organizational KPIs is common - Examples: Number of daily sales, the number of new customers, decreasing cost of operations/logistics - Build data pipelines around these critical KPIs - What data can we use that we already have access to? - What kind of internal data do we need to capture? - What kind of third party/external data could be useful? - What is the least amount of data available that can be used to approximate each KPI metric? - After the simplest version of the metric is built, you can brainstorm on how it can be improved. - Personalize a general metric to your specific business. - Split a general metric into smaller metrics can increase accuracy (e.g.¬†product line, geography, etc.) - What kind of storage will we need? - Adding a pipeline for a new data source - Capture a sample of the data to examine its potential. (e.g.¬†cookies on your website) - If metric measured from new data is potentially valuable, build a simple, less robust (without sacrificing too much data integrity) pipeline to your lake. - Meaning fewer built-in quality checks - Doing it this way does add technical debt - If metric feeds into a company-wide metric, then additional checks will need to be added - E.g. no duplicate datasets, data corruption, or data losses. - After 6 months or so, if metric remains useful, make pipeline more robust. - Adding third party data pipeline (e.g.¬†Equifax) - E.g. demographic, income data - If a product has a certain demographics associated with it, this data can be used to weight customers relative to the demographic specifics of that product. - Get a sample first to determine if it‚Äôs worth it - 3rd party data is expensive to buy and usually a reoccurring cost. - Can be time-consuming and resource intensive to add the pipeline - Get transparency on how the data was collected and validity checks used - Periodically reevaluate whether the metrics fundamentally still make sense from a business perspective and how they can be improved. - Expand pipelines to capture additional data in order to continue to refine metrics - Drop and add metrics as business and trends change. - Redefine metrics as needed -",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/python-reticulate.html",
    "href": "qmd/python-reticulate.html",
    "title": "reticulate",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Python",
      "reticulate"
    ]
  },
  {
    "objectID": "qmd/python-reticulate.html#sec-py-retic-misc",
    "href": "qmd/python-reticulate.html#sec-py-retic-misc",
    "title": "reticulate",
    "section": "",
    "text": "Before interactively running python in RStudio, start REPL\nreticulate::repl_python()",
    "crumbs": [
      "Python",
      "reticulate"
    ]
  },
  {
    "objectID": "qmd/python-reticulate.html#sec-py-retic-pynr",
    "href": "qmd/python-reticulate.html#sec-py-retic-pynr",
    "title": "reticulate",
    "section": "Python in R Scripts",
    "text": "Python in R Scripts\n\nVia source_python\n\nExample\nlibrary(tidyverse)\nlibrary(reticulate)\nsource(\"funs.R\")\nuse_virtualenv(\"../../\")\nsource_python(\"funs.py\")\n\n# stuff\n\nfor (r in 1:nrow(res)) {\n  cat(r, \"\\n\")\n\n  tmp_wikitext &lt;- get_wikitext(res$film[r], res$year[r]) # r fun\n\n  # skip if get_wikitext fails\n  if (is.na(tmp_wikitext)) next\n  if (length(tmp_wikitext) == 0) next\n\n  # give the text to openai\n  tmp_chat &lt;- tryCatch(\n    get_results(client, tmp_wikitext), # py fun\n    error = \\(x) NA\n  )\n\n  # if openai returned a dict of 2\n  if (length(tmp_chat) == 2) {\n    res$writer[r] &lt;- tmp_chat$writer\n    res$producer[r] &lt;- tmp_chat$producer\n  }\n}\nget_results is a python function defined in funs.py",
    "crumbs": [
      "Python",
      "reticulate"
    ]
  },
  {
    "objectID": "qmd/python-reticulate.html#sec-py-retic-rmark",
    "href": "qmd/python-reticulate.html#sec-py-retic-rmark",
    "title": "reticulate",
    "section": "RMarkdown",
    "text": "RMarkdown\n\nAlso see Quarto &gt;&gt; R and Python\nBasic set-up\n---\ntitle: \"R Notebook\"\noutput: html_notebook\n---\n\n\n```{r}\nknitr::opts_chunk$set(\n¬† echo = TRUE,\n¬† message = FALSE,\n¬† warning = FALSE\n)\n```\n```{r}\nlibrary(reticulate) \n```\n```{python}\nimport pandas as pd \nimport numpy as np\n```",
    "crumbs": [
      "Python",
      "reticulate"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html",
    "href": "qmd/post-hoc-analysis-multilevel.html",
    "title": "Multilevel",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Post-Hoc Analysis",
      "Multilevel"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-misc",
    "href": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-misc",
    "title": "Multilevel",
    "section": "",
    "text": "Also see Post-Hoc Analysis, general\nPackages\n\n{effectsize} - Has many of the metrics discussed here and others ‚Äî¬†with confidence intervals",
    "crumbs": [
      "Post-Hoc Analysis",
      "Multilevel"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-tukey",
    "href": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-tukey",
    "title": "Multilevel",
    "section": "Tukey Test",
    "text": "Tukey Test\n\nDifference in effects\nExample: Is there a statistically significant difference between the estimated effects of the categories of the fixed effect, ‚ÄúSeason‚Äù Data from Multilevel Modeling and Effects Statistics for Sports Scientists in R\n\n{multcomp}{emmeans}\n\n\nlibrary(multcomp)\n# pairwise comparisons\nfit_tukey &lt;- glht(fit, linfct=mcp(Season=\"Tukey\"))\nsummary(fit_tukey)\n##¬†\n## Simultaneous Tests for General Linear Hypotheses\n##¬†\n## Multiple Comparisons of Means: Tukey Contrasts\n##¬†\n##¬†\n## Fit: lmer(formula = Distance ~ Season + (1 | Athlete), data = data)\n##¬†\n## Linear Hypotheses:\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate Std.  Error z value Pr(&gt;|z|)¬† ¬†\n## Postseason - Inseason == 0¬† ¬†     36.71¬† ¬†90.08¬†  0.408¬† ¬† 0.911¬† ¬†\n## Preseason - Inseason == 0¬† ¬†    1166.00¬† ¬†90.08¬† 12.944¬†  &lt;1e-05 ***\n## Preseason - Postseason == 0¬†    1129.29¬† 110.32¬† 10.236¬†  &lt;1e-05 ***\n## ---\n## Signif. codes:¬† 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## (Adjusted p values reported -- single-step method)\n\n\nemmeans(fit, specs = pairwise ~ Season)\n## $emmeans\n##¬† Season¬† ¬† emmean¬† SE¬†  df lower.CL upper.CL\n##¬† Inseason¬† ¬† 5104 137 20.8¬† ¬†  4818¬† ¬† 5389\n##¬† Postseason¬† 5140 151 30.6¬† ¬†  4831¬† ¬† 5449\n##¬† Preseason¬† ¬†6270 151 30.6¬† ¬†  5961¬† ¬† 6579\n## Degrees-of-freedom method: kenward-roger¬†\n## Confidence level used: 0.95¬†\n## $contrasts\n##¬† contrast¬† ¬† ¬† ¬† ¬† ¬† ¬†  estimate¬† ¬† SE¬† df t.ratio p.value\n##¬† Inseason - Postseason¬† ¬†  -36.7¬† 90.1 978¬† -0.408 0.9125¬†\n##¬† Inseason - Preseason¬† ¬† -1166.0¬† 90.1 978 -12.944 &lt;.0001¬†\n##¬† Postseason - Preseason¬† -1129.3 110.3 978 -10.236 &lt;.0001¬†\nDegrees-of-freedom method: kenward-roger¬†\nP value adjustment: tukey method for comparing a family of 3 estimates\n\n\n\n\nInterpretation\n\nThere is NOT a difference between the effect that Postseason has on Distance and the effect that Inseason has on Distance.\nThere is a difference with between the other two pairs of categores\nEstimated mean distance given season type\n\nI‚Äôm not sure these estimates are appropriate in this situation since the Season variable is inherently unbalanced.\nAlso see emmeans Post-Hoc Analysis, emmeans",
    "crumbs": [
      "Post-Hoc Analysis",
      "Multilevel"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-cohensd",
    "href": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-cohensd",
    "title": "Multilevel",
    "section": "Cohen‚Äôs D",
    "text": "Cohen‚Äôs D\n\nStandardized difference in means given a grouping variable\nGenerally recommended to use \\(g_{\\text{rm}}\\) or \\(g_{\\text{av}}\\)\n\nStandard practice is use whichever one of those two values is closer to \\(d_s\\) , because it helps make the result comparable with between-subject studies.\nCorrection for bias can be important when dof &lt; 50\nAppropriate Version Per Use Case\n\n\n\n\n\n\n\nUse\nVersion\n\n\n\n\nIndependent groups, power analyses where \\(\\sigma_\\text{pop}\\) is known or \\(\\sigma\\) is calculated with \\(n\\)\n\\(d_{\\text{pop}}\\)\n\n\nIndependent groups, power analyses where \\(\\sigma_\\text{pop}\\) is unknown or \\(\\sigma\\) is calculated with \\(n-1\\)\n\\(d_s\\)\n\n\nIndependent groups, corrects for small sample bias; report for use in meta-analyses\n\\(g\\)\n\n\nIndependent groups, when treatment might affect SD\n\\(\\Delta\\)\n\n\nCorrelated groups; generally recommended over \\(g_{\\text{rm}}\\)\n\\(g_{\\text{av}}\\)\n\n\nCorrelated groups; more conservative than \\(g_{\\text{av}}\\)\n\\(g_{\\text{rm}}\\)\n\n\nCorrelated groups; power analyses\n\\(d_z\\)\n\n\n\n\nNotes from: Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs (Lakens)\nCan be used to compare effects across studies, even when the dependent variables are measured in different ways\n\nExamples\n\nWhen one study uses 7-point scales to measure dependent variables, while the other study uses 9-point scales\nWhen completely different measures are used, such as when one study uses self-report measures, and another study used physiological measurements.\n\n\nThe bias-corrected version is known as Hedges‚Äô g, and in the r family of effect sizes, the correction for eta squared (Œ∑2) is known as omega squared (œâ2)\nGuidelines\n\nRange: 0 to \\(\\infty\\)\nCohen (1992)\n\n|d| &lt; 0.2 ‚Äúnegligible‚Äù\n|d| &lt; 0.5 ‚Äúsmall‚Äù\n|d| &lt; 0.8 ‚Äúmedium‚Äù\notherwise ‚Äúlarge‚Äù\n\nOthers: Automated Interpretation of Indices of Effect Size\nValues should not be interpreted rigidly\n\ne.g.¬†Small effect sizes can have large consequences, such as an intervention that leads to a reliable reduction in suicide rates with an effect size of d = 0.1.\n\nThe only reason to use these benchmarks is when the findings are extremely novel, and cannot be compared to related findings in the literature.\n\nTwo groups of Independent Observations (Between-Subjects)\n\\[\n\\begin{align}\nd_s &= \\frac{\\bar X_1 - \\bar X_2}{\\sqrt{\\frac{(n_1-1)SD^2_1 + (n_2-1)SD^2_2}{n_1 + n_2 - 2}}}\\\\\n&= t\\;\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\\\\n& \\approx \\frac{2t}{\\sqrt{N}}\n\\end{align}\n\\]\n\nWhere the denominator is the pooled standard deviation\n\\(t\\) is the t-value of two-sample t-test\nTypically used in an a priori power analysis for between-subjects designs\nHedges‚Äô g (bias-corrected)\n\\[\ng_s = d_s \\times \\left(1-\\frac{3}{4(n_1 + n_2) - 9}\\right)\n\\]\n\nThe same correction is used for all types of Cohen‚Äôs d\nThe difference between Hedges‚Äôs gs and Cohen‚Äôs ds is very small, especially in sample sizes above 20\n\nInterpretation: A percentage of the standard deviation. Best to relate it to other effect sizes in the literature and it‚Äôs practical consequences if possible.\n\ne.g.¬†\\(d_s = 0.5\\) says the difference in means is half a standard deviation.\n\nWhenever standard deviations differ substantially between groups, Glass‚Äôs \\(\\Delta\\) should also be reported\n\nOne Sample or Correlated Samples (Within-Subjects)\n\\[\n\\begin{aligned}\n&d_z = \\frac{M_{\\text{diff}}}{S_{\\text{diff}}} = \\frac{t}{\\sqrt{n}} \\\\\n&\\begin{aligned}\n\\text{where} \\quad S_{\\text{diff}}^{(1)} &= \\sqrt{\\frac{\\sum(X_{\\text{diff}} - M_{\\text{diff}})^2}{N-1}} \\\\\nS_{\\text{diff}}^{(2)} &= \\sqrt{\\text{SD}_1^2 + \\text{SD}_2^2 - (2\\cdot r\\cdot \\text{SD}_1 \\cdot \\text{SD}_2)}\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\\(M_{\\text{diff}}\\) is the difference between the mean (M) of the difference scores and the comparison value, \\(\\mu\\) (typically 0)\n\nFor paired data, the mean of the difference scores is equal to the difference in means of the two groups, so you may see it described or calculated either way.\n\n\\(X_{\\text{diff}}\\) are the difference scores (i.e.¬†the difference between the repeated measurements)\n\\(S_\\text{diff}\\) is the SD of the difference scores.\n\nIt can be calculated two different ways, but I doubt both are equal to each other.\nThe second way seems to be the preferred way since it incorporates a correlation measure.\n\n\\(t\\) is the t-value of a paired samples t-test\n\\(r\\) is the correlation between measurements\n\nRepeated Measures (Within-Subjects)\n\\[\nd_{\\text{rm}} = d_z \\cdot \\sqrt{2(1-r)}\n\\]\n\nAlternative\n\\[\nd_{\\text{av}} = \\frac{M_{\\text{diff}}}{\\frac{\\text{SD}_1 + \\text{SD}_2}{2}}\n\\]\n\nIgnores the correlation between measures\n\nIf it is believe that the intervention/treatment affected the SD after the intervention, then it is advised to only use either (pre-treatment) \\(\\text{SD}_1\\) (recommended) or (post-treatment) \\(\\text{SD}_2\\) and report which one is used. The calculated effect is then known as Glass‚Äôs \\(\\boldsymbol{\\Delta}\\)\n\nExample: Distance (outcome), Season (Grouping variable)\n\nComparing Distance means given Season (3 levels) type\nData from Multilevel Modeling and Effects Statistics for Sports Scientists in R\n\n\n{effsize}{rstatix}{esci}\n\n\nAnother package, {effectsize}, is similar in that its formula arg only allows for grouping variables with only 2 levels\n\nMay have other features though, since it‚Äôs part of the easystats suite.\n\nlibrary(effsize)\neffsize::cohen.d(preseason_data$Distance, inseason_data$Distance)\n##¬†\n## Cohen's d\n##¬†\n## d estimate: 0.9157833 (large)\n## 95 percent confidence interval:\n##¬† ¬† lower¬† ¬† upper¬†\n## 0.7493283 1.0822383\n\nSeason is a categorical fixed effect with 3 levels\nOther Available Arguments: hedges.correction, pooled, paired, within, noncentral\n\n\n\nlibrary(rstatix)\ndata %&gt;%¬†\n¬† rstatix::cohens_d(Distance ~ Season, ci = TRUE)\n\n#&gt;     .y.¬† ¬† ¬†   group1¬† ¬†  group2¬† ¬† effsize¬† ¬†  n1¬† ¬†  n2   conf.low conf.high magnitude¬†\n#&gt; *  &lt;chr&gt;¬† ¬†    &lt;chr&gt;¬† ¬† ¬† &lt;chr&gt;¬† ¬† ¬† ¬†&lt;dbl&gt;   &lt;int&gt;  &lt;int&gt;¬† ¬† &lt;dbl&gt;¬† ¬† &lt;dbl&gt; &lt;ord&gt;¬† ¬† ¬†\n#&gt; 1 Distance   Inseason¬† Postseason   -0.0317¬†   600¬†   200¬† ¬†  -0.18¬† ¬† ¬†0.13  negligible\n#&gt; 2 Distance   Inseason¬†  Preseason¬†   -0.877¬† ¬† 600¬†   200¬† ¬†  -1.06¬† ¬† -0.71       large¬† ¬† ¬†\n#&gt; 3 Distance Postseason   Preseason¬†   -0.884¬† ¬† 200¬†   200¬† ¬†  -1.09¬† ¬† -0.68       large\n\nSame types of arguments as {effsize} are available and also bootstrap CIs\nMagnitude (interpretation) by Cohen‚Äôs (1992) guidelines\n\n\n\nestimate &lt;- esci::estimate_mdiff_two(\n  data = mydata,\n  outcome_variable = Prediction,\n  grouping_variable = Exposure,\n  conf_level = 0.95,\n  assume_equal_variance = TRUE\n)\nestimate$es_smd |&gt; \n  tidyr::gather(key = \"type\", \n                value = \"value\")\n#&gt;                      type             value\n#&gt; 1   outcome_variable_name        Prediction\n#&gt; 2  grouping_variable_name          Exposure\n#&gt; 3                  effect            20 ‚Äí 1\n#&gt; 4             effect_size 0.571611929854665\n#&gt; 5                      LL 0.327273973938463\n#&gt; 6                      UL  0.81492376943417\n#&gt; 7               numerator  11.3842850063322\n#&gt; 8             denominator  19.8603120279963\n#&gt; 9                      SE 0.124402744976289\n#&gt; 10                     df               268\n#&gt; 11               d_biased 0.573217832141019\n\nestimate$es_smd_properties$message\n#&gt; This standardized mean difference is called d_s because the standardizer used was s_p. d_s has been corrected for bias. Correction for bias can be important when df &lt; 50.  See the rightmost column for the biased value.\n\nFairly large effect: d = 0.57 95% CI [0.33, 0.81] and the confidence interval is fairly narrow\nMakes available the type of cohen‚Äôs d and the denominator used",
    "crumbs": [
      "Post-Hoc Analysis",
      "Multilevel"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html#common-language-effect-size",
    "href": "qmd/post-hoc-analysis-multilevel.html#common-language-effect-size",
    "title": "Multilevel",
    "section": "Common Language Effect Size",
    "text": "Common Language Effect Size\n\nAKA Probability of Superiority\nConverts the effect size into a percentage which is supposed to more understandable for laymen\nMisc\n\nNotes from The Common Language Effect Size Statistic\nPackages\n\n{ebtools::cles}\n\n\nInterpretation\n\nBetween-Subjects: The probability that a randomly sampled person from the first group will have a higher observed measurement than a randomly sampled person from the second group\nWithin-Subjects: The probability that an individual has a higher value on one measurement than the other.\n\nFormula\n\nAssumes variables are normally distributed and \\(\\sigma_1 = \\sigma_2\\)\n\nOriginal paper gives some evidence that these formulas are pretty robust to violations though.\nRecommended only for continuous variables\n\nBetween-Subjects\n\\[\n\\begin{align}\n\\tilde d &= \\frac{|M_1 - M_2|}{\\sqrt{p_1\\text{SD}_1^2 + p_2\\text{SD}_2^2}} \\\\\nZ &= \\frac{\\tilde d}{\\sqrt{2}}\n\\end{align}\n\\]\n\n\\(M_i\\): The mean of the ith group variable\n\\(p_i\\): The proportion of the sample size of the ith group variable\n\nWithin Subjects\n\\[\nZ = \\frac{|M_1 - M_2|}{\\sqrt{\\operatorname{SD}_1^2 + \\operatorname{SD}_2^2 - 2 \\times r \\times \\operatorname{SD}_1 \\times \\operatorname{SD}_2}}\n\\]\n\n\\(r\\) is the Pearson correlation between the group variables\n\n\nAlternative Generalization\n\n\\(A_{1,2} = P(X_1 &gt; X_2) + 0.5 \\times P(X_1 = X_2)\\)\nApplies for any, not necessarily continuous, distribution that is at least ordinally scaled\n\nEqual to CL in the continuous case\nInterpreted as an estimate of the value of CL that would be obtained if the distribution of X were continuous.",
    "crumbs": [
      "Post-Hoc Analysis",
      "Multilevel"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html#eta-squared",
    "href": "qmd/post-hoc-analysis-multilevel.html#eta-squared",
    "title": "Multilevel",
    "section": "Eta Squared",
    "text": "Eta Squared\n\nNotes from: Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs (Lakens)\nEffect Size for ANOVA\nMeasures the proportion of the variation in Y that is associated with membership of the different groups defined by X, or the sum of squares of the effect divided by the total sum of squares\neta squared is an uncorrected effect size estimate that estimates the amount of variance explained based on the sample, and not based on the entire population.\npartial eta squared (Œ∑2p) to improve the comparability of effect sizes between studies, which expresses the sum of squares of the effect in relation to the sum of squares of the effect and the sum of squares of the error associated with the effect.\nAlthough Œ∑2p is more useful when the goal is to compare effect sizes across studies, it is not perfect, because Œ∑2p differs when the same two means are compared in a within-subjects design or a between-subjects design.\nAn \\(\\eta^2\\) of 0.13 means that 13% of the total variance can be accounted for by group membership.\nCIs should be at 90%, because if you use 95%, it‚Äôs possible that even with a significant F-test, the CI will contain 0. For 90%, this doesn‚Äôt happen.\nEta Squared\n\\[\n\\eta^2 = \\frac{\\text{SS}_{\\text{effect}}}{\\text{SS}_{\\text{total}}}\n\\]\n\n\\(\\text{SS}_{\\text{effect}}\\) and \\(\\text{SS}_{\\text{total}}\\) are obtained from the ANOVA results\nThe correction for eta squared (\\(\\eta^2\\)) is known as omega squared (\\(\\omega^2\\)). Still biased but less biased. The difference is typically small, and the bias decreases as the sample size increases.\n\\[\n\\begin{align}\n\\omega^2 &= \\frac{\\operatorname{df}_{\\text{effect}}(\\operatorname{MS_{\\text{effect}}}-\\operatorname{MS_{\\text{error}}})}{\\operatorname{SS_{\\text{total}}} + \\operatorname{MS_{\\text{error}}}} \\quad \\text{(between-subjects)} \\\\\n\\omega^2 &= \\frac{\\operatorname{df}_{\\text{effect}}(\\operatorname{MS_{\\text{effect}}}-\\operatorname{MS_{\\text{error}}})}{\\operatorname{SS_{\\text{total}}} + \\operatorname{MS_{\\text{subjects}}}} \\quad \\text{(within-subjects)} \\\\\n\\end{align}\n\\]\n\nPartial Eta Squared\n\\[\n\\begin{align}\n\\eta_p^2 &= \\frac{\\operatorname{SS_{\\text{effect}}}}{\\operatorname{SS_{\\text{effect}}} + \\operatorname{SS_{\\text{error}}}} \\quad \\text{(fixed and measured variables)}\\\\\n\\eta_p^2 &= \\frac{F \\times \\operatorname{df}_{\\text{effect}}}{F \\times \\operatorname{df}_{\\text{effect}} + \\operatorname{df}_{\\text{error}}} \\quad \\text{(fixed variables)}\n\\end{align}\n\\]\n\nfixed (e.g., manipulated), not random (e.g., measured)\nBias-Lessened\n\\[\n\\omega_p^2 = \\frac{\\operatorname{df}_{\\text{effect}}(\\operatorname{MS_{\\text{effect}}}-\\operatorname{MS_{\\text{error}}})}{\\operatorname{df}_{\\text{effect}} \\times  \\operatorname{MS_{\\text{effect}}} + (N - \\operatorname{df}_{\\text{effect}}) \\times \\operatorname{MS_{\\text{error}}}}\n\\]\n\nSame equation whether it‚Äôs for between-subject designs and within-subject designs\n\n\nRecommend researchers report Œ∑2G and/or Œ∑2p, at least until generalized omega-squared is automatically provided by statistical software packages\n\nFor designs where all factors are manipulated between participants, Œ∑2p and Œ∑2G are identical, so either effect size can be reported. For within-subjects designs and mixed designs where all factors are manipulated, Œ∑2p can always be calculated from the F-value and the degrees of freedom using formula 13, but Œ∑2G cannot be calculated from the reported results,and therefore I recommend reporting Œ∑2G for these designs\nsupplementary spreadsheet provides a relatively easy way to calculate Œ∑2G for commonly used designs. For designs with measured factors or covariates, neither Œ∑2p nor Œ∑2G can be calculated from the\n\nAppropriate Version Per Use Cases\n\n\n\n\n\n\n\n\nUse Case\nVersion\nLess Biased Version\n\n\n\n\nComparisons within a single study\n\\(\\eta^2\\)\n\\(\\omega^2\\)\n\n\nPower analyses, and for comparisons of effect sizes across studies with the same experimental design\n\\(\\eta_p^2\\)\n\\(\\omega_p^2\\)\n\n\nMeta-Analyses to compare across various experimental designs\n\\(\\eta_G^2\\)\n\\(\\omega_G^2\\)\n\n\n\nGuidelines\n\nCohen‚Äôs benchmarks were developed for comparisons between unrestricted populations (e.g., men vs.¬†women), and using these benchmarks when interpreting the Œ∑2p effect size in designs that include covariates or repeated measures is not consistent with the onsiderations upon which the benchmarks were based.\n\nAlthough \\(\\eta_G^2\\) can be compared using these guidelines, it is preferable to compare effect sizes with those in the literature.",
    "crumbs": [
      "Post-Hoc Analysis",
      "Multilevel"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html",
    "href": "qmd/feature-reduction.html",
    "title": "Feature Reduction",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-misc",
    "href": "qmd/feature-reduction.html#sec-feat-red-misc",
    "title": "Feature Reduction",
    "section": "",
    "text": "Curse of Dimensionality\n\nIts when there are more variables than observations.\nCauses the least squares coefficient estimates to lose uniqueness.\nCauses overfitting in ML algorithms\n\nPackages\n\n{intRinsic} - Likelihood-Based Intrinsic Dimension Estimators; implements the ‚ÄòTWO-NN‚Äô and ‚ÄòGride‚Äô estimators and the ‚ÄòHidalgo‚Äô Bayesian mixture model\n\nProvides a clustering function for the Hidalgo model\nGraphical outputs built using ggplot2 so they are customizable\nSee section 5 (Summary and discussion) of the vignette for the recommended workflow and examples\n\n{Rdimtools} - feature selection, manifold learning, and intrinsic dimension estimation (IDE) methods\n\nCurrent version delivers 145 Dimension Reduction (DR) algorithms and 17 Intrinsic Dimension Estimator (IDE) methods.\n\n{RDRToolbox} - nonlinear dimension reduction with Isomap and LLE\n\nFor Time Series, see\n\nForecasting, Multivariate &gt;&gt; Dynamic Factor Models\n(Below) Dynamic Mode Decomposition\n{freqdom.fda} (Paper) - Dynamic Functional Principal Components Analysis (FPCA)",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-terms",
    "href": "qmd/feature-reduction.html#sec-feat-red-terms",
    "title": "Feature Reduction",
    "section": "Terms",
    "text": "Terms\n\nIntrinsic Dimension (ID) - the minimal number of parameters needed to represent all the information contained in the data without significant information loss. A necessary piece of information to have before attempting to perform any dimensionality reduction, manifold learning, or visualization tasks. An indicator of the complexity of the features of a dataset.\nIsomap (IM) - nonlinear dimension reduction technique presented by Tenenbaum, Silva and Langford in 2000 [3, 4]. In contrast to LLE, it preserves global properties of the data. That means, that geodesic distances between all samples are captured best in the low dimensional embedding\nLocally Linear Embedding (LLE) - introduced in 2000 by Roweis, Saul and Lawrence. It preserves local properties of the data by representing each sample in the data by a linear combination of its k nearest neighbors with each neighbor weighted independently. LLE finally chooses the low dimensional representation that best preserves the weights in the target space.\nProjection Methods - maps the original data to a lower-dimensional space. The projection function can be linear, as in the case of PCA or or nonlinear, as in the case of locally linear embedding, Isomap, and tSNE.\nGeometric Methods - rely on the topology of a dataset, exploiting the properties of the distances between data points",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-pca",
    "href": "qmd/feature-reduction.html#sec-feat-red-pca",
    "title": "Feature Reduction",
    "section": "PCA",
    "text": "PCA\n\nDescription\n\nCreates a subset of variables that maximises the covariance with the initial variable set, in order to store as much information as possible in a lower dimension.\nCompute an orthogonal basis of the space created by the original set of variables. The vectors creating this basis are the eigenvectors of the variance-covariance matrix. Reducing the dimension is then easily done by selecting the eigenvectors that are most representative of the initial data: those that contain most of the covariance. The amount of covariance stored by the vectors is quantified by the eigenvalues: the larger the eigenvalue, the more interesting its associated vectors.\nProjects variables orthogonally which removes correlation between predictor variables. The projection is in the direction of maximum variation such that the variation is distributed unequally among the transformed vectors. This¬†allows the user to reduce the feature space while still being able to capture most of variance in the data.\nThe principal components are equal to linear combinations of the correlated variables and these components are orthogonal to each other.\nWhy? When multiple variables are highly correlated to each other it causes the math used calculate regression models to break down. High dimension datasets also require large amount computational resources. Too many columns compared to the number of rows.\n\n\n\nMisc\n\nAs a multicollinearity detector?\n\n‚Äúuse principal component analysis, and examine the screeplot, or proportion of variation explained by a subset of principal components. If all (or almost all) of the variation is explained with a small subset of all the variables, it means you have a multicollinearity problem. You will need to drop some variables or do some other dimension reduction to fix it before choosing your final model.‚Äù\nI mean what if you have 20 variables and 3 are collinear, would this be detectable with PCA? I don‚Äôt think so. Seem more likely that it would take a large portion of your variables being collinear for it to be detectable in this fashion.\n\n\n\n\nPreprocessing\n\nNotes from thread\nCenter variables\n\ncenter = T is default in prcomp( )\nIf variables are NOT on similar scales, then the data need to be scaled, also.\n\nProbably safer to always scale.\n\n\nSqrt any count variables\nLog any variable with a heavy tail\nIf you have too many features, use a sparse matrix to speed the process.\n\n\n\nDiagnostics\n\nTest for localization (repo with R code/docs)\n\nBad: if you make a histogram of a component (or loading) vector and it has really big outliers (aka localization)\n\nMeans this vector is mostly noise\n\nSolution: Regularized spectral clustering (links to resources)\nD_r = Diagonal(1/ sqrt(rs + mean(rs))\nD_c = Diagonal(1/ sqrt(cs + mean(cs))\n# Do SVD on\nD_r %*% A %*% D_c\n\nA is your matrix\nrs is a vector containing the row sums of the matrix\ncs is a vector containing the column sums of the matrix\n\n\n\n\n\nSteps\n\nCenter data in design matrix, A (n x p)\n\nIf data are centered and scaled then the computation in step 2 will result in the correlation matrix instead of the covariance matrix.\n\nCompute \\(n \\times n\\) Covariance Matrix,\n\\[\nC_x = \\frac{1}{n-1}AA^T\n\\]\n\nAlso seen \\(A^T A\\) but I don‚Äôt think it matters. The upper triangle and the lower triangle of this product are just reverse covariances of each other and thus equal and I suspect the order just switches flips the triangles. The eigenvectors/eigenvalues get reordered later on anyways.\nThe diagonal of this matrix is the variable variances.\n\nCalculate eigenvectors and eigenvalues: \\(C_x V = D_\\lambda V\\) shows the covariance matrix as a transformation matrix. \\(D\\) is a diagonal matrix (\\(p\\times p\\)) with eigenvalues along the diagonal. \\(V\\) is a matrix (\\(p \\times p\\)) of eigenvectors\n\\[\nD_\\lambda = VC_x V^{-1}\n\\]\nOrder eigenvalues from largest to smallest\nOrder the eigenvectors according to the order of their corresponding eigenvalues\nEquation for the ith value of the PC1 vector: \\(\\text{PC1}_i = V_{(,1)} \\cdot A_{(i,)}\\)\n\n\\(\\text{PC2}\\) is similar except \\(V_{(,2)}\\) is used\nWhere all the variables in \\(A\\) have been standardized and \\(V\\) contains the loadings (see below)\n\n\n\n\nNotes\n\n\\(AA^T\\) is positive definite\n\nWhich means it‚Äôs symmetric\nWhich means it has real eigenvalues¬†and orthogonal eigenvectors\nWhich means the eigenvectors have covariances = 0\nWhich means the eigenvectors aren‚Äôt correlated.\n\nThe eigenvalues are eigenvector‚Äôs standard deviations which determines how much variance is explained by that PC.\nThe variance of a variable is the dot-product of itself and it‚Äôs transpose, \\(x_i \\cdot x^t_i\\)\nThe covariance between two variables, \\(x_i \\cdot x^t_j\\)\nIn step 3 equation, eigenvalues give the magnitude (length of vector) and eigenvectors the direction after being transformed by the covariance matrix.\nElements in a PC vector are called scores and elements in the V eigenvector are called loadings.\n\nThe loadings are the coefficients in the linear combination of variables that equals the PC vector\nLoadings range from -1 to 1\nVariables with high loadings (usually defined as .4 in absolute value or higher because this suggests at least 16% of the measured variable variance overlaps with the variance of the component) are most representative of the component\nThe sign of a loading (+ or -) indicates whether a variable and a principal component are positively or negatively correlated.\n\nScaling your design matrix variables just means your using a correlation matrix instead of a covariance matrix.\nPCA is sensitive to outliers. Variance explained will be inflated in the direction of the outlier\n\nGuessing this means components strongly influenced by variables with outlier values will have their variance-explained value inflated\n\nRow order of data matters as to which interpretation (latent) of component is valid from Principle Components and Penguins\n\nUsed data from palmerpenguins to create a ‚Äúpenguin size‚Äù variable from performing PCA on the data.\nIn one row order, high values of pc1 were associated with high body mass, but after scrambling the rows, high values of pc1 were associated with low body mass.\nHave to be careful when adding new data to the PCA-created feature. It might arbitrarily change the sign of the component and change the meaning of the feature.\n\nPCA doesn‚Äôt take the response variable into account (unsupervised). Therefore, the directions (eigenvectors) obtained may be well-suited for the predictor variables, but not necessarily optimal for predicting the response. It does often produce pretty good results though.\n\nAn alternative would be Partial Least Squares (PLS) which does take the response into account (supervised).\n\nIn practice, PLS reduces bias while potentially increasing the variance so the benefit vs PCA regression is usually a wash.\nCapable of handling multivariate regression\nPopular in chemometrics for analyzing spectra.\n\n\n\n\n\nPlots\n\nMisc\n\nNotes from: https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/multivariate/how-to/principal-components/interpret-the-results/all-statistics-and-graphs/\n(See bkmk) use broom::augment to add original data to pca output. Coloring the points by categorical variables can help with interpreting the components\nAlso see pkgs in notebook for visualization options\n\nScore\n\n\nClusters\n\nIf data follow a multivariate normal distribution then scores should be randomly distributed around zero\nIf there are clusters, then there may be multiple distributions present\n\nExtreme points (e.g.¬†point in bottom right) might be outliers and it might be worthwhile to investigate them further\n\nLoadings\n\n\nNeed to imagine an axis at (0,0). Don‚Äôt know why they don‚Äôt plot them with those axes.\nFinding the largest variable influences on a PC can used to interpret it‚Äôs meaning (think latent variable)\nArrows\n\nA (near) horizontal arrow (along the x-axis) describes that the feature contributes strongly toward PC1.\nA (near) vertical arrow (along the y-axis) describes that a feature contributes strongly towards PC2.\n\nValues\n\nLoadings range from -1 to 1\nThe termination coordinate of the line gives the loading values for that variable for both PCs\nLoadings (absolute magnitude) close to 0 indicate that the variable has little influence on that PC\nLarger the absolute value of the loading the greater the influence on that PC\nNegative values have negative influence on the latent variable that the PC represents (vice versa for positive values)\n\nAngle\n\nacute angles represent a positive correlation between those variables\nobtuse angles represent a negative correlation between those variables\n90 degree angles represent independence between those variables\n\nExample\n\nAge, Residence, and Employ have large influences on PC1 (interpretation: financial stability)\nCredit cards, Debt, and Education have large influences on PC2 (interpretation: credit history)\nSays, ‚ÄúAs the number credit cards increases, credit history (PC2 interpretation) becomes more negative.‚Äù\n\n\nBi-Plot\n\n\nCombination plot of the score and loading plot\nCan augment pca output (see top of section) with original data and color the scores by different categorical variables\n\nIf a categorical variable level is clustered around Education, you could say as Education rises, the more likely that that person is &lt;categorical level&gt;.\nIn turn, that categorical level would be either positively or negatively (depending on the loading sign) associated with that PC.\n\n\nInterpretation\n\nExample: Bluejays\n\nLoadings\n\n\nPC2 represents the difference between bill size and skull size\n\nLoadings together with components plot\n\n\nMale birds larger than female birds\n\nIf you look at the loadings plot, negative pc1 corresponds to larger size and the components plot shows males with negative PC1 values\n\nBoth sexes have large and short bills relative to their overall size\n\nMales and females both show values above and below 0 in PC2\nLarger bills but smaller bodies (+PC2) and larger bodies but smaller bills (-PC2)\n\n\nVariance Explained\n\n\nOverall bird size explains &gt; 50% of the variation in measurements\n\n\n\nExample: How much variation in a principal component can be explained by a categorical variable\n# Penguins dataset\n# pca_values is a prcomp() object\npca_points &lt;-¬†\n¬† # first convert the pca results to a tibble\n¬† as_tibble(pca_values$x) %&gt;%¬†\n¬† # now we'll add the penguins data\n¬† bind_cols(penguins)\n## # A tibble: 6 x 12\n##¬† ¬† PC1¬† ¬† PC2¬† ¬† PC3¬† ¬† PC4 species island bill_length_mm bill_depth_mm\n##¬† &lt;dbl&gt;¬† &lt;dbl&gt;¬† &lt;dbl&gt;¬† &lt;dbl&gt; &lt;fct&gt;¬† &lt;fct&gt;¬† ¬† ¬† ¬† ¬† &lt;dbl&gt;¬† ¬† ¬† ¬† &lt;dbl&gt;\n## 1 -1.85 -0.0320¬† 0.235¬† 0.528 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 39.1¬† ¬† ¬† ¬† ¬† 18.7\n## 2 -1.31¬† 0.443¬† 0.0274¬† 0.401 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 39.5¬† ¬† ¬† ¬† ¬† 17.4\n## 3 -1.37¬† 0.161¬† -0.189¬† -0.528 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 40.3¬† ¬† ¬† ¬† ¬† 18¬†\n## 4 -1.88¬† 0.0123¬† 0.628¬† -0.472 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 36.7¬† ¬† ¬† ¬† ¬† 19.3\n## 5 -1.92 -0.816¬† 0.700¬† -0.196 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 39.3¬† ¬† ¬† ¬† ¬† 20.6\n## 6 -1.77¬† 0.366¬† -0.0284¬† 0.505 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 38.9¬† ¬† ¬† ¬† ¬† 17.8\n## # ‚Ä¶ with 4 more variables: flipper_length_mm &lt;int&gt;, body_mass_g &lt;int&gt;,\n## #¬† sex &lt;fct&gt;, year &lt;int&gt;\n\npc1_mod &lt;-¬†\n¬† lm(PC1 ~ species, pca_points)\nsummary(pc1_mod)\n## Call:\n## lm(formula = PC1 ~ species, data = pca_points)\n##¬†\n## Residuals:\n##¬† ¬† Min¬† ¬† ¬† 1Q¬† Median¬† ¬† ¬† 3Q¬† ¬† Max¬†\n## -1.3011 -0.4011 -0.1096¬† 0.4624¬† 1.7714¬†\n##¬†\n## Coefficients:\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value Pr(&gt;|t|)¬† ¬†\n## (Intercept)¬† ¬† ¬† -1.45753¬† ¬† 0.04785¬† -30.46¬† &lt;2e-16 ***\n## speciesChinstrap¬† 1.06951¬† ¬† 0.08488¬† 12.60¬† &lt;2e-16 ***\n## speciesGentoo¬† ¬† 3.46748¬† ¬† 0.07140¬† 48.56¬† &lt;2e-16 ***\n## ---\n## Signif. codes:¬† 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##¬†\n## Residual standard error: 0.5782 on 330 degrees of freedom\n## Multiple R-squared:¬† 0.879,¬† Adjusted R-squared:¬† 0.8782¬†\n## F-statistic:¬† 1198 on 2 and 330 DF,¬† p-value: &lt; 2.2e-16\n\nFrom https://bayesbaes.github.io/2021/01/28/PCA-tutorial.html\nAdjusted R-squared: 0.8782\n\n\nCan be seen visually in this chart by looking at the points in relation to the x-axis where species is segregated pretty nicely.\n\n\n\n\n\nOutliers\n\nMahalanobis Distance (MD)\n\nThis method might be problematic. Supposedly outliers affect the covariance matrix which affects the PCA, which affects the scores, which affects theMahalanobis distance (MD). So the MD might be biased and not be accurate in determining outliers\n\nRobust forms of PCA (see section below) would be recommended if you suspect outliers in your data.\n\nDisplays the Mahalanobis distance (MD) for each observation and a reference line to identify outliers. The Mahalanobis distance is the distance between each data point and the centroid of multivariate space (the overall mean).\n\nOutliers determined by whether the Mahalanobis Distance is greater than the square root of the Chi-Square statistic where m is the number of variables and Œ± = 0.05\n\nNo outliers in the chart above as all MDs lower than the threshold at 4.4\n\n\nLeverage Points and Orthogonal Outliers\n\nNotes from https://towardsdatascience.com/multivariate-outlier-detection-in-high-dimensional-spectral-data-45878fd0ccb8\nTypes\n\nLeverage Points\n\ncharacterized by a high score distance\ngood leverage points also have short orthogonal distance and bad leverage points have long orthogonal distances\ngood leverage points have a positive effect\n\nOrthogonal Outliers\n\ncharacterized by a high orthogonal distance\n\n\nType determined by (see article for the math)\n\nScore DIstance (SD) - the distance an observation is from center of K-dimensional PCA subspace\nOrthogonal Distance (OD) -¬†the deviation ‚Äî i.e.¬†lack of fit ‚Äî of an observation from the k-dimensional PCA subspace\nOutliers are determined by Chi-Square test very similar to the¬†Mahalanobis Distance method (see above).\n\n\n\nIn the example shown, the red dots are data known to be measurement errors. Most of the red dots are captured in the orthogonal and bad sections but quite a few normal observation (blue) points too. So this method needs to be used as a guide and followed up upon when it flags points.\n\n\n\nHotelling‚Äôs T2 and SPE/DmodX (Complementary Tests)\n\n{{pca}}\nHotelling‚Äôs T2 works by computing the chi-square tests across the top n_components for which the p-values are returned that describe the likeliness of an outlier. This allows for ranking the outliers from strongest to weak.\nSPE/DmodX (distance to model) based on the mean and covariance of the first 2 PCs\n\n\n\n\nExtensions\n\nRobust PCA\n\nData with outliers and high dimensional data (p &gt;&gt; n) are not suitable for regular PCA where p is the number of variables.\nLow Dim methods (only valid when n &gt; 2p) that find robust (against outliers) estimates of the covariance matrix\n\nS-estimator, MM-estimator, (Fast)MCD-estimator, re-weighted MCD- (RMCD) estimator\n\nHigh Dim Methods\n\nRobust PCA by projection-pursuit (PP-PCA)\n\nfinds directions for eigenvectors that maximize a ‚Äúprojection index‚Äù instead of directions that maximize variance\n\nMAD or Qn-estimator is used a projection index\n\n\nSpherical PCA (SPCA)\n\nhandles outliers by projecting points onto a sphere instead of a line or plane\n\nalso uses MAD or Qn-estimator\n\n\nRobust PCA (ROBPCA)\n\ncombines projection index approach with low dim robust covariance estimation methods somehow\n\nRobust Sparse PCA (ROSPCA)\n\nsame but uses sparse pca\napplicable to both symmetrically distributed data and skewed data\n\n\n\nKernel PCA\n\nPackages: {kernlab}\nNonlinear data (notebook)\nPCA in a hypothetical (kernel trick), higher dimensional space\nWith more dimensions, data points become more separable.\nResults depend on type of kernel\n\nGaussian Kernel\n\nTuning parameter: sigma\n\n\n\n\n\n\nTidymodels\n\nRecipe step\n# if only using dummy vars, no sure if normalization is necessary\n# step_normalize(&lt;pca variables&gt;)\nstep_pca(starts_with(\"tf_\"), num_comp = tune())\n# don't forget to include num_comp in your tuning grid\nTaking a tidymodel‚Äôs recipe object and performing PCA\ntf_mat &lt;- recipe_obj %&gt;%\n¬† ¬† # normalizing tokenized indicators (?)\n¬† ¬† # since these are all dummy vars, not sure if a normalization step is necessary)\n¬† ¬† step_normalize(starts_with(\"tf_\")) %&gt;%\n¬† ¬† prep() %&gt;%\n¬† ¬† bake() %&gt;%\n¬† ¬† # only want to pca text features\n¬† ¬† select(starts_with(\"tf_\") %&gt;%\n¬† ¬† as.matrix()\n\ns &lt;- svd(tf_mat)\n# scree plot\ntidy(s, matrix = \"d\") %&gt;%\n¬† ¬† filter(PC &lt;= 50) %&gt;%\n¬† ¬† ggplot(aes(x = PC, y = percent)) +\n¬† ¬† geom_point()\n\nmatrix (tidy arg):\n\n‚Äúu‚Äù, ‚Äúsamples‚Äù, ‚Äúscores‚Äù, or ‚Äúx‚Äù: Returns info about the map from the original space to the pc space\n‚Äúv‚Äù, ‚Äúrotation‚Äù, ‚Äúloadings‚Äù, or ‚Äúvariables‚Äù: Returns information about the map from the pc space to the original space\n‚Äúd‚Äù, ‚Äúeigenvalues‚Äù, or ‚Äúpcs‚Äù: Returns information about the eigenvalues\n\n\nExample\nlibrary(tidymodels)¬†\nlibrary(workflowsets)¬†\nlibrary(tidyposterior)¬†\ndata(meats, package= \"modeldata\")¬†\n# Keep only the water outcome¬†\nmeats &lt;- select(meats, -fat, -protein)¬†\nset.seed(1)¬†\nmeat_split &lt;- initial_split(meats)¬†\nmeat_train &lt;- training(meat_split)¬†\nmeat_test &lt;- testing(meat_split)¬†\nset.seed(2)¬†\nmeat_folds &lt;- vfold_cv(meat_train, repeats = 3)\nbase_recipe &lt;-¬†\n¬† recipe(water ~ ., data = meat_train) %&gt;%¬†\n¬† step_zv(all_predictors()) %&gt;%¬†\n¬† step_YeoJohnson(all_predictors()) %&gt;%¬†\n¬† step_normalize(all_predictors())¬†\npca_recipe &lt;-¬†\n¬† base_recipe %&gt;%¬†\n¬† step_pca(all_predictors(), num_comp = tune())¬†\npca_kernel_recipe &lt;-¬†\n¬† base_recipe %&gt;%¬†\n¬† step_kpca_rbf(all_predictors(), num_comp = tune(), sigma = tune())",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-efa",
    "href": "qmd/feature-reduction.html#sec-feat-red-efa",
    "title": "Feature Reduction",
    "section": "Exploratory Factor Analysis (EFA)",
    "text": "Exploratory Factor Analysis (EFA)\n\nIdentifies a number of latent factors that explain correlations between observed variables\n\nFrequently employed in social sciences where the main interest lies in measuring and relating unobserved constructs such as emotions, attitudes, beliefs and behaviour.\nLatent variables, referred to also as factors, account for the dependencies among the observed variables, referred to also as items or indicators, in the sense that if the factors are held fixed, the observed variables would be independent.\nIn exploratory factor analysis the goal is the following: for a given set of observed variables x1, . . . , xp one wants to find a set of latent factors Œæ1, . . . , Œæk, fewer in number than the observed variables (k &lt; p), that contain essentially the same information.\nIn confirmatory factor analysis, the objective is to verify a social theory. Hence, a factor model is specifed in advance and its fit to the empirical data is tested.\n\nMisc\n\nPackages\n\n{psych} - factor analysis, item response theory, reliability analysis\n{factominer} - Multiple Factor Analysis (MFA}\n{fspe} - Model selection method for choosing number of factors\n\nUses the connection between model-implied correlation matrices and standardized regression coefficients to do model selection based on out-of-sample prediction errors\n\n\nTwo main approaches for analysing ordinal variables with factor models:\n\nUnderlying Response Variable (URV)\n\nThe ordinal variables are generated by underlying continuous variables partially observed through their ordinal counterparts. (also see Regression, Ordinal &gt;&gt; Cumulative Link Models (CLM))\n\nItem Response Theory (IRT)\n\nOrdinal indicators are treated as they are.\n\n\n\nMethods for selecting the right number of factors\n\nMisc\n\nIssue: more factors always improve the fit of the model\n\nParallel Analysis: analyze the patterns of eigenvalues of the correlation matrix\nModel Selection: likelihood ratio tests or information criteria\n\nComparison with PCA\n\nPCA is a technique for reducing the dimensionality of one‚Äôs data, whereas EFA is a technique for identifying and measuring variables that cannot be measured directly (i.e.¬†latent factor)\nWhen variables don‚Äôt have anything in common, EFA won‚Äôt find a well-defined underlying factor, but PCA will find a well-defined principal component that explains the maximal amount of variance in the data.\nDifferences in the results between PCA and EFA don‚Äôt tend to be obvious in practice. As the number of variables (&gt;40 variables) involved in the analysis grows, results from PCA and EFA become more and more similar.\nSimilarly calculated method to PCA, but FA is an analysis on a reduced correlation matrix, for which the ones in the diagonal have been replaced by squared multiple correlations (SMC)\n\nA SMC is the estimate of the variance that the underlying factor(s) explains in a given variable (aka communality).\n\nThe variability in measured variables in PCA causes the variance in the principal component. This is in contrast to EFA, where the latent factor is seen as causing the variability and pattern of correlations among measured variables\nAn eigenvalue decomposition of the full correlation matrix is done in PCA, yet for EFA, the eigenvalue decomposition is done on the reduced correlation matrix\nFactor Analysis is a latent variable measurement model\n\nThe causal relationship is flipped in FA as compared to PCA.\n\n\nF is the latent variable (instead of component in PCA), b is a weight (like loadings in PCA), Y is a predictor variable, and u is an error\n\nHere, b estimates how much F contributes to Y",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-autoenc",
    "href": "qmd/feature-reduction.html#sec-feat-red-autoenc",
    "title": "Feature Reduction",
    "section": "Autoencoders",
    "text": "Autoencoders\n\nUnsupervised neural networks that learn efficient coding from the input unlabelled data. They try to reconstruct the input data by minimizing the reconstruction loss\nMisc\n\nUndercomplete Autoencoder (AE) ‚Äî the most basic and widely used type, frequently referred to as an Autoencoder\nSparse Autoencoder (SAE) ‚Äî uses sparsity to create an information bottleneck\nDenoising Autoencoder (DAE) ‚Äî designed to remove noise from data or images\nVariational Autoencoder (VAE) ‚Äî encodes information onto a distribution, enabling us to use it for new data generation\n\nLayers\n\nEncoder: Mapping from Input space to lower dimension space\nDecoder: Reconstructing from lower dimension space to Output space\n\nProcess\n\n\nEncodes the input data (X) into another dimension (Z), and then reconstructs the output data (X‚Äô) using a decoder network\nThe encoded embedding (Z) is preferably lower in dimension compared to the input layer and contains all the efficient coding of the input layer\nOnce the reconstruction loss is minimized, the learned weights or embeddings, in the Encoder layer can be used as features in ML models and the Encoder layer can be used to generate embeddings on future data.\n\nSparse Autoencoder (SE)\n\n\nUses regularization\nDimension reduction in the center is achieved through deactivating neurons\nExample\n\nThe model consists of 5 layers: one input, three hidden and one output.\nInput and output layers contain 784 neurons each (the shape of our data, i.e number of columns), with the size of hidden layers reduced to 16 neurons each.\nWe will train the model over 50 epochs and plot a loss chart (see below).\nWe will separate the encoder part of the model and save it to our project directory. Note, if you are not planning to reuse the same model afterwards, you don‚Äôt need to keep a copy of it.",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-dmd",
    "href": "qmd/feature-reduction.html#sec-feat-red-dmd",
    "title": "Feature Reduction",
    "section": "Dynamic Mode Decomposition (DMD)",
    "text": "Dynamic Mode Decomposition (DMD)\n\nCombines PCA and fourier transform\nSupposed to handle time series better than PCA\n{{pydmd}}",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html",
    "href": "qmd/forecasting-statistical.html",
    "title": "Statistical",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-misc",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-misc",
    "title": "Statistical",
    "section": "",
    "text": "Packages\n\nCRAN Task View\n\nFor intermittent data, see Logistics &gt;&gt; Demand Planning &gt;&gt; Intermittent Demand\nLet the context of the decision making process determine the units of the forecast\n\ni.e.¬†Don‚Äôt forecast on a hourly scale just because you can.\n\nWhat can be forecast depends on the predictability of the event:\n\nHow well we understand the factors that contribute to it;\n\nWe have a good idea of the contributing factors: electricity demand is driven largely by temperatures, with smaller effects for calendar variation such as holidays, and economic conditions.\n\nHow much data is available;\n\nThere is usually several years of data on electricity demand available, and many decades of data on weather conditions.\n\nHow similar the future is to the past;\n\nFor short-term forecasting (up to a few weeks), it is safe to assume that demand behaviour will be similar to what has been seen in the past.\n\nWhether the forecasts can affect the thing we are trying to forecast.\n\nFor most residential users, the price of electricity is not dependent on demand, and so the demand forecasts have little or no effect on consumer behaviour.\n\n\nStarting a project\n\nUnderstand the dgp through eda and talking to domain experts\n\nHow are sales generated? (e.g.¬†online, brick and mortar,‚Ä¶)\n\nWhat is the client currently using to forecast?\n\nModel that you need to beat\nWhere does it fail?\n\nBiased? underfitting or overfitting somewhere\nMissing seasonality?\n\n\nWhat is the loss function?\n\nCarrying this many items in inventory results in this cost\nIf we‚Äôre out of stock and lose this many sales, how much does this cost\n\nWhat does the client really want?\n\nHow is success measured\n\n\nFable models produce different results with NAs in the time series\n\nIn rolling cfr project, steinmetz‚Äôs manually calc‚Äôd rolling 7-day means and his lagged vars had NAs, models using data with and without NAs had different score\n\nIt is helpful to keep track of and understand what our forecast bias has historically been.¬† Even where we are fortunate enough to show a history of bias in both directions.\nForecasting shocks is difficult for an algorithm\n\nIt can better to smooth out (expected) shocks (Christmas) in the training data and then add an adjustment to the predictions during the dates of the shocks.\nThe smoothed out data will help the algorithm produce more accurate predictions for days when there isn‚Äôt an expected shock.\nExamples of shocks that may need training data to have manual adjustments and not be smoothed by an algorithm\n\nOne-Time spikes due to abnormal weather conditions\nOne-Off promotions\nA sustained marketing campaign that is indistinguishable from organic growth.\n\n\nIntermittent(or sporadic) time series (lotsa zeros).\n\n{thief} has the latest methods while {tsintermittent} has older methods\n\nBenchmark models\n\nNaive\n28-day moving average (i.e.¬†4 week MA)",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-terms",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-terms",
    "title": "Statistical",
    "section": "Terms",
    "text": "Terms\n\nWeak stationarity (commonly referred to as just stationarity)(aka covariance stationary) - Implies that the mean and the variance of the time series are finite and do not change with time.\nCointegration - \\(x_t\\) and \\(y_t\\) are cointegrated if \\(x_t\\) and \\(y_t\\) are \\(I(1)\\) series and there exists a \\(\\beta\\) such that \\(z_t = x_t - \\beta y_t\\) is an \\(I(0)\\) series (i.e.¬†stationary).\n\nImportant for understanding stochastic or deterministic trends.\nThe differences in the means of the set of cointegrated series remain constant over time, without offering an indication of directionality\nMight have low correlation, and highly correlated series might not be cointegrated at all.\nCan use Error Correction Model (ECM) with differenced data and inserting a error correction term (residuals from a OLS regression)\n\nStochastic - No value of a variable is known with certainty. Some values may be more likely than others (probabilistic). Variable gets mapped onto a distribution.",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-preproc",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-preproc",
    "title": "Statistical",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nFilling in gaps\n\nBi-directional forecast method from AutoML for time series: advanced approaches with FEDOT framework\n\nSteps\n\nSmooth series prior to the gap\n\nThey used a ‚ÄúGaussian filter w/sigma = 2‚Äù (not sure what that is)\n\nCreate lagged features of the smoothed series\nForecast using ridge regression where h = length of gap\nRepeat in the opposite direction using the series after the gap\nUse the average of the forecasts to fill the gap in the series.\n\n\n\nLog before differencing (SO post)\nDetrend or Difference\n\n(The goal is to get a stationary series, so if one doesn‚Äôt work try the other.)\nDifferencing (for unit root processes)(stochastic trend)\n\nif the process requires differencing to be made stationary, then it is called difference stationary and possesses one or more unit roots.\n\nSometimes see charts of roots and a unit circle. I read this in an article about VAR models ‚Äúprocess is stationary if all the roots \\(z_1, \\ldots , z_n\\) of the determinant \\(\\det(\\psi(z))\\), or \\(\\det(I ‚àí Bz) = 0\\), lie outside of the unit circle.‚Äù\n\nOne advantage of differencing over detrending to remove trend is that no parameters are estimated in the differencing operation.\nOne disadvantage, however, is that differencing does not yield an estimate of the stationary process\nIf the goal is to coerce the data to stationarity, then differencing may be more appropriate.\nDifferencing is also a viable tool if the trend is fixed\n\nRandom Walking looking series should be differenced and not detrended.\n\nBackshift operator notation:\n\nIn general: \\(\\nabla^d = (1 ‚àí B)^d\\)\n\nWhere \\(d\\) is the order of differencing\nFractional differencing is when \\(0 \\lt d \\lt 1\\)\n\nWhen \\(0 \\lt d \\lt 0.5\\), the series is classified as a long term memory series (often used for environmental time series arising in hydrology)\n\nIf \\(d\\) is negative, then its called forward-shift differencing\n\nExamples:\n\nIdentities:\n\\[\nB y_t = y_{t-1} \\\\\nB^2 y_t = y_{t-2}\n\\]\nSeasonal Difference:\n\\[\n(1 - B)(1 - B^m) y_t = (1 - B - B^m + B^{m + 1})y_t = y_t - y_{t-1} - y_{t-m} + y_{t-m-1}\n\\]\nARIMA : AR(p)I(d) = MA(q)\n\\[\n(1-\\phi_1 B - \\cdots - \\phi_p B^p)(1-B)^d y_t = c+(1+\\theta_1 B + \\cdots + \\theta_q B^q)\\epsilon_t\n\\]\nARIMA(1,1,1)(1,1,1)4 for quarterly data (m = 4)\n\\[\n(1-\\phi_1 B)(1-\\Phi B^4)(1-B)(1-B^4)y_t = (1+\\theta_1 B)(1+\\Theta B^4)\\epsilon_t\n\\]\n\n\n\nDetrending (for trend-stationary processes)(deterministic trend)\n\nIt is possible for a time series to be non-stationary, yet have no unit root and be trend-stationary\n\na trend-stationary process is a stochastic process from which an underlying trend (function solely of time) can be removed (detrended), leaving a stationary process.\n\nIf an estimate of the stationary process is essential, then detrending may be more appropriate.\nHow is this back-transformed after forecasting?\n\nmaybe look at ‚Äúforecasting with STL‚Äù section in fpp2\n\n\nIn both unit root and trend-stationary processes, the mean can be growing or decreasing over time; however, in the presence of a shock, trend-stationary processes are mean-reverting (i.e.¬†transitory, the time series will converge again towards the growing mean, which was not affected by the shock) while unit-root processes have a permanent impact on the mean (i.e.¬†no convergence over time).\nTesting\n\nKPSS test: H0 = Trend-Stationary, Ha = Unit Root.\n\nurca::ur_kpss the H0 is stationarity\ntseries::kpss.test(res, null = \"Trend\") where H0 is ‚Äútrend-stationarity‚Äù\n\nDickey-Fuller tests: H0 = Unit Root, Ha = Stationary or Trend-Stationary depending on version\nKPSS-type tests are intended to complement unit root tests, such as the Dickey‚ÄìFuller tests. By testing both the unit root hypothesis and the stationarity hypothesis, one can distinguish series that appear to be stationary, series that appear to have a unit root, and series for which the data (or the tests) are not sufficiently informative to be sure whether they are stationary.\n\nSteps:\n\nADF:\n\nIf H0 rejected. The trend (if any) can be represented by a deterministic linear trend.\nIf H0 is not rejected then we apply the KPSS test.\n\nKPSS :\n\nIf H0 rejected then we conclude that there is a unit root and work with the first differences of the data.\n\nUpon the first differences of the series we can test the significance of other regressors or choose an ARMA model.\n\nIf H0 is not rejected then data doesn‚Äôt contain enough information. In this case it may be safer to work with the first differences of the series.\n\n\nSteps when using an ARIMA:\n\nSuppose the series is not trending\n\nIf the ADF test (without trend) rejects, then apply model directly\nIf the ADF test (without trend) does not reject, then model after taking difference (maybe several times)\n\nSuppose the series is trending\n\nIf the ADF test (with trend) rejects, then apply model after detrending the series\nIf the ADF test (with trend) does not reject, then apply model after taking difference (maybe several times)",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-diag",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-diag",
    "title": "Statistical",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nTesting for significant difference between model forecasts\n\nNemenyi test\n\nsutils::nemenyi\n\nMCB\n\ngreybox::rmcb",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-alg",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-alg",
    "title": "Statistical",
    "section": "Algorithms",
    "text": "Algorithms\n\nMisc\n\nAuto Arima, ETS and Theta are general-purpose methods particularly well-suited for monthly, quarterly and annual data\nTBATS and STL will also handle multiple seasonalities such as arise in daily and weekly data.\nWhen to try nonlinear models (see Forecasting, Nonlinear)\n\nLinear prediction methods (e.g.¬†ARIMA) don‚Äôt produce adequate predictions\nChaotic nature of the time series is obvious (e.g.¬†frequent, unexplainable shocks that can‚Äôt be explained by noise)\n\n\n\n\nRegression (including ARIMA)\n\nMisc\n\nDouble check auto_arima, for the parameters, (p, d, q), one should pick q to be at least p (link)\nSometimes the error terms are called random shocks.\nIf using lm and there are NAs, make sure to use na.action = NULL else they get removed and therefore dates between variables won‚Äôt match-up. See lm doc for further details on best practices.\nARIMA models make h-step out predictions by iterating 1-step forward predictions and feeding the intermediate predictions in as if they were actual observations (0 are used for the errors)\nPolynomial Autoregression (AR) models exponentiate the lags. So the design matrix includes the lags and the exponentiated series.\n\nIf the polynomial is order 3, then order 2 is also included. So, now, the design matrix would be the lags, the square of each lag, and the cube of each lag\nExample:\nlibrary(dplyr); library(timetk)\n# tbl w/polynomial design matrix of order 3\n# value is the ts values\npoly_tbl &lt;- group_tbl %&gt;%\n¬† tk_augment_lags(.value = value, .lags = 1:4) %&gt;%\n¬† mutate(across(contains(\"lag\"), \n¬†        .fns = list(~.x^2, ~.x^3), \n¬†        .names = \"{.col}_{ifelse(.fn==1, 'quad','cube')}\"))\n\n.fn is the item number in the .fns list.\nSquared lag 2 will have the name ‚Äúvalue_lag2_quad‚Äù\n\n\n\nTypes\n\nAR: single variable with autoregressive dependent variable terms\nARMA: same as AR but errors models as a moving average\nARIMA: same as ARMA but with differencing the timeseries\nSARIMA: same as ARIMA but also with seasonal P, D, Q terms\nARMAX: same as ARMA but with additional exogenous predictors\nDynamic Regression: OLS regression with modeled (usually arima) errors\n\nOLS vs ARIMA\n\nJohn Mount\n\nThe fear in using standard regression for time series problems is that the error terms are likely correlated.\n\nSo one can no longer appeal to the Gauss Markov Theorem (i.e.¬†OLS is BLUE) to be assured of good out of sample performance (link)\n\n\nRyer and Chan regarding Dynamic Regression vs OLS\n\n‚ÄúRegression (with arima errors) coefficient estimate on Price is similar to that from the OLS regression fit earlier, but the standard error of the estimate is about 10% lower than that from the simple OLS regression. This illustrates the general result that the simple OLS estimator is consistent but the associated standard error is generally not trustworthy‚Äù\n\nHyndman\n\n‚ÄúThe forecasts from a regression model with autocorrelated errors are still unbiased, and so are not ‚Äúwrong,‚Äù but they will usually have larger prediction intervals than they need to. Therefore we should always look at an ACF plot of the residuals.‚Äù\nThe estimated coefficients are no longer the best estimates, as some information has been ignored in the calculation;\n\nMeaning modeling the errors to take into account the autocorrelation\n\nAny statistical tests associated with the model (e.g., t-tests on the coefficients) will be incorrect.\n\nAffected by the bloated std errors\n\nThe AICc values of the fitted models are no longer a good guide as to which is the best model for forecasting.\nIn most cases, the p-values associated with the coefficients will be too small, and so some predictor variables will appear to be important when they are not. This is known as ‚Äúspurious regression.‚Äù\n\n\nTrend (\\(\\beta\\) \\(t\\)) is modeled by setting the variable \\(t\\) to just an index variable (i.e.¬†\\(t = 1, \\ldots, T\\)). Modeling quadratic trend would be adding in \\(t^2\\) to the model formula.\n\nHyndman suggests that using splines is a better approach than using t2\nFrom Steinmitz‚Äôs CFR article\n\nInstead of trend (like in {forecast}), he‚Äôs using poly(date, 2) to include a quadratic trend\n\n\nR2 and Adjusted-R2\n\nAppropriate for time series (i.e.¬†estimate of the population R2), as long as the data are stationary and weakly dependent\n\ni.e.¬†The variances of both the errors and the dependent variable do not change over time.\ni.e.¬†If \\(y_t\\) has a unit root (Integrated of order 1, I(1)) (needs differenced)\n\n\nInterpretation of coefficients\n\\[\ny_t = \\alpha + \\beta_0 x_t + \\beta_1 x_{t-1} + \\cdots + \\beta_s x_{t-s} + \\cdots + \\beta_q x_{t-q}\n\\]\n\nIf \\(x\\) increases by one unit today, the change in \\(y\\) will be \\(\\beta_0+\\beta_1+...+\\beta_s\\) after \\(s\\) periods; This quantity is called the \\(s\\)-period interim multiplier. The total multiplier is equal to the sum of all \\(\\beta\\) s in the model.\n\nResiduals\n\nTypes\n\n‚ÄúRegression‚Äù is for the main model\n\nOriginal data minus the effect of the regression variables\n\n‚ÄúInnovation‚Äù is for the error model\n\nDefault arg\nHyndman uses these for dynamic regression residual tests\n\n\nAutocorrelation tests\n\nFailing the test does not necessarily mean that (a) the model produces poor forecasts; or (b) that the prediction intervals are inaccurate. It suggests that there is a little more information in the data than is captured in the model. But it might not matter much.\nBreusch-Godfrey test designed for pure regression or straight AR model\n\nDoes handle models with lagged dependent vars as predictors\nLM (lagrange multiplier) test\nforecast::checkresiduals can calculate it and display it, but you don‚Äôt have access to the values programmatically\n\nDefaults for lag is \\(\\min(10,n/5)\\) for nonseasonal and \\(\\min(2m, n/5)\\) for seasonal where the frequency is seasonality, m\nlag &lt;- ifelse(freq &gt; 1, 2 * freq, 10)\nlag &lt;- min(lag, round(length(residuals)/5))\nlag &lt;- max(df+3, lag)\n\n{lmtest} and {DescTools} (active) packages have the function that forecast uses but only takes lm objects\n\nDurbin-Watson designed for pure regression\n\nError term can‚Äôt be correlated with predictor to use this test\n\nSo no lagged dependent variables can be used as predictors\nThere is an durbin alternate test mentioned in stata literature that can do lagged variables but I haven‚Äôt seen a R version that specifies that‚Äôs the version it is.\n\n{lmtest} and {DescTools} takes a lm object and has a small sample size correction available\n{car::durbinWatsonTest} takes a lm object or residual vector.\n\nOnly lm returns p-value. Residual vector returns DW statistic\n\np-values \\(\\lt 0.05\\) \\(\\rightarrow\\) Autocorrelation present\nDW statistic guide (\\(0 \\lt \\text{DW} \\lt 4\\))\n\nAround 2 \\(\\rightarrow\\) No Autocorrelation\nSignifcantly \\(\\lt 2\\) \\(\\rightarrow\\) Positive Correlation\n\nSaw values \\(\\lt 1\\) have p-values = 0\n\nSignificantly \\(\\gt 2\\) \\(\\rightarrow\\) Negative Correlation\n\n\nLjung-Box\n\nFor dynamic regression, arima, ets, etc.\n\nThere‚Äôs a SO post that shows this shouldn‚Äôt be used for straight regression\n\nFor straight AR models, the comments show it should be fine as long as lags \\(\\gt\\) model [df]{arg-text} (see below)\n\n\nTest is whether a group of lagged residuals has significant autocorrelation, so an acf of the residuals might show individual spikes but the group as a whole may not have significant autocorrelation\n\nIf you see a spike in the residuals, may be interesting to include that lag number in the group of lags and see if significance of the group changes\n\n{feasts::ljung_box}\n\nRequires numeric residuals vector, model degrees of freedom, number of lags to check\n\nThe model df is number of variables used in the regression + intercept + p + q (of ARIMA error model)\n\ne.g.¬†Model with predictors: trend + cases and an error model: arima (2,1,1) had df = 2 (predictors: trend, cases) + 1 (intercept) + 2 (p) + 1 (q) = 6 d.f.\ndof &lt;- length(fit$coef)\n\nSee Breusch-Godfrey section for number of lags to use\n\n\np-values \\(\\lt 0.05\\) \\(\\rightarrow\\) autocorrelation present\n\n\n\nSpectral analysis takes the approach of specifying a time series as a function of trigonometric components (i.e.¬†Regression with fourier terms)\n\nA smoothed version of the periodogram, called a spectral density, can also be constructed and is generally preferred to the periodogram.\n\n\n\n\nRandom Walk\n\nA process integrated to order 1, (an I(1) process) is one where its rate of change is stationary. Brownian motion is a canonical I(1) process because its rate of change is Gaussian white noise, which is stationary. But the random walk itself is not stationary. So the \\(t+1\\) value of a random walk is just the value at \\(t\\) plus a number sampled from some bell curve.\nCharacteristics\n\nLong periods of apparent trends up or down\nSudden and unpredictable changes in direction\n\nA special case of an autoregressive model\n\\[\ny_t = c + \\phi_1 y_{t-1} + \\cdots + \\phi_p y_{t-p} + \\epsilon_t\n\\]\n\nWhere \\(c=0\\), \\(p=1\\), \\(\\phi = 1\\), and \\(\\epsilon \\sim \\mathcal {N}(0, s)\\)\n\nDrift\n\n\n\n\n\n\n\n\nFeature\nRandom Walk without Drift\nRandom Walk with Drift\n\n\n\n\nSteps\nPurely random, equal probability left/right\nBiased, one direction slightly more likely\n\n\nChange in value\nAverage change is zero\nAverage change includes a constant drift\n\n\nPath\nZig-zag around starting point\nZig-zag with upward/downward trend\n\n\nMean\nStays roughly the same\nIncreases/decreases over time depending on drift\n\n\nVariance\nIncreases with time\nIncreases with time\n\n\nStationarity\nNon-stationary\nNon-stationary\n\n\n\nExamples with and without drift\n\n\n\n\n\n\nProphet\n\nThe basic methodology is an iterative curve-matching routine, where Prophet will then train your data on a bigger period, then predict again and this will repeat until the end point is reached.\nThe development team of Prophet claim that its strengths are:\n\nWorking with high-frequency data (hourly, daily, or weekly) with multi-seasonality, such as hour of day, day of week and time of year;\nSpecial events and bank holidays that are not fixed in the year;\nAllowing for the presence of a reasonable number of missing values or large outliers;\nAccounting for changes in the historical trends and non-linear growth curves in a dataset.\n\nFurther advantages include the ability to train from a moderate sized dataset, without the need for specialist commercial software, and fast start up times for development.\nDisadvantages\n\nNo autoregressive (i.e.¬†lags of target series) features since it‚Äôs a curve-fitting algorithm\n\nTime series decomposition by prophet:\n\n\\(g(t)\\): Logistic or linear growth trend with optional linear splines (linear in the exponent for the logistic growth). The library calls the knots ‚Äúchange points.‚Äù\n\\(s(t)\\): Sine and cosine (i.e.¬†Fourier series) for seasonal terms.\n\\(h(t)\\): Gaussian functions (bell curves) for holiday effects (instead of dummies, to make the effect smoother).\n\n\n\n\nKalman Filter\n\nMisc\n\nNotes from How a Kalman filter works, in pictures\nIf a dynamic system is linear and with Gaussian noise (inaccurate measurements, etc.), the optimal estimator of the hidden states is the Kalman Filter\n\nFor nonlinear systems, we use the extended Kalman filter, which works by simply linearizing the predictions and measurements about their mean. (I may do a second write-up on the EKF in the future)\nGood for predictions where the measurements of the outcome variable over time can be noisy\n\nAssumptions\n\nGaussian noise\nMarkov property\n\nIf you know \\(x_{t‚àí1}\\), then knowledge of \\(x_{t‚àí2},\\ldots , x_0\\) doesn‚Äôt give any more information about xt (i.e.¬†not much autocorrelation if at all)\n\n\ntl;dr\n\nA predicted value from a physically-determined autoregression-type equation with 1 lag that gets adjusted for measurement error\n\nAdvantages\n\nLight on memory (they don‚Äôt need to keep any history other than the previous state)\nVery fast, making them well suited for real time problems and embedded systems\n\nUse cases\n\nEngineering: common for reducing noise from sensor signals (i.e.¬†smoothing out measurements)\nDetection-based object tracking (computer vision)\n\n\n\n\nFirst set of equations\n\n\nNotes\n\nThis set of equations deals physical part of the system. It‚Äôs kinda how we typically forecast.\n\nThe \\(\\hat x_k\\) equation is pretty much like a typical auto-regression plus explanatory variables except for the F matrix which may require knowledge of system dynamics\n\nWiki shows a term, \\(w_k\\), added to the end of the \\(\\hat x_k\\) equation. \\(w_k\\) is the process noise and is assumed to be drawn from a zero mean multivariate normal distribution,\n\nThe new best estimate is a prediction made from previous best estimate, plus a correction for known external influences.\n\n\\(\\hat x_k\\): The step-ahead predicted ‚Äústate‚Äù; \\(\\hat x_{k-1}\\) is the current ‚Äústate‚Äù\n\\(u_k\\) (‚Äúcontrol‚Äù vector): An explanatory variable(s)\n\\(F_k\\) (‚Äúprediction‚Äù matrix) and \\(B_k\\) (‚Äúcontrol‚Äù matrix) are transformation matrices\n\n\\(F_k\\) was based on one of Galileo‚Äôs equations of motion in the example so this might be very context specific\nMight need to based on substantial knowledge of the system to create a system of linear equations (i.e.¬†\\(F_k\\) matrix) that can be used to model the it.\n\n\nAnd the new uncertainty is predicted from the old uncertainty, with some additional uncertainty from the environment.\n\n\\(P_k\\) and \\(P_{k-1}\\) are variance/covariance matrices for the step-ahead predicted state and current state respectively\n\\(Q_k\\) is the uncertainty term for the variance/covariance matrix of the predicted state distribution\n\n\n\n\nSecond Set of Equations\n¬†\n\nNotes\n\nThese equations refine the prediction of the first set of equations by taking into account various sources of measurement error in the observed outcome variable\nThe equations do this by finding the intersection, which is itself a distribution, of the transformed prediction distribution, \\(Œº_0\\), and the measurement distribution, \\(Œº_1\\).\n\n\nThis mean, \\(\\mu'\\), of this intersection distribution is the predicted value that most likely to be the true value\n\n\n\\(H_k\\) is a transformation matrix that maps the predicted state (result of the first set of equations), \\(\\hat x_k\\) , to the measurement space\n\nWhere \\(H_k \\cdot \\hat x_k\\) is the expected measurement (pink area) (i.e.¬†Mean of the distribution of transformed prediction)\n\n\\(\\vec z_k\\) is the mean of the measurement distribution (green area)\n\\(\\hat x_k'\\) is the intersection of the transformed prediction distribution and the measurement distribution (i.e.¬†the predicted state thats most likely to true)\n\\(R_k\\) is the uncertainty term for variance/covariance matrix for the measurement distribution\n\\(K'\\) is called the Kalman Gain\n\nDidn‚Äôt read anything interpretative about the value. Just seems to a mathematical construct that‚Äôs part of the derivation.\nIn the derivation, it starts out as the ratio of the measurement covariance matrix to the sum of the measurement variance covariance matrix and the transformed prediction variance covariance matrix\n\n\n\n\n\nProcess\n\n\n\nHyperparameters\n\n\\(Q\\) is the process noise covariance\n\nControls how sensitive the model will be to process noise.\n\n\\(R\\) is the measurement noise variance\n\nControls how quickly the model adapts to changes in the hidden state.\n\n\nGuessing ‚Äústd‚Äù is the default value?\n\n\n\n\n\n\nExponential Smoothing\n\nThe general idea is that future values are a weighted average of past values, with the weights decaying exponentially as we go back in time\nMethods\n\nSimple Exponential Smoothing\ndouble Exponential Smoothing or Holt‚Äôs Method (for time series with a trend)\nTriple Exponential Smoothing or Holt-Winter‚Äôs method (for time series with a trend and sesaonality)\n\n\n\n\nTBATS\n\nTrigonometric seasonality, Box-Cox transformation, ARMA errors, Trend, and Seasonal components\nCan treat non-linear data, solve the autocorrelation problem in residuals since it uses an ARMA model, and it can take into account multiple seasonal periods\nRepresents each seasonal period as a trigonometric representation based on Fourier series. This allows the model to fit large seasonal periods and non-integer seasonal periods",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-intv",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-intv",
    "title": "Statistical",
    "section": "Interval Forecasting",
    "text": "Interval Forecasting\n\nNotes from Video: ISF 2021 Keynote\nInterval data is commonly analyzed by modeling the range (difference between interval points)\n\nRange data doesn‚Äôt provide information about the variation of the mean (aka level) over time.\nRange only provides information about the boundaries, where interval analysis provides information about the boundary and the interior of the interval.\n\nProvides more information than point forecasts.\nData examples:\n\nDaily Temperature, Stock Prices: Each day a high and low values are recorded\nStock Price Volatility, Bid-Ask spread use hi-lo value differences\nIntra-House Inequality: difference between wife and husband earnings\nUrban-Rural income gap\nInterval-Valued Output Growth Rate: China reports it‚Äôs targeted growth rate as a range now.\nDiastolic and Systolic blood pressure\n\nOthers: Blood Lipid, White Blood Cell Count, Hemoglobin\n\n\nExamples where (generalized) intervals can be modeled instead of differences:\n\nStock Volatility\n\nGARCH models often used to model volitility but Conditional Autoregressive Range (CARR) gives better forecasts\n\nBecause GARCH model is only based on the closing price but the CARR model uses the range (difference).\n\nDynamic Interval Modeling\n\nUse Autoregressive Interval (ARI) model to estimate the parameters using an interval time series (not the range)\nThen take the forecasted left and right values of the interval to forecast the volatility range in a CARR model\nThe extra information of the interval data over time (instead of a daily range) yields a more efficient estimation of the parameters\n\n\nCapital Asset Pricing Model (CAPM)\n\nAlso see Finance, Valuation &gt;&gt; Cost of Capital &gt;&gt; WACC &gt;&gt; Cost of Equity\nStandard Equation\n\\[\nR_t - R_{ft} = \\alpha + Œ≤(R_{mt} - R_{ft}) + \\epsilon_t\n\\]\n\n\\(R_t\\): Return of Certain Portfolio\n\\(R_{ft}\\): Risk-Free Interest Fate\n\\(R_{mt}\\): Return of Market Portfolio\n\\(R_t - R_{ft}\\): Asset Risk Premium\n\nInterval-based version\n\\[\nY_t = (\\alpha_0 + \\beta_0I_0) + \\beta X_t + u_t\n\\]\n\n\\(I_0 = [-0.5, 0.5]\\)\n\\(Y_t = [R_{ft}, R_t]\\)\n\\(X_t = [R_{ft}, R_{mt}]\\)\nThe \\(R_t - R_{ft}\\) can then be calculated by taking the difference of the interval bounds of the interval-based predictions\n\n\n\nModel the center of the interval and the range in a bi-variate VAR model (doesn‚Äôt use all points in the interval data)\n\nBi-variate Nonlinear Autoregressive Model for center and range\n\nHas an indicator variable that captures nonlinearity of interval data\n\nSpace-time autoregressive model\n\nAutoregressive Conditional Interval model\n\nThe interval version of an ARMA model\n\ndepends on lags and lagged residuals\n\nACI(p,q):\n\n\\[\nY_t = (\\alpha_0 + \\beta_0 I_0) + \\sum_{j=`}^p \\beta_jY_{t-j} + \\sum_{j=1}^p \\gamma_ju_{t-j} + u_t\n\\]\n\n\\(\\alpha_0\\), \\(\\beta_0\\), \\(\\beta_j\\), \\(\\gamma_j\\) are unknown scalar parameters\n\\(I_0 = [-\\frac{1}{2},\\; \\frac{1}{2}]\\) is a unit interval\n\\(\\alpha_0 + \\beta_0I_0 = [\\frac{\\alpha_0 - \\beta_0}{2},\\: \\frac{\\alpha_0 + \\beta_0}{2}]\\) is a constant interval intercept\n\\(u_t\\) is the interval residuals that satisfies \\(\\mathbb{E}(u_t\\;|\\;I_{t-1}) = [0,0]\\)\n\\(Y_t\\) is a random interval variable\n\nObjective function that gets minimized is called \\(D_k\\) distance\n\n\\(D^2_K [u_t(\\theta),0]\\)\n\n\\(u_t(\\theta)\\) is the interval residuals\n\\(K\\) refers to some kind of kernel function\n\nIt‚Äôs a wacky quadratic with constants a,b,c\nMeasures the distance between all pairs of points\n\n\nThe minimization is a two-stage process\n\nFinds the optimal kernel, \\(K\\), then uses it to minimize the residuals to estimate the parameters\n\n\n\nThreshold Autoregressive Interval (TARI)\n\nNonlinear ACI model and interval version of TAR(p) model (¬Ø\\_(„ÉÑ)_/¬Ø)\n2-Procedure Model\n\nBasically 2 autoregressive equations with an \\(i_1u_t\\) or \\(i_2u_t\\) added on to the end.\nThe interval series, \\(Y_t\\) ,follows one of the equations based on threshold variable \\(q_t\\) is less than or equal to a threshold parameter, \\(\\gamma\\) or greater than.\n\nEstimation is similar to ACI model\nFor more details, need to research what a TAR model (Terasvirta, Tjostheim, and Granger 2010) is",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html",
    "href": "qmd/networks-knowledge-graphs.html",
    "title": "Knowledge Graphs",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Networks",
      "Knowledge Graphs"
    ]
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#sec-net-kg-misc",
    "href": "qmd/networks-knowledge-graphs.html#sec-net-kg-misc",
    "title": "Knowledge Graphs",
    "section": "",
    "text": "Notes from:\n\nWhat is a knowledge graph?\nHow to Convert Any Text Into a Graph of Concepts\n\nTools\n\nkgl - Knowledge Graph Query Language\n\nInterface and sytax to query knowledge graph data\n\n\nUse Cases\n\nCalculate the centralities for any node, to understand how important a concept (node) is to the body of work\nAnalyze connected and disconnected sets of concepts, or calculate communities of concepts for a deep understanding of the subject matter.\nUsed to implement Graph Retrieval Augmented Generation (GRAG or GAG). Can give much better results than RAG when querying an LLM about documents.\n\nRetrieving the context that is the most relevant for the query with a simple semantic similarity search is not always effective. Especially, when the query does not provide enough context about its true intent, or when the context is fragments across a large corpus of text.\n\nRetail: Knowledge graphs have been for up-sell and cross-sell strategies, recommending products based on individual purchase behavior and popular purchase trends across demographic groups.\nEntertainment: Knowledge graphs are also leveraged for artificial intelligence (AI) based recommendation engines for content platforms, like Netflix, SEO, or social media. Based on click and other online engagement behaviors, these providers recommend new content for users to read or watch.\nFinance: This technology has also been used for know-your-customer (KYC) and anti-money laundering initiatives within the finance industry. They assist in financial crime prevention and investigation, allowing banking institutions to understand the flow of money across their clientele and identify noncompliant customers.\nHealthcare: Knowledge graphs are also benefiting the healthcare industry by organizing and categorizing relationships within medical research. This information assists providers by validating diagnoses and identifying treatment plans based on individual needs.",
    "crumbs": [
      "Networks",
      "Knowledge Graphs"
    ]
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#sec-net-kg-terms",
    "href": "qmd/networks-knowledge-graphs.html#sec-net-kg-terms",
    "title": "Knowledge Graphs",
    "section": "Terms",
    "text": "Terms\nKnowledge Graph - Also known as a semantic network, represents a network of real-world entities ‚Äî i.e.¬†objects, events, situations, or concepts ‚Äî and illustrates the relationship between them. Each node represents a concept and each edge is a relationship between a pair of such concepts. This information is usually stored in a graph database and visualized as a graph structure, prompting the term knowledge ‚Äúgraph.‚Äù",
    "crumbs": [
      "Networks",
      "Knowledge Graphs"
    ]
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#sec-net-kg-proc",
    "href": "qmd/networks-knowledge-graphs.html#sec-net-kg-proc",
    "title": "Knowledge Graphs",
    "section": "Process",
    "text": "Process\n\n\nCorpus Example:\nMary had a little lamb,\nYou‚Äôve heard this tale before;\nBut did you know she passed her plate,\nAnd had a little more!\nSteps\n\nSplit the corpus of text into chunks. Assign a chunk_id to each of these chunks.\nFor every text chunk, extract concepts and their semantic relationships using a LLM. This relation is assigned a weight of W1. There can be multiple relationships between the same pair of concepts. Every such relation is an edge between a pair of concepts.\nConsider that the concepts that occur in the same text chunk are also related by their contextual proximity. This relation is assigned a weight of W2. Note that the same pair of concepts may occur in multiple chunks.\nGroup similar pairs, sum their weights, and concatenate their relationships. So now we have only one edge between any distinct pair of concepts. The edge has a certain weight and a list of relations as its name.\nPopulate nodes (concepts) and edges (relations) in a graph data structure or a graph database.\nVisualize",
    "crumbs": [
      "Networks",
      "Knowledge Graphs"
    ]
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#examples",
    "href": "qmd/networks-knowledge-graphs.html#examples",
    "title": "Knowledge Graphs",
    "section": "Examples",
    "text": "Examples",
    "crumbs": [
      "Networks",
      "Knowledge Graphs"
    ]
  }
]