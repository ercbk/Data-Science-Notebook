[
  {
    "objectID": "qmd/causal-inference.html",
    "href": "qmd/causal-inference.html",
    "title": "Causal Inference",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-misc",
    "href": "qmd/causal-inference.html#sec-causinf-misc",
    "title": "Causal Inference",
    "section": "",
    "text": "Notes from\n\nhttps://fabiandablander.com/r/Causal-Inference.html\n\nStatistical models measure associations (e.g. linear, non-linear) which is mutual information among the variables\n\ne.g. wind and leaves moving in a tree (doesn’t answer whether the leaves moving creates the wind or the wind creates leaving moving)\n\nCausal inference predicts the conseqences after an intervention (i.e. action)\n\nYou must know the direction of causation in order to predict the conseqences of an intervention (unlike measuring associations)\nAnswers the question, “What happens if I do this?”\n\nCausal inference is able to reconstruct unobserved counterfactual outcomes.\n\nAnswers the question, “What happens if I had done something else?”\n\nCausal assumptions are necessary in order to make causal inferences\n\nmultiple regression does not distinguish causes from confounds\np-values are not causal statements\n\nDesigned to control type I error rate\n\nAIC, etc are purely predictive\n\nCausal Experiment Assumptions\n\nsee tlverse workshop notes and ebook for listing of assumptions and definitions,  https://tlverse.org/acic2019-workshop/intro.html#identifiability\n\nThe tlverse Project seeks to use ML models to calculate causal effects. Uses Super Learner ensembling and Targeted Maximum Likelihood Estimation (TMLE) which they call Targeted Learning.\n\nIgnorability - By randomly assigning treatment, researchers can ensure that the potential outcomes are independent of treatment assignment, so that the average difference in outcomes between the two groups can only be attributable to treatment\n\nEngineering outcome variables using potential adjustment variables does not automatically adjust for those variables in your model\n\nNotes from There Are No Magic Outcome Variables\nExample\n\n\nP is population density\nX is the variable of interest\nGDP and P have been used to create GDP/P\nP influences X and provides a backdoor path to GDP/P, so P must be adjusted for\nEven if P doesn’t influence X, the point is that constructing GDP/P using P doens’t automatically adjust for P\n\n\nRandomized experiments remove all paths from the treatment variable, X\n\n\nAdjusting for Z, B, and C can add precision to measurement of the treatment effect since they are causal to Y, but they aren’t necessary to get an unbiased estimate of the treatment effect.\n\nTable 2 fallacy (Notes from McElreath video, 2022 SR Lecture 6)\n\n\nThe 2nd table presented in a paper is usually a summary of all the effects of a regression. The fallacy is that the coefficient of each variable is treated as causal.\nExample: The effect of HIV on Stroke\n\nThe model is lm(Stroke ~ HIV + Smoke + Age)\n\nOnly the coefficient of the HIV variable should be treated as causal and none of the other adjustment variables (Smoke, Age)\n\nThe effects for Smoke and Age are only partial.\nThere are likely unobserved confounding variables, U, on the effect of Smoking on Stroke (e.g. other lifestyle variables).\n\nSmoke is confounded so it’s causal estimate is biased\nAge is also confounded since Smoke is now a collider and has been conditioned upon. This opens the non-causal path, Age-Smoke-U-Stroke.\n\nAge-Smoke is frontdoor, but the backdoor path, Smoke-U, also becomes a backdoor path for Age once Smoke is conditioned upon. (aka sub-backdoor path)\nSo any open path that contains a backdoor path must also be closed\n\n\n\nSolutions\n\nDon’t include effect estimates of adjustment variables\nExplicitly interpret each effect estimate according to the causal model\n\nSee 2022 SR at the end of Lecture 6 where McElreath breaks down the interpretation of each adjustment variable estimated effect.\n\n\n\nPartial Identification (Handling Unobserved Confounds)\n\nMisc\n\nAlso see\n\nPaper: Hidden yet quantifiable: A lower bound for confounding strength using randomized trials (code)\n\nUsing RCT results and Observational data, this paper proposes a statistical test and a method for determining the lower bound confounder strength.\nIn the context of pharmacuticals, RCT results are evidently often released after FDA approval, but this method can be used in any field where there’s a combination of RCT and observational studies..\n\n\n\nSometimes the confounding paths of a DAG model can be not be resolved.\n\nFor confounders that influence the treatment and outcome, see:\n\nStructural Causal Models &gt;&gt; Bayesian examples\nIf there’s a mediator, see Other Articles &gt;&gt; Frontdoor Adjustment\n\nMeasure proxies for the unobserved confound if it’s not practical/ethical to measure\n\ni.e. If the confound is ability, then test scores, letters of recommendation, etc. could be proxies.\n\nExample: 2022 SR Lecture 10 video, code\n\n\nA: Admitted to Grad School, G: Gender, D: Dept, u: Ability, T1,2,3: 3 Test Scores\n\nAbility is latent variable/unobserved confounder\nTest Scores are proxies for Ability\n\nBoth models are fit simultaneously\nCouldn’t find a way to use {brms} to code this and Kurz didn’t included it in his brms SR book.\n\n\n\nA biased estimate is better than no estimate. It can provide an upper bound\nFind a natural experiment or design one\nSensitivity Analysis\n\nAfter the analyis, you should be able to make the statement, “In order for the confound to be responsible for the entire causal effect, it was have to be .”\n\n\nPackages\n\n{tipr} - tools for tipping point sensitivity analyses\n\nSteps for using sensitivity analysis\n\nPerform a sensitivity analysis to determine plausibly how much of the causal effect is due to confounding paths\n\nAssume the confound exists, model it’s consequences for different strengths/kinds of influence\nExample: 2022 SR Lecture 10 video, code \n\nA: Admitted to Grad School, G: Gender, D: Dept, u: Unobserved Confounder\nBoth models are fit simultaneously\nValues for β and γ are specified and u is estimated as a parameter\nI think Gender (G) is an interaction in both models which I didn’t think was possible given there are no arrows of influence from gender to u.\n\nSince gender is a moderator it wouldn’t necessarily have to be an influence arrow, it would only need to be an arrow from G to the effect of u on D (see Moderator Analysis), so maybe this is kosher\nCould also be that I’m misunderstanding McElreath’s code he uses to specify his models with {Rethinking}.\n\nCouldn’t find a way to use {brms} to code this and Kurz didn’t included it in his brms SR book.\n\n\nUse previous studies that have effect strengths of those potential confounding variables\nCompare the strengths from the previous studies to the strength determined from the sensitivity analysis. The difference is a good guess for the strength of the causal effect of your treatment variable.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-causdes",
    "href": "qmd/causal-inference.html#sec-causinf-causdes",
    "title": "Causal Inference",
    "section": "Causal Design",
    "text": "Causal Design\n\nNotes from McElreath video\nMisc\n\nWhen trying to determine the relationship (e.g. linear, nonlinear) between variables and remove inconsequential variables, the Double Debiased ML procedure might be useful.\n\nDouble Debiased Machine Learning - basic concepts, links to papers, videos\nEconML (Microsoft) and causalml (Uber) has included the method in their libraries\n\n\nWhen trying to infer causal relationships, we should not blindly enter all variables into a regression in order to “control” for them, but think carefully about what the underlying causal DAG could look like. Otherwise, we might induce spurious associations (e.g. confounding such as collider bias).\nOverview\n\nMake a causal model (i.e. DAG)\n\nNeed background information in order to make the causal assumptions represented in the DAG\nDAGs only show whether or not a variable influences another, not how the influence occurs (e.g. DAGs can’t show interactions between variables or whether the association is non-linear)\n\nUse it to design data collection and statistical procedures\n\nSteps:\n\nDetermine two variables of interest (exposure, outcome) that you want to determine if a causal relationship exists and what effect the exposure has.\nUse domain knowledge or prior scholarship to determine the relevant variable and the likely associations between all variables in data\nCreate the DAG\n\nIdentify the direct causal path between exposure and outcome\nIdentify other explanatory variables and label their directions of influence with each other, the exposure, and the outcome variable\nConsider which variables (especially the exposure and the outcome) have unobserved variables influencing them.\n\nAnalyze the DAG\n\nIdentify colliders and use d-separation to determine conditional independencies\nIdentify additional paths (backdoor paths, sub-backdoor paths) between exposure and outcome\nUse the backdoor criterion to determine the set of variables that need to be adjusted for in order to block all backdoor paths with only the direct causal path remaining open.\nAdd additional adjustment variables that are causal to the outcome variable (but don’t confound the treatment effect) in order to add precision to the estimate of the treatment effect\n\nCreate simulated data that fits the DAG (i.e. a generative model)\nPerform statistical analysis (i.e. SCMs) on the simulated data  to make sure you can measure the causal effect.\nDesign experiment and collect the data\nRun the statistical analysis on the collected data and calculate the average causal effect (ACE) under the assumptions that your DAG and model specifications are correct.\nBased on your results, revise the DAG and SCM as necessary and repeat as necessary\n\nBad Adjustment Variables (Code and more details included in 2022 SR, Lecture 6)\n\nFor all examples, Z is the adjustment variable that’s being considered; X is the treatment and Y is the outcome\n\nIn each scenario, including Z produces a biased estimate of X, so the correct model is Y ~ X.\n\nM-bias\n\n\nZ doesn’t have a direct causal influence on the either X or Y, but when it’s conditioned upon it becomes a collider due to unobserved confounds that have a direct causal influence on X and Y.\nCommon issue in Political Science and network analysis\nExample\n\nY: Health of Person 2\nX: Health of Person 1\nZ: Friendship status\n\nPre-treatment variable (tend to be open to collider paths) since they could be friends before the exposure\n\nU: Hobbies of Person 1\nV: Hobbies of Person 2\n\n\nPost-Treatment Bias\n\n\nZ is a mediator and conditioning upon Z blocks the path from X to Y, but opens the backdoor path through the unobserved confound, U.\nCommon in medical studies  \nExample\n\nY: Lifespan\nX: Win Lottery\nZ: Happiness\nU: Contextual Confounds\n\n\nSelection Bias\n\n\nSame as collider bias\n\nThis version adds an unobserved confounder\n\nExample\n\nY: Income\nX: Education\nZ: Values\nU: Family\n\n\nCase-Control Bias\n\n\nZ is a descendent. Since Z has information about Y, conditioning on it will narrow the variation of Y and distort the measured effect of X.\nAlso see Association &gt;&gt; Single Path DAGs &gt;&gt; Descendent\nExample\n\nY: Occupation\nX: Education\nZ: Income\n\n\nPrecision Parasite\n\n\n2 versions: with and without U\n\nWithout U, conditioning on Z removes variation from X and lessens (but doesn’t bias) the precision of the estimated effect of X on Y (i.e. inflated std.error)\nWith U, the effect of X is biased and that bias is amplified when Z is included.\n\n\nPeer Bias\n\n\nClassic DAG of the Berkley Admission-Race-Department study\nAlso see Structural Causal Models &gt;&gt; Example (Bayesian Peer Bias)\nX is race, E is department, Q is unobserved (e.g. student quality), Y is Admission\nDepartment cannot be conditioned upon because it’s a collider with Q and would bias the estimate of X through a sub-backdoor path, X-E-Q-Y\nOnly the total effect of X on Y can be estimated (Y ~ X) since E cannot be conditioned upon but that’s not interesting and maybe not precise",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-terms",
    "href": "qmd/causal-inference.html#sec-causinf-terms",
    "title": "Causal Inference",
    "section": "Terms",
    "text": "Terms\n\nAverage Causal Efffect (ACE) - average population effect that’s calculated from an intervention (see Counterfactual definition for info on Individual Causal Effects)\n\nIf X is binary, then   is the average causal effect (see Simpson’s Paradox example)\n\nCalculated from a contingency table\n\nAlso, \n\nThis looks like the interpretation of the slope in a regression model.\n\n\nBackdoor Criterion - A valid causal estimate is available if it is possible to condition on variables such that all backdoor paths are closed\n\nGiven two nodes, X and Y, an adjustment set, L, fulfills the backdoor criterion if \n\nno member in L is a descendant of X and\nmembers in L block all backdoor paths (“shutting the backdoor”) between X and Y.\n\nAdjusting for L thus yields the causal effect of X→Y.\nAfter executing an intervention, the conditional distribution in the observational DAG (seeing) will correspond to the interventional distribution (doing) when blocking the spurious path. (see Simpson’s Paradox example)\n\nBackdoor Path - A non-causal path that enters a causal variable in a DAG rather than exits it.\n\ne.g. the path that connects a collider to a causal variable points from the collider to the causal variable\nSub-backdoor Path - this path begins with a frontdoor path but through conditioning on a variable, it opens a connecting backdoor path which biases the treatment effect\n\nsee Misc &gt;&gt; Table 1 Fallacy and Causal Design &gt;&gt; Bad Adjustment Variables &gt;&gt; Peer Bias\n\n\nThe causal effect is the distribution of Y when we change x, averaged over the distributions of the adjustment variables (Z)\nCausal Hierarchy (lowest to highest)\n\nAssociation\n\nassociated action: Seeing - observational; observing the value of Y when X = x\n\n , observational distribution; What values Y would likely take on if X happened to equal x.\n\n\nIntervention\n\nassociated action (do-Calculus): Doing -  experimental; observing the value of Y after setting X = x\n\n , interventional distribution; What values Y would likely take on if X would be set to x.\nUsing the do operator allows us to make inferences about the population but not individuals.\ndo(X) means to cut all of the backdoor paths into X, as if we did a manipulative experiment. The do-operator changes the graph, closing the backdoors.\nThe do-operator defines a causal relationship, because Pr(Y|do(X)) tells us the expected result of manipulating X on Y, given a causal graph.\n\nWe might say that some variable X is a cause of Y when Pr(Y|do(X)) &gt; Pr(Y|do(not-X)).\n\n(makes more sense to me with a binary outcome, Pr(Y = 1|do(X), but maybe Y as a continuous variable can be defined a subset. …I dunno)\n\n\nThe ordinary conditional probability comparison, Pr(Y|X) &gt; Pr(Y|not-X), is not the same. It does not close the backdoor.\nNote that what the do-operator gives you is not just the direct causal effect. It is the total causal effect through all forward paths.\n\nTo get a direct causal effect, you might have to close more backdoors.\n\nThe do-operator can also be used to derive causal inference strategies even when some backdoors cannot be closed.\n\n\nCounterfactual\n\nassociated action: Imagining - what would be the outcome if the alternative would’ve happened.\nIndividual Causal Effects can be calculated but it requires stronger assumptions and deeper understanding of the causal mechanisms\n\nNeed to research this part further.\nIf the underlying SCM is linear then the ICE = ACE.\n\n\n\nA collider along a path blocks that path. However, conditioning on a collider (or any of its descendants) unblocks that path\n\nWhen a collider is conditioned upon, the change in the association between the two nodes it separates is called collider bias.\n\ne.g. if Z is a collider between X and Y, conditioning upon Z will induce an association between X and Y.\n\n\nA conditioning set, \\(L\\), is the set of nodes we condition on (it can be empty).\nConfounding is the situation where a (possibly unobserved) common cause obscures the causal relationship between two or more variables.\n\nThere is more than one causal path between two nodes.\nA causal effect of X on Y is confounded if  \nCollider bias is a type of confounding. When a collider is controlled for, a second (or more) path opens, and the effect is confounded\n\nX and Y are d-separated by [L if conditioning on all members in [L blocks all paths between the nodes, X and Y.\n\nTool for checking the conditional independencies which are visualized in DAGs.\n\nA descendant is a node connected to a parent node by that parent node’s outgoing arrow.\nFrontdoor Adjustment - In a causal chain with three nodes X→Z→Y, we can estimate the effect of X on Y indirectly by combining two distinct quantities: (Useful for when unobserved confounders prevent direct causal estimation)\n\nThe estimate of the effect of X on Z, P(Z|do(X))\nThe estimate of the effect of Z on Y, P(Y|do(Z), X)\n\nFrontdoor Path - a path that exits a causal variable in a DAG rather than enters it.\n\ne.g. the path that connects a causal variable, X, to an outcome variable, Y, has an arrow that points from X to Y.\n\nMarkov Equivalence - A set of DAGs, each with the same conditional independencies\nMediation Analysis - seeks to identify and explain the mechanism or process that underlies an observed relationship between an independent variable and a dependent variable via the inclusion of a third hypothetical variable, known as a mediator variable (z-variable in the DAGs of “pipes” below)\n\nIncluding a mediator and the independent variable in a regression will result in the independent variable not being signficant and the mediator being significant.\n\nModeration Analysis - Like mediation analysis, it allows you to test for the influence of a third variable, Z, on the relationship between variables X and Y, but rather than testing a causal link between these other variables, moderation tests for when or under what conditions an effect occurs.\nA node is a parent of another node if it has an outgoing arrow to that node\nA path from X to Y is a sequence of nodes and edges such that the start and end nodes are X and Y, respectively.\nResidual Confounding occurs when a confounding variable is measured imperfectly or with some error and the adjustment using this imperfect measure does not completely remove the effect of the confounding variable.\n\nExample: Women who smoke during pregnancy have a decreased risk of having a Down syndrome birth.\n\nThis is puzzling, as smoking is not often thought of as a good thing to do. Should we ask women to start smoking during pregnancy?\nIt turns out that there is a relationship between age and smoking during pregnancy, with younger women being more likely to indulge in this bad habit. Younger women are also less likely to give birth to a child with Down syndrome. When you adjust the model relating smoking and Down syndrome for the important covariate of age, then the effect of smoking disappears. But when you make the adjustment using a binary variable (age&lt;35 years, age &gt;=35 years), the protective effect of smoking appears to remain.\n\n\nStructural Causal Models (SCMs) - relate causal and probabilistic statements; each equation is a causal statement\n\n\n\n“:=” is the assignment operator\nX is a direct cause of Y which it influences through the function f( )\n\nwhere f is a statistical model\n\nThe noise variables, ϵX and ϵY, are assumed to be independent.\n\nThere are Stochastic and Deterministic SCMs. Deterministic SCMs presented in article.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-assoc",
    "href": "qmd/causal-inference.html#sec-causinf-assoc",
    "title": "Causal Inference",
    "section": "Association",
    "text": "Association\n\n\n\nFar left: lm(Y ~ X); X and Y show a linear correlation when Z is NOT conditioned upon\nLeft: lm(Y ~ X + Z); X and Y show NO linear correlation when Z is conditioned upon\nRight: lm(Y ~ X); X and Y show NO linear correlation when Z is NOT conditioned upon\nFar Right:  lm(Y ~ X + Z); X and Y show a linear correlation when Z is conditioned upon",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-singpath",
    "href": "qmd/causal-inference.html#sec-causinf-singpath",
    "title": "Causal Inference",
    "section": "Single path DAGs",
    "text": "Single path DAGs\n\n\nFor each of these DAGs, Z would be the only member of the conditioning set.\nThe first 3 DAGs represent the scatter plots above\n\nZ only blocks the path between X and Y when it’s conditioned upon.\n\nX and Y are associated (e.g. linear correlation, mutual information, etc.) when Z is ignored\nConditioning on Z results in X and Y no longer being associated (i.e. conditional independence)\n\nThe first and second DAGs are elemental confounds or relations called “Pipes.”\n\nThe left one\n\nIn general, DO NOT add these variables to your model\n\nThese paths are causal so they shouldn’t be blocked\nIf your goal isn’t causal inference, then adding these variables might provide predictive information\ne.g. If there was a causal arrow from X to Y, the far left DAG would NOT have a backdoor path and therefore Z would not  be conditioned upon to block the path, X-Z-Y\n\nThe path from X to Z is a frontdoor path since the arrow exits X.\n\n\nSometimes you DO condition on these variables\n\nDuring mediation analysis, you condition on these variables as part of the process to determine how much of the effect goes through Z.\nThe mediation path can have an important interpretation depending on your research question\n\ne.g. indirect descrimination\n\nSee Statistical Rethinking &gt;&gt; Chapter 11 &gt;&gt; Conclusion of Berkeley Admissions example\n\nalso Lecture 9 2022 video\n\n\n\n\n\nThe right one is a backdoor path and should be conditioned on.\nEverything you can learn about Y from X (or vice versa) happens through Z, therefore learning about X separately provides no additional information\nZ is traditionally labelled a mediator\n\nThe third DAG is an elemental confound  or relation called a “Fork.”\n\nIn general, add these variables to your model\nThese are backdoor paths and are NOT causal\nX and Y have a common cause in Z and some of the mutual information about Z they each contain, overlaps, and creates an association (when Z isn’t conditioned upon).\n\n\nThe fourth DAG is an elemental confound or relation called a “Collider.”\n\n\nIn general, do NOT add these variables to your model\nZ blocks the path between X and Y unless conditioned upon.\nAn association between X and Y is induced  by conditioning on Z, lm(Y ~ X + Z)\n\nX and Y are independent causes of Z. Z contains information about both X and Y, but X doesn’t contain any information about Y and vice versa.\nA small X and a sufficiently large Y (and vice versa) can produce a Z = 1. So X and Y have compensatory relationship in causing Z.\n\ni.e. For a given value of Z, learning something about X tells us what Y might have been.\n\n\n\nThe last elemental confound or relation is called a “Descendent.”\n\n\nConditioning on a descendent variable, D, is like conditioning on the variable, Z itself, but weaker. A descendent is a variable influenced by another variable.\nControlling for D will also control, to a lesser extent, for Z. The reason is that D has some information about Z. This will (partially) open the path from X to Y, because Z is a collider. The same holds for non-colliders. If you condition on a descendent of Z in the pipe, it’ll still be like (weakly) closing the pipe.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-dualpath",
    "href": "qmd/causal-inference.html#sec-causinf-dualpath",
    "title": "Causal Inference",
    "section": "Dual path DAGs",
    "text": "Dual path DAGs\n\n\nCausal paths do not flow against arrows but associations can.\nTwo examples of DAGs representing confounding\n\nThese are the 2 middle DAGs above with an additional path from X to Y\nIf Z is NOT conditioned on (i.e. top path is not blocked), then the causal effect of X on Y would be confounded.\n\n\n\n\nThe paths from X to Y:\n\nThe path through Z matches the first DAG.\n\nTherefore X and Y are conditionally independent given Z.\n\nThe path through W matches the fourth DAG\n\nTherefore X and Y are conditionally dependent given W.\n\n\nThe path through W (collider) is blocked unless W is conditioned upon\nThe path through Z is open unless Z is conditioned upon\nIf Z and W are conditioned upon, then the path between X and Y is open through W and an association is present.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-interv",
    "href": "qmd/causal-inference.html#sec-causinf-interv",
    "title": "Causal Inference",
    "section": "Intervention",
    "text": "Intervention\n\n\nSince actual interventions are usually unfeasible, we want to be able to determine causality with observational data. This requires two assumptions:\n\nThe intervention occurs locally. Which means that only the variable we target is the one that receives the intervention.\nThe mechanism by which variables interact do not change through interventions; that is, the mechanism by which a cause brings about its effects does not change whether this occurs naturally or by intervention\n\nThe Doing row of DAGs (aka manipulated DAGs) represents setting X = x\n\nFor DAGs 1 and 4, Y is still affected\n\nMoving from seeing to doing didn’t change anything\n\n\nFor DAGs 2 and 3, Y is now UNaffected\n\nUsing the assumptions and some mathematical manipulation (See article for details):\n\n\n\nThus, the interventional distribution we care about is equal to the (observational) conditional distribution of Y given X when we adjust for Z\n\n\n\n\nThe rule: After an intervention, incoming arrows are cut from the node where the intervention took place.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-confound",
    "href": "qmd/causal-inference.html#sec-causinf-confound",
    "title": "Causal Inference",
    "section": "Confounding",
    "text": "Confounding\n\n\nThe backdoor criterion tells us which variable we need to adjust for in order to for our model to yield a causal relationship between two variables (i.e. graphically, nodes)\n\nBlocks all spurious, that is, non-causal paths between X and Y.\nLeaves all directed paths from X to Y unblocked\nCreates no spurious paths\n\nExample\n\nCausal effect of Z on U is confounded by X because in addition to the legitimate causal path Z→Y→W→U, there is also an unblocked path Z←X→W→U which confounds the causal effect\n\nSince X’s arrow enters the causal variable of interest, Z, it’s arrow is a backdoor path and needs to be blocked/closed\nThere are some descendant nodes that make the confounding a little difficult to parse out, but this graph is essentially\n\n\nwhich is the same as the second example DAG for confounding in the Association section\n\n\nThe backdoor criterion would have us condition on X, which blocks the spurious path and renders the causal effect of Z on U unconfounded.\n\nThe reduced, confounding DAG above is the same as the third DAG (without the path from Z to U) in the Association section. Conditioning on Z in that example blocked the path between X and Y, so it makes sense that conditioning on X in the reduced DAG would block the Z to X to U path. And therefore, the Z←X→W→U would also be blocked in the complete DAG.\n\nNote that conditioning on W would also block this spurious path; however, it would also block the causal path, Z→Y→W→U.\n\n\nIf we breakdown the complete DAG into the modular components involving W, we can see these are the same as the first example DAG in the Association section.\nW is also collider for X and Y, but I don’t think that has any bearing when discussing the causal effect of Z on U.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-appsimp",
    "href": "qmd/causal-inference.html#sec-causinf-appsimp",
    "title": "Causal Inference",
    "section": "Application: Simpson’s Paradox Example",
    "text": "Application: Simpson’s Paradox Example\n\nSex as the adjustment variable           \n\nPatients CHOOSE whether or not to take a drug to cure some disease.\nMen choosing to take the drug recover at a higher percentage that those that didn’t\nWomen choosing to take the drug recover at a higher percentage that those that didn’t\nBut overall, those that chose to take the drug recovered at a lower percentage than those that didn’t.\nSo should a doctor prescribe the drug or not?\nSuppose we know that women are more likely to take the drug, that being a woman has an effect on recovery more generally, and that the drug has an effect on recovery. \nCreate DAGs\n\n\nS=1 as being female,\nD=1 as having chosen to take the drug\nR=1 as having recovered\nThe right DAG indicates either forcing everyone to either take the drug or not take the drug\nNotice that   therefore our calculated effect will be confounded.\n\nBackdoor criterion says the manipulated DAG (right) will correspond to the observational DAG (left) if we condition on Sex.\n\n\nUse intervention formula from Intervention section\n\n\nAverage Causal Effect = 0.832 - 0.782 = 0.050. So the drug has a positive effect on average.\n\n\nBlood Pressure as the adjustment variable \n\nBlood Pressure instead of sex is used as the adjustment. Blood Pressure is a post-treatment variable.\nRelatively same observations as before. High or Low Blood Pressure with the drug produces better results than those that chose not to take the drug. Yet overall, those that chose the drug recovered at a lower percentage.\n\nSince Blood Pressure (B) is post-treatment, it has no effect on whether the patient takes the drug or not (D).\nTaking or not taking the drug (D) has an indirect effect on recovery (R) through Blood Pressure (B) along with a direct effect.   so our calculated effect will be unconfounded.\n\nSo with BP as the adjustment variable, the drug now has a small, negative effect (harmful), 0.78 - 0.83 = -0.05\n\nThe unconfounded, average causal effect for the population is negative, therefore the doctor should NOT prescribe the drug.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-scms",
    "href": "qmd/causal-inference.html#sec-causinf-scms",
    "title": "Causal Inference",
    "section": "Structural Causal Models (SCMs)",
    "text": "Structural Causal Models (SCMs)\n\nYou add additional assumptions to your DAG to derive a causal estimator\n“Full Luxury” Bayesian approach\n\n“Full Luxury” is just a term coined by McElreath; it’s just a bayesian model but bayesian models can fully model a DAG where standard regression approachs can fail (see examples)\nNo other approach will find something that the bayesian approach doesn’t\n\nMain disadvantage is that it can be computationally intensive (same with all baysian models)\n\nProvides ways to add “causes” for missingness and measurement error\n\nExample (2 Moms)\n\nNotes from McElreath video\nHypothesis: a mother’s family size is causal to her daughter’s family size\n\nTruth: no relationship\n\nVariables:\n\nM - Mother’s family size (i.e. number of children the birth)\nD - Daughter’s family size\nB1 - Mother’s birth order; binary, first born or not\nB2 - Daughters’ birth order; binary, first born or not\nU - unobserved confounds  (shown as curved dotted line)\n\n\n\nUnobserved confounds (economic status, education, cultural background, etc.) are causal to both Mother and Daughter (curved dotted line) which makes regression, D ~ M, impossible\n\nSee Baysian Two Moms example below for results of a typical regression\nStill possible to calculate the effect of M on D with SCMs\n\n\nAssumptions: Relationships are linear (i.e. linear system)\nCausal Effects\n\n\nWe want m which is the causal effect of M on D\nAssumes causal effect of birth order is the same on mother and daughter\nAside: There is no arrow/coefficient from M to B2 because it’s not germane to the calculation of m\n\nCalculate linear effect (i.e. regression coefficient) without a regression model using a linear system of equations\n\nNote: a regression coefficent, β = cov(X,Y) / var(X)\nWe can’t calculate the covariance of M and D directly because it depends on unobserved confounders but we can calculate the covariance between B1 and D and use that to get m.\nThe covariance for each path is the product of the path coefficients and the variance of the originating causal variable.\nPath B1 → M: cov(B1, M) = b*var(B1)\nPath B1 → D: cov(B1, D) = b*m*var(B1)\n2 equations and 2 unknowns, m and b\nSolve for b in the first equation, substitute b into the second equation, and solve for m\n\nm = cov(B1, D) / cov(B1, M)\n\nStill need an uncertainty of this value (e.g. bootstrap)\n\n\nExample (Bayesian 2 Moms)\n\nSee previous example for link, hypothesis, and definition of the variables\n\nFunctions (right side)\n\nEach variable’s function’s inputs are variables that are causal influences (i.e. have arrows pointing at the particular variable\n\ne.g. M has two arrows pointing at it in the DAG: B1 and u\n\n\nCode\n\nThe assumption is that this is a lineary system, so M and D have Normal distributions for their functions with means as linear regression equations\nB1 and B2 are binary so they get bernoulli distributions\nU gets a standard normal prior\n\nAside: evidently this is a typical prior for latent variables in psychology\n\np, intercepts, sd, k get typical priors for bayesian regressions\n\nResults\n\n\nTruth: no effect\n1st 3 lm models shows how the unobserved confound biases the estimate when using a typical regression model to estimate the causal effect\n\nIncluding B2 adds precision to the biased estimate since it is causal to the outcome D while adding B1 increases the bias\n\nBayesian model isn’t fooled because U is specified as an input to the functions for M and D\n\nInterpretation: There is no reliable estimate of an effect. The most likely effect is a moderately positive one but it could also be negative.\nAdding more simulated data to this example will move the point estimate towards zero\n\n\n\nExample (Bayesian Peer Bias)\n\nAlso see Causal Design &gt;&gt; Bad Adjustment Variables &gt;&gt; Peer Bias\nHypothesis: racial discrimination in acceptance of applicatioon to Berkeley grad schools\n\nTruth: moderate negative effect, -0.8\n\nVariables:\n\nX is race, E is department, Q is an unobserved confound (latent variable: student quality), Y is binary; Admission/No Admission\nR1 and R2 are proxy variables for Q (e.g. test scores, lab work, extracurriculars, etc.)\n\nAssumptions: System is linear\nDAG and Code\n\n\nXX is the race variable with X as the coefficient in the code\n\nThis code uses his {rethinking} package so some of this syntax is unfamiliar\n\nR1 and R2 are shown in the DAG to be influenced by student quality, Q\nEvery prior is normal except for Q’s coefficient\n\nResults\n\n\nTruth: -0.8\n1st 3 glm models shows how the unobserved confound, Q, biases the estimate when using a typical logistic regression model to estimate the causal effect\nBayesian model isn’t fooled because Q is specified as an input to the function for Y\n\nInterpretation: There is a reliably negative effect (no 0 in the CI). The most likely effect is a moderately negative one.\nNot quite equal to the truth but reliably negative and the point estimate is closer than the glms\n\n\n\nExample\n\nAssumptions: Relationships between variables are linear and error terms are independent\nEquations\n\n,  \n\n\nDAG 1 (left) shows the association DAG which represents the SCM\nmanipulated DAG 1 (middle) shows intervention where z is set to a constant\n\nincoming causal arrows get cutoff the intervening variable\n\nmanipulated DAG 1 (right) shows intervention where x is set to a constant\n\nSimulation of the SCM (n = 1000) (code in article)\n\n\nZ is more predictive of Y than X\n\nSimulate interventions (code in article)\n\n\nLeft - histogram of SCM for Y without an intervention\nMiddle - Intervention on Z\n\nconfirms the DAG which shows no effect on Y and Z is not causal\n\nRight - intervention on X\n\nconfirms the DAG which shows an intervention on X produces an effect on Y and X is causal\n\nAverage Causal Effect (ACE) can be determined by subtracting the expected values of interventions where  X = x +1 and  X = x",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-ctrfact",
    "href": "qmd/causal-inference.html#sec-causinf-ctrfact",
    "title": "Causal Inference",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nExample(code in article): Test whether Grandma’s home remedy can speed recovery time for the common cold\n\nSCM\n\n\nT is 1/0, i.e. whether patient receives Grandma’s treatment, with p = 0.5; \nR is recovery time\nμ is the intercept\nβ is the average causal effect, since\n\n\nwhere \n\n\nFrom fitting the model, we find μ = 7, β = -2, Τ = 0, ε1 = 0.78\n\nTherefore, the Individual Causal Effect for patient 1\n\n\nJust plug and chug where we substitute T = 1 into the SCM and we already have the T = 0 part from the model\n\n\nIn this case, the SCM is linear, so the ICE = ACE.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-medanal",
    "href": "qmd/causal-inference.html#sec-causinf-medanal",
    "title": "Causal Inference",
    "section": "Mediation Analysis",
    "text": "Mediation Analysis\n\n\nFigure\n\nc’ is the direct effect of X on the outcome after the indirect path has been removed (i.e. conditioned upon, outcome ~ X + mediator)\nc is the to total effect (outcome ~ X)\nc - c’ equals the indirect effect\nSee definitions below\n\nAllows you to test for the influence of a third variable, the mediator, on the relationship between (i.e. effect of) the treatment variable, X, and the Outcome variable, Y.\nMisc\n\nNotes from: Mediation Models\n\nOverview of packages (Aug 2020)\n\n{brms} very flexible in terms of models. You’ll just have to calculate the effects by hand unless some outside package (e.g. sjstats) takes a brms model and does it for you.\n\nSee below for formulas. {mediation} papers should have other formulas for other types of models (e.g. poisson, binomial)\n\n{mediation} handles a lot for you. Method isn’t bayesian but is very similar to it in a frequentist-bootstrappy-simulation way.\n\nPackage has been substantially updated since that article was written.\n\n\nAlso see\n\nOther Articles &gt;&gt; Frontdoor Adjustment\nStatistical Rethinking &gt;&gt; Chapter 11 &gt;&gt; Conclusion of Berkeley Admissions example\n\nalso Lecture 9 2022 video\n\nebook (w/brms) Introduction to Mediation, Moderation, and Conditional Process Analysis\n\nIncluding a mediator and the independent variable in a regression will result in the independent variable not being signficant and the mediator being significant.\n\nExample: Causal effect of education on income\n\nSay occupation is your mediator. Education has a big impact on your occupation, which in turn has a big impact on your income. You don’t want to control for a mediator if you are interested in the full effect of X on Y! Because a huge part of how X impacts on Y is precisely through the mediation of C, in our case choice of and access to occupation, given a certain level of education. If you ‘control’ for occupation you will be greatly underestimating the importance of education.\n\n\nWhen would you want to only measure the Direct Effect?\n\nExample: Determining the amount of remuneration for discrimination\n\nFrom Simulating confounders, colliders and mediators\nVariables\n\nOutcome: Pay Gap\nTreatment: Gender\n\nIn this case, this variable is actually “gender discrimination in the current workplace in making a pay decision” (for which we use actual, observed Gender as a proxy)\n\nMediators: Occupation and Experience\n\nWhen determining whether a type of descrimination exists, you don’t want to condtion on the mediators, because the effect of gender will be underestimated. So, you’d want the total effect. But here, discrimation is already determined and Gender is now a proxy variable. Under Gender’s new definition, Occupation and Experience might influence the amount of “gender discrimiation,” so they can’t be definitively labelled mediators any more.\nSo if you want to estimate that final “equal pay for equal work” step of the chain then yes it is legitimate to control for occupation and experience.\n\n\nShould always compare a mediation model to a model without mediation\n\nAn unnecessary mediation model will almost certainly be weaker and probably more confusing than the model you would otherwise have.\n\nAverage Causal Mediation Effect (ACME) (aka Indirect Effect)- the expected difference in the potential outcome when the mediator took the value that it would have under the treatment condition as opposed to the control condition, while the treatment status itself is held constant.\n\nIf this isn’t significant, there isn’t a mediation effect\nIt is possible that the ACME takes different values depending on the baseline treatment status. Shown by analyzing the interaction between the treatment variable and the mediator\nδ(t) = E[Y (t, M(t1)) − Y (t, M(t0))]\n\nwhere\n\nt, t1, t0 are particular values of the treatment T such that t1 ≠ t0,\nM(t) is the potential mediator\nY (t, m) is the potential outcome variable\n\n\n\nAverage Direct Effect (ADE) - the expected difference in the potential outcome when the treatment is changed but the mediator is held constant at the value that it would have if the treatment equals t.\n\nζ(t) = E[Y (t1, M(t)) − Y (t0, M(t))]\n\nThe Total Effect of the treatment on the outcome is ACME + ADE.\n\nConditions where you likely do NOT need mediation analysis :\n\nIf you cannot think of your model in temporal or physical terms, such that X necessarily leads to the mediator, which then necessarily leads to the outcome.\nIf you could see the arrows going either direction.\nIf when describing your model, everyone thinks you’re talking about an interaction (a.k.a. moderation).\nIf there is NO strong correlation between key variables (variables of interest) and mediator, and if there is NO strong correlation between mediator and the outcome.\n\nSobel test - tests whether the suspected mediator’s influence on the independent variable is significant.\n\nPerforming the test in R via bda::mediation.test - article\n\nMethods\n\nBaron & Kenny’s (1986) 4-step indirect effect method has low power\nProduct-of-Paths (or difference in coefficients)\n\nc - c’ = a*b (see figure at start of this section) where c - c’ is the indirect effect (aka ACME)\n\nif either a or b are nearly zero, then the indirect effect can only be nearly zero\nFormula only appropriate for the analysis of causal mediation effects when both the mediator and outcome models are linear regressions where treatment (IV) and moderator enter the models additively (e.g. without interaction)\n\nEffect formulas for models with an interaction between treatment and moderator (Paper)\n\nmediator: M = α2 + β2Ti + ξT2Xi + εi2(T~i`)\noutcome: Y = α~3 + β3Ti + γMi + κTiMi + ξT3Xi + εi3(Ti, Mi)\nACME = β2(γ + κt) where t = 0,1\nADE = β3 + κ{α2 + β2t + ξT2Ε(Xi)}\nATE = β2γ + β3 +κ{α2 + β2 + ξT2Ε(Xi)}\n\nAlternatively, fit Y = α1 + β1Ti + ξT1Xi + ηTTiXi + εi1\n\nThen ATE = β1 + ηTE(Xi)\n\n\nNotes\n\nVariables\n\nT is treatment, M is mediator, X is a set of adjustment variables\n\nThe exponentiated T in ξT is to let you know it can be a set of coefficients for a set of adjustment variables (I guess)\n\n\nCouldn’t figure out why curly braces are being used\nACME with have two estimates (t=0, t=1)\nATE (average total effect)\nΕ(Xi) is the sample average of each adjustment variable and it’s multiplied by its associated ξ2 coefficient\nSee paper for other types of models\n\n\n{lavaan}, {brms}\n\nTingley, Yamamoto, Hirose, Keele, & Imai, 2014\n\nQuasi-bayesian approach (paper ,esp Appendix D, for details)\n\nFits the mediation and outcome models (see 1st example)\nTakes the coefficients and vcov matrices from both models\n\nUses the coefs (means) and vcovs (variances) as inputs to a mvnorm function to simulate distributions for the coefficients.\nI do not understand what these are used for… would have to look at the code.\n\nSamples predictions of each model K times for treatment = 1, then for treatment = 0\nCalcs difference between predictions for each set of samples, then averages to get the ACME\n\nAssumes Sequential Ignorability\n\nRequires treatment randomization or an equivalent assignment mechanism\nmediator is also ignorable given the observed treatment and pre-treatment confounders. This additional assumption is quite strong because it excludes the existence of (measured or unmeasured) post-treatment confounders as well as that of unmeasured pretreatment confounders. This assumption, therefore, rules out the possibility of multiple mediators that are causally related to each other (see Section 6 for the method that is designed to deal with such a scenario).\nCan’t be tested but a sensitivity analysis can be conducted using mediation::medsens (see vignette)\n\n{mediation} (vignette)\n\nMultiple types of models for both mediator and outcome\n\nincluding multilevel model functions from {lme4} supported\n\nMethods for:\n\n‘moderated’ mediation\n\nthe magnitude of the ACME depends on (or is moderated by) a pre-treatment covariate. Such a pre-treatment covariate is called a moderator. (see Moderator Analysis)\nACME can depend on treatment status (i.e. interaction between treatment and mediator), but this situation is talking about a separate variable moderating the effect of the treatment on the mediator.\n\nmultiple mediators (which violates sequential ingnorability but can be handled)\nvarious experimental designs (e.g. parallel, crossover)\ntreatment non-compliance\n\nUses MASS (so may have conflicts with dplyr)\nNo latent variable capabilities\n\n\nEtsy article calculates generalized average causal mediation effect (GACME) and generalized average direct effect (GADE) and uses a known mediator to measure the direct causal effect even when the DAG has multiple unknown mediators (paper, video, R code linked in article)\n\nExample: Tingley, 2014 Method\n\nEquations\n\n\n\nPredictions for “job_seek” in the mediator model (top) are used as predictor values in the outcome model (bottom).\n\nData: data(jobs, package = 'mediation')\n\ndepress2: outcome, numeric: Measure of depressive symptoms post-treatment. The outcome variable.\ntreat: treatment, binary: whether participant was randomly selected for the JOBS II training program.\n\n1 = assignment to participation.\n\njob_seek: mediator, ordinal: measures the level of job-search self-efficacy with values from 1 to 5.\necon_hard: adjustment, ordinal: Level of economic hardship pre-treatment with values from 1 to 5.\nsex: adjustment, binary: 1 = female\nage: adjustment, numeric: Age in years\n\n{mediation}\nmodel_mediator &lt;- lm(job_seek ~ treat + econ_hard + sex + age, data = jobs)\nmodel_outcome  &lt;- lm(depress2 ~ treat + econ_hard + sex + age + job_seek, data = jobs)\n\n# Estimation via quasi-Bayesian approximation \nmediation_result &lt;- mediate(\n  model_mediator, \n  model_outcome, \n  sims = 500,\n  treat = \"treat\",\n  mediator = \"job_seek\"\n)\n\nSummary - summary(mediation_result)\n\n\nerror bar plot also available via plot(mediation_result)\nSays ACME isn’t significant, therefore no mediation effect detected.\n“Prop Mediated” is supposed to be the ratio of the indirect effect to the total.\n\nHowever this is not a proportion, and can even be negative, and so “it is mostly a meaningless number.”\n\n\n\n\nExample: product-of-paths (or difference in coefficients)\n\n{lavaan}\nsem_model = '\n  job_seek ~ a*treat + econ_hard + sex + age\n  depress2 ~ c*treat + econ_hard + sex + age + b*job_seek\n  # direct effect\n  direct := c\n  # indirect effect\n  indirect := a*b\n  # total effect\n  total := c + (a*b)\n'\nmodel_sem = sem(sem_model, data=jobs, se='boot', bootstrap=500)\nsummary(model_sem, rsq=T)  # compare with ACME in mediation\nDefined Parameters:\n                  Estimate  Std.Err  z-value  P(&gt;|z|)\n    direct          -0.040    0.045  -0.904    0.366\n    indirect        -0.016    0.012  -1.324    0.185\n    total            -0.056    0.046  -1.224    0.221\n\nAlso outputs the typical summary regression estimates, std.errors, pvals, R2 etc.\nBootstraps std.errors\nSame results for “indirect” here as with {mediation} ACME estimate\nR2s are poor for both regression models which could be why no mediation effect is detected.\n\n{brms}\nmodel_mediator &lt;- bf(job_seek ~ treat + econ_hard + sex + age)\nmodel_outcome  &lt;- bf(depress2 ~ treat + job_seek + econ_hard + sex + age)\nmed_result = brm(\n  model_mediator + model_outcome + set_rescor(FALSE), \n  data = jobs\n)\nsummary(med_result) # regression results\n# using brms we can calculate the indirect effect as follows\nhypothesis(med_result, 'jobseek_treat*depress2_job_seek = 0')\n\nExact same brms syntax (except priors are specified) as in Statistical Rethinking &gt;&gt; Chapter 5 &gt;&gt; Counterfactual Plots\nExample has a mediator DAG as well.\nhypothesis tests H0: a*b == 0\n\npval &lt; 0.05 says there is a mediation effect.\n\n\n{sjstats}\n\nsjstats::mediation(med_result) %&gt;% kable_df()\n\nmediator (b): the effect of “job_seek” on “depress2”\nindirect (c-c’): ACME\ndirect (c’): ADE\nproportion mediated: See {mediation} example",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-modanal",
    "href": "qmd/causal-inference.html#sec-causinf-modanal",
    "title": "Causal Inference",
    "section": "Moderation Analysis",
    "text": "Moderation Analysis\n\n\nMisc\n\nAlso see Introduction to Mediation, Moderation, and Conditional Process Analysis\n\nLike mediation analysis, it allows you to test for the influence of a third variable, Z (moderator), on the relationship between variables X and Y, but rather than testing a causal link between these other variables, moderation tests for when or under what conditions an effect occurs.\n\nModerators are conceptually different from mediators (“when” (moderator) vs “how/why” (mediator)).\n\nThere can be moderated mediation effect though. (see Mediation Analysis &gt;&gt; Methods &gt;&gt; {mediation})\n\nModerators can stengthen, weaken, or reverse the nature of a relationship.\nSome variables may be a moderator or a mediator depending on your question.\n\nAssumption: assumes that there is little to no measurement error in the moderator variable and that the DV did not CAUSE the moderator.\n\nIf moderator error is likely to be high, researchers should collect multiple indicators of the construct and use SEM to estimate latent variables.\nThe safest ways to make sure your moderator is not caused by your DV are to experimentally manipulate the variable or collect the measurement of your moderator before you introduce your IV.\n\nModeration can be tested by interacting variables of interest (moderator x IV) and plotting the simple slopes of the interaction, if present.\n\nSee Regression, Interactions for simple slopes/effects analysis\nMean center both your moderator and your IV to reduce multicolinearity and make interpretation easier. (“c” in variable names indicates variable was centered)\n\nExample: academic self-efficacy (moderator)(confidence in own’s ability to do well in school) moderates the relationship between task importance (independent variable (IV)) and the amount of test anxiety (outcome) a student feels (Nie, Lau, & Liau, 2011).\n\nStudents with high self-efficacy experience less anxiety on important tests (task importance) than students with low self-efficacy while all students feel relatively low anxiety for less important tests.\nSelf-efficacy (Z) is considered a moderator in this case because it interacts with task importance (X), creating a different effect on test anxiety (Y) at different levels of task importance.\n\nExample: What is the relationship between the number of hours of sleep (X, independent variable (IV)) a graduate student receives and the attention that they pay to this tutorial (Y, outcome) and is this relationship influenced by their consumption of coffee (Z, moderator)\nmod &lt;- lm(Y ~ Xc + Zc + Xc*Zc)\nsummary(mod)\n## Coefficients:\n##            Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept) 48.54443    1.17286  41.390  &lt; 2e-16 ***\n## Xc          5.20812    0.34870  14.936  &lt; 2e-16 ***\n## Zc          1.10443    0.15537  7.108 2.08e-10 ***\n## Xc:Zc        0.23384    0.04134  5.656 1.59e-07 ***\n\nSince we have significant interactions in this model, there is no need to interpret the separate main effects of either our IV or our moderator\nPlot the simple slopes (1 SD above and 1 SD below the mean) of the moderating effect\n\n\nFor details on this plot and analysis, see Regression, Interactions &gt;&gt; OLS &gt;&gt; numeric:numeric &gt;&gt; Calculate simple slopes for the IV at 3 representative values for the moderator variable\nInterpretation\n\nThose who drank less coffee (moderator, black line) paid more attention (outcome) with the more sleep (IV) that they got last night but paid less attention overall than average (the red line).\nThose who drank more coffee (moderator, green line) paid more attention (outcome) when they slept more (IV) as well and paid more attention than average.\nThe difference in the slopes for those who drank more or less coffee (moderator) shows that coffee consumption moderates the relationship between hours of sleep and attention paid",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-sr",
    "href": "qmd/causal-inference.html#sec-causinf-sr",
    "title": "Causal Inference",
    "section": "Statistical Rethinking",
    "text": "Statistical Rethinking\n\nMisc\n\nArrows indicate directions of influence\nArrows in DAGs “create” correlations\n\ni.e. if arrow, then correlation\nThe direction it points determines whether its association is causal or not.\n\nUnlike a statistical model, a DAG, if it is correct, will tell you the consequences of intervening to change a variable.\n** The data alone can never tell us when a DAG is right. But the data can tell us when a DAG is wrong. **\nMany dynamical systems cannot be usefully represented by DAGs, because they have complex behavior that is sensitive to initial conditions. But these models can still be analyzed and causal interventions designed from them.\nA DAG path means any series of variables you could walk through to get from one variable to another, ignoring the directions of the arrows.\nThe variable, U, in DAGs represents one or more unobserved variables\n\nUsually has circle around the U or is just represented by a dashed line\n\n“Conditioned upon,” “adjusted for,” or “controlled for” is all the same thing\n“a” or “α” is used in bayesian formulas to represent the intercept\nNotation\n\nX is not independent of Y, i.e \nconditional independence: Y is not associated with some variable X, after conditioning on some other variable Z, i.e. \n\nthey are statements of which variables should be associated with one another (or not) in the data.\nthey are statements of which variables become dis-associated when we condition on some other set of variables.\nThere is no other path of influence from X to Y except through Z\n\n\n(Total ) Causal Effect and Direct Causal Effect\n\n\nWeight (W) is the outcome, Height (H) and Sex (S) are explanatory\n(Total) Causal Effect is simply, W ~ S\nDirect Causal Effect shuts the backdoor paths, W ~ S + H\n\nSometimes we want the total causal effect and not the direct causal effect. (e.g. if H is a post-treatment variable, see SR, Ch.6)\n\n\n\n\n\nTestable Implications\n\nDiffering associations between plausible DAGs that are testable through statistical models\nAny DAG may imply that some variables are independent of others under certain conditions.\nNO conditional independencies → NO testable implications\nwww.dagitty.net - Enter DAG and it will give you the Adjustment Set and Testable Implications\nExample\n\nQuestion: What is the causal relationship between Divorce Rate (D), Marriage Rate (M), and Median Age at Marriage (A)\nData:\n\n2 regressions are fit\n\nD ~ α + βM\n\nShows that M is positively correlated with D\n\nD ~ α + βA\n\nShows that A is negatively correlated with D\n\n\n\nPlausible DAGs (note: marriage cannot influence your age… technically)\n\n\n\nA directly influences D\nM directly influences D\nA directly influences M\nReasoning: First, Age can have a direct effect, perhaps because younger people change faster than older people and are therefore more likely to grow incompatible with a partner. Second, it can have an indirect effect by influencing the marriage rate. If people get married earlier, then the marriage rate may rise, because there are more young people. Consider for example if an evil dictator forced everyone to marry at age 65. Since a smaller fraction of the population lives to 65 than to 25, forcing delayed marriage will also reduce the marriage rate. If marriage rate itself has any direct effect on divorce, maybe by making marriage more or less normative, then some of that direct effect could be the indirect effect of age at marriage.\n\n\n\nSimilar to 1 except M does not directly influence D\nReasoning This DAG is plausible even though there’s a correlation between M and D (regression 1). It could be that M derives it’s correlation with D through it’s association with A.\n\nThe direction of influence doesn’t prevent a correlation between M and D\n\n\n\nTestable implications\n\nDAG 1\n\nThe DAG shows all three are associated to each other, i.e. \nIt would be natural to think about measuring correlation and if a pair shows no correlation you could discard the DAG, but it is NOT a good test since there are many ways two variables can show correlation yet not be directly associated. (see reasoning under DAG 2 above and under DAG2 below)\nDAG1 has NO conditional independencies and therefore, NO testable implications\n\nDAG 2\n\nThis DAG also shows all three variables are associated with each other.\nD and M are associated with one another, because A influences them both. They share a cause, and this leads them to be correlated with one another through that cause. But suppose we condition on A. All of the information in M that is relevant to predicting D is in A. So once we’ve conditioned on A, M tells us nothing more about D\nThe testable implication is that D is independent of M, conditional on A, i.e. \n\n(Conditioning on A does not make D independent of M, because M really influences D all by itself in this model.)\n\ni.e A and M are marginally dependent\n\n\n\nOnly difference between both DAGs is the conditional independence in DAG2.\n\nTest\n\nRun a multiple regression D ~ α + βMM + βAA\nIf the effect measured from regression 1 disappears in the multiple regression, then we can discard DAG 1. If the effect remains, then we discard DAG 2.\n\n\nDAGs that are consistent with the data associations (M & N are associated but the causal relationship isn’t known)\n\nwhere U is an unknown variable. Unobserved variables are circled.\n\nAll three DAGs have no conditional independencies and therefore not testable implications\n\nA set of DAGs, each with the same conditional independencies known as a Markov Equivalence\n\nData cannot eliminate any of these DAGS. Domain knowledge must be used to reduce the number of Markov Equivalent DAGs.\n\n\n\n\n“Shutting the backdoor” to potential confounding paths\n\nSection 6.4\nwww.dagitty.net - Enter DAG and it will give you the Adjustment Set and Testable Implications\nRecipe\n\nList all of the paths connecting X (the potential cause of interest) and Y (the outcome).\nClassify each path by whether it is open or closed. A path is open unless it contains a collider.\nClassify each path by whether it is a backdoor path. A backdoor path has an arrow entering X.\nIf there are any backdoor paths that are also open, decide which variable(s) to condition on to close it.\n\nIf you have a choice between two variables where conditioning on either will close a backdoor path and one of them is causal to the outcome variable, then condition on the variable that is causal to the outcome variable. It will add precision to the estimate of the treatment effect.\nAny frontdoor paths that lead to backdoor paths must also be closed (see Misc &gt;&gt; Table 2 fallacy)\n\n\nExamples:\n\n\n\nProblem: We want to measure the causal effect of X –&gt; Y\nPotential confounding paths: XUAC, XUBC\n\nXUAC doesn’t have a collider so a variable needs conditioned on (aka adjusted for)\n\nU is unobserved, so either A or C. C directly influences Y, so it’s more efficient and will “aid in precision.”\n\nXUBC has a collider, B. So, no need to condition on any variable\n\nSolution: Y ~ a + X + C\n\nlibrary(dagitty)\ndag_6.1 &lt;- dagitty( \"dag { \n    U [unobserved]\n    X -&gt; Y\n    X &lt;- U &lt;- A -&gt; C -&gt; Y\n    U -&gt; B &lt;- C\n}\")\nadjustmentSets( dag_6.1 , exposure=\"X\" , outcome=\"Y\" )\n#&gt; { C }\n#&gt; { A }\n\n\nProblem: We want to measure the causal effect of the number of Waffle Houses, W, on Divorce, D.\nPotential confounding paths: WSM, WSA, WSMA (Also WSAM but McElreath on says there are 3. Maybe a combo of same letters is equivalent?)\n\nWSM doesn’t have a collider and therefore either S or M needs conditioned on\nWSA doesn’t have a collider and therefor either S or A needs conditioned on\nWSMA has a collider, M. So that path is blocked\nM is a choice for WSM but it’s a collider so it’s out. S is in both WSM and WSA, so conditioning on it kills two birds.\n\nSolution: D ~ a + W + S\n\nlibrary(dagitty)\ndag_6.2 &lt;- dagitty( \"dag {\n    A -&gt; D\n    A -&gt; M -&gt; D\n    A &lt;- S -&gt; M\n    S -&gt; W -&gt; D\n}\")\nadjustmentSets( dag_6.2 , exposure=\"W\" , outcome=\"D\" )\n#&gt; { A, M }\n#&gt; { S }\n\nEvidently conditioning on A and M is also a solution\n\nConditioning on M does close WSM but would then open WSMA. So, by then conditioning on A which is on a fork (or pipe depending on the path) it closes WSMA.\n\nIn his brms ebook, Kurz fits these regressions and a couple others for comparison. There wasn’t a consensus point estimate for W in the regressions that adjust for S and A + M.\n\nMcElreath mentions, “This DAG is obviously not satisfactory–it assumes there are no unobserved confounds, which is very unlikely for this sort of data.”\nThe inconsistent point estimates are probably do to an omitted variable(s) that is confounding the regression.\n\nConditional independencies:\nimpliedConditionalIndependencies( dag_6.2 )\n#&gt; A _||_ W | S\n#&gt; D _||_ S | A, M, W\n#&gt; M _||_ W | S",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-othart",
    "href": "qmd/causal-inference.html#sec-causinf-othart",
    "title": "Causal Inference",
    "section": "Other Articles",
    "text": "Other Articles\n\nFrontdoor Adjustment\n\nFrom http://arelbundock.com/posts/frontdoor/\nUseful when an unobserved confounder creates a backdoor path that prevents direct causal estimation\nIn a causal chain with three nodes X→Z→Y, we can estimate the effect of X on Y indirectly by combining two distinct quantities:\n\nThe estimate of the effect of X on Z, P(Z|do(X))\nThe estimate of the effect of Z on Y, P(Y|do(Z), X)\n\nAssumptions\n\nFull mediation: there is no direct path from X to Y, except through Z.\nUn-confoundedness 1: There is no open backdoor from X to Z.\nUn-confoundedness 2: All backdoors from Z to Y are blocked by X\n\nExample: 1\n\nOur goal is to estimate P(Y|do(X)). Unfortunately, this relationship between X and Y is confounded by the unobserved variable U, via this backdoor path: X←U→Y. Therefore, we cannot estimate the causal quantity of interest directly.\n\n\ncause X, a mediator Z, an outcome Y, and an unobserved confounder U\n\nlibrary(data.table)\nset.seed(731460) \nN = 1e5\nU = rbinom(N, 1, prob = .2)\nX = rbinom(N, 1, prob = .1 + U * .6)\nZ = rbinom(N, 1, prob = .3 + X * .5)\nY = rbinom(N, 1, prob = .1 + U * .3 + Z * .5)\ndat = data.table(X, Z, Y)\n\n# truth\ncoef(lm(Y ~ X + U))[\"X\"]\n## 0.2549541\nEstimate the effect of X on Z, P(Z|do(X))\nstep1 = lm(Z ~ X, dat)\nEstimate the effect of Z on Y, P(Y|do(Z), X)\nstep2 = lm(Y ~ Z + X, dat)\nCombine both estimates by multiplication\ncoef(step1)[\"X\"] * coef(step2)[\"Z\"]\n## 0.2496002\n\nExample 2\n\nSame as first example but using {dosearch} package\nlibrary('dosearch')\n   data1 &lt;- \"P(X, Y, Z)\"\nquery1 &lt;- \"P(Y | do(X))\"\ngraph1 &lt;- \"U -&gt; X\n          U -&gt; Y\n          X -&gt; Z\n          Z -&gt; Y \"\n   # compute\n   frontdoor &lt;- dosearch(data1, query1, graph1)\n   frontdoor\n\nOutput:\n\nEstimate the causal effect\ndat[, `P(X)`    := fifelse(X == 1, mean(X), 1 - mean(X)) ][\n    , `P(Z|X)`  := mean(Z), by = X                      ][\n    , `P(Y|Z,X)` := mean(Y), by = .(Z, X)                ][\n    , `P(Z|X)`  := mean(Z), by = X                      ][\n    , Y := NULL                                          ]\ndat = unique(dat)\ndat[, `P(Y|do(Z))` := sum(`P(Y|Z,X)` * `P(X)`), by = Z]\n`P(Y|do(X=0))` = with(dat[X == 0], \n  `P(Z|X)`          [Z == 1] * \n  `P(Y|do(Z))`      [Z == 1] +\n  (1 - `P(Z|X)`)    [Z == 0] * \n  `P(Y|do(Z))`      [Z == 0]\n)\n`P(Y|do(X=1))` = with(dat[X == 1], {\n  `P(Z|X)`          [Z == 1] * \n  `P(Y|do(Z))`      [Z == 1] +\n  (1 - `P(Z|X)`)    [Z == 0] * \n  `P(Y|do(Z))`      [Z == 0]\n})\n`P(Y|do(X=1))` - `P(Y|do(X=0))`\n## 0.249766\nComparison\n\nTruth: 0.2549541\nlm: 0.2496002\ndosearch: 0.249766",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/business-plots.html",
    "href": "qmd/business-plots.html",
    "title": "Business Plots",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Business Plots"
    ]
  },
  {
    "objectID": "qmd/business-plots.html#sec-bizplots-misc",
    "href": "qmd/business-plots.html#sec-bizplots-misc",
    "title": "Business Plots",
    "section": "",
    "text": "{modelplotr}\n\nGithub, Vignette\nNice implementations but package is not maintained\nNotes for marketing and financial graphs taken from articles and vignettes introducing that package.",
    "crumbs": [
      "Business Plots"
    ]
  },
  {
    "objectID": "qmd/business-plots.html#sec-bizplots-market",
    "href": "qmd/business-plots.html#sec-bizplots-market",
    "title": "Business Plots",
    "section": "Marketing Plots",
    "text": "Marketing Plots\n\nTL;DR - Most useful/popular are the Cumulative Gains and Cumulative Response graphs.\nThe example objective is to select the customers of a bank that are most likely to respond to an offer to purchase a “term deposit”. The outcome is binary: “term deposit” or “no”\nInformation from models used in these plots\n\nPredicted probability for the target class\nX-Axis: Equally sized groups based on this predicted probability\n\ne.g. Splitting observations into deciles. Top 10% in predicted probability for target class would be in the first decile.\n\nNumber of observed target class observations in these groups\n\nThe test dataset is used for the plots to get a realistic idea of what a marketing campaign in the would would produce.\n\nResponse Plot has some GOF capability so I could maybe see using the validation set with that plot to compare models with.\n\n\n\nCumulative Gains\n\n\nAKA Gains Plot\nAnswers the question: “When we apply the model and select the best X quantiles, what % of the actual target class observations can we expect to target?”\n\ny-axis = % of positive events (1s in binary classification) out of the entire dataset\n\nHow to apply:\n\nChoose a probability threshold (i.e. the corresponding quantile on the x-axis). The graph shows the percentage of observations on the y-axis that are within that threshold\nChoose the percentage of customers that you can afford to target with your campaign. The corresponding quantile on the x-axis shows the quantile and therefore the associated probability of positive result.\n\n“When we select 20% with the highest probability according to gradient boosted trees, this selection holds 87% of all term deposit cases in test data.”\n\nSays using the top 20% will include 87% of all the 1s (in binary classification) in the entire dataset.\n\n\nIf the gains is 87%, then there are potentially 13% of the total 1s that won’t be included in the campaign if we only target the top 20% percent.\n\nwizard model (perfect model) line - line takes steepest route to 100% on y-axis as possible, depending on the percentage of your outcome variable is the target level.\n\nFor the graph above, it looks like around 12% of the outcome variable values are the positive event case since the line reaches the 100% on the y-axis a little past the 1st decile. So the perfect model predicts all those values as being the positive class.\n\n\n\n\nCumulative Lift\n\n\nAKA Index or Lift Plot\nEspecially useful for companies with little to no experience with data models\nAnswers the question: “When we apply the model and select the best X quantiles, how many times better is that than using no model at all?”\n“no model at all” (i.e. coin flip) is a random model (also seen in the gains plot) is represented by a horizontal line at y = 1 or 100% depending on how the y-axis is specified. It is the ratio of the % of actual target category observations in each quantile to the overall % of actual target category observations after randomization of the rows of the data set.\nThe amount of lift can’t be generalized to all models and all data sets. So there aren’t guidelines as to what is a “good” lift score and what isn’t. If 50% of your data belongs to the target (positive) class of interest, a perfect model would ‘only’ do twice as good (lift: 2) as a random selection. If 10% of the data belong to the positive class, then lift = 10 or 1000% is the best possible lift score.\nHow to apply:\n\nChoose a quantile (x-axis) and the corresponding y value can be used to explain to stakeholders how many times or what percent better this model is at selecting the top prospects than random selection.\n\n“A term deposit campaign targeted at a selection of 20% of all customers based on our gradient boosted trees model can be expected to have a 4 times higher response (434%) compared to a random sample of customers.”\n\n\n\n\n\nResponse Plot\n\n\nPlots the percentage of *target class* observations per quantile\n\nnote: the cumulative gains y-axis is total observations where this plot’s y-axis is just positive class (1s in a binary classification model)\n\nAnswers the question: “When we apply the model and select quantile X, what is the expected % of target class observations in that quantile?” but also gives information about the model fit.\nHow to apply:\n\nThis plot is more important in what it tells about the model fit than what it says about how many observations are in a particular quantile\n\nA good fitting model will have a sharp sloping line with the highest response % in the lower quantiles. This says that the model is giving high probability scores to the vast majority of the positive class observations\nFor model comparison: the earlier the line crosses the horizontal (random model) line should indicate a steeper slope and therefore a better fit.\n\n“When we select decile 1 (10th percentile) according to model gradient boosted trees in dataset test data the % of term deposit cases in the selection is 51%.”\nThe horizontal line represents a random model (i.e. the % of target class cases in the total set)\n\nFrom the quantile where the line intersects the horizontal dashed-line and onwards, the % of target class cases is lower than a random selection of cases would hold.\n\n\n\n\n\nCumulative Response\n\n\nAnswers the question: “When we apply the model and select up until quantile X, what is the expected % of target class observations in the selection?\n\nOften used to decide - together with business colleagues - up until what decile to select for a marketing campaign\n\nHow to apply:\n\n“When we select quantiles 1 until 30 according to model gradient boosted trees in dataset test data, the % of term deposit cases in the selection is 36%.”\n\nIn other words, targeting these customers should produce a response rate (percent of customers purchasing a term deposit) of 35% on average as compared to randomly selecting the same number of customers which is 12% (term deposits/total obs for the test set).\nThe y-axis is the percentage of 1s (in binary classification) in that subset (quantiles from 1 to 30). Different from cumulative gains where the y-axis is the percentage of 1s in the entire dataset.\n\nIs that response big enough to have a successfull campaign, given costs and other expectations? Will the absolute number of sold term deposits meet the targets? Or do we lose too much of all potential term deposit buyers by only selecting the top 30%? To answer that question, we can go back to the cumulative gains plot.\nThe dashed horizontal is the same as in the Response Plot",
    "crumbs": [
      "Business Plots"
    ]
  },
  {
    "objectID": "qmd/business-plots.html#sec-bizplots-fin",
    "href": "qmd/business-plots.html#sec-bizplots-fin",
    "title": "Business Plots",
    "section": "Financial Plots",
    "text": "Financial Plots\n\nExample objective is to select the customers of a bank that are most likely to respond to an offer to purchase a “term deposit”. The outcome is binary: “term deposit” or “no”\n\nfixed costs = $75,000 (a tv commercial and some glossy print material)\nvariable costs per unit = $50 (customers are given an incentive to buy)\nprofit per unit = $250\n\nInformation from models used in these plots\n\nSame stuff as Marketing Plots\nFixed Costs (e.g. sales force expenses, advertising campaigns, sales promotion, and distribution costs)\nVariable Costs per unit (e.g.sales commission, bonuses, and performance allowances)\nProfit per Sale\n\nThe test dataset is used for the plots to get a realistic idea of what a marketing campaign in the would would cost and return. A validation set could be used on the Revenue and Costs Plot and models could be compared based risk of nonprofitability.\n\n\nProfit Plot\n\n\nAnswers the question: “When we apply the model and select up until quantile X, what is the expected profit of the campaign?”\nHow to apply:\n\nThe most profitable quantile is the one directly under the apex of the curve.\nThe most profitable quantile is highlighted by default, but this can be specified if so desired\nannotation means?\n\n\n\n\nCosts and Revenues Plot\n\n\nAnswers the question: “When we apply the model and select up until decile X, what are the expected revenues and investments of the campaign?”\nThe costs are the cumulative costs of selecting up until a given decile and consist of both fixed costs and variable costs.\nThe revenues take into account the expected response % - as plotted in the cumulative response plot - as well as the expected revenue per response.\nSolid curve is the revenue and the dashed diagonal line is the total costs\nHow to apply:\n\nThe campaign is profitable in the plot area where revenues exceed costs.\nGives an idea of the range of spending that can be considered while the campaign remains profitable. Ranges could be associated with risk. The smaller the range, the greater the risk given the uncertainty of the models. Various campaign ranges could be compared based on this risk.\nSee profits plot for optimal quantile.\n\n\n\n\nROI Plot\n\n\nAnswers the question: “When we apply the model and select up until decile X, what is the expected % return on investment of the campaign?”\nThe quantile at which the campaign profit is maximized is not necessarily the same as the quantile where the campaign ROI is maximized\n\nIt can be the case that a bigger selection (higher decile) results in a higher profit, however this selection needs a larger investment (cost), impacting the ROI negatively.\nSo maximum ROI can be considered the most effficient use of resources, but it takes money to make (the most) money.\n\nBasic formula for ROI = Net Profit / Total Investment * 100\nHow to apply:\n\nThe quantile directly underneath the apex of the curve is where the ROI is maximized.",
    "crumbs": [
      "Business Plots"
    ]
  },
  {
    "objectID": "qmd/web-design.html",
    "href": "qmd/web-design.html",
    "title": "Web Design",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Web",
      "Web Design"
    ]
  },
  {
    "objectID": "qmd/web-design.html#sec-webdes-misc",
    "href": "qmd/web-design.html#sec-webdes-misc",
    "title": "Web Design",
    "section": "",
    "text": "Resources\n\nWebsite Inspiration Catelog\n\nFirst-load under five seconds budget (guideline for non-highend mobile devices)\n\nJavaScript-heavy content: ~1.3MB, with 650KB of HTML, CSS, images, and fonts and 650KB of JavaScript\nMarkup-centric stacks: 2.5MB, with 2.4MB of HTML, CSS, images, and fonts and 100KB of JavaScript.\nSee hrbrmstr daily drop for more details on how developer tools can be used to analyze the size of webpages.\n\nUsing “Brand” to help choose font, palette, and imagery\n\nNotes from Erik Kennedy Video\nBrand is just adjectives to describe your business, organization, etc.\n\ne.g. Trustworthy, Geeky, Casual, Precise, Fun, Technical, etc.\n\nCommon Brands\n\n\n“Neat, modern”, “Luxury, formal”, etc. are more of what I’d consider brand adjectives\n“Clean & Simple”,“Fancy”, “Techie”, etc. are how I’d describe the sites that epitomize those brands, but they could also be brand descriptors\n\nBlending Brands\n\n\nShows names of company websites that most represent the brand/website types\ne.g. The Apple website is a blend of Techie and Fancy.\n\n\nWebsites should be under 14kb (article)\n\nMost web servers TCP slow start algorithm starts by sending 10 TCP packets which works out to 14kb\n\n404 pages\n\nGuidelines\n\nbe brief: the message on the 404 page should be straightforward and easy to understand, informing the user that the page they were trying to access is not available.\nbe contrite: the tone of the 404 page should be friendly and apologetic, acknowledging the user’s inconvenience and expressing empathy.\nbe helpful: provide links to other areas of the website or a search box that can help users find what they’re looking for quickly and easily.\nbe informative: include contact information, such as a feedback form, social media account, or email address to give users an alternative way to reach out to you for assistance.\nbe you: incorporate your brand’s visual identity, including logos and colors, to help reinforce brand recognition and create a cohesive user experience.\n\nExamples\n\n404 Page SVG Animations That Maximize Visitor Retention\n21 Stunning 404 Pages to Convert Lost Visitors 2023\nguinslym/awesome-404: A curated list of awesome 404 web pages greynoise’s ‘404’ equivalent and hrbmstr’s.",
    "crumbs": [
      "Web",
      "Web Design"
    ]
  },
  {
    "objectID": "qmd/git-general.html",
    "href": "qmd/git-general.html",
    "title": "25  General",
    "section": "",
    "text": "25.1 Misc",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#misc",
    "href": "qmd/git-general.html#misc",
    "title": "25  General",
    "section": "",
    "text": "View HTML file in browser\n\nSyntax: “https://raw.githack.com/&lt;acct name&gt;/&lt;repo name&gt;/&lt;branch name&gt;/&lt;directory name&gt;/&lt;file name&gt;.html”\n\nInstalling from a git repo (From link)\n\nMake a fork of the repo and then clone it to your local machine.\nTo update, after setting an upstream remote (git remote add upstream git://github.com/benfulcher/hctsa.git) you can use git pull upstream main.\nTo update the submodule in the repo, git submodule update --init\n\nStart R project and Git repo in whichever order (I think)\n\nCreate R project in RStudio\n\nChoose “New Directory” for all the templated projects (e.g. quarto book, shiny, etc.). None of the other choices have them.\n\nIf you’ve already created a directory, it will NOT overwrite this directory or add to it. So you’ll either have alter the name of your old directory or choose a new name.\n\n\nCreate repo on Github\n\nAdd license and readme\n\nDo work\nTools &gt;&gt; Version Control &gt;&gt; Project Set-up &gt;&gt; Version Control System &gt;&gt; Select Git\nOpen terminal and go to working directory of project\ngit checkout -B main\ngit pull origin main --allow-unrelated-histories\ngit add .\ngit commit -m \"initial commit\"\ngit push --set-upstream origin main \n\nTurn off “LF will be replaced by CRLF the next time Git touches it”\n\nMessage spams terminal when committing changes from a window machines. Has to do with line endings in windows vs unix.\nTurn off: git config core.autocrlf true\nSee SO post for more details\n\nURL format to download files from repositories\n\nhttps://raw.githubusercontent.com/user/repository/branch/filename\n\n# Or evidently this way works too\n# adds ?raw=true to the end of the url\nfeat_all_url &lt;- url(\"https://github.com/notast/hierarchical-forecasting/blob/main/3feat_all.RData?raw=true\")\nload(feat_all_url)\nclose(feat_all_url)\nGet filelist from repo and download to a directory\n\n** Directory urls change as commits are made **\n\nlibrary(httr)\n\n# example: get url for the data dir of covidcast repo\nreq &lt;- httr::GET(\"https://api.github.com/repos/ercbk/Indiana-COVIDcast-Dashboard/git/trees/master?recursive=1\") %&gt;% \n  httr::content()\n# alphabetical order\ntrees &lt;- req$tree %&gt;% \n  map(., ~pluck(.x, 1)) %&gt;% \n  as.character()\n# returns 20 which is first instance, so 19 should the \"data\" folder\ndetect_index(trees, ~str_detect(., \"data/\"))\n# url for data dir\nreq$tree[[19]]$url\n\n# example\n# Get all the file paths from a repo\nreq &lt;- GET(\"https://api.github.com/repos/etiennebacher/tidytuesday/git/trees/master?recursive=1\")\n# any request errors get printed\nstop_for_status(req)\nfile_paths &lt;- unlist(lapply(content(req)$tree, \"[\", \"path\"), use.names = F)\n# file_path wanted &lt;- filter file path to file you want\n# gets the very last part of the path\nfile_wanted &lt;- basename(file_path_wanted)\norigin &lt;- paste0(\"https://raw.githubusercontent.com/etiennebacher/tidytuesday/master/\", file_wanted)\ndestination &lt;- \"output-path-with-filename-ext\"\n# if file doesn't already exist, download it from repo into destination\nif (!file.exists(destination)) {\n      # if root dir doesn't exist create it\n      if (!file.exists(\"_gallery/img\")) {\n        dir.create(\"_gallery/img\")\n      }\n      download.file(origin, destination)\nThe insides of .git",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#optimizations",
    "href": "qmd/git-general.html#optimizations",
    "title": "25  General",
    "section": "25.2 Optimizations",
    "text": "25.2 Optimizations\n\nFor large repos, simple actions, like running git status or adding new commits can take many seconds. Cloning repos can take many hours.\nBenefits\n\nIt improves the overall performance of your development workflow, allowing you to work more efficiently. This is especially important when working with large organizations and open source projects, where multiple developers are constantly committing changes to the same repository. A faster repository means less time waiting for Git commands such as git clone or git push to finish. It helps to optimize the storage space, as large files are replaced by pointers which take up less space. This can help avoid storage issues, especially when working with remote servers.\n\nMisc\n\nSee How to Improve Performance in Git: The Complete Guide\n\nExplainer, config settings, advanced gc, checkout, and clone commands\n\n\nUse .gitignore\n\nGenerated files, like cache or build files\n\nThey will be modified at each different generation — and there’s no need to keep track of those changes.\n\nThird-party libraries\n\nInstead, aim for a list of the required dependencies (and the correct version) so that everyone can download and install them whenever the repo is cloned.\n\nFor example, with a package.json file for JavaScript projects you can (and should) exclude the /node_modules folder.\n.DS_Store files (which are automatically created by macOS) are another good candidate\n\n\n\nGit LFS\n\nDesigned specifically to handle large file versioning. LFS saves your local repositories from becoming unnecessarily big, preventing you from downloading unnessary data.\n\nGit LFS intercepts any large files and sends them to a separate server, leaving a smaller pointer file in the repository that links to the actual asset on the Git LFS server.\n\nThis is an extension to the standard Git feature set, so you will need to make sure that your code hosting provider supports it (all the popular ones do).\nAlso need to download and install the CLI extension on your machine before installing it in your repository.\nSet-Up\n$ git lfs install\n$ git lfs track \"*.wav\"\n$ git lfs track \"images/*.psd\"\n$ git lfs track \"videos\"\n$ git add .gitattributes\n\nTells Git LFS which file extensions it should manage.\n.gitattributes notes the file names and patterns in this text file and, just like any other change, it should be staged and committed to the repository.\nCan now add files and commit as normal\nList all file extensions being tracked: git lfs track\nList all files being managed: git lfs ls-files\n\n\nDon’t download the version history if you don’t need to\n\ngit clone –depth 1 gitj@github.com:name/repo.git",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#troubleshooting",
    "href": "qmd/git-general.html#troubleshooting",
    "title": "25  General",
    "section": "25.3 Troubleshooting",
    "text": "25.3 Troubleshooting\n\nDiverged Branches\n\n\nKeeps asking for username/password when pushing\n\nSolution: You (or if you used usethis::use_github/git) probably set-up a https connection when you need a ssh connection.\n\nsee https://docs.github.com/en/get-started/getting-started-with-git/managing-remote-repositories#changing-a-remote-repositorys-url to change from https to ssh.\n\n\nUndo a commit, but save changes made (e.g. you forgot to pull before you pushed)\n\nSteps\n\ngit log - Shows commit history. Copy the hash for your last commit\ngit diff &lt;last commit hash&gt; &gt; patch - save the diff of the latest commit to a file\ngit reset --hard HEAD^ to revert to the previous commit\n\n**After this, your changes will be lost locally **\n\ngit log - confirm that you are now at the previous commit\ngit pull - correct the mistake you made in first place\npatch -p1 &lt; patch - apply the changes you originally made\ngit diff - to confirm that the changes have been reapplied\nNow, you do the regular commit, push routine\n\n\nUndo uncommitted changes: git stash followed by git stash drop\n\n“but only use if you commit often” - guessing this is not good if your commit is somehow large and/or involves multiple files\n\nSearch commits by string: git log --grep &lt;string&gt;",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#pulling",
    "href": "qmd/git-general.html#pulling",
    "title": "25  General",
    "section": "25.4 Pulling",
    "text": "25.4 Pulling\n\nSave your changes, pull in an update, apply your changes\ngit stash\ngit pull\ngit stash pop\n\ngit stash pop throws away the (topmost, by default) stash after applying it, whereas\ngit stash apply leaves it in the stash list for possible later reuse (or you can then git stash drop it).\n\nRe potential merge conflicts\n\n“For instance, say your stashed changes conflict with other changes that you’ve made since you first created the stash. Both pop and apply will helpfully trigger merge conflict resolution mode, allowing you to nicely resolve such conflicts… and neither will get rid of the stash, even though perhaps you’re expecting pop too. Since a lot of people expect stashes to just be a simple stack, this often leads to them popping the same stash accidentally later because they thought it was gone.”\n\nPulling is fetching + merging\n\nFetching just gets the info about the commits made to the remote repo\ngit fetch origin\nSome technical discussion for always using git pull –ff\n\nhttps://blog.sffc.xyz/post/185195398930/why-you-should-use-git-pull-ff-only-git-is-a\nhttps://megakemp.com/2019/03/20/the-case-for-pull-rebase/\nit’s still confusing but pull rebase sounds fine to me\n–global tag says do it for all my repos\nnot sure what the true and only are for\n\ngit pull –help will open doc in browser\n\n\nPulling by rebase\n\nLocal: using this method as default\ngit config pull.rebase true\ngit pull\nRemote\ngit pull --rebase\n\nPulling by fast-forward\n\nLocal: using this method as default\ngit config --global pull.ff only\ngit pull\nRemote\ngit pull --ff",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#branching",
    "href": "qmd/git-general.html#branching",
    "title": "25  General",
    "section": "25.5 Branching",
    "text": "25.5 Branching\n\nMisc\n\nCreate a new branch for each ticket you are working on or each data model. It can get sloppy when you put all your code changes on one branch.\n\nCreate a branch (e.g. “testing”)\ngit branch testing\nWork in a branch\ngit checkout testing\nThe files in your working directory change to the version saved in that branch\nIt adds, removes, and modifies files automatically to make sure your working copy is what the branch looked like on your last commit to it.\nCreate and work in a branch\n# new way\ngit switch -c testing\nor\ngit checkout -b testing\nor\ngit branch testing\ngit checkout testing\ncreates the branch and switches you to working in that branch\nIf you did a bunch of changes in a codebase, only to realize that you’re working on `master`,  switch will bring those local changes with you to the new branch. So I guess they won’t affect master then.\n\nUnless If you already committed to main, then those changes are both in your new branch and in main. So you would still have to clean up the main branch.\n\nDeleting a branch\n\nlocal branch\ngit branch -d testing\n\nremote branch\ngit push &lt;remoteName&gt; --delete &lt;branchName&gt;\nSee existing branches\ngit branch\nSee what has been commited the remote repo branches\ngit fetch origin\ngit branch -vv\n“origin” is the name of the remote\nresult\ntesting    7e424c3 [origin/testing: ahead 2, behind 1] change abc \nmaster      1ae2a45 [origin/master] Deploy index fix\n* issue    f8674d9 [origin/issue: behind 1] should do it         \ncart        5ea463a Try something new\nformat: branch, last commit sha-1, local branch status vs remote branch status, commit message\nthe star indicates the HEAD pointer’s location (where you’re at, i.e. checkout)\ntesting branch\n\n“ahead 2” means  I committed twice to the local testing branch and this work has not been pushed to the remote testing branch repo yet.\n“behind 1” means someone has pushed a commit to the remote testing branch repo and we haven’t merged this work to our local testing branch\n\nGet the last 10 branches that you’ve committed to locally:\ngit branch --sort=-committerdate | head -n 10\nRename branch\n# change locally\ngit branch --move &lt;bad-branch-name&gt; &lt;corrected-branch-name&gt;\n# change remotely in repo\ngit push --set-upstream origin &lt;corrected-branch-name&gt;\n# confirm change\ngit branch --all\nHEAD determines to which branch new commits are added\n\nExample\n\n“testing” branch is created (not shown in above picture)\n\nHEAD points at “master” branch\n“master” branch and the new “testing” branch both point at commit, f30ab.\nf30ab commit points to previous commit 34ac2\n\nuser executes checkout to “testing” branch (not shown in picture)\n\nHEAD now points to testing branch\n\nuser commits 87ab2 (shown in pic)\n\n87ab2 is committed to the “testing” branch\n“testing” branch is now ahead of the “master” branch by 1 commit\n\n\nExample\n\nEverything above happens but now another user commits the master branch.\n\nBoth branches are in conflict. The testing branch is ahead and behind by 1 commit\n\n\n\nMerging\n\n\nNotes\n\nNEVER merge your branch locally on your machine with the master branch, ALWAYS merge online via pull request\n\nSteps\n\nPush final changes and use of a pull request\nSwitch to master branch locally and pull the merged changes\n\n\n\nUpdate branch with work that’s been done in master branch\n\nAfter updating your local branch, push to remote repo (no commit necessary)\n# while in branch\ngit merge master\n\n\nFast-Forward\n\nExample\n\nBefore the merge\n\nthe testing branch is 1 commit ahead of the master branch and the master branch doesnt have a new commit\n\nAfter the merge\n\nmaster is moved forward to the testing branch commit\n\n\nCode (merging work in branch with the master branch for production)\n# currently in test branch\ngit checkout master\ngit merge testing\n\nLines in file are marked\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.html\n# &lt;div id=\"footer\"&gt;contact : email.support@github.com&lt;/div&gt;\n# =======\n# &lt;div id=\"footer\"&gt;\n# please contact us at support@github.com\n# &lt;/div&gt;\n# &gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html\nAbove ======= is the master branch version of the code and below is the iss53 branch version\nMake necessary changes and save the file\ngit add . or git add &lt;resolved file&gt;\n\nTells git that conflict is resolved\n\nCheck status to confirm everything has been resolved\ngit status\n\n    On branch master\n    All conflicts fixed but you are still merging.\n      (use \"git commit\" to conclude merge)\n    Changes to be committed:\n      modified:  index.html\ngit commit\n\nno message required (there’s a default message) but you can add one if you want\n\nExample\n\niss53 branch ahead of master by 2 commits (c3, c5) and behind 1 commit (c2)\nSame code as Fast-Forward merge but git handles the merge a bit differently\ngit checkout master \ngit merge iss53\n\n\n\nC6 (right pic) is called a “merge commit.” Its created by git and points to two commits instead of one.\nNo need to merge with master (i.e. update local iss53 branch with c4 changes in master) before committing final changes\n\nIf there are changes in the same lines of code C4 and C5, then there will be a conflict (See below, Conflicts &gt;&gt; Example)\n\n\nConflicts\n\nExample\n\nChanged files in C4 (see above example) are in the same lines of the same files that you made changes to in C5\n\nRemember: you’re now in the master branch since you did checkout master as part of the merge code\nSteps\n\nCheck status to which files are causing the conflict (e.g. index.html)\ngit status\n  Unmerged paths:\n  (use \"git add &lt;file&gt;...\" to mark resolution) \n    both modified:      index.html\n\n\n\n\nMoving between branches\n\nfrom master to testing\ngit checkout testing\n\nlocal files are deleted and replaced with branch versions\n\nalternative: worktree\n\nExample\n\nWhat happens when you move from branch-a to branch-b\nBRANCH-A        BRANCH-B\nalpha.txt      alpha.txt\nbravo.txt\ncharlie.txt    charlie.txt\n                delta.txt\n\nbravo text is deleted from your local disc and delta.txt is added\nIf any changes to alpha.txt or charlie.txt have been made and no commit has been made, the checkout will be aborted\n\nSo either revert the changes or commit the changes\n\nUntracked files or newly created files\n\nIf you have branch-A checked out and you create a new file called echo.txt, Git will not touch this file when you checkout branch-B. This way, you can decide that you want to commit echo.txt against branch-B without having to go through the hassle of (1) move the file outside the repo, (2) checkout the correct branch, and (3) move the file back into the repo.",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#collaboration",
    "href": "qmd/git-general.html#collaboration",
    "title": "25  General",
    "section": "25.6 Collaboration",
    "text": "25.6 Collaboration\n\nAdd collaborators to your repository\nOne person invites the others and provides them with read/write access (github docs)\n\nSteps\n\nGo to the settings for your repository\nmanage access &gt;&gt; “invite a collaborator”\n\nSearch for each collaborator by full name, acct name, or email\nClick “Add &lt;name&gt; to &lt;repo&gt;”\n\nEach collaborator will need to accept the invitation\n\nSent by email",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html",
    "href": "qmd/db-normalization.html",
    "title": "Normalization",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html#sec-db-norm-misc",
    "href": "qmd/db-normalization.html#sec-db-norm-misc",
    "title": "Normalization",
    "section": "",
    "text": "Organizing according to data attributes to reduce or eliminate data redundancy (i.e. having the same data in multiple places).\n\nIt gives you a set of rules to be able to start categorizing your data and forming a layout\n\nBy establishing structure in a database, you are able to help establish a couple of important things: data integrity and scalability.\n\nIntegrity ensures that data is entered correctly and accurately.\nScalability ensures you have organized the data in a way that it is more computationally efficient when you start to run SQL queries.\n\nNotes from When Spreadsheets Aren’t Good Enough: A Lesson in Relational Databases\n\nGives an example of normalizing a dataset through a MySQL analysis\n\nPackages\n\n{{autonormalize}} - analyzes transaction df and creates relational tables - python library for automated dataset normalization",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html#sec-db-norm-terms",
    "href": "qmd/db-normalization.html#sec-db-norm-terms",
    "title": "Normalization",
    "section": "Terms",
    "text": "Terms\n\nDimension Tables - Contains data about how the data in Fact Table is being analyzed. They facilitate the fact table in gathering different dimensions on the measures which are to be taken.\nFact Tables - Contain data corresponding to any business process. Every row represents any event that can be associated with any process. It stores quantitative information for analysis",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html#sec-db-norm-form",
    "href": "qmd/db-normalization.html#sec-db-norm-form",
    "title": "Normalization",
    "section": "Forms",
    "text": "Forms\n\nDatabases are often considered as “normalized” if they meet the third normal form\nSee A Complete Guide to Database Normalization in SQL for details on the other 4 forms.\n\nAlso gives an example of normalizing a dataset through a posgresSQL analysis\n\nFirst normal form (1NF)\n\nEvery value in each column of a table must be reduced to its most simple value, also known as atomic.\n\nAn atomic value is one where there are no sets of values within a column. (i.e. 1 value per cell)\n\nThere are no repeating columns or rows within the database.\nEach table should have a primary key which can be defined as a non-null, unique value that identifies each row insertion. Second normal form (2NF)\nConforms to first normal form rules.\nAdjust columns so that each table only contains data relating to the primary key.\nForeign keys are used to establish relationships between tables. Third normal form (3NF)\nConforms to both first and second normal form rules.\nNecessary to shift or remove columns (attributes) that are transitively dependent, which means they rely on other columns that aren’t foreign or primary keys.",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html#sec-db-norm-schema",
    "href": "qmd/db-normalization.html#sec-db-norm-schema",
    "title": "Normalization",
    "section": "Schema",
    "text": "Schema\n\nMisc\n\nFactors that influence normalizing dimension tables\n\nData redundancy concerns: If minimizing redundancy is crucial, normalization might be preferred.\nQuery performance priorities: If query performance is paramount, denormalization often offers advantages.\nData consistency requirements: High consistency needs might favor normalization.\nMaintenance complexity: Denormalized dimensions can be simpler to maintain in some cases.\n\nDon’t use external IDs as primary keys\n\nSince you don’t control those IDs, they can change the format and break your queries.\n\n\nStar\n\n\nExample: A Star schema of sales data with dimensions such as customer, product & time.\nIn a star schema, as the structure of a star, there is one fact table in the middle and a number of associated dimension tables.\nThe fact table consists of primary information. It surrounds the smaller dimension lookup tables which will have details for different fact tables. The primary key which is present in each dimension is related to a foreign key which is present in the fact table.\nThe fact tables are in 3NF form and the dimension tables are in denormalized form. Every dimension in star schema should be represented by the only one-dimensional table.\n\nSnowflake\n\n\nSnowflake schema acts like an extended version of a star schema. There are additional subdimensions added to dimensions.\nUnlike the Star schema, dimensions are normalized.\nCan be slower than star schemas due to complex joins across multiple tables, but achieves better storage efficiency compared to star schemas due to reduced data redundancy.\nThere are hierarchical relationships and child tables involved that can have multiple parent tables.\nThe advantage of snowflake schema is that it uses small disk space. The implementation of dimensions is easy when they are added to this schema.\n\nFact Constellation or Galaxy\n\n\nA fact constellation can consist of multiple fact tables. These are more than two tables that share the same dimension tables — like connected Star schema.\nThe shared dimensions in this schema are known as conformed dimensions. Denormalization in shared dimension tables might increase storage size compared to fully normalized schemas.\nDimensions can be normalized but is rare in this schema due the level of complexity already present.\nUseful when aggregation of fact tables is necessary. Fact constellations are considered to be more complex than star or snowflake schemas. Therefore, more flexible but harder to implement and maintain. Joins across multiple fact and dimension tables can lead to complex queries with potential performance impacts.",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html#sec-db-norm-dsgn",
    "href": "qmd/db-normalization.html#sec-db-norm-dsgn",
    "title": "Normalization",
    "section": "Design",
    "text": "Design\n\nMisc\n\nOracle Data Model Documentation\n\nConsiderations\n\n7 Vs\n\nVolume: How big is the incoming data stream and how much storage is needed?\nVelocity: Refers to speed in which the data is generated and how quickly it needs to be accessed.\nVariety: What format the data needs to be stored? Structured such as tables or Unstructured such as text, images, etc.\nValue: What value is derived from storing all the data?\nVeracity:How trustworthy the data source, type and its processing are?\nViscosity: How the data flows through the stream and what is the resistance and the processability?\nVirality: Ability of the data to be distributed over the networks and its dispersion rate across the users_\n\nData Quality (See Database, Engineering &gt;&gt; Data Quality) completeness, uniqueness, timeliness, validity, accuracy, and consistency\n\nComponents\n\n\nMetamodeling:\n\nDefines how the conceptual, logical, and physical models are consistently linked together.\nProvides a standardized way of defining and describing models and their components (i.e. grammar, vocabulary), which helps ensure consistency and clarity in the development and use of these models.\nData ownership should be assigned based on a mapping of data domains to the business architecture domains (i.e. market tables to the marketing department?)\n\nConceptual Modeling - Involves creating business-oriented views of data that capture the major entities, relationships, and attributes involved in particular domains such as Customers, Employees, and Products.\nLogical Modeling - Involves refining the conceptual model by adding more detail, such as specifying data types, keys, and relationships between entities, and by breaking conceptual domains out into logical attributes, such as Customer Name, Employee Name, and Product SKU.\nPhysical Data Modeling - Involves translating the logical data model into specific database schemas that can be implemented on a particular technology platform\n\nProcess (article, article, article)\n\nUnderstand the Core Business Requirements\n\nCreate a catalogue of reporting stories for each stakeholder to an idea of the reports that each will want generated\n\nThese will inform you of the data requirements\ne.g. “As a marketing manager, I need to know the number of products the customer bought last year in order to target them with an upsell offer.”\n\nFrom the story above, I can determine that we will need to aggregate the number of products per customer based on sales from the previous year.\n\n\n\nSelect the tools and technologies:\n\nUsed to build and manage the data warehouse. This may include selecting a database management system (DBMS), data integration and extraction tools, and analysis and visualization tools.\nWarehouses - See Brands\nSee Production, Tools &gt;&gt;\n\nOrchestration\nELT/ETL Operations\n\n\nChoose a data model\n\nIdentify Business Processes\n\nFocus on business process and not business departments as many departments share the same business process\nIf we focus on department, we might end up with multiple copies of models and have different sources of truth.\n\nChoose a data model from the Business Process\n\nStart with the most impactful model with the lowest risk\n\nConsult with the stakeholders\n\nShould be used frequently and be critical to the business and also it must be built accurately\n\nDecide on the data granularity\n\nMost atomic level is the safest choice since all the types of queries is typically unknown\nNeed to consider the size and complexity of the data at the various granularities, as well as the resources available/costs for storing and processing it.\nExamples\n\nCustomer Level - easy to answer questions about individual customers, such as their purchase history or demographic information.\nTransaction Level - easy to answer questions about individual transactions, such as the products purchased and the total amount spent.\nDaily or Monthly?\n\n\n\nCreate Conceptual Data Models (Tables)\n\nThese represent abstract relationships that are part of your business process.\nExplains at the highest level what respective domains or concepts are, and how they are related.\nThe elements within the reporting stories should be consistent with these models\n\nExample: Retail Sales\n\nTime, Location, Product, and Customer.\n\nTime might be used to track sales data over different time periods (e.g. daily, monthly, yearly).\nLocation might be used to track sales data by store or region.\nProduct might be used to track sales data by product category or specific product.\nCustomer might be used to track sales data by customer demographics or customer loyalty status.\n\n\n\nExample:\n\n\nTransactions form a key concept, where each transaction can be linked to the Products that were sold, the Customer that bought them, the method of Payment, and the Store the purchase was made in — each of which constitute their own concept.\nConnectors show that each individual transaction can have at most one customer, store, or employee associated with it, but these in turn can be associated with many transactions (multi-prong connector into Transactions)\n\nExample:\n\n\nEach Customer (1 prong connector) can have 0 or more Orders (multi-prong connector)\nEach Order can have 1 or more Products\nEach Product can have 0 or more Orders\n\n\nCreate Logical Data Models\n\nBreakdown each entity of the conceptual model into attributes\n\nExample:\n\nExample:\n\n\n\nCreate Physical Data Models\n\nDetails are added on where exactly (e.g., in what table), and in what format, these data attributes exist.\n\ne.g. finalizing table names, column names, data types, indexes, constraints, and other database objects\n\nTranslate the logical data model into specific database schemas that can be implemented on a particular technology platform\n\ne.g. dimensional modelling in a star schema or normalisation in a 3rd normal form in a snowflake model.\n\nExample:\n\nExample: Dimension model in a star schema\n\n\nfact_ (quantitative) and dim_ (qualitative)\n\n\nMake Design and Environment decisions\n\nDecide on:\n\nPhysical data models\nHistory requirements\nEnvironment provisions & set up\n\n\nBuild a prototype (aka wireframe) of the end product\n\nThe business end-user may have a vision, they couldn’t coherently articulate at the requirement phase.\nThe prototype need not use real-world data or be in the reporting tool.\n\n** Profile known sources data **\n\nLearn about the data quality issues, and try and remediate those issues before designing your data pipelines.\n\nIf an issue cannot be resolved, you will have to handle it in your data pipeline\n\n\nBuild, Test, and Iterate\n\nCreate ETL jobs or data pipelines\n\nIteratively need to unit test the individual components of the pipeline.\n\nThe data will need to be moved from the source system into our physical warehouse\nProfile data\n\nData types, and if conversion is required\nThe amount of history that needs to be pulled\n\nValidate the model’s output numbers with the business end-user\n\nProgress towards Data Maturity (see Job, Organizational and Team Development &gt;&gt; Data Maturity)",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/cli.html",
    "href": "qmd/cli.html",
    "title": "CLI",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-misc",
    "href": "qmd/cli.html#sec-cli-misc",
    "title": "CLI",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly’s suggestions are prioritized in real time with a small neural network\n\nPath to a folder that’s above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder\n\nDebian vs. Ubuntu (from ChatGPT)\n\nStability vs. Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it’s known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu’s.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as “Debian spins,” catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-r",
    "href": "qmd/cli.html#sec-cli-r",
    "title": "CLI",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n      janitor::clean_names() %&gt;%\n      select(date, geo, county = name, negative, positive) %&gt;%\n      filter(geo == \"County\") %&gt;%\n      mutate(date = lubridate::as_date(date)) %&gt;%\n      select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it’ll search automatically\n35548",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-awk",
    "href": "qmd/cli.html#sec-cli-awk",
    "title": "CLI",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‘$13 &gt; 98’ adult_t.csv|head\nFilter lines with “Doctorate” and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-bash",
    "href": "qmd/cli.html#sec-cli-bash",
    "title": "CLI",
    "section": "Bash",
    "text": "Bash\n\nMisc\n\nNotes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn’t take notes on\n\nBash Scripting on Linux: The Complete Guide - video course\n\nResources\n\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n“&gt;” redirects the output from a program to a file.\n\n“&gt;&gt;” does the same thing, but it’s appending to an existing file instead of overwriting it, if it already exists.\n\n\n\n\n\nCommands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you’re using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you’re about to move does already exist under the new directory,\n\nThis way you don’t accidentally overwrite files that you didn’t mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\n\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\nOutput to file: echo “This is an example for redirect” &gt; file1.txt\nAppend line to file: echo “This is the second line of the file” &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to “tmp” directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name “my_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n“rwx” stand for read, write, and execute rights, respectively\nThe 3 “rwx” blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a “d” for directory or “l” for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - “super user” - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‘error’, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo “No such directory exist.Check”\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn’t exist, then the message “No such directory exists. check” message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for “error” and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for “error” in each line. Lines with “error” get written to “error_file.txt”\n\nFilter lines\ngrep -i “Doctorate” adult_t.csv |grep -i “Husband”|grep -i “Black”|csvlook\n# -i, --ignore-case-Ignore  case  distinctions,  so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i “Doctorate” adult_t.csv | wc -l\n\nCounts how many rows have “Doctorate”\n\n-wc is “word count”\n\n\n\n\n\n\nVariables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via “$”\n# local\nev_car=’Tesla’\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=’Tesla’\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it’s okay not to is on the left-hand-side of an [[ ]] condition. But even there I’d recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or –help or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this\n\n\n\n\n\nScripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like –silent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script’s directory close to the start of the script.\n\nAnd it’s usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck. Heed its warnings.\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don’t give executable permission to the script file.\nStarts with “#!” the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory “/bin”\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you’re good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n    #!/bin/bash\n    echo ‘Hello World’\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n    set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n    echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n    exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n    echo do awesome stuff\n}\nmain \"$@\"\n\n\n\nJob Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.\n\n\n\n\ntmux (‘terminal multiplexer’)\n\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‘moose’\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named “moose”\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you’ve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‘moose’\n\n\nPane Creation and Navigation\n\ncontrol+b then press ” (i.e. shift+’): add another terminal pane below\ncontrol+b then press % (i.e. shift+5) : add another terminal pane to the right\ncontrol+b then press → : move to the terminal pane on the right (similar for left, up, down)\n\n\n\n\nSSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n“ssh-keygen” is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named “id_rsa.pub” and a private key named “id_rsa”, and place both into your “~/.ssh” directory\nYou’ll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: “~/.ssh/config”\nContents\nHost dev\n  HostName remote\n  IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist\n\n\n\n\nVim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you’ll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you’re in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file\n\n\n\n\nPackages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to “upgrading” the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and “updates” them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade\n\n\n\n\nExpressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-powsh",
    "href": "qmd/cli.html#sec-cli-powsh",
    "title": "CLI",
    "section": "Powershell",
    "text": "Powershell\n\nComments: &lt;# comment #&gt;\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you’re already in the directory that you want the folder in. You can also use a path, e.g. \"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it’s a backslash (\\), but in Powershell, it’s a backtick ( ` )\n*Don’t forget that there’s a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it’s open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-batscri",
    "href": "qmd/cli.html#sec-cli-batscri",
    "title": "CLI",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g. timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-wsl",
    "href": "qmd/cli.html#sec-cli-wsl",
    "title": "CLI",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for –distribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html",
    "href": "qmd/cloud-services.html",
    "title": "4  Cloud Services",
    "section": "",
    "text": "4.1 Misc",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#misc",
    "href": "qmd/cloud-services.html#misc",
    "title": "4  Cloud Services",
    "section": "",
    "text": "See Cloud Costs Every Programmer Should Know for various service estimates in order to perform back-of-the-napkin calculations of project costs\nFor “Stead-State Workloads” requiring HPC, cloud compute doesn’t make economic sense\n\nSteady-State workloads are projects that are run near constantly\n\nSee Thread for discussion on scenarios, issues, and risks of your data center (DC) in the Cloud vs on-prem.\nExamples:\n\nAcademia: Where academics are in a queue to run experiments on the a HPC cluster\nWeather Forecasting: Forecasts are required nearly in real time, so these models run constantly\nFinancial transaction processing at a bank: The bank’s systems handle a constant stream of transactions\nInventory management system for a manufacturing plant: The system constantly receives updates on raw materials, production output, and finished goods.\nOthers: Week or two long analysis runs at hedge funds, genomic analysis jobs, a swath of AI training / fine tuning, Oil and Gas where they are plowing through seismic data constantly\n\n\n\nRStudio Server on your docker image allows you to access an ide connected to the server through a browser. Useful so you can make sure the correct packages are installed.\nServerless computing is a method of providing backend services on an as-used basis.\n\nA serverless provider allows users to write and deploy code without the hassle of worrying about the underlying infrastructure\nCharged based on their computation and do not have to reserve and pay for a fixed amount of bandwidth or number of servers, as the service is auto-scaling\ne.g. AWS Lambda (i.e. resources only get spun-up when an event is triggered)\n\nNVIDIA GPU Guide (thread)\n\nRTX 20-series or 30-series GPUs are forbidden from inclusion in data centers\nGeneral Recommendations (Oct 2022)\n\nA100 for model training\nT4 for inference workloads\n\nK80\n\nReleased in 2015, the K80 contained a lot of VRAM for the time (24 GB)\nCame before tensor cores and is relatively weak by today’s standards\nOnly okay for learning purposes\n\nP4\n\nReleased in 2016\nValue came from its low power consumption\nMay find it priced higher than its upgraded version (the T4), so recommended to avoid it\n\nT4\n\nReleased in 2018\nSignificant upgrade for inference workloads compared to the P4\nExtremely low power consumption, tensor cores, and plenty (16GB) of VRAM\nCheap, so if you have an inference workload, recommended to strongly consider a T4\n\nP100\n\nBig improvement for model training workloads over the K80 when released\nLess RAM (16GB) than K80\nWay more compute  than K80\n\nCan see memory savings from using mixed-precision training\n\nNo tensor cores\n\nV100\n\nHuge upgrade over the P100\nSame VRAM as P100 many but more CUDA cores\nIntroduces Tensor Cores\nMore cost-efficient than the P100\n\nA100\n\nnewest data center GPU\nupgraded tensor cores\nmost benchmarks show 3x+ faster training compared to the V100\n80GB VRAM\nPrice tag might be big, but it’s usually worth it over the V100",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#price-management",
    "href": "qmd/cloud-services.html#price-management",
    "title": "4  Cloud Services",
    "section": "4.2 Price Management",
    "text": "4.2 Price Management\n\nspot instances for cheaper machines\nautoscaling (kubernetes?) to handle peak usage times (spin-up more machines) while saving during slow times (spin down excess machines)\nUse opensource project management tools (dvc, airflow, etc)\nGoogle\n\nThe Google Kubernetes Engine (GKE) control plane is free, whereas Amazon’s (EKS) costs $0.20 an hour.\n\nAWS\n\nWith a well-defined framework of tag keys and values applied across different AWS resources, billing breakdowns by tag prove extremely useful for greater insight on the source of AWS charges — especially if resources are tagged by department, or team, or different layers of organizational granularity.\nReserved Instances - commit to specific configurations for one or three years at reduced cost\nSpot Instances - pay significantly lower costs but potential for applications to be interrupted\nSavings Plans\n\nEC2 Instance Savings Plans to reduce compute charges for specific instance types and AWS regions\n\nSavings of up to 72%\n\nCompute Savings Plans to reduce compute costs irrespective of type and region.\n\nSavings up to 66% and extends to ECS Fargate and Lambda functions.\n\n\nImage Management\n\nData Lifecycle Manager - automates the creation, retention, and deletion of images\n\nWill not manage images and snapshots created by other means, and it also excludes instance store-backed images.\nEC2 Recycle Bin - serves as a safety net to avoid the accidental deletion of resources — retaining images and snapshots for a configurable time where we may restore them before they are deleted permanently.\n\n\nLambda\n\nCloudwatch - Lambda automatically creates log groups for its functions, unless a group already exists matching the name /aws/lambda/[{functionName}]{style='color: #990000'}. These default groups do not configure a log retention period, leaving logs to accumulate indefinitely and increasing CloudWatch costs.\n\nExplicitly configure groups with matching names and a retention policy to maintain a manageable volume of logs.\n\nMemory Optimization - AWS Lambda Power Tuning can help to identify optimizations, albeit with notable initial costs given the underlying use of AWS Step Functions.\n\nLambda charges based on compute time in GB-seconds, where the duration in seconds is measured from when function code executes until it either returns or otherwise terminates, rounded up to the nearest millisecond. To reduce these times, we desire optimal memory configuration.\n\n\nS3 Lifecycle Configuration\n\nCharged for how much data stored, but also which S3 storage classes are utilized.\n\nStandard (default) class is the most expensive, permitting regular access to objects with high availability and short access times.\nInfrequent Access (IA) classes offer reduced cost for data which requires limited access (usually once per month)\nArchival options via Glacier deliver further cost reductions.\n\nConfiguring the lifecycle allows you to automatically transfer data to different storage classes and thereafter permanently delete it, X and Y days respectively after data creation",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#kaggle",
    "href": "qmd/cloud-services.html#kaggle",
    "title": "4  Cloud Services",
    "section": "4.3 Kaggle",
    "text": "4.3 Kaggle\n\nFree\n\n4-core CPU instances w/30 GB RAM\n2-core CPU, 2xT4 GPU w/13GB RAM\n\nT means tensor cores\n1 hour spent using 2xT4’s takes the same amount of your quota as a P100 (old free gpu offering)\n\nMeans 30-40 hours of free, multi-GPU compute per week",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#saturn-cloud",
    "href": "qmd/cloud-services.html#saturn-cloud",
    "title": "4  Cloud Services",
    "section": "4.4 Saturn Cloud",
    "text": "4.4 Saturn Cloud\n\nSaturn Cloud Recipes\n\nJSON files that specify your environment\nGood for keeping track of server dependencies (e.g. linux libraries)\n\nDunno about R packages",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#google-cloud-platform-gcp",
    "href": "qmd/cloud-services.html#google-cloud-platform-gcp",
    "title": "4  Cloud Services",
    "section": "4.5 Google Cloud Platform (GCP)",
    "text": "4.5 Google Cloud Platform (GCP)\n\nBigQuery sandbox is Google’s GCP free tier cloud SQL database. It’s free but your data only lasts 60 days at a time.\nGCP allows users to run deep learning workloads on TPUs\nSince data expires after 60 days, back-up the model coefficients and performance score tables to Google Sheets. Article suggested this is possible through WebUI.\nAs of Nov.19, regression, logistic regression, and k-nn are the only models available to be run with the sql query editor\nhttps://cloud.google.com/free/\n\n$300 credit for 12 months\nAlways free:\n\n2M requests for containers\n1 GB storage\n\nScalable NoSQL document database.\n50,000 reads, 20,000 writes, 20,000 deletes per day\n\nFunctions\n\n1 f1-micro instance per month (Available only in region: us-west1, Iowa: us-central1, South Carolina: us-east1)\n30 GB-months HDD\n5 GB-months snapshot in select regions\n1 GB network egress from North America to all region destinations per month (excluding China and Australia)\n\nKubernetes\n\nOne-click container orchestration via Kubernetes clusters, managed by Google.\nNo cluster management fee for clusters of all sizes\nEach user node is charged at standard Compute Engine pricing\n\nApp Engine\n\n28 instance hours per day\n5 GB Cloud Storage\nShared memcache\n1,000 search operations per day, 10 MB search indexing\n100 emails per day\n\nBigQuery\n\nFully managed, petabyte scale, analytics data warehouse.\n1 TB of querying per month\n10 GB of storage\n\nOther Stuff\n\nYour free trial credit applies to all GCP resources, with the following exceptions:\n\n* You can’t have more than 8 cores (or virtual CPUs) running at the same time.\n* You can’t add GPUs to your VM instances.\n* You can’t request a quota increase. For an overview of Compute Engine quotas, see Resource quotas.\n* You can’t create VM instances that are based on Windows Server images.\n\nYou must upgrade to a paid account to use GCP after the free trial ends. To take advantage of the features of a paid account (using GPUs, for example), you can upgrade before the trial ends. When you upgrade, the following conditions apply:\n\n* Any remaining, unexpired free trial credit remains in your account.\n* Your credit card on file is charged for resources you use in excess of what’s covered by any remaining credit.\nYou can upgrade your account at any time after starting the free trial. The following conditions apply depending on when you upgrade:\n* If you upgrade before the trial is over, your remaining credit is added to your paid account. You can continue to use the resources you created during the free trial without interruption.\n* If you upgrade within 30 days of the end of the trial, you can restore the resources you created during the trial.\n* If you upgrade more than 30 days after the end of the trial, your free trial resources are lost.\n\nSpot Instances (Preemptible VM)\n\nusage capped at 24 hrs\npricing is fixed and not market-driven\n\nGoogle price calculator: https://cloud.google.com/products/calculator/#id=3115f19f-4ff0-4c57-9028-69cb994fe7ca\nExample\n\ncreating a cluster with:\n\n1 x Dataproc cluster node with 30 GB of RAM\n3 x Dataproc worker nodes with 15 GB of RAM\nUsing less than 5 GB of disk space in a bucket\nAnd running the cluster for only 4 hrs\nWould cost only around $5 at the end of the month\n\n\nFree Tier\n\nincludes a 12-month free trial with $300 credit to use with any GCP services and an Always Free benefit, which provides limited access to many common GCP resources\nUse to test out, but KEEP EVERYTHING SMALL (data, hardware, etc). Need to upgrade it to see the true benefit. Free tier resources look like my desktop computer. Whatever cash is leftover should transfer to account.\nhttps://cloud.google.com/free/docs/gcp-free-tier#how-to-upgrade\nupgrade it from the free trial to a paid account through the GCP Console clicking the Upgrade button at the top of the page\n\n\n\nSteps for new project\n\nGo to interface https://console.cloud.google.com/\ncreate a project. “select a project” on top bar –&gt; “new project” on top right –&gt; choose name (optionally a folder/organization if you have one) –&gt; create\n(article wasn’t very reliable and went on talk about a python implementation so I stopped here\n\nTips\n\nApp Engine\n\nDon’t use App Engine Standard environments — big brother G wants you to use rather Flex environments, otherwise, they’ll punish you.\nReview cost analysis regularly to make sure there are no surprising costs.\nMake sure you clean up redundant App Engine application versions to prevent G from robbing you.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#microsoft-azure",
    "href": "qmd/cloud-services.html#microsoft-azure",
    "title": "4  Cloud Services",
    "section": "4.6 Microsoft Azure",
    "text": "4.6 Microsoft Azure\n\nhttps://azure.microsoft.com/en-us/free/?WT.mc_id=Revolutions-blog-davidsmi\nhttps://visualstudio.microsoft.com/dev-essentials/\n\nstarts azure trial but gives you free sql server developer edition\n\nWon’t be charged until you choose to upgrade.\n12 months access to $ services for free\n$200 credit for any service for 30 days\n\nAt the end of the 30 days, I think the remainder goes into your account after you change to a pay-to-play account\n\nAccess to the services that are always free\n\nAzure Kubernetes Service (AKS)\nFunctions\n\n1,000,000 requests per month\na solution for easily running small pieces of code in the cloud. You can write just the code you need for the problem at hand, without worrying about a whole application or the infrastructure to run it.\nExample use case: for handling WebAPI requests and sending the different data and results to where it needs to go.\n\nApp Service\n\n10 web, mobile, or API apps\n\nActive Directory B2C (identity)\n\n50,000 authentications per month\n\nMachine Learning Server\n\nDevelop and run R and Python models on your platform of choice.\n\nSQL Server 2017 Developer Edition\n\nBuild, test, and demostrate applications in a non-production environment.\n\nOther stuff\n\nBlob storage\n\nobject storage solution for the cloud\noptimized for storing massive amounts of unstructured data\n\nSpot Instances (Low Priority VM)\n\nnot time limit on instance usage\nno warning on termination by Azure\n\nTips\n\nIf you can’t create a service, because Azure servers are under maintenance for more than a couple of minutes — check out your permissions and registrations under the “Resource providers” panel.\nIf you see any strange errors on the Azure Portal — just change the filters’ values.\nIf you use Azure Machine Learning, and your scoring function cannot locate your source code — deliver the code as a Model and add it explicitly to the sys.path in the init function.\nIf you use Azure Machine Learning, don’t use Batch Endpoints — it looks like they are not ready yet — just use the regular Published Pipelines. In fact, “Batch endpoint” is just a wrapper around a published pipeline.\nDon’t include flask in your Azure conda environment specification.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#aws",
    "href": "qmd/cloud-services.html#aws",
    "title": "4  Cloud Services",
    "section": "4.7 AWS",
    "text": "4.7 AWS\n\nInstance types\n\nc-type instances are compute heavy\nr-type instances are RAM heavy\nm-type instances are balanced\n“Each thread is represented as a virtual CPU (vCPU) on the instance. An instance has a default number of CPU cores, which varies according to instance type. For example, an m5.xlarge instance type has two CPU cores and two threads per core by default—four vCPUs in total.”\nspot prices from 03/24/2020, all calculations over the previous month\ngen purpose\n\nm6g.8xlarge\n\ngen purpose, 32 vcpu, 128 gb\nnewer graviton, didn’t see any specs, but supposed to be much better than the xenon 1st gen\n\nm5.8xlarge\n\ngen purpose, 32 vcpu, 128 gb\nolder 3.1 ghz, xenon\non-demand $1.54/hr\n\nm5a.8xlarge\n\ngen purpose 32 vcpu, 128 gb\n2.4 ghz, slower processor speed than m5\n\nm5n.8xlarge\n\ngen purpose 32 vcpu, 128 gb\n3.1 ghz, xenon specialized for neural networks, ML tasks\nn.virg, 71% savings, &lt;5% interruption\nohio, 83% savings, &lt;5% interruption\non-demand $1.90/hr\npotential spot price = $0.32\n\nm5dn.8xlarge\n\nsame but with 2 ssd hard drives\n\nm4.10xlarge\n\ngen purpose 40 vcpu, 160 gb\n2.4 ghz\nsmaller write-up, get the sense these are older processors/instances\n\n\ncompute optimized\n\nRequires HVM AMIs that include drivers for ENA (network adaptor) and NVMe (ssd hard drives)\n\nseems standard on a lot of instances (gen purpose and here), shouldn’ t be an issue\n\nc5.9xlarge\n\n36 vcpu, 72 gb\n3.4 ghz\non-demand $1.53/hr\n\nc5d.9xlarge\n\nsame but with ssd\n\nc5n.9xlarge\n\n36 vcpu, 96 gb\n3.0 ghz, built for task needing high throughput for networking\non-demand, $1.94/hr\n\nc4.8xlarge\n\n36 vcpu, 60 gb\n2.9 ghz\n67% savings, &lt;5% interruption\non-demand $1.59/hr\npotential spot price = $0.52\n\n\nmemory optimized\n\nr5.8xlarge\n\n32 vcpu, 256 gb\n3.1 ghz\nn.virg, 72% savings, 5-10% interruption\nn.cal, 76% savings, &lt;5% interruption\non-demand $2.02/hr\npotential spot price = $0.48\n\nr5a.8xlarge\n\n32 vpu, 256 gb\n2.5 ghz\n\nr5n.8xlarge\n\n32 vcpu, 256 gb\n3.1 ghz, neural network optimized\nus.west. oregon 76% savings, 5-10% interruption\non-demand $2.38/hr\npotential spot price = $0.57\n\nr4.8xlarge\n\n32 vcpu, 244 gb\n2.3 ghz\n\nz1d.6xlarge\n\n24 vcpu, 192 gb\n4.0 ghz\non-demand $2.23\n\n\naccelerated computing\n\ninf1.6xlarge\n\n24 vcpu, 48 gb\nbuilt for ML\non-demand $1.91/hr\n\n\n\nFree Tier (12 months after sign-up)\n\naws.amazon.com – pricing (top) – free tier (mid) – create a free account (mid)\nEC2\n\n750 hrs/mo of t2-micro instance usage\n\nfor Linux, Windows, RHEL, SLES AMIs\n\n\nElastic Block Storage (EBS)\n\n30 GB\ncan be connected to an ec2\n\nElastic Container Registry\n\n500 MB per month\n\nfor storing and retrieving Docker images\nexample in course was a basic nginx image and it was 50MB\n\n\nS3\n\n5 GB of standard storage (high availability/ high durability)\n20,000 Get Requests, 2000 Put Requests per month\n\nElastic Load Balancing\n\n750 hrs per month shared between classic and application load balancers\n\nno idea what the differences are between classic and application\n\n\n\nPricing\n\nPrice per GPU as of 29-06-2023\n\n\nExamples\n\nr3.4xlarge 16 CPUs, 122 GB RAM, 1 x 320 SSD, Spot Price: $0.1517/h\n\nTrained H2O GBM, RF, XGBoost, DeepLearning. Cluster ran for 2 hr 40 min. Total Cost = around $0.42\nhttps://www.daeconomist.com/post/2019-01-15-partii/\n\n\nStorage\n\nS3\n\ncharged by amount stored\n\n$0.023/GB for standard (for first 50 TB)\n0.004/GB for glacier and 0.00099/GB deep glacier\n\ntakes longer to retrieve and not always available\n\n\nfree inbound transfer\nfree transfer between aws services (e.g. S3 to EC2) within the same region\n\nAurora\n\nstorage + inbound/outbound: $0.20 per million requests\n\n\nConsolidated Biling\n\na separate account. All company individual accounts (marketing, sales, etc.) bills are pooled into this account\nhas no access to services\nhas no permissions to access services in other accounts\npooled bill counted towards potential discount billing\n\nCalculators\n\nTotal Cost of Ownership (TCO) calculator\n\ncompares cost of running a project on-premises to aws cloud\n\naws pricing calculator\n\ncalculates price of running a cloud application\ncalculator.aws.com\nestimates cost per service, per service group, and total infrastructure\nhelps find right ec2 instance and region\n\n\nBilling and Cost Management console\n\ncost explorer\n\nview and analysis costs and usage\n\n\n\n\nSpot Instances\n\nSummary\n\nGo to spot advisor and find instances that fit budget and compute requirements\nPrepare strategy for interruption\nOther services\n\nAs of Jan 01, 2019, cloudyr’s aws.ec2 PKG didn’t support all spot instances.\nno time limit on instance usage\nAWS gives a 2 min warning when it decides it needs your spot instance\npricing is market driven depending on capacity levels at the time\nAvailable actions when Amazon “interrupts” your instance:\n\nHibernation:\n\n“like closing your laptop display”\nsaves data and memory and reboots once instance is available again\nRight before interruption, a daemon on the instance freezes the memory and stores it in Elastic Block Store (EBS) root volume\nYour EC2 will retain this root volume and any other EBS data volumes\nOnce market price falls below bid price, instance resumes with memory restored from disk to RAM\nYou aren’t charged while instance is in hibernation, but EBS volumes do cost $.\nAvailable for instance types: C3, C4, M4, R3, and R4 with &lt; 100 GB RAM on Amazon’s Linux, Ubuntu, and Windows\nAll this is done by something called the EC2 Hibernation Agent which sound like its just the name of the program on the servers\n\nStop\n\n“like shutting down your computer to be turned on later”\nlose whatever is in RAM but retain EBS data volumes ($)\nrestores once bid price &lt; market price\n\nTerminate\n\n***default option***\neverything deleted\n\n\nSpot Advisor\n\n**always use this before spinning up spot instances **\nhttps://aws.amazon.com/ec2/spot/instance-advisor\nInput\n\nvCPUs\nMemory size\nPlatform (linux?)\navailability zone (region?)\namount required (number of instances?)\n\noutput\n\ninstance type\nvCPUs\nMemory (GB)\nSavings over On-Demand (%)\nFrequency of termination (%)\n\nliklihood your instance will get terminated\n\n\n\nRunInstance API\n\nFor requesting a spot instance through CLI I think\nLooks like you send something that looks like a python dict with max price, type, region, etc. to this API\n\nSpot Blocks\n\nallows you to set a finite duration that your instance will run for\n\n1 to 6 hrs\nno interruption during that time\n\ntypically 30 to 45% cheaper than on-demand and maybe an additional 5% cheaper during non-peak hours for the region\nrecommended for batch runs\n\nStrategy\n\nUse regions with largest pools of spot instances\n\nLargest pools\n\nus.east.1 (north.virginia)\neu.west.1(ireland)\n\nThese regions have most types/most instances available\nTypically can go uninterrupted for weeks\nless price fluctuation = more certainty\n\n\nSmallest pools\n\neu.central.1 (frankfort)\nap.south.1 (mumbai)\nap.southeast.1 (singapore)\n\ntypically get interrupted within days\n\n\n\nRun groups of instances that come from multiple spot pools\n\nTo used different compute types, jobs/tasks need to be in containers\nspot pools are instances with same region, type, OS, etc.\napplications running on instances from a least 5 different pools can cut interruptions by up to 80%\n\n\nManaging/preparing for interruptions\n\nOnly use for jobs that are short lived\n\ndevelopment and staging environments, short data processing, proof-of-concept, etc.\n\nBuild internal management system that automatically handles interruptions\n\nlook at spot pool historical prices for past 90 days\n\nlooking for least volatile pools\nolder generation (e.g. c-family, m-family) tend to be most stable\n\n\nUse 3rd party platform that manages spot instances and interruptions\n\nSpotinst - uses ML to choose and manage instances that optimizes price and provide continuous activity for apps that are without a single point of failure.\n\nUses on-demand as a fall-back.\nSLA guarantees 99.9% availability.\nSnapshots volumes to migrate data to new instances in case of interruption.\nworks with other services and platforms (kubernetes, codedeploy, etc.)\n\nSpot Fleet - aws service, automanages groups of spot instances according to either of the following strategies:\n\nstrategy options\n\nlowest price - lowest price instances\ndiversified - spread instances across pools\n\nAfter receiving 2 min warning,\n\ntake snapshots of AMI and any attached EBS volumes and use them to launch a new instance.\n\nsnapshot of AMI\n\non EC2 dashboard – left panel – instances – instances\n\nright-click instance – image – create AMI\n\nimage is in left -panel – Images – AMIs\n\n\n\nActually both snapshots might be able to taken in left panel – spot requests\n\nsee AWS note – EC2 for further details\n\n\n\n\n\n\nneed to drain and detach instance from elastic load balancer if one is used\nIf using auto-scaling, need to create an on-demand group and a spot instance group\n\n\nKubernetes\n\nAfter receiving 2 minute interruption warning from AWS:\n\nDetach instance from elastic load balancer (ELB) is one is being used\nMark instance as unschedulable (?)\n\nprevents new pods (group of containers on an instance that performs a job) from being scheduled on that node\nunderlying compute capacity and scheduling of resources of the pods needs to be monitored. Compute capacity and pod resource requirements need to match.\n\n\n\n\n\nComparison\n\nMisc\n\nNotes from\n\nThe Top Clouds Evaluated Such That You Don’t Need to Repeat Our Mistakes\nAWS vs GCP reliability is wildly different\n\nNo services for blockchain development, quantum computing, and graph databases in GCP (May 2022)\nhttps://cloud-gpus.com/ - tool for comparing gpu compute prices across vendors\n\nData centers\n\nCloser the resources are to your business, the less latency\n(May 2022) GCP has caught up and surpassed AWS in the number of data centers and regions that are available\n\nCompute\n\nCheapest vCPU\n\nGCP “e2-micro-preemptible” with 2 vCPU and 1 GB memory.\n\n48% lower than “t4g.nano” from AWS\n5 times lower than “A0” from Azure.\n\nAWS is in-between GCP and Azure in terms of price (i.e. Azure most expensive for cheap vCPUs)\n\nMore performant GCP instances usually cost approximately the same as their analogs from other cloud providers\n\nAzure servers cost the same or slightly less than AWS\n\nGCP: dedicated PostgreSQL server\n\nCheapest instances are 25% lower than the competitors\n\nGPU on-demand availability\n\nConclusion: Assuming you need on-demand boxes to succeed right when you need them, the consensus seems to clearly point to AWS. If you can stand to wait or be redundant to spawn failures, maybe Google’s hardware acceleration customizability can win the day.\nStats\n\nAWS consistently spawned a new GPU in under 15 seconds (average of 11.4s).\nGCP on the other hand took closer to 45 seconds (average of 42.6s).\nAWS encountered one valid launch error in these two weeks whereas GCP had 84\n\nCaveats\n\nGCP allows you to attach a GPU to an arbitrary VM as a hardware accelerator - you can separately configure quantity of the CPUs as needed.\nAWS only provisions defined VMs that have GPUs attached\n\n\n\nRecommendations\n\nAzure\n\nYou use the Microsoft Office stack (Word, Teams, OneDrive, SharePoint, etc.) and/or C# programming language.\nYou head neither for the cheapest servers nor for the most expensive ones — you need something in the middle.\nYou need a memory-optimized solution rather than a general-purpose or a compute-optimized one.\nYou read about the current bugs and inconsistencies in Azure, and it does not scare you.\n\nAWS\n\nYou are rich.\nYou have AWS experts in your team.\nYou build an enterprise-level long-term project.\nOR you just want to rent a cheap virtual machine, and you don’t care about all the other facilities.\n\nGCP\n\nYou are a start-up company.\nYou can’t invest much time in learning AWS and dealing with Azure bugs.\nYou don’t need much flexibility and configuration facilities from the cloud.\nYou are ready to accept the approaches dictated by the platform.\nYou need either a general-purpose or a compute-optimized solution, but not a memory-optimized one.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html",
    "href": "qmd/db-engineering.html",
    "title": "Engineering",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-misc",
    "href": "qmd/db-engineering.html#sec-db-eng-misc",
    "title": "Engineering",
    "section": "",
    "text": "If you’re developing an application, a good rule of thumb is to write your frequently run queries in such a way that they return a response within 500 ms\nColumn storage files (parquet) are more lightweight, as adequate compression can be made for each column. Row storage doesn’t work in that way, since a single row can have multiple data types.\n\n\n(See below) Apache Avro is smaller file size than most row format file types (e.g. csv)\n\n{pins}\n\nConvenient storage method\nUse when:\n\nObject is less than a 1 Gb\n\nUsed {butcher} for large model objects\n\nSome model objects store training data\n\n\n\nBenefits\n\nJust need the pins board name and name of pinned object\n\nThink the set-up is supposed to be easy\n\nEasy to share; don’t need to understand databases",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-terms",
    "href": "qmd/db-engineering.html#sec-db-eng-terms",
    "title": "Engineering",
    "section": "Terms",
    "text": "Terms\n\nACID - A database transaction, by definition, must be atomic, consistent, isolated and durable. These are popularly known as ACID properties.  These properties can ensure the concurrent execution of multiple transactions without conflict. Guarantees data validity despite errors and ensure that data does not become corrupt because of a failure of some sort.\n\nCrucial to business use cases that require a high level of data integrity such as transactions happening in banking.\n\nBatch processing - performing an action on data, such as ingesting it or transforming it, at a given time interval.\nBTEQ - Batch Teradata Query (like SQL) is simply a utility and query tool for Teradata which is a relational database system Creating a BTEQ script to load data from a flat-file.\nConcurrency - multiple computations are happening at the same time\nData Dump - A file or a table containing a significant amount of data to be analysed or transferred. A table containing the “data dump” of all customer addresses.\nData Mart - A subset of a data warehouse, created for a very specific business use case. Finance data mart storing all the relevant financial information required by the Accounting team to process their month-end cycles.\nData Integration - Usually, the hardest part of the project, where multiple sources of data are integrated into a singular application/data warehouse. Integrating finance and customer relationship systems integrating into an MS SQL server database.\nData Lake - A repository for all kinds of structured and unstructured data. Mainly based on Hadoop storage technology. Called a lake as it is flexible enough to store anything from raw data to unstructured email files. Hadoop Data Lake. Storing logs of all customers called into the inbound call centre including call duration.\nData Mesh - Decentralized design where data is owned and managed by teams across the organisation that understands it the most, known as domain-driven ownership. tl;dr - Each department controls they’re own data from ingestion to “data products.” This data product is then made a available to the other departments for them to use in their projects. Each department has their own engineers, scientists, and analysts.\n\nEach business unit or domain aims to infuse product thinking to create quality and reusable data products — a self-contained and accessible data set treated as a product by the data’s producers — which can then published and shared across the mesh to consumers in other domains and business units — called nodes on the mesh.\nEnables teams to work independently with greater autonomy and agility, while still ensuring that data is consistent, reliable and well-governed.\nYou don’t have to figure out who’s in charge of what data, who gets to access it, who needs to protect it and what controls and monitoring is in place to ensure things don’t go wrong.\nExample: Banking\n\nCredit risk domain’s own data engineers can independently create and manage their data pipelines, without relying on a centralised ingestion team far removed from the business and lacking in credit expertise. This credit team will take pride in building and refining high-quality, strategic, and reusable data products that can be shared to different nodes (business domains) across the mesh.\n\n\nData Models - A way of organising the data in a way that it can be understood in a real-world scenario. Taking a huge amount of data and logically grouping it into customer, product and location data.\nData Quality - A discipline of measuring the quality of the data to improve and cleanse it. Checking Customer data for completeness, accuracy and validity.\nData Replication - There are multiple ways to do this, but mainly it is a practice of replicating data to multiple servers to protect an organisation against data loss. Replicating the customer information across two databases, to make sure their core details are not lost.\nDenormalization - database optimization technique in which we add redundant data to one or more tables. Designers use it to tune the performance of systems to support time-critical operations. Done in order to avoid costly joins. Me: Seems like it’s kind of like a View except a View might have calculated columns in it.\nDimensions - A data warehousing term for qualitative information. Name of the customer or their country of residence.\nDistributed SQL -  a single logical database deployed across multiple physical nodes in a single data center or across many data centers if need be; all of which allow it to deliver elastic scale and resilience. Billions of transactions can be handled in a globally distributed database.\nEDW - The same as a data warehouse except it includes all the data within an organisation. This means that the entire enterprise can rely on this warehouse for their business decisions. Organising sales, customer, marketing and finance data in an enterprise data warehouse to be able to create several key management reports.\nEmbedded aka In-Process\n\nEmbedded database as in a database system particularly designed for the “embedded” space (mobile devices and so on.) This means they perform reasonably in tight environments (memory/CPU wise.)\nEmbedded database as in databases that do not need a server, and are embedded in an application (like SQLite.) This means everything is managed by the application.\n\nFacts - A data warehousing term for quantitative information. The number of orders placed by a customer.\nFlat File - Commonly used to transfer data due to their basic nature; flat files are a single table storing data in a plain text format. All customer order numbers stored in a comma-separated value (.csv) file\nHTAP - Hybrid Transactional Analytical Processing - System that attempts be good at both OLAP and OLTP\nMaster Data - This is data that is the best representation of a particular entity in the business. This gives you a 360 view of that data entity by generally consolidating multiple data sources. Best customer data representation from multiple sources of information.\nMulti-Master - allows data to be stored by a group of computers, and updated by any member of the group. All members are responsive to client data queries. The multi-master replication system is responsible for propagating the data modifications made by each member to the rest of the group and resolving any conflicts that might arise between concurrent changes made by different members.\n\nAdvantages\n\nAvailability: If one master fails, other masters continue to update the database.\nDistributed Access: Masters can be located in several physical sites, i.e. distributed across the network.\n\nDisadvantages\n\nConsistency: Most multi-master replication systems are only loosely consistent, i.e. lazy and asynchronous, violating ACID properties. (mysql’s multi-master is acid compliant)\nPerformance: Eager replication systems are complex and increase communication latency.\nIntegrity: Issues such as conflict resolution can become intractable as the number of nodes involved rises and latency increases.\n\nCan be contrasted with primary-replica replication, in which a single member of the group is designated as the “master” for a given piece of data and is the only node allowed to modify that data item. Other members wishing to modify the data item must first contact the master node. Allowing only a single master makes it easier to achieve consistency among the members of the group, but is less flexible than multi-master replication.\n\nNiFi - It is an open-source extract, transform and load tool (refer to ETL), this allows filter, integrating and joining data. Moving postcode data from a .csv file to HDFS using NiFi.\nNormalization - A method of organizing the data in a granular enough format that it can be utilised for different purposes over time. Organizing according to data attributes reduces or eliminates data redundancy (i.e. having the same data in multiple places). Usually, this is done by normalizing the data into different forms such as 1NF (normal form) or 3NF (3rd normal form) which is the most common. (See DB, Relational &gt;&gt; Normalization)\n\nTaking customer order data and creating granular information model; order in one table, item ordered in another table, customer contact in another table, payment of the order in another table. This allows for the data to be re-used for different purposes over time.\n\nNULL indexes - These are the indexes that contain a high ratio of NULL values\nObject-Relational Mapping (ORM) - Allows you to define your data models in Python classes, which are then used to create and interact with the database. See {{SQLAlchemy}}\nODS - Operational data store generally stores limited and current information to help simple queries. Unable to handle historical or complex data queries. An ODS for daily stock fluctuations in a warehouse help the warehouse manager decide what to prioritise in the next order delivery.\nOLAP - Online Analytical Processing - large chunks of tables are read to create summaries of the stored data\n\nUse chunked-columnar data representation\n\nOLTP - Online Transactional Processing - rows in tables are created, updated and removed concurrently\n\ntraditionally use a row-based data representation\npostgres excels at this type of processing\n\nRDBMS - Relational database management system. All of the above examples are RDBMS, meaning they store data in a structured format using rows and columns.\n\nA Microsoft SQL server database.\n\nReal-Time Processing (aka Event Streaming) - each new piece of data that is picked up triggers an event, which is streamed through the data pipeline continuously\nReverse ETL - Instead of ETL where data is transformed before it’s stored or ELT where data is stored and transformed while in storage, Reverse ETL performs transformations in the pipeline between Storage and the Data Product.\n\nSCD Type 1–6 - A method to deal with changes in the data over time in a data warehouse. Type 1 is when history is overwritten whereas Type 2 (most common) is when history is maintained each time a change occurs.\n\nWhen a customer changes their address; SCD Type 1 would overwrite the old address with the new one, whereas Type 2 would store both addresses to maintain history.\n\nSchemas - A term for a collection of database objects. These are generally used to logically separate data within the database and apply access controls.\n\nStoring HR data in HR schema allows logical segregation from other data in the organisation.\n\nSharding - Horizontal Partitioning — divides the data horizontally and usually on different database instances, which reduces performance pressure on a single server.\n\nStaging - The name of a storage area that is temporary in nature; to allow for processing of ETL jobs (refer to ETL). Typically data is loaded from a source database into the staging area database where it is transformed. Once transformed, it’s loaded into the production database where analytics can be performed on it.\n\nA staging area in an ETL routine to allow for data to be cleaned before loading into the final tables.\n\nTransactional Data - This is data that describes an actual event.\n\nOrder placed, a delivery arranged, or a delivery accepted.\n\nUnstructured Data - Data that cannot be nicely organised in a tabular format, like images, PDF files etc.\n\nAn image stored on a data lake cannot be retrieved using common data query languages.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-datqual",
    "href": "qmd/db-engineering.html#sec-db-eng-datqual",
    "title": "Engineering",
    "section": "Data Quality",
    "text": "Data Quality\n\nAlso see Production, Data Validation\nAccuracy - addresses the correctness of data, ensuring it represents real-world situations without errors. For instance, an accurate customer database should contain correct and up-to-date addresses for all customers.\nCompleteness - extent your datasets have all the required information on every record\n\nMonitor: missingness\n\nConsistency - extent that no contradictions in the data received from different sources. Data should be consistent in terms of format, units, and values. For example, a multinational company should report revenue data in a single currency to maintain consistency across its offices in various countries.\nTimeliness - Data should be available at the time it’s required in the system\nValidity - ensuring that data adheres to the established rules, formats, and standards.\n\nMonitor: variable types/classes, numeric variable: ranges, number of decimal places, categorical variable: valid categories, spelling\n\nUniqueness - no replication of the same information twice or more. They appear in two forms; duplicate records and information duplication in multiple places.\n\nMonitor: duplicate rows, duplicate columns in multiple tables",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-costopt",
    "href": "qmd/db-engineering.html#sec-db-eng-costopt",
    "title": "Engineering",
    "section": "Cost Optimization",
    "text": "Cost Optimization\n\nAlso see\n\npage 53 in notebook\nGoogle, BigQuery &gt;&gt; Optimization\n\nAvoid disk operations, make sure that you look out for hints & information in the EXPLAIN PLAN of your query. (e.g. using SORT without an index)\n\nWhen you see filesort, understand that it will try to fit the whole table in the memory in many chunks.\n\nIf the table is too large to fit in memory, it will create a temporary table on disk.\n\nLook out for a using filesort with or without a combination of using temporary.\n\nSplit tables with many columns Might be efficient to split the less-frequently used data into separate tables with a few columns each, and relate them back to the main table by duplicating the numeric ID column from the main table.\n\nEach small table can have a primary key for fast lookups of its data, and you can query just the set of columns that you need using a join operation.\n\nPrimary keys should be global integers.\n\nIntegers consume less memory than strings, and they are faster to compare and hash\n\nJoins\n\nWith correlated keys\n\nThe query planner won’t recognize the correlated keys and do nested loop join when a hash join is more efficient\nI don’t fully understand what correlated keys on a join are, but see SQL &gt;&gt; Terms &gt;&gt; Correlated/Uncorrelated queries\n\nIn the example below, a group of merge_commit_ids will only be from 1 repository id, so the two keys are associated in a sort of traditional statistical sense.\n\nSolutions\n\nUse LEFT_JOIN instead of INNER_JOIN\nUse extended statistics\nCREATE STATISTICS ids_correlation ON repository_id, merge_commit_id FROM pull_requests;\n\n“repository_id” and “merge_commit_id” are the correlated keys\nI’m not sure if “ids_correlation” is a function or just a user-defined name\nPostgreSQL ≥13 will recognize correlation and the query planner will make the correct calculation and perform a hash join\n\n\n\n\nPre-join data before loading it into storage\n\nIf a group of tables is frequently joined and frequently queried, then pre-joining will reduce query costs\ncan be done using an operational transform system such as Spark, Flow, or Flink (dbt can parallelize runs and work w/Spark)\n\nIndexes{#sec-db-eng-costopt-index}\n\nIndexes help in filtering data faster as the data is stored in a predefined order based on some key columns.\n\nIf the query uses those key columns, the index will be used, and the filter will be faster.\n\nSuitable for any combination of columns that are used in filter, group, order, or join\nMySQL Docs\nDon’t use indexes with LIKE\nCluster a table according to an index\n\nAlso see Google, BigQuery &gt;&gt; Optimization &gt;&gt; Partition and Cluster\nRearranges the rows of a table on the disk\nDoesn’t stay “clustered” if table is updated\n\nSee pg_repack for a solution\n\nExample\n-- create index\nCREATE INDEX pull_requests_repository_id ON pull_requests (repository_id, number)\n-- cluster table\nCLUSTER pull_requests USING pull_requests_repository_id\n\n\nUseful for queries such as\nSELECT *\nFROM pull_requests\nWHERE repository_id IN (...) AND number &gt; 1000\nBest Pactices\n\nAvoid too many indexes\n\nA copy of the indexed column + the primary key is created on disk\nIndexes add to the cost of inserts, updates, and deletes because each index must be updated\nBefore creating an index, see if you can repurpose an existing index to cater to an additional query\nCreate the least possible number of indexes to cover most of your queries (i.e. Covering Indexes).\n\nMakes effective use of the index-only scan feature\nAdd INCLUDE to the create index expression\nExample\n-- query\nSELECT y FROM tab WHERE x = 'key';\n-- covering index, x\nCREATE INDEX tab_x_y ON tab(x) INCLUDE (y);\n-- if the index, x, is unique\nCREATE UNIQUE INDEX tab_x_y ON tab(x) INCLUDE (y);\n\ny is called a non-payload column\n\nDon’t add too many non-payload columns to an index. Each one duplicates data from the index’s table and bloat the size of the index.\n\n\nExample: Query with function\n-- query\nSELECT f(x) FROM tab WHERE f(x) &lt; 1;\n-- covering index, x\nCREATE INDEX tab_f_x ON tab (f(x)) INCLUDE (x);\n\nWhere f() can be MEAN, MEDIAN, etc.\n\n\n\nFix unusable indexes\n\nIssues related to data types, collation (i.e. how it’s sorted), character set (how the db encodes characters), etc\nSometimes you can make the indexes work by explicitly forcing the optimizer to use them. (?)\n\nRepurpose or delete stale indexes\n\nIndexes are designed to serve an existing or a future load of queries on the database\nWhen queries change, some indexes originally designed to serve those queries might be completely irrelevant now\nAutomate stale index removal. Dbs keep statistics. Write a script to either notify you or just delete the index if it’s older and not been used past a certain threshold\n\nUse the most cost efficient index type\n\nExample: If your use case only needs a regular expression search, you’re better off having a simple index than a Full Text index.\n\nFull Text indexes occupy much more space and take much more time to update\n\n\nDon’t index huge tables (&gt; 100M rows), partition instead\n\nThen prune the partitions (partition pruning) you don’t need and create indexes for the partitioned tables you do keep.\n\n\nPartitioning\n\nAlso see Google, BigQuery &gt;&gt; Optimization &gt;&gt; Partition and Cluster\nSplits your table into smaller sub-tables under the hood\n\nNot viewable unless you check the table directory to see the multiple files that have been created\n\nThe same goes for indexes on that table.\n\n\nUse on tables with at least 100 million rows (BigQuery recommends &gt; 1 GB) Partitioning helps reduce table size and, in turn, reduces index size, which further speeds up the Data Warehouse (DWH) operations. But, partitioning also introduces complexity in the queries and increases the overhead of managing more data tables, especially backups. So try a few of the other performance techniques before getting to Sharding.\nPartition columns should always be picked based on how you expect to use the data, and not depending on which column would evenly split the data based on size.\n\nExample: partition on county because your analysis or transformations will largely be done by county even though since some counties may be much larger than others and will cause the partitions to be substantially imbalanced.\n\n\nUse ELT (e.g. load data from on-prem server to cloud, then transform) instead of ETL (transform data while on-prem, then load to cloud) for data pipelines\n\nMost of the time you have a lot of joins involved in the transformation step\n\nSQL joins are one of the most resource-intensive commands to run. Joins increase the query’s runtime exponentially as the number of joins increases.\nExample\n\nRunning 100+ pipelines with some pipelines having over 20 joins in a single query.\nEverything facilitated by airflow (see bkmk for code)\nETL: postgres on-prem server, sql queries with joins, tasks ran 12+ hours, then the transformed data is loaded to google storage\n\n13+ hrs for full pipeline completion\n\nELT: running the queries with the joins, etc. with bigquery sql on the data after it’s been loaded into google storage.\n\n6+ hrs for full pipeline completion\n\n\n\n\nUse Materialized Views\n\nA smaller data object that contains the subset of data resulting from a specific query\nWhereas a query happens after data is loaded, a materialized view is a precomputation\nThe computation is done once, and changes to the data are incorporated as they occur, making subsequent updates to the view much cheaper and more efficient than querying the entire database from scratch\n\nFetching a large table will be slower if you try to use multiple cores.\n\nYou have to divide up the table and recombine it. Plus setting up parallel network processes takes time.\nThe time used to fetch some data from the internet depends massively on the internet bandwidth available on your router/network.\n\nUse Random Access via http range header + sparse-hilbert index to optimize db for query searches\nCITEXT extension makes it so you don’t have use lower or upper which are huge hits on performance (at least they are in WHERE expressions) GIN custom indexes for LIKE and ILIKE\nCREATE EXTENSION IF NOT EXISTS btree_gin;\nCREATE EXTENSION IF NOT EXISTS pg_trgm;\nCREATE INDEX index_users_on_email_gin ON users USING gin (email gin_trgm_ops);\n\nCREATE EXTENSION adds btree and pg_trgm extensions\nindex_users_on_email_gin is the name of the index\nusers is the table\nUSING gin (email gin_trgm_ops)\n\ngin specifies that it’s a gin index\nemail is the field\ngin_trgm_ops is from the pg_trgm extension. It splits the index into trigrams which is necessary for the gin index to work with LIKE or ILIKE\n\nSlower to update than the standard ones. So you should avoid adding them to a frequently updated table.\n\nGiST indexes are very good for dynamic data and fast if the number of unique words (lexemes) is under 100,000, while GIN indexes will handle 100,000+ lexemes better but are slower to update.\n\n\nNULLS LASTputs the NULLS in a field in any sorting operations at the end\n\nThe default behavior of ORDER BY will put the NULLS first, so if you use LIMIT , you might get back a bunch of NULLS.\nUsing NULLS LAST fixes this behavior but its slow even on an indexed column\n\nExample: ORDER BY email DESC NULLS LAST LIMIT 10\n\nInstead use two queries\nSELECT *\nFROM users\nORDER BY email DESC\nWHERE email IS NOT NULL LIMIT 10;\n\nSELECT *\nFROM users\nWHERE email IS NULL LIMIT 10;\n\nThe first one would fetch the sorted non-null values. If the result does not satisfy the LIMIT, another query fetches remaining rows with NULL values.\n\n\nRebuild Null Indexes\nDROP INDEX CONCURRENTLY users_reset_token_ix;\nCREATE INDEX CONCURRENTLY users_reset_token_ix ON users(reset_token)\nWHERE reset_token IS NOT NULL;\n\nDrops and rebuilds an index to only include NOT NULL rows\nusers_reset_token_ix is the name of the index\nusers is the table\nI assume “reset_token has to be the field\n\nWrap multiple db update queries into a single transaction\n\nImproves the write performance unless the database update is VERY large.\nA large-scale update performed by a background worker process could potentially timeout web server processes and cause a user-facing app outage\nFor large db updates, add batching\n\nExample: db update has a 100K rows, so update 10K at a time.\nUPDATE messages SET status = 'archived'\n  WHERE id IN\n  (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 0);\nUPDATE messages SET status = 'archived'\n  WHERE id IN\n  (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 10000);\nUPDATE messages SET status = 'archived'\n  WHERE id IN\n  (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 20000);\n\nmessages is the table name\nI guess OFFSET is what’s key here.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-ets",
    "href": "qmd/db-engineering.html#sec-db-eng-ets",
    "title": "Engineering",
    "section": "Event Tracking Systems",
    "text": "Event Tracking Systems\n\nEvents are queued, then batch inserted into your db.\n\nStreaming events does not scale very well and is not fault tolerant.\n\nCommercial Services\n\nSegment\n\nMost popular option\nVery expensive\nSusceptible to ad blockers\nOnly syncs data once per hour or two\nMissing a few key fields in the schema it generates (specifically, session and page ids).\n\nFreshpaint is a newer commercial alternative that aims to solve some of these issues.\n\nOpen Source (each with a managed offering if you don’t feel like hosting it yourself)\n\nSnowplow is the oldest and most popular, but it can take a while to setup and configure.\nRudderstack is a full-featured Segment alternative.\nJitsu is a pared down event tracking library that is laser focused on just getting events into your warehouse as quickly as possible.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-stream",
    "href": "qmd/db-engineering.html#sec-db-eng-stream",
    "title": "Engineering",
    "section": "Streaming",
    "text": "Streaming\n\nStreaming or near real-time (i.e. micro-batch) data\nQuestions\n\nWhat would be the data flow rate in that pipeline?\nDo you require real-time analytics, or is near-real-time sufficient? \n\nData Characteristics\n\nIt is ingested near-real-time.\nUsed for real-time reporting and/or calculating near-real-time aggregates. Aggregation queries on it are temporal in nature so any aggregations defined on the data will be changed over time as the data comes.\nIt is append-only data but can have high ingestion rates so needs support for fast writes.\nHistorical trends can be analyzed to forecast future metrics.\n\nRelational databases can’t handle high ingestion rates and near-real-time aggregates without extensions.\nSteaming is the most expensive way to process the data in the majority of cases. Typically batch ingesting into warehouses is free, but streaming may not be.\nUse Cases: anomaly detection and fraud prevention, real-time personalized marketing and internet of things.\nTools:\n\nApache Kafka - Flexible, connects to app servers, other microservices, databases, sensor networks, financial networks, etc. and can feed the data to same types of systems including analytical tools.\n\nUtilizes a publish-subscribe model where producers (i.e. sources) publish data to topics and consumers (e.g. DBs, BI tools, Processing tools) subscribe to specific topics to receive relevant data.\nHighly scalable due to its distributed architecture, allowing data handling across multiple nodes.\nConfluent’s Kafka Connect - Open source and Commerical Connectors\n\nApache Flume - Similar to Kafka but easier to manage, more lightweight, and built to output to storage (but not as flexible as Kafka)\n\nLess scalable as data ingestion is handled by individual agents, limiting horizontal scaling.\nIts lightweight agents and simple configuration make it ideal for log collection\nCan also handle Batch workloads\nAble to perform basic preprocessing, e.g. filtering specific log types or converting timestamps to a standard format\n\nAmazon Kinesis - A managed, commercial alternative to Kafka. Charges based on data throughput and storage. Additional features include data firehose for delivery to data stores and Kinesis analytics for real-time analysis.\nApache Flink - Processes streaming data with lower latency than Spark Streaming, especially at high throughputs. Less likely to duplicate data. Uses SQL. Steeper learning curve given its more advanced features.\nApache Spark Streaming - See Apache, Spark &gt;&gt; Streaming\nGoogle Pub/Sub - Uses Apache Beam programming API to construct processing pipelines\n\nGoogle Dataflow can create processing pipelines using streaming data from Pub/Sub. Developers write their pipelines using Beam’s API, and then Beam translates them into specific instructions for Flink or Spark to execute.\nIf you have existing workflows around Hadoop or Spark or expertise in those frameworks, then Google Dataproc allows you to reuse that code. It also allows you to used other libraries that aren’t available in Dataflow. Supports various languages like Java, Python, and Scala.\nFor short-lived batch jobs, Dataproc might be more cost-effective. Although, Dataflow’s serverless nature avoids idle resource charges while Dataproc clusters incur costs even when idle.\n\n\nArchitectures\n\nNotes from\n\nData Pipeline Design Patterns\n\nETL\n\n\nKinesis collects data from a server (e.g. app) and continuously feeds it to a lambda function for transformation. Transformed data is deposited into a S3 bucket, queried using Athena, and visualized using Quicksight.\n\nHybrid (Streaming and Batch)\n\n\nKinesis streams data to S3 and when a threshold is reached, a lambda trigger activates a transformation/batch load to the BQ warehouse\n\n\nTimeScale DB\n\nOpen source extension for postgresql\nSupport all things postgresql like relational queries, full SQL support(not SQL-like) as well as the support of real-time queries\nSupports an ingestion of 1.5M+ metrics per second per server\nNear-real-time aggregation of tables\nProvides integration with Kafka, kinesis, etc for data ingestion.\nCan be integrated with any real-time visualization tool such as Graphana\n\nPipeline DB\n\nOpen source extension for postgresql\nSimilar features as TimeScale DB\nEfficiency comes from it not storing raw data\n\nUsually, it’s recommended to store raw data",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-otools",
    "href": "qmd/db-engineering.html#sec-db-eng-otools",
    "title": "Engineering",
    "section": "Other Tools",
    "text": "Other Tools\n\nDataFold monitors your warehouse and alerts you if there are any anomalies (e.g. if checkout conversion rate drops suddenly right after a deploy).\nHightouch lets you sync data from your warehouse to your marketing and sales platforms.\nWhale is an open source tool to document and catalog your data. \nRetool lets you integrate warehouse data into your internal admin tools.\nGrowth Book that plugs into your data warehouse and handles all of the complicated querying and statistics required for robust A/B test analysis.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-relational.html",
    "href": "qmd/db-relational.html",
    "title": "Relational",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Relational"
    ]
  },
  {
    "objectID": "qmd/db-relational.html#sec-db-rel-misc",
    "href": "qmd/db-relational.html#sec-db-rel-misc",
    "title": "Relational",
    "section": "",
    "text": "Packages\n\n{dplyr}\n\ncompute stores results in a remote temporary table\ncollect retrieves data into a local tibble.\ncollapse doesn’t force computation, but instead forces generation of the SQL query.\n\nsometimes needed to work around bugs in dplyr’s SQL generation.\n\n\n{dm}\n\nCan join multiple tables from a db, but keeps the meta info such as table names, primary and foreign keys, size of original tables etc.\n\n\nRelational databases do not keep all data together but split it into multiple smaller tables. That separation into sub-tables has several advantages:\n\nAll information is stored only once, avoiding repetition and conserving memory\nAll information is updated only once and in one place, improving consistency and avoiding errors that may result from updating the same value in multiple locations\nAll information is organized by topic and segmented into smaller tables that are easier to handle\n\nOptimized for a mix of read and write queries that insert/select a small number of rows at a time and can handle up to 1TB of data reasonably well.\nThe main difference between a “relational database” and a “data warehouse” is that the former is created and optimized to “record” data, whilst the latter is created and built to “react to analytics”.\nTypes\n\nEmbedded aka In-Process (see Databases, Engineering &gt;&gt; Terms): DuckDB (analytics) and SQLite (transactional)\nServer-based: postgres, mysql, SQL Server\n\nMix of transactional and analytical\nDistributed SQL (database replicants across regions or hybrid (on-prem + cloud)\n\nmysql, postgres available for both in AWS Aurora (See below)\npostgres available using yugabytedb\nSQL Server on Azure SQL Database\nCloud Spanner on GCP\n\n\n\nApache Avro\n\nRow storage file format unlike parquet\nA single Avro file contains a JSON-like schema for data types and the data itself in binary format\n4x slower reading than csv but 1.5x faster writing than csv\n1.7x smaller file size than csv\n\nWrapper for db connections (e.g. con_depA &lt;- connect_databaseA(username = ..., password = ...) )\n# ... other stuff including code for \"connect_odbc\" function\n\n# connection attempt loop\nwhile(try &lt; retries) {\n    con &lt;- connect_odbc(source_db = \"&lt;database name&gt;\"\n                        username = username,\n                        password = password)\n    if(class(con) == \"NetexxaSQL\") {\n        try &lt;- retries + 1\n    } else if (!\"NetezzaSQL\" %in% class(con) & try &lt; retries {\n        warning(\"&lt;database name&gt; connection failed. Retrying...\")\n        try &lt;- try + 1\n        Sys.sleep(retry_wait)\n    } else {\n        try &lt;- try + 1\n        warning(\"&lt;database name&gt; connection failed\")\n    }\n}\n\nGuessing “NetezzaSQL” is some kind of error code for a failed connection to the db\n\nBenchmarks\n\nExample\n\nData\n\n~54,000,000 rows and 6 columns\n10 .rds files with gz compression is 220MB total,\n\nIf they were .csv, 1.5 GB\n\nSQLite file is 3 GB\nDuckDB file is 2.5 GB\nArrow creates a structure of directories, 477 MB total\n\nOperation: read, filter, group_by, summarize\nResults\n##  format          median_time mem_alloc\n##  &lt;chr&gt;              &lt;bch:tm&gt; &lt;bch:byt&gt;\n## 1 R (RDS)              1.34m    4.08GB\n## 2 SQL (SQLite)          5.48s    6.17MB\n## 3 SQL (DuckDB)          1.76s  104.66KB\n## 4 Arrow (Parquet)      1.36s  453.89MB\n\nTradional relational db solutions balloon up the file size\n\nSQLite 2x, DuckDB 1.66x (using csv size)",
    "crumbs": [
      "Databases",
      "Relational"
    ]
  },
  {
    "objectID": "qmd/db-relational.html#sec-db-rel-brands",
    "href": "qmd/db-relational.html#sec-db-rel-brands",
    "title": "Relational",
    "section": "Brands",
    "text": "Brands\n\nSQLite vs MySQL as transactional dbs (article)\n\nSQLite:\n\nEmbedded, size ~600KB\nLimited data types\nBeing self-contained, other clients on a network would not have access to the database (no multi-users) unlike with MySQL\nNo built-in authentication that is supported\nMultiple processes are able to access the database at the same time, but making changes at the same time is not something supported\nUse Cases\n\nData being confined in the files of the device is not a problem\nNetwork access to the db is not needed\nApplications that will minimally access the database and not require heavy calculations\n\n\nMySQL:\n\nopposites of the sqlite stuff\nSize ~600MB\nsupports replication and scalability\nSecurity is a large; built-in features to keep unwanted people from easily accessing data\nUse cases\n\ntransactions are more frequent like on web or desktop applications\nif network capabilities are a must\nmulti-user access and therefore security and authentication\nlarge amounts of data\n\n\n\nMySQL\n\nInstallation docs\nBasic intro\nSee SQL notebook\n\nSQLite\n\n{RSQLite}\n\nCloud SQL - Google service to provide hosting services for relational dbs (see Google, BigQuery &gt;&gt; Misc). Can use postgres, mysql, etc. on their machines.\n\nCloud SQL Insights - good query optimization tool\n\nAWS RDS for db instances (see Database, postgres &gt;&gt; AWS RDS)\n\nAvailable: Amazon Aurora, MySQL, MariaDB, postgres, Oracle, Microsoft SQL Server\nRDS (Relational Database Service)\n\nBenefits over hosting db on EC2: AWS handles scaling, availability, backups, and software and operating system updates\n\n\nAWS Aurora - MySQL- and PostgreSQL-compatible enterprise-class database\n\nStarting at &lt;$1/day.\nSupports up to 64TB of auto-scaling storage capacity, 6-way replication across three availability zones, and 15 low-latency read replicas.\nCreate MySQL and Postgres instances using AWS Cloudformation",
    "crumbs": [
      "Databases",
      "Relational"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html",
    "href": "qmd/db-warehouses.html",
    "title": "Warehouses",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-misc",
    "href": "qmd/db-warehouses.html#sec-db-ware-misc",
    "title": "Warehouses",
    "section": "",
    "text": "Resources\n\nEasy way to create Live and Staging environments for your Data\n\nTutorial on setting up Staging and Production environments for a BQ warehouse and a Github repo where the Dataform code lives. Pull Requests move data from the staging area to the production db.\n\n\nThe main difference between a “relational database” and a “data warehouse” is that the former is created and optimized to “record” data, whilst the latter is created and built to “react to analytics.”\nOptimized for read-heavy workloads that scan a small number of columns across a very large number of rows and can easily scale to petabytes of data\nCons\n\nCan become expensive when an organization needs to scale them\nDo not perform well when handling unstructured or complex data formats.\n\nPros\n\nIntegrating multiple data sources in a single database for single queries\nMaintaining data history, improving data quality, and keeping data consistency\nProviding a central view for multiple source system across the enterprise\nRestructuring data for fast performance on complex queries\n\nSet up alerts and monitoring\n\nETL Monitoring: See if anything failed to load. It can be a failed file with the wrong format or a streaming data record that doesn’t comply with table schema.\nData Quality Checks: Any missing datasets from external providers data malformed, i.e. NULL, formats, etc.\nCost Monitoring: Identify any long-running queries and costs\nUsage Monitoring: Alert if when there’s activity on your account.",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#olap-vs-oltp",
    "href": "qmd/db-warehouses.html#olap-vs-oltp",
    "title": "Warehouses",
    "section": "OLAP vs OLTP",
    "text": "OLAP vs OLTP\n\n\nOLAP (Online Analytical Processing)(aka the Cube)(Data Warehouses)\n\ndb designed to optimize performance in analysis-intensive applications\nAggregates transactions to be less frequent but more complex\nExamples: Snowflake, Bigquery\n\nOLTP (Online Transaction Processing) db designed for frequent, small transactions\n\nExecutes a number of transactions occurring concurrently (i.e. at the same time)\nUse cases: online banking, shopping, order entry, or sending text messages\n\nData model: OLTP systems typically use a normalized data model, which means that data is stored in multiple tables and relationships are defined between the tables. This allows for efficient data manipulation and ensures data integrity. OLAP systems, on the other hand, often use a denormalized data model, where data is stored in a single table or a small number of tables. This allows for faster querying, but can make data manipulation more difficult.\nData volume: OLTP systems typically deal with smaller amounts of data, while OLAP systems are designed to handle large volumes of data.\nQuery complexity: OLTP systems are designed to handle simple, short queries that involve a small number of records. OLAP systems, on the other hand, are optimized for more complex queries that may involve aggregating and analyzing large amounts of data.\nData updates: OLTP systems are designed to support frequent data updates and insertions, while OLAP systems are optimized for read-only access to data.\nConcurrency: OLTP systems are designed to support high levels of concurrency and handle a large number of transactions simultaneously. OLAP systems, on the other hand, are optimized for batch processing and may not perform as well with high levels of concurrency.",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-brands",
    "href": "qmd/db-warehouses.html#sec-db-ware-brands",
    "title": "Warehouses",
    "section": "Brands",
    "text": "Brands\n\nAmazon Redshift\n\nRedshift is best when you have data engineers who want control over infrastructure costs and tuning.\n\nGoogle BigQuery\n\nBest when you have very spiky workloads (i.e. not steady-state).\nNot charged any computational expense for loading the data into its storage.\nProactive Storage Cost Optimization.\n\nIf there is data in BigQuery that hasn’t been altered in over 90 days, they move it into a long-term storage tier that is half the price of regular storage.\n\nCaches query results for 24 hours after running a query.\n\nBigQuery stores the results of a query as a temp table, and if the underlying data has not changed, you will not be charged for running the same query twice in that timeframe.\n\n\nSnowflake\n\nA cloud data warehouse for analytics. It’s columnar, which means that data is stored (under the hood) in entire columns instead of rows; this makes large analytical queries faster, so it’s a common choice for how to build analytical DBs.\nBest when you have a more continuous usage pattern\nSupport for semi-structured data, data sharing, and data lake integration\nResource: Snowflake Data Warehouse Tutorials\nSnowpark - Allows you to run Python code inside the database. Brings code to the warehouse instead of importing the data to your code. Has a DataFrame API so you can run cleaning code that is more readable when coded with python than SQL or ML models on the data inside of the warehouse.\nList all schemas\nselect\n    \"database_name\" as DATABASE_NAME\n    ,\"name\" as SCHEMA_NAME\nfrom table(result_scan(last_query_id()))\nwhere SCHEMA_NAME not in ('INFORMATION_SCHEMA') -- optional filter(s)\n;\n\nAzure Synapse Analytics\n\nFully managed, cloud-based data warehousing service offered by Microsoft Azure. It offers integration with Azure Machine Learning and support for real-time analytics.\n\nData Bricks\n\nCompany behind spark technology and have built a cloud-based data warehousing service.\n\nTeradata\nSAP HANA\nClickHouse\n\nOpensource, built by Yandex (Russian search engine)\n\nApache Hadoop running Apache Hive\n\nHive: an open-source data warehouse solution for Hadoop infrastructure. It is used to process structured data of large datasets and provides a way to run HiveQL queries.\n\nResource: Apache Hive Tutorial with Examples",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-strat",
    "href": "qmd/db-warehouses.html#sec-db-ware-strat",
    "title": "Warehouses",
    "section": "Strategies",
    "text": "Strategies\n\nInmon\n\n\nPrioritizes accuracy and consistency of data above all else.\nQuerying is pretty fast (data marts)\nTends to be a lot of upfront work, however subsequent modifications and additions are quite efficient.\nRecommended if:\n\nData accuracy is the most important characteristic of your warehouse\nYou have time/resources to do a lot of upfront work\n\n\nKimball\n\n\nLess structured approach, which speeds up the initial development cycle.\nFuture iterations require the same amount of work, which can be costly if you’re constantly updating the warehouse Fast querying but very few quality checks\nRecommended if:\n\nIf you’re business requirements are well-defined and stable\nYou are querying lots of data often\n\n\nData Vault\n\n\nTrys to fix disadvantages of Kimball and Inmon strategies by waiting to the last minute to develop any kind of structure\nWorkflow: Sources –&gt; unstructured storage (data lake) –&gt; Staging which supports operations such as batch and streaming processes –&gt; data vault which stores all raw data virtually untouched (non-relational db?)\nAdvantages: efficient, fast to implement, and highly dynamic\nDisadvantages: querying can be quite slow\n\nUh doesn’t seem to be much cleaning either\n\nRecommended if:\n\nYour business goals change often\nYou need cheap server and storage costs",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-dsgn",
    "href": "qmd/db-warehouses.html#sec-db-ware-dsgn",
    "title": "Warehouses",
    "section": "Design",
    "text": "Design\n\nMisc\n\nNotes from\n\nData Warehouse Design Patterns\n\nBest practice to keep only a portion of data in the RAW database and use it to update our “BASE” or “PROD” database tables.\n\nDatabases Inside the Warehouse:\n\nRaw or Source: Raw data is inported into this db; source of truth\nBase or Prod: Where data is imported from Source db and has had basic field transformations; storage\nAnalytics: Where data is ready to be queried; ad-hoc analytics and materialized queries and views\n\nEnvironments\n\nEach db will have development and production branches\n\nDevelopment: staging, mocking and developing data transformations\nProduction: Data is validated and transformations applied on a schedule\n\n\nExample: Snowflake Warehouse",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-trig",
    "href": "qmd/db-warehouses.html#sec-db-ware-trig",
    "title": "Warehouses",
    "section": "Database Triggers",
    "text": "Database Triggers\n\n\nA database trigger is a function that gets triggered every time a record is created or updated (or even deleted) in the source table (in this case, a transactional table)\nDatabase triggers provide an effective, solution to extracting data from the transactional system and seamlessly integrating it into the data warehouse while also not adversely impacting that system.\nUse case — You see a couple of data points in your transactional system’s tables that you would require for your reporting metrics but these data points are not being provided by your transactional system’s API endpoints. So, there is no way you can write a script in Python or Java to grab these data points using the API. You cannot use direct querying on your transactional system as it can negatively impact its performance.\nMisc\n\nNotes from Harnessing Triggers in the Absence of API Endpoints\n\nProvides a detailed step-by-step\n\nIf your transactional system does not have a lot of traffic (or) is not directly used by end-user applications, then it can be set up as a synchronous process. In that case, the lambda or the Azure functions would need to have the trigger event as the transactional database’s staging table. The appropriate database connection information would also need to be provided.\n\nDatabase Triggers\n\nDDL Triggers - Set up whenever you want to get notified of structural changes in your database\n\nUseful when you wish to get alerted every time a new schema is defined; or when a new table is created or dropped. Hence, the name DDL (Data Definition Language) triggers.\n\nDML Triggers - Fired when new records are inserted, deleted, or updated\n\ni.e. You’re notified anytime a data manipulation change happens in a system.\n\n\nSyntax: &lt;Timing&gt; &lt;Event&gt;\n\nTrigger Event - The action that should activate the trigger.\nTrigger Timing - Whether you need the trigger to perform an activity before the event occurs or after the event occurs.\n\nSpecialized triggers provided by cloud services\n\nAWS\n\nLambda Triggers: These triggers help initiate a lambda function when a specified event happens. Events can be internal to AWS, or external in nature. Internal events can be related to AWS services such as Amazon S3, Amazon DynamoDB streams, or Amazon Kinesis. External events can come in from the database trigger of a transactional system outside of AWS or an IoT event.\nCloudwatch Events: If you have used standalone relational databases such as Microsoft SQL Server and SQL Server Management Studio (SSMS), you may have used SQL Server Agent to notify users of a job failure. Cloudwatch is specific to AWS and is used not only to notify users of a job failure but also to trigger Lambda functions and to respond to events. The important difference between a CloudWatch Event and a Lambda Trigger is that while Lambda triggers refer to the capability of AWS Lambda to respond to events, CloudWatch Events is a broader event management service that can handle events from sources beyond Lambda. On a side note, while SQL Server Agent requires an email server to be configured, Cloudwatch has no such requirement.\n\nAzure\n\nBlob Trigger: Azure blobs are similar to S3 buckets offered by AWS. Similar to how Amazon S3 notifications can be used to get alerts about changes in S3 buckets; blob triggers can be used to get notified of changes in Azure blob containers.\nAzure Function Trigger: These are the Azure equivalent of AWS Lambda Function Triggers. These triggers can be used to initiate an Azure function in response to an event within Azure or an external event, such as an external transactional database trigger, an HTTP request, or an IoT event hub stream. Azure functions can also be initiated based on a pre-defined schedule using a Timer Trigger.\n\n\nExample: Transfer data from a transactional database to a warehouse (See article for further details)\n\nIdentify table in transactional db with data you want\nCreate a staging table that’s exactly like the transaction table\n\nEnsure that you don’t have any additional constraints copied over from the source transactional table. This is to ensure as minimal impact as possible on the transactional system.\nFor a bulk data transfer of historical transaction data:\n\nCREATE TABLE AS SELECT (SELECT * INTO in SQL Server) while creating the staging table. This will create the staging table pre-populated with all the data currently available in the transaction table.\nDo an empty UPDATE on all the records in the transaction table\n\ne.g. UPDATE TABLE Pricing_info SET OperationDate=OperationDate\nThis is not a recommended approach as it could bog down the transactional system due to the number of updates and undo statements generated. Moreover, the transaction table will also be locked during the entire update operation and will be unavailable for other processes thus impacting the transactional system. This method is okay to use if your transaction table is extremely small in size.\n\n\nIn addition to that, also have a column to indicate the operation performed such as Insert, Update, Delete).\n\nSet up a DML trigger directly on the transaction table\n\nAll DML events namely Insert, Delete, and Update in the transaction table should have a separate trigger assigned to them.\n\nThe below example shows the trigger for Insert. The rest of the triggers are created similarily — just by substituting 2 INSERTs (trigger event, select statement) for DELETE or UPDATE (See article for code) and using a different name in CREATE\n\nInsert trigger in (SQL Server)\n-- Create the trigger\nCREATE TRIGGER TransactionTrigger_pricing_Insert\nON Pricing_info\n--Trigger Event\nAFTER INSERT\nAS\nBEGIN\n    -- Insert new records into the staging table\n    INSERT INTO StagingTable_pricing (ID, Column1, Column2, OperationType)\n    SELECT ID, Column1, Column2, 'INSERT'\n    FROM inserted\nEND;\n\n“Pricing_info” is the name of transactional table with the data you want\n“StagingTable_pricing” is the name of the staging table\nAFTER INSERT where AFTER is the trigger timing and INSERT is the trigger event\nIn the SELECT statement, “INSERT” is the value for that extra column in the staging table that tells us which type of operation this was.\n\n\nSet-up the specialized trigger in the warehouse\n\nAWS \n\nA database DML trigger in the transactional system’s database. Whenever a new record comes into the transactional database table, the trigger would insert the new data into a staging table within the transactional database.\n\nIf you based it on a schedule (using AWS Cloudwatch events), the Lambda trigger would trigger a lambda function to grab the data from the staging table to a table in the datawarehouse (Redshift)\n\n\nAzure \n\nWhen the timer trigger activates, it would run the Azure Function which would then pick up the new/updated/deleted records from the staging table.",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/misc.html",
    "href": "qmd/misc.html",
    "title": "Misc",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-misc",
    "href": "qmd/misc.html#sec-misc-misc",
    "title": "Misc",
    "section": "",
    "text": "Windows\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + Tab\nChange application\n\n\nCtrl + ~\nChange window within an application\n\n\n\nBrowser\n\n\n\n\n\n\n\nAction\nShortcut\n\n\n\n\nTo Address Bar\nCtrl + L\n\n\nOpen a new window\nCtrl + n\n\n\nOpen a new window in Incognito mode\nCtrl + Shift + n\n\n\nOpen a new tab, and jump to it\nCtrl + t\n\n\nReopen previously closed tabs in the order they were closed\nCtrl + Shift + t\n\n\nJump to the next open tab\nCtrl + Tab or Ctrl + PgDn\n\n\nJump to the previous open tab\nCtrl + Shift + Tab or Ctrl + PgUp\n\n\nJump to a specific tab\nCtrl + 1 through Ctrl + 8\n\n\nJump to the rightmost tab\nCtrl + 9\n\n\nOpen your home page in the current tab\nAlt + Home\n\n\nOpen the previous page from your browsing history in the current tab\nAlt + Left arrow\n\n\nOpen the next page from your browsing history in the current tab\nAlt + Right arrow\n\n\nClose the current tab\nCtrl + w or Ctrl + F4\n\n\nClose the current window\nCtrl + Shift + w or Alt + F4\n\n\nMinimize the current window\nAlt + Space then n\n\n\nMaximize the current window\nAlt + Space then x\n\n\nQuit Google Chrome\nAlt + f then x\n\n\nMove tabs right or left\nCtrl + Shift + PgUp or Ctrl + Shift + PgDn\n\n\n\nR-devel (&gt;= 4.4.0) gained a command-line option to adjust the limit connections (previous limit was 128 parallel workers)\n$ R\n&gt; parallelly::availableConnections()\n[1] 128\n\n$ R --max-connections=512\n&gt; parallelly::availableConnections()\n[1] 512",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-rstud",
    "href": "qmd/misc.html#sec-misc-rstud",
    "title": "Misc",
    "section": "RStudio",
    "text": "RStudio\n\nJob: Run script in the background\nlibrary(rstudioapi)\njobRunScript(\"wfsets_desperation_tune.R\", name = \"tune\", exportEnv = \"R_GlobalEnv\")\n\nNeed to look up args\nI think exportEnv takes the variables in your current environment and runs the script with them as inputs\n\nShortcuts\n\n\n\n\n\n\n\nShortcut\nDescription\n\n\n\n\nAlt + Shift + k\nKeyboard Shortcuts\n\n\nCtrl + Shift + p\nCommand Palette\n\n\nCtrl + Shift + f\nFind in Files\n\n\nCtrl + Alt + up/down\nMultiple Cursors\n\n\nCtrl + Shift + z\nReverse Undo\n\n\nCtrl + Shift + a\nFormat highlighted code (style/linter the code)\n\n\nCtrl + d\nDelete current line\n\n\nAlt + up/down\nYank line up or down\n\n\nCtrl + Alt + up/down\nCopy the above line (or selected lines) down or up\n\n\nCtrl + .\nGo to file/function name\n\n\nAlt + Shift + m\nFocus on Terminal\n\n\n\n\nCustomizing Shortcuts in RStudio\n{shrtcts} - Make anything a shortcut in RStudio",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-hack",
    "href": "qmd/misc.html#sec-misc-hack",
    "title": "Misc",
    "section": "Hackathon Criteria",
    "text": "Hackathon Criteria",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-update",
    "href": "qmd/misc.html#sec-misc-update",
    "title": "Misc",
    "section": "Update R",
    "text": "Update R\n\nMisc\n\n{rig} - r version management system\nupdate.packages(checkBuilt = TRUE, ask = FALSE) is supposed to search for packages in other R versions and update them in the new R version, but I haven’t tried it, yet.\nErrors when compiling from source may require installing libraries and they’ll supply code to install via “pacman”\n\nOpen Start &gt;&gt; scroll down to RTools40 &gt;&gt; RTools Bash\nPaste pacman code and hit enter to install\n\nProblem packages in the past\n\n{brms} dependency, {igraph}, didn’t have a binary on CRAN and wouldn’t compile from source even with correct libraries installed.\n\nSol’n: install.packages(\"igraph\", repos = 'https://igraph.r-universe.dev')\n\ninstalls dev version from r-universe\n\n\nSome {easystats} packages had gave {pak} some problems. No difficulties using install.packages with default repo or if they had a r-universe repo though.\n\n\nSteps\n\nCopy user installed packages in current R version\n\nIn R:\nsquirrel &lt;- names(installed.packages(priority = \"NA\")[, 1]) # user installed packages\nreadr::write_rds(squirrel, \"packages.rds\")\n\nThen, close RStudio\n\n\nRTools: Check to see if you have the latest because you’ll need it to compile some of newest versions of packages.\n\nYour rtools folder has the version in it’s folder name.\nrtools website has the latest version and an .exe to download\n\nCheck/Update rig version\n\nIn powershell: rig --version\nCheck current rig release: link\nDownload and install if your version isn’t current\n\nInstall new version of R\n\nClose R if not already closed\nrig add release installs the latest version of R.\nrig default &lt;new_r_version&gt; sets that version as the default\n\nAdd R and RTools to path\n\nRight-click Windows &gt;&gt; System &gt;&gt; (right panel) Advanced System Settings &gt;&gt; Environment Variables &gt;&gt; Under User Variables, highlight Path, click Edit &gt;&gt; Click Add\n\nR: Add path to directory with all the RScript, R exe, etc. e.g. “C:\\Program Files\\R\\R-4.2.3\\bin\\x64”\nRTools: e.g. “C:\\rtools43\\usr\\bin”\n\n\nOpen R and confirm new version\n\nIf RStudio\n\nThe setting of the new version to the “default” version of R in rig should result in RStudio loading the new version.\nIf not, Tools &gt;&gt; Global Options &gt;&gt; General\n\nUnder “R version”, click “change” button; choose new R version\nQuit session and restart RStudio\n\n\n\nInstall “high maintenance” packages\n\nI’ve had issues with {pak} installing packages that need to be compiled. Maybe be worth trying {pak} first to see if they’ve fixed it.\n{cmdstanr} doesn’t live on CRAN, so you have to use: install.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\n\nCheck for latest cmdstan version\n\nAfter loading the package, library(cmdstanr) , it should run a check on your cmdstan version and tell you if there’s a newer version.\nTo update, first check toolchain: check_cmdstan_toolchain()\n\nMight tell you to update RTools or that you need some C++ library added\n\nFix C++ toolchain with check_cmdstan_toolchain(fix = TRUE)\nUpdate cmdstan: install_cmdstan()\nMay need to install {rstudioapi} and run rstudioapi::restartSession() (programmatically) or just ctrl + shift + f10 so that this package can be used as a dependency for other packages that need to be installed.\n\n\n{rstanarm}: install.packages(\"rstanarm\")\n\nInstall other packages\nmoose &lt;- readRDS(\"packages.rds\")\nmoose &lt;- moose[!moose %in% c(\"cmdstanr\", \"rstanarm\", \"ebtools\", \"translations\", \"&lt;RStudio add-ins&gt;\")]\n\n# Next time, add a try/catch? or maybe purrr::safely, so that it continues through errors. Also, need to log pkgs that do error.\nfor (i in seq_len(length(moose))) {\n  print(moose[i])\n  pak::pkg_install(moose[i])\n}\n\nfs::file_delete(\"packages.rds\")\n\n{ebtools} is my personal helper package.\n{translations} is a system package that shouldn’t have been included when I saved the packages from previous version, but was when I recently updated. Might not be necessary to include it in the excuded packages in the future.\n\nCheck for updates of RStudio (link)\n\nCurrent version under Help &gt;&gt; About Rstudio\nPossible to check for updates under Help &gt;&gt; Check for Updates, but that’s failed me before.",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html",
    "href": "qmd/spreadsheets.html",
    "title": "Spreadsheets",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-misc",
    "href": "qmd/spreadsheets.html#sec-spdsht-misc",
    "title": "Spreadsheets",
    "section": "",
    "text": "Some Excel files are binaries and in order to use download.file, you must set mode = “wb”\ndownload.file(url, \n              destfile = glue(\"{rprojroot::find_rstudio_root_file()}/data/cases-age.xlsx\"), \n              mode = \"wb\")\nIndustry studies show that 90 percent of spreadsheets containing more than 150 rows have at least one major mistake.",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-cats",
    "href": "qmd/spreadsheets.html#sec-spdsht-cats",
    "title": "Spreadsheets",
    "section": "Catastrophes",
    "text": "Catastrophes\n\nReleasing confidential information\n\nIrish police accidently handed out officers private information when sharing sheets with statistics due to a freedom of information request. (link)\n\nErrors when combining sheets\n\nWales dismissed anaesthesiologists after mistakenly deeming them “unappointable.” Spreadsheets from different areas lacked standardization in formatting, naming conventions, and overall structure. To make matters worse, data was manually copied and pasted between various spreadsheets, a time-consuming and error-prone process. (link)\nWhen consolidating assets from different spreadsheets, the spreadsheet data was not “cleaned” and formatted properly. The Icelandic bank’s shares were subsequently undervalued by as much as £16 million. (link)\n\nData entry errors\n\nCryto.com accidentally transferred $10.5 million instead of $100 into the account of an Australian customer due to an incorrect number being entered on a spreadsheet. (link)\nNorway’s $1.5tn sovereign wealth fund lost $92M, on an error relating to how it calculated its mandated benchmark. A person used the wrong date, December 1st instead of November 1st. (link)",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-bprac",
    "href": "qmd/spreadsheets.html#sec-spdsht-bprac",
    "title": "Spreadsheets",
    "section": "Best Practices",
    "text": "Best Practices\n\nNotes from Data organization in spreadsheets\n\nBe consistent\nWrite dates like YYYY-MM-DD\nDon’t leave any cells empty\nPut just one thing in a cell\nOrganize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row)\nCreate a data dictionary\nDon’t include calculations in the raw data files\nDon’t use font color or highlighting as data\nChoose good names for things\nMake backups\nUse data validation to avoid data entry errors\nSave the data in plain text files.",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-transspr",
    "href": "qmd/spreadsheets.html#sec-spdsht-transspr",
    "title": "Spreadsheets",
    "section": "Transitioning from Spreadsheet to DB",
    "text": "Transitioning from Spreadsheet to DB\n\nMisc\n\nWhen you start to have multiple datasets or when you want to make use of several columns in one table and other columns in another table you should consider going the local database route.\nUse db “normalization” to figure out a schema\nAlso see\n\nDatabases, Engineering &gt;&gt; Schema\nDatabases, Warehouses &gt;&gt; Design a Warehouse\n\n\nDB advantages over spreadsheets:\n\nEfficient analysis: Relational databases allow information to be retrieved quicker to then be analyzed with SQL (Structured Query Language), to then run queries.\n\nOnce spreadsheets get large, they can lag or freeze when opening, editing, or performing simple analyses in them.\n\nCentralized data management: Since relational databases often require a certain type or format of data to be input into each column of a table, it’s less likely that you’ll end up with duplicate or inconsistent data.\nScalability: If your business is experiencing high growth, this means that the database will expand, and a relational database can accommodate an increased volume of data.\n\nStart documenting the spreadsheets\n\nfile names, file paths\nUnderstand where values are coming from\n\nsource (e.g. department, store, sensor), owner\n\nHow rows of data are being generated\n\nwho/what is inputting the data\n\nHow does each spreadsheet/notebooks/set of spreadsheets fit in the company’s business model\n\nHow are they being used and by whom\n\nMap the spreadsheets relationships to one another\n\nSee Databases, Warehouses &gt;&gt; Design a Warehouse",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/sql.html",
    "href": "qmd/sql.html",
    "title": "SQL",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-misc",
    "href": "qmd/sql.html#sec-sql-misc",
    "title": "SQL",
    "section": "",
    "text": "Resources\n\nPublicly Available SQL Databases: Need to email administrator to gain access\nSQL for Data Scientists in 100 Queries\n\nSQLite, administrative commands, query commands, JSON ops, python\n\n\ndplyr::show_query can convert a dplyr expression to SQL for db object (e.g. dbplyr,  duckdb, arrow)\nQueries in examples\n\nWindow Functions\n\nAverage Salary by Job Title\nAverage Unit Price for each CustomerId\nRank customers by amount spent\nCreate a new column that ranks Unit Price in descending order for each CustomerId\nCreate a new column that provides the previous order date’s Quantity for each ProductId\nCreate a new column that provides the very first Quantity ever ordered for each ProductId\nCalculate a cumulative moving average UnitPrice for each CustomerId\nRank customers for each department by amount spent\nFind the model and year of the car that been on the lot the longest\nCreate a subset (CTE)\n\nCalculate a running monthly total (aka cumsum)\n\nAlso running average\n\nCalculate a running monthly total for each account id\nCalculate a 3 months rolling running total using a window that includes the current month.\nCalculate a 7 months rolling running total using a window where the current month is always the middle month\nCalculate the number of consecutive days spent in each country\n\n\nCTE\n\nAverage monthly cost per campaign for the company’s marketing efforts\nCount the number of interactions of new users\nThe average top Math test score for students in California\n\nBusiness Queries\n\n7-day Simple Moving Average (SMA)\nRank product categories by shipping cost for each shipping address\nDaily counts of open jobs (where “open” is an untracked daily status)\nGet the latest order from each customer\nOverall median price\nMedian price for each product\nOverall median price and quantity\n\nProcessing Expressions\n\nProvide subtotals for a hierarchical group of fields (e.g. family, category, subcategory)\n\nSee NULLs &gt;&gt; COALESCE\n\n\n\nOrder of Operations\n\n\nHigher ranked functions can be inserted inside lower ranked functions\n\ne.g a window function can be inside a SELECT function but not inside a WHERE clause\nThere are exceptions and hacks around this in some cases\n\n\nTypes of Commands\n\nData Query Language (DQL) - used to find and view data without making any permanent changes to the database.\nData Manipulation Language (DML) - used to make permanent changes to the data, such as updating values or deleting them.\nData Definition Language (DDL) - used to make permanent changes to the table, such as creating or deleting a table.\nData Control Language (DCL) - used for administrative commands, such as adding or removing users of different tables and databases.\nTransact Control Language (TCL) - advanced SQL that deals with transaction level statements.\n\nMicrosoft SQL Server format for referencing a table:\n\n[database].[schema].[tablename]\nAlternative\nUSE my_data_base\nGO\n\nCheck if a table is updatable\nSELECT table_name, is_updatable\nFROM information_schema.views\n\nUseful if some of the tables you are working with are missing values that you need to add\nIf not updatable, then you’ll need to contact the database administrator to request permission to update that specific table\nShow all tables\nSHOW FULL TABLES -- mysql",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-setup",
    "href": "qmd/sql.html#sec-sql-setup",
    "title": "SQL",
    "section": "Set-up",
    "text": "Set-up\n\npostgres\n\nDownload postgres\npgAdmin is an IDE commonly used with postgres\n\nOpen pgAdmin and click on “Add new server.”\n\nSets up connection to existing server so make sure postgres is installed beforehand\n\nCreate Tables\n\nhome &gt;&gt; Databases (1) &gt;&gt; postgres &gt;&gt; Query Tool\n\nIf needed, give permission to pgAdmin to access data from a folder\n\nMight be necessary to upload csv files\n\nImport csv file\n\nright-click the table name &gt;&gt; Import/Export\nOptions tab\n\nSelect import, add file path to File Name, choose csv for format, select Yes for Header, add , for Delimiter\n\nColumns tab\n\nuncheck columns not in the csv (probably the primary key)\n\nWonder if NULLs will be automatically inserted for columns in the table that aren’t in the file.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-terms",
    "href": "qmd/sql.html#sec-sql-terms",
    "title": "SQL",
    "section": "Terms",
    "text": "Terms\n\nBatch - a set of sql statements e.g. statements between BEGIN and END\nCompiled object - you can create a function written in C/C++ and load into the database (at least in postgres) to achieve high performance.\nCorrelated Columns - tells how good the match between logical and physical ordering is.\nCorrelated/Uncorrelated Subqueries\n\ncorrelated- a type of query, where inner query depends upon the outcome of the outer query in order to perform its execution\n\nA correlated subquery can be thought of as a filter on the table that it refers to, as if the subquery were evaluated on each row of the table in the outer query\n\n\nuncorrelated - a type of sub-query where inner query doesn’t depend upon the outer query for its execution.\n\nIt is an independent query, the results of which are returned to and used by the outer query once (not per row).\n\n-- Uncorrelated subquery:\n-- inner query, c1, only depends on table2\nselect c1, c2\n  from table1 where c1 = (select max(x) from table2);\n\n-- Correlated subquery:\n-- inner query, c1, depends on table1 and table2\nselect c1, c2\n  from table1 where c1 = (select x from table2 where y = table1.c2);\n\nFunctions execute at a different level of priority and are handled differently than Views. You will likely see better performance.\nIndex - a quick lookup table (e.g. field or set of fields) for finding records users need to search frequently. An index is small, fast, and optimized for quick lookups. It is very useful for connecting the relational tables and searching large tables. (also see DB, Engineering &gt;&gt; Cost Optimizations)\nMigrations (schema) - version control system for your database schema. Management of incremental, reversible changes and version control to relational database schemas. A schema migration is performed on a database whenever it is necessary to update or revert that database’s schema to some newer or older version.\nPhysical Ordering - A PostgreSQL table consists of one or more files of 8KB blocks (or “pages”). The order in which the rows are stored in the file is the physical ordering.\nPredicate - defines a logical condition being applied to rows in a table. (e.g. IN, EXISTS, BETWEEEN, LIKE, ALL, ANY)\nScalar/Non-Scalar Subqueries\n\nA scalar subquery returns a single value (one column of one row). If no rows qualify to be returned, the subquery returns NULL.\nA non-scalar subquery returns 0, 1, or multiple rows, each of which may contain 1 or multiple columns. For each column, if there is no value to return, the subquery returns NULL. If no rows qualify to be returned, the subquery returns 0 rows (not NULLs).\n\nSelectivity - the fraction of rows in a table or partition that is chosen by the predicate\n\nRefers to the quality of a filter in its ability to reduce the number of rows that will need to be examined and ultimately returned\n\nWith a high selectivity, using the primary key or indexes to get right to the rows of interest\nWith a low selectivity, a full table scan would likely be needed to get the rows of interest.\n\nHigher selectivity means: more unique data; fewer duplicates; fewer number of rows for each key value\nUsed to estimate the cost of a particular access method; it is also used to determine the optimal join order. A poor choice of join order by the optimizer could result in a very expensive execution plan.\n\nSoft-deleted - An operation in which a flag is used to mark data as unusable, without erasing the data itself from the database\nSurrogate key - very similar to a primary key in that it is a unique value for an object in a table. However, rather than being derived from actual data present in the table, it is a field generated by the object itself. It has no business value like a primary key does, but is rather only used for data analysis purposes. Can be generated using different columns that already exist in your table or more often from two or more tables. dbt function definition\n\nExamples: PostgreSQL serial column, Oracle sequence column, or MySQL auto_increment column, Snowflake _file + _line columns\nExample: Each employee id is concatenated with a department id (e.g. marketing or finance)\n\n\nTransaction - a set of queries tied together such that if one query fails, the entire set of queries are rolled back to a pre-query state if the situation dictates.\n\nA database transaction, by definition, must be atomic, consistent, isolated and durable. These are popularly known as ACID properties.  These properties can ensure the concurrent execution of multiple transactions without conflict.\n\nViews - database objects that represent saved SELECT queries in “virtual” tables.\n\nContains a query plan.  For each query executed, the planner has to evaluate what’s being asked and calculate an optimal path. Views already have this plan calculated so it allows subsequent queries to be returned with almost no friction in processing aside from data retrieval.\nSome views are updateable but under certain conditions (1-1 mapping of rows in view to underlying table, no group_by, etc.)",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-basics",
    "href": "qmd/sql.html#sec-sql-basics",
    "title": "SQL",
    "section": "Basics",
    "text": "Basics\n\nCreate Tables\n\nIf you don’t include the schema as part of the table name (e.g. schema_name.table_name), pgadmin automatically places it into the “public” schema directory\nField Syntax: name, data type, constraints\nExample: Create table as select (CTAS)\nCREATE TABLE new_table AS \nSELECT * \nFROM old_table \nWHERE condition;\nExample: Table 1 (w/primary key)\nDROP TABLE IF EXISTS classrooms CASCADE;\nCREATE TABLE classrooms (\n    id INT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,\n    teacher VARCHAR(100)\n    );\n -- OR\nCREATE TABLE classrooms ( \n    id INT GENERATED ALWAYS AS IDENTITY, \n    teacher VARCHAR(100)\n    PRIMARY KEY(id) \n    );   \n\n“classrooms” is the name of the table; “id” and “teacher” are the fields\nCASCADE - postgres won’t delete the table if other tables point to it, so cascade will override measure.\nGENERATED ALWAYS AS IDENTITY - makes it so you don’t have to keep track of which “id” values have been used when adding rows. You can ommit the value for “id” and just add the values for the other fields\nSee tutorial for options, usage, removing, adding, etc. this constraint\nINSERT INTO classrooms\n    (teacher)\nVALUES\n    ('Mary'),\n    ('Jonah');\n\nAlso see Add Data &gt;&gt; Example: chatGPT\n\n\nExample: Table 2 (w/foreign key)\nDROP TABLE IF EXISTS students CASCADE;\nCREATE TABLE students (\n    id INT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,\n    name VARCHAR(100),\n    classroom_id INT,\n    CONSTRAINT fk_classrooms\n        FOREIGN KEY(classroom_id)\n        REFERENCES classrooms(id)\n);\n\n“students” is the name of the table; “id”, “name”, and “classroom_id” are the fields\nCreate a foreign key that points to the “classrooms” table\n\nExpression\n\nfk_classrooms is the name of the CONSTRAINT\n“classroom_id” is the field that will be the FOREIGN KEY\nREFERENCES points the foreign key to the classrooms table’s primary key, “id”\n\nforeign keys can point to any table\n\n\nPostgres won’t allow you to insert a row into students with a “classroom_id” that doesn’t exist in the “id” field of classrooms but will allow you to use a NULL placeholder\n-- Explicitly specify NULL\nINSERT INTO students\n    (name, classroom_id)\nVALUES\n    ('Dina', NULL); \n\n-- Implicitly specify NULL\nINSERT INTO students\n    (name)\nVALUES\n    ('Evan');\n\nAlso see Add Data &gt;&gt; Example: chatGPT\n\n\n\nExample\nCREATE TABLE members (\n    id serial primary key,\n    second_name character varying(200) NOT NULL,\n    date_joined date NOT NULL DEFAULT current_date,\n    member_id integer references members(id),\n    booking_start_time timestamp without timezone NOT NULL\n\nThe “serial” data type does the same thing as GENERATED ALWAYS AS IDENTITY (see first example), but is NOT compliant with the SQL standard. Use GENERATED ALWAYS AS IDENTITY\n“references” seems to be another old way to create foreign keys (see 2nd example for proper way)\n“character varying” - variable-length with limit (e.g limit of 200 characters)\n\ncharacter(n), char(n) are for fixed character lengths; text is for unlimited character lengths\n\n“current_date” is a function that will insert the current date as a value\n“timestamp without timezone” is literally that\n\nalso available: time with/without timezone, date, interval (see Docs for details)\n\n\nExample: MySQL\nCREATE DATABASE products;\n\nCREATE TABLE `products`.`prices` (\n  `pid` int(11) NOT NULL AUTO_INCREMENT,\n  `category` varchar(100) NOT NULL,\n  `price` float NOT NULL,\n  PRIMARY KEY (`pid`)\n);\n\nINSERT INTO products.prices\n    (pid, category, price)\nVALUES\n    (1, 'A', 2),\n    (2, 'A', 1),\n    (3, 'A', 5),\n    (4, 'A', 4),\n    (5, 'A', 3),\n    (6, 'B', 6),\n    (7, 'B', 4),\n    (8, 'B', 3),\n    (9, 'B', 5),\n    (10, 'B', 2),\n    (11, 'B', 1)\n;\n\n\n\nAdd Data\n\nExample: Copy/Paste table values into chatGPT to get the query\n\nExample: Add data via .csv\nCOPY assignments(category, name, due_date, weight)\nFROM 'C:/Users/mgsosna/Desktop/db_data/assignments.csv'\nDELIMITER ','\nCSV HEADER;\n\n“assignments” is the table; “category”, “name”, “due_date”, “weight” are fields that you want to import from the csv file\n** The order of the columns must be the same as the ones in the CSV file **\nHEADER keyword to indicate that the CSV file contains a header\nMight need to have superuser access in order to execute the COPY statement successfully\n\n\n\n\nUpdate Table\n\nUpdate target table by transaction id (BQ)(link)\ninsert target_table (transaction_id)\n  select transaction_id \n  from source_table \n  where transaction_id &gt; (select max(transaction_id) from target_table)\n;\n\nMight not be possible with denormalized star-schema datasets in modern data warehouses.\n\nUpdate Target Table by transaction date (BQ) (link)\nmerge last_online t\nusing (\n  select\n      user_id,\n      last_online\n  from\n    (\n        select\n            user_id,\n            max(timestamp) as last_online\n\n        from \n            connection_data\n        where\n            date(_partitiontime) &gt;= date_sub(current_date(), \n                                             interval 1 day)\n        group by\n            user_id\n\n    ) y\n\n) s\non t.user_id = s.user_id\nwhen matched then\n  update set last_online = s.last_online, \n             user_id = s.user_id\nwhen not matched then\n  insert (last_online, user_id) \n    values (last_online, user_id)\n;\nselect * from last_online\n;\n\nMERGE performs UPDATE, DELETE, and INSERT\n\nUPDATE or DELETE clause can be used when two or more data match.\nINSERT clause can be used when two or more data are different and do not match.\nThe UPDATE or DELETE clause can also be used when the given data does not match the source.\n\n_partitiontime is a field BQ creates to record the row’s ingestion time (See Google, Big Query &gt;&gt; Optimization &gt;&gt; Partitions\n\nUpdate Target Table with Source Data (link)\nMERGE INTO target_table tgt\nUSING source_table src \n ON tgt.customer_id = src.customer_id\nWHEN MATCHED THEN\n UPDATE SET\n   tgt.is_active = src.is_active,\n   tgt.updated_date = '2024-04-01'::DATE\nWHEN NOT MATCHED THEN\n INSERT\n   (customer_id, is_active, updated_date)\n VALUES\n (src.customer_id, src.is_active, '2024-04-01'::DATE)\n; \n\nThe statement uses the MERGE keyword to conditionally update or insert rows into a target table based on a source table.\nIt matches rows between the tables using the ON clause and the customer_id column.\nThe WHEN MATCHED THEN clause specifies the update actions for matching rows.\nThe WHEN NOT MATCHED THEN clause specifies the insert actions for rows that don’t have a match in the target table.\nThe ::DATE cast ensures that the updated_date value is treated as a date.\n\n\n\n\nSubqueries\n\n**Using CTEs instead of subqueries make code more readable**\n\nSubqueries make it difficult to understand their context in the larger query\nThe only way to debug a subquery is by turning it into a CTE or pulling it out of the query entirely.\nCTEs and subqueries have a similar runtime, but subqueries make your code more complex for no reason.\n\nNotes from How to Use SubQueries in SQL\n\nAlso shows the alt method of creating a temporary table to compute the queries\n\nUse cases\n\nFiltering rows from a table with the context of another.\nPerforming double-layer aggregations such as average of averages or an average of sums.\nAccessing aggregations with a subquery.\n\nTables used in examples\n\nStore A (store_a)\n\n\nStore B is similar\n\n\nExample: Filtering rows\nselect * \nfrom sandbox.store_b\nwhere product_id IN (\n    select product_id\n    from sandbox.store_b\n    group by product_id \n    having count(product_id) &gt;= 3\n);\n\nfilters the rows with products that have been bought at least three times in store_b\n\nExample: Multi-Layer Aggregation\nselect avg(average_price.total_value) as average_transaction from (\n  select transaction_id, sum(price_paid) as total_value\n  from sandbox.store_a\n  group by transaction_id\n  ) as average_price\n;\n\ncomputes the average of all transactions\ncan’t apply an average directly, as our table is oriented to product_ids and not to transaction_ids\n\nExample: Filtering the table based on an Aggregation\nselect @avg_transaction:= avg(agg_table.total_value)\nfrom (\n  select transaction_id, sum(price_paid) as total_value\n  from sandbox.store_a\n  group by transaction_id\n) as agg_table;\n\nselect * \nfrom sandbox.store_a\nwhere transaction_id in (\n  select transaction_id\n  from sandbox.store_a\n  group by transaction_id\n  having sum(price_paid) &gt; @avg_transaction\n)\n\nfilters transactions that have a value higher than the average (where the output must retain the original product-oriented row)\n\n\n\n\nJoins\n\n\n\nCross Join -  acts like an expand_grid; where each value in the join key column gets all combinations of rows in both tables (also see above pic)\n\n\nEfficient join\nWhen you add the where clause, the cross join acts similarly to an inner join, except you aren’t joining it on any specified column\nExample:\nSELECT\n  schedule.event,\n  calendar.number_of_days\nFROM schedule\nCROSS JOIN calendar\nWHERE schedule.total_number &lt; calendar.number_of_days\n\nOnly join the row in the “schedule” table with the rows in the “calendar” table that meet the specified condition\n\n\nNatural Join - don’t need to specify join columns; need to have two columns in each table with the same name\n\nUse cases\n\nThere are a lot of common columns with the same name across multiple tables\n\nThey will all be used as joining keys.\n\nYou don’t want to type out all of the common columns in select just to avoid outputting the same columns multiple times.\n\n\nselect *\nfrom table_a\nnatural join table_b\n;\n\n-- natural + outer\nselect *\nfrom table_a\nnatural outer join table_b\n;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-r",
    "href": "qmd/sql.html#sec-sql-r",
    "title": "SQL",
    "section": "R",
    "text": "R\n\nGet query from dplyr code\ntbl_to_sql &lt;- function(tbl) {\n  dplyr::show_query(tbl) |&gt; \n    capture.output() |&gt; \n    purrr::discard_at(1) |&gt; \n    paste(collapse = \" \")\n}\n\nTransforms query into a string\nAlso see Generating SQL with {dbplyr} and sqlfluff\n\nConnect to or Create a SQLite database\ncon &lt;- DBI::dbConnect(drv = RSQLite::SQLite(),\n                      here::here(\"db_name.db\"),\n                      timeout = 10)\nConnect to Microsoft SQL Server\ncon &lt;- DBI::dbConnect(odbc::odbc(), \n                      Driver = \"SQL Server\", \n                      Server = \"SERVER\", \n                      Database = \"DB_NAME\", \n                      Trusted_Connection = \"True\", \n                      Port = 1433)\nClose connection: dbDisconnect(con)\nCreate a table from a data source: df &lt;- dbplyr::tbl(con, \"&lt;table name&gt;\")\n\nAllows you to use dplyr verbs with a remote database table then collect\n\nCancel a running query (postgres)\n# Store PID\npid &lt;- DBI::dbGetInfo(conn)$pid\n\n# Cancel query and get control of IDE back\n# SQL command\nSELECT pg_cancel_backend(&lt;PID&gt;)\n\nUseful if query is running too long and you want control of your IDE back\n\nCreate single tables from a list of tibbles to a database\npurrr::map2(table_names, list_of_tbls, ~ dbWriteTable(con, .x, .y))\nLoad all tables from a database into a list\ntables &lt;- dbListTables(con) \nall_data &lt;- map(tables, dbReadTable, conn = con)\nCan use map_dfr if all the tables have the same columns\nDynamic queries with {glue}\n\nExample: MS SQL Server\nvars &lt;- c(\"columns\", \"you\", \"want\", \"to\", \"select\")\ndate_var &lt;- 'date_col'\nstart_date &lt;- as.Date('2022-01-01')\ntoday &lt;- Sys.Date()\ntablename &lt;- \"yourtablename\"\nschema_name &lt;- \"yourschema\"\nquery &lt;- glue_sql(.con = con, \"SELECT TOP(10) {`vars`*} FROM {`schema_name`}.{`tablename`} \")\nDBI::dbGetQuery(con, query)\n\nvars format collapses the vars vector, separated by commas, so that it resembles a SELECT statement\n\n\nPRQL\n\nDocs\n{prqlr}\nA dplyr + SQL hybrid language\nUsing with DuckDB\nlibrary(prqlr); library(duckdb)\ncon &lt;- dbConnect(duckdb(), dbdir = \":memory\")\ndbWriteTable(con, \"mtcars\", mtcars)\n\"from mtcars | filter cyl &gt; 6 | select {cyl, mpg}\" |&gt; \n  prql_compile() |&gt; \n  dbGetQuery(conn = con)",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-bestp",
    "href": "qmd/sql.html#sec-sql-bestp",
    "title": "SQL",
    "section": "Best Practices",
    "text": "Best Practices\n\nMisc\n\nResources\n\nSQL Style Guide\n\nUse aliases only when table names are long enough so that using them improves readability (but choose meaningful aliases)\nDo not use SELECT *. Explicitly list columns instead\nUse comments to document business logic\nA comment at the top should provide a high-level description\nUse an auto-formatter\nGeneral Optimizations\n\nRemoving duplicates or filtering out null values at the beginning of your model will speed up queries\nReplace complex code with window functions\n\nExample: Replace GROUP_BY + TOP with a partition + FIRST_VALUE()\nFIRST_VALUE(test_score) OVER(PARTITION BY student_name ORDER BY test_score DESC)\nExample: AVG(test_score) OVER(PARTITION BY student_name)\n\n\n\nCTEs\n\nBreak down logic in CTEs using WITH … AS\nThe SELECT statement inside each CTE must do a single thing (join, filter or aggregate)\nThe CTE name should provide a high-level explanation\nThe last statement should be a SELECT statement querying the last CTE\n\nJoins\n\nUse WHERE for filtering, not for joining\nFavor LEFT JOIN over INNER JOIN; in most cases, it’s essential to know the distribution of NULLs\nAvoid using”Self-Joins.” Use window functions instead (see Google, BigQuery &gt;&gt; Optimization for details on self-joins)\nWhen doing equijoins (i.e., joins where all conditions have the something=another form), use the USING keyword\nBreak-up joins using OR into UNION because SQL uses nested operations for JOIN + OR queries which slow things.\n\nBad\n\nGood\n\nUNION simply joins the outputs of two separate SELECT statements and retains only one occurrence of duplicated rows if there are any.\n\n\nStyle Guide",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-index",
    "href": "qmd/sql.html#sec-sql-index",
    "title": "SQL",
    "section": "Indexes",
    "text": "Indexes\n\nMisc\n\nAn index may consist of up to 16 columns\nThe first column of the index must always be present in the query’s filter, order , join or group operations to be used\n\nCreate Index on an existing table (postgres)\nCREATE INDEX\n    score_index ON grades(score, student);\n\n“score_index” is the name of the index\n“grades” is the name of the table\n“score” and “student” are fields to be used as the indexes\n\nCreate Index that only uses a specific character length\n/* mysql */\nCREATE TABLE test (blob_col BLOB, INDEX(blob_col(10)));\n\nindex only uses the first 10 characters of the column value of a BLOB column type\n\nCreate index with multiple columns\n/* mysql */\nCREATE TABLE test (\n    id        INT NOT NULL,\n    last_name  CHAR(30) NOT NULL,\n    first_name CHAR(30) NOT NULL,\n    PRIMARY KEY (id),\n    INDEX name (last_name,first_name)\n)\nUsage of multiple column index (** order of columns is important **)\nSELECT * FROM test WHERE last_name='Jones';\nSELECT * FROM test\n  WHERE last_name='Jones' AND first_name='John';\nSELECT * FROM test\n  WHERE last_name='Jones'\n  AND (first_name='John' OR first_name='Jon');\nSELECT * FROM test\n  WHERE last_name='Jones'\n  AND first_name &gt;='M' AND first_name &lt; 'N';\n\nIndex is used when both columns are used as part of filtering criteria or when only the left-most column is used\nif you have a three-column index on (col1, col2, col3), you have indexed search capabilities on (col1), (col1, col2), and (col1, col2, col3).\n\nInvalid usage of multiple column index\nSELECT * FROM test WHERE first_name='John';\nSELECT * FROM test\n  WHERE last_name='Jones' OR first_name='John';\n\nThe “name” index won’t be used in these queries since\n\nfirst_name is NOT the left-most column specified in the index\nOR is used instead of AND\n\n\nCreate index with DESC, ASC\n/* mysql */\nCREATE TABLE t (\n  c1 INT, c2 INT,\n  INDEX idx1 (c1 ASC, c2 ASC),\n  INDEX idx2 (c1 ASC, c2 DESC),\n  INDEX idx3 (c1 DESC, c2 ASC),\n  INDEX idx4 (c1 DESC, c2 DESC)\n);\n\nUsed by ORDER BY\n\nSee Docs to see what operations and index types support Descending Indexes\n\nNote: idx_a on column_p, column_q desc is not the same as an * Index idx_a on column_q desc, column p or, * Index idx_b on column_p desc, column q\n\nUsage of Descending Indexes\nORDER BY c1 ASC, c2 ASC    -- optimizer can use idx1\nORDER BY c1 DESC, c2 DESC  -- optimizer can use idx4\nORDER BY c1 ASC, c2 DESC  -- optimizer can use idx2\nORDER BY c1 DESC, c2 ASC  -- optimizer can use idx3\n\nSee previous example for definition of idx* names\nSee Docs to see what operations and index types support Descending Indexes",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-part",
    "href": "qmd/sql.html#sec-sql-part",
    "title": "SQL",
    "section": "Partitioning",
    "text": "Partitioning\n\nMisc\n\nAlso see\n\nMySQL Docs\nGoogle, BigQuery &gt;&gt; Optimization &gt;&gt; Partitioning and Clustering\nDB, Engineering &gt;&gt; Cost Optimization &gt;&gt; Partitioning\n\nall of your queries to the partitioned table must contain the partition_key in the WHERE clause",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-views",
    "href": "qmd/sql.html#sec-sql-views",
    "title": "SQL",
    "section": "Views",
    "text": "Views\n\nA smaller data object that contains the subset of data resulting from a specific query\nWhereas a query happens after data is loaded, a materialized view is a precomputation\nThe computation is done once, and changes to the data are incorporated as they occur, making subsequent updates to the view much cheaper and more efficient than querying the entire database from scratch.\nCreate a View\n\nExample: Create view as select (CVAS)\nCREATE VIEW high_earner AS \nSELECT p.id AS person_id, j.salary\nFROM People p\nJOIN Job j \nON p.job = j.title\nWHERE j.salary &gt;= 200000;\n\nQuery a view (same as a table): SELECT * FROM high_earner\nUpdate view\nCREATE OR REPLACE VIEW high_earner AS \nSELECT p.id AS person_id, j.salary\nFROM People p\nJOIN Job j \nON p.job = j.title\nWHERE j.salary &gt;= 150000;\n\nExpects the query output to retain the same number of columns, column names, and column data types. Thus, any modification that results in a change in the data structure will raise an error.\n\nList views\n\nSELECT * \nFROM information_schema.views\nWHERE table_schema NOT IN ('pg_catalog', 'information_schema');\n\n“table_name” has the names of the views\n“view_definition” shows the query stored in the view\nWHERE command is included to omit built-in views from PostgreSQL.\n\nDelete view: DROP VIEW high_earner;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-vars",
    "href": "qmd/sql.html#sec-sql-vars",
    "title": "SQL",
    "section": "Variables",
    "text": "Variables\n\nMisc\n\nAlso see Business Queries &gt;&gt; Medians\n\nUser-defined\n\nDECLARE and SET\n-- Declare your variables\nDECLARE @start date\nDECLARE @stop date\n-- SET the relevant values for each variable\nSET @start = '2021-06-01'\nSET @stop = GETDATE()\n\nDECLARE sets the variable type (e.g. date)\nSET assigns a value\n\nOr just use DECLARE\nDECLARE @Iteration Integer = 0;\nExamples\n\nExample: Exclude 3 months of data from the query\nSELECT t1.[DATETIME], COUNT(*) AS vol\nFROM Medium.dbo.Earthquakes t1\nWHERE t1.[DATETIME] BETWEEN @start AND DATEADD(MONTH, -3, @stop)\nGROUP BY t1.[DATETIME]\nORDER BY t1.[DATETIME] DESC;\n\nSee above for the definitions of @start and @stop\n\nExample: Apply a counter\n-- Declare the variable (a SQL Command, the var name, the datatype)\nDECLARE @counter INT;\n-- Set the counter to 20\nSET @counter = 20;\n-- Print the initial value\nSELECT @counter AS _COUNT;\n-- Select and increment the counter by one\nSELECT @counter = @counter + 1;\n-- Print variable\nSELECT @counter AS _COUNT;\n-- Select and increment the counter by one\nSELECT @counter += 1;\n-- Print the variable\nSELECT @counter AS _COUNT;\n\n\nSystem\n\nROWCOUNT - returns the number of rows affected by the last previous statement\n\nExample\nBEGIN\n    SELECT\n        product_id,\n        product_name\n    FROM\n        production.products\n    WHERE\n        list_price &gt; 100000;\n    IF @@ROWCOUNT = 0\n        PRINT 'No product with price greater than 100000 found';\nEND",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-funs",
    "href": "qmd/sql.html#sec-sql-funs",
    "title": "SQL",
    "section": "Functions",
    "text": "Functions\n\n“||” Concantenate strings. e.g ‘Post’ || ‘greSQL’ –&gt; PostgreSQL\nBEGIN…END - defines a compound statement or statement block. A compound statement consists of a set of SQL statements that execute together. A statement block is also known as a batch\n\nA compound statement can have a local declaration for a variable, a cursor, a temporary table, or an exception\n\nLocal declarations can be referenced by any statement in that compound statement, or in any compound statement nested within it.\nLocal declarations are invisible to other procedures that are called from within a compound statement\n\n\nCOMMIT - a transaction control language that is used to permanently save the changes done in the transaction in tables/databases. The database cannot regain its previous state after its execution of commit.\nDATEADD - adds units of time to a variable or value\n\ne.g. DATEADD(month, -3, '2021-06-01')\n\nsubtracts 3 months from 2021-06-01\n\n\nDATE_TRUNC - pulls a component of a date object.\n\ne.g. date_trunc('month', date_var) as month\n\nDENSE_RANK- similar to the RANK , but it does not skip any numbers even if there is a tie between the rows.\n\nValues are ranked by the column specified in ORDER BY expression of the window function\n\nEXPLAIN - a means of running your query as a what-if to see what the planner thinks about it. It will show the process the system goes through to get to the data and return it.\nEXPLAIN\nSELECT\n    s.id AS student_id,\n    g.score\nFROM\n    students AS s\nLEFT JOIN\n    grades AS g\n    ON s.id = g.student_id\nWHERE\n    g.score &gt; 90\nORDER BY\n    g.score DESC;\n/*\nQUERY PLAN\n----------\nSort (cost=80.34..81.88 rows=617 width=8)\n[...] Sort Key: g.score DESC\n[...] -&gt; Hash Join (cost=16.98..51.74 rows=617 width=8)\n[...] Hash Cond: (g.student_id = s.id)\n[...] -&gt; Seq Scan on grades g (cost=0.00..33.13 rows=617 width=8)\n[...] Filter: (score &gt; 90)\n[...] -&gt; Hash (cost=13.10..13.10 rows=310 width=4)\n[...] -&gt; Seq Scan on students s (cost=0.00..13.20 rows=320 width=4)\n*/\n\nSequentially scanning (“Seq Scan”) the grades and students tables because the tables aren’t indexed\n\nAny Seq Scan, parallel or not, is sub-optimal\n\nEXPLAIN (BUFFERS) also shows how may data pages the database had to fetch using slow disk read operations (“read”), and how many of them were cached in memory (“shared hit”)\n\nEXPLAIN ANALYZE - tells the planner to not only hypothesize on what it would do, but actually run the query and show the results.\n\nshows where indexes are being hit — or not hit as it may be. You can step through and re-optimize your basic and complex queries.\n\nGETDATE() - Gets the current date\nGO - Not a sql function. Used by some interpreters as a reset.\n\ni.e. any variables set before the GO statement will now not be recognized by the interpreter.\nHelps to separate code into different sections\n\nISDATE - boolean - checks that a variable is a date type\nQUALIFY - clause filters the results of window functions.\n\nuseful when answering questions like fetching the most XXX value of each category\nQUALIFY does with window functions as what HAVING does with GROUP BY. As a result, in the order of execution, QUALIFY is evaluated after window functions.\nSee Business Queries &gt;&gt; Get the latest order from each customer\n\nUNNEST - BigQuery - takes an ARRAY and returns a table with a row for each element in the ARRAY (docs)\n\nGoogle Analytics, Analysis &gt;&gt; Example 17",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-udfs",
    "href": "qmd/sql.html#sec-sql-udfs",
    "title": "SQL",
    "section": "User Defined Functions (UDF)",
    "text": "User Defined Functions (UDF)\n\nMisc\n\nAvailable in SQL Server (Docs1, Docs2), Postgres (Docs), BigQuery (docs), etc.\nKeep a dictionary with the UDFs you’ve created and make sure to share it with any collaborators.\nCan be persistent or temporary\n\nPersistent UDFs can be used across multiple queries, while temporary UDFs only exist in the scope of a single query\n\n\nCreate\n\nExample: temporary udf (BQ)\nCREATE TEMP FUNCTION AddFourAndDivide(x INT64, y INT64)\nRETURNS FLOAT64\nAS (\n  (x + 4) / y\n);\nSELECT\n  val, AddFourAndDivide(val, 2)\nFROM\n  UNNEST([2,3,5,8]) AS val;\nExample: persistent udf (BQ)\nCREATE FUNCTION mydataset.AddFourAndDivide(x INT64, y INT64)\nRETURNS FLOAT64\nAS (\n  (x + 4) / y\n);\n\nSELECT\n  val, mydataset.AddFourAndDivide(val, 2)\nFROM\n  UNNEST([2,3,5,8,12]) AS val;\n\nDelete persistent udf: DROP FUNCTION &lt;udf_name&gt;\nWith Scalar subquery (BQ)\nCREATE TEMP FUNCTION countUserByAge(userAge INT64)\nAS (\n  (SELECT COUNT(1) FROM users WHERE age = userAge)\n);\nSELECT\n  countUserByAge(10) AS count_user_age_10,\n  countUserByAge(20) AS count_user_age_20,\n  countUserByAge(30) AS count_user_age_30;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-loops",
    "href": "qmd/sql.html#sec-sql-loops",
    "title": "SQL",
    "section": "Loops",
    "text": "Loops\n\nMisc\n\nposgres docs for loops\n\nWHILE\n\nExample: Incrementally add to a counter variable\n-- Declare the initial value\nDECLARE @counter INT;\nSET @counter = 20;\n-- Print initial value\nSELECT @counter AS _COUNT;\n-- Create a loop\nBEGIN;\n-- Loop code starting point\nWHILE @counter &lt; 30\nSELECT @counter = @counter + 1;\n-- Loop finish\nEND;\n-- Check the value of the variable\nSELECT @counter AS _COUNT;\n\nCursors (Docs)\n\nRather than executing a whole query at once, it is possible to set up a cursor that encapsulates the query, and then read the query result a few rows at a time.\n\nOne reason for doing this is to avoid memory overrun when the result contains a large number of rows. (However, PL/pgSQL users do not normally need to worry about that, since FOR loops automatically use a cursor internally to avoid memory problems.)\nA more interesting usage is to return a reference to a cursor that a function has created, allowing the caller to read the rows. This provides an efficient way to return large row sets from functions.\n\nExample (article (do not pay attention dynamic sql. it’s for embedding sql in C programs))\nDECLARE\n    cur_orders CURSOR FOR \n        SELECT order_id, product_id, quantity\n        FROM order_details\n        WHERE product_id = 456;\n    product_inventory INTEGER;\nBEGIN\n    OPEN cur_orders;\n    LOOP\n        FETCH cur_orders INTO order_id, product_id, quantity;\n        EXIT WHEN NOT FOUND;\n        SELECT inventory INTO product_inventory FROM products WHERE product_id = 456;\n        product_inventory := product_inventory - quantity;\n        UPDATE products SET inventory = product_inventory WHERE product_id = 456;\n    END LOOP;\n    CLOSE cur_orders;\n    -- do something after updating the inventory, such as logging the changes\nEND;\n\nA table called “products” that contains information about all products, including the product ID, product name, and current inventory. You can use a cursor to iterate through all orders that contain a specific product and update its inventory.\nA cursor called “cur_orders” that selects all order details that contain a specific product ID. We then define a variable called “product_inventory” to store the current inventory of the product.\nInside the loop, we fetch each order ID, product ID, and quantity from the cursor, subtract the quantity from the current inventory and update the products table with the new inventory value.\nFinally, we close the cursor and do something after updating the inventory, such as logging the changes.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-winfun",
    "href": "qmd/sql.html#sec-sql-winfun",
    "title": "SQL",
    "section": "Window Functions",
    "text": "Window Functions\n\nUnlike GROUP BY, keeps original columns after an aggregation\n\n\nAllows you to work with both aggregate and non-aggregate values all at once\n\nBetter performance than using GROUP BY + JOIN to get the same result\n\n\nDespite the order of operations, if you really need to have a window function inside a WHERE clause or GROUP BY clause, you may get around this limitation by using a subquery or a WITH query\n\nExample: Remove duplicate rows\nWITH temporary_employees as\n(SELECT \n  employee_id,\n  employee_name,\n  department,\n  ROW_NUMBER() OVER(PARTITION BY employee_name,\n                                department,\n                                employee_id) as row_count\nFROM Dummy_employees)\n\nSELECT *\nFROM temporary_employees\nWHERE row_count = 1\n\n3 Types of Window Functions\n\n\nLEAD() will give you the row AFTER the row you are finding a value for.\nLAG() will give you the row BEFORE the row you are finding a value for.\nFIRST_VALUE() returns the first value in an ordered, partitioned data output.\n\nGeneral Syntax\n\n\nwindow_function is the name of the window function we want to use (e.g. see above)\n\nexpression is the name of the column that we want the window function operated on.\n\nMay not be necessary depending on what window_function is used\n\nOVER is just to signify that this is a window function\n\nPARTITION BY divides the rows into partitions so we can specify which rows to use to compute the window function\n\npartition_list is the name of the column(s) we want to partition by (i.e. group_by)\n\nORDER BY is used so that we can order the rows within each partition. This is optional and does not have to be specified\n\norder_list is the name of the column(s) we want to order by\n\nROWS (optional; typically not used) used to subset the rows within each partition.\n\nframe_clause defines how much to offset from our current row\nSyntax: ROWS BETWEEN &lt;starting_row&gt; AND &lt;ending_row&gt;\n\nOptions for starting and ending row\n\nUNBOUNDED PRECEDING — all rows before the current row in the partition, i.e. the first row of the partition\n[some #] PRECEDING — # of rows before the current row\nCURRENT ROW — the current row\n[some #] FOLLOWING — # of rows after the current row\nUNBOUNDED FOLLOWING — all rows after the current row in the partition, i.e. the last row of the partition\n\nExamples\n\nROWS BETWEEN 3 PRECEDING AND CURRENT ROW — this means look back the previous 3 rows up to the current row.\nROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING — this means look from the first row of the partition to 1 row after the current row\nROWS BETWEEN 5 PRECEDING AND 1 PRECEDING — this means look back the previous 5 rows up to 1 row before the current row\nROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING — this means look from the first row of the partition to the last row of the partition\n\n\n\n\nExample: Average Salary by Job Title\n\n\nTables for Examples\n\nExample: Average Unit Price for each CustomerId\n\nSELECT CustomerId, \n      UnitPrice, \n      AVG(UnitPrice) OVER (PARTITION BY CustomerId) AS “AvgUnitPrice”\nFROM [Order] \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Average Unit Price for each group of CustomerId AND EmployeeId\n\nSELECT CustomerId, \n      EmployeeId, \n      AVG(UnitPrice) OVER (PARTITION BY CustomerId, EmployeeId) AS “AvgUnitPrice”\nFROM [Order] \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Create a new column that ranks Unit Price in descending order for each CustomerId\n\nSELECT CustomerId, \n      OrderDate, \n      UnitPrice, \n      ROW_NUMBER() OVER (PARTITION BY CustomerId ORDER BY UnitPrice DESC) AS “UnitRank”\nFROM [Order] \nINNER JOIN OrderDetail \nON [Order].Id = OrderDetail.OrderId\n\nSubstituting RANK in place of ROW_NUMBER should produce the same results\nNote that ranks are skipped (e.g. rank 3 for ALFKI) when there are rows with the same rank\n\nIf you don’t want ranks skipped, use DENSE_RANK for the window function\n\n\nExample: Create a new column that provides the previous order date’s Quantity for each ProductId\n\nSELECT ProductId, \n      OrderDate, \n      Quantity, \n      LAG(Quantity) OVER (PARTITION BY ProductId ORDER BY OrderDate) AS \"LAG\"\nFROM [Order] \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\n\nUse LEAD for the following quantity\n\nExample: Create a new column that provides the very first Quantity ever ordered for each ProductId\n\nSELECT ProductId, \n      OrderDate, \n      Quantity, \n      FIRST_VALUE(Quantity) OVER (PARTITION BY ProductId ORDER BY OrderDate) AS \"FirstValue\"\nFROM [Order] \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Calculate a cumulative moving average UnitPrice for each CustomerId\n\nSELECT CustomerId, \n      UnitPrice, \n      AVG(UnitPrice) OVER (PARTITION BY CustomerId \n      ORDER BY CustomerId \n      ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS “CumAvg”\nFROM [Order]\nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Rank customers for each department by amount spent\nSELECT\n    customer_name,\n    customer_id,\n    amount_spent,\n    department_id,\n    RANK(amount_spent) OVER(ORDER BY amount_spent DESC PARTITION BY department_id) AS spend_rank\nFROM employees\nExample: Find the model and year of car that been on lot the longest\nSELECT \nFIRST_VALUE(name) OVER(PARTITION BY model, year ORDER BY date_at_lot ASC) AS oldest_car_name\nmodel,\nyear\nFROM cars\nRunning Totals/Averages (Cumulative Sums)\n\nUses SUM as the window function\n\nJust replace SUM with AVG to get running averages\n\nExample\n\n\nGenerate a new dataset grouped by month, instead of timestamp. (CTE)\n\nOnly include three fields: account_id, occurred_month and total_amount_usd\nOnly computed for the following accounts: 1041 , 1051, 1061, 10141.\n\nCompute a running total ordered by occurred_month, without collapsing the rows in the result set.\n\nDisplay 2 columns: occurred_month and cum_amnt_usd_by_month\n\n\nBecause no partition was specified, the running total is applied on the full dataset and ordered by (ascending) occurred_month\n\nExample running total by grouping variable\n\nUsing previous CTE\n\nCompute a running total by account_id, ordered by occurred_month, and account_id (i.e. a separate running total for each account_id.)\n\nDisplay 3 columns: account_id, occurred_month, and cum_mon_amnt_usd_by_account\n\n\n\nSame as previous example except a partition column (account_id) is added\n\nExample Running total over various window lengths\n\nUsing previous CTE\n\nCompute a 3 months rolling running total using a window that includes the current month.\nCompute a 7 months rolling running total using a window where the current month is always the middle month.\n\n\nFirst case uses 2 PRECEDING rows and the CURRENT_ROW\nSecond case uses 3 PRECEDING rows and 3 FOLLOWING rows and the CURRENT_ROW\n\nExample: Calculate the number consecutive days spent in each country (sqlite)\nwith ordered as (\n  select \n    created,\n    country,\n    lag(country) over (order by created desc)\n      as previous_country\n  from \n    raw\n),\ngrouped as (\n  select \n    country, \n    created, \n    count(*) filter (\n      where previous_country is null\n      or previous_country != country\n    ) over (\n      order by created desc\n      rows between unbounded preceding\n      and current row\n    ) as grp\n  from \n    ordered\n)\nselect\n  country,\n  date(min(created)) as start,\n  date(max(created)) as end,\n  cast(\n    julianday(date(max(created))) -\n    julianday(date(min(created))) as integer\n  ) as days\nfrom \n  grouped\ngroup by\n  country, grp\norder by\n  start desc;\n\nPost\n\nGoes over the code and thought process step-by-step with shows original data and results during intermediate steps\n\nThread\n\nEvidently only sqlite and postgres support filter. Someone in the thread suggest an alternate method.\n\nOutput:\ncountry         start         end           days\nUnited Kingdom  2023-06-08  2023-06-08  0\nUnited States   2019-09-02  2023-05-11  1347\nFrance          2019-08-25  2019-08-31  6\nMadagascar      2019-07-31  2019-08-07  7\nFrance          2019-07-25  2019-07-25  0\nUnited States   2019-05-04  2019-06-30  57\nUnited Kingdom  2018-08-29  2018-09-10  12\nUnited States   2018-08-05  2018-08-10  5",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-ctes",
    "href": "qmd/sql.html#sec-sql-ctes",
    "title": "SQL",
    "section": "Common Table Expressions (CTE)",
    "text": "Common Table Expressions (CTE)\n\nThe result set of a query which exists temporarily and for use only within the context of a larger query. Much like a derived table, the result of a CTE is not stored and exists only for the duration of the query.\nAlso see\n\nWindow Functions &gt;&gt; Running Totals &gt;&gt; Examples\nGoogle, Google Analytics, Analysis &gt;&gt; Examples 12-15, 18, 19\n\nUse Cases\n\nNeeding to reference a derived table multiple times in a single query\nAn alternative to creating a view in the database\nPerforming the same calculation multiple times over across multiple query components\n\nImproves readability and usually no performance difference\n\nPrior to PostgreSQL 12, https://hakibenita.com/be-careful-with-cte-in-postgre-sql , something with the caching mechanism created a bottleneck. Currently, version 13 is the latest, so hopefully not a common problem anymore.\n\nSteps\n\nInitiate a CTE using “WITH”\nProvide a name for the result soon-to-be defined query\nAfter assigning a name, follow with “AS”\nSpecify column names (optional step)\nDefine the query to produce the desired result set\nIf multiple CTEs are required, initiate each subsequent expression with a comma and repeat steps 2-4.\nReference the above-defined CTE(s) in a subsequent query\n\nSyntax\nWITH\nexpression_name_1 AS\n(CTE query definition 1)\n[, expression_name_X AS\n  (CTE query definition X)\n, etc ]\nSELECT expression_A, expression_B, ...\nFROM expression_name_1\nExample\n\nComparison with a “derived” query\n“What is the average monthly cost per campaign for the company’s marketing efforts?”\nUsing CTE workflow\n-- define CTE:\nWITH Cost_by_Month AS\n(SELECT campaign_id AS campaign,\n      TO_CHAR(created_date, 'YYYY-MM') AS month,\n      SUM(cost) AS monthly_cost\nFROM marketing\nWHERE created_date BETWEEN NOW() - INTERVAL '3 MONTH' AND NOW()\nGROUP BY 1, 2\nORDER BY 1, 2)\n\n-- use CTE in subsequent query:\nSELECT campaign, avg(monthly_cost) as \"Avg Monthly Cost\"\nFROM Cost_by_Month\nGROUP BY campaign\nORDER BY campaign\nUsed derived query\n-- Derived\nSELECT campaign, avg(monthly_cost) as \"Avg Monthly Cost\"\nFROM\n    -- this is where the derived query is used\n    (SELECT campaign_id AS campaign,\n      TO_CHAR(created_date, 'YYYY-MM') AS month,\n      SUM(cost) AS monthly_cost\n    FROM marketing\n    WHERE created_date BETWEEN NOW() - INTERVAL '3 MONTH' AND NOW()\n    GROUP BY 1, 2\n    ORDER BY 1, 2) as Cost_By_Month\nGROUP BY campaign\nORDER BY campaign\n\nExample\n\nCount the number of interactions of new users\nSteps\n\nGet new users\nCount interactions\nGet interactions of new users\n\n\nWITH new_users AS (\n    SELECT id\n    FROM users\n    WHERE created &gt;= '2021-01-01'\n),\ncount_interactions AS (\n    SELECT id,\n        COUNT(*) n_interactions\n    FROM interactions\n    GROUP BY id\n),\ninteractions_by_new_users AS (\n    SELECT id,\n        n_interactions\n    FROM new_users\n        LEFT JOIN count_interactions USING (id)\n)\n\nSELECT *\nFROM interactions_by_new_users\nExample\n\nFind the average top Math test score for students in California\nSteps\n\nGet a subset of students (California)\nGet a subset of test scores (Math)\nJoin them together to get all Math test scores from California students\nGet the top score per student\nTake the overall average\n\nDerived Query (i.e. w/o CTE)\nSELECT AVG(score)\nFROM \n  (SELECT students.id, MAX(test_results.score) as score\n  FROM students \n  JOIN schools ON (\n    students.school_id = schools.id AND schools.state = 'CA'\n  )\n  JOIN test_results ON (\n    students.id = test_results.student_id\n    AND test_results.subject = 'math'\n  )\n  GROUP BY students.id) as tmp\nUsing CTE\nWITH\n  student_subset as (\n    SELECT students.id \n    FROM students \n    JOIN schools ON (\n      students.school_id = schools.id AND schools.state = 'CA'\n    )\n  ),\n  score_subset as (\n    SELECT student_id, score \n    FROM test_results \n    WHERE subject = 'math'\n  ),\n  student_scores as (\n    SELECT student_subset.id, score_subset.score\n    FROM student_subset \n    JOIN score_subset ON (\n        student_subset.id = score_subset.student_id\n    )\n  ),\n  top_score_per_student as (\n    SELECT id, MAX(score) as score \n    FROM student_scores \n    GROUP BY id\n  )\n\nSELECT AVG(score) \nFROM top_score_per_student",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-str",
    "href": "qmd/sql.html#sec-sql-str",
    "title": "SQL",
    "section": "Strings",
    "text": "Strings\n\nConcatenate\n\nAlso see Processing Expressions &gt;&gt; NULLs\n“||”\nSELECT 'PostgreSQL' || ' ' || 'Databases' AS result;\n\n    result\n--------------\nPostgreSQL Databases\nCONCAT\nSELECT CONCAT('PostgreSQL', ' ', 'Databases') AS result;\n\n    result\n--------------\nPostgreSQL Databases\nWith NULL values\nSELECT CONCAT('Harry', NULL, 'Peter');\n\n--------------\nHarryPeter\n\n“||” won’t work with NULLs\n\nColumns\nSELECT first_name, last_name, \nCONCAT(first_name,' ' , last_name) \"Full Name\" \nFROM candidates;\n\nNew column, “Full Name”, is created with concatenated columns\n\n\nSplitting (BQ)\nSELECT\n*,\nCASE WHEN ARRAY_LENGTH(SPLIT(page_location, '/')) &gt;= 5 \n          AND\n          CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+')\n          AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN \n                                      ('accessories','apparel','brands','campus+collection','drinkware',\n                                        'electronics','google+redesign',\n                                        'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                        'office','shop+by+brand','small+goods','stationery','wearables'\n                                        )\n                OR\n                LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN \n                                      ('accessories','apparel','brands','campus+collection','drinkware',\n                                        'electronics','google+redesign',\n                                        'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                        'office','shop+by+brand','small+goods','stationery','wearables'\n                                        )\n          )\n          THEN 'PDP'\n          WHEN NOT(CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+'))\n          AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                        'electronics','google+redesign',\n                                        'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                        'office','shop+by+brand','small+goods','stationery','wearables'\n                                        )\n                OR \n                LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                          'electronics','google+redesign',\n                                          'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                          'office','shop+by+brand','small+goods','stationery','wearables'\n                                          )\n          )\n          THEN 'PLP'\n      ELSE page_title\n      END AS page_title_adjusted \nFROM \n  unnested_events\n\nFrom article, gist\nQuery is creating a new categorical column, “page_title_adjusted,” that is “PDP” when a substring in “page_location” is one of a set of words, and “PLP” when it’s not, and the value of page_title otherwise.\nSPLIT splits the string by separator, ‘/’\nCONTAINS_SUBSTR is looking for substring with a “+”\n[SAFE_OFFSET(3)] pulls the 4th substring (think this indexes by 0?)\nAfter it’s been reversed via ARRAY_REVERSE (?)\nELSE says use the value for page_title when length of the substrings after splitting page_location is 5 or less\n“unnested_events” is a CTE",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-arr",
    "href": "qmd/sql.html#sec-sql-arr",
    "title": "SQL",
    "section": "Arrays",
    "text": "Arrays\n\nMisc\n\nPostGres\n\nIndexing Arrays starts at 1, not at 0\n\n\nCreate Array (BQ)\nSELECT ARRAY\n  (SELECT 1 UNION ALL\n  SELECT 2 UNION ALL\n  SELECT 3) AS new_array;\n+-----------+\n| new_array |\n+-----------+\n| [1, 2, 3] |\n+-----------+\n\nSELECT\n  ARRAY\n    (SELECT AS STRUCT 1, 2, 3\n    UNION ALL SELECT AS STRUCT 4, 5, 6) AS new_array;\n+------------------------+\n| new_array              |\n+------------------------+\n| [{1, 2, 3}, {4, 5, 6}] |\n+------------------------+\n\nSELECT ARRAY\n  (SELECT AS STRUCT [1, 2, 3] UNION ALL\n  SELECT AS STRUCT [4, 5, 6]) AS new_array;\n+----------------------------+\n| new_array                  |\n+----------------------------+\n| [{[1, 2, 3]}, {[4, 5, 6]}] |\n+----------------------------+\nCreate a table with Arrays (Postgres)\n\nCREATE TEMP TABLE shopping_cart (\n  cart_id serial PRIMARY KEY,\n  products text ARRAY\n  );\nINSERT INTO\n  shopping_cart(products)\nVALUES\n  (ARRAY['product_a', 'product_b']),\n  (ARRAY['product_c', 'product_d']),\n  (ARRAY['product_a', 'product_b', 'product_c']),\n  (ARRAY['product_a', 'product_b', 'product_d']),\n  (ARRAY['product_b', 'product_d']);\n\n-- alt syntax w/o ARRAY\nINSERT INTO\n  shopping_cart(products)\nVALUES\n  ('{\"product_a\", \"product_d\"}');\n\nAlso see Basics &gt;&gt; Add Data &gt;&gt; Example: chatGPT\n\nSubset an array (postgres)\n\nSELECT\n  cart_id,\n  products[1] AS first_product -- indexing starts at 1\nFROM\n  shopping_cart;\nSlice an array (postgres)\n\nSELECT\n  cart_id,\n  products [1:2] AS first_two_products\nFROM\n  shopping_cart\nWHERE\n  CARDINALITY(products) &gt; 2;\nUnnest an array (postgres)\n\nSELECT\n  cart_id,\n  UNNEST(products) AS products\nFROM\n  shopping_cart\nWHERE\n  cart_id IN (3, 4);\n\nUseful if you want to perform a join\n\nFilter according to items in arrays (postgres)\nSELECT\n  cart_id,\n  products\nFROM\n  shopping_cart\nWHERE\n  'product_c' = ANY (products);\n\nOnly rows with arrays that have “product_c” will be returned\n\nChange array values using UPDATE, SET\n-- update arrays \nUPDATE\n  shopping_cart\nSET\n  products = ARRAY['product_a','product_b','product_e']\nWHERE\n  cart_id = 1;\n\nUPDATE \n  shopping_cart\nSET\n  products[1] = 'product_f'\nWHERE\n  cart_id = 2;\nSELECT\n  *\nFROM\n  shopping_cart\nORDER BY cart_id;\n\nFirst update: all arrays where cart_id == 1 are set to [‘product_a’,‘product_b’,‘product_e’]\nSecond update: all array first values where cart_id == 2 are set to ‘product_f’\n\nInsert array values\n\nARRAY_APPEND - puts value at the end of the array\nUPDATE\n  shopping_cart\nSET\n  products = ARRAY_APPEND(products, 'product_x')\nWHERE\n  cart_id = 1;\n\narrays in product column where cart_id == 1 get “product_x” appended to the end of their arrays\n\nARRAY_PREPEND - puts value at the beginning of the array\nUPDATE \n  shopping_cart\nSET\n  products = ARRAY_PREPEND('product_x', products)\nWHERE\n  cart_id = 2;\n\narrays in product column where cart_id == 2 get “product_x” prepended to the beginning of their arrays\n\n\nARRAY_REMOVE - remove array item\nUPDATE\n  shopping_cart\nSET\n  products = array_remove(products, 'product_e')\nWHERE cart_id = 1;\n\narrays in product column where cart_id == 1 get “product_e” removed from their arrays\n\nARRAY_CONCAT(BQ), ARRAY_CAT(postgres) - Concantenate\nSELECT ARRAY_CONCAT([1, 2], [3, 4], [5, 6]) as count_to_six;\n+--------------------------------------------------+\n| count_to_six                                    |\n+--------------------------------------------------+\n| [1, 2, 3, 4, 5, 6]                              |\n+--------------------------------------------------+\n\n-- postgres\nSELECT\n  cart_id,\n  ARRAY_CAT(products, ARRAY['promo_product_1', 'promo_product_2'])\nFROM shopping_cart\nORDER BY cart_id;\nARRAY_TO_STRING - Coerce to string (BQ)\nWITH items AS\n  (SELECT ['coffee', 'tea', 'milk' ] as list\n  UNION ALL\n  SELECT ['cake', 'pie', NULL] as list)\nSELECT ARRAY_TO_STRING(list, '--') AS text\nFROM items;\n+--------------------------------+\n| text                          |\n+--------------------------------+\n| coffee--tea--milk              |\n| cake--pie                      |\n+--------------------------------+\n\nWITH items AS\n  (SELECT ['coffee', 'tea', 'milk' ] as list\n  UNION ALL\n  SELECT ['cake', 'pie', NULL] as list)\nSELECT ARRAY_TO_STRING(list, '--', 'MISSING') AS text\nFROM items;\n+--------------------------------+\n| text                          |\n+--------------------------------+\n| coffee--tea--milk              |\n| cake--pie--MISSING            |\n+--------------------------------+\nARRAY_AGG - gather values of a group by variable into an array (doc)\n\nMakes the output more readable\nExample: Get categories for each brand\n-- without array_agg\nselect\n    brand,\n    category\nfrom order_item\ngroup by brand, category\norder by brand, category\n;\nResults:\n| brand  | category  | \n| ------ | ---------- | \n| Arket  | jacket    |\n| COS    | shirts    |\n| COS    | trousers  | \n| COS    | vest      |\n| Levi's | jacket    |\n| Levi's | jeans      |\n\n-- with array_agg\nselect\n  brand,\n  array_agg(distinct category) as all_categories\nfrom order_item\ngroup by brand\norder by brand\n;\nResults:\n| brand  | all_categories              | \n| ------ | ---------------------------- | \n| Arket  | ['jacket']                  |\n| COS    | ['shirts','trousers','vest'] |\n| Levi's | ['jacket','jeans']          |\n| Uniqlo | ['shirts','t-shirts','vest'] |\n\nARRAY_SIZE - function takes an array or a variant as input and returns the number of items within the array/variant (doc)\n\nExample: How many categories does each brand have?\nselect\n  brand,\n  array_agg(distinct category) as all_categories,\n  array_size(all_categories) as no_of_cat\nfrom order_item\ngroup by brand\norder by brand\n;\nResults:\n| brand  | all_categories              | no_of_cat |\n| ------ | ---------------------------  | --------- |\n| Arket  | ['jacket']                  | 1        |\n| COS    | ['shirts','trousers','vest'] | 3        |\n| Levi's | ['jacket','jeans']          | 2        |\n| Uniqlo | ['shirts','t-shirts','vest'] | 3        |\n\n-- postgres using CARDINALITY to get array_size\nSELECT\n  cart_id,\n  CARDINALITY(products) AS num_products\nFROM\n  shopping_cart;\n\nARRAY_CONTAINS checks if a variant is included in an array and returns a boolean value. (doc)\n\nVariant is just a specific category\nNeed to cast the item you’d like to check as a variant first\nSyntax: ARRAY_CONTAINS(variant, array)\nExample: What brands have jackets?\nselect\n  brand,\n  array_agg(distinct category) as all_categories,\n  array_size(all_categories) as no_of_cat,\n  array_contains('jacket'::variant,all_categories) as has_jacket\nfrom order_item\ngroup by brand\norder by brand\n;\nResults:\n| brand  | all_categories              | no_of_cat | has_jacket |\n| ------ | ---------------------------  | --------- | ---------- |\n| Arket  | ['jacket']                  | 1        | true      |\n| COS    | ['shirts','trousers','vest'] | 3        | false      |\n| Levi's | ['jacket','jeans']          | 2        | true      |\n| Uniqlo | ['shirts','t-shirts','vest'] | 3        | false      |\n\n-- postgres contains_operator, @&gt;\nSELECT\n  cart_id,\n  products\nFROM\n  shopping_cart\nWHERE\n  products  @&gt; ARRAY['product_a', 'product_b'];\n\n“@&gt;” example returns all rows with arrays containing product_a and product_b",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-bizq",
    "href": "qmd/sql.html#sec-sql-bizq",
    "title": "SQL",
    "section": "Business Queries",
    "text": "Business Queries\n\nSimple Moving Average (SMA)\n\nExample: 7-day SMA including today\nSELECT\n  Date, Conversions,\n  AVG(Conversions) OVER (ORDER BY Date ROWS BETWEEN 6 PRECEDING AND\n  CURRENT ROW) as SMA\nFROM daily_sales\n\nExample: 3-day SMA not including today\nselect\n  date,\n  sales,\n  avg(sales) over (order by date\n        rows between 3 preceding and current row - 1) as moving_avg\nfrom table_daily_sales\nExample: Rank product categories by shipping cost for each shipping address\n\nSELECT Product_Category,\n  Shipping_Address,\n  Shipping_Cost,\n  ROW_NUMBER() OVER\n              (PARTITION BY Product_Category,\n                            Shipping_Address\n              ORDER BY Shipping_Cost DESC) as RowNumber,\n  RANK() OVER \n        (PARTITION BY Product_Category,\n                      Shipping_Address\n        ORDER BY Shipping_Cost DESC) as RankValues,\n  DENSE_RANK() OVER \n              (PARTITION BY Product_Category,\n                            Shipping_Address \n              ORDER BY Shipping_Cost DESC) as DenseRankValues\nFROM Dummy_Sales_Data_v1\nWHERE Product_Category IS NOT NULL\nAND Shipping_Address IN ('Germany','India')\nAND Status IN ('Delivered')\n\nRANK() retrieves ranked rows based on the condition of ORDER BY clause. As you can see there is a tie between 1st two rows i.e. first two rows have same value in Shipping_Cost column (which is mentioned in ORDER BY clause).\nDENSE_RANK is similar to the RANK , but it does not skip any numbers even if there is a tie between the rows. This you can see in Blue box in the above picture.\nRank resets to 1 when “Shipping_Address” changes location\n\nExample: Total order quantity for each month\nSELECT strftime('%m', OrderDate) as Month,\n      SUM(Quantity) as Total_Quantity\nfrom Dummy_Sales_Data_v1\nGROUP BY strftime('%m', OrderDate)\n\nstrftime extracts the month (%m) from the datetime column, “OrderDate”\n\nExample: Daily counts of open jobs\n\nThe issue is that there aren’t rows for transactions that remain in a type of holding status\n\ne.g. Job Postings website has date columns for the date the job posting was created, the date the job posting went live on the website, and the date the job posting was taken down (action based timestamps), but no dates for the status between “went live” and “taken down”.\n\n\n-- create a calendar column\nSELECT parse_datetime('2020–01–01 08:00:00', 'yyyy-MM-dd H:m:s') + (interval '1' day * d) as cal_date from \nFROM ( SELECT\nROW_NUMBER() OVER () -1 as d\nFROM\n(SELECT 0 as n UNION SELECT 1) p0,\n(SELECT 0 as n UNION SELECT 1) p1,\n(SELECT 0 as n UNION SELECT 1) p2,\n(SELECT 0 as n UNION SELECT 1) p3,\n(SELECT 0 as n UNION SELECT 1) p4,\n(SELECT 0 as n UNION SELECT 1) p5,\n(SELECT 0 as n UNION SELECT 1) p6,\n(SELECT 0 as n UNION SELECT 1) p7,\n(SELECT 0 as n UNION SELECT 1) p8,\n(SELECT 0 as n UNION SELECT 1) p9,\n(SELECT 0 as n UNION SELECT 1) p10\n)\n\n-- left-join your table to the calendar column\nSelect\n    c.cal_date,\n    count(distinct opp_id) as \"historical_prospects\"\nFrom calendar c\nLeft Join\n    opportunities o\n    on\n        o.stage_entered ≤ c.cal_date \n        and (o.stage_exited is null or o.stage_exited &gt; c.cal_date)\n\nCalendar column should probably be a CTE\nNotes from Using SQL to calculate trends based on historical status\nSome flavours of SQL have a generate_series function, which will create this calendar column for you\nFor one particular month, then create an indicator column with “if posting_publish_date ≤ 2022–01–01 and (posting_closed_date is null or posting_closed_date &gt; 2022–01–31) then True” and then filter for True and count.\n\nExample: Get the latest order from each customer\n-- Using QUALIFY\nselect\n    date,\n    customer_id,\n    order_id,\n    price\nfrom customer_order_table\nqualify row_number() over (partition by customer_id order by date desc) = 1\n;\n\n-- CTE w/window function\nwith order_order as\n(\nselect\n    date,\n    customer_id,\n    order_id,\n    price,\n    row_number() over (partition by customer_id order by date desc)   \n    as order_of_orders\nfrom customer_order_table \n)\n\nselect\n    *\nfrom order_order\nwhere order_of_orders = 1\n;\nResults:\n| date      | customer_id | order_id | price |\n|------------|-------------|----------|-------|\n| 2022-01-03 | 002        | 212      | 350  |\n| 2022-01-06 | 005        | 982      | 300  |\n| 2022-01-07 | 001        | 109      | 120  |\nMedians\n\nNotes from How to Calculate Medians with Grouping in MySQL\n\nVariables:\n\npid: unique id variable\ncategory: A or B\nprice: random value between 1 and 6\n\n\nExample: Overall median price\nSELECT AVG(sub.price) AS median\nFROM ( \n    SELECT @row_index := @row_index + 1 AS row_index, p.price\n    FROM products.prices p, (SELECT @row_index := -1) r\n    WHERE p.category = 'A'\n    ORDER BY p.price \n) AS sub\nWHERE sub.row_index IN (FLOOR(@row_index / 2), CEIL(@row_index / 2))\n;\n\nmedian|\n------+\n   3.0|\n\n@row_index is a SQL variable that is initiated in the FROM statement and updated for each row in the SELECT statement.\nThe column whose median will be calculated (the price column in this example) should be sorted. It doesn’t matter if it’s sorted in ascending or descending order.\nAccording to the definition of median, the median is the value of the middle element (total count is odd) or the average value of the two middle elements (total count is even). In this example, category A has 5 rows and thus the median is the value of the third row after sorting. The values of both FLOOR(@row_index / 2) and CEIL(@row_index / 2) are 2 which is the third row. On the other hand, for category B which has 6 rows, the median is the average value of the third and fourth rows.\n\nExample: Median price for each product\nSELECT\n    sub2.category,\n    CASE WHEN MOD(sub2.total, 2) = 1 THEN sub2.mid_prices\n         WHEN MOD(sub2.total, 2) = 0 THEN (SUBSTRING_INDEX(sub2.mid_prices, ',', 1) + SUBSTRING_INDEX(sub2.mid_prices, ',', -1)) / 2\n    END AS median    \nFROM \n    (\n        SELECT \n            sub1.category,\n            sub1.total,\n            CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', CEIL(sub1.total/2)), ',', '-1')\n                 WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', sub1.total/2 + 1), ',', '-2')\n            END AS mid_prices\n        FROM \n            (\n                SELECT\n                    p.category,\n                    GROUP_CONCAT(p.price ORDER BY p.price) AS prices,\n                    COUNT(*) AS total\n                FROM products.prices p\n                GROUP BY p.category\n            ) sub1\n    ) sub2\n;\n\ncategory|median|\n--------+------+\nA       |3     |\nB       |3.5   |\n\nBreaking down the subqueries\n\nSort prices per category\nSELECT\n    category,\n    GROUP_CONCAT(price ORDER BY p.price) AS prices,\n    COUNT(*) AS total\nFROM products.prices p\nGROUP BY p.category\n;\n\ncategory|prices     |total|\n--------+-----------+-----+\nA       |1,2,3,4,5  |    5|\nB       |1,2,3,4,5,6|    6|\n\nIf your table has a lot of data, GROUP_CONCAT would not contain all the data. In this case, you increase the limit for GROUP_CONCAT by: SET GROUP_CONCAT_MAX_LEN = 100000;\n\nGet middle prices according to whether the total count is an odd or even number\nSELECT \n    sub1.category,\n    sub1.total,\n    CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', CEIL(sub1.total/2)), ',', '-1')\n         WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', sub1.total/2 + 1), ',', '-2')\n    END AS mid_prices\nFROM \n    (\n        SELECT\n            p.category,\n            GROUP_CONCAT(p.price ORDER BY p.price) AS prices,\n            COUNT(*) AS total\n        FROM products.prices p\n        GROUP BY p.category\n    ) sub1\n;\n\ncategory|total|mid_prices|\n--------+-----+----------+\nA       |    5|3         |\nB       |    6|3,4       |\n\nWe use the MOD function (modulo) to check if the total count is an odd or even number.\nThe SUBSTRING_INDEX function is used twice to extract the middle elements.\n\n\n\nExample: Overall median of price and quantity\nSELECT\n    CASE WHEN MOD(sub2.total, 2) = 1 THEN sub2.mid_prices\n         WHEN MOD(sub2.total, 2) = 0 THEN (SUBSTRING_INDEX(sub2.mid_prices, ',', 1) + SUBSTRING_INDEX(sub2.mid_prices, ',', -1)) / 2\n    END AS median_of_price,\n    CASE WHEN MOD(sub2.total, 2) = 1 THEN sub2.mid_quantities\n         WHEN MOD(sub2.total, 2) = 0 THEN (SUBSTRING_INDEX(sub2.mid_quantities, ',', 1) + SUBSTRING_INDEX(sub2.mid_prices, ',', -1)) / 2\n    END AS median_of_quantity\nFROM \n    (\n        SELECT \n            sub1.total,\n            CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', CEIL(sub1.total/2)), ',', '-1')\n                 WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', sub1.total/2 + 1), ',', '-2')\n            END AS mid_prices,\n            CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.quantities, ',', CEIL(sub1.total/2)), ',', '-1')\n                 WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.quantities, ',', sub1.total/2 + 1), ',', '-2')                 \n            END AS mid_quantities\n        FROM \n            (\n                SELECT\n                    COUNT(*) AS total,\n                    GROUP_CONCAT(o.price ORDER BY o.price) AS prices,\n                    GROUP_CONCAT(o.quantity ORDER BY o.quantity) AS quantities\n                FROM products.orders o\n            ) sub1\n    ) sub2\n;\n\n\nmedian_of_price|median_of_quantity|\n---------------+------------------+\n3              |30                |\n\nSimilar to previous example\nVariables: order_id, price, quantity",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-trans",
    "href": "qmd/sql.html#sec-sql-trans",
    "title": "SQL",
    "section": "Transactions",
    "text": "Transactions\n\nMisc\n\nAlso see\n\nTerms &gt;&gt; Transaction\nDatabase, Warehouses &gt;&gt; Database Triggers - Shows how to efficiently transfer data from a transactional database to a warehouse/relational database by setting up event triggers and staging tables.\n\nWhen the transaction is successful, COMMIT is applied. When the transaction is aborted, incorrect execution, system failure ROLLBACK occurs.\n\nOnly used with INSERT, UPDATE and DELETE\nBEGIN TRANSACTION: It indicates the start point of an explicit or local transaction.\n\nRepresents a point ast which the data referenced by a connection is logically and physically consistent.\nIf errors are encountered, all data modifications made after the BEGIN TRANSACTION can be rolled back to return the data to this known state of consistency\nSyntax: BEGIN TRANSACTION transaction_name ;\n\nSET TRANSACTION: Places a name on a transaction.\n\nSyntax: SET TRANSACTION [ READ WRITE | READ ONLY ];\n\nCOMMIT: used to permanently save the changes done in the transaction in tables/databases. The database cannot regain its previous state after its execution of commit.\n\nIf everything is in order with all statements within a single transaction, all changes are recorded together in the database is called committed. The COMMIT command saves all the transactions to the database since the last COMMIT or ROLLBACK command\nExample: Delete records\nDELETE FROM Student WHERE AGE = 20;\nCOMMIT;\n\nDeletes those records from the table which have age = 20 and then commits the changes in the database.\n\n\nROLLBACK: used to undo the transactions that have not been saved in the database. The command is only been used to undo changes since the last commit\n\nIf any error occurs with any of the SQL grouped statements, all changes need to be aborted. The process of reversing changes is called rollback. This command can only be used to undo transactions since the last COMMIT or ROLLBACK command was issued.\nSyntax: ROLLBACK;\n\nSAVEPOINT: creates points within the groups of transactions in which to ROLLBACK.\n\nSyntax: SAVEPOINT &lt;savepoint_name&gt;;\nA savepoint is a point in a transaction in which you can roll the transaction back to a certain point without rolling back the entire transaction.\nRemove a savepoint: RELEASE SAVEPOINT &lt;savepoint_name&gt;\nExample: Rollback a deletion\nSAVEPOINT SP1;\n//Savepoint created.\nDELETE FROM Student WHERE AGE = 20;\n//deleted\nSAVEPOINT SP2;\n//Savepoint created.\nROLLBACK TO SP1;\n//Rollback completed.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-procexp",
    "href": "qmd/sql.html#sec-sql-procexp",
    "title": "SQL",
    "section": "Processing Expressions",
    "text": "Processing Expressions\n\nUse multiple conditions in a WHERE expression\nselect\n    *\nfrom XXX_table\nwhere 1=1\n    (if condition A) and clause 1 \n    (if condition B) and clause 2 \n    (if condition C) and clause 3\n;\n\nThe “1=1” prevents errors that would occur when the first condition doesn’t apply to any rows.\n\nCan also use “true”\n\n\nSelect unique rows without using DISTINCT\n\nUsing UNION\nSELECT employee_id,\n      employee_name,\n      department\nFROM Dummy_employees\nUNION\nSELECT employee_id,\n      employee_name,\n      department\nFROM Dummy_employees\n\nthere must be same number and order of columns in both the SELECT statements\n\nUsing INTERSECT\nSELECT employee_id,\n      employee_name,\n      department\nFROM Dummy_employees\nINTERSECT\nSELECT employee_id,\n      employee_name,\n      department\nFROM Dummy_employees\n\nThere must be same number and order of columns in both the SELECT statements\n\nUsing ROW_NUMBER\nWITH temporary_employees as (\n  SELECT\n    employee_id,\n    employee_name,\n    department,\n    ROW_NUMBER() OVER(PARTITION BY employee_name,\n                                  department,\n                                  employee_id) as row_count\n  FROM Dummy_employees\n)\n\nSELECT *\nFROM temporary_employees\nWHERE row_count = 1\nUsing GROUP BY\nSELECT employee_id,\n      employee_name,\n      department\nFROM Dummy_employees\nGROUP BY employee_id,\n        employee_name,\n        department\n\nJust need to group by all the columns. Useful to use in conjunction with aggregate functions.\n\n\nCASE WHEN\nSELECT OrderID,\n      OrderDate,\n      Sales_Manager,\n      Quantity,\n      CASE WHEN Quantity &gt; 51 THEN 'High'\n            WHEN Quantity &lt; 51 THEN 'Low'\n            ELSE 'Medium' \n      END AS OrderVolume\nFROM Dummy_Sales_Data_v1\n\nEND AS specifies the name of the new column, “OrderVolume”\nELSE specifies the value when none of the conditions are met\n\nIf you did not mention ELSE clause and no condition is satisfied, the query will return NULL for that specific record\n\n\nPivot Wider\n\nSELECT Sales_Manager,\n      COUNT(CASE WHEN Shipping_Address = 'Singapore' THEN OrderID\n            END) AS Singapore_Orders,\n\n      COUNT(CASE WHEN Shipping_Address = 'UK' THEN OrderID\n            END) AS UK_Orders,\n\n      COUNT(CASE WHEN Shipping_Address = 'Kenya' THEN OrderID\n            END) AS Kenya_Orders,\n\n      COUNT(CASE WHEN Shipping_Address = 'India' THEN OrderID\n            END) AS India_Orders\nFROM Dummy_Sales_Data_v1\nGROUP BY Sales_Manager\n\nDepending on your use-case you can also use different aggregation such as SUM, AVG, MAX, MIN with CASE statement.\n\n\n\nNULLs\n\nDivision and NULLS\n\nAny division with NULL values with have a result of NULL.\nisNull allows to get a different resulting value\nSELECT IsNull(&lt;column&gt;, 0) / 45\n\nAll NULL values in the column will replaced with 0s during the division operation.\n\n\nCOALESCE\n\nSubstitute a default value in place of NULLs\nSELECT COALESCE(column_name, 'Default Value') AS processed_column\nFROM table_name;\n\nSELECT COALESCE(order_date, current_date) AS processed_date\nFROM orders;\n\nSELECT\n  product ||' - '||\n  COALESCE(subcategory, category, family, 'no product description ')\n    AS product_and_subcategory\nFROM stock\n\n3rd Expression: If there is a NULL in subcategory, then it looks in category, then into family, and finally if all those fields have NULLs, it uses “no product description” as the value.\n\nConcantenating Strings where NULLs are present\nSELECT COALESCE(first_name, '') || ' ' || COALESCE(last_name, '') AS full_name\nFROM employees;\n\nNULLs are replaced with an empty string so transformation doesn’t break\n\nPerforming calculations involving numeric columns where there are NULLs\nSELECT COALESCE(quantity, 0) * COALESCE(unit_price, 0) AS total_cost\nFROM products;\n\nSELECT product,\n  quantity_available,\n  minimum_to_have,\n  COALESCE(minimum_to_have, quantity_available * 0.5) AS threshold\nFROM stock\n\nNULLs are substituted with 0s so the calcuation doesn’t break\n\nAs part of a join in case keys have missing values\nSELECT *\nFROM employees e\nLEFT JOIN departments d ON COALESCE(e.department_id, 0) = COALESCE(d.id, 0);\nWith Aggregate Functions\nSELECT department_id, COALESCE(SUM(salary), 0) AS total_salary\nFROM employees\nGROUP BY department_id;\nMake hierarchical subtotals output more readable\n\nSELECT COALESCE(family,'All Families') AS family,\n COALESCE(category,'All Categories') AS category,\n COALESCE(subcategory,'All Subcategories') AS subcategory,\n SUM(quantity_available) as quantity_in_stock\nFROM stock\nGROUP BY ROLLUP(family, category, subcategory)\nORDER BY family, category, subcategory\n\nROLLUP clause assumes a hierarchy among the columns family, category, and subcategory. Thus, it generates all the grouping sets that make sense considering the hierarchy: GROUP BY family, GROUP BY family, category and GROUP BY family, category, subcategory.\n\nThis is the reason why ROLLUP is often used to generate subtotals and grand totals for reports.\n\nWithout COALESCE , the text in the unused columns for the subtotals would be NULLs.\n\n\n\n\n\nDuplicated Rows\n\nRemove duplicated rows with window function\nWITH temporary_employees as \n(SELECT \n  employee_id, \n  employee_name, \n  department, \n  ROW_NUMBER() OVER(PARTITION BY employee_name, \n                                department, \n                                employee_id) as row_count \nFROM Dummy_employees)\n\nSELECT * \nFROM temporary_employees \nWHERE row_count = 1\nUse a hash column as id column, then test for duplicates, remove them or investigate them (BigQuery)\n\nWITH\n    inbound_zoo_elephants AS (\n        SELECT *\n        FROM flowfunctions.examples.zoo_elephants\n    ),\n    add_row_hash AS (\n        SELECT\n            *,\n            TO_HEX(MD5(TO_JSON_STRING(inbound_zoo_elephants))) AS hex_row_hash\n        FROM inbound_zoo_elephants\n    )\n\nSELECT\n    COUNT(*) AS records,\n    COUNT(DISTINCT hex_row_hash) AS unique_records\nFROM add_row_hash\n\nNo duplicate records found, since “records” = 9 and “unique_records” = 9\n\nif records &gt; unique_records, duplicates exist\n\nCan select distinct hex_row_hash if you want to remove duplicates\nCan count hex_row_hash then filter where hex_row_hash &gt; 1 to find which rows are duplicates\nNotes from link\nDescription\n\nflowfunctions is the project name\nexamples is a directory (?)\nzoo_elephants is the dataset\n\nSteps\n\nTO_JSON_STRING - creates column with json string for each row\nMD5 hashes that string\nTO_HEX makes it alpha-numeric and gets rid of the symbols in the hash\n\nEasier to deal with in BigQuery\nAssume this is still unique (?)\n\n\nNote: By adding “true” value, TO_JSON_STRING(inbound_zoo_elephants, true) , TO_JSON_STRING adds line breaks to the json string for easier readability.\nHashing function options\n\nMD5 -  shortest one (16 characters), fine for this use case\n\ncryptographically broken, returns 16 characters and suffices for our use-case. Other options are\n\nFARM_FINGERPRINT - returns a signed integer of variable length\nSHA1, SHA256 and SHA512, which return 20, 32 and 64 bytes respectively and are more secure for cryptographic use cases.\n\n\n\n\n\nNested Data\n\nRecursive CTE\n\nRecursive CTEs are used primarily when you want to query hierarchical data or graphs. This could be a company’s organizational structure, a family tree, a restaurant menu, or various routes between cities\nAlso see\n\nWhat Is a Recursive CTE in SQL?\n\nTutorial, 3 examples, and links to other articles\n\n\nSyntax\nWITH RECURSIVE cte_name AS (\n    cte_query_definition (the anchor member)\n    UNION ALL\n    cte_query_definition (the recursive member)\n)\n\nSELECT *\nFROM  cte_name;\nExample: : postgres\nWITH RECURSIVE category_tree(id, name, parent_id, depth, path) AS (\n  SELECT id, name, parent_id, 1, ARRAY[id]\n  FROM categories\n  WHERE parent_id IS NULL\n  UNION ALL\n  SELECT categories.id, categories.name, categories.parent_id, category_tree.depth + 1, path || categories.id\n  FROM categories\n  JOIN category_tree ON categories.parent_id = category_tree.id\n)\n\nSELECT id, name, parent_id, depth, path\nFROM category_tree;\n\nCTE (WITH) + RECURSIVE says it’s a recursive query.\nUNION ALLcombines the results of both statements.\n\nExample is defined by 2 Select statements\n\nAnchor Member: First SELECT statement selects the root nodes of the category tree (nodes with no parent)\n\nRoot node is indicated by “parent_id” = NULL\n\nRecursive member: Second SELECT statement selects the child nodes recursively\n\nAlso see Arrays for further examples of the use of UNION ALL\n\nThe “depth” column is used to keep track of the depth of each category node in the tree.\n\n“1” in the first statement\n“category_tree.depth + 1” in the second statement\n\nWith every recursion, the CTE will add 1 to the previous depth level, and it will do that until it reaches the end of the hierarchy\n\n\nThe “path” column is an array that stores the path from the root to the current node.\n\n“ARRAY[id]” in the first statement\n“path || categories.id” in the second statement\n\n“||” concatenates “path” and “id” columns (See Strings)\n\n\n\n\n\n\n\nBinning\n\nCASE WHEN\n\nSELECT\n Name, \n Grade,\n CASE\n  WHEN Grade &lt; 10 THEN '0-9'\n  WHEN Grade BETWEEN 10 and 19 THEN '10-19'\n  WHEN Grade BETWEEN 20 and 29 THEN '20-29'\n  WHEN Grade BETWEEN 30 and 39 THEN '30-39'\n  WHEN Grade BETWEEN 40 and 49 THEN '40-49'\n  WHEN Grade BETWEEN 50 and 59 THEN '50-59'\n  WHEN Grade BETWEEN 60 and 69 THEN '60-69'\n  WHEN Grade BETWEEN 70 and 79 THEN '70-79'\n  WHEN Grade BETWEEN 80 and 89 THEN '80-89'\n  WHEN Grade BETWEEN 90 and 99 THEN '90-99'\n  END AS Grade_Bucket\n FROM students\n\nBETWEEN is inclusive of the end points\nFlexible for any size of bin you need\n\nFLOOR\nSELECT\n Name,\n Grade,\n FLOOR(Grade / 10) * 10 AS Grade_Bucket\nFROM students\n\nCan easily scale up the number of bins without having to increase the lines of code\nOnly useful for evenly spaced bins\n\nLEFT JOIN on preformatted table\nCREATE OR REPLACE TABLE bins (\n    Lower_Bound INT64,\n    Upper_Bound INT64,\n    Grade_Bucket STRING\n);\n\nINSERT bins (Lower_Bound, Upper_Bound, Grade_Bucket)\nVALUES\n (0, 9, '0-9')\n (10, 19, '10-19')\n (20, 29, '20-29')\n (30, 39, '30-39')\n (40, 49, '40-49')\n (50, 59, '50-59')\n (60, 69, '60-69')\n (70, 79, '70-79')\n (80, 89, '80-89')\n (90, 99, '90-99');\n\nSELECT\n A.Name, \n A.Grade,\n B.Grade_Bucket\nFROM students AS A\nLEFT JOIN bins AS B\nON A.Grade BETWEEN B.Lower_Bound AND B.Upper_Bound\n\n“bins” table acts a template that funnels the values from your table into the correct bins\n\n\n\n\nTime Series\n\nExtract components from date-time columns\n/* MySQL */\nEXTRACT(part_of_date FROM date_time_column_name)\nYEAR(date_time_column_name)\nMONTH(date_time_column_name)\nMONTHNAME(date_time_column_name)\nDATE_FORMAT(date_time_column_name)\n\n/* SQLte */\nSELECT strftime('%m', OrderDate) as Month\n\nstrftime codes\n\n\nPreprocess Time Series with 4 Lags (article)\nWITH top_customers as (\n    --- select the customter ids you want to track\n),\ntransactions as (\n    SELECT \n      cust_id, \n      dt, \n      date_trunc('hour', cast(event_time as timestamp)) as event_hour, \n      count(*) as transactions\n    FROM ourTable\n    WHERE\n        dt between cast(date_add('day', -7, current_date) as varchar) \n        and cast(current_date as varchar)\n    GROUP BY 1,2,3 Order By event_hour asc\n)\n\nSELECT transactions.cust_id,\n      transactions.event_hour,\n      day_of_week(transactions.event_hour) day_of_week,\n        hour(transactions.event_hour) hour_of_day,\n        transactions.transactions as transactions,\n        LAG(transactions,1) OVER \n          (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag1,\n        LAG(transactions,2) OVER \n          (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag2,\n        LAG(transactions,3) OVER \n          (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag3,\n        LAG(transactions,4) OVER \n          (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag4\nFROM transactions \n    join top_customers \n      on transactions.cust_id = top_customers.cust_id\n\n/* output */\n\"cust_id\", \"event_hour\", \"day_of_week\", \"hour_of_day\", \"transactions\", \"lag1\", \"lag2\", \"lag3\", \"lag4\"\n\"Customer-123\",\"2023-01-14 00:00:00.000\",\"6\",\"0\",\"4093\",,,,,,\n\"Customer-123\",\"2023-01-14 01:00:00.000\",\"6\",\"1\",\"4628\",\"4093\",,,,,\n\"Customer-123\",\"2023-01-14 02:00:00.000\",\"6\",\"2\",\"5138\",\"4628\",\"4093\",,,,\n\"Customer-123\",\"2023-01-14 03:00:00.000\",\"6\",\"3\",\"5412\",\"5138\",\"4628\",\"4093\",,,\n\"Customer-123\",\"2023-01-14 04:00:00.000\",\"6\",\"4\",\"5645\",\"5412\",\"5138\",\"4628\",\"4093\",\n\"Customer-123\",\"2023-01-14 05:00:00.000\",\"6\",\"5\",\"5676\",\"5645\",\"5412\",\"5138\",\"4628\",\n\"Customer-123\",\"2023-01-14 06:00:00.000\",\"6\",\"6\",\"6045\",\"5676\",\"5645\",\"5412\",\"5138\",\n\"Customer-123\",\"2023-01-14 07:00:00.000\",\"6\",\"7\",\"6558\",\"6045\",\"5676\",\"5645\",\"5412\",\n\nDataset contains number of transactions made per customer per hour.\n2 WITH clauses: the first just extracts a list of customers we are interested in. Here you can add any condition that is supposed to filter in or out specific customers (perhaps you want to filter new customers or only include customers with sufficient traffic). The second WITH clause simply creates the first data set — Dataset A, which pulls a week of data for these customers and selects the customer id, date, hour, and number of transactions.\nFinally, the last and most important SELECT clause generates Dataset B, by using SQL lag() function on each row in order to capture the number of transactions in each of the hours that preceded the hour in the row.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-tools",
    "href": "qmd/sql.html#sec-sql-tools",
    "title": "SQL",
    "section": "Tools",
    "text": "Tools\n\nChatSQL: Convert plain text to MySQL query by ChatGPT\n{{sqlglot}} - no dependency Python SQL parser, transpiler, optimizer, and engine\n\nFormat SQL or translate between nearly twenty different SQL dialects.\n\nIt doesn’t just transpile active SQL code, too. Moves comments from one dialect to another.\n\nThe parser itself can be customized\nCan also help you analyze queries, traverse parsed expression trees, and incrementally (and, programmatically) build SQL queries.\nsupport for optimizing SQL queries, and performing semantic diffs.\nCan be used to unit test queries through mocks based on Python dictionaries.\nExample: : translate duckdb to hive\nimport sqlglot\nsqlglot.transpile(\n  \"SELECT EPOCH_MS(1618088028295)\", \n  read = \"duckdb\", \n  write = \"hive\"\n)[0]\n---\n'SELECT FROM_UNIXTIME(1618088028295 / 1000)'",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/bayes-reporting.html",
    "href": "qmd/bayes-reporting.html",
    "title": "Reporting",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Bayes",
      "Reporting"
    ]
  },
  {
    "objectID": "qmd/bayes-reporting.html#sec-bayes-rep-misc",
    "href": "qmd/bayes-reporting.html#sec-bayes-rep-misc",
    "title": "Reporting",
    "section": "",
    "text": "Also see Mathematics, Statistics &gt;&gt; Descriptive &gt;&gt; Understanding CI, sd, and sem Bars\n{posterior} rvars class\n\nobject class that’s designed to interoperate with vectorized distributions in {distributional}, to be able to be used inside data.frame()s and tibble()s, and to be used with distribution visualizations in the {ggdist}.\nDocs\n\nRemember CIs of parameter estimates including zero are not evidence of the null hypothesis (i.e. β = 0).\n\nEspecially if CIs are broad and most of the posterior probability distribution is massed away from zero\n\nVisualization for differences (Thread)",
    "crumbs": [
      "Bayes",
      "Reporting"
    ]
  },
  {
    "objectID": "qmd/bayes-reporting.html#sec-bayes-rep-sdfe",
    "href": "qmd/bayes-reporting.html#sec-bayes-rep-sdfe",
    "title": "Reporting",
    "section": "Significant Digits for Estimates",
    "text": "Significant Digits for Estimates\n\nMisc\n\nNotes from: Bayesian workflow book - Digits\n\nBefore we can answer how many chains and iterations we need to run, we need to know how many significant digits we want to report\nMCMC in general doesn’t produce independent draws and the effect of dependency affects how many draws are needed to estimate different expectations\nGuidelines in general\n\nIf the posterior would be close to a normal(μ,1), then\n\nFor 2 significant digit accuracy,\n\n2000 independent draws from the posterior would be sufficient for that 2nd digit to only sometimes vary.\n4 chains with 1000 iterations after warmup is likely to give near two significant digit accuracy for the posterior mean. The accuracy for 5% and 95% quantiles would be between one and two significant digits.\nWith 10,000 draws, the uncertainty is 1% of the posterior scale which would often be sufficient for two significant digit accuracy.\n\nFor 1 significant digit accuracy, 100 independent draws would be often sufficient, but reliable convergence diagnostics may need more iterations than 100.\nFor posterior quantiles, more draws may be needed (need more draws to get values towards the tails of the posterior)\n\nSome quantities of interest may have posterior distribution with infinite variance, and then the ESS and MCSE are not defined for the expectation.\n\nIn such cases, use median instead of mean and mean absolute deviation (MAD) instead of standard deviation.\nVariance of parameter posteriors\nas_draws_rvars(brms_fit) %&gt;%\n    summarise_draws(var = distributional::variance) \n#&gt;    variable  var\n#&gt;    &lt;chr&gt;    &lt;dbl&gt;\n#&gt;  1 mu        11.6\n#&gt;  2 tau      12.8\n#&gt;  3 theta[1]  39.7\n#&gt;  4 theta[2]  21.5\n\n\nSteps\n\nCheck convergence diagnostics for all parameters\n\ne.g. RHat, ESS, autocorrelation plots (see Diagnostics, Bayes)\n\nLook at the posterior for quantities of interest and decide how many significant digits is reasonable taking into account the posterior uncertainty (using SD, MAD, or tail quantiles)\n\nYou want to be able to distinguish you upper or lower CI from the point estimate\n\ne.g. Point estimate is 2.1 and you upper CI is 2.1 then you want at least another significant digit.\n\n\nCheck that MCSE is small enough for the desired accuracy of reporting the posterior summaries for the quantities of interest.\n\nCalculate the range of variation due to MC sampling for your paramter (See MCSE example)\n\nMC sampling error is the average amount of variation that’s expected from changing seeds and re-running the analysis\n\nIf the accuracy is not sufficient (i.e. range is too wide), report less digits or run more iterations.\n\n\nMonte Carlo standard error (MCSE) - uncertainty about a parameter estimate due to MCMC sampling error\n\nPackages\n\n{posterior} is the preferred package for brms objects\n{mcmcse} - methods to calculate MCMC standard errors for means and quantiles using sub-sampling methods. (Different calculation than used by Stan)\nbayestestR::mcse uses Kruschke 2015 method of calculation\n\nExample: brms, MCSE quantiles\n# Coefficient and CI estimates for the \"beta100\" variable\nas_draws_rvars(brms_fit) %&gt;%\n  subset_draws(\"beta100\") %&gt;%\n  summarize_draws(mean, ~quantile(.x, probs = c(0.05, 0.95)))\n#&gt; variable  mean      5%   95%\n#&gt; beta100   1.966  0.673 3.242\n\nas_draws_rvars(brms_fit) %&gt;%\n  subset_draws(\"beta100\") %&gt;% # select variable\n  summarize_draws(mcse_mean, ~mcse_quantile(.x, probs = c(0.05, 0.95)))\n#&gt; variable  mcse_mean  mcse_q5 mcse_q95\n#&gt; beta100       0.013    0.036    0.033\n\nSpecification\n\n“mcse_mean” and “mean” are available as preloaded functions that summary_draws can use out of the box\n“mcse_quantile” (also in {posterior}) and “quantile” are not preloaded functions so they’re called as lambda functions\n\nThese are MCSE values for\n\nthe summary estimate (aka point estimate) which is the mean of the posterior in this case\nAnd the CI values of that summary estimate\n\nTail quantiles will have greater amounts of error sampling in the tails of the posterior than in the bulk (i.e. less accurate tail estimates)\nFewer points, more uncertainty\n\n\nCalculate the range of variation due to Monte Carlo\n\nMultiply the MCSE values by 2, the likely range of variation due to Monte Carlo is ±0.02 for mean and ±0.07 for 5% and 95% quantiles\n\nMultiplying by 2, since I guess they’re assuming a normal distribution posterior, therefore estimate ± 1.96 * SE\n\n\nConclusion for “beta100” coefficient\n\nIf the mean estimate for beta100 is reported as 2 (rounded up from 1.966), then there is unlikely to be any variation in that estimate due to MCMC sampling. (i.e. okay to report the estimate as 2)\n\nThis is because\n\n1.966 + 0.02 = 1.986 which would still be rounded up to 2\n1.966 - 0.02 = 1.946 which would still be rounded up to 2\n\n\n\nDraws and iterations\n\nWith an MCSE in the 100ths (e.g. 0.07), 4 times more iterations would halve the MCSEs\nWith an MCSE in the 1000ths (e.g. 0.007), 64 times more iterations would halve the MCSEs\nMCSEs depend on the quantity type. Continuous quantities (e.g. parameter estimates) have more information than discrete quantities (e.g. indicator values used to calculate probabilities).\n\nFor example, above, the estimate for whether the temperature increase is larger than 4 degrees per century has high ESS, but the indicator variable contains less information (than continuous values) and thus much higher ESS would be needed for two significant digit accuracy.",
    "crumbs": [
      "Bayes",
      "Reporting"
    ]
  },
  {
    "objectID": "qmd/bayes-reporting.html#probabilistic-inference-of-estimates",
    "href": "qmd/bayes-reporting.html#probabilistic-inference-of-estimates",
    "title": "Reporting",
    "section": "Probabilistic Inference of Estimates",
    "text": "Probabilistic Inference of Estimates\n\nMisc\n\nNotes from: Bayesian workflow book - Digits\n\nExample: probability that an estimate is positive\nas_draws_rvars(brms_fit) %&gt;%\n  # binary 1/0, posterior samples &gt; 0\n  mutate_variables(beta0p = beta100 &gt; 0) %&gt;% \n  subset_draws(\"beta0p\") %&gt;% # select variable\n  summarize_draws(\"mean\", mcse = mcse_mean)\n\n#&gt; variable  mean  mcse\n#&gt;   beta0p 0.993 0.001\n99.3% probability the estimate is above zero +/- 0.2% (= 2*MCSE)\nMCSE indicates that we have enough MCMC iterations for practically meaningful reporting that the probability that the variable (e.g. temperature) is increasing (i.e. slope is positive) is larger than 99%\nExample: probability that an estimate &gt; 1,2,3,4\nas_draws_rvars(brms_fit) %&gt;%\n  subset_draws(\"beta100\") %&gt;%\n  # binary 1/0 variable\n  mutate_variables(beta1p = beta100 &gt; 1,\n                  beta2p = beta100 &gt; 2,\n                  beta3p = beta100 &gt; 3,\n                  beta4p = beta100 &gt; 4) %&gt;%\n  subset_draws(\"beta[1-4]p\", regex=TRUE) %&gt;%\n  summarize_draws(\"mean\", mcse = mcse_mean, ESS = ess_mean)\n\n#&gt; variable  mean mcse  ESS\n#&gt;  beta1p 0.896 0.006 3020\n#&gt;  beta2p 0.487 0.008 4311\n#&gt;  beta3p 0.088 0.005 3188\n#&gt;  beta4p 0.006 0.001 3265\nTaking into account MCSEs given the current posterior sample, we can summarise these as\n\np(beta100&gt;1) = 88%–91%,\np(beta100&gt;2) = 46%–51%,\np(beta100&gt;3) = 7%–10%,\np(beta100&gt;4) = 0.2%–1%.\n\nTo get these probabilities estimated with 2 digit accuracy would again require more iterations (16-300 times more iterations depending on the quantity), but the added iterations would not change the conclusion radically.perature in the center of the time range (instead defining prior for temperature at year 0).",
    "crumbs": [
      "Bayes",
      "Reporting"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html",
    "href": "qmd/post-hoc-analysis-anova.html",
    "title": "ANOVA",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-misc",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-misc",
    "title": "ANOVA",
    "section": "",
    "text": "Packages\n\n{car} - Anova function that computes all 3 types of ANOVA table\n\nCan also be applied to glm models to produce Analysis of Deviance tables (e.g. logistic, poisson, etc.)\nThink the other packages wrap this function, so they can be used instead in order to advantage of their plotting, testing conveniences.\n\n{grafify} - ANOVA wrappers, plotting, wrappers for {emmeans}\n{afex} - Analysis of Factorial EXperiments\n\nANOVA helper functions that fit the lm, center, apply contrasts, etc. in one line of code\n\nExample: afex::aov_car(Y ~ group * condition + Error(id), data = d)\nType III used, Factor variables created, Sum-to-Zero contrast is applied\n\nEffect plotting functions\n\n\nNotes from\n\nEverything You Always Wanted to Know About ANOVA\n\nANOVA vs. Regression (GPT-3.5)\n\nDifferent Research Questions:\n\nANOVA is typically used when you want to compare the means of three or more groups to determine if there are statistically significant differences among them. It’s suited for situations where you’re interested in group-level comparisons (e.g., comparing the average test scores of students from different schools).\nRegression, on the other hand, is used to model the relationship between one or more independent variables and a dependent variable. It’s suitable for predicting or explaining a continuous outcome variable.\n\nData Type:\n\nANOVA is traditionally used with categorical independent variables and a continuous dependent variable. It helps assess whether the categorical variable has a significant impact on the continuous variable.\n\nThere are other variants such as ANCOVA (categorical and continuous IVs) and Analysis of Deviance (discrete outcome)\n\nRegression can be used with both categorical and continuous independent variables to predict a continuous dependent variable or to examine the relationship between variables.\n\nMultiple Factors:\n\nANOVA is designed to handle situations with multiple categorical independent variables (factors) and their interactions. It is useful when you are interested in understanding the combined effects of several factors.\nRegression can accommodate multiple independent variables as well, but it focuses on predicting the value of the dependent variable rather than comparing groups.\n\nHypothesis Testing:\n\nANOVA tests for differences in means among groups and provides p-values to determine whether those differences are statistically significant.\nRegression can be used for hypothesis testing, but it’s more often used for estimating the effect size and making predictions.\n\nAssumptions:\n\nANOVA assumes that the groups are independent and that the residuals (the differences between observed values and group means) are normally distributed and have equal variances.\nRegression makes similar assumptions about residuals but also assumes a linear relationship between independent and dependent variables.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-gen",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-gen",
    "title": "ANOVA",
    "section": "General",
    "text": "General\n\nFamily of procedures which summarizes the relationship between the underlying model and the outcome by partitioning the variation in the outcome into components which can be uniquely attributable to different sources according to the law of total variance.\nEssentially, each of the model’s terms is represented in a line in the ANOVA table which answers the question how much of the variation in Y can be attributed to the variation in X?\n\nWhere applicable, each source of variance has an accompanying test statistic (oftenF), sometimes called the omnibus test, which indicates the significance of the variance attributable to that term, often accompanied by some measure of effect size.\n\nOne-Way ANOVA - 1 categorical, independent variable\n\nDetermines whether there is a statistically significant difference in the means of the dependent variable across the different levels of the independent variable.\nExample: A researcher wants to compare the average plant height grown using three different types of fertilizer. They would use a one-way ANOVA to test if there is a significant difference in height between the groups fertilized with each type.\n\nTwo-Way ANOVA - 2 categorical, independent variables\n\nExample: 3 treatments are given to subjects and the researcher thinks that females and males will have different responses in general.\n\nTest whether there are treatment differences after accounting for sex effects\nTest whether there are sex differences after accounting for treatment effects\nTest whether the treatment effect is different for females and males if you allow the treatment \\(\\times\\) sex interaction to be in the model\n\n\nTypes\n\nTL;DR;\n\nI don’t see a reason not to run type III every time.\nType I: Sequential Attribution of Variation\nType II: Simultaneous Attribution of Variation\n\nFor interactions: Sequential-Simultaneous Attribution of Variation\n\nType III: Simultaneous Attribution of Variation for Main Effects and Interactions\nIf the categorical explanatory variables in the analysis are balanced, then all 3 types will give the same results. The results for each variable will be it’s unique contribution.\n\nExample:\n# balanced\ntable(d$Rx, d$condition)\n#&gt;           Ca Cb\n#&gt;   Placebo  5  5\n#&gt;   Dose100  5  5\n#&gt;   Dose250  5  5\n\n# imbalanced\ntable(d$group, d$condition)\n#&gt;      Ca Cb\n#&gt;   Gb  6  6\n#&gt;   Ga  5  6\n#&gt;   Gc  4  3\n\n\nType I: Sequential Sum of Squares\n\nVariance attribution is calculated sequentially so the order of variables in the model matters. Each term is attributed with a portion of the variation (represented by its SS) that has not yet been attributed to any of the previous terms.\nRarely used in practice because the order in which variation is attributed isn’t usually important\nExample: Order of terms matters\nanova(lm(Y ~ group + X, data = d))\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Response: Y\n#&gt;           Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \n#&gt; group      2    8783    4391  0.0918 0.912617   \n#&gt; X          1  380471  380471  7.9503 0.009077 **\n#&gt; Residuals 26 1244265   47856                    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm(Y ~ X + group, data = d))\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Response: Y\n#&gt;           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \n#&gt; X          1  325745  325745  6.8067 0.01486 *\n#&gt; group      2   63509   31754  0.6635 0.52353  \n#&gt; Residuals 26 1244265   47856                  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSum of Squares values change based on the order of the terms in the model\nIn the first model,\n\nThe effect of group does not represent its unique contribution to Y’s variance, but instead its total contribution.\n\nThis reminds me of a dual path DAG where group is influenced by X. Here X’s variance contribution is included in group’s contribution since X is not conditioned upon. (See Causal Inference &gt;&gt; Dual Path DAGs)\n\nThe effect of X represents only what X explains after removing the contribution of group — the variance attributed to X is strictly the variance that can be uniquely attributed to X, controlling for group\n\n\n\nType II: Simultaneous Sum of Squares\n\nThe variance attributed to each variable is its unique contribution — variance after controlling for the other variables. Order of terms does not matter.\nExample\ncar::Anova(m, type = 2)\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: Y\n#&gt;            Sum Sq Df F value   Pr(&gt;F)   \n#&gt; group       63509  2  0.6635 0.523533   \n#&gt; X          380471  1  7.9503 0.009077 **\n#&gt; Residuals 1244265 26                    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSum of Squares values are equal to values of the Type 1 results when each variable is last.\nNote that factor variables, e.g. group, are treated as 1 term and not broken down into dummy variables for each level.\n\nWith interactions, the method of calculation could be called, Sequential-Simultaneous.\n\nTerms are evaluated simultaneously in groups based on type of term, e.g. main effects, 2-way interactions, 3-way interactions, etc., but sequentially according to the order of that term where the order of main effects &lt; 2-way interactions &lt; 3-way interactions, etc.\nAll main effects (1st order) are tested simultaneously (accounting for one another), then all 2-way interactions (2nd order) are tested simultaneously (accounting for the main effects and one another), and finally the 3-way interaction is tested (accounting for all main effects and 2-way interactions).\nSo, if you use this way to test a model with interactions, only the highest order term’s Sum of Squares represents a unique variance contribution.\n\n\nType III: Simultaneous-Simultaneous Sum of Squares\n\nThe Sum-of-Squares for each main effect and interaction is calculated as its unique contribution (i.e. takes into account all other terms of the model).\nUnlike Type II, it allows you compare variance contributions for every term in your model.\nWithout centering continuous variables and applying sum-to-zero contrasts to categorical variables, tests results can change depending on the categorical level of the moderator. (Also see Regression, Linear &gt;&gt; Contrasts &gt;&gt; Sum-to-Zero)\n\nExample\n\nNo Centering, No Sum-to-Zero Contrasts\nm_int &lt;- lm(Y ~ group * X, data = d)\n\nd$group &lt;- relevel(d$group, ref = \"Gb\")\nm_int2 &lt;- lm(Y ~ group * X, data = d)\n\ncar::Anova(m_int, type = 3)\n#&gt;             Sum Sq Df F value    Pr(&gt;F)    \n#&gt; (Intercept) 538630  1 22.9922 6.994e-05 ***\n#&gt; group       738108  2 15.7536 4.269e-05 ***\n#&gt; X           101495  1  4.3325   0.04823 *  \n#&gt; group:X     682026  2 14.5566 7.246e-05 ***\n#&gt; Residuals   562240 24     \n\ncar::Anova(m_int2, type = 3)\n#&gt;             Sum Sq Df F value    Pr(&gt;F)    \n#&gt; (Intercept) 219106  1  9.3528  0.005402 ** \n#&gt; group       738108  2 15.7536 4.269e-05 ***\n#&gt; X           910646  1 38.8722 1.918e-06 ***\n#&gt; group:X     682026  2 14.5566 7.246e-05 ***\n#&gt; Residuals   562240 24  \n\nThe sum of squares and p-value change for X when the categorical variable’s reference level changed which shouldn’t matter given this is an omnibus test (i.e. the categorical variable is treated as 1 entity and not set of dummy variables).\n\nCentered, Sum-to-Zero Contrasts Applied\n# center, contr.sum\nd_contr_sum &lt;- d |&gt; \n  mutate(X_c = scale(X, scale = FALSE))\ncontrasts(d_contr_sum$group) &lt;- contr.sum\nm_int_cont_sum &lt;- lm(Y ~ group * X_c, data = d_contr_sum)\ncar::Anova(m_int_cont_sum, type = 3)\n#&gt;              Sum Sq Df  F value    Pr(&gt;F)    \n#&gt; (Intercept) 4743668  1 202.4902 3.401e-13 ***\n#&gt; group         19640  2   0.4192   0.66231    \n#&gt; X_c          143772  1   6.1371   0.02067 *  \n#&gt; group:X_c    682026  2  14.5566 7.246e-05 ***\n#&gt; Residuals    562240 24\n\n# change reference level\nd_rl &lt;- d_contr_sum |&gt; \n  mutate(group_rl = relevel(group, ref = \"Gb\"))\ncontrasts(d_rl$group_rl) &lt;- contr.sum\ncar::Anova(lm(Y ~ group_rl * X_c, data = d_rl),\n           type = 3)\n#&gt;               Sum Sq Df  F value    Pr(&gt;F)    \n#&gt; (Intercept)  4743668  1 202.4902 3.401e-13 ***\n#&gt; group_rl       19640  2   0.4192   0.66231    \n#&gt; X_c           143772  1   6.1371   0.02067 *  \n#&gt; group_rl:X_c  682026  2  14.5566 7.246e-05 ***\n#&gt; Residuals     562240 24   \n\nNow when the reference level is changed, the sum-of-squares and p-value for X remain the same.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#assumptions",
    "href": "qmd/post-hoc-analysis-anova.html#assumptions",
    "title": "ANOVA",
    "section": "Assumptions",
    "text": "Assumptions\n\nEach group category has a normal distribution.\nEach group category is independent of each other and identically distributed (iid)\nGroup categories have of similar variance (i.e. homoskedastic variance)\n\nIf this is violated\n\nIf the ratio of the largest variance to the smallest variance is less than 4, then proceed with one-way ANOVA (robust to small differences)\nIf the ratio of the largest variance to the smallest variance is greater than 4, perform a Kruskal-Wallis test. This is considered the non-parametric equivalent to the one-way ANOVA. (example)\n\nEDA\ndata %&gt;%\n  group_by(program) %&gt;%\n  summarize(var=var(weight_loss))\n#&gt; A tibble: 3 x 2\n#&gt;   program  var   \n#&gt; 1 A      0.819\n#&gt; 2 B      1.53 \n#&gt; 3 C      2.46\nPerform a statisical test to see if these variables are statistically significant (See Post-Hoc Analysis, Difference-in-Means &gt;&gt; EDA &gt;&gt; Tests for Equal Variances)",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-math",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-math",
    "title": "ANOVA",
    "section": "Mathematics",
    "text": "Mathematics\n\nAsides:\n\nThis lookd like the variance formula except for not dividing by the sample size to get the “average” squared distance\nSSA formula - the second summation just translates to multiplying by ni, the group category sample size, since there is no j in that formula\n\nCalculate SSA and SSE\n\\[\n\\begin{align}\n\\text{SST} &= \\text{SSA} + \\text{SSE} \\\\\n&= \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (x_{i,j} - \\mu)^2 \\\\\n&= \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (\\bar x_i - \\mu)^2 + \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (x_{i,j} - \\bar x_i)^2\n\\end{align}\n\\]\n\n\\(\\text{SST}\\): Sum of Squares Total\n\\(\\text{SSA}\\): Sum of Squares between categories, treatments, or factors\n\n“A” stands for attributes (i.e. categories)\n\n\\(\\text{SSE}\\): Sum of Squares of Errors; randomness within categories, treatments, or factors\n\\(x_{ij}\\): The jth observation of the ith category\n\\(\\bar x_i\\): The sample mean of category i\n\\(\\mu\\): The overall sample mean\n\\(n_i\\): The group category sample size\n\\(a\\): The number of group categories\n\nCalculate MSA and MSE\n\\[\n\\begin{align}\n\\text{MSE} &= \\frac{\\text{SSE}}{N-a} \\\\\n\\text{MSA} &= \\frac{\\text{SSA}}{a-1}\n\\end{align}\n\\]\n\nWhere N is the total sample size\n\nCalculate the F statistic and P-Value\n\\[\nF = \\frac{\\text{MSA}}{\\text{MSE}}\n\\]\n\nFind the p-value (need a table to look it up)\nIf our F statistic is less than the critical value F statistic for a \\(\\alpha = 0.05\\) than we cannot reject the null hypothesis (no statistical difference between categories)\n\nDiscussion\n\nIf there is a group category that has more variance than the others’ attribute error (SSA), we should then pick that up when we compare it to the random error (SSE)\n\nIf a group is further away from the overall mean, then it will increase SSA and thus influence the overall variance but might not always increase random error",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#diagnostics",
    "href": "qmd/post-hoc-analysis-anova.html#diagnostics",
    "title": "ANOVA",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nEta Squared\n\nMetric to describe the effect size of a variable\nRange: [0, 1]; values closer to 1 indicating that a specific variable in the model can explain a greater fraction of the variation\nlsr::etaSquared(anova_model) (use first column of output)\nGuidelines\n\n0.01: Effect size is small.\n0.06: Effect size is medium.\nLarge effect size if the number is 0.14 or above\n\n\nPost-ANOVA Tests\n\nAssume approximately Normal distributions\nFor links to more details about each test, https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/post-hoc/\nDuncan’s new multiple range test (MRT)\n\nWhen you run Analysis of Variance (ANOVA), the results will tell you if there is a difference in means. However, it won’t pinpoint the pairs of means that are different. Duncan’s Multiple Range Test will identify the pairs of means (from at least three) that differ. The MRT is similar to the LSD, but instead of a t-value, a Q Value is used.\n\nFisher’s Least Significant Difference (LSD)\n\nA tool to identify which pairs of means are statistically different. Essentially the same as Duncan’s MRT, but with t-values instead of Q values.\n\nNewman-Keuls\n\nLike Tukey’s, this post-hoc test identifies sample means that are different from each other. Newman-Keuls uses different critical values for comparing pairs of means. Therefore, it is more likely to find significant differences.\n\nRodger’s Method\n\nConsidered by some to be the most powerful post-hoc test for detecting differences among groups. This test protects against loss of statistical power as the degrees of freedom increase.\n\nScheffé’s Method\n\nUsed when you want to look at post-hoc comparisons in general (as opposed to just pairwise comparisons). Scheffe’s controls for the overall confidence level. It is customarily used with unequal sample sizes.\n\nTukey’s Test\n\nThe purpose of Tukey’s test is to figure out which groups in your sample differ. It uses the “Honest Significant Difference,” a number that represents the distance between groups, to compare every mean with every other mean.\n\nDunnett’s Test\n\nLike Tukey’s this post-hoc test is used to compare means. Unlike Tukey’s, it compares every mean to a control mean.\n{DescTools::DunnettTest}\n\nBenjamin-Hochberg (BH) Procedure\n\nIf you perform a very large amount of tests, one or more of the tests will have a significant result purely by chance alone. This post-hoc test accounts for that false discovery rate.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-oneway",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-oneway",
    "title": "ANOVA",
    "section": "One-Way",
    "text": "One-Way\n\nMeasures if there’s a difference in means between any group category\nExample: 1 control, 2 Test groups\n\nData\ndata &lt;- data.frame(Group = rep(c(\"control\", \"Test1\", \"Test2\"), each = 10),\nvalue = c(rnorm(10), rnorm(10),rnorm(10)))\ndata$Group&lt;-as.factor(data$Group)\nhead(data)\n#&gt;   Group      value\n#&gt; 1 control  0.1932123\n#&gt; 2 control -0.4346821\n#&gt; 3 control  0.9132671\n#&gt; 4 control  1.7933881\n#&gt; 5 control  0.9966051\n#&gt; 6 control  1.1074905\nFit model\nmodel &lt;- aov(value ~ Group, data = data)\nsummary(model)\n#&gt;             Df    Sum Sq   Mean Sq  F value  Pr(&gt;F) \n#&gt; Group        2     4.407    2.2036     3.71  0.0377 *\n#&gt; Residuals   27    16.035    0.5939\n\n# or\nlm_mod &lt;- lm(value ~ Group, data = data)\nanova(lm_mod)\n\nP-Value &lt; 0.05 says at least 1 group category has a statistically significant different mean from another category\n\nDunnett’s Test\nDescTools::DunnettTest(x=data$value, g=data$Group)\n\n#&gt; Dunnett's test for comparing several treatments with a control : \n#&gt;     95% family-wise confidence level\n#&gt; $control\n#&gt;                     diff    lwr.ci      upr.ci  pval   \n#&gt; Test1-control -0.8742469 -1.678514 -0.06998022 0.0320 * \n#&gt; Test2-control -0.7335283 -1.537795  0.07073836 0.0768 .\n\nMeasures if there is any difference between treatments and the control\nThe mean score of the test1 group was significantly higher than the control group. The mean score of the test2 group was not significantly higher than the control group.\n\nTukey’s HSD\nstats::TukeyHSD(model, conf.level=.95)\n\nMeasures difference in means between all categories and each other",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ancova",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ancova",
    "title": "ANOVA",
    "section": "ANCOVA",
    "text": "ANCOVA\n\nAnalysis of Covariance is used to measure the main effect and interaction effects of categorical variables on a continuous dependent variable while controlling the effects of selected other continuous variables which co-vary with the dependent variable.\nMisc\n\nAnalysis of covariance is classical terminology for linear models but we often use the term for nonlinear models (Harrell)\nSee also\n\nHarrell - Biostatistics for Biomedical Research Ch. 13\n\n\nAssumptions\n\nIndependent observations (i.e. random assignment, avoid is having known relationships among participants in the study)\nLinearity: the relation between the covariate(s) and the dependent variable must be linear.\nNormality: the dependent variable must be normally distributed within each subpopulation. (only needed for small samples of n &lt; 20 or so)\nHomogeneity of regression slopes: the beta-coefficient(s) for the covariate(s) must be equal among all subpopulations. (regression lines for these individual groups are assumed to be parallel)\n\nFailure to meet this assumption implies that there is an interaction between the covariate and the treatment.\nThis assumption can be checked with an F test on the interaction of the independent variable(s) with the covariate(s).\n\nIf the F test is significant (i.e., significant interaction) then this assumption has been violated and the covariate should not be used as is.\nA possible solution is converting the continuous scale of the covariate to a categorical (discrete) variable and making it a subsequent independent variable, and then use a factorial ANOVA to analyze the data.\n\n\nThe covariate (adjustment variable) and the treatment are independent\nmodel &lt;- aov(grade ~ technique, data = data)\nsummary(model)\n\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; technique    2    9.8    4.92    0.14  0.869\n#&gt; Residuals  87 3047.7  35.03\n\nH0: variables are independent\n\n\nHomogeneity of variance: variance of the dependent variable must be equal over all subpopulations (only needed for sharply unequal sample sizes)\n# response ~ treatment\nleveneTest(exam ~ technique, data = data)\n\n#&gt;       Df F value    Pr(&gt;F)   \n#&gt; group  2  13.752 6.464e-06 ***\n#&gt;       87\n\n# alt test\nfligner.test(size ~ location, my.dataframe)\n\nH0: Homogeneous variance\nThis one fails\n\nFit\nancova_model &lt;- aov(exam ~ technique + grade, data = data)\ncar::Anova(ancova_model, type=\"III\")\n\n#&gt;                 Sum Sq Df F value    Pr(&gt;F)   \n#&gt;     (Intercept) 3492.4  1 57.1325 4.096e-11 ***\n#&gt;     technique  1085.8  2  8.8814 0.0003116 ***\n#&gt;     grade          4.0  1  0.0657 0.7982685   \n#&gt;     Residuals  5257.0 86\n\nWhen adjusting for current grade (covariate), study technique (treatment) has a significant effect on the final exam score (response).\n\nDoes the effect differ by treatment\npostHocs &lt;- multicomp::glht(ancova_model, linfct = mcp(technique = \"Tukey\"))\nsummary(postHocs)\n\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   \n#&gt; B - A == 0   -5.279      2.021  -2.613  0.0284 * \n#&gt; C - A == 0    3.138      2.022   1.552  0.2719   \n#&gt; C - B == 0    8.418      2.019   4.170  &lt;0.001 ***\n\nAlso see Post-Hoc Analysis, Multilevel &gt;&gt; Tukey’s Test\n\\(A\\), \\(B\\), and \\(C\\) are the study techniques (treatment)\nSignificant differences between \\(B\\) and \\(A\\) and a pretty large difference between \\(B\\) and \\(C\\).\n\nExample: RCT\n\\[\n\\begin{align}\n\\text{post}_i &\\sim \\mathcal{N}(\\mu_i, \\sigma_\\epsilon)\\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{tx}_i + \\beta_2 \\text{pre}_i\n\\end{align}\n\\]\nw2 &lt;- glm(\n  data = dw,\n  family = gaussian,\n  post ~ 1 + tx + pre)\n\nSpecification\n\npost, pre: The post-treatment and pre-treatment measurement of the outcome variable\ntx: The treatment indicator variable\n\\(\\beta_0\\): Population mean for the outcome variable in the control group\n\\(\\beta_1\\): Parameter is the population level difference in pre/post change in the treatment group, compared to the control group.\n\nAlso a causal estimate for the average treatment effect (ATE) in the population, τ\n\nBecause pre is added as a covariate, both \\(\\beta_0\\) and \\(\\beta_1\\) are conditional on the outcome variable, as collected at baseline before random assignment.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html",
    "href": "qmd/visualization-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-misc",
    "href": "qmd/visualization-general.html#sec-vis-gen-misc",
    "title": "General",
    "section": "",
    "text": "Notes from\n\nFriends Don’t Let Friends\n\nMicrosoft Paint 3D\n\nLocation: Start &gt;&gt; All Programs &gt;&gt; Paint 3D\nHightlight Text\n\nClick 2D Shapes (navbar) &gt;&gt; Select square (side panel)\nLeft click and hold &gt;&gt; Extend area around text you want to highlight &gt;&gt; Release\nChoose Line Type color and Sticker Opacity level (37%)\nOn area surrrounding text\n\nIf needed, make area size adjustment dragging little box-shaped icons that are along the outside\nOn the right side, click the check mark icon to finalize\n\nClick Menu (left-side on navbar) &gt;&gt; save as &gt;&gt; Image\n\nIt adds a png extension, but you just need to type the name.\n\n\n\nAlt Text\n\nThe guiding principle is to write alt text that gives disabled readers as close to the same experience as nondisabled readers as possible.\n\nggplot2\n\nDon’t use stat calculating geoms and set axis limits with scale_y_continuous\n\n\nSee examples of the behavior in this thread\n\nDefaults for any {ggplot2} geom using the default_aes field (i.e. GeomBlah$default_aes )\n\nFractional Data\n\nUse Stacked Bars instead of Pie or Circular or Donut\n\nHumans are better at judging lengths than angles (article)\n\n\nFactorial Experiments\n\nDon’t use bars factorial experiments\n\nCheck outcome ranges by group when facetting",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-concepts",
    "href": "qmd/visualization-general.html#sec-vis-gen-concepts",
    "title": "General",
    "section": "Concepts",
    "text": "Concepts\n\nExploration and Analysis\n\nGoal: explore a new dataset, gertan overview, find answers to specific questions\nFast iteration of many generic charts, don’t customize or worry about color schemes, etc.\n\nExplanation\n\nGoal: help others understand a relationship in the data\nUse as few charts as possible, carefully chosen\nSequence so that they are easy to understand\nAdd interaction to help people get a better understanding\n\nPresentation\n\nGoal: walk your audience through an argument, help them come to a decision\nFocus on polishing charts: colors, legends, titles, etc.\nHighlighting of key elements (which might be considered biasing in Exploration)\nPossibly use of unusual charts for memorability\nSequence to make a specific point",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-svg",
    "href": "qmd/visualization-general.html#sec-vis-gen-svg",
    "title": "General",
    "section": "SVG",
    "text": "SVG\n\nBetter for doing post-processing in Inkscape and gimp\nSVGs won’t be pixelated when you zoom in like PNGs are\nD3 outputs SVG\nsvglite PKG\n\nusing svglite instead of base::svg( ) allows you alter text in Inkscape or Illustrator\nrequires the used fonts to be present on the system it is viewed on.\n\nThe vast majority of interactive data visualizations on the web are now based on D3.js which often renders to SVG and it all seems to behave. Still, this is something to be mindful of, and a reason to use svg() if exactness of the rendered text is of prime importance\n\nFile size will be dramatically smaller",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-layout",
    "href": "qmd/visualization-general.html#sec-vis-gen-layout",
    "title": "General",
    "section": "Layout",
    "text": "Layout\n\nFacetting vs Single Graph\n\nLayout based on experiement design\n\nAlign title ALL the way to the left (ggplot: plot.title.position = “plot”)\nremove legends\n\nuse colored text in title (ggtext)\nlabel points or lines\nlast resort: place legend underneath title/subtitle\n\ngrid lines\n\nremove if possible\nsparse and faint if needed\n\naxis labels\n\nremove if obvious (e.g brands of cars)\ncreate a title that informs about the axis labels\nshould always be horizontal\n\nflip axis, don’t angle them 45 degrees\n\n\ntext\n\nleft-align most text\ncan center a subtitle if it helps with making the graph more symmetrical\nsome labels can be right-aligned\n\nRemove all borders\nMaximize white space\n\ndon’t cram visuals together\n\nWorking memory. A cognitive limitation that affects plot comprehension is the limit on working memory. Typically, working memory is limited to approximately seven (plus or minus two) items, or chunks. In practice, this means that categorical scales with more than seven categories decrease readability, increase comprehension time, and require significant attentional resources, because it is not possible to hold the legend mapping in working memory.\nThe use of redundant aesthetics that activate the same gestalt principles (such as color and shape in a scatter plot, which both activate similarity) results in higher identification of corresponding data features. In addition, dual encoding increases the accessibility of a chart to individuals who have impaired color vision or perceptual processing (e.g., dyslexia, dysgraphia). This experimental evidence directly contradicts the guidelines popularized by Tufte (1991), which suggest the elimination of any feature that is not dedicated to representing the core data, including redundant encoding and other unnecessary graphical elements.\nggplot themes\n\nCedric Sherer (article)\ntheme_set(theme_minimal(base_size = 15, base_family = \"Anybody\"))\ntheme_update(\n  axis.title.x = element_text(margin = margin(12, 0, 0, 0), color = \"grey30\"),\n  axis.title.y = element_text(margin = margin(0, 12, 0, 0), color = \"grey30\"),\n  panel.grid.minor = element_blank(),\n  panel.border = element_rect(color = \"grey45\", fill = NA, linewidth = 1.5),\n  panel.spacing = unit(.9, \"lines\"),\n  strip.text = element_text(size = rel(1)),\n  plot.title = element_text(size = rel(1.4), face = \"bold\", hjust = .5),\n  plot.title.position = \"plot\"\n)",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-ar",
    "href": "qmd/visualization-general.html#sec-vis-gen-ar",
    "title": "General",
    "section": "Aspect Ratio",
    "text": "Aspect Ratio\n\nMisc\n\nGolden Rectangle\n```{{{r, fig.width = 6, fig.asp = 1.618}}}\n```\nGet consistent outputs\n\nRStudio pane displays in 72dpi which can mislead you on what your output looks like.\nThink {ragg} is supposed to have taken care of the inconsistency in terms of printing on different OSes\nUsing {camcorder}\n\nStart “recording” plots\ncamcorder::gg_record(\n  dir = \"imgs\",\n  width = 12,\n  height = 12*9/16,\n  dpi = 300,\n  bg = \"white\"  # Makes sure background is actually white an not transparent\n)\n\nAll plots will immediately be exported as a .png-file to the directory specified\nAll plots will be displayed in the viewer with dimensions and resolution that you specified and not in the plots pane in RStudio\n300 dpi is pretty standard and default of ggsave\n\nDo work. Export final png file in directory when done and delete the rest\nRegarding Fonts\n\nIf using {ragg}, then all is fine.\nIf using {showtext}, then you have to set resolution in options, showtext_opts(dpi = 300)\n\n\n\n\nTwitter\n\nVideo: 1105 x 1920\n\nLine Charts\n\nMatters most if two different line charts are being compared\n\nThe core idea of “banking” is that the slopes in a line chart are most readable if they average to 45°.\nUse ggthemes::bank_slopes(x, y, method = c(\"ms\", \"as\"))\n\n2 methods (that req. no optimization) from Jeer, Maneesh who followed Cleveland’s 45° guideline\ndocs\n\n“The problem with banking is that sometimes you need the chart in a certain aspect ratio to fit into a page layout. Especially if banking produces portrait sized charts. But why not let the optimal chart ratio define your layout? For instance, you can put the additional information to the side of the chart. Remember that the main goal of banking is to increase the readability of the line slopes. In the following example, the slopes for Nuclear and Renewables would have been much more difficult to see, if the chart would have been ‘squeezed’ to a landscape aspect.” (article)",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-typo",
    "href": "qmd/visualization-general.html#sec-vis-gen-typo",
    "title": "General",
    "section": "Typography",
    "text": "Typography\n\nCSS Length Units\n\nAbsolute Lengths\n\n* Pixels (px) are relative to the viewing device. For low-dpi devices, 1px is one device pixel (dot) of the display. For printers and high resolution screens 1px implies multiple device pixels.\n\n\ncm\ncentimeters\n\n\nmm\nmillimeters\n\n\nin\ninches (1in = 96px = 2.54cm)\n\n\npx*\npixels (1px = 1/96th of 1in)\n\n\npt\npoints (1pt = 1/72 of 1in)\n\n\npc\npicas (1pc = 12 pt)\n\n\n\nRelative Lengths\n\nThe em and rem units are practical in creating perfectly scalable layout! * Viewport = the browser window size. If the viewport is 50cm wide, 1vw = 0.5cm.\n\n\n\n\n\n\nem\nRelative to the font-size of the element (2em means 2 times the size of the current font)\n\n\nex\nRelative to the x-height of the current font (rarely used)\n\n\nch\nRelative to the width of the “0” (zero)\n\n\nrem\nRelative to font-size of the root element\n\n\nvw\nRelative to 1% of the width of the viewport*\n\n\nvh\nRelative to 1% of the height of the viewport*\n\n\nvmin\nRelative to 1% of viewport’s* smaller dimension\n\n\nvmax\nRelative to 1% of viewport’s* larger dimension\n\n\n%\nRelative to the parent element\n\n\n\n\nFont Weight\n\n400 is the same as normal, and 700 is the same as bold\n\nFonts\n\nAdelle\n\nA serif font that doesn’t go overboard. Good for short paragraphs.\n\nAlegreya\nBarlow\n\nSlender font\n\nFira Code Retina\n\ncode syntax highlighting\n@import url(“https://cdn.rawgit.com/tonsky/FiraCode/1.205/distr/fira_code.css”);\n\nLora\n\nbody\nUsed in COVID-19 project &gt;&gt; Static Charts, Hospitals\n@import url(‘https://fonts.googleapis.com/css2?family=Lora&display=swap’);\n\nMerriweather\n\nSimilar to Adelle, but has a bit more pronounced hooks\n\nMontserrat\n\nSimple design that can handle long lines of text. I like it for minimal plots.\n\nPrata\n\nheader\nUsed in ericbook-distill\n@import url(‘https://fonts.googleapis.com/css2?family=Cinzel&display=swap’);\n\nReforma family\n\nonly one I have is Roboto, need to import and load the rest using extrafont pkg\n\nRoboto family\n\nDancho shiny apps\n\np, body: 100 wt\nHeaders, (h1, h2, etc.): 400 wt\n\nRoboto Slab\n\nNot sure if this is exact font used but it’s very similar. Only difference I spotted was the “3.”\n\n\nTitillium Web Bold\n\nheaders\nUsed in ebtools\n@import url(‘https://fonts.googleapis.com/css?family=Titillium+Web&display=swap’);\n\n\nNumbers\n\nshould all have the same height (Lining)\nshould all have the same width (Tabular)\n\nUsing {showtext}\nlibrary(showtext)\n#load font\nfont_add_google(name = \"Metal Mania\", family = \"metal\")\nfont_add_google(name = \"Montserrat\", family = \"montserrat\")\nshowtext_auto()",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-annot",
    "href": "qmd/visualization-general.html#sec-vis-gen-annot",
    "title": "General",
    "section": "Annotation",
    "text": "Annotation\n\nPeople love annotations (thread, paper). More text, the better.\n\nTheir takeaway from the chart is more likely to resemble the annotation if it takes the form of L2 and/or L4 and is close to the data\n\nExample: Financial Times\n\n\nTitle (L2) is used for part of the takaway message\n\nSubtitle used to describe the Y-Axis\n\nChart annotation paragraph (L4) gives contextual information\n\n\nWhen to annotate\n\na design element in your visualization that needs explaining\na data point or series that you want readers to see, like an outlier\nreaders should know something to better understand why certain data points look the way they do\n\nRemove the color key/legend and directly label your categories\n\nIf the screen is small (e.g. mobile), then it’s better to keep the legend\n\nMake it obvious which units your data uses.\n\nDon’t just put units in the description, but also in axis labels, tooltips, and annotations\n\nFor large numbers (e.g. 20 million), try to use B, M, K instead of an annotation somewhere that says something like “in thousand”\nTooltips\n\nConsider not just stating the numbers in tooltips, but also the category\n\ne.g. “3.4% unemployed” instead of “3.4%,” or “+16% revenue” instead of “+16%”\n\nUse a transparent background by setting the alpha channel of CSS background-colorto a number less than 1\n\ne.g. 0.3 using rgba(255, 255, 255, 0.3)\n\nWith a transparent background, text behind the tooltip can interfere with the text in the tooltip, so also apply backdrop-filter\n\nExample:\n.tooltip {\n  background-color: rgba(255, 255, 255, 0.3);\n  -webkit-backdrop-filter: blur(2px);\n  backdrop-filter: blur(2px);\n}\n@media (prefers-contrast: more) {\n  .tooltip {\n    background-color: white;\n    -webkit-backdrop-filter: none;\n    backdrop-filter: none;\n  }\n}\n\nExample shows a tooltip that has an HTML class of “tooltip”.\nblur is measured in pixels and the image size varies with screen width, so the optimal blur size here may vary for you depending on the dimensions of your browser window.\n\nApplies a Gaussian blur to the target element’s background with the standard deviation specified as the argument (e.g. two pixels).\n\nAs of Mar 2023, doesn’t work on Safari, so adding -webkit-backdrop-filter allows it to work on Safari\n@media (prefers-contrast: more)checks if your user has informed their operating system or browser that they prefer increased contrast. When they do, this chunk then overrides the applied styles.\n\n\n\nTransparent backgrounds might work better with thematic maps and less with scatter plots\nDon’t center-align your text\nUse straightforward phrasings\nMove axis labels nearest the most important chart objects (e.g. bars)\n\n\nIf the higher bars are what’s most important and they’re on the right, then usea right-side axis\n\nFonts for annotation\n\n\nUse what readers are most used to (e.g. sans-serif regular, &gt;12px, (almost) black text\nIf you need to need a lot of words and they don’t fit, don’t use smaller font, use a tooltip instead\n\nOn mobile screens you can also hide the least important annotations, or move them below the visualization\n\n\nLead the eye with font sizes, styles, and colors\n\n\nThe biggest and boldest text with the highest contrast against the background should be reserved for the most important information.\n\nDon’t overdo it though\n\nUse only two levels of hierarchy that are clearly different from each other — like a 12px gray and a 14px black\nEmphasize within the annotations using boldness\n\nKeep labels horizontal\n\n\nUse a text outline\n\n\nSet the stroke around your letters, using the background color of your chart.\n\nBe conversational first and precise later",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-color",
    "href": "qmd/visualization-general.html#sec-vis-gen-color",
    "title": "General",
    "section": "Color",
    "text": "Color\n\nMisc\n\nWhen choosing bg and fg colors, keep in mind that it’s generally a good idea to pick colors with a similar hue but a large difference in their luminance.\nDatawrapper guide\nWhen using several subplots together to tell a story and they each have their own color scheme. Blend a color into each color scheme to produce a more unified look\n\nExample: Blending blue into a plot with green color scheme.\n\n\nBreakpoints for scales\n\nHow to choose an interpolation for your color scale\n\nCharts (see prismatic PKG to do this manipulation within ggplot)\nPalette composition methods\n\nComplimentary\n\nopposite sides of the color wheel (2 colors)\ncontrast\n\nAnalogous\n\nsame side of the color wheel (multiple)\ngradient\n\nTriadic\n\nforms triangle on the color wheel\nvibrant, contrast\n\nOthers\n\nsplit complimentary (popular)\n\nComprised of one color and two colors symmetrically placed around it. This strategy adds more variety than complementary color schemes by including three hues without being too jarring or bold. Using this method, we end up with combinations that include warm and cool hues that are more easily balanced than the complementary color schemes\n\nquadratic\n\n\nAdjustments once you chosen a color (hue) to create variations\n\nMove brightness up for lighter variations and down for darker variations\nThen, move saturation in the opposite way you moved brightness\n\nSave colors you find attractive\n\ninstant eyedropper (windows)\nThen use HSL (hue, saturation, lightness) slider for adjustments\n\nBackgrounds\n\nWhite\n\nbright, used a lot\ntry ivory or a light gray\nshades of eggshell, link\n\nAvoid black (or REALLY dark) unless situation calls for it\n\ndark is fine\n\n\nLightest and darkest colors should have meaning (e.g. min, max, mean, zero) and not just some arbitrary numbers\n\nWhat to do when you have a lot of categories\n\nSimply don’t show different colors Does your chart work without colors?\n\n1 color and a discrete axis with the categories\n\nShow shades, not hues Can you make the chart less confetti-like?\n\nAlthough, consider not using shades when the parts are as or more important than the totals\n\nEmphasize Can you only use color for your most important categories?\nLabel directly Can you use the same or similar colors but label them?\nMerge categories Can you put categories together?\nGroup categories, but keep showing them Can strokes help to tell categories apart?\n\nChange the chart type Will another chart type rely less on colors?\n“Small multiply” it Can you split the categories into multiple charts? (i.e. facet by category)\nAdd other indicators Can you add symbols, patterns, line widths, or dashes?\n\n\nDoesn’t use any color — just opacity, thickness, and dotted lines.\n\nUse tooltips and hover effects Can smaller categories be hidden with them?\n\nColor scales should be chosen to best match the data values and plot type: If the goal is to show magnitude, a univariate color scheme is typically preferable, while a double-ended color scale is typically more effective when showing data that differ in sign and magnitude. Where possible, color scales should use a minimal number of hues, varying intensity or lightness of the color to show magnitude, and transitioning through neutral colors (white, light yellow) when utilizing a gradient. Cognitive load can also be reduced by selecting colors with cultural associations that match the data display, such as the use of blue for men and red (or pink) for women, or the use of blue for cold temperatures and red/orange for warm temperatures.\n\nIt is also important to consider the human perceptual system, which does not perceive hues uniformly: We can distinguish more shades of green than any other hue, and fewer shades of yellow, so green univariate color schemes will provide finer discriminability than other colors because the human perceptual system evolved to work in the natural world, where shades of green are plentiful.\n\n\nFigure above shows the International Commission on Illumination (CIE) 1931 color space, which maps the wavelength of a color to a physiologically based perceptual space; a significant portion of the color space is dedicated to greens and blues, while much smaller regions are dedicated to violet, red, orange, and yellow colors. This unevenness in mapping color is one reason that the multi-hued rainbow color scheme is suboptimal—the distance between points in a given color space may not be the same as the distance between points in perceptual space. As a result of the uneven mapping between color space and perceptual space, multi-hued color schemes are not recommended.",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-types",
    "href": "qmd/visualization-general.html#sec-vis-gen-types",
    "title": "General",
    "section": "Chart Types",
    "text": "Chart Types\n\nBar Graphs\n\nDon’t use bar graphs for anything except counts. Audiences have trouble with the abstraction.\nFor averages, used errorbar charts or use median + raincloud.\nGuide\n\n\nStacked Bar\n\nReplacement for pie charts et al when dealing with fractional data\nAlways reorder stacks\n\n\nRmd tutorial for reordering optimation\n\n\nBox Plots\n\n\nSmall data - emphasize the points\nLarge data - emphasize the box\n\nLine Charts\n\nSometimes it’s appropriate not to use zero as the baseline\nHaving the y-axis not intersect the x-axis can minimize the risk of confusing the readers with a non-zero baseline chart\nTime Series of ordinal discrete data by category\n\nordinal data has 3 levels\n\n\nHeatmaps\n\nReorder rows and columns to produce a more meaningful visualization\n\n\nGuide on reordering heatmaps\nIf order is important, then this may not be possible\n\nReorder by clustering\n\n\nNetwork Graphs\n\nAlways try different multiple layout methodologies",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-maps",
    "href": "qmd/visualization-general.html#sec-vis-gen-maps",
    "title": "General",
    "section": "Maps",
    "text": "Maps\n\nAbove rules also apply\nRemove as many extraneous elements as possible\n\nHard because maps have so many necessary elements\n\nBorders, Labels, etc.\n\nIn cloropleths, remove unnecessary borders (e.g. along coastlines)\n\n\n“Borders as lines” is much less cluttered\nArticle, rmapshaper::ms_innerlines() keeps only the necessary inner borders in the “geometry” column of the spatial dataset.\n\n\nPay close attention to typography hierarchy\n\nBold, Font size, etc\n\nUse iconography to help users identify what you want them to see\nNumeric values (thread)\n\nPalettes: use a sequential (top row) or diverging (bottom row)\n\n\nFor diverging palettes\n\n\nThe middle value should be light on a light background (top left) or dark on a dark background (bottom left)\n\n\nBackgrounds:\n\n\nLight background: darker color on the value of interest (usually the higher value) (top left)\nDark background: lighter color on the value of interest (usually the higher value) (bottom left)\n\n\nTry not to use Rainbow palettes, because they are misleading\n\nThe rainbow and jet colors are problematic as the change in color is not perceptually uniform, leading to distinct ‘bands’ of certain colors. This causes misleading jumps and emphasizes certain values, most likely without the intention to highlight them. (Cedric Scherer)\n(acceptable) rainbow called “Turbo” if you need one (article)\n\nCode - see comments for links to R scripts and improved versions of Turbo\n\nOther Alternatives",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-area",
    "href": "qmd/visualization-general.html#sec-vis-gen-area",
    "title": "General",
    "section": "Area",
    "text": "Area\n\nIn general, these charts aren’t good for noisy data and data with many categories\n\nHave issues when values increase sharply (see video. around 50:13)\n\nExperiment with the order of the groups\n\nEvents that you’re looking for are probably only visable when there’s a particular order\nMost of the time, putting the most stable groups at the bottom produces the best results",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-ts",
    "href": "qmd/visualization-general.html#sec-vis-gen-ts",
    "title": "General",
    "section": "Time Series",
    "text": "Time Series\n\nHorizon Charts\n\nSee Anomaly Detection &gt;&gt; Charts\nEspecially useful for showing data with large amplitudes in a short vertical space",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-uncert",
    "href": "qmd/visualization-general.html#sec-vis-gen-uncert",
    "title": "General",
    "section": "Uncertainty",
    "text": "Uncertainty\n\nVisualizing only inferential uncertainty can lead to significant overestimates of treatment effects\n\n\nWhen possible, plot individual data points alongside statistical estimates\n\nTranslate percentages into counts (e.g. “a 1 out of 5 chance” rather than “a 20% chance”)\n\n{riskyr} - icon arrays and less sophisticated viz for the above chart\nicon arrays\n\nExamples\n\nbase rates and error rates (paper)\nrelative risks (paper)\n\n\nWaffle plots are similar to icon arrays\n\nquantile dotplots\n\n{ggdist} (many examples and flavors)\n\nhypothetical outcome plots\n\nConsists of multiple individual plots (frames), each of which depicts one draw from a distribution (use case for animation)\nBest suited for multivariate judgments like how reliable a perceived difference between two random variables is\nIllustration of the process\n\nYou create a distribution to sample from or using known distribution and parameters or bootstrapping the sample and sample from each bootstrap.\nEach sample/draw is presented on the right side of the distribution plot (fig 1) (final product)\n\nI think it would be better if after each draw the previous draw remained but was de-emphasized (i.e. turned light gray)\nAnother example would McElreath’s lecture video on posterior prediction distribution.\n\nFigs 2 and 3 show a sequence of draws from a joint distribution of uncorrelated variables (fig 2) and correlated variables (fig 3)\n\nExample: NYT on interpreting jobs reports\n\n2 facets: accelerating job growth (left), steady job growth (right)\nFor each facet,\n\nthe left plot is static, and the right plot is animated showing different noisy samples of the same underlying dgp\nthe left plot shows what normals perceive the distribution to look like for the given interpretation (e.g. accelerating job growth), and the right plot shows what real (i.e. noisy) data with the same interpretion looks like.\n\n\n\n\nFan charts\n\n\nshows a 90% interval broken divided into 30% increments (left) or 10% increments (right)\n\nShow previous forecasts\n\n\nTruth is in dark blue with light blue branches showing previous forecasts",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-mob",
    "href": "qmd/visualization-general.html#sec-vis-gen-mob",
    "title": "General",
    "section": "Mobile",
    "text": "Mobile\n\nMisc\n\nRStudio plots are displayed in 96 dpi and ggsave uses 300 dpi as default\n\ni.e. viewed plots won’t look the same as the saved plots using default settings\n\n\nUse sharp color contrasts when highlighting\nMinimal readable size is 16, but 22 is recommended\nAspect ratio of 4:3 or 1024 x 768 pixels\n\nAnother article say 1:2\n\nBar Charts should be horizontal to make charts with many categories readable\n\nMobile screens are more tall than wide so labels on the y-axis makes more sense than on the x-axis\n\nR\n\nSet-up external window with aspect ratio (e.g. 1:2)\ndev.new(width=1080, height=2160, unit=\"px\", noRStudioGD = TRUE)\n\nnoRStudioGD = TRUE says any new plots appear in the new graphics window rather than the RStudio graphics device\nCan also use windows(), x11(), or png() from {ragg}\n\n\nUse Quarto (or Rmd) for developement\n#| dpi: 300     \n#| fig.height: 7.2     \n#| fig.width: 3.6     \n#| dev: \"png\"     \n#| echo: false     \n#| warning: false     \n#| message: false`\n\nThis way your dpi and aspect ratio are set and you can view the final output without having to save the png and viewing it separately to see how it looks\nfig.height and fig.width are always given in inches\n\nIf you haven’t set your Quarto document to be self-contained, then the images have also already been saved for you - probably in a folder called documentname_files/figure-html/",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html",
    "href": "qmd/quarto-rmarkdown.html",
    "title": "Quarto",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-misc",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-misc",
    "title": "Quarto",
    "section": "",
    "text": "Packages\n\n{quarto}\n\nResources\n\nDocs\nReference\nTroubleshooting\n\nquarto --version - Must be in RStudio Terminal\nquarto check - Must be in RStudio Terminal - versions and engine checks\n$ quarto check\n[&gt;] Checking versions of quarto binary dependencies...\n      Pandoc version 3.1.1: OK\n      Dart Sass version 1.55.0: OK\n[&gt;] Checking versions of quarto dependencies......OK\n[&gt;] Checking Quarto installation......OK\n      Version: 1.3.340\n      Path: C:\\Users\\tbats\\AppData\\Local\\Programs\\Quarto\\bin\n      CodePage: 1252\n[&gt;] Checking basic markdown render....OK\n[&gt;] Checking Python 3 installation....OK\n      Version: 3.8.1 (Conda)\n      Path: C:/Users/tbats/Miniconda3/python.exe\n      Jupyter: 4.9.1\n      Kernels: python3\n(\\) Checking Jupyter engine render....2023-04-28 10:18:15,018 - traitlets - WARNING - Kernel\nProvisioning: The 'local-provisioner' is not found.  This is likely due to the presence of multiple jupyter_client distributions and a        previous distribution is being used as the source for entrypoints - which does not include 'local-provisioner'.  That distribution should     be removed such that only the version-appropriate distribution remains (version &gt;= 7).  Until then, a 'local-provisioner' entrypoint will     be automatically constructed and used.\nThe candidate distribution locations are: ['C:\\\\Users\\\\tbats\\\\Miniconda3\\\\lib\\\\site-packages\\\\jupyter_client-5.3.4.dist-info',                'C:\\\\Users\\\\tbats\\\\Miniconda3\\\\lib\\\\site-packages\\\\jupyter_client-7.0.6.dist-info']\n[&gt;] Checking Jupyter engine render....OK\n[&gt;] Checking R installation...........OK\n      Version: 4.2.3\n      Path: C:/PROGRA~1/R/R-42~1.3\n      LibPaths:\n        - C:/Users/tbats/AppData/Local/R/win-library/4.2\n        - C:/Program Files/R/R-4.2.3/library\n      knitr: 1.42\n      rmarkdown: 2.20\n[&gt;] Checking Knitr engine render......OK\nCLI\n\nquarto render to compile a document\nquarto preview to render a live preview that automatically updates when the source files are saved\n\nUsing a development verison of Quarto\n\nFirst Usage\n\nChange directories to where you want to store the dev version\nClone repo and change to the cloned directory\ngit clone https://github.com/quarto-dev/quarto-cli\ncd quarto-cli\nDisable Anti-Virus\nRun Configuration Script\n\nWindows Command Prompt\ncmd /k configure.cmd\n\n\\k keeps the window open in case it errors\n\nPowershell\nInvoke-Item configure.cmd\nLinux/MacOS\n./configure.sh\nThis will take a minute or two as it checks versions, installs dependencies like pandoc, etc.\n\nAdd path to quarto.cmd to PATH\n\nAfter the configuration file runs, it will output the path you need to put on PATH, e.g. \"C:\\Users\\erc\\Documents\\Quarto\\quarto-cli\\package\\dist\\bin\"\n\nEnable Anti-Virus\nShould be able to use in RStudio\n\nI was not able to use the RStudio terminal for quarto commands (e.g. quarto check) though.\nTo find the version, I just opened powershell and ran quarto –version just to make sure it was running and on PATH.\n\nNot sure if they use this every time but it was 99.9.9 instead of the verion in the changelog.\n\nI also rendered a qmd file using quarto-cmd from the root directory of quarto-cli to see if it matched the output from RStudio. (cd qmd then quarto preview forecasting-statistical.qmd --to html --no-watch-inputs --no-browse)\n\n\nSubsequent Development Versions\n\nChange directory to quarto-cli and git pull\n\n\nShortcuts\n\nNew R chunk: ctrl + alt + i\nBuild whole book: ctrl+shift b\nRender page and preview book: ctrl+shift k\n\nUsing yaml style for chunk options\n\nConvert Rmd chunk options to Quarto: knitr::convert_chunk_header(\"doc.rmd\", \"doc.qmd\")\nAnchor Link - A link, which allows the users to flow through a website page. It helps to scroll and skim-read easily. A named anchor can be used to link to a different part of the same page (like quickly navigating) or to a specific section of another page.\n\nThis is the “#sec-moose” id that can be added to headers which it allows to be referenced within the document or in other documents.\n\nMathJax commands\n\nFont Size: \\tiny{ }, \\scriptsize{ }, \\small{ }, \\normal{ }, \\large{ }, \\Large{ }, \\LARGE{ }, \\huge{ }, \\Huge{ }\n\nLightbox\n\nDocs\nGrouping images for lightbox carousel: ![A Lovely Image](mv-1.jpg){group=\"my-gallery\"}",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-quarto",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-quarto",
    "title": "Quarto",
    "section": "Syntax",
    "text": "Syntax\n\nAlign code chunk under bullet and add indented comment below chunk\n-   [Example]{.ribbon-highlight} (using a SQL Query; method 1)\n\n    ``` r\n    # open dataset\n    ds &lt;- arrow::open_dataset(dir_out, partitioning = \"species\")\n    # open connection to DuckDB\n    con &lt;- dbConnect(duckdb::duckdb())\n    # register the dataset as a DuckDB table, and give it a name\n    duckdb::duckdb_register_arrow(con, \"my_table\", ds)\n    # query\n    dbGetQuery(con, \"\n      SELECT sepal_length, COUNT(*) AS n\n      FROM my_table\n      WHERE species = 'species=setosa'\n      GROUP BY sepal_length\n    \")\n\n    # clean up\n    duckdb_unregister(con, \"my_table\")\n    dbDisconnect(con)\n    ```\n\n    -   filtering using a partition, the WHERE format is '\\&lt;partition_variable\\&gt;=\\&lt;partition_value\\&gt;'\n\nSpace between bullet and top ticks\nSpace between bottom ticks and bullet\nNote alignment of text\n\nAdd Code Annotations\n-   [Partition a large file and write to arrow format]{.underline}\n\n    ``` r\n    lrg_file &lt;- open_dataset(&lt;file_path&gt;, format = \"csv\") # &lt;1&gt;\n    lrg_file %&gt;%\n        group_by(var) %&gt;% # &lt;2&gt;\n        write_dataset(&lt;output_dir&gt;, format = \"feather\") # &lt;3&gt;\n    ```\n\n    1.  Pass the file path to `open_dataset()`\n\n    2.  Use `group_by()` to partition the Dataset into manageable chunks\n\n    3.  Use `write_dataset()` to write each chunk to a separate Parquet file---all without needing to read the full CSV file into R\n\n    -   `open_dataset` is fast because it only reads the metadata of the file system to determine how it can construct queries",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#chunk-options-and-yaml",
    "href": "qmd/quarto-rmarkdown.html#chunk-options-and-yaml",
    "title": "Quarto",
    "section": "Chunk Options and YAML",
    "text": "Chunk Options and YAML\n\nSet global chunk options in yaml\n\nEnable Margin Notes\n---\n# YAML front matter\nreference-location: margin\n---\n!expr to render code within chunk options\n\ne.g. figure caption: #| fig-cap: !expr glue::glue(\"The mean temperature was {mean(airquality$Temp) |&gt; round()}\")\n\nConditional Code Chunk Evaluation\n\nExample: document output type\n\nSet value in a code chunk\n```{r setup}\n# Include in first chunk of .qmd\n# Get output file type\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\n```\nUse !expr sytax to determine evaluation status\n\nExample: eval chunk based on output type\n```{r}\n#| eval: !expr out_type == \"html\"\n\n# code to create interactive {plotly}\n```\n\n```{r}\n#| eval: !expr out_type == \"docx\"\n\n# code to create static {ggplot2}\n```\n\n\nExample: Use parameterization to set value\n---\ntitle: \"test\"\nformat: html\nparams:\n  my_value: false\n---\n\nmy_value can then be used throughout the document to determine chunk evaluation status\n\n\ncolumn: screen-inset yaml markup is used to show a very wide table\nCLI\n\nquarto render to compile a document\nquarto preview to render a live preview that automatically updates when the source files are saved\n\nGraphics\n\nCode Chunk\n#| dpi: 300\n#| fig.height: 7.2\n#| fig.width: 3.6\n#| dev: \"png\"\n#| echo: false\n#| warning: false\n#| message: false\n\nExample shows settings for a graph for mobile\nfig.height and fig.width are always given in inches\n\n\nIf you haven’t set your Quarto document to be self-contained, then the images have also already been saved for you - probably in a folder called documentname_files/figure-html/\nformat: \n  html:\n    embed-resources: true\nYAML Example\n\nNested Tabs\n\nKnitr Hooks\n\nNotes from Writing knitr hooks\n\nAlso has a knitr hook example that alters cell output (e.g. only prints 4 lines of a vector)\n\nChunk Hooks\n\nChunk hooks get called twice: once before knitr executes the code in the chunk, and once again afterwards\nThe function can take up to four arguments, all of which are optional:\n\nbefore: A logical value indicating whether the function is being called before or after the code chunk is executed\noptions: The list of chunk options\nenvir: The environment in which the code chunk is executed\nname: The name of the code chunk option that triggered the hook function\n\nThe chunk hook is called for its side effects not the return value. However, if it returns a character output, knitr will add that output to the document output as-is.\nExample: Chunk Timer\n\nCode\ncreate_timer_hook &lt;- function() {\n  start_time &lt;- NULL\n  function(before, options) {\n    if (before) {\n      start_time &lt;&lt;- Sys.time()\n    } else {\n      stop_time &lt;- Sys.time()\n      elapsed &lt;- difftime(stop_time, start_time, units = \"secs\")\n      paste(\n        \"&lt;div style='font-size: 70%; text-align: right'&gt;\",\n        \"Elapsed time:\", \n        round(elapsed, 2), \n        \"secs\",\n        \"&lt;/div&gt;\"\n      )\n    }\n  }\n}\nknitr::knit_hooks$set(timer = create_timer_hook())\n\nThe hook is triggered the first time (with before = TRUE) to record the system time somewhere (e.g., in a variable called start_time). Then, when the hook is triggered the second time (with before = FALSE), it records the system time again (e.g., as stop_time), and computes the difference in time.\n\nUse in a cell\n```{r}\n#| timer: true\nrunif(10000)\n```\nOutput",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#r-and-python",
    "href": "qmd/quarto-rmarkdown.html#r-and-python",
    "title": "Quarto",
    "section": "R and Python",
    "text": "R and Python\n\nIf only R or R and Python, the notebook is rendered by {knitr}\nIf only Python, the notebook is rendered by jupyter\nSet-up\n\n{reticulate} automatically comes loaded in Quarto and it knows to use it when it sees a python block, so you don’t need to load the package\nQuarto will select a version of Python using the Python Launcher on Windows or system PATH on MacOS and Linux. You can override the version of Python used by Quarto by setting the QUARTO_PYTHON environment variable.\n\nIn CLI on Windows, type py is see which version the Python Launcher , and therefore Quarto, is using and py –list to see which versions are installed.\n\n\nR\n```{r}\n#| label: read-data\n#| echo: true\n#| message: false\n#| cache: true\nlemurs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-08-24/lemur_data.csv')\n```\nPython\n```{python}\n#| label: modelling \n#| echo: true \n#| message: false\n\nlemur_data_py = r.lemur_data \nimport statsmodels.api as sm \ny = lemur_data_py[[\"Weight\"]] \nx = lemur_data_py[[\"Age\"]] \nx = sm.add_constant(x) \nmod = sm.OLS(y, x).fit() \nlemur_data_py[\"Predicted\"] = mod.predict(x) \nlemur_data_py[\"Residuals\"] = mod.resid`\n```\n\nUse r. to access the data in the R chunk\nThe first execution of a python cell starts reticulate::repl_python() in the terminal\n\n(back to) R\n```{r}\n#| label: plotting \n#| echo: true \n#| output-location: slide \n#| message: false \n#| fig-align: center \n#| fig-alt: \"Scatter plot of predicted and residual values for the fitted linear model.\" \n\nlibrary(reticulate) \nlibrary(ggplot2) \nlemur_residuals &lt;- py$lemur_data_py \nggplot(data = lemur_residuals, aes(x = Predicted, y = Residuals)) +\n  geom_point(colour = \"#2F4F4F\") +\n  geom_hline(yintercept = 0,\n            colour = \"red\") +\n  theme(panel.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"),\n        plot.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"))\n```\n\nUse py$ to access the data in the Python chunk *\nMust call library(reticulate) in order for Quarto to recognize py$",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#layouts",
    "href": "qmd/quarto-rmarkdown.html#layouts",
    "title": "Quarto",
    "section": "Layouts",
    "text": "Layouts\n\n2 cols (1 col: text, 1 col: image)\n\n::: {layout=\"[50,50]\"}\n\n::: column\nEvery Quarto project starts with a Quarto file that has the extension `.qmd`.\n\n\nThis particular one analyzes children's early words, but every `.qmd` includes the same three basic elements inside:\n\n\n- A block of metadata at the top, between two fences of `---`s. This is written in [YAML](https://learnxinyminutes.com/docs/yaml/). \n- Narrative text, written in [Markdown](https://commonmark.org/help/tutorial/). \n- Code chunks in gray between two fences of ```` ``` ````, written with R or another programming language.\n\n\nYou can use all three elements to develop your code and ideas in one reproducible document.\n:::\n\n![](img/01-source.png)\n:::\n2 figures, 2 columns (i.e. side-by-side) with captions at the top\n---\nfig-cap-location: top\n---\n\n-   Words\n    -   Predictions of Standard RF vs Oblique RF\n\n        ::: {layout-ncol=\"2\"}\n        ![Standard Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-axpred-1.png){fig-align=\"left\" width=\"432\"}\n\n        ![Oblique Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-oblpred-1.png){fig-align=\"left\" width=\"432\"}\n        :::\n\n        -   Words  \n\nfig-cap-location: bottom is default;\nfig-cap-location: margin is buggy, at least in for project type book. Captions are added to the margins but bullet points mysteriously disappear during rendering to html\n\n2 charts side-by-side extending past body margins\n```{r}\n#| label: my-figure\n#| layout-ncol: 2\n#| column: page\nggplot() + ...\nggplot() + ...\n```\n\n“layout-ncol” says 2 side-by-side columns\n“column: page” says extend column width to the width of the page",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#webr",
    "href": "qmd/quarto-rmarkdown.html#webr",
    "title": "Quarto",
    "section": "WebR",
    "text": "WebR\n\nSet-Up\n\nInstall the extension alongside your blog post by running quarto add coatless/quarto-webr\nAdd the extension to your blog by adding filters: [\"webr\"] to your post’s frontmatter\nInstead of {r} code chunks, use {webr-r} ones\n\nInstall CRAN packages on page load\nfilters:\n  - \"webr\"\nwebr:\n  packages:\n  - \"dplyr\"\n  - \"tidyr\"\n  - \"purrr\"\n  - \"tibble\"\n  - \"crayon\"\n\nAdd to frontmatter\n\nInstall R-Universe Package\n```{webr-r}\n#| context: setup\nwebr::install(\"collateral\", repos = c(\"https://jimjam-slam.r-universe.dev\"))\n```\n\nR-Universe packages must be installed in code cells",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/data.table.html",
    "href": "qmd/data.table.html",
    "title": "data.table",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#sec-dt-misc",
    "href": "qmd/data.table.html#sec-dt-misc",
    "title": "data.table",
    "section": "",
    "text": "Syntax\nDT[i, j, by]\n\n##   R:                 i                 j        by\n## SQL:  where | order by   select | update  group by\n\nTake data.table DT, subset rows using i, and manipulate columns with j, grouped according to by.\n\nResources\n\nDocs but it’s difficult to find anything.\n\nThe philosophy of the package is highly dependent on syntax, so the reference page is not very useful in finding out how to perform certain operations as it usually is with other packages.\nThe search doesn’t include the articles which contain a lot of information.\nAlso, it’s an old package, and every old article, changelog, etc. is in the docs. So, if you find something you think answers your question, it may be that that syntax is outdated.\n\nIntroduction to data.table (vignette)\nSyntax Reference (link)\nSymbol Reference (link)\n\nsetDT(df)- Fast conversion of a data frame or list to a data.table without copying\n\nUse when working with larger data sets that take up a considerable amount of RAM (several GBs) because the operation will modify each object in place, conserving memory.\nas.data.table(matrix) should be used for matrices\ndat &lt;- data.table(df) can be used for small datasets but there’s no reason to.\nsetDT(copy(df)) if you want to work with a copy of the df instead of converting the original object.\n\nChaining: see Pivoting &gt;&gt; melt &gt;&gt; Multiple variables stored in column names for an example\nPiping\ndt |&gt; \n   _[, do_stuff(column), by = group] |&gt; \n   _[, do_something_else(othr_col), by = othr_grp]\n\nThe _ placeholder allows you to use R’s native pipe.\nExample\npenguins[species == \"Chinstrap\"] |&gt; \n  _[ , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n# or\npenguins[species == \"Chinstrap\"] |&gt; \n  DT( , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island))\n\nSymbols\n\n.SD is a data.table containing the Subset of DT’s Data for each group, excluding any columns used in by (or keyby). Its usage is still confusing to me.\n:= is the walrus operator. let is an alias. Think it acts like dplyr::mutate or maybe dplyr::summarize. (Docs)\nDT[i, colC := mean(colB), by = colA]\nDT[i,\n   `:=`(colC = sum(colB),\n        colD = sum(colE))\n   by = colF]\nDT[i,\n   let(colC = sum(colB),\n       colD = sum(colE)),\n   by = colF] \n.I is the row index. It’s an integer vector equal to seq_len(nrow(x))\ndt &lt;- data.table(\n  a = 1:3,\n  b = 4:6\n)\ndt[, .(a, b, rowsum = sum(.SD)), by = .I]\n#&gt;        I     a     b rowsum\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1:     1     1     4      5\n#&gt; 2:     2     2     5      7\n#&gt; 3:     3     3     6      9",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#basic-usage",
    "href": "qmd/data.table.html#basic-usage",
    "title": "data.table",
    "section": "Basic Usage",
    "text": "Basic Usage\n\nUsing i\n\nWe can subset rows similar to a data.frame- except you don’t have to use DT$ repetitively since columns within the frame of a data.table are seen as if they are variables.\nWe can also sort a data.table using order(), which internally uses data.table’s fast order for performance.\nWe can do much more in i by keying a data.table, which allows blazing fast subsets and joins. We will see this in the “Keys and fast binary search based subsets” and “Joins and rolling joins” vignette.\n\n\n\nUsing j\n\nSelect columns the data.table way: DT[, .(colA, colB)].\nSelect columns the data.frame way: DT[, c(\"colA\", \"colB\")].\nCompute on columns: DT[, .(sum(colA), mean(colB))].\nProvide names if necessary: DT[, .(sA =sum(colA), mB = mean(colB))].\nCombine with i: DT[colA &gt; value, sum(colB)].\n\n\n\nUsing by\n\nUsing by, we can group by columns by specifying a list of columns or a character vector of column names or even expressions. The flexibility of j, combined with by and i makes for a very powerful syntax.\nby can handle multiple columns and also expressions.\nWe can keyby grouping columns to automatically sort the grouped result.\nWe can use .SD and .SDcols in j to operate on multiple columns using already familiar base functions. Here are some examples:\n\nDT[, lapply(.SD, fun), by = ..., .SDcols = ...] - applies fun to all columns specified in .SDcols while grouping by the columns specified in by.\nDT[, head(.SD, 2), by = ...] - return the first two rows for each group.\nDT[col &gt; val, head(.SD, 1), by = ...] - combine i along with j and by.",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#columns",
    "href": "qmd/data.table.html#columns",
    "title": "data.table",
    "section": "Columns",
    "text": "Columns\n\nRename Columns\nsetnames(DT, \n         old = c(\"SIMD2020v2_Income_Domain_Rank\",\n                 \"SIMD2020_Employment_Domain_Rank\",  \n                 \"SIMD2020_Health_Domain_Rank\",\n                 \"SIMD2020_Education_Domain_Rank\", \n                 \"SIMD2020_Access_Domain_Rank\", \n                 \"SIMD2020_Crime_Domain_Rank\",    \n                 \"SIMD2020_Housing_Domain_Rank\",\n                 \"CP_Name\"),\n\n         new = c(\"Income\", \"Employment\", \n                 \"Health\",   \"Education\",\n                 \"Access\",  \"Crime\", \n                 \"Housing\", \"areaname\"))",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#sec-dt-filter",
    "href": "qmd/data.table.html#sec-dt-filter",
    "title": "data.table",
    "section": "Filtering",
    "text": "Filtering\n\nFast filtering mechanism; reorders rows (increasing) to group by the values in the key columns. Reordered rows make them easier to find and subset.\n\nAll types of columns can be used except list and complex\n\nOperations covered in this section\n\nFiltering\nFilter, select\nFilter, groupby, summarize\nIf-Else\n\nSet Keys - Says order in the increasing direction according to origin and then dest.\nsetkey(flights, origin, dest)\nhead(flights)\n#    year month day dep_delay arr_delay carrier origin dest air_time distance hour\n# 1: 2014     1   2        -2       -25      EV    EWR  ALB      30      143    7\n# 2: 2014     1   3        88        79      EV    EWR  ALB      29      143   23\n# 3: 2014     1   4       220       211      EV    EWR  ALB      32      143   15\n# 4: 2014     1   4        35        19      EV    EWR  ALB      32      143    7\n# 5: 2014     1   5        47        42      EV    EWR  ALB      26      143    8\n# 6: 2014     1   5        66        62      EV    EWR  ALB      31      143   23\nFilter by origin == “JFK” and dest == “MIA”\nflights[.(\"JFK\", \"MIA\")]\n#      year month day dep_delay arr_delay carrier origin dest air_time distance hour\n#    1: 2014    1   1        -1       -17      AA    JFK  MIA      161    1089   15\n#    2: 2014    1   1         7        -8      AA    JFK  MIA      166    1089    9\n#    3: 2014    1   1         2        -1      AA    JFK  MIA      164    1089   12\n#    4: 2014    1   1         6         3      AA    JFK  MIA      157    1089    5\n#    5: 2014    1   1         6       -12      AA    JFK  MIA      154    1089   17\n#  ---                                                                             \n# 2746: 2014   10  31        -1       -22      AA    JFK  MIA      148    1089   16\n# 2747: 2014   10  31        -3       -20      AA    JFK  MIA      146    1089    8\n# 2748: 2014   10  31         2       -17      AA    JFK  MIA      150    1089    6\n# 2749: 2014   10  31        -3       -12      AA    JFK  MIA      150    1089    5\n# 2750: 2014   10  31        29         4      AA    JFK  MIA      146    1089   19\nFilter by only the first key column (origin): flights[\"JFK\"]\nFilter by only the second key column (dest)\nflights[.(unique(), \"MIA\")]\n#      year month day dep_delay arr_delay carrier origin dest air_time distance hour\n#    1: 2014    1   1        -5       -17      AA    EWR  MIA      161    1085   16\n#    2: 2014    1   1        -3       -10      AA    EWR  MIA      154    1085    6\n#    3: 2014    1   1        -5        -8      AA    EWR  MIA      157    1085   11\n#    4: 2014    1   1        43        42      UA    EWR  MIA      155    1085   15\n#    5: 2014    1   1        60        49      UA    EWR  MIA      162    1085   21\n#  ---                                                                             \n# 9924: 2014   10  31       -11        -8      AA    LGA  MIA      157    1096   13\n# 9925: 2014   10  31        -5       -11      AA    LGA  MIA      150    1096    9\n# 9926: 2014   10  31        -2        10      AA    LGA  MIA      156    1096    6\n# 9927: 2014   10  31        -2       -16      AA    LGA  MIA      156    1096   19\n# 9928: 2014   10  31         1       -11      US    LGA  MIA      164    1096   15\nFilter by origin and dest values, then select a arr.delay column: flights[.(\"LGA\", \"TPA\"), .(arr_delay)]\nFilter by origin and dest values, then summarize and pull maximum of arr_delay\nflights[.(\"LGA\", \"TPA\"), max(arr_delay)]\n# [1] 486\nFilter by origin value, group_by month, summarize( max(dep_delay))\nans &lt;- flights[\"JFK\", max(dep_delay), keyby = month]\nhead(ans)\n#    month  V1\n# 1:    1  881\n# 2:    2 1014\n# 3:    3  920\n# 4:    4 1241\n# 5:    5  853\n# 6:    6  798\nkey(ans)\n# [1] \"month\"\n\nkeyby groups and sets the key to month\n\nFilter by three origin values, one dest value, return the last row for each match\nflights[.(c(\"LGA\", \"JFK\", \"EWR\"), \"XNA\"), mult = \"last\"]\n#    year month day dep_delay arr_delay carrier origin dest air_time distance hour\n# 1: 2014     5  23       163       148      MQ    LGA  XNA      158    1147  18\n# 2:   NA    NA  NA        NA        NA      NA    JFK  XNA       NA      NA  NA\n# 3: 2014     2   3       231       268      EV    EWR  XNA      184    1131  12\n\nFiltering by more than one key value returns combinations of the first key and second key\nRemember setting a key reorders (increasing)",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#joins",
    "href": "qmd/data.table.html#joins",
    "title": "data.table",
    "section": "Joins",
    "text": "Joins\n\nLeft Equal Join\nDT &lt;- lookup[DT, on = .(DataZone = Data_Zone)]\nDT &lt;- merge(lookup, DT, by.x = \"DataZone\", by.y = \"Data_Zone\")\n\nDT: A datatable where the id column is Data_Zone\nlookup: A datatable where the id column is DataZone\nBoth datatables have the same number of rows so that makes this an Equal Join\nDT is joined to lookup, so the columns of lookup appear first (farthest left) then DT’s columns (farthest right) of the joined datatable.\nSubset Notation: The output datatable has the id column, DataZone, which is from lookup but the rows are ordered the same way as the input table, DT.\n\nIt’s weird that the output’s rows are ordered according to the input datatable\n\nmerge: The output datatable has the id column, DataZone, which is from lookup, and the rows are ordered according to lookup\nThe subset way is the “data.table” way, because you perform calculations on the output using the j position whereas with merge, it would require a chain or an extra line of code. But if the order of rows of the output matters, then I can’t find a way to reproduce the merge ordering using the subset method.",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#sec-dt-cond",
    "href": "qmd/data.table.html#sec-dt-cond",
    "title": "data.table",
    "section": "Conditionals",
    "text": "Conditionals\n\nIfelse using hour\nsetkey(flights, hour) # hour has values 0-24\nflights[.(24), hour := 0L]\n\nifelse(hour == 24, 0, TRUE)\nConsequence: since a key column value has changed, hour is no longer a key",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#sec-dt-pivot",
    "href": "qmd/data.table.html#sec-dt-pivot",
    "title": "data.table",
    "section": "Pivoting",
    "text": "Pivoting\n\npivot_longer and melt\n\nBasic\nrelig_income |&gt;\n  pivot_longer(!religion, # keep religion as a column\n              names_to = \"income\", # desired name for new column\n              values_to = \"count\") # what data goes into the new column?\nmelt(DT, id.vars = \"religion\",\n    variable.name = \"income\",\n    value.name = \"count\",\n    variable.factor = FALSE) # added to keep output consistent with tidyr\nColumns have a common prefix and missing values are dropped\nbillboard |&gt;\n  pivot_longer(\n    cols = starts_with(\"wk\"),\n    names_to = \"week\",\n    names_prefix = \"wk\",\n    values_to = \"rank\",\n    values_drop_na = TRUE\n  )\nmelt(DT,\n    measure.vars = patterns(\"^wk\"),\n    variable.name = \"week\",\n    value.name = \"rank\",\n    na.rm = TRUE)\nMultiple variables stored in column names\nwho &lt;- data.table(id = 1, new_sp_m5564 = 2, newrel_f65 = 3)\n#         id new_sp_m5564 newrel_f65\n#      &lt;num&gt;        &lt;num&gt;      &lt;num&gt;\n#   1:     1            2          3\n\nmelt(who,\n     measure.vars = measure(diagnosis,\n                            gender,\n                            ages,\n                            pattern = \"new_?(.*)_(.)(.*)\"))\n#       id diagnosis gender   ages value\n#    &lt;num&gt;    &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt;\n# 1:     1        sp      m   5564     2\n# 2:     1       rel      f     65     3\n\n# with tidyr \nwho |&gt; \n  tidyr::pivot_longer(\n    cols = !id,\n    names_to = c(\"diagnosis\", \"gender\", \"age\"),\n    names_pattern = \"new_?(.*)_(.)(.*)\",\n    values_to = \"count\")\n# # A tibble: 2 × 5\n#           id diagnosis gender age   count\n#        &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n# 1          1 sp        m      5564      2\n# 2          1 rel       f      65        3\n\ntstrsplit is DT’s tidyr::separate\n\nMatrix to long\nanscombe |&gt;\n  pivot_longer(\n    everything(),\n    cols_vary = \"slowest\",\n    names_to = c(\".value\", \"set\"),\n    names_pattern = \"(.)(.)\" \n  )\nDT[,melt(.SD,\n            variable.name = \"set\",\n            value.name = c(\"x\",\"y\"),\n            variable.factor = FALSE,\n            measure.vars = patterns(\"^x\",\"^y\"))]\n\n\n\npivot_wider and dcast\n\nData in examples\n\nfish_encounters\n## # A tibble: 114 × 3\n##    fish  station  seen\n##    &lt;fct&gt; &lt;fct&gt;    &lt;int&gt;\n##  1 4842  Release     1\n##  2 4842  I80_1       1\n##  3 4842  Lisbon      1\n##  4 4842  Rstr        1\n##  5 4842  Base_TD     1\n##  6 4842  BCE         1\n##  7 4842  BCW         1\n##  8 4842  BCE2        1\n##  9 4842  BCW2        1\n## 10 4842  MAE         1\n## # … with 104 more rows\n\nBasic\nfish_encounters |&gt;\n  pivot_wider(names_from = station, values_from = seen)\n\ndcast(DT, fish ~ station, value.var = \"seen\")\nFill in missing values\nfish_encounters |&gt;\n  pivot_wider(names_from = station, values_from = seen, values_fill = 0)\n\ndcast(DT, fish ~ station, value.var = \"seen\", fill = 0)\n# alt\nDT[, dcast(.SD, fish ~ station, value.var = \"seen\", fill = 0)]\n\nRather than have the DT inside dcast, we can use .SD and have dcast inside DT, which is helpful for further chaining. (see applied to melt above)\n\nGenerate column names from multiple variables\nus_rent_income |&gt;\n  pivot_wider(\n    names_from = variable,\n    values_from = c(estimate, moe)\n  )\n\ndcast(DT, GEOID + NAME ~ variable, \n          value.var = c(\"estimate\",\"moe\"))\n# alt\ndcast(DT, ... ~ variable, \n      value.var = c(\"estimate\",\"moe\"))\n\nAlternative: pass “…” to indicate all other unspecified columns\n\nSpecify a different names separator\nus_rent_income |&gt;\n  pivot_wider(\n    names_from = variable,\n    names_sep = \".\",\n    values_from = c(estimate, moe)\n  )\n\ndcast(DT, GEOID + NAME ~ variable,\n      value.var = c(\"estimate\",\"moe\"), \n      sep = \".\")\n# alt\nDT[, dcast(.SD, GEOID + NAME ~ variable,\n    value.var = c(\"estimate\",\"moe\"), \n          sep = \".\")]\n\nAlternative: Rather than have the DT inside dcast, we can use .SD and have dcast inside DT, which is helpful for further chaining. (see applied to melt above)\n\nControlling how column names are combined\nus_rent_income |&gt;\n  pivot_wider(\n    names_from = variable,\n    values_from = c(estimate, moe),\n    names_vary = \"slowest\"\n  ) |&gt; names()\n\nDT[, dcast(.SD, GEOID + NAME ~ variable,\n          value.var = c(\"estimate\",\"moe\"))\n  ][,c(1:3,5,4,6)] |&gt; names()\n\n## [1] \"GEOID\"          \"NAME\"            \"estimate_income\" \"moe_income\"     \n## [5] \"estimate_rent\"  \"moe_rent\"\n\nSee {tidyr::pivot_wider} docs and the names_vary arg\n\nAggregation\nwarpbreaks %&gt;%\n  pivot_wider(\n    names_from = wool,\n    values_from = breaks,\n    values_fn = mean\n  )\ndcast(DT, tension ~ wool, \n          value.var = \"breaks\", fun = mean)\n# alt\nDT[, dcast(.SD, tension ~ wool, \n      value.var = \"breaks\", fun = mean)]\n\n## # A tibble: 3 × 3\n##  tension    A    B\n##  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 L        44.6  28.2\n## 2 M        24    28.8\n## 3 H        24.6  18.8\n\nAlternative: Rather than have the DT inside dcast, we can use .SD and have dcast inside DT, which is helpful for further chaining. (see applied to melt above)",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#tidyr",
    "href": "qmd/data.table.html#tidyr",
    "title": "data.table",
    "section": "tidyr",
    "text": "tidyr\n\nseparate via tstrsplit\ndt &lt;- data.table(x = c(\"00531725 Male 2021 Neg\", \"07640613 Female 2020 Pos\"))\n#                           x\n#                      &lt;char&gt;\n# 1:   00531725 Male 2021 Neg\n# 2: 07640613 Female 2020 Pos\n\ncols &lt;- c(\"personID\", \"gender\", \"year\", \"covidTest\")\n\ndt[, tstrsplit(x,\n               split = \" \",\n               names = cols,\n               type.convert = TRUE)]\n#    personID gender  year covidTest\n#       &lt;int&gt; &lt;char&gt; &lt;int&gt;    &lt;char&gt;\n# 1:   531725   Male  2021       Neg\n# 2:  7640613 Female  2020       Pos\n\n\ndt[, tstrsplit(x,\n               split = \" \",\n               names = cols,\n               type.convert = list(as.character = 1,\n                                   as.factor = c(2, 4),\n                                   as.integer = 3)\n               )]\n#    personID gender   year covidTest\n#      &lt;char&gt; &lt;fctr&gt;  &lt;int&gt;    &lt;fctr&gt;\n# 1: 00531725   Male   2021       Neg\n# 2: 07640613 Female   2020       Pos",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#user-defined-functions",
    "href": "qmd/data.table.html#user-defined-functions",
    "title": "data.table",
    "section": "User Defined Functions",
    "text": "User Defined Functions\n\nenv\n\niris_dt &lt;- as.data.table(iris)\nsquare = function(x) x^2\n\niris_dt[filter_col %in% filter_val,\n        .(var1, var2, out = outer(inner(var1) + inner(var2))),\n        by = by_col,\n        env = list(\n          outer = \"sqrt\",\n          inner = \"square\",\n          var1 = \"Sepal.Length\",\n          var2 = \"Sepal.Width\",\n          out = \"Sepal.Hypotenuse\",\n          filter_col = \"Species\",\n          filter_val = I(\"versicolor\"),\n          by_col =  \"Species\"\n        )] |&gt; \n  head(n = 3)\n#       Species Sepal.Length Sepal.Width Sepal.Hypotenuse\n#        &lt;fctr&gt;        &lt;num&gt;       &lt;num&gt;            &lt;num&gt;\n# 1: versicolor          7.0         3.2         7.696753\n# 2: versicolor          6.4         3.2         7.155418\n# 3: versicolor          6.9         3.1         7.564390\n\nVariables are included in the standard i, j, and by syntax\nenv contains the (quoted) variable values\n\ni.e. argument values in the typical R udf syntax (function(x = val1))\nCan use other UDFs as values which is demonstrated by inner = “square”",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#sec-dt-rec",
    "href": "qmd/data.table.html#sec-dt-rec",
    "title": "data.table",
    "section": "Recipes",
    "text": "Recipes\n\nOperations covered in this section\n\ngroup_by, summarize (and arrange)\ncrosstab\n\ngroup_by, summarize (and arrange)\ndt_res &lt;- dtstudy[, .(n = .N, avg = round(mean(y), 1)), keyby = .(male, over65, rx)]\n\ntb_study &lt;- tibble::as_tibble(dtstudy)\ntb_res &lt;- tb_study |&gt;\n  summarize(n = n(),\n            avg = round(mean(y), 1),\n            .by = c(male, over65, rx)) |&gt;\n  arrange(male, over65, rx)\n\ndt automatically orders by the grouping variables, so to get the exact output, you have to add an arrange\n\nCrosstab using cube (Titanic5 dataset)\n# Note that the mean of a 0/1 variable is the proportion of 1s\nmn &lt;- function(x) mean(x, na.rm=TRUE)\n# Create a function that counts the number of non-NA values\nNna &lt;- function(x) sum(! is.na(x))\n\ncube(d, .(Proportion=mn(survived), N=Nna(survived)), by=.q(sex, class), id=TRUE)\n\n#&gt;     grouping    sex class Proportion    N\n#&gt; 1:         0 female     1  0.9652778  144\n#&gt; 2:         0   male     1  0.3444444  180\n#&gt; 3:         0   male     2  0.1411765  170\n#&gt; 4:         0 female     2  0.8867925  106\n#&gt; 5:         0   male     3  0.1521298  493\n#&gt; 6:         0 female     3  0.4907407  216\n#&gt; 7:         1 female    NA  0.7274678  466\n#&gt; 8:         1   male    NA  0.1909846  843\n#&gt; 9:         2   &lt;NA&gt;     1  0.6203704  324\n#&gt; 10:        2   &lt;NA&gt;     2  0.4275362  276\n#&gt; 11:        2   &lt;NA&gt;     3  0.2552891  709\n#&gt; 12:        3   &lt;NA&gt;    NA  0.3819710 1309",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html",
    "href": "qmd/db-dbt.html",
    "title": "5  dbt",
    "section": "",
    "text": "5.1 Misc",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-misc",
    "href": "qmd/db-dbt.html#sec-db-dbt-misc",
    "title": "5  dbt",
    "section": "",
    "text": "List of available DB Adaptors\n\nRuns on Python, so adaptors are installed via pip Notes from\nAnatomy of a dbt project\nWhat is dbt? Docs\n\nResources\n\nCreate a Local dbt Project\n\nUses docker containers to set up a local dbt project and a local postgres db to play around with\n\n\nArchitecture\n\nTypical Workflow\ndbt deps\ndbt seed\ndbt snapshot\ndbt run\ndbt run-operation {{ macro_name }}\ndbt test\nStyle Guide Components\n\nNaming conventions (the case to use and tense of the column names)\nSQL best practices (commenting code, CTEs, subqueries, etc.)\nDocumentation standards for your models\nData types of date, timestamp, and currency columns\nTimezone standards for all dates\n\nCastor - tool that takes your project and autofills much of the documentation\n\n\nHas a free tier\nVery helpful if you have the same column name in multiple datasets, you don’t have to keep defining it\nTribal Knowledge\n\nWhen a dataset is discussed in a team slack channel, Castor pulls the comments and adds them to the documentation of the dataset\n\n\nLightdash - BI tool for dbt projects - free tier for self hosting",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-setup",
    "href": "qmd/db-dbt.html#sec-db-dbt-setup",
    "title": "5  dbt",
    "section": "5.2 Set-Up",
    "text": "5.2 Set-Up\n\nBasic set-up: Article\n\nExample uses postgres adaptor\n\nWithin a python virtual environment\n\nCreate: python3 -m vevn dbt-venv\nActivate: source dbt-venv/bin/activate\n\nShould be able to see a (dbt-venv) prefix in every line on the terminal\n\nInstall dbt-core: pip install dbt-core\n\nSpecific version: pip install dbt-core==1.3.0\nConfirm installation by checking version: dbt --version\n\nInstall plugins\n\n\npip install dbt-bigquery\npip install dbt-spark\n# etc...",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-desc",
    "href": "qmd/db-dbt.html#sec-db-dbt-desc",
    "title": "5  dbt",
    "section": "5.3 Description",
    "text": "5.3 Description\n\nBuilt for data modeling\n\nmodels are like sql queries\n\nModularizes SQL code and makes it reusable across “models”\n\nRunning the orders “model” also runs the base_orders model and base_payments model (i.e. dependencies for orders)\n\nNot sure this exactly right. Seems like doing this would result in wasting time rerunning the same dependencies multiple times\n\nbase_orders and base_payments are independent in that they can also be used in other models\nCreates more dependable code because you’re using the same logic in all your models\nMakes runs faster since you aren’t wasting time and resources running the same blocks of code over and over again\nYou can schedule running sets of models by tagging them (e.g. #daily, #weekly)\nVersion Control\n\nsnapshots provide mechanism for versioning datasets\nWithin every yaml file is an option to include the version\n\npackage add-ons that allow you to interact with spark, snowflake, duckdb, redshift, etc.\nDocumentation for every step of the way\n\n.yml files can be used to generate a website (localhost:8080) around all of your dbt documentation.\n\ndbt docs generate\ndbt docs serve\n\nCurrent understanding\n\nData is brought in from warehouses via base models and basic transformations are performed (models &gt;&gt; staging directory)\nThen the data is transformed to the desired state via intermediate models and calculations performed (models &gt;&gt; marts directory)\nThen the final product is stored in the data directory",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-opt",
    "href": "qmd/db-dbt.html#sec-db-dbt-opt",
    "title": "5  dbt",
    "section": "5.4 Optimizations",
    "text": "5.4 Optimizations\n\nusing an M1, the new Apple laptops with Apple’s own CPUs improved speed by 3x\nupgrading from dbt 0.15.0 -&gt; 0.20.0 resulted in another 3x speed increase\nmoving dbt out of a container resulted in a 2x speed increase for those using a Intel CPU MacBook Pro\nRuns parallelized\n\nModels that have dependencies aren’t run until their upstream models are completed but models that don’t depend on one another are run at the same time.\nthread  parameter in your dbt_project.yml specifies how many models are permitted to run in parallel\n\nBigQuery",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-comp",
    "href": "qmd/db-dbt.html#sec-db-dbt-comp",
    "title": "5  dbt",
    "section": "5.5 Components",
    "text": "5.5 Components\n\nProject Templates\n\nStyle Guide\n\nMore detailed: link\n\nExample Starter Project\n\n\nprofiles.yml\n\nNot included in project directory\nOnly have to worry about this file if you set up dbt locally.\ndoc\nCreated by dbt init in ~/.dbt/\nContents\n\ndatabase connection, database credentials that dbt will use to connect to the data warehouse\nIf you work on multiple projects locally, the different project names (configured in the dbt_project.yml file) will allow you to set up various profiles for other projects.\n… something about “targets” but not sure what this is or how it’s used\n\n\ndbt_project.yml\n\ndoc\nmain configuration file for your project\nFields where you need to change default values to your project name:\n\nProject Name, Profile Name\n\nVariables\n\nDefined in the project yaml and used in models\n\nAccessed using var\n\nExample: Assigning States to Regions\n\nDefine variables in dbt_project.yml\nvars:\n  state_lookup:\n    Northeast:\n      - CT\n      - ME\n    Midwest:\n      - IL\n      - IN\n\n\n\n\nUsing the variables in a model\n{# Option 1 #}\nSELECT state,\n      CASE {% for k, v in var(\"state_lookup\").items() %}\n            WHEN state in ({% for t in v %}'{{ t }}'{% if not loop.last %}, {% endif %}{% endfor %}) THEN {{ k }}{% endfor %}\n            ELSE NULL END AS region\n  FROM {{ ref('my_table') }}\n\n{# Option 2 #}\nSELECT state,\n      CASE {% for k, v in var(\"state_lookup\").items() %}\n            WHEN state in ({{ t|csl }}) THEN {{ k }}{% endfor %}\n            ELSE NULL END AS region\n  FROM {{ ref('my_table') }}\n\nThis is a complicated example, see docs for something simpler\n{% ... %} are used to encapsulate for-loops and if-then conditions, see docs\n\n{# ... #} is for comments\n\nOption 2 uses a csl filter (comma-separated-list)\n\nModels section\n\nmy_new_project\n\nModels Misc\n\nKey Components of a Well-Written Data Model\n\nModularity where possible\nSame as the functional mindset: “if there’s any code that’s continually repeated, then it should be a function(i.e. its own separate model in dbt).”\nReadability\nComment\nUse CTEs instead of subqueries\nUse descriptive names\n\nExample: if you are joining the tables “users” and “addresses” in a CTE, you would want to name it “users_joined_addresses” instead of “user_addresses”\n\n\nExample: Comments, CTE, Descriptive Naming\nWITH\nActive_users AS (\n  SELECT\n    Name AS user_name,\n    Email AS user_email,\n    Phone AS user_phone,\n    Subscription_id\n  FROM users\n  --- status of 1 means a subscription is active\n  WHERE subscription_status = 1\n),\nActive_users_joined_subscriptions AS (\n  SELECT\n    Active_users.user_name,\n    active_users.user_email,\n    Subscriptions.subscription_id,\n    subscriptions.start_date ,\n    subscriptions.subscription_length\n  FROM active_users\n  LEFT JOIN subscriptions\n    ON active_users.subscription_id = subscriptions.subscription_id\n)\nSELECT * FROM Active_users_joined_subscriptions\n\n\nModel categories: staging, marts, base/intermediate\n* Staging\n    * Contains all the individual components of your project that the other layers will use in order to craft more complex data models.\n    * Each model bears a one-to-one relationship with the source data table it represents (i.e. 1 staging model per source)\n    * Typical Transformations: recasting, column renaming, basic computations (such as KBs to MBs or GBs), categorization (e.g. using CASE WHEN statements).\n        * Aggregations and joins should also be avoided\n    * Usually materialized as views.\n        * Allows any intermediate or mart models referencing the staging layer to get access to fresh data and at the same time it saves us space and reduces costs.\n    * Example\n        * Both the Stripe and Braintree payments are recast into a consistent shape, with consistent column names.\n* Marts\n    * Where everything comes together in a way that business-defined entities and processes are constructed and made readily available to end users via dashboards or applications.\n    * Since this layer contains models that are being accessed by end users it means that performance matters. Therefore, it makes sense to materialize them as tables.\n        * If a table takes too much time to be created (or perhaps it costs too much), then you may also need to consider configuring it as an incremental model.\n    * A mart model should be relatively simple and therefore, too many joins should be avoided\n    * Example\n        * A monthly recurring revenue (MRR) model that classifies revenue per customer per month as new revenue, upgrades, downgrades, and churn, to understand how a business is performing over time.\n            * It may be useful to note whether the revenue was collected via Stripe or Braintree, but they are not fundamentally separate models.\n    Base/Intermediate\n        Base\n            basic transformations (e.g. cleaning up the names of the columns, casting to different data types)\n            \n            Other models use these models as data sources\n                prevents errors like accidentally casting your dates to two different types of timestamps, or giving the same column two different names.\n                    two different timestamp castings can cause all of the dates to be improperly joined downstream, turning the model into a huge disaster\n                    \n            Usually occuring in staging\n            \n            read directly from a source, which is typically a schema in your data warehouse\n                Source object\n                    `{{ source('campaigns', 'channel'){style='color: goldenrod'}[}}]{style='color: goldenrod'}`\n                    \n                * campaigns is the name of the source in the .yml file\n                * channel is the name of a table from that source\n        Intermediate\n            Brings together the atomic building blocks that reside on staging layer such that more complex and meaningful models are constructed\n            \n            Usually occuring in marts\n            \n            Additional transformations that particular marts-models require\n            * Created to isolate complex operations\n                Typically used for joins between multiple base models\n                \n        * Should not be directly exposed to end users via dashboards or applications\n        * Other models should reference them as Common Table Expressions although there may be cases where it makes sense to materialize them as Views\n            * Macros called via `run-operation` cannot reference ephemeral objects such as CTEs\n            * Recommended to start with ephemeral objects unless this doesn’t work for the specific use case\n            * Whenever you decide to materialize them as Views, it may be easier to to do so in a [custom schema](https://docs.getdbt.com/docs/build/custom-schemas), that is a schema outside of the main schema defined in your dbt profile.\n        * If the same intermediate model is referenced by more than one model then it means your design has probably gone wrong.\n            * Usually indicates that you should consider turning your intermediate model into a macro.\n            reference the base models rather than from a source\n            * Reference object\n                * `{{ ref('base_campaign_types'){style='color: goldenrod'}[}}]{style='color: goldenrod'}`\n                * base\\_campaign\\_types is a base model\n\nTagging\n\nAllows you to run groups of models\n\nExample dbt run --models tag:daily\n\n\nDirectories models Sources (i.e. data sources) are defined in src_ .yml files in your models directory * .yml files contain definitions and tests * .doc files contain source documentation * Models (i.e. sql queries) are defined stg__yml * .yml files contain definitions and tests * .doc files contain source documentation * The actual models are the .sql files Example  * staging: * Different data sources will have separate folders underneath staging (e.g. stripe). * marts: * Use cases or departments have different folders underneath marts (e.g. core or marketing) data contains all manual data that will be loaded to the database by dbt.\n      To load the .csv files in this folder to the database, you will have to run the `dbt seed` command.\n\n      For github or other repos, do not put large files or files with sensitive information here\n          acceptable use cases: yearly budget, status mappings, category mappings, etc\n\nsnapshots\n\ncaptures of the state of a table at a particular time\ndoc\nbuild a slowly changing dimension (SCD) table for sources that do not support change data capture (CDC)\nExample\n\nEvery time the status of an order change, your system overrides it with the new information. In this case, there we cannot know what historical statuses that an order had.\nDaily snapshots of this table builds a history and allows you to track order statuses\n\n\n\nMacros\n\nsimilar to functions in excel\ndefine custom functions in the macros folder or override default macros and macros from a package\nSee bkmks for tutorials on writing custom macros with jinja\n{dbtplyr} macros\n\ndplyr tidy selectors, across, etc.\n\n\nSeeds\n\nSeeds are csv files that you add to your dbt project to be uploaded to your data warehouse.\n\nUploaded into your data warehouse using the dbt seed command\n\nBest suited to static data which changes infrequently.\n\nExamples of use cases:\n\nA list of unique codes or employee ids that you may need in your analysis but is not present in your current data.\nA list of mappings of country codes to country names\nA list of test emails to exclude from analysis\n\n\nReferenced in downstream models the same way as referencing models — by using the ref function",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-pkgs",
    "href": "qmd/db-dbt.html#sec-db-dbt-pkgs",
    "title": "5  dbt",
    "section": "5.6 Packages",
    "text": "5.6 Packages\n\npackages.yml\n\nlist of external dbt packages you want to use in your project\n\navailable packages: link\nformat\n\n        packages:\n            - package: dbt-labs/dbt_utils\n              version: 0.7.3\n\nInstall packages - dbt deps\nsome of the packages (there are a lot of packages and I didn’t get through them all)\n\n{audit-helper}\n\ncompares columns, queries; useful if refactoring code or migrating db\n\n{codegen}\n\ngenerate base model, barebones model and source .ymls\n\n{dbt-athena} - adaptor for AWS Athena\n{dbt-expectations}\n\ndata validation based on great expectations py lib\n\n{dbt-utils}\n\nton of stuff for tests, queries, etc.\n\n{dbtplyr} macros\n\ndplyr tidy selectors, across, etc.\n\n{dbt-duckdb} - adapter for duckdb\n{external-tables}\n\ncreate/replace/refresh external tables\nGuessing this means any data source (e.g. s3, spark, google, another db like snowflake, etc.) that isn’t the primary db connected to the dbt project\n\n{logging}\n\nprovides out-of-the-box functionality to log events for all dbt invocations, including run start, run end, model start, and model end.\ncan slow down runs substantially\n\n{re_data}\n\ndashboard for monitoring, macros, models\n\n{profiler}\n\nimplements dbt macros for profiling database relations and creating doc blocks and table schemas (schema.yml) containing said profiles\n\n{spark-utils}\n\nenables use of (most of) the {dbt-utils} macros on spark",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-dvaut",
    "href": "qmd/db-dbt.html#sec-db-dbt-dvaut",
    "title": "5  dbt",
    "section": "5.7 Data Validation and Unit Tests",
    "text": "5.7 Data Validation and Unit Tests\n\n\nBuilt-in support for CI/CD pipelines to test your “models” and stage them before committing to production\n\nMisc\n\nSee also\n\nDocs\npackages (see above)\nHow to do Unit Testing in dbt\n\n\n\nMost of the tests are defined in a models-type .yml file in the models directory\nUses pre-made or custom macros\n\ncustom (aka singular) tests should be located in a tests folder.\n\ndbt will evaluate the SQL statement.\nThe test will pass if no row is returned and failed if at least one or more rows are returned.\nUseful for testing for some obscurity in the data\nExample: Check for duplicate rows when joining two tables\n\n\n\nselect \na.id \nfrom {{ ref(‘table_a’) }} a \nleft join {{ ref(‘table_b’) }} b \non a.b_id = b.id \ngroup by a.id \nhaving count(b.id)&gt;1\n\ni.e. If I join table a with table b, there should only be one record for each unique id in table a\nProcess\n\njoin the tables on their common field\ngroup them by the id that should be distinct\ncount the number of duplicates created from the join.\n\nThis tells me that something is wrong with the data.\nadd a having clause to filter out the non-dups\n\n\nCan be applied to a model or a column\nRun test - dbt test\nMock data\n\nData used for unit testing SQL code\nTo ensure completeness, it’s best if analysts or business stakeholders are the ones provide test cases or test data\nStore in the “data” folder (typically .csv files)\n\neach CSV file represents one source table\nshould be stored in a separate schema (e.g. unit_testing) from production data\ndbt seed (see below, Other &gt;&gt; seeds) command is used to load mock data into the data warehouse\n\n\nTests\n\nfreshness (docs) - used to define the acceptable amount of time between the most recent record, and now, for a table\n\nExample\n\n\n\nsources:\n  - name: users\n    freshness:\n      warn_after:\n        count: 3\n        period: day\n      error_after:\n        count: 5\n        period: day\n\nExample\nExample: Unit Test\n\nFig shows the mock data (.csv) files\nAdd test to dbt_project.yml\n\n\nseeds:\n  unit_testing:\n    revenue:\n      schema: unit_testing\n      +tags:\n        - unit_testing\n\nEvery file in the unit_testing/revenue folder will be loaded into unit_testing\nExecuting dbt build -s +tag:unit_testing will run all the seeds/models/tests/snapshots with tag unit_testing and their upstreams\nCreate macro that switches the source data in the model being tested from production data (i.e. using { source() } ) to mock data (i.e. using ref ) when a unit test is being run\n\n{% macro select_table(source_table, test_table) %}\n      {% if var('unit_testing', false) == true %}\n\n            {{ return(test_table) }}\n      {% else %}\n            {{ return(source_table) }}\n      {% endif %}\n{% endmacro %}\n\nArticle calls this file “select_table.sql”\n2 inputs: “source_table” (production data) and “test_table” (mock data)\nmacro returns the appropriate table based on the variable in the dbt command\n\nIf the command doesn’t provide unit_testing variable or the value is false , then it returns source_table , otherwise it returns test_table.\n\nAdd macro code chunk to model\n\n{{ config\n    (\n        materialized='table',\n        tags=['revenue']\n    )\n}}\n{% set import_transaction = select_table(source('user_xiaoxu','transaction'), ref('revenue_transaction')) %}\n{% set import_vat = select_table(source('user_xiaoxu','vat'), ref('revenue_vat')) %}\nSELECT\n    date\n    , city_name\n    , SUM(amount_net_booking) AS amount_net_booking\n    , SUM(amount_net_booking * (1 - 1/(1 + vat_rate)))  AS amount_vat\nFROM {{ import_transaction }}\nLEFT JOIN {{ import_vat }} USING (city_name)\nGROUP BY 1,2\n\nInside the {%...%} , the macro “select_table” is called to set the local variables, “import_transaction” and “import_vat” which are later used in the model query\nModel file is named “revenue2.sql”\nRun model and test using mock data: dbt build -s +tag:unit_testing --vars 'unit_testing: true'\n\nRun model with production data (aka source data): dbt build -s +tag:revenue --exclude tag:unit_testing\n\nCompare output\n\nversion: 2\nmodels:\n  - name: revenue\n    meta:\n      owner: \"@xiaoxu\"\n    tests:\n      - dbt_utils.equality:\n          compare_model: ref('revenue_expected')\n          tags: ['unit_testing']\n\nmodel properties file that’s named “revenue.yml” in the models directory\nBy including tags: ['unit_testing'] we can insure that we don’t run this test in production (see build code above with --exclude tag:unit_testing\nMacro for comparing numeric output\n\n{% test advanced_equality(model, compare_model, round_columns=None) %}\n{% set compare_columns = adapter.get_columns_in_relation(model) | map(attribute='quoted') %}\n{% set compare_cols_csv = compare_columns | join(', ') %}\n{% if round_columns %}\n    {% set round_columns_enriched = [] %}\n    {% for col in round_columns %}\n        {% do round_columns_enriched.append('round('+col+')') %}\n    {% endfor %}\n    {% set selected_columns = '* except(' + round_columns|join(', ') + \"), \" + round_columns_enriched|join(', ') %}\n{% else %}\n    {% set round_columns_csv = None %}\n    {% set selected_columns = '*' %}\n{% endif %}\nwith a as (\n    select {{compare_cols_csv}} from {{ model }}\n),\nb as (\n    select {{compare_cols_csv}} from {{ compare_model }}\n),\na_minus_b as (\n    select {{ selected_columns }} from a\n    {{ dbt_utils.except() }}\n    select {{ selected_columns }} from b\n),\nb_minus_a as (\n    select {{ selected_columns }} from b\n    {{ dbt_utils.except() }}\n    select {{ selected_columns }} from a\n),\nunioned as (\n    select 'in_actual_not_in_expected' as which_diff, a_minus_b.* from a_minus_b\n    union all\n    select 'in_expected_not_in_actual' as which_diff, b_minus_a.* from b_minus_a\n)\nselect * from unioned\n{% endtest %}\n\nFile called “advanced_equality.sql”",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html",
    "href": "qmd/geospatial-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-misc",
    "href": "qmd/geospatial-general.html#sec-geo-gen-misc",
    "title": "General",
    "section": "",
    "text": "QGIS - free and open source\nArcGIS - expensive and industry-standard\nspatiotemporal data — data cubes with spatial and regular temporal dimensions — such as\n\ne.g. gridded temperature values (raster time series) and vector data with temporal records at regular temporal instances (e.g. election results in states).\n\n{stars} - regular intervals\n{sftime} - irregular intervals\n\n\nSpatial Resampling\n\nCreates cross-validation folds by k-means clustering coordinate variables\nlibrary(tidymodels)\nlibrary(spatialsample)\nset.seed(123)\nspatial_splits &lt;- spatial_clustering_cv(landslides, coords = c(\"x\", \"y\"), v = 5)\n\n# fit a logistic model\nglm_spec &lt;- logistic_reg()\nlsl_form &lt;- lslpts ~ slope + cplan + cprof + elev + log10_carea \nlsl_wf &lt;- workflow(lsl_form, glm_spec)\ndoParallel::registerDoParallel() \nregular_rs &lt;- fit_resamples(lsl_wf, bad_folds)",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-terms",
    "href": "qmd/geospatial-general.html#sec-geo-gen-terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nCensus Block Groups - ~600–3,000 population; the smallest geography reported; Wiki\nCensus Tract - ~4,000 average population; Docs\n\nAlso see Survey, Census Data &gt;&gt; Geographies\n\nGraticules - a network of lines on a map that delineate the geographic coordinates (degrees of latitude and longitude.)\n\nUse of graticules is not advised, unless the graphical output will be used for measurement or navigation, or the direction of North is important for the interpretation of the content, or the content is intended to display distortions and artifacts created by projection. Unnecessary use of graticules only adds visual clutter but little relevant information. Use of coastlines, administrative boundaries or place names permits most viewers of the output to orient themselves better than a graticule\n{sf::st_graticule}\n\nRaster Data - Grid data (instead of point/polygon data in Vector Data) where each square on this grid is a small cell, and each cell holds a single value representing some real-world phenomenon, e.g. elevation, temperature, land cover type, rainfall amount, or color of a pixel in a satellite image. The entire collection of these cells and their values is what we call raster data. Raster data is better for continuous phenomena like elevation, soil moisture, or temperature. Most data from satellites and aerial photography comes in raster form.\nVector Data - Data that uses points, lines, and polygons (instead of grid cells like Raster Data) to represent features like roads, buildings, or country borders. Vector data is precise and good for discrete objects.\nVRT - File format that allows a virtual GDAL dataset to be composed from other GDAL datasets with repositioning, and algorithms potentially applied as well as various kinds of metadata altered or added. VRT descriptions of datasets can be saved in an XML format normally given the extension .vrt.\n\nBasically a metadata XML file describing various properties of the actual raster file, like pixel dimensions, geolocation, etc..",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-opt",
    "href": "qmd/geospatial-general.html#sec-geo-gen-opt",
    "title": "General",
    "section": "Optimization",
    "text": "Optimization\n\nMisc\nVector Tiles\n\nMisc\n\nNotes from Push the limits of interactive mapping in R with vector tiles\n\nMcBain goes through a complete example with plenty of tips on simplification strategies and hosting mbtiles files\n\nIssues (solution: Vector Tiles)\n\nLimited number of features with DOM canvas\n\nThere’s a limit to how many features leaflet maps can handle, because at some point the DOM gets too full and your browser stops being able to parse it.\n\nLimited number of maps on same webpage\n\nOnce you start rendering spatial data on WebGL canvasses instead of the DOM you’ll find there is a low number of WebGL contexts that can co-exist on any one web page, typically limiting you to only around 8 maps.\n\nFile sizes blow up to hundreds of MB\n\nTrying to reuse WebGL maps by toggling on and off different layers of data for the user at opportune times. This is an improvement, but data for all those layers piles up, and your toolchain wants to embed this in your page as reams of base64 encoded text. Page file sizes are completely blowing out.\n\n\n\nUse Cases\n\nSimplification of geometry is not desirable, e.g. because of alignment issues\n\ne.g. The zoomed-in road network has to align with the road network on the basemap, so that viewers can see features that lie along sections of road.\n\nSimplification of geometry doesn’t really help, you still have too many features\nCumulatively your datasets are too large to handle.\n\nVector Tiles  - contain arrays of annotated spatial coordinate data which is combined with a separately transmitted stylesheet to produce the tile image.\n\ni.e. The edges of the roads, the boundaries of buildings etc. Not an image, but the building blocks for one\nDifferent stylesheets can use the same vector data to produce radically different looking maps that either highlight or omit data with certain attributes\nMapbox Vector Tiles (MVT) - specification; the de-facto standard for vector tile files\n\nstored as a Google protocol buffer - a tightly packed binary format.\n\n\nMBTiles - by Mapbox; describe a method of storing an entire MVT tileset inside a single file.\n\nInternally .mbtiles files are SQLlite databases containing two tables: metadata and tiles.\n\ntiles table\n\nindexed by z,x,y\ncontains a tile_data column for the vector tile protocol buffers, which are compressed using gzip\n\n\nSQLite format and gzip compression help with efficient retrieval and transmission\n\nUsing vector tiles we can have unlimited reference layers. Each one contributes nothing to the report file size since it is only streamed on demand when required.\nWorkflow to convert data to .tbtiles\n\nIn R, read source data as an sf, and wrangle\n\nTippecanoe expects by epsg 4326 by default\n\nWrite data out to geojson\nOn the command line, convert geojson to .mbtiles using the tippecanoe command line utility.\n\nTippecanoe sources\n\nMapbox version - repo\n\nMcBain says, he uses this version and hasn’t had any problems\nREADME has helpful cookbook section\n\nActively maintained community forked version - repo\nMay be a headache to get dependencies if using Windows\n\nAlternatively it can output a folder structure full of protocol buffer files.\n\nExample\ntippecanoe -zg \\\n          -o abs_mesh_blocks.mbtiles \\\n          --coalesce-densest-as-needed \\\n          --extend-zooms-if-still-dropping \\\n          mb_shapes.geojson\n\nMapping\n\nExample\nlibrary(mvtview)\nlibrary(rdeck)\n\n# Fire up the server\nserve_mvt(\"abs_mesh_blocks.mbtiles\", port = 8765)\n# Serving your tile data from http://0.0.0.0:8765/abs_mesh_blocks.json.\n# Run clean_mvt() to remove all server sessions.\n\nmesh_blocks &lt;- jsonlite::fromJSON(\"http://0.0.0.0:8765/abs_mesh_blocks.json\")\n\n# Map the data\nrdeck(\n    initial_bounds = structure(meshblocks$bounds, crs = 4326, class = \"bbox\") # set map limits using the tilejson\n) |&gt;\n  add_mvt_layer(\n    data = rdeck::tile_json(\"http://0.0.0.0:8765/abs_mesh_blocks.json\"),\n    get_fill_color = scale_color_linear(\n      random_attribute\n    ),\n    opacity = 0.6\n  )\n\nSee McBain article for options on hosting .mbtiles files\nRegarding “abs_mesh_blocks”: {mvtview} provides a way to fetch the metadata table from .mbtiles as json by querying a json file with the same name as the .mbitles file.\nThe structure of ‘tilejson’ is yet another specification created by Mapbox, and is supported in deck.gl (and therefore {rdeck}) to describe tile endpoints.",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-gridsys",
    "href": "qmd/geospatial-general.html#sec-geo-gen-gridsys",
    "title": "General",
    "section": "Grid Systems",
    "text": "Grid Systems\n\nMisc\n\nExplainer: Why using hexbins to visualize Australian electoral map is better than a typical provincial map.\n\ntl;dr: Geographical size distorts what the value is trying to measure. The value is the party that wins the parliamentary seat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe bar graph shows the values the map is trying to visualize geographically. The hexabins better represent the close race by removing the distorting element which is the geographical size of the provinces.\nEach voting district (hexabin) is voting for 1 representative and has the same number of voters, but districts can have vastly different areas depending on population density.\n\nKeep unit at constant size (like hexabins) but alter hex shape to keep state shape.\n\n\nA better U.S. house election results map?\nResults\n\nstate size depends on number of districts which depends on population and therefore correctly conveys voting results visually across the whole country\nDistricts get distorted but the states retain their shape and so distortion of the overall visualization is minimized\n\n\n\n\nUber’s H3 grid system -\n\nMisc\n\nPackages: {h3r}, {h3-r}\ndocs\nAdd census data to H3 hexagons, calculate overlaps (article)\nFor large areas, you can reduce the number of hexagons by merging some hexagons into larger hexagons.\n\nReduces storage size\nIssue: leaves small gaps between hexagons\n\nmight not matter for your use case\n\nSolution: use Microsoft’s Quadkeys approach (see article)\n\n\nEach hexagon has a series of smaller hexagons that sit (mostly) inside of another, which creates a hierarchy that can be used for consistent referencing and analysis, all the way down to lengths of 2 feet for the edges.\n“Hexagons were an important choice because people in a city are often in motion, and hexagons minimize the quantization error introduced when users move through a city. Hexagons also allow us to approximate radiuses easily.”\nRe other shapes: “We could use postal code areas, but such areas have unusual shapes and sizes which are not helpful for analysis, and are subject to change for reasons entirely unrelated to what we would use them for. Zones could also be drawn by Uber operations teams based on their knowledge of the city, but such zones require frequent updating as cities change and often define the edges of areas arbitrarily”\nGrid systems can have comparable shapes and sizes across the cities that Uber operates in and are not subject to arbitrary changes. While grid systems do not align to streets and neighborhoods in cities, they can be used to efficiently represent neighborhoods by clustering grid cells. Clustering can be done using objective functions, producing shapes much more useful for analysis. Determining membership of a cluster is as efficient as a set lookup operation.\n16 Resolutions\n\n0 - 15 (0 being coarsest and 15 being finest)\nEach finer resolution has cells with one seventh the area of the coarser resolution. Hexagons cannot be perfectly subdivided into seven hexagons, so the finer cells are only approximately contained within a parent cell.\nThe identifiers for these child cells can be easily truncated to find their ancestor cell at a coarser resolution, enabling efficient indexing. Because the children cells are only approximately contained, the truncation process produces a fixed amount of shape distortion. This distortion is only present when performing truncation of a cell identifier; when indexing locations at a specific resolution, the cell boundaries are exact.\nWant a resolution granular enough to introduce variability and wide enough to capture the effects of an area\nExample of resolution 6 in Iowa",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-feats",
    "href": "qmd/geospatial-general.html#sec-geo-gen-feats",
    "title": "General",
    "section": "Features",
    "text": "Features\n\nCarto Spatial Features dataset ($) - https://carto.com/spatial-data-catalog/browser/?country=usa&category=derived&provider=carto\n\nResolution: Quadgrid level 15 (with cells of approximately 1x1km) and Quadgrid level 18 (with cells of approximately 100x100m).\n\nGuessing if the areas you’re interested in have high population density, then maybe 100 x 100 m cells would be more useful\n\nFeatures\n\nTotal population\nPopulation by gender\nPopulation by age and gender (e.g. female_0_to_19)\nPOIs by category\n\nRetail Stores\nEducation\n\nNumber of education related POIs, incuding schools, universities, academies, etc.\n\nFinancial\n\nNumber of financial sector POIs, including ATMs and banks.\n\nFood, Drink\n\nNumber of sustenance related POIs, including restaurants, bars, cafes and pubs.\n\nHealthcare\n\nNumber of healthcare related POIs, including hospitals\n\nLeisure\n\nNumber of POIs related to leisure activities, such as theaters, stadiums and sport centers.\n\nTourism\n\nNumber of POIs related to tourism attractions\n\nTransportation\n\nNumber of transportation related POIs, including parking lots, car rentals, train stations and public transport stations.\n\n\n\n\nCarto Data Observatory ($) - https://carto.com/spatial-data-catalog/browser/dataset/mc_geographic\\_\\_4a11e98c/\n\nFeatures\n\nGeo id\nRegion id\nIndustry\nTotal Transactions Amount Index\nTransaction Count Index\nAccount Count Index\nAverage Ticket Size Index\nAverage Frequency of Transaction per Card Index\nAverage Spend Amount by Account Index",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-interx",
    "href": "qmd/geospatial-general.html#sec-geo-gen-interx",
    "title": "General",
    "section": "Interactions",
    "text": "Interactions\n\nSimilar to interpolation but keeps the original spatial units as interpretive framework. Hence, the map reader can still rely on a known territorial division to develop its analyses\n\nThey produce understandable maps by smoothing complex spatial patterns\nThey enrich variables with contextual spatial information.\n\nMisc\n\nResources\n\nGetting Started with Potential - nice little mathematical summary, some background\n\nPackages\n\n{potential}: spatial interaction modeling via Stewart Potentials. Also capable of interpolation\n\n\nThere are two main ways of modeling spatial interactions: the first one focuses on links between places (flows), the second one focuses on places and their influence at a distance (potentials).\nComparisons ({potential}article)\n\nGDP per capita (cloropleth)\n\n\nTypical cloropleth at the municipality level\nValues have been binned\n\nPotential GDP per Capita (interaction)\n\n\nStewart Potentials have smoothed the values\nMunicipality boundaries still intact, so you could perform an analysis based on these GDP regions\n\nSmoothed GDP per Capita (interpolation)\n\n\nSimilar results as the interaction model except there are no boundaries",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-interp",
    "href": "qmd/geospatial-general.html#sec-geo-gen-interp",
    "title": "General",
    "section": "Interpolation",
    "text": "Interpolation\n\nMeasurements can have strong regional variance, so the geographical distribution of measurements can have a strong influence on statistical estimates.\n\nExample: Temperature\n\n\nTwo different geographical distributions of sensors\n\n\nA concentration of sensors in North can lead to a cooler average regional temperature and vice versa for the South.\n\nDistribution of temperatures across the region for 1 day.\n\n\nWith this much variance in temperature, a density of sensors in one area can distort the overall average.\n\nInterpolation evens out the geographical distribution of measurments\n\n\n\nThe process of using points with known values to estimate values at other points. In GIS applications, spatial interpolation is typically applied to a raster with estimates made for all cells. Spatial interpolation is therefore a means of creating surface data from sample points.\nMisc\n\n{gstat} - Has various interpolation methods.\n\nKriging\n\n\nConsiders not only the distances but also the other variables that have a linear relationship with the estimation variables\nUses a correlated Gaussian process to guess at values between data points\n\n\nUncorrelated - white noise\nCorrelated - smooth\n\nThe closer in distance two points are to each other, the more likely they are to have a similar value (i.e. geospatially correlated)\nExample: Temperature\n\n\nFewer known points means greater unceretainty\n\n\nInputs:\n\nThe measured values at the sampling points,\nThe geometric coordinates of the sampling points,\nThe geometric coordinates of the target points to interpolate,\nThe “calibrated” probabilistic model, with the spatial correlation obtained by data\n\nOutputs:\n\nThe estimated values at the target points,\nThe estimated uncertainty (variance) at the target points.",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/production-tools.html",
    "href": "qmd/production-tools.html",
    "title": "38  Tools",
    "section": "",
    "text": "38.1 Misc",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-misc",
    "href": "qmd/production-tools.html#sec-prod-tools-misc",
    "title": "38  Tools",
    "section": "",
    "text": "Overview of some 2021 tools Descriptions in article \nAWS Batch - Managed service for computational jobs. Alternative to having to maintain a kubernetes cluster\n\nTakes care of keeping a queue of jobs, spinning up EC2 instances, running code and shutting down the instances.\nScales up and down depending on how many jobs submitted.\nAllows you to execute your code in a scalable fashion and to request custom resources for compute-intensive jobs (e.g., instances with many CPUs and large memory) without requiring us to maintain a cluster\nSee bkmks: Hosting &gt;&gt; AWS &gt;&gt; Batch\nPackages:\n\n{crew.aws.batch}",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-stckcomprnk",
    "href": "qmd/production-tools.html#sec-prod-tools-stckcomprnk",
    "title": "38  Tools",
    "section": "38.2 Stack Component Rankings",
    "text": "38.2 Stack Component Rankings\n\nDB format\n\narrow files\n\nELT Operations\n\n*dbt\n\nGoogle’s alternative is Dataform\nAWS’s alternative is Databrew\n\n*Spark\n*Google Big Query SQL\n*AWS Athena\n\nOrchestration and monitoring\n\n*Targets\n\n+ {cronR} for orchestration + scheduling\n\n*Mage-AI\n*AWS Glue\nPrefect\nAirflow\n\nData Ingestion\n\nAirbyte (data ingestion)\nfivetran (data ingestion)\n\nCan “process atomic REST APIs to extract data out of SAAS silos and onto your warehouse”\n\nterraform (multi-cloud management)\n\nTracking/Versioning for Model Building\n\n*DVC\nMLFlow\n\nReporting\n\nblastula (email), xaringan (presentation), RMarkdown (reports), flexdashboard (dashboards),\nRStudio Connect (publishing platform to stakeholders)\n\ndashboards, apps\non-demand and scheduled reports\npresentations\nAPIs (?)\nPublish R and Python\nEnterprise security\nCan stay in RStudio\n\n\nVisualization Platforms\n\nLooker*\nPowerBI, DataStudio",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-depman",
    "href": "qmd/production-tools.html#sec-prod-tools-depman",
    "title": "38  Tools",
    "section": "38.3 Dependency Management",
    "text": "38.3 Dependency Management\n\nR\n\nr2u for linux installations\n\n“for Ubuntu 20.04 and 22.04 it provides _all_ of CRAN (and portion of BioConductor) as binary #Rstats packages with full, complete and automatic resolution of all dependencies for full system integration. If you use `bspm` along with it you can even use this via `install.packages()` and friends. Everything comes from a well connected mirror”",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#data-versioning",
    "href": "qmd/production-tools.html#data-versioning",
    "title": "38  Tools",
    "section": "38.4 Data versioning",
    "text": "38.4 Data versioning\n\nFlat Table by Github\n\nHas a Github action associated with it\nHas a datetime commit message\nLists as a feature that it tracks differences from one commit to the next, but doesn’t a normal data commit doe the same thing?\n\nLumberjack R package\n\nAdd functions to your processing script\ntracks using a log file\noptions for changes you want to track",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-dating",
    "href": "qmd/production-tools.html#sec-prod-tools-dating",
    "title": "38  Tools",
    "section": "38.5 Data Ingestion",
    "text": "38.5 Data Ingestion\n\nFiveTran\n\nFree-tier\nSync raw data sources\n\nevery 1hr for starter plan, every 15 minutes both standard plans, every 5 min for enterprise plan",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-orch",
    "href": "qmd/production-tools.html#sec-prod-tools-orch",
    "title": "38  Tools",
    "section": "38.6 Orchestration",
    "text": "38.6 Orchestration\n\n38.6.1 Airflow\n\nWebpage\nOpen-source platform for authoring, scheduling, and executing data pipelines.\n\nFeatures for managing and monitoring data pipelines, including integration with various data storage and processing technologies. Similar to the Unix cron utility — you write scripts and schedule them to run every X minutes.\nAirflow can be used for any sort of scheduling task, but is often used for scheduling data modeling. schedule, run and monitor the refresh of our data warehouse\nMonitoring on-prem checking Airflow logs is not user-friendly (better in AWS MWAA)\n\ndifferent types of logs for task, web server, scheduler, worker, and DAGs\nhave to SSH into the server and run commands which becomes more complicated when you want to use distributed servers for scalability.\n\nRequires you to create a central logging storage and make additional setup to make all servers write logs into that single place\n\n\nServer-based remains active even when not running jobs –&gt; continually incurring cost\n\nNo latency since servers are always running\n\nProblems\nLong feedback loop\n\nWhile programming, instant feedback of your DAG becomes crucial when you want a sanity check before your code goes too far.\nTo see the graph view, which is mainly for visualizing dependencies in DAGs, your code needs to be in the folder of an Airflow scheduler that can be picked up. The airflow scheduler also takes time to render and parse your DAG until it shows up.\nMakes debugging difficult during the development cycle, so some engineers write more lines of code and test them all together. If the lines of code become unmanageable on one screen, you might vaguely remember what to validate and what dependencies to check.\n\nDifficult with local development\n\na docker image can be used to inject as much production-related information as possible. But it’s still not 100% copy, and it takes tremendous effort to develop and maintain that docker image.\nEven if you set up dev, staging, and production environments for running Airflow, they aren’t totally isolated and developers can end-up interfering with one another. Services/Extensions Astronomer offers a managed Airflow service.\nAmazon Managed Workflows for Apache Airflow (MWAA) - managed Airflow service\n\nOrchestrate jobs in EMR, Athena, S3, or Redshift\n\nGlue\n\nAirflow has the glue operator\n\nCloudFormation can be used to configure and manage\n\nallows for autoscaling which saves on costs by scaling down when usage is low\nstill needs a server running even when not running jobs\nmonitoring much easier since all the logs are written into CloudWatch search certain logs using Logs Insights\n\nhave a dashboard that displays usage of server resources like CPU, memory, and network traffic.\nmonitor numerous other Airflow-specific metrics.\nset up alerts and manage notification recipients programmatically.\n\n\nCost factors\n\nInstance size\nAdditional worker instance\nAdditional scheduler instance\nMeta database storage\n\nPotential Issues:\n\nResources are shared on multiple jobs so performance can suffer if:\n\nDon’t distribute trigger times evenly\nMisconfigure your maximum worker count\n\n\nOperate through AWS SDK\n\nCan\n\ncreate, update, and delete MWAA environments and retrieve their environment information that includes logging policies, number of workers, schedulers\nrun Airflow’s internal commands to control DAGs\n\nCan’t\n\nSome of Airflow’s native commands like backfill (check this AWS document), dags list, dags list-runs, dags next-execution, and more\n\n\n\n\n\n\n\n38.6.2 AWS Glue\n\nCloud-based data integration service that makes it easy to move data between data stores.\n\nIncludes a data catalog for storing metadata about data sources and targets, as well as a ETL (extract, transform, and load) engine for transforming and moving data.\nIntegrates with other AWS services, such as S3 and Redshift, making it a convenient choice for users of the AWS ecosystem. Serverless (i.e. costs only incurred when triggered by event) Each job triggers separate resources, so if one job overloads resources, it doesn’t affect other jobs\nJobs experience latency since instances have to spin-up and install packages\nCost Charged by Data Processing Unit (DPU) multiplied by usage hours\n\nJob types:\n\nPython shell: you can choose either 0.0625 or 1 DPU.\nApache Spark: you can use 2 to 100 DPUs.\nSpark Streaming: you can use 2 DPUs to 100 DPUs.\n\n\n\nCan run Spark\nMonitoring\n\nCloudwatch\nGlueStudio within Glue Clicking number sends you to Cloudwatch where you can drill down into jobs\nCloudFormation can be used to configure and manage\nGlue SDK available\n\n\n\n\n38.6.3 Prefect\n\nEasier to manage for smaller data engineer teams or a single data engineer\nmore user friendly than Airflow; Better UI; more easily discover location and time of errors\npurely python\nMisc\n\nadd slack webhook for notifications\nHas slack channel to get immediate help with issues or questions\n\nautomatic versioning for every flow, within every project\n\nalso document the models deployed with each version in the README they provide with every flow\n\nComponents\n\nTasks - individual jobs that do one unit of work\n\ne.g. a step that syncs Fivetran data or runs a dbt model\n\nFlows - functions that consist of a bunch of smaller tasks, or units of work, that depend on one another\n\ne.g. 1 flow could be multiple tasks running Fivetran syncs and dbt models\n\nExample:\nfrom prefect import flow, task\n@flow(name=\"Create a Report for Google Trends\")\ndef create_pytrends_report(\n    keyword: str = \"COVID\", start_date: str = \"2020-01-01\", num_countries: int = 10\n):\n\nThese flows are then scheduled and run by whatever types of agents you choose to set up.\n\nSome options include AWS ECS, GCP Vertex, Kubernetes, locally, etc.\n\nDeployments (docs)\n\nAlso see Create Robust Data Pipelines with Prefect, Docker, and GitHub\nDefintions\n\nSpecify the execution environment infrastructure for the flow run\nSpecify how your flow code is stored and retrieved by Prefect agents\nCreate flow runs with custom parameters from the UI\nCreate a schedule to run the flow\n\nSteps\n\nBuild the deployment definition file and optionally upload your flow to the specified remote storage location\nCreate the deployment by applying the deployment definition\n\nSyntax: prefect deployment build [OPTIONS] &lt;path-to-your-flow&gt;:&lt;flow-name&gt;\nExample:\nprefect deployment build src/main.py:create_pytrends_report \\\n  -n google-trends-gh-docker \\\n  -q test\n\nDeployment for the flow create_pytrends_report (see flow example) from the file, “src/main.py”\n-n google-trends-gh-docker specifies the name of the deployment to be google-trends-gh-docker.\n-q test specifies the work queue to be test . A work queue organizes deployments into queues for execution.\nOutput\n\n“create_pytrends_report-deployment.yaml” file and a “.prefectignore” created in the current directory.\n\n“create_pytrends_report-deployment.yaml”:  specifies where a flow’s code is stored and how a flow should be run.\n“.prefectignore”:  prevents certain files or directories from being uploaded to the configured storage location.\n\n\n\n\n\n38.6.4 Azure Data Factory\n\nAllows users to create, schedule, and orchestrate data pipelines for moving and transforming data from various sources to destinations.\nData Factory provides a visual designer for building pipelines, as well as a range of connectors for integrating with various data stores and processing technologies.\nExample: Demand Planning Project\n\n\n\n38.6.5 Mage-AI\n\nEnables users to define DAG regardless of the choice of languages (python/SQL/R)\nWeb-based IDE, so its mobility allows working from different devices, and sharing becomes more straightforward.\n\nUI layout feels like using RStudio. It has many sections divided into different areas.\nOne of the areas is the DAG visualization which provides instant feedback to the user on the task relationship.\n\nDAGs\n\nThe pipeline or DAG is constructed with modular blocks—a block maps to a single file.\nBlock Options\n\nExecution with upstream blocks: this triggers all upstream blocks to get the data ready for the current block to run\nExecute and run tests defined in the current block: this focuses on the current block to perform testing.\nSet block as dynamic: this changes the block type into the dynamic block, and it fits better to create multiple downstream blocks at runtime.\n\nManipulate dependencies via drag and drop\n\nmage-ai keeps track of the UI changes the user made and automatically builds the dependencies DAG into the YAML file. (./pipelines/{your_awesome_pipeline_name}/metadata.yaml)\n\nVisualize data in each block\n\nHelpful for inspecting your input data and further validating the transformation.\nOnce the chart has been created, it will also be attached to the current block as the downstream_blocks.\n\n\nR\n\nAllows users to write the main ETL (Extraction, Transformation, and Loading) blocks using R.\n\n\n\n\n38.6.6 kestra\n\nPopular orchestration libraries such as Airflow, Prefect, and Dagster require modifications to the Python code to use their functionalities. You may need to modify the data science code to add orchestration logic\nKestra, an open-source library, allows you to develop your Python scripts independently and then ​​seamlessly incorporate them into data workflows using YAML files.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-eltetl",
    "href": "qmd/production-tools.html#sec-prod-tools-eltetl",
    "title": "38  Tools",
    "section": "38.7 ELT/ETL Operations",
    "text": "38.7 ELT/ETL Operations\n\n38.7.1 Misc\n\ndbt - see DB, dbt\nGoogle Dataform - Docs, Best Practices\n\n\n\n38.7.2 AWS DataBrew\n\nFeatures to clean and transform the data to ready it for further processing or feeding to machine learning models\n\nNo coding; pay for what you use; scales automatically\nover 250 transformations\nAllows you to add custom transformations with lambda functions",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-modexptrk",
    "href": "qmd/production-tools.html#sec-prod-tools-modexptrk",
    "title": "38  Tools",
    "section": "38.8 Model Experimentation/Version Tracking",
    "text": "38.8 Model Experimentation/Version Tracking\n\n38.8.1 DVC\n\nTracks data and models while model building\nStore code and track changes in a Git repository while data/models are in AWS/GCP/Azure/etc. storage\nTracking changes\n\nSteps\n\nhashes every file in the directory data,\nadds it to .gitignore and\ncreates a small file data.dvc that is added to Git.\n\nBy comparing hashes, DVC knows when files change and which version to restore.\n\nInitial Steps\n\nGoto project directory -cd &lt;path to local github repo&gt;\nInitialize DVC - dvc init\nAdd a data path/uri - dvc remote add -d remote path/to/remote\n\ncan be Google Drive, Amazon S3, Google Cloud Storage, Azure Storage, or on your local machine\ne.g. Google Drive: dvc remote add -d remote gdrive://&lt;hash&gt;\n\nThe hash will the last part of the URL, e.g. “https://drive.google.com/drive/u/0/folders/1v1cBGN9vS9NT6-t6QhJG”\n\nConfirm data set-up: dvc config -l\n\nThe config file is located inside “.dvc/”\nTo version your config on github: git add .dvc/config\n\n\nAdd data/ to .gitignore\n\nExample showed adding every file in the repo manually but this seems easier\n\nAdd, commit, and push all files to repo\n\nMain differences to regular project initialization\n\ndata/ directory doesn’t get pushed to github\ndata.dvc file gets pushed to github\n\n\nSet-up DVC data cache\n\nCan be local directory/s3/gs/gdrive/etc\nExample: S3\n            dvc remote add -d myremote s3://mybucket/path\n            git add .dvc/config\n            git commit -m \"Configure remote storage\"\n            git push\n            dvc push\n\n\nI’m guessing .dvc/config is created with dvc remote add  and wasn’t there before. Otherwise in steps 3 and 4, I need to add the files manually.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-modmon",
    "href": "qmd/production-tools.html#sec-prod-tools-modmon",
    "title": "38  Tools",
    "section": "38.9 Model/Data Drift Monitoring",
    "text": "38.9 Model/Data Drift Monitoring\n\nArize AI\n\nDocs\nAccessed through Rest API, Python SDK, or Cloud Storage Bucket\n\nFiddler AI Monitoring: fiddler.ai has a suite of tools that help in making the AI explainable, aid in operating ML models in production, monitor ML models and yes data & model drift detection is one of them\nEvidently: EvidentlyAI is another open-source tool, which helps in evaluating and monitoring models in production. If you are not using Azure ML and looking for a non-commercial tool that is simple to use, evidentlyai is a good place to start.\nAzure ML\n\nMonitors data; uses wasserstein distance\n\nAWS Glue DataBrew\n\nmonitors features\ncalculates full suite of summary stats + entropy\n\nCan be exported to a bucket and then download to measure change over time\n\nAccessed through console or programmatically\nGenerates reports that can be viewed in console or be exported in html, pdf, etc.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-appclustmon",
    "href": "qmd/production-tools.html#sec-prod-tools-appclustmon",
    "title": "38  Tools",
    "section": "38.10 App/Cluster Monitoring",
    "text": "38.10 App/Cluster Monitoring\n\n38.10.1 Prometheus\n\nDon’t use for ML monitoring (from article)(maybe for apps?)\n\nNeed to use multiple Prometheus Metric types for cross-component monitoring\nNeed to define histogram buckets up front for single-component monitoring\nCorrectness of query results depending on scraping interval\nInability to handle sliding windows\nDisgusting-looking PromQL queries\nHigh latency for cross-component metrics (i.e., high-cardinality joins)\n\nMisc\n\nPrometheus is not a time series database (TSDB). It merely leverages a TSDB.\nBecause Prometheus scrapes values periodically, some Metric types (e.g., Gauges) can lose precision if the Metric value changes more frequently than the scraping interval. This problem does not apply to monotonically increasing metrics (e.g., Counters).\nMetrics can be logged with arbitrary identifiers such that at query time, users can filter Metrics by their identifier value.\nPromQL is flexible – users can compute many different aggregations (basic arithmetic functions) of Metric values over different window sizes, and these parameters can be specified at query time.\n\nMetric values (Docs):\n\nCounter: a cumulative Metric that monotonically increases. Can be used to track the number of predictions served, for example.\nGauge: a Metric that represents a single numerical value that can arbitrarily change. Can be used to track current memory usage, for example.\nHistogram: a Metric that categorizes observed numerical values into user-predefined buckets. This has a high server-side cost because the server calculates quantiles at query time.\nSummary: a Metric that tracks a user-predefined quantile over a sliding time window. This has a lower server-side cost because quantiles are configured and tracked at logging time. Also, the Summary Metric doesn’t generally support aggregations in queries.\n\nProcess\n\n\nUsers instrument their application code to log Metric values.\nThose values are scraped and stored in a Prometheus server.\nThe values can be queried using PromQL and exported to a visualization tool like Grafana\n\nThe are R packages that might make querying these metrics easier so you don’t have to learn PromQL",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-oth",
    "href": "qmd/production-tools.html#sec-prod-tools-oth",
    "title": "38  Tools",
    "section": "38.11 Other",
    "text": "38.11 Other\n\nTerraform\n\nProvision infrastructure across 300+ public clouds and services using a single workflow through yaml files\n\nAutomates and makes these workflows reproducible\nArticle on using it with R\n\n\nDatadog - Monitor servers in all cloud hosts in one place — alerts, metrics, logs, traces, security incidents, etc.\nPagerDuty - Automated incident management — alerts, notifications, diagnostics, logging, etc.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html",
    "href": "qmd/project-analyses.html",
    "title": "Analyses",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-misc",
    "href": "qmd/project-analyses.html#sec-proj-anal-misc",
    "title": "Analyses",
    "section": "",
    "text": "Also see\n\nLogistics, Demand Planning &gt;&gt; Stakeholder Questions\n\nThese are questions for the stakeholder(s) when preparing to create a forecasting model, but many apply to other types of projects including Analysis.\n\n\nSee Thread on an analysis workflow using {targets}",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-analy-gen",
    "href": "qmd/project-analyses.html#sec-proj-analy-gen",
    "title": "Analyses",
    "section": "General",
    "text": "General\n\nGeneral Questions\n\n“What variables are relevant to the problem I’m trying to solve?”\n“What are the key components of this data set?”\n“Can this data be categorized?”\n“Is this analysis result out of the ordinary?”\n“What are the key relationships?”\n“Is this the best way this company could be carrying out this task?”\n“What will happen under new conditions?”\n“What factors are best used to determine or predict this eventuality?”\n\nBreak down the problem into parts and focus on those during EDA\n\nAlso see Decison Intelligence &gt;&gt; Mental Models for details on methods to break down components\nExample: Why are sales down?\n\nHow are sales calculated?\n\ne.g. Total Sales = # of Orders * Average Order Value\n\nBreakdown # of orders and average order value\n\nnumber of orders = number of walk-ins * % conversion\n\nHas walk-ins or conversion declined?\n\nAverage Order Value\n\nBin avg order value by quantiles, plot and facet or group by binned groups. Is one group more responsible for the decline than others?\n\n\nIs there regional or store or brand variability? (grouping variables)\n\n\nDrill down into each component until the data doesn’t allow you to go any farther.\nSegment data by groups\n\nColor or facet by cat vars\nPay attention to counts of each category (may need to collapse categories)\nCommon segments in product analytics\n\nFree vs Paid users\nDevice Type (desktop web vs mobile web vs native app)\nTraffic Source (people coming from search engines, paid marketing, people directly typing in your company’s URL into their browser, etc.)\nDay of the Week.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-tropf",
    "href": "qmd/project-analyses.html#sec-proj-anal-tropf",
    "title": "Analyses",
    "section": "TROPICS framework",
    "text": "TROPICS framework\n\nMisc\n\nFor analyzing changes in key performance metrics\nFrom https://towardsdatascience.com/answering-the-data-science-metric-change-interview-question-the-ultimate-guide-5e18d62d0dc6\nComponents: Time, Region, Other Internal Products, Platform, Industry and Competitors, Cannibalization, Segmentation\n\nTime\n\nWhat to explore\n\nHow has our performance been trending over the last few weeks (or months)?\n\nExample: If we saw a 10% increase in the last week, was the percentage change in the weeks before also 10%? In which case the 10% may actually be pretty normal? Or was the change lower? Higher?\n\nIs this change seasonal? Do we see the same spike around this time each year?\n\nExample: Does WhatsApp see a spike in messages sent during the holiday season?\n\nWas the change sudden or gradual? Did we see a sudden spike or drop overnight? Or has the metric gradually been moving in this direction over time?\n\nExample: If product usage jumps by 50% overnight could there be a bug in our logging systems?\n\nAre there specific times during the day or week where this change is more pronounced?\n\nSolution examples\n\nIf the change is seasonal then there may not necessarily be anything you need to ‘solve’ for. But, you can leverage this to your advantage.\n\nExample: Amazon sales may jump up on Black Friday so they would want to make sure they have the proper infrastructure in place so the site doesn’t crash. They may also see if there are certain types of products that are popular purchases and increase their inventory accordingly.\n\nIf there is a sudden decline, there may be a bug in the logging or a new feature or update recently launched that’s creating problems that you may need to roll back.\nIf there’s a gradual decline, it may indicate a change in user behavior.\n\nExample: If the time spent listening to music is declining because people prefer to listen to podcasts then Spotify may want to focus more of their content inventory on podcasts.\n\n\n\nRegion\n\nWhat to explore\n\nIs this change concentrated in a specific region or do we see a similar change across the board?\n\nSolution examples\n\nThere may be newly enforced regulations in countries that are affecting your product metrics. You would need to do further research to assess the impacts of these regulations and potential workarounds.\n\nExample: Uber was temporarily banned in London in 2019 for repeated safety failures which resulted in a series of lawsuits and court cases.\n\nPopular local events may also be potential explanations. While these may not be areas to ‘solve’ for they can be opportunities to take advantage of.\n\nExample: Coachella season means a jump in the number of Airbnb bookings in Southern California that are capitalized on by surge pricing.\n\n\n\nOther Internal Products\n\nWhat to explore\n\nIs this change specific to one product or is it company-wide? How does this metric vary across our other product offerings?\n\nExample: If the Fundraising feature on Facebook is seeing increased usage, is the swipe up to donate feature on Instagram (which Facebook owns) also seeing a similar uptick?\n\nAre there other metrics that have also changed in addition to the one in question?\n\nExample: If the time spent on Uber is going down, is the number of cancellations by drivers also declining (implying people are spending less time on the app because they’re having a more reliable experience)?\n\n\nSolution examples\n\nIf there is a metric change across our other features and products, it’s likely a larger problem we should address with multiple teams and may need a Public Relations consultant.\n\nExample: Elon + Twitter.\n\n\n\nPlatform\n\nWhat to explore\n\nMobile vs Desktop?\nMac vs Windows?\nAndroid vs iOS?\n\nSolution examples\n\nIf there was a positive change in our metric on a specific platform (e.g. iOS) and coincides with an (iOS) update we released, we would want to do a retrospective to determine what about that update was favorable so we can double down on it. Alternatively, if the metric change was negative, we may want to reconsider and even roll back the update.\nIf the change was due to a change in the platform experience (e.g. app store placement, ratings) we may want to seek advice from our marketing team since this is a top of the funnel problem\nIf users are showing astrong preference for a specific platform, we want to make sure that the experience of the preferred platform is up to par. We also need to make sure our platform-specific monetization strategies are switching to follow the trend.\n\nExample: Facebook’s ad model was initially tied to the desktop app only and had to be expanded as mobile became the platform of preference.\n\n\n\nIndustry & Competitors\n\nWhat to explore\n\nWhen our decline began, was there a new competitor or category that emerged?\n\nExample: Did the number of users listening to Apple podcasts go down when Clubhouse came on to the scene?\n\nHave competitors changed their offering lately?\nIs the category as a whole declining?\n\nSolution examples\n\nIf the category is shifting as a whole, we should begin looking at larger-scale changes to the app.\n\nExample: What Kodak should have done.\n\nIf there’s a new competitor taking our market share, we can begin with reactivation campaigns on churned users. We may also want to conduct user research to understand the gap between our offering and those of our competitors\n\n\nCannibalization\n\nWhat to explore\n\nAre other products or features in our offering experiencing growth in the face of our decline or vice versa?\nHave we released a new feature that is drawing users away from our old features? If so, can we fully attribute the release of the new feature with the decline in the metric of our feature in question?\n\nExample: When Facebook released reactions, did the number of comments on a post go down because people found it easier to press a react button instead of writing a comment?\n\n\nSolution examples\n\nCannibalization may not necessarily be a bad thing. We need to determine whether this shift in user interest across our features is favorable by determining whether the new features align better with the goals of the business.\nCannibalization may also be an indication of but it is indicative of a change in user behavior. In which case we may want to consider if perhaps our core metrics need to change as user behaviors change.\n\nExample: If users care more about watching Instagram stories than engaging with the Instagram feed we may want to optimize for retention (because the ephemeral nature of stories is more likely to motivate users to keep coming back to the platform) instead of time spent on the app.\n\nWe can also look at ways to bridge the two features together to create a more unified platform.\n\n\nSegmentation\n\nWhat to explore\n\nHow does this metric vary by user type:\n\nAge, sex, education\nPower users versus casual users * New users versus existing users\n\nHow does this metric vary by different attributes of the product:\n\nExample: If the time spent watching YouTube videos is going down, is it across longer videos or shorter clips? Is it only for DIY videos or interview tutorial content? Is the same number of people that started watching a video the same but a large chunk of them stop watching it halfway through?\n\n\nSolution examples\n\nIf the metric varies between new and existing users then maybe there is a overcrowding effect.\n\nExample: Reddit forums could hit a critical mass where new users feel lost and less likely to engage than existing users resulting in a drop in engagements per user\n\nIf users are dropping off at certain parts of the funnel then maybe the experience at that funnel step is broken.\n\nExample: While the same number of people are starting carts on Amazon there may be a drop in purchases if the payment verification system isn’t working.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-actanl",
    "href": "qmd/project-analyses.html#sec-proj-anal-actanl",
    "title": "Analyses",
    "section": "Actionable Analyses",
    "text": "Actionable Analyses\n\nNotes from: Driving Product Impact With Actionable Analyses\nActionable insights do not only provide a specific data point that might be interesting, but lay out a clear narrative how this insight is connected to the problem at hand, what the ramifications are, as well as possible options and next steps to take with the associated benefits/risks of (not) acting upon these.\nNot Actionable: Users under the age of 25 hardly use audiobooks.\n\nIs this good, bad? Should they be listening to audiobooks and is there anything we should do about it?\n\nActionable: Users under the age of 25 hardly use audiobooks because they never explore the feature in the app. However users who listen to audiobooks have a 20% higher retention rate.\n\nThis information tells us that audiobooks represent a potential opportunity to increase retention amongst younger users, however there seems to be more work to be done to encourage users exploring this feature.\n\nSteps\n\nProblem Statement: High-level business problem to solve (e.g. Increasing Retention, Conversion Rate, Average Order Value)\n\nCan also be in regards to a metric that’s believed to be highly associated with a North Star metric like a Primary metric (See KPIs)\n\nOpportunity Areas: Areas or problems with a strong connection to the problem at hand\n\nInvestigate behaviors of users with the behavior that you’re interested in (i.e. high or low values of the desired metric).\nDiscovering the characteristics of these users can help to figure out ways to encourage other users to act similarily or gain insight into the type of users you want to attract.\n\nLevers: Different ways to work on the opportunity areas\n\nA lever should be data-based and able to be validated on whether working to increase or decrease the lever will lead to a positive solution to the problem statement.\nThere are typically multiple levers for a given opportunity area\n\nThese should be ordered in terms of priority, and priority should be given to the lever that is believed to result in the greatest impact on the opportunity area that will result in the greatest impact on the solution to the problem statement.\n\n\nExperiments [Optional]: Concrete implementation of a specific lever that can help prove/disprove our hypotheses.\n\nOptional but always helpful to convey recommendations and suggestions with concrete ideas for what the team could or should be building.\n\n\nExample\n\n\nProblem Statement: How can we increase daily listening time for premium users in the Spotify app?\n\nHypothesis: Daily Listening Time is strongly connected to retention for premium users and hensce to monthly revenue.\n\nOpportunity Areas:\n\nUsers who use auto-generated playlists have a x% higher daily listening time\nUsers who subscribed to at least 3 podcasts have a x% higher listening time per day than those who did not subscribe to any.\nUsers who listen to audiobooks have a x% higher daily listening time.\n\nLevers:\n\nOpportunity Area: Increase the percentage of users under 25 using audiobooks from x% to y%.\nQuestions:\n\nDo users not see the feature?\nDo users see the feature but don’t engage with the feature?\nDo users engage with the feature but drop off after a short amount of time?\n\nFinding: Users under 25 engage less with the Home Screen, the only screen where Audiobooks are promoted, and hence don’t see this feature in the App. This is likely leading low usage and engagement.\nLever: Increase prominence of Audiobooks within the app\nPrioritzation Table for Report\n\n\nExperiments:\n\n\n“We predict that adding a banner promoting Audiobooks when the App opens [Experiment Change] will increase younger users’ daily listening time [Problem] because more younger users will see and listen to Audiobooks [Lever]. We will know this is true when we see an increase in young users using Audiobooks [Lever], followed by an increase in the daily listening time for younger users [Validation Metrics].”\nIf there is no significant increase in audiobook usage, then there many other ways to increase the visibility of a feature which can be the hypotheses of further experiments.\nIf , however, there is a significant increase in users using Audiobooks (lever) but no effect on daily listening time (main problem), then the lever is invalidated and we can move on to the next one.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-edap",
    "href": "qmd/project-analyses.html#sec-proj-anal-edap",
    "title": "Analyses",
    "section": "Exploratory Data Analysis Research Plan",
    "text": "Exploratory Data Analysis Research Plan\n\nNotes from Pluralsight Designing an Exploratory Data Analysis Research Plan\n\nSee code &gt;&gt; rmarkdown &gt;&gt; reports &gt;&gt; edarp-demo.Rmd\n\nDuring the development of the EDARP, all stakeholders can align their expectiations. Buy-in from the aligned stakeholders can help sell the project to the organization.\nEach section should have an introduction with a description about whats in it\nMock Schedule\n\nWeek 1: Data request by a department\nWeek 2: Data Scientist and department meet to formalize the research questions\n\nWorking backwards from the desired output can help frame the right questions to ask during this period\n\nWeek 3: Clear metrics are established. The use case of the product is defined (i.e. who’s using it and what decisions are to be made). Sponsorship is set. Budgets are allocated.\nWeek 4: EDARP is finalized with everyone understanding the objectves, budget, product design, and product usage\nWeek 6: Data Scientist delivers the product to the department.\n\nSections of the Report\n\nAbstract\n\nHighlights the research questions\nWho the stakeholders are\nMetrics of success\nExample:\n\n“The foundational task was to develop sales insights across stores. Through the identification and inclusion of various business groups, data were gathered and questions were formed. The business groups included are Marketing, IT, Sales and Data Science. From this process we defined the primary goal of this research. This research adds understanding to how sales are driven across stores and develops a predictive model of sales across stores. These outcomes fit within budget and offer an expected ROI of 10%.”\n\n\nFigures and Tables\n\nOptional depending on audience\nSection where all viz is at\n\nIntroduction\n\nDetailed description of metrics of success\n\nExample\n\nROI 8%\nR2 75%\nInterpretability\n\n\n\nStakeholders\n\nMarketing\n\nList of people\n\nIT\nSales\nData Science\n\nBudget and Financial Impact\n\nNot always known, but this section is valuable if you’re able to include it.\nPotential vendor costs\nInfrastructure costs\nApplication developement\nFinancial impact, completed by finance team, result in an expected ROI of blah%\n\nMethods\n\nData description\nData wrangling\n\nWhat were the variables of interest and why (“data wrangling involved looking at trends in sales across stores, store types, and states”)\n\nAutocorrelation\n\n“Testing for autocorrelation was completed leading to insights in seasonality across the stores. We examined by the ACF an PACF metrics in the assessment of autocorrelation”\n\nClustering\nOutliers\n\nDescription of algorithm comparison and model selection\n\nWords not code or results\nExample\n\nInvolved training and testing regression, random forest,…\nRegression model served as a benchmark comparison across 5 models\nA discussion of interpretability and expected ROI guided the choice of the final model\n\n\n\n\nResults and Discussion\n\n“This section highlights the thought process that went into wrangling the data and building the models. A few of the insights gained in observation of the data are shared. Also, the assessment of the model is discussed at the end of the section.”\nVisualizing the Data (i.e. EDA viz - descriptive, outliers, clusters)\n\nFigures\nInsights\nRepeat as needed\n\nVariable Importance\nFinal Model\n\nModel Assessment\n\nAlgorithm comparison metrics\nDynamic visual of model output\n\nSimple shiny graph with a user input and a graph\n\ne.g. Choose store number - graph of sales forecast\n\n\n\n\nConclusion\n\nExample:\n\n“The research explored the possibility of building a predictive model to aid in forecasting sales across stores. We found that, given the established metrics of ROI greater than 8%, R-square of greater than .75 and interpretability in the models, this reasearch has resulted in a viable model for the business. Additionally, it was discovered the presence of some outlier phenomena in the data which has been identified by the stakeholders as acceptable noise. Further we discovered that there is a latent grouping to the stores across sales, store type and assortment. This insight will be used to guide marketings action in the future.”\n\n\nAppendix\n\nSchedule of Maintenance\nFuture Research",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-datmet",
    "href": "qmd/project-analyses.html#sec-proj-anal-datmet",
    "title": "Analyses",
    "section": "Data Meta-Metrics",
    "text": "Data Meta-Metrics\n\nNotes from Data Meta Metrics\nMetrics for categorizing the quality of data being used in your analysis\nYou can be very confident about the methodologies you’re using to analyze data, but if there are issues with the underlying dataset, you might not be so confident in the results of an analysis or your ability to repeat the analysis.\n\nIdeally, we should be passing this information — our confidences and our doubts — on to stakeholders alongside any results or reports we share.\n\nUse Cases\n\nConvey the quality of the data and its collection process to technical and non-technical audience\nHelpful for diagnosing the strengths and weaknesses of data storage and collection across multiple departments.\nDevelop a data improvement process with an understanding of what data you do and don’t have and what you can and can’t collect.\n\nGood data: You know how and when it’s collected, it lives in a familiar database, and represents exactly what you expect it to represent.\nLess-Than-Stellar data: Data that comes with an “oral history” and lots of caveats and exceptions when it comes to using it in practice.\n\ne.g. When you ask a department for data and their responses are “Anna needs to download a report with very specific filters from a proprietary system and give you the data” or “Call Matt and see if he remembers”\n\nPotential Metrics - The type of metrics you use can depend on the analysis your doing\n\n\nRelevance: Ability to answer the question we were asking of it\nTrustworthiness: Will the data be accurate based on how it was collected, stored, and managed?\nRepeatability: How accessible is this data? Can the ETL process be faithfully reproduced?\n\nSlide Report Examples\n\nGood Data\n\nBad Data\n\n\nWith bad data, notes on why the data is bad are included in the slide.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html",
    "href": "qmd/db-postgres.html",
    "title": "Postgres",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-misc",
    "href": "qmd/db-postgres.html#sec-db-pstgr-misc",
    "title": "Postgres",
    "section": "",
    "text": "Notes from\n\nCourse: linkedin.learning.postgresql.client.applications\nCourse: Linux.Academy.PostgreSQL.Administration.Deep.Dive\n\nResources\n\nPostgreSQL is Enough - Links to various applications/extensions resources\n\nEverything is case sensitive, so use lowercase for db and table names\nCheck postgres sql version - psql --version or -V\nSee flag options - psql --help\nIf there’s a “#” in the prompt after logging into a db, then that signifies you are a super-user\nMeta commands (i.e. commands once you’re logged into the db)\n\n\\du - list roles (aka users + permissions)\n\\c  - switches databases\n\\password  - assign a password to a user (prompt will ask for the password twice)\n\nCan also use ALTER ROLE for this but the password will then be in the log\n\n\nUnlogged Table - Data written to an unlogged table will not be logged to the write-ahead-log (WAL), making it ideal for intermediate tables and considerably faster. Note that unlogged tables will not be restored in case of a crash, and will not be replicated.",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-gstgr-ext",
    "href": "qmd/db-postgres.html#sec-db-gstgr-ext",
    "title": "Postgres",
    "section": "Extensions",
    "text": "Extensions\n\npg_analytics\n\nIntro, Repo\nArrow and Datafusion integrated with Postgres\nDelta Lake tables behave like regular Postgres tables but use a column-oriented layout via Apache Arrow and utilize Apache DataFusion, a query engine optimized for column-oriented data\nData is persisted to disk with Parquet\nThe delta-rs library is a Rust-based implementation of Delta Lake. This library adds ACID transactions, updates and deletes, and file compaction to Parquet storage. It also supports querying over data lakes like S3, which introduces the future possibility connecting Postgres tables to cloud data lakes.\n\npg_bm25\n\nIntro, Repo\nRust-based extension that significantly improves Postgres’ full text search capabilities\n\nBuilt to be an Elasticsearch inside of a postgres db\n\nPerformant on large tables, adds support for operations like fuzzy search, relevance tuning, or BM25 relevance scoring (same algo as Elasticsearch), real-time search — new data is immediately searchable without manual reindexing\n\nQuery times over 1M rows are 20x faster compared to tsquery and ts_ran (built-in search and sort)\n\n\npg_sparse\n\nIntro, Repo\nEnables efficient storage and retrieval of sparse vectors using HNSW\n\nSPLADE outputs sparse vectors with over 30,000 entries. Sparse vectors can detect the presence of exact keywords while also capturing semantic similarity between terms.\n\nFork of pgvector with modifications\nCompatible alongside both pg_bm25 and pgvector\n\npgvector\n\nRepo\nAlso see Databases, Vector Databases for alternatives and comparisons\nEnables efficient storage and retrieval of dense vectors using HNSW\n\nOpenAI’s text-embedding-ada-002 model outputs dense vectors with 1536 entries\n\nExact and Approximate Nearest Neighbor search\nL2 distance, Inner Product, and Cosine Distance\nSupported inside AWS RDS",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-dock",
    "href": "qmd/db-postgres.html#sec-db-pstgr-dock",
    "title": "Postgres",
    "section": "Docker",
    "text": "Docker\n\nSteps\n\nStart docker desktop\nStart powershell\ndocker run --name pg_database -p 5432:5432 -e POSTGRES_PASSWORD=ericb2022 -d postgres:latest\n\n1st 5432 is local computer port\n2nd 5432 is the required postgres image port\n-e is for defining an environment variable; here its the db password that I set to ericb2022\n-d\n\nRuns the container in the background\nAllows you to run commands in the same terminal window that you used the container run command in\n\n“postgres:latest” is the name of the image to build the container from\n\nClose powershell\nIn docker desktop, the “pg_database” container should be running\n\nConnect to the db\n\nSteps\n\npsql should be in your list of path environment variables\n\nRight-click Start &gt;&gt; System &gt;&gt; advanced settings (right panel) &gt;&gt; environment variables &gt;&gt; highlight path &gt;&gt; edit\n“C:\\Program Files\\PostgreSQL\\14\\bin”\n\n** Note the “14” in the path which is the current version. Therefore, when postgres is updated, this path will have to be updated **\n\n\npsql --host localhost --port 5432 --dbname postgres --username postgres\n\nNote these are all default values, so this is equivalent to psql -U postgres\n–host (-h) is the ip address or computer name that you want to connect to\n\nlocalhost is for the docker container that’s running\n\n5432 is the default –port (-p) for a postgres container\n–dbname (-d) is the name of the database on the server\n\n“postgres” is a db that ships with postgres\n\n–username (-U) is a username that has permission to access the db\n\n“postgres” is the default super-user name\n\n\nA prompt will then ask you for that username’s password\n\nThe container above has the password ericb2022\n\nThis didn’t work for me, needed to use my postgres password that I set-up when I installed postgres and pgAdmin.\nMy local postgres server and the container are listening on the same port, so maybe if I changed the first port number to something else, it would connect to the container.\n\n\nTo exit db, \\q\n\n\nCreate a db\n\nSteps\n\ncreatedb -h localhost -p 5432 -U postgres -O eric two_trees\n\n-U is the user account used to create the db\n-O is used to assign ownership to another user account\n\n“role” (i.e. user account) must already exist\n\n“two_trees” is the name of the new db\nYou will be prompted for user’s password\n\nList of dbs on the server\n\npsql -h localhost -p 5432 -U postgres -l\n\n-l lists all dbs on server\nYou will be prompted for user’s password\n\n\n\n\nRun a sql script\n\npsql -d acweb -f test.sql\n\n-d is for the database name (e.g. acweb)\n-f is for running a file (e.g. test.sql)\n\n\nAdd users\n\nCreate user/role (once inside db)\nCREATE USER &lt;user name1&gt;;\nCREATE ROLE &lt;user name2&gt;;\nALTER ROLE &lt;user name2&gt; LOGIN\n\nCREATE USER will give the user login attribute/permission while CREATE ROLE will not\n\nALTER ROLE gives the user attributes/permissions (e.g. login permission)\n\nCreate user/role (at the CLI) - createuser &lt;user name&gt;",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-pgadm",
    "href": "qmd/db-postgres.html#sec-db-pstgr-pgadm",
    "title": "Postgres",
    "section": "pgAdmin",
    "text": "pgAdmin\n\nCreate a server\n\nRight-click on servers &gt;&gt; create &gt;&gt; server\n\nGeneral tab &gt;&gt; enter name\nConnection tab\n\nHost name/address: computer name or ip address where the server is running\n\nlocal: localhost or 127.0.0.1\n\nPort: default = 5432\nMaintenance database: db you want to connect to\n\nIf you haven’t created it yet, just use default “postgres” which autmatically created during installation\n\nusername/password\n\nu: default is postgres\np: installation password\nTick Save password\n\n\nClick Save\n\n\nCreate a db\n\nRight-click databases &gt;&gt; create &gt;&gt; databases &gt;&gt; enter name (lowercase) and click save\n\nCreate a table\n\nVia gui\n\nClick db name &gt;&gt; schema &gt;&gt; public &gt;&gt; right-click tables &gt;&gt; create &gt;&gt; tables\nGeneral tab\n\nEnter the table name (lower case)\n\nColumns tab\n\nEnter name, data type, whether there should be a “Not Null” constraint, and whether it’s a primary key\nAdd additional column with “+” icon in upper right\nIf you’re going to fill the table with a .csv file, make sure the column names match\n\nClick save\nTable will be located at db name &gt;&gt; schema &gt;&gt; public &gt;&gt; tables\n\nVia sql\n\nOpen query tool\n\nRight-click  or Schemas or Tables &gt;&gt; query tool\nClick Tools menu dropdown (navbar) &gt;&gt; query tool\n\nRun CREATE TABLE statement\n\nIf you don’t include the schema as part of the table name, pgadmin automatically places it into the “public” schema directory (e.g. public.table_name)\n\n\n\nImport csv into an empty table\n\nMake sure the column names match\nRight-click table name &gt;&gt; import/export\nOptions tab\n\nMake sure import is selected\nSelect the file\nIf you have column names in your csv, select Yes for Header\nSelect “,” for the Delimiter\n\nColumns tab\n\nCheck to make sure all the column names are there\n\nClick OK\n\nQuery Table\n\nRight-click table &gt;&gt; query editor\nQuery editor tab\n\nType query &gt;&gt; click ▶ to run query",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-rds",
    "href": "qmd/db-postgres.html#sec-db-pstgr-rds",
    "title": "Postgres",
    "section": "AWS RDS",
    "text": "AWS RDS\n\nMisc\n\nNotes from Create an RDS Postgres Instance and connect with pgAdmin\n\nSteps\n\nSearch AWS services for “RDS” (top left navbar)\nCreate Database\n\nClick “Create Database”\n\nCreate Database\n\nChoose Standard create or Easy Create\n\nEasy Create - uses “best practices” settings\n\nSelect postgres\n\nAlso available: Amazon Aurora, MySQL, MariaDB, Oracle, Microsoft SQL Server\n\nTemplates\n\nProduction\n\nMulti-AZ Deployment - Multiple Availability Zones\nProvisioned IOPS Storage - Increased output\n\nDev/Test\nRree tier\n\n750 hrs of Amazon RDS in a Single-AZ db.t2.micro Instance.\n20 GB of General Purpose Storage (SSD).\n20 GB for automated backup storage and any user-initiated DB Snapshots.\n\nRDS pricing page\n\nSettings\n\nDB Instance Identifier - enter name\nSet master username, master username password\n\nDB Instance\n\ndb.t3.micro or db.t4g.micro for free tier\n\ndev/test, production has many other options\n\n\nStorage\n\nDefaults: SSD with 20GB\nAutoscaling can up the storage capacity to a default 1000GB",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-py",
    "href": "qmd/db-postgres.html#sec-db-pstgr-py",
    "title": "Postgres",
    "section": "Python",
    "text": "Python\n\n{{psycopg2}}\n\nMisc\n\nNotes from Fastest Way to Load Data Into PostgreSQL Using Python\ntl;dr\n\nLarge Data: use copy_to\nMedium to Small Data:\n\nTime and memory isn’t an issue: Use extract_values or maybe copy_to if you don’t have JSON.\n\n\n\nConnect to db\nimport psycopg2\n\nconnection = psycopg2.connect(\n    host=\"localhost\",\n    database=\"testload\",\n    user=\"haki\",\n    password=None,\n)\nconnection.autocommit = True\nCreate a table\ndef create_staging_table(cursor) -&gt; None:\n    cursor.execute(\"\"\"\n        DROP TABLE IF EXISTS staging_beers;\n        CREATE UNLOGGED TABLE staging_beers (\n            id                  INTEGER,\n            name                TEXT,\n            tagline             TEXT,\n            first_brewed        DATE,\n            description         TEXT,\n            image_url           TEXT,\n            abv                 DECIMAL,\n            ibu                 DECIMAL,\n            target_fg           DECIMAL,\n            target_og           DECIMAL,\n            ebc                 DECIMAL,\n            srm                 DECIMAL,\n            ph                  DECIMAL,\n            attenuation_level   DECIMAL,\n            brewers_tips        TEXT,\n            contributed_by      TEXT,\n            volume              INTEGER\n        );\n    \"\"\")\n\nwith connection.cursor() as cursor:\n  create_staging_table(cursor)\n\nThe function receives a cursor and creates a unlogged table called staging_beers.\n\nInsert many rows at once\n\nNotes from Fastest Way to Load Data Into PostgreSQL Using Python\nThe best way to load data into a database is using the copy command (last method in this section). The issue here is that copy needs a .csv file and not json.\n\nThis might be an issue just because of psycopg2 library doesn’t support json or that there is a postgres extension that isn’t supported by the library. This also might not be a problem in the future.\n\nData\nbeers = iter_beers_from_api()\nnext(beers)\n{'id': 1,\n 'name': 'Buzz',\n 'tagline': 'A Real Bitter Experience.',\n 'first_brewed': '09/2007',\n 'description': 'A light, crisp and bitter IPA brewed...',\n 'image_url': 'https://images.punkapi.com/v2/keg.png',\n 'abv': 4.5,\n 'ibu': 60,\n 'target_fg': 1010,\n...\n}\nnext(beers)\n{'id': 2,\n 'name': 'Trashy Blonde',\n 'tagline': \"You Know You Shouldn't\",\n 'first_brewed': '04/2008',\n 'description': 'A titillating, ...',\n 'image_url': 'https://images.punkapi.com/v2/2.png',\n 'abv': 4.1,\n 'ibu': 41.5,\n ...\n }\n\nData is from beers api\niter_beers_from_api is a udf that takes the json from the api and creates a generator object that iterates through each beer.\n\nInsert data in db using execute_values (low memory usage and still pretty fast)\ndef insert_execute_values_iterator(\n    connection,\n    beers: Iterator[Dict[str, Any]],\n    page_size: int = 100,\n) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        psycopg2.extras.execute_values(cursor, \"\"\"\n            INSERT INTO staging_beers VALUES %s;\n        \"\"\", ((\n            beer['id'],\n            beer['name'],\n            beer['tagline'],\n            parse_first_brewed(beer['first_brewed']),\n            beer['description'],\n            beer['image_url'],\n            beer['abv'],\n            beer['ibu'],\n            beer['target_fg'],\n            beer['target_og'],\n            beer['ebc'],\n            beer['srm'],\n            beer['ph'],\n            beer['attenuation_level'],\n            beer['brewers_tips'],\n            beer['contributed_by'],\n            beer['volume']['value'],\n        ) for beer in beers), page_size=page_size)\n\ninsert_execute_values_iterator(page_size=1000)\n\nparse_first_brewed is a udf that transforms a date string to datetime type.\nbeer[‘volume’][‘value’]: Data is in json and the value for volume is subsetted from the nested field.\nBenchmark: At page_size = 1000, 1.468s, 0.0MB of RAM used\nThe generator((bear['id'], … , bear['volume']['value'], for beer in beers) keeps data from being stored in memory during transformation\npage_size: maximum number of arglist items to include in every statement. If there are more items the function will execute more than one statement.\n\nHere arglist is the data in the form of generator\n\n\nInsert data in db using copy_from (Fast but memory intensive)\nimport io\n\ndef clean_csv_value(value: Optional[Any]) -&gt; str:\n    if value is None:\n        return r'\\N'\n    return str(value).replace('\\n', '\\\\n')\n\ndef copy_stringio(connection, beers: Iterator[Dict[str, Any]]) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        csv_file_like_object = io.StringIO()\n        for beer in beers:\n            csv_file_like_object.write('|'.join(map(clean_csv_value, (\n                beer['id'],\n                beer['name'],\n                beer['tagline'],\n                parse_first_brewed(beer['first_brewed']),\n                beer['description'],\n                beer['image_url'],\n                beer['abv'],\n                beer['ibu'],\n                beer['target_fg'],\n                beer['target_og'],\n                beer['ebc'],\n                beer['srm'],\n                beer['ph'],\n                beer['attenuation_level'],\n                beer['contributed_by'],\n                beer['brewers_tips'],\n                beer['volume']['value'],\n            ))) + '\\n')\n        csv_file_like_object.seek(0)\n        cursor.copy_from(csv_file_like_object, 'staging_beers', sep='|')\n\nclean_csv_value: Transforms a single value\n\nEscape new lines: some of the text fields include newlines, so we escape \\n -&gt; \\\\n.\nEmpty values are transformed to \\N: The string \"\\N\" is the default string used by PostgreSQL to indicate NULL in COPY (this can be changed using the NULL option).\n\ncsv_file_like_object: Generate a file like object using io.StringIO. A StringIO object contains a string which can be used like a file. In our case, a CSV file.\ncsv_file_like_object.write: Transform a beer to a CSV row\n\nTransform the data: transformations on first_brewed and volume are performed here.\nPick a delimiter: Some of the fields in the dataset contain free text with commas. To prevent conflicts, we pick “|” as the delimiter (another option is to use QUOTE).\n\n\nInsert data (streaming) in db using copy_from (Fastest and low memory but complicated, at least with json)\n\nBuffering function\nfrom typing import Iterator, Optional\nimport io\n\nclass StringIteratorIO(io.TextIOBase):\n    def __init__(self, iter: Iterator[str]):\n        self._iter = iter\n        self._buff = ''\n\n    def readable(self) -&gt; bool:\n        return True\n\n    def _read1(self, n: Optional[int] = None) -&gt; str:\n        while not self._buff:\n            try:\n                self._buff = next(self._iter)\n            except StopIteration:\n                break\n        ret = self._buff[:n]\n        self._buff = self._buff[len(ret):]\n        return ret\n\n    def read(self, n: Optional[int] = None) -&gt; str:\n        line = []\n        if n is None or n &lt; 0:\n            while True:\n                m = self._read1()\n                if not m:\n                    break\n                line.append(m)\n        else:\n            while n &gt; 0:\n                m = self._read1(n)\n                if not m:\n                    break\n                n -= len(m)\n                line.append(m)\n        return ''.join(line)\n\nThe regular io.StringIO creates a file-like object but is memory-heavy. This function creates buffer that will feed each line of the file into a buffer, stream it to copy, empty the buffer, and load the next line.\n\nCopy to db\ndef clean_csv_value(value: Optional[Any]) -&gt; str:\n    if value is None:\n        return r'\\N'\n    return str(value).replace('\\n', '\\\\n')\n\ndef copy_string_iterator(connection, beers: Iterator[Dict[str,\nAny]]) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        beers_string_iterator = StringIteratorIO((\n            '|'.join(map(clean_csv_value, (\n                beer['id'],\n                beer['name'],\n                beer['tagline'],\n                parse_first_brewed(beer['first_brewed']).isoformat(),\n                beer['description'],\n                beer['image_url'],\n                beer['abv'],\n                beer['ibu'],\n                beer['target_fg'],\n                beer['target_og'],\n                beer['ebc'],\n                beer['srm'],\n                beer['ph'],\n                beer['attenuation_level'],\n                beer['brewers_tips'],\n                beer['contributed_by'],\n                beer['volume']['value'],\n            ))) + '\\n'\n            for beer in beers\n        ))\n        cursor.copy_from(beers_string_iterator, 'staging_beers', sep='|')\n\nSimilar to other code above",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#distributed-architectures",
    "href": "qmd/db-postgres.html#distributed-architectures",
    "title": "Postgres",
    "section": "Distributed Architectures",
    "text": "Distributed Architectures\n\nMisc\n\nNotes from An Overview of Distributed PostgreSQL Architectures\nFeatures to achieve single node availability, durability, and performance - Replication - Place copies of data on different machines - Distribution - Place partitions of data on different machines - Decentralization - Place different DBMS activities on different machines\nIf transactions take on average 20ms, then a single (interactive) session can only do 50 transactions per second. You then need a lot of concurrent sessions to actually achieve high throughput. Having many sessions is not always practical from the application point-of-view, and each session uses significant resources like memory on the database server. Most PostgreSQL set ups limit the maximum number of sessions in the hundreds or low thousands, which puts a hard limit on achievable transaction throughput when network latency is involved.\n\nNetwork-Attached Block Storage (e.g. EBS)\n\n\nCommon technique in cloud-based architectures\nDatabase server typically runs in a virtual machine in a Hypervisor, which exposes a block device to the VM. Any reads and writes to the block device will result in network calls to a block storage API. The block storage service internally replicates the writes to 2-3 storage nodes.\nPros\n\nHigher durability (replication)\nHigher uptime (replace VM, reattach)\nFast backups and replica creation (snapshots)\nDisk is resizable\n\nCons\n\nHigher disk latency (~20μs -&gt; ~1000μs)\nLower IOPS (~1M -&gt; ~10k IOPS)\nCrash recovery on restart takes time\nCost can be high\n\nGuideline: The durability and availability benefits of network-attached storage usually outweigh the performance downsides, but it’s worth keeping in mind that PostgreSQL can be much faster.\n\nRead Replicas\n\n\nThe most common way of using a replica is to set it up as a hot standby that takes over when the primary fails in a high availability set up.\nHelps you scale read throughput when reads are CPU or I/O bottlenecked by load balancing queries across replicas, which achieves linear scalability of reads and also offloads the primary, which speeds up writes!\n\nThe primary usually does not wait for replication when committing a write, which means read replicas are always slightly behind. That can become an issue when your application does a read that, from the user’s perspective, depends on a write that happened earlier.\nFor example, a user clicks “Add to cart”, which adds the item to the shopping cart and immediately sends the user to the shopping cart page. If reading the shopping cart contents happens on the read replica, the shopping cart might then appear empty. Hence, you need to be very careful about which reads use a read replica.\n\nWhen load balancing between different nodes, clients might repeatedly get connected to different replica and see a different state of the database\nPowerful tool for scaling reads, but you should consider whether your workload is really appropriate for it.\nPros\n\nRead throughput scales linearly\nLow latency stale reads if read replica is closer than primary\nLower load on primary\n\nCons\n\nEventual read-your-writes consistency\nNo monotonic read consistency\nPoor cache usage\n\nGuideline: Consider using read replicas when you need &gt;100k reads/sec or observe a CPU bottleneck due to reads, best avoided for dependent transactions and large working sets.\n\nDBMS-Optimized Cloud Storage\n\n\nWhere DBMS is Database Management Software. (e.g. Aurora)\nPostgreSQL is not optimized for this architecture\nWhile the theory behind DBMS-optimized storage is sound. In practice, the performance benefits are often not very pronounced (and can be negative), and the cost can be much higher than regular network-attached block storage. It does offer a greater degree of flexibility to the cloud service provider, for instance in terms of attach/detach times, because storage is controlled in the data plane rather than the hypervisor.\nPros\n\nPotential performance benefits by avoiding page writes from primary\nReplicas can reuse storage, incl. hot standby\nCan do faster reattach, branching than network-attached storage\n\nCons\n\nWrite latency is high by default\nHigh cost / pricing\nPostgreSQL is not designed for it, not OSS\n\nGuideline: Can be beneficial for complex workloads, but important to measure whether price-performance under load is actually better than using a bigger machine.\n\nActive-Active (e.g. BDR)\n\n\nAny node can locally accept writes without coordination with other nodes.\nIt is typically used with replicas in multiple sites, each of which will then see low read and write latency, and can survive failure of other sites.\nActive-active systems do not have a linear history, even at the row level, which makes them very hard to program against.\nPros\n\nVery high read and write availability\nLow read and write latency\nRead throughput scales linearly\n\nCons\n\nEventual read-your-writes consistency\nNo monotonic read consistency\nNo linear history (updates might conflict after commit)\n\nGuideline: Consider only for very simple workloads (e.g. queues) and only if you really need the benefits.\n\nTransparent Sharding (e.g. Citus)\n\n\nTables distributed and/or replicated across multiple primary nodes using a “shard key .”\n\nEach node shows the distributed tables as if they were regular PostgreSQL tables and queries\n\nData are located in “shards” which are regular PostgreSQL tables. Joins and foreign keys that include the shard key can be performed locally.\nScaling out transactional workloads is most effective when queries have a filter on the shard key, such that they can be routed to a single shard group (e.g. single tenant in a multi-tenant app) or compute-heavy analytical queries that can be parallelized across the shards (e.g. time series / IoT).\nWhen loading data, use COPY, instead of INSERT, to avoid waiting for every row.\nPros\n\nScale throughput for reads & writes (CPU & IOPS)\nScale memory for large working sets\nParallelize analytical queries, batch operations\n\nCons\n\nHigh read and write latency\nData model decisions have high impact on performance\nSnapshot isolation concessions\n\nGuideline: Use for multi-tenant apps, otherwise use for large working set (&gt;100GB) or compute heavy queries.\n\nDistributed Key-Value Stores With SQL (e.g. Yugabyte)\n\n\nA bunch of complicated stuff I don’t understand 😅\nTables are stored in the key-value store, with the key being a combination of the table ID and the primary key.\nBetter to use PostgresSQL without this architecture.\nPros\n\nGood read and write availability (shard-level failover)\nSingle table, single key operations scale well\nNo additional data modeling steps or snapshot isolation concessions\n\nCons\n\nMany internal operations incur high latency\nNo local joins in current implementations\nNot actually PostgreSQL, and less mature and optimized\n\nGuideline: Just use PostgreSQL. For simple applications, the availability and scalability benefits can be useful.",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/renv.html",
    "href": "qmd/renv.html",
    "title": "Renv",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Renv"
    ]
  },
  {
    "objectID": "qmd/renv.html#sec-renv-misc",
    "href": "qmd/renv.html#sec-renv-misc",
    "title": "Renv",
    "section": "",
    "text": "Bruno Rodrigues posts on reproducibility\n\nMRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?\n\ntl;dr:\n\nI want to start a project and make it reproducible.\n\n{renv} and Docker\n\nThere’s an old script laying around that I want to run.\n\n{groundhog} and Docker\n\nI want to work inside an environment that enables me to run code in a reproducible way.\n\nDocker and the Posit CRAN mirror.\n\n\n\nCode longevity of the R programming language\n\nAlternatives\n\nGNU Guix\n\nSee Bruno’s “MRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?” post for his thoughts\n\n\nPosit Package Manager - Helps you install binary packages from a certain date\n\ne.g. Adding to .Rprofile “options(repos = c(CRAN = \"https://packagemanager.posit.co/cran/2024-02-06\"))\nMight be able to keep a separate .Rprofie in the project directory.\n\nInstall from github\nrenv::install(\"eddelbuettel/digest\")",
    "crumbs": [
      "Renv"
    ]
  },
  {
    "objectID": "qmd/renv.html#sec-renv-actrest",
    "href": "qmd/renv.html#sec-renv-actrest",
    "title": "Renv",
    "section": "Activating/Restoring a Project",
    "text": "Activating/Restoring a Project\n\nMisc\n\n** Restoring an environment comes with a few caveats: **\n\nFirst of all, renv does not install a different version of R if the recorded and current version disagree. This is a manual step and up to the user.\n\n{rig} can make switching between R versions easy\n\nThe same is true for packages with external dependencies. Those libraries, their headers and binaries also need to be installed by the user in the correct version, which is not recorded in the lockfile.\nFurthermore renv supports restoring packages installed from git repositories, but fails if the user did not install git beforehand.\n\nhttps://rstudio.github.io/renv/articles/faq.html#im-returning-to-an-older-renv-project-what-do-i-do\n\nInit steps (updates lockfile to either latest pkg versions or what you have locally)\n\nRun renv::init()\nChoose “Discard the lockfile and re-initialize the project”\n\nRestore steps (looking to reproduce the original project results)\n\nRun renv::restore()\nUpdate any packages with outside dependencies\n\nRMarkdown depends on pandoc which is usually installed by installing RStudio. So if you updated RStudio, then the rmarkdown version in the project’s lockfile may not work with your current pandoc version. You’ll have to update rmarkdown (or revert your pandoc version… shhhyea, as if)\n\n\nIssues\n\nA pkg installation fails (and therefore restore fails) because some pkg requires another pkg to be a more up-to-date version or some other reason\n\n**If it’s something with a shit ton of dependencies like rstanarm, you might as well use renv::init( ) method**\nExamples:\n\n{pkgload} failed to install because I had rlang v.4.0.6 instead of &gt; v4.09\n{RCurl} failed because it couldn’t find some obsure file\n\nSolution (update the package):\n\nupdate(\"pkg\") (or go to the github and search for the latest stable version) to see what the latest stable version is.\nrenv::modify() allows you edit the lockfile. Change pkg version in lockfile to that latest version\nrerun renv::restore()\nRepeat as necessary",
    "crumbs": [
      "Renv"
    ]
  },
  {
    "objectID": "qmd/renv.html#sec-renv-font",
    "href": "qmd/renv.html#sec-renv-font",
    "title": "Renv",
    "section": "Installing Fonts",
    "text": "Installing Fonts\n\nUsing a package like extrafont, it won’t find any fonts installed so you have to point it to the system path\nSteps\n\nInstall {systemfonts}\nRun systemfonts::match_font(\"&lt;name of font you have installed&gt;\") and copy path (don’t include file (i.e. .ttf))\nRun extrafont::",
    "crumbs": [
      "Renv"
    ]
  },
  {
    "objectID": "qmd/renv.html#using-local-directory-with-package-tarbell-to-install-a-package",
    "href": "qmd/renv.html#using-local-directory-with-package-tarbell-to-install-a-package",
    "title": "Renv",
    "section": "Using Local directory with Package Tarbell to Install a Package",
    "text": "Using Local directory with Package Tarbell to Install a Package\n\nNeeded to install an old XML package version that wasn’t available through CRAN mirror. Errored looking libxml parser. Discussion says you need to use libxml2.\nResources\n\nrenv using local package directory\n\nhttps://rstudio.github.io/renv/articles/local-sources.html\n\nInstructions on compiling old XML package with rtools40\n\nhttps://github.com/r-windows/rtools-installer/issues/3\nhttps://github.com/r-windows/checks/issues/5#issue-335598042\n\nInstall libxml2 library\n\nhttps://github.com/r-windows/docs/blob/master/packages.md#xml\n\nFinding pacman package manager to install libxml2\n\nhttps://github.com/r-windows/docs/blob/master/rtools40.md#readme\n\n\nIn the project directory, create renv/local directory. Then, download/move the package version’s tar.gz file to that “local” folder\n&gt; Sys.setenv(LIB_XML = \"$(MINGW_PREFIX)\")\n&gt; Sys.setenv(LOCAL_CPPFLAGS = \"-I/mingw$(WIN)/include/libxml2\")\n&gt; install.packages('renv/local/XML_3.99-0.3.tar.gz', repos = NULL, type = 'source')",
    "crumbs": [
      "Renv"
    ]
  },
  {
    "objectID": "qmd/renv.html#errors",
    "href": "qmd/renv.html#errors",
    "title": "Renv",
    "section": "Errors",
    "text": "Errors\n\nFails to retrieve package\n\nSolutions:\n\nInstall from github\nrenv::install(\"eddelbuettel/digest\")\nRevert to previous version\nremotes::install_version(\"cachem\", version = \"1.0.3\", repos = \"http://cran.us.r-project.org\")\nrenv::install(\"cachem@1.0.3\")",
    "crumbs": [
      "Renv"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-syntax",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-syntax",
    "title": "Quarto",
    "section": "Syntax",
    "text": "Syntax\n\nInline code\n-   Total number of counties: **`{r} polling_places |&gt; filter(state == \"Alabama\") |&gt; distinct(county_name) |&gt; count()`**\n-   Total number of polling places: **`{r} polling_places |&gt; filter(state == \"Alabama\") |&gt; count()`**\n-   Election Day: **`{r} polling_places |&gt; filter(state == \"Alabama\") |&gt; pull(election_date) |&gt; unique()`**\nAlign code chunk under bullet and add indented comment below chunk\n-   [Example]{.ribbon-highlight} (using a SQL Query; method 1)\n\n    ``` r\n    # open dataset\n    ds &lt;- arrow::open_dataset(dir_out, partitioning = \"species\")\n    # open connection to DuckDB\n    con &lt;- dbConnect(duckdb::duckdb())\n    # register the dataset as a DuckDB table, and give it a name\n    duckdb::duckdb_register_arrow(con, \"my_table\", ds)\n    # query\n    dbGetQuery(con, \"\n      SELECT sepal_length, COUNT(*) AS n\n      FROM my_table\n      WHERE species = 'species=setosa'\n      GROUP BY sepal_length\n    \")\n\n    # clean up\n    duckdb_unregister(con, \"my_table\")\n    dbDisconnect(con)\n    ```\n\n    -   filtering using a partition, the WHERE format is '\\&lt;partition_variable\\&gt;=\\&lt;partition_value\\&gt;'\n\nSpace between bullet and top ticks\nSpace between bottom ticks and bullet\nNote alignment of text\n\nAdd Code Annotations\n-   [Partition a large file and write to arrow format]{.underline}\n\n    ``` r\n    lrg_file &lt;- open_dataset(&lt;file_path&gt;, format = \"csv\") # &lt;1&gt;\n    lrg_file %&gt;%\n        group_by(var) %&gt;% # &lt;2&gt;\n        write_dataset(&lt;output_dir&gt;, format = \"feather\") # &lt;3&gt;\n    ```\n\n    1.  Pass the file path to `open_dataset()`\n\n    2.  Use `group_by()` to partition the Dataset into manageable chunks\n\n    3.  Use `write_dataset()` to write each chunk to a separate Parquet file---all without needing to read the full CSV file into R\n\n    -   `open_dataset` is fast because it only reads the metadata of the file system to determine how it can construct queries\nFootnote\nwords [^1]\n\n[^1]: Data from https://github.com/rfordatascience/tidytuesday\nFor PDF output, you need pagebreaks:\n{{&lt; pagebreak &gt;}}",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-yaml",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-yaml",
    "title": "Quarto",
    "section": "YAML",
    "text": "YAML\n\nSet global chunk options in yaml\n\n\nFor code cells\nexecute:\n  echo: false\n  message: false\n  warning: false\n\nEnable Margin Notes\n---\n# YAML front matter\nreference-location: margin\n---\n!expr to render code within chunk options\n\ne.g. figure caption: #| fig-cap: !expr glue::glue(\"The mean temperature was {mean(airquality$Temp) |&gt; round()}\")\n\ncolumn: screen-inset yaml markup is used to show a very wide table\nIf you haven’t set your Quarto document to be self-contained, then the images have also already been saved for you - probably in a folder called documentname_files/figure-html/\nformat: \n  html:\n    embed-resources: true\nDate first published and date modified using the current date:\n---\ndate: 2024-01-01\ndate-modified: today\n---\nYAML Examples\n\nExample",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-chunk",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-chunk",
    "title": "Quarto",
    "section": "Chunk Options",
    "text": "Chunk Options\n\nGraphics\n\nCode Chunk\n#| label: \"fig-statemap\"\n#| dpi: 300\n#| fig.height: 7.2\n#| fig.width: 3.6\n#| dev: \"png\"\n#| echo: false\n#| warning: false\n#| message: false\n\nExample shows settings for a graph for mobile\nfig.height and fig.width are always given in inches\n\nReference Figure\n1 See polling place locations in @fig-statemap.\n\nConditional Code Chunk Evaluation\n\nExample: document output type\n\nSet value in a code chunk\n```{r setup}\n# Include in first chunk of .qmd\n# Get output file type\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\n```\nUse !expr sytax to determine evaluation status\n\nExample: eval chunk based on output type\n```{r}\n#| eval: !expr out_type == \"html\"\n\n# code to create interactive {plotly}\n```\n\n```{r}\n#| eval: !expr out_type == \"docx\"\n\n# code to create static {ggplot2}\n```\n\n\nExample: Use parameterization to set value\n---\ntitle: \"test\"\nformat: html\nparams:\n  my_value: false\n---\n\nmy_value can then be used throughout the document to determine chunk evaluation status\n\n\nKnitr Hooks\n\nNotes from Writing knitr hooks\n\nAlso has a knitr hook example that alters cell output (e.g. only prints 4 lines of a vector)\n\nChunk Hooks\n\nChunk hooks get called twice: once before knitr executes the code in the chunk, and once again afterwards\nThe function can take up to four arguments, all of which are optional:\n\nbefore: A logical value indicating whether the function is being called before or after the code chunk is executed\noptions: The list of chunk options\nenvir: The environment in which the code chunk is executed\nname: The name of the code chunk option that triggered the hook function\n\nThe chunk hook is called for its side effects not the return value. However, if it returns a character output, knitr will add that output to the document output as-is.\nExample: Chunk Timer\n\nCode\ncreate_timer_hook &lt;- function() {\n  start_time &lt;- NULL\n  function(before, options) {\n    if (before) {\n      start_time &lt;&lt;- Sys.time()\n    } else {\n      stop_time &lt;- Sys.time()\n      elapsed &lt;- difftime(stop_time, start_time, units = \"secs\")\n      paste(\n        \"&lt;div style='font-size: 70%; text-align: right'&gt;\",\n        \"Elapsed time:\", \n        round(elapsed, 2), \n        \"secs\",\n        \"&lt;/div&gt;\"\n      )\n    }\n  }\n}\nknitr::knit_hooks$set(timer = create_timer_hook())\n\nThe hook is triggered the first time (with before = TRUE) to record the system time somewhere (e.g., in a variable called start_time). Then, when the hook is triggered the second time (with before = FALSE), it records the system time again (e.g., as stop_time), and computes the difference in time.\n\nUse in a cell\n```{r}\n#| timer: true\nrunif(10000)\n```\nOutput",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-rpy",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-rpy",
    "title": "Quarto",
    "section": "R and Python",
    "text": "R and Python\n\nIf only R or R and Python, the notebook is rendered by {knitr}\nIf only Python, the notebook is rendered by jupyter\nSet-up\n\n{reticulate} automatically comes loaded in Quarto and it knows to use it when it sees a python block, so you don’t need to load the package\nQuarto will select a version of Python using the Python Launcher on Windows or system PATH on MacOS and Linux. You can override the version of Python used by Quarto by setting the QUARTO_PYTHON environment variable.\n\nIn CLI on Windows, type py is see which version the Python Launcher , and therefore Quarto, is using and py –list to see which versions are installed.\n\n\nR\n```{r}\n#| label: read-data\n#| echo: true\n#| message: false\n#| cache: true\nlemurs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-08-24/lemur_data.csv')\n```\nPython\n```{python}\n#| label: modelling \n#| echo: true \n#| message: false\n\nlemur_data_py = r.lemur_data \nimport statsmodels.api as sm \ny = lemur_data_py[[\"Weight\"]] \nx = lemur_data_py[[\"Age\"]] \nx = sm.add_constant(x) \nmod = sm.OLS(y, x).fit() \nlemur_data_py[\"Predicted\"] = mod.predict(x) \nlemur_data_py[\"Residuals\"] = mod.resid`\n```\n\nUse r. to access the data in the R chunk\nThe first execution of a python cell starts reticulate::repl_python() in the terminal\n\n(back to) R\n```{r}\n#| label: plotting \n#| echo: true \n#| output-location: slide \n#| message: false \n#| fig-align: center \n#| fig-alt: \"Scatter plot of predicted and residual values for the fitted linear model.\" \n\nlibrary(reticulate) \nlibrary(ggplot2) \nlemur_residuals &lt;- py$lemur_data_py \nggplot(data = lemur_residuals, aes(x = Predicted, y = Residuals)) +\n  geom_point(colour = \"#2F4F4F\") +\n  geom_hline(yintercept = 0,\n            colour = \"red\") +\n  theme(panel.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"),\n        plot.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"))\n```\n\nUse py$ to access the data in the Python chunk *\nMust call library(reticulate) in order for Quarto to recognize py$",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-lay",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-lay",
    "title": "Quarto",
    "section": "Layouts",
    "text": "Layouts\n\n2 cols (1 col: text, 1 col: image)\n\n::: {layout=\"[50,50]\"}\n\n::: column\nEvery Quarto project starts with a Quarto file that has the extension `.qmd`.\n\n\nThis particular one analyzes children's early words, but every `.qmd` includes the same three basic elements inside:\n\n\n- A block of metadata at the top, between two fences of `---`s. This is written in [YAML](https://learnxinyminutes.com/docs/yaml/). \n- Narrative text, written in [Markdown](https://commonmark.org/help/tutorial/). \n- Code chunks in gray between two fences of ```` ``` ````, written with R or another programming language.\n\n\nYou can use all three elements to develop your code and ideas in one reproducible document.\n:::\n\n![](img/01-source.png)\n:::\n2 figures, 2 columns (i.e. side-by-side) with captions at the top\n---\nfig-cap-location: top\n---\n\n-   Words\n    -   Predictions of Standard RF vs Oblique RF\n\n        ::: {layout-ncol=\"2\"}\n        ![Standard Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-axpred-1.png){fig-align=\"left\" width=\"432\"}\n\n        ![Oblique Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-oblpred-1.png){fig-align=\"left\" width=\"432\"}\n        :::\n\n        -   Words  \n\nfig-cap-location: bottom is default;\nfig-cap-location: margin is buggy, at least in for project type book. Captions are added to the margins but bullet points mysteriously disappear during rendering to html\n\n2 charts side-by-side extending past body margins\n```{r}\n#| label: my-figure\n#| layout-ncol: 2\n#| column: page\nggplot() + ...\nggplot() + ...\n```\n\n“layout-ncol” says 2 side-by-side columns\n“column: page” says extend column width to the width of the page\n\nNested Tabs",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-auto",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-auto",
    "title": "Quarto",
    "section": "Automation",
    "text": "Automation\n\nIteration and Parameterization\n\nNotes from\n\nVelásquez R-Ladies Nairobi: Code, Slides, Video\n\nIt involves having a “child” document as a template and running it repeatedly with different parameters\nThe “main” document includes the output from the child document\nRendering Options\n\nCLI: e.g. quarto render polling-places-report.qmd -P state:'California'\n{quarto}:\nquarto::quarto_render(\n  input = here::here(\"polling-places-report.qmd\"),\n  execute_params = list(state = \"California\")\n)\n\nExample: Create a report for each parameter value. In each report, use the parameter value (e.g. state) to iterate through a template file that makes a tables (1 for each county) based on that value.\nMain Report Document\n---\ntitle: \"Polling Places Report - `r params$state`\"\nparams:\n  state: \"California\"\n---\n\n```{r}\n#| results: hide\n\nlibrary(dplyr)\n\ncounties &lt;- polling_places |&gt; \n  filter(state == params$state) |&gt; \n  distinct(county_name) |&gt; \n  pull()\n\nexpanded_child &lt;- \n  counties |&gt; \n    purrr::map(\\(county) {\n      knitr::knit_expand(\"../_template.qmd\", \n                         current_county = county))\n      }|&gt; \n    purrr::flatten()\n\nparsed_child &lt;- knitr::knit_child(text = unlist(expanded_child))\n```\n\n`{r} parsed_child`\n\nThe document that gets published, emailed, etc.\nparams specified in YAML\n\nValue can also be used in the title of the document via inline R code\n\nEach county is iterated through the child document (_template.qmd) via current_county variable and knit_expand\nparsed_child is a list of the template file outputs.\nThen, parsed_child is converted to a character vector by unlist and all the results are printed in the document by the inline R code\n\nChild Document (i.e. Template)\n### {{current_county}} COUNTY\n\n-   Total Polling Places: `{r} polling_places |&gt; filter(state == params$state, county_name == \"{{current_county}}\") |&gt; count()`\n-   Example Locations:\n\n```{r}\npolling_places |&gt; \n  filter(state == params$state, \n         county_name == \"{{current_county}}\") |&gt; \n  head(6) |&gt; \n  select(name, address.x) |&gt; \n  kbl(format = \"markdown\")\n```\n\nParameter value is used to get county data and create tables for each.\nNo YAML is necessary in child document\n\nparams values are automatically available through knitr::knit_expand that’s executed in the Main document\n\nThe county variable is utilized by the template file using the double curly braces, {{current_county}}\nkbl outputs in markdown format so the table is correctly rendered in the Main document.\n\nRendering Script\npolling_places &lt;-\n  readr::read_csv(here::here(\"data\", \"geocoded_polling_places.csv\"))\n\n# create quarto::render arguments df\npolling_places_reports &lt;-\n  polling_places |&gt;\n  dplyr::distinct(state) |&gt;\n  dplyr::slice_head(n = 5) |&gt;\n  dplyr::mutate(\n    output_format = \"html\",\n    output_file = paste0(tolower(state),\n                         \"-polling-places\"),\n    execute_params = purrr::map(state,\n                                \\(state) list(state = state))\n  ) |&gt;\n  # default output is html, so that variable not selected\n  dplyr::select(output_file, execute_params) \n\n# iterate through args and create reports\npurrr::pwalk(\n  .l = polling_places_reports,\n  .f = quarto::quarto_render,\n  input = here::here(\"main_report_document.qmd\"),\n  .progress = TRUE\n)\n\nCreates a report for each params value (e.g. state)\nGenerates a dataframe for each set of arguments to be fed to quarto::quarto_render.",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-webr",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-webr",
    "title": "Quarto",
    "section": "WebR",
    "text": "WebR\n\nSet-Up\n\nInstall the extension alongside your blog post by running quarto add coatless/quarto-webr\nAdd the extension to your blog by adding filters: [\"webr\"] to your post’s frontmatter\nInstead of {r} code chunks, use {webr-r} ones\n\nInstall CRAN packages on page load\nfilters:\n  - \"webr\"\nwebr:\n  packages:\n  - \"dplyr\"\n  - \"tidyr\"\n  - \"purrr\"\n  - \"tibble\"\n  - \"crayon\"\n\nAdd to frontmatter\n\nInstall R-Universe Package\n```{webr-r}\n#| context: setup\nwebr::install(\"collateral\", repos = c(\"https://jimjam-slam.r-universe.dev\"))\n```\n\nR-Universe packages must be installed in code cells",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/db-vector.html",
    "href": "qmd/db-vector.html",
    "title": "Vector Databases",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Vector Databases"
    ]
  },
  {
    "objectID": "qmd/db-vector.html#sec-db-vect-misc",
    "href": "qmd/db-vector.html#sec-db-vect-misc",
    "title": "Vector Databases",
    "section": "",
    "text": "Vector databases store embeddings and provide fast similarity searches\nComparison (link)\n\n\nOpen-Source and hosted cloud: If you lean towards open-source solutions, Weviate, Milvus, and Chroma emerge as top contenders. Pinecone, although not open-source, shines with its developer experience and a robust fully hosted solution.\nPerformance: When it comes to raw performance in queries per second, Milvus takes the lead, closely followed by Weviate and Qdrant. However, in terms of latency, Pinecone and Milvus both offer impressive sub-2ms results. If nmultiple pods are added for pinecone, then much higher QPS can be reached.\nCommunity Strength: Milvus boasts the largest community presence, followed by Weviate and Elasticsearch. A strong community often translates to better support, enhancements, and bug fixes.\nScalability, advanced features and security: Role-based access control, a feature crucial for many enterprise applications, is found in Pinecone, Milvus, and Elasticsearch. On the scaling front, dynamic segment placement is offered by Milvus and Chroma, making them suitable for ever-evolving datasets. If you’re in need of a database with a wide array of index types, Milvus’ support for 11 different types is unmatched. While hybrid search is well-supported across the board, Elasticsearch does fall short in terms of disk index support.\nPricing: For startups or projects on a budget, Qdrant’s estimated $9 pricing for 50k vectors is hard to beat. On the other end of the spectrum, for larger projects requiring high performance, Pinecone and Milvus offer competitive pricing tiers.",
    "crumbs": [
      "Databases",
      "Vector Databases"
    ]
  },
  {
    "objectID": "qmd/db-vector.html#sec-db-vect-bran",
    "href": "qmd/db-vector.html#sec-db-vect-bran",
    "title": "Vector Databases",
    "section": "Brands",
    "text": "Brands\n\nQdrant - open source, free, and easy to use (example)\nChroma - can be used as a local in-memory (example)\nPinecone - Data Elixir is using this store for their chatbot; has a free tier\nPostgres with pgvector: Supports exact and approximate nearest neighbor search; L2 distance, inner product, and cosine distance; any language with a Postgres client\n\nAlso see Databases, PostgreSQL &gt;&gt; Extensions &gt;&gt; pgvector and pg_sparse for sparse embeddings (e.g. SPLADE)",
    "crumbs": [
      "Databases",
      "Vector Databases"
    ]
  },
  {
    "objectID": "qmd/scraping.html",
    "href": "qmd/scraping.html",
    "title": "Scraping",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-misc",
    "href": "qmd/scraping.html#sec-scrap-misc",
    "title": "Scraping",
    "section": "",
    "text": "Packages\n\n{rvest}\n{rselenium}\n{selenium}\n{selenider} - Wrapper functions around {chromote} and {selenium} functions that utilize lazy element finding and automatic waiting to make scraping code more reliable\n{shadowr} - For shadow DOMs\n\nIn loops, use Sys.sleep (probably) after EVERY selenium function. Sys.sleep(1) might be all that’s required. ({selenider} fixes this problem)\n\nsee Projects &gt; foe &gt; gb-level-1_9-thread &gt; scrape-gb-levels.R\nMight not always be needed, but absolutely need if you’re filling out a form and submitting it.\nMight even need one at the top of the loop\nIf a Selenium function stops working, adding Sys.sleeps are worth a try.\n\nSometimes clickElement( ) stops working for no apparent reason. When this happens used sendKeysToElement(list(\"laptops\",key=\"enter\"))\nIn batch scripts (.bat), sometimes after a major windows update, the Java that selenium uses will trigger Windows Defender (WD) and cause the scraping script to fail (if you have it scheduled). If you run the .bat script manually and then when the WD box rears its ugly head, just click ignore. WD should remember after that and not to mess with it.\nRSelenium findElement(using = \"\") options “class name” : Returns an element whose class name contains the search value; compound class names are not permitted.\n\n“css selector” : Returns an element matching a CSS selector.\n“id” : Returns an element whose ID attribute matches the search value.\n“name” : Returns an element whose NAME attribute matches the search value.\n“link text” : Returns an anchor element whose visible text matches the search value.\n“partial link text” : Returns an anchor element whose visible text partially matches the search value.\n“tag name” : Returns an element whose tag name matches the search value.\n“xpath” : Returns an element matching an XPath expression.",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-terms",
    "href": "qmd/scraping.html#sec-scrap-terms",
    "title": "Scraping",
    "section": "Terms",
    "text": "Terms\n\nStatic Web Page: A web page (HTML page) that contains the same information for all users. Although it may be periodically updated, it does not change with each user retrieval.\nDynamic Web Page: A web page that provides custom content for the user based on the results of a search or some other request. Also known as “dynamic HTML” or “dynamic content”, the “dynamic” term is used when referring to interactive Web pages created for each user.",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-rvest",
    "href": "qmd/scraping.html#sec-scrap-rvest",
    "title": "Scraping",
    "section": "rvest",
    "text": "rvest\n\nMisc\n\nNotes from: Pluralsight.Advanced.Web.Scraping.Tactics.R.Playbook\n\nUses css selectors or xpath to find html nodes\nlibrary(rvest)\npage &lt;- read_html(\"&lt;url&gt;\")\nnode &lt;- html_element(page, xpath = \"&lt;xpath&gt;\"\n\nFind css selectors\n\nselector gadget\n\nclick selector gadget app icon in Chrome in upper right assuming you’ve installed it already\nclick item on webpage you want to scrape\n\nit will highlight other items as well\n\nclick each item you DON’T want to deselect it\ncopy the selector name in box at the bottom of webpage\nUse html_text to pull text or html_attr to pull a link or something\n\ninspect\n\nright-click item on webpage\nclick inspect\nhtml element should be highlighted in elements tab of right side pan\nright-click element –&gt; copy –&gt; copy selector or copy xpath\n\n\n\nExample: Access data that needs authentication (also see RSelenium version)\n\nnavigate to login page\nsession &lt;- session(\"&lt;login page url&gt;\")\nFind “forms” for username and password\nform &lt;- html_form(session)[[1]]\nform\n\nEvidently there are multiple forms on a webpage. He didn’t give a good explanation for why he chose the first one\n“session_key” and “session_password” are the ones needed\n\nFill out the necessary parts of the form and send it\nfilled_form &lt;- html_form_set(form, session_key = \"&lt;username&gt;\", session_password = \"&lt;password&gt;\")\nfilled_form # shows values that inputed next the form sections\nlog_in &lt;- session_submit(session, filled_form)\nConfirm that your logged in\nlog_in # prints url status = 200, type = text/html, size = 757813 (number of lines of html on page?)\nbrowseURL(log_in$url) # think this maybe opens browser\n\nExample: Filter a football stats table by selecting values from a dropdown menu on a webpage (also see RSelenium version)\n\nAfter set-up and navigating to url, get the forms from the webpage\nforms &lt;- html_form(session)\nforms # prints all the forms\n\nThe fourth has all the filtering menu categories (team, week, position, year), so that one is chosen\n\nFill out the form to enter the values you want to use to filter the table and submit that form to filter the table\nfilled_form &lt;- html_form_set(forms[[4]], \"team\" = \"DAL\", \"week\" = \"all\", \"position\" = \"QB\", \"year\" = \"2017\")\nsubmitted_session &lt;- session_submit(session = session, form = filled_form)\nLook for the newly filtered table\ntables &lt;- html_elements(submitted_session, \"table\")\ntables\n\nUsing inspect, you can see the 2nd one has &lt;table class = “sortable stats-table…etc\n\nSelect the second table and convert it to a dataframe\nfootball_df &lt;- html_table(tables[[2]], header = TRUE)",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-rsel",
    "href": "qmd/scraping.html#sec-scrap-rsel",
    "title": "Scraping",
    "section": "RSelenium",
    "text": "RSelenium\n\nAlong with installing package you have to know the version of the browser driver of the browser you’re going to use\n\nhttps://chromedriver.chromium.org/downloads\nFind Chrome browser version\n\nThrough console\nsystem2(command = \"wmic\",\n        args = 'datafile where name=\"C:\\\\\\\\Program Files         (x86)\\\\\\\\Google\\\\\\\\Chrome\\\\\\\\Application\\\\\\\\chrome.exe\" get Version /value')\n\nList available Chrome drivers\nbinman::list_versions(appname = \"chromedriver\")\n\nIf no exact driver version matches your browser version,\n\nEach version of the Chrome driver supports Chrome with matching major, minor, and build version numbers.\n\nExample: Chrome driver 73.0.3683.20  supports all Chrome versions that start with 73.0.3683\n\n\n\n\nStart server and create remote driver\n\na browser will pop up and say “Chrome is being controlled by automated test software”\n\nlibrary(RSelenium)\ndriver &lt;- rsDriver(browser = c(\"chrome\"), chromever = \"&lt;driver version&gt;\", port = 4571L) # assume the port number is specified by chrome driver ppl.\nremDr &lt;- driver[['client']] # can also use $client\nNavigate to a webpage\nremDr$navigate(\"&lt;url&gt;\")\nremDR$maxWindowSize(): Set the size of the browser window to maximum.\n\nBy default, the browser window size is small, and some elements of the website you navigate to might not be available right away\n\nGrab the url of the webpage you’re on\nremDr$getCurrentUrl()\nGo back and forth between urls\nremDr$goBack()\nremDr$goForward()\nFind html element (name, id, class name, etc.)\nwebpage_element &lt;- remDr$findElement(using = \"name\", value = \"q\") \n\nSee Misc section for selector options\nWhere “name” is the element class and “q” is the value e.g. name=“q” if you used the inspect method in chrome\nAlso see Other Stuff &gt;&gt; Shadow DOM elements &gt;&gt; Use {shadowr} for alternate syntax to search for web elements\n\nHighlight element in pop-up browser to make sure you have the right thing\nwebpage_element$highlightElement()\nExample: you picked a search bar for your html element and now you want to use the search bar from inside R\n\nEnter text into search bar\nwebpage_element$sendKeysToElement(list(\"Scraping the web with R\"))\nHit enter to execute search\nwebpage_element$sendKeysToElement(list(key = \"enter\"))\n\nYou are now on the page with the results of the google search\n\nScrape all the links and titles on that page\nwebelm_linkTitles &lt;- remDr$findElement(using = \"css selector\", \".r\") \n\nInspect showed ”\n\n\n. Notice he used “.r”. Says it will pick-up all elements with “r” as the class.\n\nGet titles\n# first title\nwebelm_linkTitles[[1]]$getElementText()\n\n# put them all into a list\ntitles &lt;- purrr::map_chr(webelm_linkTitles, ~.x$getElementText())\ntitles &lt;- unlist(lapply(\n    webelm_linkTitles, \n    function(x) {x$getElementText()}\n\nExample: Access data that needs user authentication (also see rvest version)\n\nAfter set-up and navigating to webpage, find elements where you type in your username and password\nwebelm_username &lt;- remDr$findElement(using = \"id\", \"Username\")\nwebelm_pass &lt;- remDr$findElement(using = \"id, \"Password\")\nEnter username and password\nwebpage_username$sendKeysToElement(list(\"&lt;username&gt;\"))\nwebpage_pass$sendKeysToElement(list(\"&lt;password&gt;\"))\nClick sign-in button and click it\nwebelm_sbutt &lt;- remDr$findElement(using = \"class\", \"psds-button\")\nwebelm_sbutt$clickElement()\n\nExample: Filter a football stats table by selecting values from a dropdown menu on a webpage (also see rvest version)\n\nThis is tedious — use rvest to scrape this if possible (have to use rvest at the end anyways). html forms are the stuff.\nAfter set-up and navigated to url, find drop down “team” menu element locator using inspect in the browser and use findElement\nwebelem_team &lt;- remDr$findElement(using = \"name\", value = \"team\") # conveniently has name=\"team\" in the html\n\nAlso see Other Stuff &gt;&gt; Shadow DOM elements &gt;&gt; Use {shadowr} for alternate syntax to search for web elements\n\nclick team dropdown\nwebelem_team$clickElement()\nGo back to inspect in the browser, you should be able to expand the team menu element. Left click value that you want to filter team by to highlight it. Then right click the element and select “copy” –&gt; “copy selector”. Paste selector into value arg\nwebelem_DAL &lt;- remDr$findElement(using = \"css\", value = \"edit-filters-0-team &gt; option:nth-child(22)\")\nwebelem_DAL$clickElement()\n\nAlso see Other Stuff &gt;&gt; Shadow DOM elements &gt;&gt; Use {shadowr} for alternate syntax to search for web elements\nRepeat process for week, position, and year drop down menu filters\n\nAfter you’ve selected all the values in the dropdown, click the submit button to filter the table\nwebelem_submit &lt;- remDr$findElement(using = \"css\", value =     \"edit-filters-0-actions-submit\") \nwebelem_submit$clickElement()\n\nFinds element by using inspect on the submit button and copying the selector\n\nGet the url of the html code of the page with the filtered table. Read html code into R with rvest.\nurl &lt;- remDr$getPageSource()[[1]]\nhtml_page &lt;- rvest::read_html(url)\n\nIf you want the header, getPageSource(header = TRUE)\n\nUse rvest to scrape the table. Find the table with the stats\nall_tables &lt;- rvest::html_elements(html_page, \"table\")\nall_tables\n\nUsed the “html_elements” version instead of “element”\nThird one has “&lt;table class =”sortable stats-table full-width blah blah”\n\nSave to table to dataframe\nfootball_df &lt;- rvest::html_table(all_tables[[3]], header = TRUE)",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-ostuff",
    "href": "qmd/scraping.html#sec-scrap-ostuff",
    "title": "Scraping",
    "section": "Other Stuff",
    "text": "Other Stuff\n\nClicking a semi-infinite scroll button (e.g. “See more”)\n\nExample: For-Loop\n# Find Page Element for Body\nwebElem &lt;- remDr$findElement(\"css\", \"body\")\n\n# Page to the End\nfor (i in 1:50) {\n  message(paste(\"Iteration\",i))\n  webElem$sendKeysToElement(list(key = \"end\"))\n\n  # Check for the Show More Button\n  element&lt;- try(unlist(\n      remDr$findElement(\n        \"class name\",\n        \"RveJvd\")$getElementAttribute('class')), silent = TRUE)\n\n  #If Button Is There Then Click It\n  Sys.sleep(2)\n  if(str_detect(element, \"RveJvd\") == TRUE){\n    buttonElem &lt;- remDr$findElement(\"class name\", \"RveJvd\")\n    buttonElem$clickElement()\n  }\n\n  # Sleep to Let Things Load\n  Sys.sleep(3)\n}\n\narticle\nAfter scrolling to the “end” of the page, there’s a “show me more button” that loads more data on the page\n\nExample: Recursive\nload_more &lt;- function(rd) {\n  # scroll to end of page\n  rd$executeScript(\"window.scrollTo(0, document.body.scrollHeight);\", args = list())\n\n  # Find the \"Load more\" button by its CSS selector and ...\n  load_more_button &lt;- rd$findElement(using = \"css selector\", \"button.btn-load.more\")\n\n  # ... click it\n  load_more_button$clickElement()\n\n  # give the website a moment to respond\n  Sys.sleep(5)\n}\n\nload_page_completely &lt;- function(rd) {\n  # load more content even if it throws an error\n  tryCatch({\n    # call load_more()\n    load_more(rd)\n    # if no error is thrown, call the load_page_completely() function again\n    Recall(rd)\n  }, error = function(e) {\n    # if an error is thrown return nothing / NULL\n  })\n}\n\nload_page_completely(remote_driver)\n\narticle\nRecall is a base R function that calls the same function it’s in.\n\n\nShadow DOM elements\n\n#shadow-root and shadow dom button elements\nMisc\n\nTwo options: {shadowr} or JS script\n\nExample: Use {shadowr}\n\nMy stackoverflow post\nSet-up\npacman::p_load(RSelenium, shadowr)\ndriver &lt;- rsDriver(browser = c(\"chrome\"), chromever = chrome_driver_version)\n# chrome browser\nchrome &lt;- driver$client\nshadow_rd &lt;- shadow(chrome)\nFind web element\n\nSearch for element using html tag\n\n\nwisc_dl_panel_button4 &lt;- shadowr::find_elements(shadow_rd, 'calcite-button')\nwisc_dl_panel_button4[[1]]$clickElement()\n\nShows web element located in #shadow-root\nSince there might be more than one element with the “calcite-button” html tag, we use the plural, find_elements, instead of find_element\nThere’s only 1 element returned, so we use [[1]] index to subset the list before clicking it\n\nSearch for web element by html tag and attribute\nwisc_dl_panel_button3 &lt;- find_elements(shadow_rd, 'button[aria-describedby*=\"tooltip\"]')\nwisc_dl_panel_button3[[3]]$clickElement()\n\n“button” is the html tag which is subsetted by the brackets, and “aria-describedby” is the attribute\nOnly part of the attribute’s value is used, “tooltip,” so I think that’s why “*=” instead of just “=” is used. I believe the “*” may indicate partial-matching.\nSince there might be more than one element with this  html tag + attribute combo, we use the plural, find_elements, instead of find_element\nThere are 3 elements returned, so we use [[3]] index to subset the list to element we want before clicking it\n\n\nExample: Use a JS script and some webelement hacks to get a clickable element\n\nMisc\n\n“.class_name”\n\nfill in spaces with periods\n\n“.btn btn-default hidden-xs” becomes “.btn.btn-default.hidden-xs”\n\n\n\nYou can find the element path to use in your JS script by going step by step with JS commands in the Chrome console (bottom window)\n\nSteps\n\nWrite JS script to get clickable element’s elementId\n\nStart with element right above first shadow-root element and use querySelector\nMove to the next element inside the next shadow-root element using shadowRoot.querySelector\nContinue to desired clickable element\n\nIf there’s isn’t another shadow-root that you have to open, then the next element can be selected usingquerySelector\nIf you do have to click on another shadow-root element to open another branch, then used shadowRoot.querySelector\nExample\n\n\n“hub-download-card” is just above shadow-root so it needs querySelector\n“calcite-card” is an element that’s one-step removed from shadow-root, so it needs shadowRoot.querySelector\n“calcite-dropdown” (type = “click”) is not directly (see div) next to shadow-root , so it can selected using querySelector\n\n\nWrite and execute JS script\nwisc_dlopts_elt_id &lt;- chrome$executeScript(\"return document.querySelector('hub-download-card').shadowRoot.querySelector('calcite-card').querySelector('calcite-dropdown');\")\n\nMake a clickable element or just click the damn thing\n\nclickable element (sometimes this doesn’t work; needs to be a button or type=click)\n\nUse findElement to find a generic element class object that you can manipulate\nUse “@” ninja-magic to force elementId into the generic webElement to coerce it into your button element\nUse clickElement to click the button\n\n# think this is a generic element that can always be used\nmoose &lt;- chrome$findElement(\"css\", \"html\")\nmoose@.xData$elementId &lt;- as.character(wisc_dlopts_elt_id)\nmoose$clickElement()\n\nClick the button\nchrome$executeScript(\"document.querySelector('hub-download-card').shadowRoot.querySelector('calcite-card').querySelector('calcite-dropdown').querySelector('calcite-dropdown-group').querySelector('calcite-dropdown-item:nth-child(2)').click()\")\n\n\n\nGet data from a hidden input\n\narticle\nHTML Element\n&lt;input type=\"hidden\" id=\"overview-about-text\" value=\"%3Cp%3E100%25%20Plant-Derived%20Squalane%20hydrates%20your%20skin%20while%20supporting%20its%20natural%20moisture%20barrier.%20Squalane%20is%20an%20exceptional%20hydrator%20found%20naturally%20in%20the%20skin,%20and%20this%20formula%20uses%20100%25%20plant-derived%20squalane%20derived%20from%20sugar%20cane%20for%20a%20non-comedogenic%20solution%20that%20enhances%20surface-level%20hydration.%3Cbr%3E%3Cbr%3EOur%20100%25%20Plant-Derived%20Squalane%20formula%20can%20also%20be%20used%20in%20hair%20to%20increase%20heat%20protection,%20add%20shine,%20and%20reduce%20breakage.%3C/p%3E\"&gt;\nExtract value and decode the text\noverview_text &lt;- webpage |&gt;\n  html_element(\"#overview-about-text\") |&gt;\n  html_attr(\"value\") |&gt;\n  URLdecode() |&gt;\n  read_html() |&gt;\n  html_text()\n\noverview_text\n#&gt; [1] \"100% Plant-Derived Squalane hydrates your skin while supporting its natural moisture barrier.",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html",
    "href": "qmd/db-lakes.html",
    "title": "Lakes",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-misc",
    "href": "qmd/db-lakes.html#sec-db-lakes-misc",
    "title": "Lakes",
    "section": "",
    "text": "Data is stored in structured format or in its raw native format without any transformation at any scale.\n\nHandling both types allows all data to be centralized which means it can be better organized and more easily accessed.\n\nOptimal for fit for bulk data types such as server logs, clickstreams, social media, or sensor data.\nIdeal use cases\n\nBackup for logs\nRaw sensor data for your IoT application,\nText files from user interviews\nImages\nTrained machine learning models (with the database simply storing the path to the object)\n\nLower storage costs due to their more open-source nature and undefined structure\nOn-Prem set-ups have to manage hardward and environments\n\nIf you wanted to separate stuff like test data from production data, you also probably had to set up new hardware.\nIf you had data in one physical environment that had to be used for analytical purposes in another physical environment, you probably had to copy that data over to the new replica environment.\n\nHave to keep a tie to the source environment to ensure that the stuff in the replica environment is still up-to-date, and your operational source data most likely isn’t in one single environment. It’s likely that you have tens — if not hundreds — of those operational sources where you gather data.\n\nWhere on-prem set-ups focus on isolating data with physical infrastructure, cloud computing shifts to focus on isolating data using security policies.\n\nObject Storage Systems\n\nCloud data lakes provide organizations with additional opportunities to simplify data management by being accessible everywhere to all applications as needed\nOrganized as collections of files within directory structures, often with multiple files in one directory representing a single table.\n\nPros: highly accessible and flexible\nMetadata Catalogs are used to answer these questions:\n\nWhat is the schema of a dataset, including columns and data types\nWhich files comprise the dataset and how are they organized (e.g., partitions)\nHow different applications coordinate changes to the dataset, including both changes to the definition of the dataset and changes to data\n\nHive Metastore (HMS) and AWS Glue Data Catalog are two popular catalog options\n\nContain the schema, table structure and data location for datasets within data lake storage\n\n\nIssues:\n\nDoes not coordinate data changes or schema evolution between applications in a transactionally consistent manner.\n\nCreates the necessity for data staging areas and this extra layer makes project pipelines brittle",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-nosql.html",
    "href": "qmd/db-nosql.html",
    "title": "NoSQL",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "NoSQL"
    ]
  },
  {
    "objectID": "qmd/db-nosql.html#sec-db-nosql-misc",
    "href": "qmd/db-nosql.html#sec-db-nosql-misc",
    "title": "NoSQL",
    "section": "",
    "text": "High-Performance data ingestion and retrieval for specific applications, especially with structured or semi-structured data.\n\nData lakes can also store semi-structured data, but they don’t have the responsiveness (i.e. low latency, reading, writing) for apps, etc.\nNoSQL dbs can be specialized for particular tasks like social media, email, gaming, etc.\n\nApplications (Google Gemini)\n\nData Analytics: For real-time log monitoring, frequent analysis, smaller datasets, and short term to medium term storage. For long-term storage, a data lake would be a better choice.\nCaching: Due to their high speed and low latency, they are perfect for caching frequently accessed data, significantly improving application performance. Popular use cases include caching user sessions, search results, and product information in e-commerce platforms.\nSession Management: In web applications, they efficiently store user session data, shopping carts, and temporary preferences. This enables seamless user experience and personalization across sessions.\nIoT and Real-time Data: Sensors and devices in IoT ecosystems generate large volumes of real-time data. These types of dbs excel at capturing and storing sensor readings, timestamps, and device states, offering real-time insights and analytics.\nGaming and Leaderboards: Prominent use in online gaming for storing player profiles, high scores, and game state information. Their fast retrieval and update capabilities ensure smooth gameplay and accurate leaderboards.\nSocial Media and Messaging: Storing user profiles, connections, messages, and notifications benefits greatly from the scalability and efficient retrieval. This enables handling millions of users and delivering real-time interactions.\nAuthentication and Authorization: Securel storage of user credentials, session tokens, and access control information. This enables efficient user authentication and authorization, securing access to sensitive data and functionalities.\nConfiguration Management: Storing application configuration settings, API keys, and environment variables in a key-value store simplifies management and deployment. This allows for dynamic configuration changes and simplifies scaling processes.\n\nBrands: ScyllaDB, Cassandra, MongoDB, DynamoDB",
    "crumbs": [
      "Databases",
      "NoSQL"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-brands",
    "href": "qmd/db-lakes.html#sec-db-lakes-brands",
    "title": "Lakes",
    "section": "Brands",
    "text": "Brands\n\nHadoop\n\nTraditional format for data lakes\n\nAmazon S3\n\nTry to stay &lt;1000 entries per level of hierarchy when designing the partitioning format. Otherwise there is paging and things get expensive.\nAWS Athena ($5/TB scanned)\n\nAWS Athena is serverless and intended for ad-hoc SQL queries against data on AWS S3\n\n\nMicrosoft Azure Data Lake Storage (ADLS)\nMinio\n\nOpen-Source alternative to AWS S3 storage.\nGiven that S3 often stores customer PII (either inadvertently via screenshots or actual structured JSON files), Minio is a great alternative to companies mindful of who has access to user data.\n\nOf course, AWS claims that AWS personnel doesn’t have direct access to customer data, but by being closed-source, that statement is just a function of trust.\n\n\nDatabricks Delta Lake\nGoogle Cloud Storage\n\n5 GB of US regional storage free per month, not charged against your credits.",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-iceb",
    "href": "qmd/db-lakes.html#sec-db-lakes-iceb",
    "title": "Lakes",
    "section": "Apache Iceberg",
    "text": "Apache Iceberg\n\nOpen source table format that addresses the performance and usability challenges of using Apache Hive tables in large and demanding data lake environments.\nInterfaces\n\nDuckDB can query Iceberg tables in S3 with an extension, docs\nAthena can create Iceberg Tables\nGoogle Cloud Storage has something called BigLake that can create Iceberg tables\n\nFeatures\n\nTransactional consistency between multiple applications where files can be added, removed or modified atomically, with full read isolation and multiple concurrent writes\nFull schema evolution to track changes to a table over time\nTime travel to query historical data and verify changes between updates\nPartition layout and evolution enabling updates to partition schemes as queries and data volumes change without relying on hidden partitions or physical directories\nRollback to prior versions to quickly correct issues and return tables to a known good state\nAdvanced planning and filtering capabilities for high performance on large data volumes\nThe full history is maintained within the Iceberg table format and without storage system dependencies\n\nSupports common industry-standard file formats, including Parquet, ORC and Avro\nSupported by major data lake engines including Dremio, Spark, Hive and Presto\nQueries on tables that do not use or save file-level metadata (e.g., Hive) typically involve costly list and scan operations\nAny application that can deal with parquet files can use Iceberg tables and its API in order to query more efficiently\nComparison",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/eda-general.html",
    "href": "qmd/eda-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-",
    "href": "qmd/eda-general.html#sec-eda-gen-",
    "title": "General",
    "section": "",
    "text": "First contact with an unfamiliar database\n\nselect * from limit 50\nLook for keys/fields to connect tables\nMake running list of Q’s, try to answer them by poking around first\nFind team/code responsible for DB and ask for time to review questions – communication can be a superpower here!\n\nUse domain knowledge to assess peculier relationships\n\nExample: Is there a nonlinear relationship between Driver hours and Incentive Level\n\n\nCommon sense says if we raise payment bonuses, we should see more drivers want to work more hours.\nReason behind the relationship shown in this chart is omitted variables: weather and holiday.\n\nIncentives stop having an effect on drivers because they hate going out in shitty weather and want to stay home with their family on the holidays.",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-bascln",
    "href": "qmd/eda-general.html#sec-eda-gen-bascln",
    "title": "General",
    "section": "Basic Cleaning",
    "text": "Basic Cleaning\n\nTidy column names\nShrink long column names to something reasonable enough for an axis label\nMake sure continuous variables aren’t initially coded as categoricals and vice versa\nMake note of columns with several values per cell and will need to be separated into multiple columns (e.g. addresses)\nFind duplicate rows\n\nSee\n\nCode, Snippets &gt;&gt; Cleaning\nSQL &gt;&gt; Processing Expressions &gt;&gt; Duplicates\nPython, Pandas &gt;&gt; Distinct\n\nThese can cause data leakage if the same row is in the test and train sets.\n\nMake a note to remove columns that the target is a function of\n\ne.g. Don’t use monthly salary to predict yearly salary\n\nRemove columns that occur after the target event\n\ne.g. Using info occurring in or after a trial to predict something pre-trial\n\nYou won’t have this info beforehand when you make your prediction\n\n\nOrdinal categorical\n\nReorder by a number in the text (parse_number)\nmutate(income_category = fct_reorder(income_category, parse_number(income_category)),\n      # manually fix category that is still out of order\n      # moves \"Less thatn $40K\" to first place in the levels\n      income_category = fct_relevel(income_category, \"Less than $40K\"))",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-pkgs",
    "href": "qmd/eda-general.html#sec-eda-gen-pkgs",
    "title": "General",
    "section": "Packages",
    "text": "Packages\n\nBase R coplot can be used for quick plots of all combinations of categorical and continuous variables for up to 4 variables\n\nSee Continuous Predictor vs Outcome &gt;&gt; Continuous Outcome for examples\n\n{skimr::skim} - Overall summary, check completion percentage for vars with too many NAs\n{dataexplorer}\ncreate_report(airquality)\ncreate_report(diamonds, y = \"price\") # specify response variable\n\nRuns multiple functions to analyze dataset\n\n{dataxray} - Table with interactive distributions, summary stats, missingness, proportions. (dancho article/video)\n{trelliscope} - Quick, interactive, facetted pairwise plots, built with JS\n{explore} - Interactive data exploration or automated report\n\nexplore - If you want to explore a table, a variable or the relationship between a variable and a target (binary, categorical or numeric). The output of these functions is a plot (automatically checks if an attribute is categorical or numerical, chooses the best plot-type and handles outliers).\ndescribe - If you want to describe a dataset or a variable (number of na, unique values, …) The output of these functions is a text.\nexplain - To create a simple model that explains a target. explain_tree() for a decision tree, explain_forest() for a random forest and explain_logreg() for a logistic regression.\nreport - To generate an automated report of all variables. A target can be defined (binary, categorical or numeric)\nabtest - To test if a difference is statistically significant\n\n{visdat} has decent visualization for group comparison, missingness, correlation, etc.\n{Hmisc::describe}\nsparkline::sparkline(0)\ndes &lt;- describe(d)\nplot(des) # maybe for displaying in Viewer pane\nprint(des, 'both') # maybe just a console df of the numbers\nmaketabs(print(des, 'both'), wide=TRUE) # for Quarto\n\n“both” says display “continuous” and “categorical”\n“continuous”\n\n“categorical”\n\nColumns (from Hmisc Ref Manual)\n\n“Info”: Info which is a relative information measure using the relative efficiency of a proportional odds/Wilcoxon test on the variable relative to the same test on a variable that has no ties. Info is related to how continuous the variable is, and ties are less harmful the more untied values there are. The formula for Info is one minus the sum of the cubes of relative frequencies of values divided by one minus the square of the reciprocal of the sample size. The lowest information comes from a variable having only one distinct value following by a highly skewed binary variable. Info is reported to two decimal places.\n“Mean” and “Sum” (Binary): , the sum (number of 1’s) and mean (proportion of 1’s)\n\n\nLux - Jupyter notebook widget that provides visual data profiling via existing pandas functions which makes this extremely easy to use if you are already a pandas user. It also provides recommendations to guide your analysis with the intent function. However, Lux does not give much indication as to the quality of the dataset such as providing a count of missing values for example.\n{{pandas_profiling}} - Produces a rich data profiling report with a single line of code and displays this in line in a Juypter notebook. The report provides most elements of data profiling including descriptive statistics and data quality metrics. Pandas-profiling also integrates with Lux.\n{{sweetviz}} - Provides a comprehensive and visually attractive dashboard covering the vast majority of data profiling analysis needed. This library also provides the ability to compare two versions of the same dataset which the other tools do not provide.\n{{ydata-profiling}} - Data profiling, automates, and standardizes the generation of detailed reports, complete with statistics and visualizations",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-miss",
    "href": "qmd/eda-general.html#sec-eda-gen-miss",
    "title": "General",
    "section": "Missingness",
    "text": "Missingness\n\nAlso see\n\nMissingness\nModel Building, tidymodels &gt;&gt; Recipe &gt;&gt; Imputation\n\nPackages\n\n{naniar} - tidy ways to summarize, visualize, and manipulate missing data with minimal deviations from the workflows in ggplot2 and tidy data\n{qreport} - Harrell package\n\nA few of the charts aren’t intuitive and don’t have good documentation in terms of explaining how to interpret them.\nFits an ordinal logistic regression model to describe which types of subjects (based on variables with no NAs) tend to have more variables missing.\nHierarchically clusters variables that have similar observations missing\nSee naclus docs, RMS Ch.19.1, R Workflow Ch.2.7 (interprets the clustering), Ch.6 (interpretes the ordinal regression) (possibly more use cases in that ebook)\n\n\nQuestions\n\nWhich features contain missing values?\nWhat proportion of records for each feature comprises missing data?\nIs the missing data missing at random (MAR) or missing not at random (MNAR) (i.e. informative)?\nAre the features with missing values correlated with other features?\n\nCategoricals for binary classification\n\ntrain_raw %&gt;%\n  select(\n    damaged, precipitation, visibility, engine_type,\n    flight_impact, flight_phase, species_quantity\n  ) %&gt;%\n  pivot_longer(precipitation:species_quantity) %&gt;%\n  ggplot(aes(y = value, fill = damaged)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(vars(name), scales = \"free\", ncol = 2) +\n  labs(x = NULL, y = NULL, fill = NULL)\n\nThe NAs (top row in each facet) aren’t 50/50 between the two levels of the target. The target is imbalanced and the NAs seem to be predictive of “no damage,” so they aren’t random.\nSince these NAs look predictive, you can turn them into a category by using step_unknown in the preprocessing recipe.",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-out",
    "href": "qmd/eda-general.html#sec-eda-gen-out",
    "title": "General",
    "section": "Outliers",
    "text": "Outliers\n\nAlso see Outliers\nAbnormalities due to likely data entry errors\n\nExample: store == “open” and sales == 0 or store == “closed” and sales &gt; 0\n\nPotential sol’n: replace 0’s (open) with mean sales and sales &gt;0 (closed) with 0s\n\n\nExtreme counts in charts when grouping by a cat var\n\nWhy is one category’s count so low or so high?\n\nMay need subject matter expert\n\nWhat can be done to increase or decrease that category’s count?\n\nFor prediction, experiment with keeping or removing outliers while fitting baseline models",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-grpcal",
    "href": "qmd/eda-general.html#sec-eda-gen-grpcal",
    "title": "General",
    "section": "Group Calculations",
    "text": "Group Calculations\n\nAlso see Feature Engineering, General &gt;&gt; Domain Specific\nVariance of Value by Group\n\nExample: how sales vary between store types over a year\nimportant to standardize the value by group\n\ngroup_by(group), mutate(sales = scale(sales))\n\nWhich vary wildly and which are more stable\n\nRates by Group\n\nExample: sales($) per customer\n\ngroup_by(group), mutate(sales_per_cust = sum(sales)/sum(customers)\n\n\nAvg by Group(s)\ndat %&gt;%\nselect(cat1, cat2, num) %&gt;%\ngroup_by(cat1, cat2) %&gt;%\nsummarize(freq = n(),\n          avg_cont = mean(num))",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-cont",
    "href": "qmd/eda-general.html#sec-eda-gen-cont",
    "title": "General",
    "section": "Continuous Variables",
    "text": "Continuous Variables\n\nDoes the variable have a wide range. (i.e. values across multiple magnitudes: 101 and 102 and … etc.)\n\nIf so, log the variable\n\nHistogram - Check shape of distribution\nggplot(aes(var)) +\n    geom_histogram()\n\nLooking at skew. Is it roughly normal?\nDoes filter(another_var &gt; certain_value (see below) help it look more normal?\nIs it multi-modal\n\nSee Regression, Other &gt;&gt; Multi-Modal(visuals, tests, modelling, etc.)\n{{gghdr}} - Visualization of Highest Density Regions in ggplot2\nInteractions &gt;&gt; Outcome: Categorical &gt;&gt; Binary Outcome (pct_event) vs Discrete by Discrete (or binary in this case)\n\nIs the variable highly skewed\n\nIf so, try:\n\nChanging units (min to hr),\nfilter(some_var &gt; some_value)\nsome combination of the above make more normal?\n\nNormality among predictors isn’t necessary, but I think it improves fit or prediction somewhat\n\nlog transformation may help some if the skew isn’t too extreme\n\n\n\nQ-Q plot to check fit against various distributions\n\n{ggplot}\nggplot(data)+\n    stat_qq(aes(sample = log_profit_rug_business))+\n    stat_qq_line(aes(sample = log_profit_rug_business))+\n    labs(title = 'log(profit) Normal QQ')\n\nA plot of the sample (or observed) quantiles of the given data against the theoretical (or expected) quantiles.\nSee article for the math and manual code\nstat_qq, stat_qq_line default distributions are Normal\nggplot::stat_qq docs have some good examples on how to use q-q plots to test your data against different distributions using MASS::fitdistr to get the distributional parameter estimates. Available distributions: “beta”, “cauchy”, “chi-squared”, “exponential”, “gamma”, “geometric”, “log-normal”, “lognormal”, “logistic”, “negative binomial”, “normal”, “Poisson”, “t” and “weibull”\n\n{dataexplorer}\n## View quantile-quantile plot of all continuous variables\nplot_qq(diamonds)\n\n## View quantile-quantile plot of all continuous variables by feature `cut`\nplot_qq(diamonds, by = \"cut\") \nSkewed Variables\n\nx &lt;- list()\nn &lt;- 300\nx[[1]] &lt;- rnorm(n)\nx[[2]] &lt;- exp(rnorm(n))\nx[[3]] &lt;- -exp(rnorm(n))\n\npar(mfrow = c(2,3), bty = \"l\", family = \"Roboto\")\n\nqqnorm(x[[1]], main = \"Normal\")\nqqnorm(x[[2]], main = \"Right-skewed\")\nqqnorm(x[[3]], main = \"Left-skewed\")\nlapply(x, function(x){plot(density(x), main = \"\")})\nGood fits\n\nnormal distribution\n\nBad fits\n\nUniform data tested against a normal distibution\n\nUniform data tested against an exponential distribution\n\n\n\n\nIs the mean/median above or below any important threshold?\n\ne.g. CDC considers a BMI &gt; 30 as obese. Health Insurance charges rise sharply at this threshold\n\nIs there an important threshold value?\n\n1 value –&gt; split into a binary\nMultiple values –&gt; Multinomial\n\nExamples\n\nBinary\n\nWhether a user spent more than $50 or didn’t (See Charts &gt;&gt; Categorical Predictors vs Outcome)\nIf user had activity on the weekend or not\n\nMultinomial\n\nTimestamp to morning/afternoon/ night,\nOrder values into buckets of $10–20, $20–30, $30+\n\n\n\nEmpirical Cumulative Density function (ecdf)\n\nggplot(aes(x = numeric_var, color = cat) +\n    stat_ecdf()\n\nShows the percentage of sample (y-axis) that are below a numeric_var value (x-axis)\n{sfsmisc::ecdf.ksCI} - plots the ecdf and 95% CIs (see Harrell for details of the CI calculation)\nCan view alongside a table of group means to see if the different percentiles differ from the story of just looking at the mean.\ndata %&gt;%\n    group_by(categorical_var) %&gt;%\n    summarize(mean(numeric_var))",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-cat",
    "href": "qmd/eda-general.html#sec-eda-gen-cat",
    "title": "General",
    "section": "Categorical/Discrete Variables",
    "text": "Categorical/Discrete Variables\n\nCount number of rows per category level (or use skimr or DataExplorer)\ntbl %&gt;% count(cat_var, sort = True)\nLooking for how skewed data might be (only a few categories have most of the obs)\nIf levels are imbalanced, consider: initial_split(data, strata = imbalanced_var)\nFor cat vars with levels with too few counts, consider lumping together\n\nLevels with too few data will have large uncertainties about the effect and the bloated std.devs can cause some models to throw errors\n\nCount NAs (or use skimr or DataExplorer)\ntbl %&gt;%\n  map_df(~ sum(is.na(.))) %&gt;%\n  gather(key = \"feature\", value = \"missing_count\") %&gt;%\n  arrange(desc(missing_count))\nVars with too many NAs, may need to be dropped or imputed\n\nSome models don’t handle NAs\n\nIf the number of NAs is within tolerance and you decide to impute, you need to find out what kind of “missingness” you have before you choose the imputation method. Some cause issues with certain types of missingness. (e.g. mean and missing-not-at-random (MNAR))\nYear variable\ndata |&gt;\n    count(year) |&gt;\n    arrange(desc(year)) |&gt;\n    ggplot(aes(year, n)) +\n    geom_line()\n\nLooking for skew.\nIs data older or more recent?\n\nFree Text Sometimes these columns are just metadata (a url, product description, etc.), but other times they could have valuable information (e.g. customer feedback). If a column seems like it contains valuable information for your prediction task, you generate features from it text length, appearance/frequency of certain keywords, etc.\n\nTokenize\n\nSee below code for “Facetted bar by variable with counts of the values” and the use of separate_rows to manually tokenize more useful when the columns don’t have stopwords\n\n\nVisualize value counts for multiple variables\n\nFacetted bar by variable with counts of the values\n\ncategorical_variables &lt;- board_games %&gt;%\n      # select all cat vars\n      select(game_id, name, family, category, artist, designer, mechanic) %&gt;%\n      # \"type\" receives all colnames; \"value\" receives their values\n      gather(type, value, -game_id, -name) %&gt;%\n      filter(!is.na(value)) %&gt;%\n      # Some values of vars are free text separated by commas; code makes each value into a separate row\n      separate_rows(value, sep = \",\") %&gt;%\n      arrange(game_id)\ncategorical_counts &lt;- categorical_variables %&gt;%\n      count(type, value, sort = TRUE)\n\ncategorical_counts %&gt;%\n      # type is gathered colnames of the variables\n      group_by(type) %&gt;%\n      # high cardinality variables, so only show top 10\n      top_n(10, n) %&gt;%\n      ungroup() %&gt;%\n      mutate(value = fct_reorder(value, n)) %&gt;%\n      ggplot(aes(value, n, fill = type)) +\n      geom_col(show.legend = FALSE) +\n      facet_wrap(~ type, scales = \"free_y\") +\n      coord_flip() +\n      labs(title = \"Most common categories\")\n\n“type” has the names of the variables, “value” has the levels of the variable",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-corr",
    "href": "qmd/eda-general.html#sec-eda-gen-corr",
    "title": "General",
    "section": "Correlation/Association",
    "text": "Correlation/Association\n\nMisc\n\nAlso see\n\nAssociation, General\nNotebook &gt;&gt; Statistical Inference &gt;&gt; Correlation\nInteractions &gt;&gt; Continuous Outcome &gt;&gt; Correlation Heatmaps\n\n{correlationfunnel} - Dancho’s package; bins numerics, then dummies all character and binned numerics, then runs a pearson correlation vs the outcome variable. Surprisingly it’s useful to use Pearson correlations for binary variables as long as you have a mix of 1s and 0s in each variable. (Cross-Validated post)\nchurn_df %&gt;%\n    binarize() %&gt;%\n    correlate(&lt;outcome_var&gt;) %&gt;%\n    plot_correlation_funnel()\n\ncorrelate returns a sorted? tibble in case you don’t want the plot\nThe funnel plot is a way of combining and ranking all the correlation plots into a less eye-taxing visual.\nUses stats::cor for calculation so you can pass args to it and but changing the method (e.g. method = c(\"pearson\", \"kendall\", \"spearman\") ) won’t matter, since pearson and spearman (and probably kendall) will be identical for binary variables.\n\nFor binary vs. binary, also see Association, General &gt;&gt; Discrete &gt;&gt; Binary Similarity Measures and Cramer’s V\n\nPairwise plots for patterns\n\nOutcome vs Predictor\nPredictor vs Predictor\n\nInteractions\nMulticollinearity\n\nCorrelation/Association scores for linear relationships\nHistograms for variations between categories\nExample: {{ggforce}}\n\nggplot(palmerpenguins::penguins, aes(x = .panel_x, y = .panel_y)) +\n  geom_point(aes(color = species), alpha = .5) +\n  geom_smooth(aes(color = species), method = \"lm\") +\n  ggforce::geom_autodensity(aes(color = species, fill = after_scale(color)), alpha = .7) +\n  scale_color_brewer(palette = \"Set2\", name = NULL) +\n  ggforce::facet_matrix(vars(names), layer.lower = 2, layer.diag = 3)\n\nLinear\n\n{greybox} for testing correlation between different types of variables\n\nMulticollinearity\n\nVIF (performance::check_collinearity(fit) or greybox::determ or vif(fit))\nUse PCA — if only a few (depends on the number of variables) pc explain all or almost all of the variation, then you could have a multicollinearity problem\n\nNonlinear\n\nScatterplots for non-linear patterns,\nCorrelation metrics\nAlso see General Additive Models &gt;&gt; Diagnostics for a method of determining a nonlinear relationship for either continuous or categorical outcomes.\n\nCategorical\n\n2-level x 2-level: Cramer’s V\n2-level or multi-level x multi-level\n\nChi-square or exact tests\n\nLevels vs Levels correlation\n\nMultiple Correspondence Analysis (MCA) (see bkmks &gt;&gt; Features &gt;&gt; Reduction)\n\nBinary outcome vs Numeric predictors\n# numeric vars should be in a long tbl. Use pivot longer to make two columns (e.g. metric (var names) value (value))\n\nnumeric_gathered %&gt;%\n  group_by(metric) %&gt;%\n  # rain_tomorrow is the outcome; event_level says which factor level is the event your measuring\n  roc_auc(rain_tomorrow, value, event_level = \"second\") %&gt;%\n  arrange(desc(.estimate)) %&gt;%\n  mutate(metric = fct_reorder(metric, .estimate)) %&gt;%\n  ggplot(aes(.estimate, metric)) +\n  geom_point() +\n  geom_vline(xintercept = .5) +\n  labs(x = \"AUC in positive direction\",\n      title = \"How predictive is each linear predictor by itself?\",\n      subtitle = \".5 is not predictive at all; &lt;.5 means negatively associated with rain, &gt;.5 means positively associated\")\n\n.5 is not predictive at all; &lt;.5 means negatively associated with rain, &gt;.5 means positively associated\n\n\nOrdinal\n\nPolychoric",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-contout",
    "href": "qmd/eda-general.html#sec-eda-gen-contout",
    "title": "General",
    "section": "Continuous Predictor vs Outcome",
    "text": "Continuous Predictor vs Outcome\n\nMisc\n\nIf the numeric-numeric relation isn’t linear, then the model will be misspecified: an influential variable may be overlooked or the assumption of linearity may produce a model that fails in important ways to represent the relationship.\nAlso see General Additive Models &gt;&gt; Diagnostics for a method of determining a nonlinear relationship for either continuous or categorical outcomes.\n\n\n\nContinuous Outcome\n\nContinuous vs Continuous by Continuous\n\ncoplot(lat ~ long | depth, data = quakes)\n\ncoplot is base R.\n\nExamples from Six not-so-basic base R functions\n\nThe six plots show the relationship of these two variables for different values of depth\nThe bar plot at the top indicates the range of depth values for each of the plots\nFrom lowest depth to highest depth, the default arrangement of the plots is from bottom row, left to right, and upwards\n\ne.g. The 4th lowest depth is on the top row, farthest to the left.\n\nrows = 1 would arrange all plots in 1 row.\noverlap = 0 will remove overlap between bins\n\nContinuous vs Continuous by Continuous by Continuous\n\ncoplot(lat ~ long | depth * mag, data = quakes, number = c(3, 4))\n\nShows the relationship with depth from left to right and the relationship with magnitude from top to bottom.\nnumber = c(3, 4) says you want 3 bins for depth and 4 bins for mag\nFrom lowest depth, mag to highest depth, mag, the arrangement of the plots is from bottom row, left to right, and upwards\n\ne.g. The 2nd lowest depth (columns) and 3rd lowest mag (rows) is in the 3rd from bottom row and 2nd column.\n\n\nContinuous vs Continuous by Categorical by Categorical\n\ncoplot(flowers ~ weight|nitrogen * treat, data = flowers,\n        panel = function(x, y, ...) {\n        points(x, y, ...)\n        abline(lm(y ~ x), col = \"blue\")})\n\nFrom An Introduction to R\nSame arrangement scheme as the plots above\n\ne.g. nitrogen = “medium” and treat = “tip” is the cell at middle column, top row\n\n\nScagnostics (paper) - metrics to examine numeric vs numeric relationships\n\n{scagnostics}\nScagnostics describe various measures of interest for pairs of variables, based on their appearance on a scatterplot. They are useful tool for discovering interesting or unusual scatterplots from a scatterplot matrix, without having to look at every individual plot\nMetrics: Outlying, Skewed, Clumpy, Sparse, Striated, Convex, Skinny, Stringy, Monotonic\n\n“Straight” (paper) seems to have been swapped for “Sparse” (package)\n\nPotential use cases\n\nFinding linear/nonlinear relationships\nClumping or clustered patterns could indicate an interaction with a categorical variable\n\nScore Guide\n\n\nHigh value: Red\nLow value: Blue\nCouldn’t find the ranges of these metrics in the paper or the package docs\nShows how scatterplot patterns correspond to metric values\n\n\n\n\n\nCategorical Outcome\n\nFor binary outcome, look for variation between numeric variables and each outcome level\n\n# numeric vars should be in a long tbl.\n# Use pivot longer to make two columns (e.g. metric (var names) value (value)) with the binary outcome (e.g rain_tomorrow) as a separate column\nnumeric_gathered %&gt;%\n  ggplot(aes(value, fill = rain_tomorrow)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ metric, scales = \"free\")\n# + scale_x_log10()\n\nSeparation between the two densities would indicate predictive value.\nIf one of colored density is further to the right than the other then the interpretation would be:\n\nHigher values of metric result in a greater probability of &lt;outcome category of the right-most density&gt;\n\nNormalize the x-axis with rank_percentile(value)\n\nnumeric_gathered %&gt;%\n    mutate(rank = percent_rank(value)) %&gt;%\n    ggplot(aes(rank, fill = churned)) + \n      geom_density(alpha = 0.5) + \n      facet_wrap(~ metric, scales = \"free\")\n\nNot sure why you’d do this unless there was a reason to compare the separation of densities (i.e. strength of association with outcome) between the predictors.\n\n\nEstimated AUC for binary outcome ~ numeric predictor\nnumeric_gathered &lt;- train %&gt;%\n  mutate(rainfall = log2(rainfall + 1)) %&gt;%\n  gather(metric, value, min_temp, max_temp, rainfall, contains(\"speed\"), contains(\"humidity\"), contains(\"pressure\"), contains(\"cloud\"),        contains(\"temp\"))\n\nnumeric_gathered %&gt;%\n  group_by(metric) %&gt;%\n  # \"rain_tomorrow\" is a binary factor var\n  # \"second\" says the event we want the probability for is the second level of the binary factor variable\n  yardstick::roc_auc(rain_tomorrow, value, event_level = \"second\") %&gt;%\n  arrange(desc(.estimate)) %&gt;%\n  mutate(metric = fct_reorder(metric, .estimate)) %&gt;%\n  ggplot(aes(.estimate, metric)) +\n  geom_point() +\n  geom_vline(xintercept = .5) +\n  labs(x = \"AUC in positive direction\",\n      title = \"How predictive is each linear predictor by itself?\",\n      subtitle = \".5 is not predictive at all; &lt;.5 means negatively associated with rain, &gt;.5 means positively associated\")",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-catout",
    "href": "qmd/eda-general.html#sec-eda-gen-catout",
    "title": "General",
    "section": "Categorical Predictor vs Outcome",
    "text": "Categorical Predictor vs Outcome\n\nContinuous Outcome\n\nBoxplot by Categorical\n\nfct_reorder  says order cat_var by a num_var\n\nMake sure data is NOT grouped\n\ndata %&gt;%\n    mutate(cat_var = fct_reorder(cat_var, numeric_outcome)) %&gt;%\n    ggplot(aes(numeric_outcome, cat_var)) +\n    geom_boxplot()\n\nIf all the medians line up then no relationship. A slope or nonlinear shows relationship.\n\nfct_lump  can be used to create an “other” group.\n\ndata %&gt;%\n    mutate(cat_var = fct_lump(cat_var, 8),\n          cat_var = fct_reorder(cat_var, numeric_outcome)) %&gt;%\n    ggplot(aes(numeric_outcome, cat_var)) +\n    geom_boxplot()\n\nUseful for cat_vars with too many levels which can muck-up a graph\nSays to keep the top 8 levels with the highest counts and put rest in “other”.\n\nAlso takes proportions. Negative values says keep lowest.\n\n\n\nBoxplot by Categorical (Titanic5 dataset)\n\n\nY-Axis is the “Class” categorical with 3 levels\nFor ticket price, only class 1 shows any variation\nFor Age, there’s a clear trend but also considerable overlap between classes\n\n\n\n\nCategorical Outcome\n\nHistograms of cat_vars split by response_var \ndf %&gt;%\n    select(cat_vars) %&gt;%\n    pivot_longer(key, value = cat_vars, response_var) %&gt;%\n    ggplot(aes(value)) +\n    geom_bar(fill = response_var) +\n    facet_wrap( ~key, scales = \"free\")\n\nJust looking for variation in the levels of the cat_var given response var. More variation = more likely to be a better predictor\nEach facet will be a level of the response variable\n\nError Bar Plot\n# outcome variable is a binary for whether or not it rained on that day\ngroup_binary_prop &lt;- function(tbl) {\n    ret &lt;- tbl %&gt;%\n        # count of events for each category (successes)\n        summarize(n_rain = sum(rain_tomorrow == \"Rained\"),\n                  # count of rows for each category (trials)\n                  n = n()) %&gt;%\n        arrange(desc(n)) %&gt;%\n        ungroup() %&gt;%\n        # probability of event for each category\n        mutate(pct_rain = n_rain / n,\n              # jeffreys interval\n              # bayesian CI for binomial proportions\n              low = qbeta(.025, n_rain + .5, n - n_rain + .5),\n              high = qbeta(.975, n_rain + .5, n - n_rain + .5)) %&gt;%\n        # proportion of all events for each category\n        mutate(pct = n_rain / sum(n_rain))\n        # this was the original but this would just be proportion of the total data for each caategory\n        # mutate(pct = n / sum(n))\n    ret\n}\n\n# error bar plot\n# cat vs probability of event w/CIs\ntrain %&gt;%\n    # cat predictor\n    group_by(location = fct_lump(location, 50)) %&gt;%\n    # apply custom function\n    group_binary_prop() %&gt;%\n    mutate(location = fct_reorder(location, pct_rain)) %&gt;%\n    ggplot(aes(pct_rain, location)) +\n    geom_point(aes(size = pct)) +\n    geom_errorbarh(aes(xmin = low, xmax = high), height = .3) +\n    scale_size_continuous(labels = percent, guide = \"none\", range = c(.5, 4)) +\n    scale_x_continuous(labels = percent) +\n    labs(x = \"Probability of raining tomorrow\",\n      y = \"\",\n      title = \"What locations get the most/least rain?\",\n      subtitle = \"Including 95% confidence intervals. Size of points is proportional to frequency\")\n\nBinary Outcome: Group by cat predictors and calculate proportion of event\nThis needs some tidyeval so it can generalize to other binary(?) outcome vars\n\nSimpler (uncommented) version\nsummarize_churn &lt;- function(tbl) {\n    tbl %&gt;%\n        summarize(n = n(),\n                  n_churned = sum(churned == \"yes\"),\n                  pct_churned = n_churned/n,\n                  low = qbeta(.025, n_churned + .5, n - n_churned + .5), \n                  high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\n        arrange(desc(n))\n}\n\nplot_categorical &lt;- function(tbl, categorical, ...) {\n    tbl %&gt;%       \n        ggplot(aes(pct_churned, cat_pred), ...) +\n        geom_col() +\n        geom_errorbar(aes(xmin = low, xmax = high), height = 0.2, color = red) +\n        scale_x_continuous(labels = percent) +\n        labs(x = \"% in category that churned\")\n}\n\ndata %&gt;%\n    group_by(cat_var) %&gt;%\n    summarize_churn() %&gt;%\n    plot_categorical(cat_var)\nBinary Outcome vs Two Binned Continuous\n\nsummarize_churn &lt;- function(tbl) {\n    tbl %&gt;%\n        summarize(n = n(),\n                  n_churned = sum(churned == \"yes\"),\n                  pct_churned = n_churned/n,\n                  low = qbeta(.025, n_churned + .5, n - n_churned + .5), \n                  high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\n        arrange(desc(n))\n}\n\ndata %&gt;%\n    mutate(avg_trans_amt = total_trans_amt / total_trans_ct,\n          total_transactions = ifelse(total_trans_ct &gt;= 50,\n                                        \"&gt; 50 Transactions\",\n                                        \"&lt; 50 Transactions\"),\n          avg_transaction = ifelse(avg_trans_amt &gt;= 50,\n                                      \"&gt; $50 Average\",\n                                      \"&lt; $50 Average\")\n    ) %&gt;%\n    group_by(total_transactions,avg_transaction) %&gt;%\n    summarize_churn() %&gt;%\n    ggplot(aes(total_transactions, avg_transaction)) +\n    geom_tile(aes(fill = pct_churned)) +\n    geom_text(aes(label = percent(pct_churned, 1))) +\n    scale_fill_gradient2(low = \"blue\", high = \"red\", midpoint = 0.3) +\n    labs(x = \"How many transactions did the customer do?\",\n    y = \"What was the average transaction size?\",\n    fill = \"% churned\",\n    title = \"Dividing customers into segments\")\n\nSegmentation chart\nEach customer’s spend is averaged and binned (&gt; or &lt; $50)\nEach customer’s transaction count is binned (&gt; or &lt; 50)\nThe df is grouped by both binned vars, so you get 4 subgroups\n\nProportions of each subgroup that falls into the event category of then binary variable (e.g. churn) are calculated\nLow and high quantiles for churn counts are calculated (typical calc of CIs for the proportions of binary variables)\n\nUsed to add context of whether these are high proportions, low proportions, etc.",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-inter",
    "href": "qmd/eda-general.html#sec-eda-gen-inter",
    "title": "General",
    "section": "Interactions",
    "text": "Interactions\n\nMisc\n\nY-Axis is the response, X-Axis is the explanatory variable of interest, and the Grouping Variable is the moderator\nInterpretation\n\nSignificant Interactions - The lines of the graph cross or sometimes if they converge (if there’s enough data/power)\n\nThis pattern is a visual indication that the effects of one IV change as the second IV is varied.\nIf either line has a non-linear pattern (e.g. U-Shaped), yet still cross, it may indicate a non-linear interaction\n\nNon-Significant Interactions - Lines that are close to parallel.\n\nAlso see\n\nRegression, Interactions for details\nDiagnostics, Model Agnostic &gt;&gt; DALEX &gt;&gt; Instance Level &gt;&gt; Break-Down &gt;&gt; Example: Assume Interactions\n\nTypical Format: outcome_mean vs pred_var by pred_var\ndata %&gt;% \n  group_by(pred1, pred2) %&gt;% \n  summarize(out_mean = mean(outcome)) %&gt;% \n  ggplot(aes(y = out_mean, x = pred1, color = pred2)+\n    geom_point() +\n    geom_line()\n\nMay also need a “group = pred2” in the aes function\n\n\n\n\nContinuous Outcome\n\nContinuous vs Continuous, Scatter with Smoother by a Categorical\n\nggplot(w, aes(x=age, y=price, color=factor(class))) +\n  geom_point() +\n  geom_smooth() +\n  scale_y_continuous(trans='sqrt') +\n  guides(color=guide_legend(title='Class')) +\n  hlabs(age, price)\n\nContinuous outcome has been transformed so that the lower values can be more visible\n“Class” == 1 ⨯ Age shows some variation but the other two classes do not seem to show much. Lookng at the scatter of red dots, I’m skeptical that variation being shown by the curve.\n\nAlthough the decent separation of the “Class” groups may be what indicates an informative interaction\n\n\nContinuous vs Binary by Binary\n\n\nSignificant interaction effect (crossing)\n\nVariable A had no significant effect on participants in Condition B1 but caused a decline from A1 to A2 for those in Condition B2\n\n\nContinuous vs Continuous by Categorical\n\nplot_manufacturer &lt;- function(group) {\n\n  ## check if input is valid\n  if (!group %in% mpg$manufacturer) stop(\"Manufacturer not listed in the data set.\")\n\n  ggplot(mapping = aes(x = hwy, y = displ)) +\n    ## filter for manufacturer of interest\n    geom_point(data = filter(mpg, manufacturer %in% group), \n               color = \"#007cb1\", alpha = .5, size = 4) +\n    ## add shaded points for other data\n    geom_point(data = filter(mpg, !manufacturer %in% group), \n               shape = 1, color = \"grey45\", size = 2) +\n    scale_x_continuous(breaks = 2:8*5) +\n    ## add title automatically based on subset choice\n    labs(x = \"Highway gallons\", y = \"Displacement\", \n         title = group, color = NULL)\n}\n\ngroups &lt;- unique(mpg$manufacturer)\nmap(groups, ~plot_manufacturer(group = .x))\n\nThe grouping variable is the facet variable but also highlights the dots with color\nHighlighting plus using all the data in each chart helps add context with the other groups when you want to compare groups but in a low data situation.\n\nContinuous vs Continuous by Ordinal\n\nplot_scatter_lm &lt;- function(data, var1, var2, pointsize = 2, transparency = .5, color = \"\") {\n\n  ## check if inputs are valid\n  if (!is.data.frame(data)) stop(\"data needs to be a data frame.\")\n  if (!is.numeric(pull(data[var1]))) stop(\"Column var1 needs to be of type numeric, passed as string.\")\n  if (!is.numeric(pull(data[var2]))) stop(\"Column var2 needs to be of type numeric, passed as string.\")\n  if (!is.numeric(pointsize)) stop(\"pointsize needs to be of type numeric.\")\n  if (!is.numeric(transparency)) stop(\"transparency needs to be of type numeric.\")\n  if (color != \"\") { if (!color %in% names(data)) stop(\"Column color needs to be a column of data, passed as string.\") }\n\n  g &lt;- \n    ggplot(data, aes(x = !!sym(var1), y = !!sym(var2))) +\n    geom_point(aes(color = !!sym(color)), size = pointsize, alpha = transparency) +\n    geom_smooth(aes(color = !!sym(color), color = after_scale(prismatic::clr_darken(color, .3))), \n                method = \"lm\", se = FALSE) +\n    theme_minimal(base_family = \"Roboto Condensed\", base_size = 15) +\n    theme(panel.grid.minor = element_blank(),\n          legend.position = \"top\")\n\n  if (color != \"\") { \n    if (is.numeric(pull(data[color]))) {\n      g &lt;- g + scale_color_viridis_c(direction = -1, end = .85) +\n        guides(color = guide_colorbar(\n          barwidth = unit(12, \"lines\"), barheight = unit(.6, \"lines\"), title.position = \"top\"\n        ))\n    } else {\n      g &lt;- g + scale_color_brewer(palette = \"Set2\")\n    }\n  }\n\n  return(g)\n}\n\nmap2(\n  c(\"displ\", \"displ\", \"hwy\"), \n  c(\"hwy\", \"cty\", \"cty\"),\n  ~plot_scatter_lm(\n    data = mpg, var1 = .x, var2 = .y, \n    color = \"cyl\", pointsize = 3.5\n  )\n)\n\nA continuous color scale is used for the ordinal variable\nTrend shows relationship follows the ordinal variable values for the most part which might indicate that this interaction would be predictive\n\nInteresting values might be at dots where the colors are swapped — defying the order of the ordinal variable\n\n\nContinuous vs Continuous by Categorical by Categorical\n\nplot_manufacturer_marginal &lt;- function(group, save = FALSE) {\n\n  ## check if input is valid\n  if (!group %in% mpg$manufacturer) stop(\"Manufacturer not listed in the data set.\")\n  if (!is.logical(save)) stop(\"save should be either TRUE or FALSE.\")\n\n  ## filter data\n  data &lt;- filter(mpg, manufacturer %in% group)\n\n  ## set limits\n  lims_x &lt;- range(mpg$hwy) \n  lims_y &lt;- range(mpg$displ)\n\n  ## define colors\n  pal &lt;- RColorBrewer::brewer.pal(n = n_distinct(mpg$class), name = \"Dark2\")\n  names(pal) &lt;- unique(mpg$class)\n\n  ## scatter plot\n  main &lt;- ggplot(data, aes(x = hwy, y = displ, color = class)) +\n    geom_point(size = 3, alpha = .5) +\n    scale_x_continuous(limits = lims_x, breaks = 2:8*5) +\n    scale_y_continuous(limits = lims_y) +\n    scale_color_manual(values = pal, name = NULL) +\n    labs(x = \"Highway miles per gallon\", y = \"Displacement\") +\n    theme(legend.position = \"bottom\")\n\n  ## boxplots\n  right &lt;- ggplot(data, aes(x = manufacturer, y = displ)) +\n    geom_boxplot(linewidth = .7, color = \"grey45\") +\n    scale_y_continuous(limits = lims_y, guide = \"none\", name = NULL) +\n    scale_x_discrete(guide = \"none\", name = NULL) +\n    theme_void()\n\n  top &lt;- ggplot(data, aes(x = hwy, y = manufacturer)) +\n    geom_boxplot(linewidth = .7, color = \"grey45\") +\n    scale_x_continuous(limits = lims_x, guide = \"none\", name = NULL) +\n    scale_y_discrete(guide = \"none\", name = NULL) +\n    theme_void()\n\n  ## combine plots\n  p &lt;- top + plot_spacer() + main + right + \n    plot_annotation(title = group) + \n    plot_layout(widths = c(1, .05), heights = c(.1, 1))\n\n  ## save multi-panel plot\n  if (isTRUE(save)) {\n    ggsave(p, filename = paste0(group, \".pdf\"), \n           width = 6, height = 6, device = cairo_pdf)\n  }\n\n  return(p)\n}\n\nplot_manufacturer_marginal(\"Dodge\")\n\n{ggside} should be able to add these marginal plots with fewer lines of code.\nThis is one of a set of facetted charts by the categorical, “manufacturer”\nDots are grouped by categorical, “class”\nTop boxplot shows a minivan as an outlier in terms of hwy mpg.\nBox plots and the scatter plot are combined using {patchwork}\n\nCorrelation Heatmaps\n\nFilter data by different levels of a categorical, then note how correlations between numeric predictors and the numeric outcome change\nExample: PM 2.5 pollution (outcome) vs complete dataset and filtered for Wind Direction = NE\n\nComplete\n\nWind Direction = NE\n\nInterpretation\n\nTemperature’s correlation (potentially its predictive strength) would lessen if would be interacted with Wind Direction. So we do NOT want to interact wind direction and temperature\n\nArticle didn’t show whether it increases with other directions\n\nWind Strength’s (cws) correlation with the outcome would increase if interacted with Wind Direction. So we do want to interacted wind direction and wind strength\n\nFor ML, I think you’d dummy the wind direction, then multiply windspeed times each of the dummies.\n\n\n\n\nBoxplot by Discrete (Binned) Continuous\n\npmin can be similarily used as fct_lump (see below) but for discrete integer variables\n\nIf the distribution of the discrete numeric is skewed to the right, then pmin will bin all integers larger than some number\n\nMost of the distribution are small integers and the rest will be binned into a sort of “other” category (e.g. 14)\n\nIf the distribution is skewed to the left, pmax can be used similarily.\n\ndata %&gt;%\n   mutate(integer_var = pmin(integer_var, 14) %&gt;%\n   ggplot(aes(int_var, numeric_outcome, group = int_var)) +\n   geom_boxplot()\n\nIf all the medians line up then no relationship. A slope or nonlinear pattern shows relationship.\n\n\n\n\n\n\nCategorical Outcome\n\nNumeric vs Numeric by Cat Outcome\n\nScatter with 45 degree line\nggplot(aes(num_predictor1, num_predictor2, color = cat_outcome_var)) +\n   geom_point() +\n   geom_abline(color = \"red\")\n\nLook for groupings or other patterns wrt to cat var.\nCat-var colored points above line skew more towards the higher y-var than x-var and vice versa for below the 45 degree line.\nLine also shows how linearly correlated the two num vars are.\nIf clustering present, could indicate a good interaction pair with the numeric : cat_var\n\nScatter with linear smooth (or loess)\n\nggplot(aes(num_predictor1, num_predictor2)) +\n   geom_point(alpha = 0.25) +\n   geom_smooth(aes(color = cat_outcome_var), method = \"lm\")\n\nProduces a lm line for each outcome var category\nLooking for differing trends for ranges of values on the x-axis. A pattern for one line that is substantially different from the other line\nExample: At around 28, the blue line trend rises while the red line continues to slope downwards, and they actually cross to where at some threshold of x, the relationship is the opposite. So an interaction is likely present\n\n\nBinary Outcome (pct_event) vs Discrete by Discrete (or binary in this case)\n\ndata %&gt;%\n    mutate(avg_trans_amt = total_trans_amt / total_trans_ct) %?%\n    group_by(total_trans_ct = cut(total_trans_ct, c(0,30, 40, 50, 60, 80, Inf)),\n            avg_trans_amt = ifelse(avg_trans_amt &gt;= 50, \"&gt; $50\", \"&lt; $50\") %&gt;%\n            # use to figure out best cut point(s) that keeps the ribbon width small-ish on all lines\n            # avg_trans_amt = cut(avg_trans_amt, c(0, 50, 100, 130, Inf)) %&gt;%   \n    summarize(n = n(),\n              n_churned = sum(churned == \"yes\"),\n              pct_churned = n_churned/n,\n              low = qbeta(.025, n_churned + .5, n - n_churned + .5), \n              high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\n        arrange(desc(n)) %&gt;%\n    ggplot(aes(total_trans_ct, pct_churned, color = avg_trans_amt) +\n    geom_point() +\n    geom_line() +\n    geom_ribbon(aes(ymin = low, ymax = high))           \n\nInterpretation:\n\nClear alternating trend from about 0 to 40 on the x-axis says there’s probably an interaction (at least with the binned versions of these variables) between total_trans_ct and avg_trans_amt.\n\ni.e. The relationship between transaction count and churned (binary outcome) (pct_churned) depends on the average transaction amount\n\n\nExample: The cut points for avg_trans_amt were chosen from its distribution\n\nThe distribution was bi-modal and the 3 cutpoints were the 1st mode, point that splits both modal distributions, and the 2nd mode.\n{Upsetr} might be useful to examine bimodal structure and determine cutpoints based on categorical predictor values and not just outcome values\n{gghdr} - viz for multi-modal distribtutions\nAlso see Regression, Other &gt;&gt; Mult-Modal\n\nExample of likely no interaction\n\n\nBlue and red lines move in unison. Same trend directions.\n\nThere is separation, so the mean value of percent churn is different. Also, the slopes are different, so the rates of increase and decrease would be different. I’m not convinced. I’d like to see if an interaction term wouldn’t be significant\nkaggle sliced s01e07 dataset - percent churn (y-axis), revolving balance bucketed (x-axis), color = total_transactions dicotomized. DRob video for the code.\n\n\n\nBinary Outcome (pct_event) vs Categorical by Categorical\n\nSliding Window Continuous vs Binary Outcome (Proportion of Event) by Categorical\n\nggplot(z, aes(x=price, y=`Moving Proportion`, col=factor(class))) +\n  geom_line() + guides(color=guide_legend(title='Class')) +\n  xlab(hlab(price)) + ylab('Survival')\n\n“Moving Proportion” is the mean of the binary outcome (probability of an event) over a sliding window of “Total Price”\n“Total Price” should be sorted in ascending order and grouped by “Class” before the sliding window is applied\nHarrell uses a default window of 15 observations on either side of the target point, but says the results can be noisy. Recommends passing the results through a smoother\n\nSo, might want to add a geom_smooth to the code chunk\nI might like to see the data points to see how many points at the ends of lines there are. Smoothed lines can be misleading on the boundaries.\n\n\nSliding Window Continuous vs Binary Outcome (Proportion of Event) by 2 Categoricals\n\nggplot(d, aes(x=age, y=`Moving Proportion`, col=factor(class))) +\n  geom_smooth() +\n  facet_wrap(~ sex) +\n  ylim(0, 1) + xlab(hlab(age)) + ylab('Survival') +\n  guides(color=guide_legend(title='Class'))\n\nSimilar to above but grouped by 2 variables before the sliding window calculation.\n\nGrouped Bar\n\nsummarize_churn &lt;- function(tbl) {\n    tbl %&gt;%\n        summarize(n = n(),\n                  n_churned = sum(churned == \"yes\"),\n                  pct_churned = n_churned/n,\n                # Jeffrey's Interval (Bayesian CI)\n                  low = qbeta(.025, n_churned + .5, n - n_churned + .5), \n                  high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\n        arrange(desc(n))\n}\nplot_categorical &lt;- function(tbl, categorical, ...) {\n    tbl %&gt;%       \n        ggplot(aes(pct_churned, [{{categorical}}]{style='color: goldenrod'}), ...) + \n        geom_col(position = position_dodge()) + \n        geom_errorbar(aes(xmin = low, xmax = high),\n                      height = 0.2, color = red,\n                      position = position_dodge(width = 1) +\n        scale_x_continuous(labels = percent) +\n        labs(x = \"% in category that churned\")\n}\ndata %&gt;%\n    group_by(cat_var1, cat_var2) %&gt;%\n    summarize_churn() %&gt;%\n    plot_categorical(cat_var1, fill = cat_var2, group = cat_var2)\n\nInterpretation: Probably not an interaction variable. Pct Churned by education Level doesn’t vary (much) by  Gender especially if you take the error bars into account\n\nOnly for “college” do you see a flip in the relationship where females churn more than men, but it’s still within the error bars.\n\n\n\nBinary Outcome vs Binary by Categorical\n\n\nNot certain but I’d think you’d want your outcome on the x-axis. Although, if you swapped the x-axis variable with the grouping variable, you’d probably come to the same conclusion. Therefore, it may not matter that much\nShows percent, and not counts",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/css-general.html",
    "href": "qmd/css-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CSS",
      "General"
    ]
  },
  {
    "objectID": "qmd/css-general.html#sec-css-gen-misc",
    "href": "qmd/css-general.html#sec-css-gen-misc",
    "title": "General",
    "section": "",
    "text": "Resources\n\nhttps://css-tip.com/\nWidget testing parameter values for css styling a div box\n\nEqual Column Widths\n\nCSS comment - /* comment */\nSelector formats\n\nSyntax: #&lt;class&gt;.&lt;id&gt;&lt;additional-stuff&gt;\nExample:\n\nCSS\n#header.fluid-row::before{\n}\nHTML\n&lt;div class=\"fluid-row\" id=\"header\"&gt; == $0\n::before\n&lt;/div&gt;\n\n\nInclude css styling directly into a html page\n\nExample: Via HTML style tag\n&lt;style&gt;\nbody {\n  padding: 50px 25px 0px 25px;\n  font-family: 'Roboto', sans-serif;\n  font-size: 19px;\n}\n&lt;/style&gt;\nExample: Via R chunk\nhtmltools::tags\\$link(href = \"https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght\\@0,400;0,700;1,400&display=swap\",\n                      rel = \"stylesheet\")\nExample: styling of a legend html div\n&lt;style type='text/css'&gt;\n  .my-legend .legend-title {\n    text-align: left;\n    margin-bottom: 8px;\n    font-weight: bold;\n    font-size: 90%;\n    }\n  .my-legend .legend-scale ul {\n    margin: 0;\n    padding: 0;\n    float: left;\n    list-style: none;\n    }\n  .my-legend .legend-scale ul li {\n    display: block;\n    float: left;\n    width: 50px;\n    margin-bottom: 6px;\n    text-align: center;\n    font-size: 80%;\n    list-style: none;\n    }\n  .my-legend ul.legend-labels li span {\n    display: block;\n    float: left;\n    height: 15px;\n    width: 50px;\n    }\n  .my-legend .legend-source {\n    font-size: 70%;\n    color: #999;\n    clear: both;\n    }\n  .my-legend a {\n    color: #777;\n    }\n&lt;/style&gt;\n\nSee link for details on the legend div element that uses this CSS",
    "crumbs": [
      "CSS",
      "General"
    ]
  },
  {
    "objectID": "qmd/css-general.html#centering",
    "href": "qmd/css-general.html#centering",
    "title": "General",
    "section": "Centering",
    "text": "Centering\n\nNotes from How To Center a Div\nCenter Horizontally with auto-margins\n.element {\n  max-width: fit-content;\n  margin-left: auto;\n  margin-right: auto;\n  /* margin-inline: auto*/\n}\n\nmax-width is used because if width is used instead, it would lock it to that size, and the element would overflow when the container is really narrow.\nIncluding only margin-left: auto will force the div flush with the right side and vice verse with margin-right\nmargin-inline: auto can replace both margin-left and margin-right to center the div\n\nCenter Vertically and Horizontally with Flexbox\n/* single element */\n.container {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n}\n/* multiple elements */\n.container {\n  display: flex;\n  flex-direction: row;\n  justify-content: center;\n  align-items: center;\n  gap: 4px;\n}\n\nflex-direction controls the direction in which the items are aligned, and it can have other values: column, row-reverse, column-reverse\n\nCenter Within a Viewport\n.element {\n  position: fixed;\n  inset: 0px;\n  width: 12rem;\n  height: 5rem;\n  max-width: 100vw;\n  max-height: 100dvh;\n  margin: auto;\n}\n\nUseful for elements like dialogs, prompts, and GDPR banners need to be centered within the viewport.\nComplex and has more settings that depend on the element. See article for details but there are four main concepts:\n\nFixed positioning\nAnchoring to all 4 edges with inset: 0px\nConstrained width and height\nAuto margins\n\nOmitting top: 0px will anchor the element to the bottom\n\nUse calc with max-width to make sure theres a buffer around the element\nmax-width: calc(\n    100vw - 8px * 2\n  );\n\n\nCentering Elements With Unknown Sizes\n.element {\n  position: fixed;\n  inset: 0;\n  width: fit-content;\n  height: fit-content;\n  margin: auto;\n}\n\nfit-content is doing the work\nThis might just be for a viewport",
    "crumbs": [
      "CSS",
      "General"
    ]
  },
  {
    "objectID": "qmd/code-optimization.html",
    "href": "qmd/code-optimization.html",
    "title": "Optimization",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Code",
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/code-optimization.html#sec-code-opt-misc",
    "href": "qmd/code-optimization.html#sec-code-opt-misc",
    "title": "Optimization",
    "section": "",
    "text": "A while loop is faster than a recursive function\nungroup before performing calculations in mutate or summarize when that calculation doesn’t need to be performed within-group (i.e. per factor level)\nString functions\n\n“fixed” searches, fixed = TRUE are fastest overall\n\nSearches involving fixed strings (e.g. “banana”) that don’t require regular expressions\n\nPCRE2, perl = TRUE , is fastest for regular expressions\n\nAdding Rows to a Matrix (link, link)\n\nrbind is very slow and cbind + t() isn’t much better\nfrbind &lt;- function(n) {\n    res &lt;- vector()\n    for (i in 1:n) {\n        res &lt;- rbind(res, runif(n))\n    }\n    res\n}\nfcbind &lt;- function(n) {\n    res &lt;- vector()\n    for (i in 1:n) {\n        res &lt;- cbind(res, runif(n))\n    }\n    t(res)\n}\nAdd rows to a list then combine into a matrix\n# grow list which is converted to a matrix at the end.\nflist &lt;- function(n) {\n    res &lt;- list()\n    for (i in 1:n) {\n        res[[length(res) + 1]] &lt;- runif(n)\n    }\n    do.call(rbind, res)\n}\nflist2 &lt;- function(n) {\n    res &lt;- list()\n    for (i in 1:n) {\n        res[[length(res) + 1]] &lt;- runif(n)\n    }\n    len &lt;- length(res)\n    res &lt;- res |&gt; purrr::list_c()\n    dim(res) &lt;- c(len, len)\n    res\n}\n\nlist_c uses C under the hood and might be faster on larger matrices.",
    "crumbs": [
      "Code",
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/code-optimization.html#sec-code-opt-bench",
    "href": "qmd/code-optimization.html#sec-code-opt-bench",
    "title": "Optimization",
    "section": "Benchmarking",
    "text": "Benchmarking\n\nMisc\n\nMastering Software Development in R, Ch. 2.71\n\n{bench}\n\nMisc\n\nAutomatically checks that each approach gives the same output, so that you don’t mistakenly compare apples and oranges\n\nExample: Basic\nres &lt;-\n  bench::mark(\n    approach_1 = Reduce(sum, numbers),\n    approach_2 = sum(unlist(numbers))\n  )\n\nres %&gt;% select(expression, median)\n\n#&gt; # A tibble: 2 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 approach_1  2.25µs\n#&gt; 2 approach_2 491.97ns\n\n{microbenchmark}\n\nExample: Basic\n\nrecord_temp_perf &lt;- microbenchmark(find_records_1(example_data, 27), \n                                  find_records_2(example_data, 27))\nrecord_temp_perf\n\n## Unit: microseconds\n##                              expr      min      lq      mean  median      uq\n##  find_records_1(example_data, 27)  114.574  136.680  156.1812  146.132  163.676\n##  find_records_2(example_data, 27) 4717.407 5271.877 6113.5087 5867.701 6167.709\n##        max neval\n##    593.461  100\n##  11334.064  100\n\nlibrary(ggplot2)\nautoplot(record_temp_perf)\n\nDefault: 100 iterations\nTimes are given in a reasonable unit, based on the observed profiling times (units are given in microseconds in this case).\nOutput\n\nmin - min time\nlq - lower quartile\nmean, median\nuq - upper quartile\nmax - max time",
    "crumbs": [
      "Code",
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/code-optimization.html#sec-code-opt-prof",
    "href": "qmd/code-optimization.html#sec-code-opt-prof",
    "title": "Optimization",
    "section": "Profiling",
    "text": "Profiling\n\nMisc\n\nResources\n\nMastering Software Development in R, Ch. 2.72\nAdvanced R, Ch. 23\n\n\n{profvis}\n\nExample: Basic\n\nstep_1 &lt;- function() {\n  pause(1)\n}\nstep_2 &lt;- function() {\n  pause(2)\n}\nslow_function &lt;- function() {\n  step_1()\n\n  step_2()\n\n  TRUE\n}\nresult &lt;- profvis(slow_function())\nresult\n\nBottom Row: outermost function\n2nd Row from the bottom are the functions in the next enviromental layer (e.g. “step_1” and “step_2”)\n\n“step_2” takes about 2/3 of the total function execution time or twice the execution time of “step_1”\n\n3rd Row from the bottom (top row) are the functions in each of those other functions\n\n“pause” in “step_2” takes about 2/3 of the total function execution time or twice the execution time of “pause” in “step_1”",
    "crumbs": [
      "Code",
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/code-optimization.html#sec-code-opt-py",
    "href": "qmd/code-optimization.html#sec-code-opt-py",
    "title": "Optimization",
    "section": "Python",
    "text": "Python\n\nBenchmarking\n\n{{time}}(built-in module)\nimport time\nstart = time.perf_counter()\ntime.sleep(1) # do work\nelapsed = time.perf_counter() - start\nprint(f'Time {elapsed:0.4}')\n#&gt; Time 1.001\n\nMemory Usage\n\n{{memory-profiler}}\n\nBasic usage for a function\nfrom memory_profiler import memory_usage\nmem, retval = memory_usage((fn, args, kwargs), retval=True, interval=1e-7)\n\ninterval: For very quick operations the function fn might be executed more than once. By setting interval to a value lower than 1e-6, we force it to execute only once.\nretval: Tells the function to return the result of fn.\n\nNon-interactive usage\n$ python -m memory_profiler example.py\n#&gt; Line #    Mem usage  Increment   Line Contents\n#&gt; ==============================================\n#&gt;      3                           @profile\n#&gt;      4      5.97 MB    0.00 MB   def my_func():\n#&gt;      5     13.61 MB    7.64 MB       a = [1] * (10 ** 6)\n#&gt;      6    166.20 MB  152.59 MB       b = [2] * (2 * 10 ** 7)\n#&gt;      7     13.61 MB -152.59 MB       del b\n#&gt;      8     13.61 MB    0.00 MB       return a\n\n\nProfile decorator\nimport time\nfrom functools import wraps\nfrom memory_profiler import memory_usage\n\ndef profile(fn):\n    @wraps(fn)\n    def inner(*args, **kwargs):\n        fn_kwargs_str = ', '.join(f'{k}={v}' for k, v in kwargs.items())\n        print(f'\\n{fn.__name__}({fn_kwargs_str})')\n\n        # Measure time\n        t = time.perf_counter()\n        retval = fn(*args, **kwargs)\n        elapsed = time.perf_counter() - t\n        print(f'Time   {elapsed:0.4}')\n\n        # Measure memory\n        mem, retval = memory_usage((fn, args, kwargs), retval=True, timeout=200, interval=1e-7)\n\n        # Get Peak Memory Usage\n        print(f'Memory {max(mem) - min(mem)}')\n        return retval\n\n    return inner\n\n@profile\ndef work(n):\n   for i in range(n):\n       2 ** n\n\nwork(10)\n#&gt; work()\n#&gt; Time   0.06269\n#&gt; Memory 0.0\n\nwork(n=10000)\n#&gt; work(n=10000)\n#&gt; Time   0.3865\n#&gt; Memory 0.0234375",
    "crumbs": [
      "Code",
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/code-optimization.html#sec-code-opt-rftf",
    "href": "qmd/code-optimization.html#sec-code-opt-rftf",
    "title": "Optimization",
    "section": "Replacements for Tidyverse Functions",
    "text": "Replacements for Tidyverse Functions\n\nMisc\n\nNotes from: Writing performant code with tidy tools\n\nAlso provides links to more {vctrs} recipes from their tidymodels github pull requests\n\nFor code that relies on group_by()and sees heavy traffic, see vctrs::list_unchop(), vctrs::vec_chop(), and vctrs::vec_rep_each().\n\nselect\nbench::mark(\n  dplyr = select(mtcars_tbl, hp),\n  `[.tbl_df` = mtcars_tbl[\"hp\"]\n) %&gt;%\n  select(expression, median)\n#&gt; # A tibble: 2 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyr      527.01µs\n#&gt; 2 [.tbl_df    8.08µs\n\nWinner: base R subsetting\n\nfilter\nWres &lt;-\n  bench::mark(\n    dplyr = filter(mtcars_tbl, hp &gt; 100),\n    vctrs = vec_slice(mtcars_tbl, mtcars_tbl$hp &gt; 100),\n    `[.tbl_df` = mtcars_tbl[mtcars_tbl$hp &gt; 100, ]\n  ) %&gt;%\n    select(expression, median)\nres\n#&gt; # A tibble: 3 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyr      289.93µs\n#&gt; 2 vctrs        4.63µs\n#&gt; 3 [.tbl_df    23.74µs\n\nWinner: vctrs::vec_slice \n\nmutate\nbench::mark(\n  dplyr = mutate(mtcars_tbl, year = 1974L),\n  `$&lt;-.tbl_df` = {mtcars_tbl$year &lt;- 1974L; mtcars_tbl}\n) %&gt;%\n  select(expression, median)\n\n#&gt; # A tibble: 2 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyr      302.5µs\n#&gt; 2 $&lt;-.tbl_df  12.8µs\n\nWinner: base R assignment\n\nmutate and relocate\nbench::mark(\n  mutate = mutate(mtcars_tbl, year = 1974L, .after = make_model),\n  relocate = relocate(mtcars_tbl, year, .after = make_model),\n  `[.tbl_df` = \n      mtcars_tbl[\n        c(left_cols, \n          colnames(mtcars_tbl[!colnames(mtcars_tbl) %in% left_cols])\n        )\n      ],\n  check = FALSE\n) %&gt;% \n  select(expression, median)\n\n#&gt; # A tibble: 3 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 mutate        1.2ms\n#&gt; 2 relocate    804.3µs\n#&gt; 3 [.tbl_df    19.1µs\n\nWinner: base R\n\npull\nbench::mark(\n  dplyr = pull(mtcars_tbl, hp),\n  `$.tbl_df` = mtcars_tbl$hp,\n  `[[.tbl_df` = mtcars_tbl[[\"hp\"]]\n) %&gt;%\n  select(expression, median)\n\n#&gt; # A tibble: 3 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyr      101.19µs\n#&gt; 2 $.tbl_df  615.02ns\n#&gt; 3 [[.tbl_df    2.25µs\n\nWinner: base R bracket subsetting\n\nbind_*\nbench::mark(\n  dplyr = bind_rows(mtcars_tbl, mtcars_tbl),\n  vctrs = vec_rbind(mtcars_tbl, mtcars_tbl)\n) %&gt;%\n  select(expression, median)\n\n#&gt; # A tibble: 2 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyr          44µs\n#&gt; 2 vctrs        14.3µs\n\nbench::mark(\n  dplyr = bind_cols(mtcars_tbl, tbl),\n  vctrs = vec_cbind(mtcars_tbl, tbl)\n) %&gt;%\n  select(expression, median)\n#&gt; # A tibble: 2 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyr        60.7µs\n#&gt; 2 vctrs        26.2µs\n\nWinners: vctrs::vec_cbind and vctrs::vec_rbind\n\nCreate Tibble\nbench::mark(\n  tibble = tibble(a = 1:2, b = 3:4),\n  new_tibble_df_list = new_tibble(df_list(a = 1:2, b = 3:4), nrow = 2),\n  new_tibble_list = new_tibble(list(a = 1:2, b = 3:4), nrow = 2)\n) %&gt;% \n  select(expression, median)\n#&gt; # A tibble: 3 × 2\n#&gt;  expression          median\n#&gt;  &lt;bch:expr&gt;        &lt;bch:tm&gt;\n#&gt; 1 tibble            165.97µs\n#&gt; 2 new_tibble_df_list  16.69µs\n#&gt; 3 new_tibble_list      4.96µs\n\nWinner: new_tibble_list\n\nJoins\n\nQuestions:\n\nIf this join happens multiple times, is it possible to express it as one join and then subset it when needed?\n\ni.e. if a join happens inside of a loop but the elements of the join are not indices of the loop, it’s likely possible to pull that join outside of the loop and then vctrs::vec_slice() its results inside of the loop. Am I using the complete outputted join result or just a portion? If I end up only making use of column names, or values in one column (as with joins approximating lookup tables), or pairings between two columns, I may be able to instead use $.tbl_df or [.tbl_df (see above, Pull).\n\n\nFor problems even a little bit more complex, e.g. if there were possibly multiple matching or if I wanted to keep all rows, then expressing this join with more bare-bones operations quickly becomes less readable and more error-prone. In those cases, too, joins in dplyr have a relatively small amount of overhead when compared to the vctrs backends underlying them. So, optimize carefully.\nExample: inner_join vs vctrs::vec_slice (Note: *only 0 or 1 match possible*)\nsupplement_my_cars &lt;- function() {\n  # locate matches, assuming only 0 or 1 matches possible\n  loc &lt;- vec_match(my_cars$make_model, mtcars_tbl$make_model)\n\n  # keep only the matches\n  loc_mine &lt;- which(!is.na(loc))\n  loc_mtcars &lt;- vec_slice(loc, !is.na(loc))\n\n  # drop duplicated join column\n  my_cars_join &lt;- my_cars[setdiff(names(my_cars), \"make_model\")]\n  vec_cbind(\n    vec_slice(mtcars_tbl, loc_mtcars),\n    vec_slice(my_cars_join, loc_mine)\n  )\n}\nsupplement_my_cars()\n#&gt; # A tibble: 1 × 13\n#&gt;  make_model    mpg  cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#&gt;  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Honda Civic  30.4    4  75.7    52  4.93  1.62  18.5    1    1    4    2\n#&gt; # ℹ 1 more variable: color &lt;chr&gt;\n\nbench::mark(\n  inner_join = inner_join(mtcars_tbl, my_cars, \"make_model\"),\n  manual = supplement_my_cars()\n) %&gt;%\n  select(expression, median)\n#&gt; # A tibble: 2 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 inner_join    438µs\n#&gt; 2 manual      50.7µs\n\nnest\nbench::mark(\n  nest = nest(mtcars_tbl, .by = c(cyl, am)),\n  vctrs = {\n    res &lt;- \n      vec_split(\n        x = mtcars_tbl[setdiff(colnames(mtcars_tbl), nest_cols)],\n        by = mtcars_tbl[nest_cols]\n      )\n\n    vec_cbind(res$key, new_tibble(list(data = res$val)))\n  }\n) %&gt;%\n  select(expression, median)\n\n#&gt; # A tibble: 2 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 nest        1.81ms\n#&gt; 2 vctrs      67.61µs\n\n# Results of nesting\n#&gt; # A tibble: 6 × 3\n#&gt;    cyl    am data             \n#&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;           \n#&gt; 1    6    1 &lt;tibble [3 × 10]&gt; \n#&gt; 2    4    1 &lt;tibble [8 × 10]&gt; \n#&gt; 3    6    0 &lt;tibble [4 × 10]&gt; \n#&gt; 4    8    0 &lt;tibble [12 × 10]&gt;\n#&gt; 5    4    0 &lt;tibble [3 × 10]&gt; \n#&gt; 6    8    1 &lt;tibble [2 × 10]&gt;\nglue and paste0\nvec_paste0 &lt;- function (...) {\n  args &lt;- vec_recycle_common(...)\n  exec(paste0, !!!args)\n}\n\nname &lt;- \"Simon\"\nbench::mark(\n  glue = glue::glue(\"My name is [{name}]{style='color: #990000'}.\"),\n  vec_paste0 = vec_paste0(\"My name is \", name, \".\"),\n  paste0 = paste0(\"My name is \", name, \".\"),\n  check = FALSE\n) %&gt;% \n  select(expression, median)\n\n#&gt; # A tibble: 3 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 glue        38.99µs\n#&gt; 2 vec_paste0  3.98µs\n#&gt; 3 paste0    861.01ns\n\npaste0() has some tricky recycling behavior. vec_paste0 is a middle ground in terms of both performance and safety.\nUse glue() for errors, when the function will stop executing anyway.\nFor simple pastes that are intended to be called repeatedly, use vec_paste0().",
    "crumbs": [
      "Code",
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html",
    "href": "qmd/code-snippets.html",
    "title": "Snippets",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-misc",
    "href": "qmd/code-snippets.html#sec-code-snippits-misc",
    "title": "Snippets",
    "section": "",
    "text": "Check whether an environment variable is empty\nnzchar(Sys.getenv(\"blopblopblop\"))\n#&gt; [1] FALSE\nwithr::with_envvar(\n  new = c(\"blopblopblop\" = \"bla\"),\n  nzchar(Sys.getenv(\"blopblopblop\"))\n)\nUse a package for a single instance using {withr::with_package}\n\n\nUsing library() will keep the package loaded during the whole session, with_package() just runs the code snippet with that package temporarily loaded. This can be useful to avoid namespace collisions for example\n\nRead .csv from a zipped file\n# long way\ntmpf &lt;- tempfile()\ntmpd &lt;- tempfile()\ndownload.file('https://website.org/path/to/file.zip', tmpf)\nunzip(tmpf, exdir = tmpd)\ny &lt;- data.table::fread(file.path(tmpd,\n                       grep('csv$',\n                            unzip(tmpf, list = TRUE)$Name,\n                            value = TRUE)))\nunlink(tmpf)\nunlink(tmpd)\n\n# quick way\ny &lt;- data.table::fread('curl https://website.org/path/to/file.zip | funzip')\nLoad all R scripts from a directory: for (file in list.files(\"R\", full.names = TRUE)) source(file)\nView dataframe in View as html table using {kableExtra}\ndf_html &lt;- kableExtra::kbl(rbind(head(df, 5), tail(df, 5)), format = \"html\")\nprint(df_html)",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-opts",
    "href": "qmd/code-snippets.html#sec-code-snippits-opts",
    "title": "Snippets",
    "section": "Options",
    "text": "Options\n\n{readr}\noptions(readr.show_col_types = FALSE)",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-cleaning",
    "href": "qmd/code-snippets.html#sec-code-snippits-cleaning",
    "title": "Snippets",
    "section": "Cleaning",
    "text": "Cleaning\n\nRemove all objects except: rm(list=setdiff(ls(), c(\"train\", \"validate\", \"test\")))\nRemove NAs\n\ndataframes\ndf %&gt;% na.omit\ndf %&gt;% filter(complete.cases(.))\ndf %&gt;% tidyr::drop_na()\nvariables\ndf %&gt;% filter(!is.na(x1))\ndf %&gt;% tidyr::drop_na(x1)\n\nFind duplicate rows\n\n{datawizard} - Extract all duplicates, for visual inspection. Note that it also contains the first occurrence of future duplicates, unlike duplicated or dplyr::distinct. Also contains an additional column reporting the number of missing values for that row, to help in the decision-making when selecting which duplicates to keep.\ndf1 &lt;- data.frame(\n  id = c(1, 2, 3, 1, 3),\n  year = c(2022, 2022, 2022, 2022, 2000),\n  item1 = c(NA, 1, 1, 2, 3),\n  item2 = c(NA, 1, 1, 2, 3),\n  item3 = c(NA, 1, 1, 2, 3)\n)\n\ndata_duplicated(df1, select = \"id\")\n#&gt;   Row id year item1 item2 item3 count_na\n#&gt; 1   1  1 2022    NA    NA    NA        3\n#&gt; 4   4  1 2022     2     2     2        0\n#&gt; 3   3  3 2022     1     1     1        0\n#&gt; 5   5  3 2000     3     3     3        0\n\ndata_duplicated(df1, select = c(\"id\", \"year\"))\n#&gt; 1   1  1 2022    NA    NA    NA        3\n#&gt; 4   4  1 2022     2     2     2        0\ndplyr\ndups &lt;- dat %&gt;% \n  group_by(BookingNumber, BookingDate, Charge) %&gt;% \n  filter(n() &gt; 1)\nbase r\ndf[duplicated(df[\"ID\"], fromLast = F) | duplicated(df[\"ID\"], fromLast = T), ]\n\n##        ID value_1 value_2 value_1_2\n## 2  ID-003      6      5      6 5\n## 3  ID-006      1      3      1 3\n## 4  ID-003      1      4      1 4\n## 5  ID-005      5      5      5 5\n## 6  ID-003      2      3      2 3\n## 7  ID-005      2      2      2 2\n## 9  ID-006      7      2      7 2\n## 10 ID-006      2      3      2 3\n\ndf[duplicated(df[\"ID\"], fromLast = F) doesn’t include the first occurence, so also counting from the opposite direction will include all occurences of the duplicated rows\n\n\nRemove duplicated rows\n\n{datawizard} - From all rows with at least one duplicated ID, keep only one. Methods for selecting the duplicated row are either the first duplicate, the last duplicate, or the “best” duplicate (default), based on the duplicate with the smallest number of NA. In case of ties, it picks the first duplicate, as it is the one most likely to be valid and authentic, given practice effects.\ndf1 &lt;- data.frame(\n  id = c(1, 2, 3, 1, 3),\n  item1 = c(NA, 1, 1, 2, 3),\n  item2 = c(NA, 1, 1, 2, 3),\n  item3 = c(NA, 1, 1, 2, 3)\n)\n\ndata_unique(df1, select = \"id\")\n#&gt; (2 duplicates removed, with method 'best')\n#&gt;   id item1 item2 item3\n#&gt; 1  1     2     2     2\n#&gt; 2  2     1     1     1\n#&gt; 3  3     1     1     1\nbase R\ndf[!duplicated(df[c(\"col1\")]), ]\ndplyr\ndistinct(df, col1, .keep_all = TRUE)\n\nShowing all combinations present in the data and creating all possible combinations\n\nFuzzy Join (alt to case_when)\nref.df &lt;- data.frame(\n            bucket = c(“High”, “Medium-High”, “Medium-Low”, “Low”),\n            value.high = c(max(USArrests$Assault), 249, 199, 149),\n            value.low = c(250, 200, 150, min(USArrests$Assault)))\nUSArrests %&gt;% \n  fuzzy_join(ref.df, \n                    by = c(\"Assault\"=\"value.low\",\n                          \"Assault\" = 'value.high'), \n            match_fun = c(`&gt;=`,`&lt;=`)) %&gt;% \n  select(-c(value.high, value.low))\n\nAlso does partial matches\n\n\n\n\nRemove elements of a list by name\npurrr::discard_at(my_list, \"a\")\nlistr::list_remove",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-func",
    "href": "qmd/code-snippets.html#sec-code-snippits-func",
    "title": "Snippets",
    "section": "Functions",
    "text": "Functions\n\nggplot\nviz_monthly &lt;- function(df, y_var, threshhold = NULL) {\n\n  ggplot(df) +\n    aes(\n      x = .data[[\"day\"]],\n      y = .data[[y_var]]\n    ) +\n    geom_line() +\n    geom_hline(yintercept = threshhold, color = \"red\", linetype = 2) +\n    scale_x_continuous(breaks = seq(1, 29, by = 7)) +\n    theme_minimal()\n}\n\naes is on the outside\n\nThis was a function for a shiny module\nIt’s peculier. Necessary for function or module?\n\n\nCreate formula from string\nanalysis_formula &lt;- 'Days_Attended ~ W + School'\nestimator_func &lt;-  function(data) lm(as.formula(analysis_formula), data = data)\nRecursive Function\n\nExample\n# Replace pkg text with html\nreplace_txt &lt;- function(dat, patterns) {\n  if (length(patterns) == 0) {\n    return(dat)\n  }\n\n  pattern_str &lt;- patterns[[1]]$pattern_str\n  repl_str &lt;- patterns[[1]]$repl_str\n  replaced_txt &lt;- dat |&gt;\n    str_replace_all(pattern = pattern_str, repl_str)\n\n  new_patterns &lt;- patterns[-1]\n  replace_txt(replaced_txt, new_patterns)\n}\n\nArguments include the dataset and the iterable\nTests whether function has iterated through pattern list\nRemoves 1st element of the list\nreplace_text calls itself within the function with the new list and new dataset\n\nExample: Using Recall and tryCatch\nload_page_completely &lt;- function(rd) {\n  # load more content even if it throws an error\n  tryCatch({\n      # call load_more()\n      load_more(rd)\n      # if no error is thrown, call the load_page_completely() function again\n      Recall(rd)\n  }, error = function(e) {\n      # if an error is thrown return nothing / NULL\n  })\n}\n\nload_more is a user defined function\nRecall is a base R function that calls the same function it’s in.",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-calcs",
    "href": "qmd/code-snippets.html#sec-code-snippits-calcs",
    "title": "Snippets",
    "section": "Calculations",
    "text": "Calculations\n\nCompute the running maximum per group\n(df &lt;- structure(list(var = c(5L, 2L, 3L, 4L, 0L, 3L, 6L, 4L, 8L, 4L),\n              group = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L),\n                                .Label = c(\"a\", \"b\"), class = \"factor\"),\n              time = c(1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L)),\n          .Names = c(\"var\", \"group\",\"time\"),\n          class = \"data.frame\", row.names = c(NA, -10L)))\n\ndf[order(df$group, df$time),]\n#    var group time\n# 1    5    a    1\n# 2    2    a    2\n# 3    3    a    3\n# 4    4    a    4\n# 5    0    a    5\n# 6    3    b    1\n# 7    6    b    2\n# 8    4    b    3\n# 9    8    b    4\n# 10  4    b    5\n\ndf$curMax &lt;- ave(df$var, df$group, FUN=cummax)\ndf\nvar  |  group  |  time  |  curMax\n5      a        1        5\n2      a        2        5\n3      a        3        5\n4      a        4        5\n0      a        5        5\n3      b        1        3\n6      b        2        6\n4      b        3        6\n8      b        4        8\n4      b        5        8\n\n\nTime Series\n\nBase-R\n\nIntervals\n\nDifference between dates\n# Sample dates\nstart_date &lt;- as.Date(\"2022-01-15\")\nend_date &lt;- as.Date(\"2023-07-20\")\n\n# Calculate time difference in days\ntime_diff_days &lt;- end_date - start_date\n\n# Convert days to months\nmonths_diff_base &lt;- as.numeric(time_diff_days) / 30.44  # average days in a month\n\ncat(\"Number of months using base R:\", round(months_diff_base, 2), \"\\n\")\n#&gt; Number of months using base R: 18.1 \n\n\n\n\n{lubridate}\n\nDocs\nIntervals\n\nLubridate’s interval functions\nNotes from: Wrangling interval data using lubridate\nDifference between dates\n# Load the lubridate package\nlibrary(lubridate)\n\n# Sample dates\nstart_date &lt;- ymd(\"2022-01-15\")\nend_date &lt;- ymd(\"2023-07-20\")\n\n# Calculate months difference using lubridate\nmonths_diff_lubridate &lt;- interval(start_date, end_date) %/% months(1)\n\ncat(\"Number of months using lubridate:\", months_diff_lubridate, \"\\n\")\n#&gt; Number of months using lubridate: 18 \n\n%/% is used for floor division by months. For decimals, just use /\n\nData\n(house_df &lt;- tibble(\n  person_id  = factor(c(\"A10232\", \"A10232\", \"A10232\", \"A39211\", \"A39211\", \"A28183\", \"A28183\", \"A10124\")),\n  house_id   = factor(c(\"H1200E\", \"H1243D\", \"H3432B\", \"HA7382\", \"H53621\", \"HC39EF\", \"HA3A01\", \"H222BA\")),\n  start_date = ymd(c(\"20200101\", \"20200112\", \"20211120\", \"19800101\", \"19900101\", \"20170303\", \"20190202\", \"19931023\")),\n  end_date   = ymd(c(\"20200112\", \"20211120\", \"20230720\", \"19891231\", \"20170102\", \"20180720\", \"20230720\", \"20230720\"))\n))\n\n#&gt;   A tibble: 8 × 4\n#&gt;   person_id house_id start_date end_date  \n#&gt;   &lt;fct&gt;     &lt;fct&gt;    &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 A10232    H1200E   2020-01-01 2020-01-12\n#&gt; 2 A10232    H1243D   2020-01-12 2021-11-20\n#&gt; 3 A10232    H3432B   2021-11-20 2023-07-20\n#&gt; 4 A39211    HA7382   1980-01-01 1989-12-31\n#&gt; 5 A39211    H53621   1990-01-01 2017-01-02\n#&gt; 6 A28183    HC39EF   2017-03-03 2018-07-20\n#&gt; 7 A28183    HA3A01   2019-02-02 2023-07-20\n#&gt; 8 A10124    H222BA   1993-10-23 2023-07-20\nCreate interval column\nhouse_df &lt;- \n  house_df |&gt; \n  mutate(\n    # create the interval\n    int = interval(start_date, end_date), \n    # drop the start/end columns\n    .keep = \"unused\"                      \n  )\n\nhouse_df\n#&gt;   A tibble: 8 × 3\n#&gt;   person_id house_id int                           \n#&gt;   &lt;fct&gt;     &lt;fct&gt;    &lt;Interval&gt;                    \n#&gt; 1 A10232    H1200E   2020-01-01 UTC--2020-01-12 UTC\n#&gt; 2 A10232    H1243D   2020-01-12 UTC--2021-11-20 UTC\n#&gt; 3 A10232    H3432B   2021-11-20 UTC--2023-07-20 UTC\n#&gt; 4 A39211    HA7382   1980-01-01 UTC--1989-12-31 UTC\n#&gt; 5 A39211    H53621   1990-01-01 UTC--2017-01-02 UTC\n#&gt; 6 A28183    HC39EF   2017-03-03 UTC--2018-07-20 UTC\n#&gt; 7 A28183    HA3A01   2019-02-02 UTC--2023-07-20 UTC\n#&gt; 8 A10124    H222BA   1993-10-23 UTC--2023-07-20 UTC\nIntersection Function\n\nint_intersect &lt;- function(int, int_limits) {\n  int_start(int) &lt;- pmax(int_start(int), int_start(int_limits))\n  int_end(int)   &lt;- pmin(int_end(int), int_end(int_limits))\n  return(int)\n}\n\nThe red dashed line is the reference interval and the blue solid line is the interval of interest\nThe function creates an interval thats the intersection of both intervals (segment between black parentheses)\n\nProportion of the Reference Interval\n\nint_proportion &lt;- function(dat, reference_interval) {\n\n  # start with the housing data\n  dat |&gt; \n    # only retain overlapping rows, this makes the following\n    # operations more efficient by only computing what we need\n    filter(int_overlaps(int, reference_interval)) |&gt; \n    # then, actually compute the overlap of the intervals\n    mutate(\n      # use our earlier truncate function\n      int_sect = int_intersect(int, reference_interval),\n      # then, it's simple to compute the overlap proportion\n      prop = int_length(int_sect) / int_length(reference_interval)\n    ) |&gt; \n    # combine different intervals per person\n    summarize(prop_in_nl = sum(prop), .by = person_id)\n\n}\n\nExample\nint_2017  &lt;- interval(ymd(\"20170101\"), ymd(\"20171231\"))\nprop_2017 &lt;- \n  int_proportion(dat = house_df, \n                 reference_interval = int_2017)\n\nprop_2017\n\n#&gt; # A tibble: 3 × 2\n#&gt;   person_id prop_in_nl\n#&gt;   &lt;fct&gt;          &lt;dbl&gt;\n#&gt; 1 A39211       0.00275\n#&gt; 2 A28183       0.832  \n#&gt; 3 A10124       1",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html",
    "href": "qmd/feature-engineering-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-misc",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-misc",
    "title": "General",
    "section": "",
    "text": "Tree-based Models\n\nFrom Uber, “Tree-based models are performing piecewise linear functional approximation, which is not good at capturing complex, non-linear interaction effects.”\n\nWith regression models, you have to be careful about encoding categoricals as ordinal (i.e. integers) which means one-hot encoding is better.\n\nFor example, the raw numerical encoding (0-24) of the “hour” feature prevents the linear model from recognizing that an increase of hour in the morning from 6 to 8 should have a strong positive impact on the number of bike rentals while a increase of similar magnitude in the evening from 18 to 20 should have a strong negative impact on the predicted number of bike rentals.\n\nModels with large numbers (100s) of features increases the opportunity for feature drift\nZero-Inflated Predictors/Features\n\nFor ML, transformations probably not necessary\nFor regression\n\nlog(x + 0.05)\n\nlarger effect on skew than sqrt\n\narcsinh(x) (see Continuous &gt;&gt; Transformations &gt;&gt; Logging)\n\napproximates a log but handles 0s\n\nsqrt maybe Yeo-Johnson (?)\n\n&gt;60% of values = 0, consider binning or binary",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-cont",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-cont",
    "title": "General",
    "section": "Continuous",
    "text": "Continuous\n\nBinning\n\nBenefits\n\nReduces Noise\n\nContinuous variables tend to store information with minute fluctuations that provide no added value for the machine learning task of interest\n\nMakes the feature more intuitive\n\nIs there an important threshold value?\n\n1 value –&gt; split into a binary\nmultiple values –&gt; multinomial\n\n\nMinimizes outlier influence\n\nBin and Embed\n\nSteps\n\nFind bin ranges\n\nIf sufficient data, calculate quantiles of the numeric vector to find the bin ranges\nsklearn.preprocessing.KBinsDiscretizer has a few different methods\nUse some other method to find the number/ranges of bins (see R packages)\n\nUse the indices of the bins (i.e. leftmost bin is 1, 2nd leftmost bin is 2) to discretize each value of the numeric\n\nMight need to be one-hot coded\n\nCreate an embedding of the discretized vector and use the embedding as features.\n\n\nDichotomizing is bad (post, list of papers)\n\nTypical arguments for splitting (even when there’s no underlying reason to do so) include: simplifies the statistical analysis and leads to easy interpretation and presentation of results\n\nExample: splitting at the median—leads to a comparison of groups of individuals with high or low values of the measurement, leading in the simplest case to a t test or χ2 test and an estimate of the difference between the groups (with its confidence interval) on another variable.\n\nUsing multiple categories (to create an “ordinal” variable) is generally preferable , and using four or five groups the loss of information can be quite small\nIssues:\n\nInformation is lost, so the statistical power to detect a relation between the variable and patient outcome is reduced.\n\nDichotomising a variable at the median reduces power by the same amount as would discarding a third of the data\n\nMay increase the risk of a positive result being a false positive\nMay seriously underestimate the extent of variation in outcome between groups, such as the risk of some event, and considerable variability may be subsumed within each group.\nIndividuals close to but on opposite sides of the cutpoint are characterised as being very different rather than very similar.\nConceals any non-linearity in the relation between the variable and outcome\nUsing a stat like median for a cutpoint means studies will have different cutpoints, therefore results cannot easily be compared, seriously hampering meta-analysis of observational studies\nAn “optimal” cutpoint (usually that giving the minimum P value) runs a high risk of a spuriously significant result. Effect will be overestimated and the CI too narrow\nAdjusting for the effect of a confounding variable, dichotomisation will run the risk that a substantial part of the confounding remains\n\n\nHarrell\n\nThink most of these issues are related to inference models like types of logistic regression\nA better approach that maximizes power and that only assumes a smooth relationship is to use a restricted cubic spline (regression spline; piecewise cubic polynomial) function for predictors that are not known to predict linearly. Use of flexible parametric approaches such as this allows standard inference techniques (P -values, confidence limits) to be used (See Feature Engineering, Splines)\nIssues with binning continuous variables\n\nIf cutpoints are chosen by trial and error in a way that utilizes the response, even informally, ordinary P -values will be too small and confidence intervals will not have the claimed coverage probabilities.\n\nThe correct Monte-Carlo simulations must take into account both multiple tests and uncertainty in the choice of cutpoints.\n\nUsing the “minimum p-value approach” often results in multiple cutpoints so ¯\\_(ツ)_/¯ plus multiple testing p-value adjustments need to be used.\n\nThis approach involves testing multiple cutpoints and choosing one that minimizes the p-value below a threshold.\n\nOptimal cutpoints often change from sample to sample\nThe optimal cutpoint for a predictor would necessarily be a function of the continuous values of all the other predictors\nYou’re losing variation (information) which causes a loss of power and precision\nAssumes that the relationship between the predictor and the response is flat within each interval\n\nthis assumption is far less reasonable than a linearity assumption in most cases\n\nPercentiles\n\nUsually estimated from the data at hand, are estimated with sampling error, and do not relate to percentiles of the same variable in a population\nValue of binned variable potentially takes on a different relationship with the outcome\n\ne.g. Body Mass Index has a smooth relationship with every outcome studied, and relates to outcome according to anatomy and physiology. Binning may change that relationship to being how many subjects have a similar BMI.\n\n\nMany bins usually required to make it worth it. Therefore, many dummy variables will end up being created resulting in a loss of power and precision. (i.e. more bins = more variables = more dof used)\nPoor predictive performance with Cox regression models\n\n\nThey might help with prediction using ML or DL models though\n\n“Instead of directly using marketplace health as a continuous feature, we decided to use a form of target-encoding by splitting up the metric into buckets and taking the average historical delivery duration within that bucket as the new feature. With this approach, we directly helped the model learn that very supply-constrained market conditions are correlated with very high delivery times — rather than relying on the model to learn those patterns from the relatively sparse data available.”\n\nImproving ETA Prediction Accuracy for Long Tail Events\nHelps to “represent features in a way that makes it easy for the model to learn sparse patterns.”\n\nThis article was about modeling tail events, so maybe this is most useful for features that have an association with the tail values in the outcome variable\n\n\nXGBoost seems to like numerics much more than dummies\n\nTrees may prefer larger cardinalities. So if you do bin, you’d probably want quite a few bins\nNever really seen a binned age variable do well, so guessing more than 10 at least. Though maybe Age just wasn’t important enough.\n\n\nExamples\n\nBinary\n\nWhether a user spent more than $50 or didn’t\nIf user had activity on the weekend or not\n\nMultinomial or Discrete\n\nTimestamp to morning/afternoon/ night,\nOrder values into buckets of $10–20, $20–30, $30+\nHeight, age\n\nExample: step_discretize\ndata(ames, package = \"modeldata\")\n\nrecipe(~ Lot_Frontage + Lot_Area, data = ames) |&gt;\n  step_discretize(all_numeric_predictors(), num_breaks = 5) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 × 2\n#&gt;    Lot_Frontage Lot_Area\n#&gt;    &lt;fct&gt;        &lt;fct&gt;   \n#&gt;  1 bin5         bin5    \n#&gt;  2 bin4         bin4    \n#&gt;  3 bin5         bin5    \n#&gt;  4 bin5         bin4    \n#&gt;  5 bin4         bin5    \n#&gt;  6 bin4         bin3    \n#&gt;  7 bin2         bin1    \n#&gt;  8 bin2         bin1    \n#&gt;  9 bin2         bin1    \n#&gt; 10 bin2         bin2    \n#&gt; # ℹ 2,920 more rows\n\n\n\n\nTransformations\n\nMisc\n\nAlso see:\n\nRegression, Linear &gt;&gt; Transformations\nFeature Engineering, Splines\n\nCentering\n\nNo matter how a variable is centered (e.g. around the mean, median, or other number), its linear regression coefficient will not change - only the intercept will change.\n\nGuide for choosing a scaling method for classification modeling\n\nNotes from The Mystery of Feature Scaling is Finally Solved (narrator: it wasn’t)\n\nOnly used a SVM model for experimentation so who knows if this carries over to other classifiers\n\ntldr\n\nGot time and compute resources? –&gt; Ensemble different standardization methods using averaging\nNo time and limited compute resources –&gt; standardization\n\nModels that are distribution independent or distance sensitive (e.g. SVM, kNN, ANNs) should use standardization\n\nModels that are distribution dependent (e.g. regularized linear regression, regularized logistic regression, or linear discriminant analysis) weren’t tested\n\nNo evidence that data-centric rules (e.g. normal or non-normal distributed variables, outliers present)\nFeature scaling that is aligned with the data or model can be responsible for overfitting\nEnsembling by averaging (instead of using a model to ensemble) different standarization methods\n\nExperiment used robust scaler (see below) and z-score standardization\n\nWhen they added a 3rd method it created more biased results\n\nRequires predictions to be probabilities\n\nFor ML models, this takes longer because an extra CV has to be run\n\n\n\n\n\n\nStandardization\n\nThe standard method transforms feature to have mean = 0, and standard deviation = 1\n\nNot robust to outliers\n\nFeature will be skewed\n\n\nUsing the median to center and the MAD to scale makes the transformation robust to outliers\nScaling by 2 sd/MAD instead of 1 sd/MAD can be useful to obtain model coefficients of continuous parameters comparable to coefficients related to binary predictors, when applied to the predictors (not the outcome)\nNotes from\n\nWhen conducting multiple regression, when should you center your predictor variables & when should you standardize them?\n\nReasons to standardize\n\nMost ML/DL models require it\n\nMany elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n\nMost Clustering methods require it\nPCA can only be interpreted as the singular value decomposition of a data matrix when the columns have centered\nInterpreting the intercept as the mean of the outcome when all predictors are held at their means\nPredictors with large values (country populations) can have really small regression coefficients. Standardization makes the coefficients have a more managable scale.\nSome types of models are more numerically stable with the predictors have been standardized\nEasier to set priors in Bayesian modeling\nCentering fixes collinearity issues when creating powers and interaction terms\n\nCollinearity between the created terms and the main effects\n\n\nOther Reasons why you might want to:\n\nCreating a composite score\n\nWhen you’re trying to sum or average variables that are on different scales, perhaps to create a composite score of some kind. Without scaling, it may be the case that one variable has a larger impact on the sum due purely to its scale, which may be undesirable.\nOther Examples:\n\nResearch into children’s behavioral disorders - researchers might get ratings from both parents & teachers, & then want to combine them into a single measure of maladjustment.\nStudy on the activity level at a nursing home w/ self-ratings by residents & the number of signatures on sign-up sheets for activities\n\n\nTo simplify calculations and notation.\n\nA sample covariance matrix of values that has been centered by their sample means is simply X′X (correlation matrix)\nIf a univariate random variable, X, has been mean centered, then var(X)=E(X2) and the variance can be estimated from a sample by looking at the sample mean of the squares of the observed values.\n\n\nReasons NOT to standardize\n\nWe don’t want to standardize when the value of 0 is meaningful.\n\nstep_best_normalize\n\nRequires {bestNormalize} and has bestNormalize for use outside of {tidymodels}\nChooses the best standardization method using repeated cross-validation to estimate the Pearson’s P statistic divided by its degrees of freedom (from {nortest}) which indicates closness to the Gaussian distribution.\nPackage features the method, Ordered Quantile normalization (orderNorm, or ORQ). ORQ transforms the data based off of a rank mapping to the normal distribution.\nAlso includes: Lambert W\\(\\times\\)F, Box Cox, Yeo-Johnson, arcsinh, exponential, log, square root, and has a method to add your own.\nExample\nlibrary(bestNormalize)\n\ndata(ames, package = \"modeldata\")\n\nrecipe(Sale_Price ~ Lot_Frontage + Lot_Area, data = ames) |&gt;\n  step_best_normalize(all_numeric_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 × 3\n#&gt;    Lot_Frontage Lot_Area Sale_Price\n#&gt;           &lt;dbl&gt;    &lt;dbl&gt;      &lt;int&gt;\n#&gt;  1        2.48     2.29      215000\n#&gt;  2        0.789    0.689     105000\n#&gt;  3        0.883    1.28      172000\n#&gt;  4        1.33     0.574     244000\n#&gt;  5        0.468    1.19      189900\n#&gt;  6        0.656    0.201     195500\n#&gt;  7       -0.702   -1.27      213500\n#&gt;  8       -0.669   -1.24      191500\n#&gt;  9       -0.735   -1.19      236500\n#&gt; 10       -0.170   -0.654     189000\n#&gt; # ℹ 2,920 more rows\n\nscale(var or matrix)\n\nDefault args: center = T, scale = T\nStandardizes each column of a matrix separately\nFYI scale(var) == scale(scale(var))\n\n{datawizard::standardize} - Can center by median and scale by MAD (robust), can scale by 2sd (Gelman)\n{{sklearn::RobustScaler}}\n\nStandardize by median and IQR instead of mean and sd\n\n(value − median) / IQR\n\nThe resulting variable has a zero mean and median and a standard deviation of 1, although not skewed by outliers and the outliers are still present with the same relative relationships to other values.\nstep_normalize has means, sd args, so it might be able to do this\n\nHarrell recommends substituting the gini mean difference for the standard deviation\n\nGini’s mean difference - the mean absolute difference between any two distinct elements of a vector.\n\n\nHmisc::GiniMd(x, na.rm = F) (doc)\nsjstats::gmd(x or df, ...) (doc)\n\nIf “df” then it will compute gmd for all vectors in the df\n“…” allows for use of tidy selectors\n\nManual\ngmd &lt;- function(x) {\n  n &lt;- length(x)\n  sum(outer(x, x, function(a, b) abs(a - b))) / n / (n - 1)\n  }\n\n\n\n\n\nRescaling/Normalization\n\nMisc\n\nIf the values of the feature get rescaled between 0 and 1, i.e. [0,1], then it’s called normalization\nExcept in min/max, all values of the scaling variable should be &gt; 0 since you can’t divide by 0\n{datawizard::rescale} - Scales variable to a specified range\n\nMin/Max\n\nRange: [0, 1]\n\n\nMake sure the min max value are NOT outliers. If they are outliers, then the range of your data will be more constricted that it needs to be.\n\ne.g. if values are in between 100 and 500 with an exceptional value of 25000, then 25000 is scaled as 1 and all the other values become very close to the lower bound of zero\n\nExample: Age is the predictor and Happiness is the outcome. Imagine a very strong relationship between age and happiness, such that happiness is at its maximum at age 18 and its minimum at age 65. It’ll be easier if we rescale age so that the range from 18 to 65 is one unit. Now this new variable A ranges from 0 to 1, where 0 is age 18 and 1 is age 65. (from Statistical Rethinking section 6.3.1 pg 182)\nd2 &lt;- d[ d$age&gt;17 , ] # only adults\nd2$A &lt;- ( d2$age - 18 ) / ( 65 - 18 )\n\nRange: [a, b]\n\nAlso see notebook for code to transform more than 1 variable at a time.\n\nBy max\nscaled_var = var/max(var)\n\nExample: From Statistical Rethinking, pg 246\n\n“… zero ruggedness is meaningful. So instead terrain ruggedness is divided by the maximum value observed. This means it ends up scaled from totally flat (zero) to the maximum in the sample at 1 (Lesotho, a very rugged and beautiful place).”\n\nExample: From Statistical Rethinking, pg 258\n\n“I’ve scaled blooms by its maximum observed value, for three reasons. First, the large values on the raw scale will make optimization difficult. Second, it will be easier to assign a reasonable prior this way. Third, we don’t want to standardize blooms, because zero is a meaningful boundary we want to preserve.”\n\nblooms is bloom size. So there can’t be a negative but zero makes sense.\nblooms is 2 magnitudes larger than both its predictors.\n\n\n\nBy mean\nscaled_var = var/mean(var)\n\nExample: From Statistical Rethinking, pg 246\n\n“log GDP is divided by the average value. So it is rescaled as a proportion of the international average. 1 means average, 0.8 means 80% of the average, and 1.1 means 10% more than average.”\n\n\n\n\n\nLogging\n\nUseful for skewed variables\nIf you have zeros, then its common to add 1 to the predictor values\n\nTo backtransform: exp(logged_predictor) - 1\narcsinh(x): approximates a log (at large values of x) but handles 0s:\n\nBacktransform: log(x + sqrt(1+x^2))\n\n* Don’t use these for outcome variables (See Regression, Other &gt;&gt; Zero-Inflated/Truncated &gt;&gt; Continuous for methods, Thread for discussion and link to a paper on the alternatives)\n\nThe scale of the outcome matters. The thread links to a discussion of a paper on log transforms.\nProposals in the paper are in Section 4.1. One of the recommendations is log(E[Y(0)] + Y) where (I think) E[Y(0)] is the average value of Y when Treatment = 0 but I’m not sure. Need to read the paper.\n\n\nRemember to back-transform predictions if you transformed the target variable\n# log 10 transformed target variable\npreds_intervals &lt;- predict(\n  workflows::pull_workflow_fit(lm_wf),\n  workflows::pull_workflow_prepped_recipe(lm_wf) %&gt;% bake(ames_holdout),\n  type = \"pred_int\",\n  level = 0.90\n) %&gt;% \n  mutate(across(contains(\".pred\"), ~10^.x))\nCombos\n\nLog + scale by mean\n\nExample From Statistical Rethinking Ch8 pg 246\n\n“Raw magnitudes of GDP aren’t meaningful to humans. Since wealth generates wealth, it tends to be exponentially related to anything that increases it (earlier in chapter). This is like saying that the absolute distances in wealth grow increasingly large, as nations become wealthier. So when we work with logarithms instead, we can work on a more evenly spaced scale of magnitudes”\n“Log GDP is divided by the average value. So it is rescaled as a proportion of the international average. 1 means average, 0.8 means 80% of the average, and 1.1 means 10% more than average.”",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-disc",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-disc",
    "title": "General",
    "section": "Discrete",
    "text": "Discrete\n\nQuantitative variables that are countable with no in-between the values. (e.g. integer value variables)\n\ne.g. Age, Height (depending on your scale), Year of Birth, Counts of things\n\nMany variables can be either discrete or continuous depending on whether they are “exact” or have been rounded (i.e. their scale).\n\nTime since event, distance from location\nA zip code would not be a discrete variable since it is not quantitative (i.e. don’t represent amounts of anything). The values just represent geographical locations and could just as easily be names instead of numbers. There is no inherent meaning to arithmetic operations performed on them (e.g. zip_code1 - 5 has no obvious meaning)\n\nBinning\n\nSee Binning\n\nRange to Average\n\nSo numerical range variables like Age can have greater predictive power in ML/DL algorithms by just using the average value of the range\ne.g. Age == 21 to 30 –&gt; (21+30)/2 = 25.5\n\nRates/Ratios\n\nSee Domain Specific\n\nMin/Max Rescaling\n\nSee Continuous &gt;&gt; Transformations &gt;&gt; Rescaling/Normalization",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feats-general-cats",
    "href": "qmd/feature-engineering-general.html#sec-feats-general-cats",
    "title": "General",
    "section": "Categoricals",
    "text": "Categoricals\n\nMisc\n\nSee Feature Engineering, Embeddings &gt;&gt; Engineering\nOne-Hot Encode Issues:\n\nWith high cardinality, the feature space explodes –&gt;\n\nLess power\nLikely to encounter memory problems\n\nUsing a sparse matrix is memory efficient which might make the one-hot encode feasible\n\nSparse data sets don’t work well with highly efficient tree-based algorithms like Random Forest or Gradient Boosting.\n\nModel can’t determine similarity between categories (embedding does)\nEvery kind of encoding and embedding outperforms it by a lot, especially in tree models\n\n\n\n\nCombine/Lump/Collapse\n\nCollapse categories with similar characteristics to reduce dimensionality\n\nstates to regions (midwest, northwest, etc.)\n\nLump\n\nCat vars with levels with too few counts –&gt; lump together into an “Other” category\nstep_other(cat_var, threshold = 0.01) # see\n\nFor details see Model Building, tidymodels &gt;&gt; Recipe\nLevels with too few data will have large uncertainties about the effect and the bloated std.devs can cause some models to throw errors\n\n\nCombine\n\nThe feature reduction can help when data size is a concern\n\nThink this is equivalent to a cat-cat interaction.  ML models usually algorithmically create interactions but I guess this way you get the interaction but with fewer features.\nAlso might be useful to use the same considerations that you use to choose interactions to choose which cat variables to combine.\n\nSteps\n\nCombine var1 and var2 (e.g. “dog”, “minnesota”) to create a new feature called var3 (“dog_minnesota”).\nRemove individual features (var1 and var2) from the dataset.\nencode (one-hot, dummy, etc.) var 3\n\n\n\n\n\nEncode/Hashing\n\nCat vars with high numbers of levels need encoded\nCan’t dummy var because it creates too many additional variables –&gt; reduces power\nNumeric: as.numeric(as.factor(char_var))\nTarget Encoding\n\n{collinear}\n\ntl;dr; I don’t see a method that stands out as theoretically better or worse than the others. The rnorm method would probably produce the most variance within the predictor.\ntarget_encoding_lab takes a df and encodes all categoricals using all or some of the methods\nRank (target_encoding_rank): Returns the rank of the group as a integer, starting with 1 as the rank of the group with the lower mean of the response variable\n\nwhite_noise argument might be able to used.\n\nMean (target_encoding_mean): Replaces each value of the categorical variable with the mean of the response across the category the given value belongs to.\n\nThe argument, white_noise, limits potential overfitting. Must be a value betwee 0 and 1. The value added depends on the magnitude of the response. If response is within 0 and 1, a white_noise of 0.25 will add to every value of the encoded variable a random number selected from a normal distribution between -0.25 and 0.25\n\nrnorm (target_encoding_rnorm): Computes the mean and standard deviation of the response for each group of the categorical variable, and uses rnorm() to generate random values from a normal distribution with these parameters.\n\nThe argument rnorm_sd_multiplier is used as a multiplier of the standard deviation to control the range of values produced by rnorm() for each group of the categorical predictor. Values smaller than 1 reduce the spread in the results, while values larger than 1 have the opposite effect.\n\nLOO (target_encoding_loo): Replaces each categorical value with the mean of the response variable across the other cases within the same group.\n\nThe argument, white_noise, limits potential overfitting.\n\n\n{{category_encoders}}\npip install category_encoders\nimport category_encoders as ce\ntarget_encoder = ce.TargetEncoder(cols=['cat_col_1', 'cat_col_2'])\ntarget_encoder.fit(X, y)\nX_transformed = target_encoder.transform(X_pre_encoded)\n\nCatboost Encoder\npip install category_encoders\nimport category_encoders as ce\ntarget_encoder = ce.CatBoostEncoder(cols=['cat_col_1', 'cat_col_2'])\ntarget_encoder.fit(X, y)\nX_transformed = target_encoder.transform(X_pre_encoded)\nBinary Encoding\n\nBenchmarks for decision trees:\n\nNumeric best (&lt; 1000 categories)\nBinary best (&gt; 1000 categories)\n\nStore N cardinalities using ceil(log(N+1)/log(2)) features\n\n\nHashing\n\nBeyond security and fast look-ups, hashing is used for similarity search.\n\ne.g. Different pictures of the same thing should have similar hashes\nSo, if these hashes are being binned, you’d want something a hashing algorithm thinks is similar to actually be similar in order for this to be most effective.\n\nzip codes, postal codes, lat + long would be good\nNot countries or counties since I’d think the hashing similarity would be related to how similar they are alphabetically or maybe phonetically\nMaybe something like latin species names since those have similar roots, etc. would work. (e.g. dogs are canis-whatever)\n\n\nCan’t be reversed to the original values\n\nAlthough since you have the original, it seems like you could see which cat levels are in a particular hash and maybe glean some latent variable\n\nCreates dummies for each cat but fewer of them.\n\nIt is likely that multiple levels of the column will map to the same hashed columns (even with small data sets). Similarly, it is likely that some columns will have all zeros.\n\nA zero-variance filter (via recipes::step_zv) is recommended for any recipe that uses hashed columns\n\n\ntextrecipes::step_dummy_hash - Dimension Reduction. Create dummy variables, but instead of giving each level its own column, you run the level through a hashing function (MurmurHash3) to determine the column.\n\nnum_terms: Tuning parameter tha controls the number of indices that the hashing function will map to. Since the hashing function can map two different tokens to the same index, will a higher value of num_terms result in a lower chance of collision.\nExample\ndata(ames, package = \"modeldata\")\n\nrecipe(Sale_Price ~ Neighborhood, data = ames) |&gt;\n  step_dummy_hash(Neighborhood, num_terms = 4) |&gt; # Low for example\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 × 5\n#&gt;    Sale_Price dummyhash_Neighborhood_1 dummyhash_Neighborhood_2\n#&gt;         &lt;int&gt;                    &lt;int&gt;                    &lt;int&gt;\n#&gt;  1     215000                        0                       -1\n#&gt;  2     105000                        0                       -1\n#&gt;  3     172000                        0                       -1\n#&gt;  4     244000                        0                       -1\n#&gt;  5     189900                        0                        0\n#&gt;  6     195500                        0                        0\n#&gt;  7     213500                        0                        0\n#&gt;  8     191500                        0                        0\n#&gt;  9     236500                        0                        0\n#&gt; 10     189000                        0                        0\n#&gt; # ℹ 2,920 more rows\n#&gt; # ℹ 2 more variables: dummyhash_Neighborhood_3 &lt;int&gt;,\n#&gt; #   dummyhash_Neighborhood_4 &lt;int&gt;\n\n\nLikelihood Encodings\n\nEstimate the effect of each of the factor levels on the outcome and these estimates are used as the new encoding. The estimates are estimated by a generalized linear model. This step can be executed without pooling (via glm) or with partial pooling (stan_glm or lmer). Currently implemented for numeric and two-class outcomes.\n{embed}\n\nstep_lencode_glm, step_lencode_bayes , and step_lencode_mixed\n\n\n\n\n\nOrdinal\n\nMisc\n\nIf there are NAs or Unknowns, etc.,\n\nAfter coercing into a numeric/integer, you can convert Unknowns to NA and then impute the variable\n\nAll these encodings will produce the same results for a tree model, since tree-based models rely on variable ranks rather than exact values.\n0 = “0 Children”\n1 = “1 Child”\n2 = “2 Children”\n3 = “3 Children”\n4 = “4 or more Children”\n\n1 = “0 Children”\n2 = “1 Child”\n3 = “2 Children”\n4 = “3 Children”\n5 = “4 or more Children”\n\n-100 = “0 Children”\n-85  = “1 Child”\n0    = “2 Children”\n10  = “3 Children”\n44  = “4 or more Children”\n\nVia {tidymodels}\nstep_mutate(ordinal_factor_var = as_integer(ordinal_factor_var))\n# think this uses as_numeric\nstep_ordinalscore(ordinal_factor_var)\nPolynomial Contrasts\n\nSee the section Kuhn’s book\n\nRainbow Method (article)\n\nMIsc\n\nCreates an artifical ordinal variable from a nominal variable (i.e. ordering colors according the rainbow, roy.g.biv)\nAt worst, it maintains the signal of a one-hot encode, but with tree models, it results in less splits and therefore a simpler, more efficient, and less overfit model.\nTest psuedo ordinal method by constructing a simple bayesian model with response ~ 0 + ordinal. Then, you extract the posterior for each constructed ordinal level. Pass these posteriors through a constraint that labels draws for that level that are less (or not) than the draws of the previous level. Lastly calculate the proportion of those that were less than in order to get a probability that the predictor is ordered (article &gt;&gt; “The Model” section)\n\nCode\ngrid &lt;- data.frame(\n  Layer = c(\"B\", \"C\", \"E\", \"G\", \"I\"),\n  error = 0\n)\n\ngrid_with_mu &lt;- tidybayes::add_linpred_rvars(grid, simple_mod, value = \".mu\")\n\nis_stratified &lt;- with(grid_with_mu, {\n  .mu[Layer == \"B\"] &gt; .mu[Layer == \"C\"] &\n  .mu[Layer == \"C\"] &gt; .mu[Layer == \"E\"] &\n  .mu[Layer == \"E\"] &gt; .mu[Layer == \"G\"] &\n  .mu[Layer == \"G\"] &gt; .mu[Layer == \"I\"]\n})\n\nPr(is_stratified)\n#&gt; [1] 0.78725\n“Layer” is the ordinal variable being tested\nadd_linpred_rvars extracts the mean response posteriors for each level of the variable\nResults strongly suggest that the levels of the variable (“Layer”) are ordered, with a 0.79 posterior probability.\n\n\nMethods:\n\nDomain Knowledge\nVariable Attribute (see examples)\nOthers - Best to compute these on a hold out set, so as not cause data leakage\n\nAssociation with the target variable where the value of association is used to rank the categories\nProportion of the event for a binary target variable where the value of the proportion is used to rank the categories\n\n\nIf it’s possible, use domain knowledge according the project’s context to help choose the ranking of the categories.\nThere are always multiple ways to rank the categories, so it may be worthwhile to try multiple versions of the artificial ordinal variable\n\nNot recommended to use more than log₂(K) versions, so as to not surpass the number of variables creating using One-hot (where k is the number of categories)\n\nExample: Vehicle Type\n\nCategories\n\nC: “Compact Car”\nF: “Full-size Car”\nL: “Luxury Car”\nM: “Mid-Size Car”\nP: “Pickup Truck”\nS: “Sports Car”\nU: “SUV”\nV: “Van”\n\nPotential attributes to order by: vehicle size, capacity, price category, average speed, fuel economy, costs of ownership, motor features, etc.\n\nExample: Occupation\n\nCategories\n\n1: “Professional/Technical”\n2: “Administration/Managerial”\n3: “Sales/Service”\n4: “Clerical/White Collar”\n5: “Craftsman/Blue Collar”\n6: “Student”\n7: “Homemaker”\n8: “Retired”\n9: “Farmer”\nA: “Military”\nB: “Religious”\nC: “Self Employed”\nD: “Other”\n\nPotential attributes to order by: average annual salary, by their prevalence in the geographic area of interest, or variables in a Census dataset or some other data source\n\n\n\n\n\nWeight of Evidence\n\nembed::step_woe",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-inter",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-inter",
    "title": "General",
    "section": "Interactions",
    "text": "Interactions\n\nManually\n\nNumeric ⨯ Cat\n\nDummy the cat, then multiply the numeric times each of the dummies.",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-date",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-date",
    "title": "General",
    "section": "Date",
    "text": "Date\n\nDuration\n\nDays since last purchase per customer\n\nExample: (max(invoice_date) - max_date_overall) / lubridate::ddays(1)\n\nThink ddays converts this value to a numeric\n\n\nCustomer Tenure\n\nExample: (min(invoice_date) - max_date_overall) / lubridate::ddays(1)",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-dom",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-dom",
    "title": "General",
    "section": "Domain Specific",
    "text": "Domain Specific\n\nRates/Ratios\n\nPurchase per Customer\n\nTotal Spent\n\nExample: sum(total_per_invoice, na.rm = TRUE)\n\nAverage Spent\n\nExample: mean(total_per_invoice, na.rm = TRUE)\n\n\nLet the effect of Cost vary by the person’s income\n\nmutate(cost_income = cost_of_product/persons_income)\nIntuition being that the more money you have the less effect cost will have on whether purchase something.\nDividing the feature by income is equivalent to dividing the \\(\\beta\\) by income.\n\n\nPre-Treatment Baseline\n\nExample: From Modeling Treatment Effects and Nonlinearities in A/B Tests with GAMS\n\noutcome = log(profit), treatment = exposure to internation markets, group = store\nBaseline variable is log(profit) before experiment is conducted\n\nShould center this variable",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/json.html",
    "href": "qmd/json.html",
    "title": "JSON",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-misc",
    "href": "qmd/json.html#sec-json-misc",
    "title": "JSON",
    "section": "",
    "text": "Packages\n\n{yyjsonr} - A fast JSON parser/serializer, which converts R data to/from JSON and NDJSON. It is around 2x to 10x faster than jsonlite at both reading and writing JSON.\n{RcppSimdJson} - Comparable to {yyjsonr} in performance.\n\nAlso see\n\nBig Data &gt;&gt; Larger than Memory\nSQL &gt;&gt; Processing Expressions &gt;&gt; Nested Data\nDatabases &gt;&gt; DuckDB &gt;&gt; Misc\n\nhrbmstr recommends trying duckdb before using the cli tools in “Big Data”",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-jsonlite",
    "href": "qmd/json.html#sec-json-jsonlite",
    "title": "JSON",
    "section": "{jsonlite}",
    "text": "{jsonlite}\n\nRead",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-py",
    "href": "qmd/json.html#sec-json-py",
    "title": "JSON",
    "section": "Python",
    "text": "Python\n\nExample: Parse Nested JSON into a dataframe (article)\n\nRaw JSON\n\n\n“entry” has the data we want\n“…” at the end indicates there are multiple objectss inside the element, “entry”\n\nProbably other root elements other than “feed” as well\n\n\nRead a json file from a URL using {{requests}} and convert to list\n\nimport requests\n\nurl = \"https://itunes.apple.com/gb/rss/customerreviews/id=1500780518/sortBy=mostRecent/json\"\n\nr = requests.get(url)\n\ndata = r.json()\nentries = data[\"feed\"][\"entry\"]\n\nIt looks like the list conversion also ordered the elements alphabetically\nThe output list is subsetted by the root element “feed” and the child element “entry”\n\nGet a feel for the final structure you want by hardcoding elements into a df\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    parsed_data[\"author_uri\"].append(entry[\"author\"][\"uri\"][\"label\"])\n    parsed_data[\"author_name\"].append(entry[\"author\"][\"name\"][\"label\"])\n    parsed_data[\"author_label\"].append(entry[\"author\"][\"label\"])\n    parsed_data[\"content_label\"].append(entry[\"content\"][\"label\"])\n    parsed_data[\"content_attributes_type\"].append(entry[\"content\"][\"attributes\"][\"type\"])\n    ... \nGeneralize extracting the properties of each object in “entry” with a nested loop\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    for key, val in entry.items():\n        for subkey, subval in val.items():\n            if not isinstance(subval, dict):\n                parsed_data[f\"{key}_{subkey}\"].append(subval)\n            else:\n                for att_key, att_val in subval.items():\n                    parsed_data[f\"{key}_{subkey}_{att_key}\"].append(att_val)\n\ndefaultdict creates a key from a list element (e.g. “author”) and groups the properties into a list of values where the value may also be a dict.\n\nSee Python, General &gt;&gt; Types &gt;&gt; Dictionaries\n\nFor each item in “entry”, it looks at the first key-value pair knowing that value is always a dictionary (object in JSON)\nThen handles two different cases\n\nFirst Case: The value dictionary is flat and does not contain another dictionary, only key-value pairs.\n\nCombine the outer key with the inner key to a column name and take the value as column value for each pair.\n\nSecond Case: Dictionary contains a key-value pair where the value is again a dictionary.\n\nAssumes at most two levels of nested dictionaries\nIterates over the key-value pairs of the inner dictionary and again combines the outer key and the most inner key to a column name and take the inner value as column value.\n\n\n\nRecursive function that handles json elements with deeper structures\n\ndef recursive_parser(entry: dict, data_dict: dict, col_name: str = \"\") -&gt; dict:\n    \"\"\"Recursive parser for a list of nested JSON objects\n\n    Args:\n        entry (dict): A dictionary representing a single entry (row) of the final data frame.\n        data_dict (dict): Accumulator holding the current parsed data.\n        col_name (str): Accumulator holding the current column name. Defaults to empty string.\n    \"\"\"\n    for key, val in entry.items():\n        extended_col_name = f\"{col_name}_{key}\" if col_name else key\n        if isinstance(val, dict):\n            recursive_parser(entry[key], data_dict, extended_col_name)\n        else:\n            data_dict[extended_col_name].append(val)\n\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    recursive_parser(entry, parsed_data, \"\")\n\ndf = pd.DataFrame(parsed_data)\n\nNotice the check for a deeper structure with isinstance. If there is one, then the function is called again.\nFunction outputs a dict which is coerced into dataframe\nTo get rid of “label” in column names: df.columns = [col if not \"label\" in col else \"_\".join(col.split(\"_\")[:-1]) for col in df.columns]\nobject types can be cast into more efficient types: df[\"im:rating\"] = df[\"im:rating\"].astype(int)",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/package-development.html",
    "href": "qmd/package-development.html",
    "title": "Package Development",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Package Development"
    ]
  },
  {
    "objectID": "qmd/package-development.html#sec-pkgdev-misc",
    "href": "qmd/package-development.html#sec-pkgdev-misc",
    "title": "Package Development",
    "section": "",
    "text": "Packages\n\n{goodpractices} - Give advice about good practices when building R packages. Advice includes functions and syntax to avoid, package structure, code complexity, code formatting, etc\n\nVersion Number Syntax\n\n0.0.0.9000 = the “.9000” means intital experimental or in development version (.9001 would be the second experimental version)\n0.1.0 = the “1” means this is your first “minor” version\n1.0.0 = the “1” means this is your first “major” version\n\nIn function docs, always use @examples and NOT @example\nIf you have internal functions (e.g. little helper functions) that you don’t want your users to have the regular access to then:\n\nDon’t include @export for that function’s script\nDocumentation of the function isn’t required\n\nAdd a file from another repo using usethis::use_standalone\n\nIf another package has a set of functions that you’d like to include in your package but don’t want to import their entire package, this can be used.\nIt always overwrites an existing standalone file of the same name, making it easy to update previously imported code.\n\nUsing “Starting a Package” (below) takes some time when you want to test newly written functions.\n\nAlternative: load_all() - Quickly makes function available for interactive testing, (ctrl + shift + L)\n\nChange package name (if not on CRAN): changer::changer(\"current_pkg_name\", \"new_pkg_name\")\n\nShouldn’t have the package project open when running this. I set the working dir to the root of my “Projects” directory locally (e.g Projects &gt;&gt; current_proj_directory, then set working directory to Projects)\nThis changes EVERYTHING automagically, but be sure and make a copy of the directory before running just in case.\nNeed to change repo name in the github repo’s settings before pushing changes\n\nCreate Citation file for package: usethis::use_citation()\n\nOnly execute after you’ve gone through putting your package on zenodo and have a generated citation to populate the fields of the CITATION file.\nSee indianacovid19data project for example\n\nIf planning to release on CRAN, use usethis::use_release_issue, which creates a checklist of things to fix/do before submitting to CRAN\n\nCRAN requires your pkg run without issues on all platforms. {covr} tracks test coverage for your R package and view reports locally or (optionally) upload the results to codecov or coveralls\n\nDeveloping Internal Packages\n\nDeveloping packages for your company’s specific use cases increases efficiency\n\nExamples\n\nCollection\n\nPulling data from public sources\n\nDatabase\n\nConnections and Querying\nAPI requests\nETL processes\n\nReport building\n\nData manipulation\nVisualization\n\n\n\nSee Building a team of internal R packages | Emily Riederer and VIDEO How to make internal R packages part of your team - RStudio\n\nAlso {RDepot} for management",
    "crumbs": [
      "Package Development"
    ]
  },
  {
    "objectID": "qmd/package-development.html#starting-a-package",
    "href": "qmd/package-development.html#starting-a-package",
    "title": "Package Development",
    "section": "Starting a Package",
    "text": "Starting a Package\n\nCheck package name, Create pkg, DESCRIPTION, Set-Up Git\n\nCheck if package name already taken on CRAN: available::available(\"package_name\", browse = FALSE)\n\nAsks you stuff about using “urban dictionary” to see if your package name might be offensive. Just say yes. (or it keeps asking)\nYou want it to say that your package name is available on CRAN\nShows sentiment analysis of your package name according to different dictionaries\n\nCreate directory, a project, a basic package skeleton in that directory\nsetwd(\"~/R/Projects\")\nusethis::create_package(\"package_name\")\nOpen project and fill out some of DESCRIPTION\n\nTitle, Authors, Description\n\nGo to Build pane in RStudio \\(\\rightarrow\\) more \\(\\rightarrow\\) Configure Build Tools \\(\\rightarrow\\) Make sure the “Generate documentation with Roxygen” box is ticked (tick it and click ok)\nSet-Up Git: usethis::use_git()\n\nIt will ask to commit the package skeleton to github \\(\\rightarrow\\) Choose yes to commit locally\nIt’ll ask you to restart RStudio to activate the git pane \\(\\rightarrow\\) Choose yes\nSet-up of GH Auth token, if you don’t have one\n\nThen use usethis::create_github_token() and follow steps\nRefresh session once you’ve updated .Renviron\n\nFor private repo: usethis::use_github(private = TRUE)\nFor public repo: usethis::use_github(private = FALSE, protocol = \"ssh\")\n\nChoose 1 to use ssh key\n“Are title and description okay?” \\(\\rightarrow\\) choose 3. Yes\n\n\n\n\n\nBasic Set-up\n\nUse Markdown for documentation: usethis::use_roxygen_md()\nAdd license: usethis::use_mit_license(copyright_holder = \"Eric Book\")\nAdd Readme.Rmd: usethis::use_readme_rmd()\n\nYou’ll still need to render/knit README.Rmd regularly.\nTo keep README.md up-to-date, devtools::build_readme() is handy.\nYou could also use GitHub Actions to re-render README.Rmd every time you push. An example workflow can be found here: https://github.com/r-lib/actions/tree/master/examples.\n\nAdd News/Changlog file: usethis::use_news_md()\nAdd Article/Vignette: usethis::use_vignette\nDocument, Install, and Check package\n\nBuild pane \\(\\rightarrow\\)\n\nmore \\(\\rightarrow\\) Run Document\nRun Install and Restart\nRun Check\n\n\nCommit files and Push\n\n\n\nDevelopment\n\nAdd common imported functions to DESCRIPTION\nusethis::use_tidy_eval()\nusethis::use_tibble()\nAdd data\n# read data into environment\npkg_dat &lt;- readr::read_rds(\"../path/to/data.rds\")\nusethis::use_data(pkg_dat, overwrite = TRUE)\n\nCreates data directory and adds data as an .rda file\nDocument the data: usethis::use_r(\"data\")\n\nSee indianacovid19data for examples\n\nIf you want to store binary data and make it available to the user, put it in data/. This is the best place to put example datasets. If you want to store parsed data, but NOT make it available to the user, put it in R/sysdata.rda. This is the best place to put data that your functions need.\nIf you want to store raw data, put it in inst/extdata.\n\nAdd Functions\n\nCreate R file: usethis::use_r(\"name-function-file\")\nWrite function\n\nUse pkg::function format for every external package function\n\nAdd documentation (e.g. @description, @params, @returns, @details, @references, @export, and @examples\n\nWhen pasting code into examples, use the RStudio multiple cursors feature (ctrl + alt + up/down) to add “#&gt;” to all the lines of code at once\n\nAdd packages used in function to DESCRIPTION: usethis::use_package(\"package_name\")\nMake sure hardcoded variable names are in global variables file\nMake sure any data used in @examples is added and documentedf\nIf submitting to CRAN: need examples and they should be wrapped in \\dontrun{} and \\donttest{}.\nTest\n\nBuild pane \\(\\rightarrow\\) more \\(\\rightarrow\\) Run Document (Ctrl+Shift+D) \\(\\rightarrow\\) Run Install and Restart (ctrl+shift+B) \\(\\rightarrow\\) Run Check (ctrl+shift+E)\n\nAfter writing a function or two, set-up {pkgdown}, and return here.\nAdd new function to _pkgdown.yml\nAdd new function to any additional pages of the site (e.g. vignettes, Readme.Rmd)\n\nRemember to knit\n\nRun pkgdown::build_site()\n\nKnits html pages for reference function pages\n\nCommit and Push\nRinse and Repeat for each function\n\n\n\n\npkgdown\n\nDocs\nSet-Up: usethis::use_pkgdown()\n\nCreates yaml file: _pkgdown.yml\nFill out yaml\n\nPublish website and set-up Github Actions: usethis::use_pkgdown_github_pages()\nCustomize site\n\nName css file, “extra.css”, and place in folder called, “pkgdown” (also extra.js, extra.scss)\nWhen testing out themes, css features, etc., iterating is much faster when using build_home_index(); init_site() instead of build_site(). Then, refresh html page in browser.\nSee docs for bslib variables that can be used in _pkgdown.yml (requires using bootsrap 5).",
    "crumbs": [
      "Package Development"
    ]
  },
  {
    "objectID": "qmd/package-development.html#sec-pkgdev-depend",
    "href": "qmd/package-development.html#sec-pkgdev-depend",
    "title": "Package Development",
    "section": "Dependencies",
    "text": "Dependencies\n\nImports and Depends\n\nImports just loads the package\n\nUnless there is a good reason otherwise, you should always list packages in Imports not Depends. That’s because a good package is self-contained, and minimises changes to the global environment (including the search path)\n\nDepends attaches it.\n\nLoading and Attaching\n\nLoading\n\nThe package is available in memory, but because it’s not in the search path (path that R searches for functions), you won’t be able to access its components without using ::.\n\nAttaching\n\nPuts the package in the search path. You can’t attach a package without first loading it\nBoth library() (throws error when pkg not installed )or require() (just returns false when pkg not installed) load then attach the package",
    "crumbs": [
      "Package Development"
    ]
  },
  {
    "objectID": "qmd/package-development.html#sec-pkgdev-test",
    "href": "qmd/package-development.html#sec-pkgdev-test",
    "title": "Package Development",
    "section": "Testing",
    "text": "Testing\n\nPackages\n\n{testthat} explicitly evaluates the outputs of your function but you can add a test that makes sure the checks on inputs within the function are working\n{doctext} - Generate tests from examples using {roxygen} and {testthat}\n\nRun tests (Ctrl+Shift+T): test()\n\nAlso ran when using, check()\n\nSet-up - Run usethis::use_testthat within the project directory\n\nCreates:\n\nA tests folder in your working directory\nA testthat folder in the tests folder where your tests will live\nA testthat.R file in the tests folder which organizes things for running. Don’t modify this manually.\n\n\nNames of your testing files must begin with ‘test’\n\ne.g. testing file, ‘test-my-function.R’, which lives in testthat folder\n\nWriting Tests\n\nWrite tests using the test_that function, and with each test, there’s an “expectation.”\nYou can have one or more tests in each file and one or more expectations within each test.\nExample\ntest_that(\"description of the test\", {\n        test_output1 &lt;- dat %&gt;%\n            your_package_function()\n\n        expect_equal(nrow(test_output1), 6)\n        # etc...\n})\nExample\n\n\nWhere sse is the function you’re testing\n\nExample expect_error(compute_corr(data = faithful, var1 = erruptions, var2 = waiting)\n\nerruptions isn’t a column in the dataset and should throw an error because of a check inside the function\nAlso expect_warning() available\n\nExample: Compare optional outputs of a nested function to the individual functions within that nested function\n\n\nerror is the nested function with optional outputs of sse error or mape error\nSecond chunk should say “mape calculations work”\n1st chunk checks if error with sse option output is the same as sse output\n2nd chunk checks if error with mape option output is the same as mape output\n\n\nRun Tests\n\nUse the test_file function\n\nUse the test_dir function: e.g. test_dir(wd$test_that) for running all tests.\nPress the Run Tests button in R Studio if you open the test file.\nHighlight and run the code.",
    "crumbs": [
      "Package Development"
    ]
  }
]