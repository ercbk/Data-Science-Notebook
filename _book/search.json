[
  {
    "objectID": "qmd/surveys-census-data.html",
    "href": "qmd/surveys-census-data.html",
    "title": "Census Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "title": "Census Data",
    "section": "",
    "text": "Notes from\n\nTidycensus Workshop 2024\n\nFIPS GEOID\n\npopular variable calculations from variables in ACS\nCensus Geocoder (link)\n\nEnter an address and codes for various geographies are returned\nBatch geocoding available for up to 10K records\n\nCodes for geographies returned in a .csv file\n\n\nTIGERweb (link)\n\nAllows you to get geography codes by searching for an area on a map\nOnce zoomed-in on your desired area, you turn on geography layers to find the geography code for your area.\n\nUS Census Regions",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "title": "Census Data",
    "section": "Geographies",
    "text": "Geographies\n\n\nMisc\n\n{tidycensus} docs on various geographies, function arguments, and which surveys (ACS, Census) they’re available in.\nACS Geography Boundaries by Year (link)\n\nTypes\n\nLegal/Administrative\n\nCensus gets boundaries from outside party (state, county, city, etc.)\ne.g. election areas, school districts, counties, county subdivisions\n\nStatistical\n\nCensus creates these boundaries\ne.g. regions, census tracts, ZCTAs, block groups, MSAs, urban areas\n\n\nNested Areas\n\n\nCensus Tracts\n\nAreas within a county\nAround 1200 to 8000 people\nSmall towns, rural areas, neighborhoods\n** Census tracts may cross city boundaries **\n\nBlock Groups\n\nAreas within a census tract\nAround 600 to 3000 people\n\nCensus Blocks\n\nAreas within a block group\nNot for ACS, only for the 10-yr census\n\n\nPlaces\n\nMisc\n\nOne place cannot overlap another place\nExpand and contract as population or commercial activity increases or decreases\nMust represent an organized settlement of people living in close proximity.\n\nIncorporated Places\n\ncities, towns, villages\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\nCensus Designated Places (CDPs)\n\nAreas that can’t become Incorporated Places because of state or city regulations\nConcentrations of population, housing, commericial structures\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\n\nCounty Subdivisions\n\nMinor Civil Divisions (MCDs)\n\nLegally defined by the state or county, stable entity. May have elected government\ne.g. townships, charter townships, or districts\n\nCensus County Divisions (CCDs)\n\nno population requirment\nSubcounty units with stable boundaries and recognizable names\n\n\nZip Code Tabulation Areas (ZCTAs)\n\n\nMisc\n\n{crosswalkZCTA} - Contains the US Census Bureau’s 2020 ZCTA to County Relationship File, as well as convenience functions to translate between States, Counties and ZIP Code Tabulation Areas (ZCTAs)\n\nApproximate USPS Code distribution for housing units\n\nThe most frequently occurring zip code within an census block is assigned to a census block\nThen blocks are aggregated into areas (ZCTAs)\n\nZCTAs do NOT nest within any other geographies\n\nI guess the aggregated ZCTA blocks can overlap block groups\n\n2010 ZCTAs exclude large bodies of water and unpopulated areas",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "title": "Census Data",
    "section": "American Community Survey (ACS)",
    "text": "American Community Survey (ACS)\n\nAbout\n\nYearly estimates based on samples of the population over a 5yr period\n\nTherefore a Margin of Error (MoE) is included with the estimates.\n\nDetailed social, economic, housing, and demographic characteristics\ncensus.gov/acs\n\nACS Release Schedule (releases)\n\nSeptember - 1-Year Estimates (from previous year’s collection)\n\nEstimates for areas with populations of &gt;65K\n\nOctober - 1-Year Supplemental Estimates\n\nEstimates for areas with populations between 20K-64999\n\nDecember - 5-Year Estimates\n\nEstimates for areas including census tract and block groups\n\n\nData Collected\n\nPopulation\n\nSocial\n\nAncestry, Citizenship, Citizen Voting Age  Population, Disability, Education Attainment, Fertility, Grandparents, Language, Marital Status, Migration, School Enrollment, Veterans\n\nDemographic\n\nAge, Hispanic Origin, Race, Relationship, Sex\n\nEconomic\n\nClass of worker, Commuting, Employment Status, Food Stamps (SNAP), Health Insurance, Hours/Week, Weeks/Year, Income, Industry & Occupation\n\n\nHousing\n\nComputer & Internet Use, Costs (Mortgage, Taxes, Insurance), Heating Fuel, Home Value, Occupancy, Plumbing/Kitchen Facilities, Structure, Tenure (Own/Rent), Utilities, Vehicles, Year Built/Year Movied In",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "title": "Census Data",
    "section": "Dicennial US Census",
    "text": "Dicennial US Census\n\nMisc\n\nA complete count — not based on samples like the ACS\nApplies differential privacy to preserve respondent confidentiality\n\nAdds noise to data. Greater effect at lower levels (i.e. block level)\nThe exception is that is no differetial privacy for household-level data.\n\n\n\n\nPL94-171\n\nPopulation data which the government needs for redistricting\nsumfile = “pl”\nState Populations\npop20 &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"P1_001N\",\n    year = 2020\n  )\n\nFor 2020, default is sumfile = “pl”\n\n\n\n\nDHC\n\nAge, Sex, Race, Ethnicity, and Housing Tenure (most popular dataset)\nsumfile = “dhc”\nCounty\ntx_population &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\nCensus Block (analogous to a city block)\nmatagorda_blocks &lt;- \n  get_decennial(\n    geography = \"block\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    county = \"Matagorda\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\n\n\n\nDemographic Profile\n\nPretabulated percentages from dhc\nsumfile = “dp”\n\nTabulations for 118th Congress and Island Areas (i.e. Congressional Districts)\n\nsumfile = “cd118”\n\n\nC suffix variables are counts while P suffix variables are percentages\n\n0.4 is 0.4% not 40%\n\nExample: Same-sex married and partnered in California by County\nca_samesex &lt;- \n  get_decennial(\n    geography = \"county\",\n    state = \"CA\",\n    variables = c(married = \"DP1_0116P\",\n                  partnered = \"DP1_0118P\"),\n    year = 2020,\n    sumfile = \"dp\",\n    output = \"wide\"\n  )\n\n\n\nDetailed DHC-A\n\nDetailed demographic data; Thousands of racial and ethnic groups; Tabulation by sex and age.\nDifferent groups are in different tables, so specific groups can be hard to locate.\nAdaptive design means the demographic group (i.e. variable) will only be available in certain areas. For privacy, data gets supressed when the area has low population.\n\nThere’s typically considerable sparsity especially when going down census tract\n\nArgs\n\nsumfile = “ddhca”\npop_group - Population group code (See get_pop_groups below)\n\n“all” for all groups\npop_group_label = TRUE - Adds group labels\n\n\nget_pop_groups(2020, \"ddhca\") - Gets group codes for ethnic groups\n\nFor various groups there could be at least two variables (e..g Somaili, Somali and any combination)\nFor time series analysis, analagous groups to 2020’s for 2000 is SF2/SF4 and for 2010 is SF2. (SF stands for Summary File)\n\ncheck_ddhca_groups - Checks which variables are available for a specific group\n\nExample: Somali\ncheck_ddhca_groups(\n  geography = \"county\", \n  pop_group = \"1325\", \n  state = \"MN\", \n  county = \"Hennepin\"\n)\n\nExample: Minnesota group populations\nload_variables(2020, \"ddhca\") %&gt;% \n  View()\nmn_population_groups &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"all\", # for all groups\n    pop_group_label = TRUE\n  )\n\nIncludes aggregate categories like European Alone, Other White Alone, etc., so you can’t just aggregate the value column to get the total population in Minnesota.\n\nSo, in order to calculate ethnic group ratios of the total state or county, etc. population, you need to get those state/county totals from other tables (e.g. PL94-171)\n\n\nUse dot density and not chloropleths to visualize these sparse datasets\n\nExample: Somali populations by census tract in Minneapolis\n\nhennepin_somali &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    county = \"Hennepin\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"1325\", # somali\n    pop_group_label = TRUE,\n    geometry = TRUE\n  )\n\nsomali_dots &lt;- \n  as_dot_density(\n    hennepin_somali,\n    value = \"value\", # column name which is by default, \"value\"\n    values_per_dot = 25\n  )\n\nmapview(somali_dots, \n        cex = 0.01, \n        layer.name = \"Somali population&lt;br&gt;1 dot = 25 people\",\n        col.regions = \"navy\", \n        color = \"navy\")\n\nvalues_per_dot = 25 says make each dot worth 25 units (e.g. people or housing units)\n\n\n\n\n\nTime Series Analysis\n\n{tidycensus} only has 2010 and 2020 censuses\n\nSee https://nhgis.org for older census data\n\nIssue: county names and boundaries change over time (e.g. Alaska redraws a lot)\n\nCensus gives a different GeoID to counties that get renamed even though they’re the same county.\nNA values showing up after you calculate how the value changes over time is a good indication of this problem. Check for NAs: filter(county_change, is.na(value10))\n\nExample: Join 2010 and 2020 and Calculate Percent Change\ncounty_pop_10 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P001001\", \n    year = 2010,\n    sumfile = \"sf1\"\n  )\n\ncounty_pop_10_clean &lt;- \n  county_pop_10 %&gt;%\n    select(GEOID, value10 = value) \n\ncounty_pop_20 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    year = 2020,\n    sumfile = \"dhc\"\n  ) %&gt;%\n    select(GEOID, NAME, value20 = value)\n\ncounty_joined &lt;- \n  county_pop_20 %&gt;%\n    left_join(county_pop_10_clean, by = \"GEOID\") \n\ncounty_joined\n\ncounty_change &lt;- \n  county_joined %&gt;%\n    mutate( \n      total_change = value20 - value10, \n      percent_change = 100 * (total_change / value10) \n    ) \nExample: Age distribution over time in Michigan\n\n\nCode available in the github repo or R/Workshops/tidycensus-umich-workshop-2024-main/census-2020/bonus-chart.R\nDistribution shape remains pretty much the same, but decreasing for most age cohorts, i.e. people are leaving the state across most age groups.\n\ne.g. The large hump representing the group of people in there mid-40s in 2000 steadily decreases over time.",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "title": "Census Data",
    "section": "tidycensus",
    "text": "tidycensus\n\nGet an API key\n\nRequest a key, then activate the key from the link in your email.(https://api.census.gov/data/key_signup.html)\nSet as an environment variable: census_api_key(\"&lt;api key&gt;\", install = TRUE)\n\nOr add this line to .Renviron file, CENSUS_API_KEY=‘&lt;api key’\n\n\nSearch Variables\n\nColumns\n\nName - ID of the variable (Use this in the survey functions)\nLabel - Detailed description of the variable\nContext - Subject of the table that the variable is located in.\n\nPrefixes (Variables can have combinations of prefixes)\n\nP: i.e. Person; Data available at the census block and larger\nCT: Data available at the census track and larger\nH: Data available at the Housing Unit level\n\nI think housing unit is an alternatve unit. So instead of the unit being a person, which I assume is the typical unit, it’s a housing unit (~family).\nNot affected by Differential Privacy (i.e. no noise added; true value)\nExample: Total Deleware housing units at census block level\ndp_households &lt;- \n      get_decennial(\n            geography = \"block\",\n            variables = \"H1_001N\",\n            state = \"DE\",\n            sumfile = \"dhc\",\n            year = 2020\n      )\n\n\nExample: DHC data in census for 2020\n\nvars &lt;- load_variables(2020, \"dhc\")\n\nView(vars)\n\nView table, click filter, and then search for parameters (e.g. Age, Median, etc.) with the Label, Context boxes, and overall search box\n\n\nsummary_var - Argument for supplying an additional variable that you need to calculate some kind of summary statistic\n\nExample: Race Percentage per Congressional District\n\nrace_vars &lt;- c(\n  Hispanic = \"P5_010N\", # all races identified as hispanic\n  White = \"P5_003N\", # white not hispanic\n  Black = \"P5_004N\", # black not hispanic\n  Native = \"P5_005N\", # native american not hispanic\n  Asian = \"P5_006N\", # asian not hispanic\n  HIPI = \"P5_007N\" # hawaiian, islander not hispanic\n)\n\ncd_race &lt;- \n  get_decennial(\n    geography = \"congressional district\",\n    variables = race_vars,\n    summary_var = \"P5_001N\", # total population for county\n    year = 2020,\n    sumfile = \"cd118\"\n)\n\ncd_race_percent &lt;- \n  cd_race %&gt;%\n    mutate(percent = 100 * (value / summary_value)) %&gt;% \n    select(NAME, variable, percent)\n\ngeometry = TRUE- Joins shapefile with data and returns a SF (Simple Features) dataframe for mapping\n\nMisc\n\nYou can create a discrete color palette with the at argument in the mapview function.\n\nExample\n# check min and max of your data to select range of bins\nmin(iowa_over_65, na.rm = TRUE) # 0\nmax(iowa_over_65, na.rm = TRUE) # 38.4\n\nm1 &lt;- \n  mapview(iowa_over_65, \n          zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1),\n          at = c(0, 10, 20, 30, 40))\n\nThis will result in a discrete palette with bins of 0-10, 10-20, etc. Looks like an overlap, so I’m sure which bin contains the endpoints.\n\n\n\nExample: Over 65 in Iowa by census tract\n\nlibrary(mapviw); library(viridisLite)\n\niowa_over_65 &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"DP1_0024P\",\n    state = \"IA\",\n    geometry = TRUE,\n    sumfile = \"dp\",\n    year = 2020\n  )\nm1 &lt;- \n  mapview(iowa_over_65, zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1))\nExport as an HTML file\nhtmlwidgets::saveWidget(m1@map, \"iowa_over_65.html\")\n\nCan embed it elsewhere (html report or website) by adding it as an asset",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html",
    "href": "qmd/model-building-concepts.html",
    "title": "Concepts",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html#sec-modbld-misc",
    "href": "qmd/model-building-concepts.html#sec-modbld-misc",
    "title": "Concepts",
    "section": "",
    "text": "Packages\n\n{multiverse} - makes it easy to specify and execute all combinations of reasonable analyses of a dataset\n\n\n\nPaper, Summary of it’s usage\nLots of vignettes\n\n\nRegression Workflow (Paper)\n\nMake ML model pipelines reusable and reproducible\n\n\nNotes from 7 Tips to Future-Proof Machine Learning Projects\nModularization - Useful for debugging and iteration\n\nDon’t used declarative programming. Create functions/classes for preprocessing, training, tuning, etc., and keep in separate files. You’ll call these functions in the main script\n\nHelper function\n## file preprocessing.py ##\ndef data_preparation(data):\n    data = data.drop(['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am'], axis=1)\n    numeric_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']\n    data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)\n    return data\nMain script\nfrom preprocessing import data_preparation \ntrain_preprocessed = data_preparation(train_data)\ninference_preprocessed = data_preparation(inference_data)\n\nKeep parameters in a separate config file\n\nConfig file\n## parameters.py ##\nDROP_COLS = ['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am']\nNUM_COLS = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']\nProprocessing script\n## preprocessing.py ##\nfrom parameters import DROP_COLS, NUM_COLS\ndef data_preparation(data):\n    data = data.drop(DROP_COLS, axis=1)\n    data[NUM_COLS] = data[NUM_COLS].fillna(data[NUM_COLS].mean())\n    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)\n    return data\n\n\nVersioning Code, Data, and Models - Useful for investigating drift\n\nSee tools like DVC, MLFlow, Weights and Biases, etc. for model and data versioning\n\nImportant to save data snapshots throughout the project lifecycle, for example: raw data, processed data, train data, validation data, test data and inference data.\n\nGithub and dbt for code versioning\n\nConsistent Structures - Consistency in project structures and naming can reduce human error, improve communication, and just make things easier to find.\n\nNaming examples:\n\n&lt;model-name&gt;-&lt;parameters&gt;-&lt;model-version&gt;\n&lt;model-name&gt;-&lt;data-version&gt;-&lt;use-case&gt;\n\nExample: Reduced project template based on {{cookiecutter}}\n├── data\n│   ├── output      &lt;- The output data from the model. \n│   ├── processed      &lt;- The final, canonical data sets for modeling.\n│   └── raw            &lt;- The original, immutable data dump.\n│\n├── models             &lt;- Trained and serialized models, model predictions, or model summaries\n│\n├── notebooks          &lt;- Jupyter notebooks. \n│\n├── reports            &lt;- Generated analysis as HTML, PDF, LaTeX, etc.\n│   └── figures        &lt;- Generated graphics and figures to be used in reporting\n│\n├── requirements.txt   &lt;- The requirements file for reproducing the analysis environment, e.g.\n│                         generated with `pip freeze &gt; requirements.txt`\n│\n├── code              &lt;- Source code for use in this project.\n    ├── __init__.py    &lt;- Makes src a Python module\n    │\n    ├── data           &lt;- Scripts to generate and process data\n    │   ├── data_preparation.py\n    │   └── data_preprocessing.py\n    │\n    ├── models         &lt;- Scripts to train models and then use trained models to make\n    │   │                 predictions\n    │   ├── inference_model.py\n    │   └── train_model.py\n    │\n    └── analysis  &lt;- Scripts to create exploratory and results oriented visualizations\n        └── analysis.py\n\n\nModel is performing well on the training set but much worse on the validation/test set\n\n\nAndrew Ng calls the validation set the “Dev Set” 🙄\nTest: Random sample the training set and use that as your validation set. Score your model on this new validation set\n\n“Train-Dev” is the sampled validation set\nPossibilities\n\nVariance: The data distribution of the training set is the same as the validation/test sets\n\n\nThe model has been overfit to the training data\n\nData Mismatch: The data distribution of the training set is NOT the same as the validation/test sets\n\n\nUnlucky and the split was bad\n\nSomething maybe is wrong with the splitting function\n\nSplit ratio needs adjusting. Validation set isn’t getting enough data to be representative.\n\n\n\n\nModel is performing well on the validation/test set but not in the real world\n\nInvestigate the validation/test set and figure out why it’s not reflecting real world data. Then, apply corrections to the dataset.\n\ne.g. distributions of your validation/tests sets should look like the real world data.\n\nChange the metric\n\nConsider weighting cases that your model is performing extremely poorly on.\n\n\nSplits\n\nHarrell: “not appropriate to split data into training and test sets unless n&gt;20,000 because of the luck (or bad luck) of the split.”\nIf your dataset is over 1M rows, then having a test set of 200K might be overkill (e.g. ratio of 60/20/20).\n\nMight be better to use a ratio of 98/1/1 for big data projects and 60/20/20 for smaller data projects\n\nlink\n\nShows that simple data splitting does not give valid confidence intervals (even asymptotically) when one refits the model on the whole dataset. Thus, if one wants valid confidence intervals for prediction error, we can only recommend either data splitting without refitting the model (which is viable when one has ample data), or nested CV.",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/cli.html",
    "href": "qmd/cli.html",
    "title": "CLI",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-misc",
    "href": "qmd/cli.html#sec-cli-misc",
    "title": "CLI",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly’s suggestions are prioritized in real time with a small neural network\n\nPath to a folder that’s above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder\n\nDebian vs. Ubuntu (from ChatGPT)\n\nStability vs. Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it’s known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu’s.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as “Debian spins,” catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-r",
    "href": "qmd/cli.html#sec-cli-r",
    "title": "CLI",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n      janitor::clean_names() %&gt;%\n      select(date, geo, county = name, negative, positive) %&gt;%\n      filter(geo == \"County\") %&gt;%\n      mutate(date = lubridate::as_date(date)) %&gt;%\n      select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it’ll search automatically\n35548",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-awk",
    "href": "qmd/cli.html#sec-cli-awk",
    "title": "CLI",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‘$13 &gt; 98’ adult_t.csv|head\nFilter lines with “Doctorate” and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-bash",
    "href": "qmd/cli.html#sec-cli-bash",
    "title": "CLI",
    "section": "Bash",
    "text": "Bash\n\nMisc\n\nNotes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn’t take notes on\n\nBash Scripting on Linux: The Complete Guide - video course\n\nResources\n\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n“&gt;” redirects the output from a program to a file.\n\n“&gt;&gt;” does the same thing, but it’s appending to an existing file instead of overwriting it, if it already exists.\n\n\n\n\n\nCommands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you’re using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you’re about to move does already exist under the new directory,\n\nThis way you don’t accidentally overwrite files that you didn’t mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\n\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\nOutput to file: echo “This is an example for redirect” &gt; file1.txt\nAppend line to file: echo “This is the second line of the file” &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to “tmp” directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name “my_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n“rwx” stand for read, write, and execute rights, respectively\nThe 3 “rwx” blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a “d” for directory or “l” for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - “super user” - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‘error’, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo “No such directory exist.Check”\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn’t exist, then the message “No such directory exists. check” message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for “error” and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for “error” in each line. Lines with “error” get written to “error_file.txt”\n\nFilter lines\ngrep -i “Doctorate” adult_t.csv |grep -i “Husband”|grep -i “Black”|csvlook\n# -i, --ignore-case-Ignore  case  distinctions,  so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i “Doctorate” adult_t.csv | wc -l\n\nCounts how many rows have “Doctorate”\n\n-wc is “word count”\n\n\n\n\n\n\nVariables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via “$”\n# local\nev_car=’Tesla’\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=’Tesla’\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it’s okay not to is on the left-hand-side of an [[ ]] condition. But even there I’d recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or –help or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this\n\n\n\n\n\nScripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like –silent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script’s directory close to the start of the script.\n\nAnd it’s usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck. Heed its warnings.\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don’t give executable permission to the script file.\nStarts with “#!” the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory “/bin”\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you’re good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n    #!/bin/bash\n    echo ‘Hello World’\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n    set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n    echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n    exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n    echo do awesome stuff\n}\nmain \"$@\"\n\n\n\nJob Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.\n\n\n\n\ntmux (‘terminal multiplexer’)\n\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‘moose’\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named “moose”\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you’ve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‘moose’\n\n\nPane Creation and Navigation\n\ncontrol+b then press ” (i.e. shift+’): add another terminal pane below\ncontrol+b then press % (i.e. shift+5) : add another terminal pane to the right\ncontrol+b then press → : move to the terminal pane on the right (similar for left, up, down)\n\n\n\n\nSSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n“ssh-keygen” is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named “id_rsa.pub” and a private key named “id_rsa”, and place both into your “~/.ssh” directory\nYou’ll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: “~/.ssh/config”\nContents\nHost dev\n  HostName remote\n  IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist\n\n\n\n\nVim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you’ll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you’re in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file\n\n\n\n\nPackages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to “upgrading” the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and “updates” them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade\n\n\n\n\nExpressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-powsh",
    "href": "qmd/cli.html#sec-cli-powsh",
    "title": "CLI",
    "section": "Powershell",
    "text": "Powershell\n\nMisc\n\nDocs, Sample Scripts\nUse Get-Help &lt;cmdlet-name&gt; to access help information for specific cmdlets.\n\n$_ (Docs)\n\nAutomatic Variable; Alias for $PSItem\n\nDocs for automatic variables\n\nUseful for looping through objects in a directory. See “Change Name (or Extensions) of Multiple Files” for an example of usage\nGet the properties of an object\n\nExample: Using Get-ItemProperty (Docs)\nGet-ItemProperty scrapsheet.txt | Format-List\n\nProperty names will be on the left side of the output\n\nExample: Using Get-Member (Docs)\n Get-ChildItem *.txt | Select-Object -First 1 | Get-Member\n\nSelects the first text file in the directory and gets the Properties and a bunch of other stuff like methods which also can be used with $_\nDescriptions are awful for a lot of the stuff, but for the most part, you can guess what the property is.\n\n\nSome properties that can be used with $_\n\nFile System Objects:\n\n.Name: Returns the name component of a file\n.FullName: Returns the full path of the file.\n.Length: Returns the size of the file in bytes.\n.CreationTime: Returns the date and time the file was created.\n.LastWriteTime: Returns the date and time the file was last modified.\n.IsReadOnly: Returns $true if the file is read-only, $false otherwise.\n\nProcess Objects:\n\n.Id: Returns the unique identifier (PID) of the process.\n.Name: Returns the name of the process.\n.MainWindowTitle: Returns the title of the process’s main window (if applicable).\n.WorkingDirectory: Returns the working directory of the process.\n.CPU: Returns the CPU usage of the process.\n\nRegistry Objects:\n\n.Name: Returns the name of the registry key.\n.Hive: Returns the hive of the registry key (e.g., HKLM, HKCU).\n.Value: Returns the value of the registry key (if applicable).\n.Path: Returns the full path of the registry key.\n\n\n\nComments: &lt;# comment #&gt;\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you’re already in the directory that you want the folder in. You can also use a path, e.g. \"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nChange Name (or Extensions) of Multiple Files\nGet-ChildItem *.md | Rename-Item -NewName { $_.Name -replace '.md','.qmd' }\n\nGet-ChildItem: cmdlet gets all the files in the current folder that have a .mc file extension\nResults piped to Rename-Item\n\nNewName: Has a value that’s a script block that runs before the value is submitted to the NewName parameter.\n\n$_: (automatic variable) It represents each file object as it comes to the command through the pipeline.\n-replace: Operator replaces the file extension of each file with .qmd.\n\nNotice that matching using the -replace operator is not case sensitive.\n\n\n\n\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it’s a backslash (\\), but in Powershell, it’s a backtick ( ` )\n*Don’t forget that there’s a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it’s open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-batscri",
    "href": "qmd/cli.html#sec-cli-batscri",
    "title": "CLI",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g. timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-wsl",
    "href": "qmd/cli.html#sec-cli-wsl",
    "title": "CLI",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for –distribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/json.html",
    "href": "qmd/json.html",
    "title": "JSON",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-misc",
    "href": "qmd/json.html#sec-json-misc",
    "title": "JSON",
    "section": "",
    "text": "Packages\n\n{yyjsonr} - A fast JSON parser/serializer, which converts R data to/from JSON and NDJSON. It is around 2x to 10x faster than jsonlite at both reading and writing JSON.\n{RcppSimdJson} - Comparable to {yyjsonr} in performance.\n\nAlso see\n\nBig Data &gt;&gt; Larger than Memory\nSQL &gt;&gt; Processing Expressions &gt;&gt; Nested Data\nDatabases &gt;&gt; DuckDB &gt;&gt; Misc\n\nhrbmstr recommends trying duckdb before using the cli tools in “Big Data”\n\n\nTools\n\n{listviewer}: Allows you to interactively explore and edit json files through the Viewer in the IDE. Docs show how it can be embedded into a Shiny app as well.\n\nExample\nlibrary(listviewer)\nmoose &lt;- jsonlite::read_json(\"path/to/file.json\")\njsonedit(moose)\nreactjson(moose)\n\nI’ve also used this a .config file which looked like a json file when I opened in a text editor, so this seems to work on anything json-like.\nreactjson has a copy button which is nice so that you can paste your edited version into a file.\njsonedit seems like it has more features, but I didn’t see a copy button. But there’s a view in which you can manually select everything a copy it via keyboard shortcut.",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-jsonlite",
    "href": "qmd/json.html#sec-json-jsonlite",
    "title": "JSON",
    "section": "{jsonlite}",
    "text": "{jsonlite}\n\nRead",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-py",
    "href": "qmd/json.html#sec-json-py",
    "title": "JSON",
    "section": "Python",
    "text": "Python\n\nExample: Parse Nested JSON into a dataframe (article)\n\nRaw JSON\n\n\n“entry” has the data we want\n“…” at the end indicates there are multiple objectss inside the element, “entry”\n\nProbably other root elements other than “feed” as well\n\n\nRead a json file from a URL using {{requests}} and convert to list\n\nimport requests\n\nurl = \"https://itunes.apple.com/gb/rss/customerreviews/id=1500780518/sortBy=mostRecent/json\"\n\nr = requests.get(url)\n\ndata = r.json()\nentries = data[\"feed\"][\"entry\"]\n\nIt looks like the list conversion also ordered the elements alphabetically\nThe output list is subsetted by the root element “feed” and the child element “entry”\n\nGet a feel for the final structure you want by hardcoding elements into a df\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    parsed_data[\"author_uri\"].append(entry[\"author\"][\"uri\"][\"label\"])\n    parsed_data[\"author_name\"].append(entry[\"author\"][\"name\"][\"label\"])\n    parsed_data[\"author_label\"].append(entry[\"author\"][\"label\"])\n    parsed_data[\"content_label\"].append(entry[\"content\"][\"label\"])\n    parsed_data[\"content_attributes_type\"].append(entry[\"content\"][\"attributes\"][\"type\"])\n    ... \nGeneralize extracting the properties of each object in “entry” with a nested loop\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    for key, val in entry.items():\n        for subkey, subval in val.items():\n            if not isinstance(subval, dict):\n                parsed_data[f\"{key}_{subkey}\"].append(subval)\n            else:\n                for att_key, att_val in subval.items():\n                    parsed_data[f\"{key}_{subkey}_{att_key}\"].append(att_val)\n\ndefaultdict creates a key from a list element (e.g. “author”) and groups the properties into a list of values where the value may also be a dict.\n\nSee Python, General &gt;&gt; Types &gt;&gt; Dictionaries\n\nFor each item in “entry”, it looks at the first key-value pair knowing that value is always a dictionary (object in JSON)\nThen handles two different cases\n\nFirst Case: The value dictionary is flat and does not contain another dictionary, only key-value pairs.\n\nCombine the outer key with the inner key to a column name and take the value as column value for each pair.\n\nSecond Case: Dictionary contains a key-value pair where the value is again a dictionary.\n\nAssumes at most two levels of nested dictionaries\nIterates over the key-value pairs of the inner dictionary and again combines the outer key and the most inner key to a column name and take the inner value as column value.\n\n\n\nRecursive function that handles json elements with deeper structures\n\ndef recursive_parser(entry: dict, data_dict: dict, col_name: str = \"\") -&gt; dict:\n    \"\"\"Recursive parser for a list of nested JSON objects\n\n    Args:\n        entry (dict): A dictionary representing a single entry (row) of the final data frame.\n        data_dict (dict): Accumulator holding the current parsed data.\n        col_name (str): Accumulator holding the current column name. Defaults to empty string.\n    \"\"\"\n    for key, val in entry.items():\n        extended_col_name = f\"{col_name}_{key}\" if col_name else key\n        if isinstance(val, dict):\n            recursive_parser(entry[key], data_dict, extended_col_name)\n        else:\n            data_dict[extended_col_name].append(val)\n\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    recursive_parser(entry, parsed_data, \"\")\n\ndf = pd.DataFrame(parsed_data)\n\nNotice the check for a deeper structure with isinstance. If there is one, then the function is called again.\nFunction outputs a dict which is coerced into dataframe\nTo get rid of “label” in column names: df.columns = [col if not \"label\" in col else \"_\".join(col.split(\"_\")[:-1]) for col in df.columns]\nobject types can be cast into more efficient types: df[\"im:rating\"] = df[\"im:rating\"].astype(int)",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html",
    "href": "qmd/cli-linux.html",
    "title": "Linux",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-misc",
    "href": "qmd/cli-linux.html#sec-cli-lin-misc",
    "title": "Linux",
    "section": "",
    "text": "Notes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn’t take notes on\n\nBash Scripting on Linux: The Complete Guide - video course\n\nResources\n\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n“&gt;” redirects the output from a program to a file.\n\n“&gt;&gt;” does the same thing, but it’s appending to an existing file instead of overwriting it, if it already exists.\n\n\nDebian vs. Ubuntu (from ChatGPT)\n\nStability vs. Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it’s known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu’s.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as “Debian spins,” catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-com",
    "href": "qmd/cli-linux.html#sec-cli-lin-com",
    "title": "Linux",
    "section": "Commands",
    "text": "Commands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you’re using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you’re about to move does already exist under the new directory,\n\nThis way you don’t accidentally overwrite files that you didn’t mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\n\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\nOutput to file: echo “This is an example for redirect” &gt; file1.txt\nAppend line to file: echo “This is the second line of the file” &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to “tmp” directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name “my_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n“rwx” stand for read, write, and execute rights, respectively\nThe 3 “rwx” blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a “d” for directory or “l” for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - “super user” - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‘error’, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo “No such directory exist.Check”\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn’t exist, then the message “No such directory exists. check” message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for “error” and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for “error” in each line. Lines with “error” get written to “error_file.txt”\n\nFilter lines\ngrep -i “Doctorate” adult_t.csv |grep -i “Husband”|grep -i “Black”|csvlook\n# -i, --ignore-case-Ignore  case  distinctions,  so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i “Doctorate” adult_t.csv | wc -l\n\nCounts how many rows have “Doctorate”\n\n-wc is “word count”",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-var",
    "href": "qmd/cli-linux.html#sec-cli-lin-var",
    "title": "Linux",
    "section": "Variables",
    "text": "Variables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via “$”\n# local\nev_car=’Tesla’\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=’Tesla’\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it’s okay not to is on the left-hand-side of an [[ ]] condition. But even there I’d recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or –help or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-script",
    "href": "qmd/cli-linux.html#sec-cli-lin-script",
    "title": "Linux",
    "section": "Scripting",
    "text": "Scripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like –silent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script’s directory close to the start of the script.\n\nAnd it’s usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck. Heed its warnings.\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don’t give executable permission to the script file.\nStarts with “#!” the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory “/bin”\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you’re good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n    #!/bin/bash\n    echo ‘Hello World’\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n    set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n    echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n    exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n    echo do awesome stuff\n}\nmain \"$@\"",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-jobm",
    "href": "qmd/cli-linux.html#sec-cli-lin-jobm",
    "title": "Linux",
    "section": "Job Management",
    "text": "Job Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-tmux",
    "href": "qmd/cli-linux.html#sec-cli-lin-tmux",
    "title": "Linux",
    "section": "tmux (terminal multiplexer)",
    "text": "tmux (terminal multiplexer)\n\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‘moose’\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named “moose”\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you’ve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‘moose’\n\n\nPane Creation and Navigation\n\ncontrol+b then press ” (i.e. shift+’): add another terminal pane below\ncontrol+b then press % (i.e. shift+5) : add another terminal pane to the right\ncontrol+b then press → : move to the terminal pane on the right (similar for left, up, down)",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-ssh",
    "href": "qmd/cli-linux.html#sec-cli-lin-ssh",
    "title": "Linux",
    "section": "SSH",
    "text": "SSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n“ssh-keygen” is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named “id_rsa.pub” and a private key named “id_rsa”, and place both into your “~/.ssh” directory\nYou’ll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: “~/.ssh/config”\nContents\nHost dev\n  HostName remote\n  IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-vim",
    "href": "qmd/cli-linux.html#sec-cli-lin-vim",
    "title": "Linux",
    "section": "Vim",
    "text": "Vim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you’ll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you’re in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-pkg",
    "href": "qmd/cli-linux.html#sec-cli-lin-pkg",
    "title": "Linux",
    "section": "Packages",
    "text": "Packages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to “upgrading” the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and “updates” them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-expr",
    "href": "qmd/cli-linux.html#sec-cli-lin-expr",
    "title": "Linux",
    "section": "Expressions",
    "text": "Expressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-general.html",
    "href": "qmd/cli-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-misc",
    "href": "qmd/cli-general.html#sec-cli-gen-misc",
    "title": "General",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly’s suggestions are prioritized in real time with a small neural network\n\nPath to a folder that’s above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-r",
    "href": "qmd/cli-general.html#sec-cli-gen-r",
    "title": "General",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n      janitor::clean_names() %&gt;%\n      select(date, geo, county = name, negative, positive) %&gt;%\n      filter(geo == \"County\") %&gt;%\n      mutate(date = lubridate::as_date(date)) %&gt;%\n      select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it’ll search automatically\n35548",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-awk",
    "href": "qmd/cli-general.html#sec-cli-gen-awk",
    "title": "General",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‘$13 &gt; 98’ adult_t.csv|head\nFilter lines with “Doctorate” and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html",
    "href": "qmd/cli-windows.html",
    "title": "Windows",
    "section": "",
    "text": "PowerShell",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-powsh",
    "href": "qmd/cli-windows.html#sec-cli-win-powsh",
    "title": "Windows",
    "section": "",
    "text": "Misc\n\nDocs, Sample Scripts\nUse Get-Help &lt;cmdlet-name&gt; to access help information for specific cmdlets.\nCheck version: $PSVersionTable\n\nFor a breakdown of the version number (e.g. build, revison, etc.): $PSVersionTable.PSVersion\n\nUpdate to latest stable version: github\nComments: &lt;# comment #&gt;\nClear terminal: clear or cls or Clear-Host\nBefore you’ll be able to run a script, you need to open PowerShell as administrator and execute this command: Set-ExecutionPolicy RemoteSigned\nShortcuts\n\nRun selected PowerShell code in current terminal using F8\nLaunch online help for the symbol under the cursor using Ctrl + F1\n\n\n\n\nLoops\n\nIterables\n\nArrays : $folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n$_ (Docs)\n\nAutomatic Variable; Alias for $PSItem\n\nDocs for automatic variables\n\nUseful for looping through objects in a directory. See “Change Name (or Extensions) of Multiple Files” for an example of usage\nGet the properties of an object\n\nExample: Using Get-ItemProperty (Docs)\nGet-ItemProperty scrapsheet.txt | Format-List\n\nProperty names will be on the left side of the output\n\nExample: Using Get-Member (Docs)\n Get-ChildItem *.txt | Select-Object -First 1 | Get-Member\n\nSelects the first text file in the directory and gets the Properties and a bunch of other stuff like methods which also can be used with $_\nDescriptions are awful for a lot of the stuff, but for the most part, you can guess what the property is.\n\n\nSome properties that can be used with $_\n\nFile System Objects:\n\n.Name: Returns the name component of a file\n.FullName: Returns the full path of the file.\n.Length: Returns the size of the file in bytes.\n.CreationTime: Returns the date and time the file was created.\n.LastWriteTime: Returns the date and time the file was last modified.\n.IsReadOnly: Returns $true if the file is read-only, $false otherwise.\n\nProcess Objects:\n\n.Id: Returns the unique identifier (PID) of the process.\n.Name: Returns the name of the process.\n.MainWindowTitle: Returns the title of the process’s main window (if applicable).\n.WorkingDirectory: Returns the working directory of the process.\n.CPU: Returns the CPU usage of the process.\n\nRegistry Objects:\n\n.Name: Returns the name of the registry key.\n.Hive: Returns the hive of the registry key (e.g., HKLM, HKCU).\n.Value: Returns the value of the registry key (if applicable).\n.Path: Returns the full path of the registry key.\n\n\n\n\n\n\nForeach\n\nUses a typical for-loop structure\nSee Snippets for an example of iterating over the output of Get-ChildItem\nIterate over an array\n# Create an array of folders\n$folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n\n# Perform iteration to create the same file in each folder\nforeach ($i in $folders) {\n    Add-Content -Path \"$i\\SampleFile.txt\" -Value \"This is the content of the file\"\n}\n\n$i is the for-loop variable and $folders is the iterable\nAdd-Content creates a text file in each of the folders in the array.\n\n\n\n\nForEach-Object\n\nSimilar to {purrr::map}\nIterable is piped into ForEach-Object\n$folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n$folders | ForEach-Object (Add-Content -Path \"$_\\SampleFile.txt\" -Value \"This is the content of the file\")\n\nDoes the same thing as the first example in the Foreach section\nAdd-Content creates a text file in each of the folders in the array.\n$_ is the for-loop variable — called an “automatic variable.” See Iterables section.\n\n\n\n\nForEach Method\n\nSimilar to using Pyhon’s apply on an iterable.\nMethod applied an array\n$folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n$folders.ForEach({\n    Add-Content -Path \"$_\\SampleFile.txt\" -Value \"This is the content of the file\"\n})\n\nDoes the same thing as the first example in the Foreach section\nAdd-Content creates a text file in each of the folders in the array.\n$_ is the for-loop variable — called an “automatic variable.” See Iterables section.\n\n\n\n\n\nCommands\n\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you’re already in the directory that you want the folder in. You can also use a path, e.g. \"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nChange Name (or Extensions) of Multiple Files\nGet-ChildItem *.md | Rename-Item -NewName { $_.Name -replace '.md','.qmd' }\n\nGet-ChildItem: cmdlet gets all the files in the current folder that have a .mc file extension\nResults piped to Rename-Item\n\nNewName: Has a value that’s a script block that runs before the value is submitted to the NewName parameter.\n\n$_: (automatic variable) It represents each file object as it comes to the command through the pipeline.\n-replace: Operator replaces the file extension of each file with .qmd.\n\nNotice that matching using the -replace operator is not case sensitive.\n\n\n\n\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it’s a backslash (\\), but in Powershell, it’s a backtick ( ` )\n*Don’t forget that there’s a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it’s open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"\n\n\n\n\nSnippets\n\nRead in name of servers and ping each of them\n\n$servers = Get-Content .\\servers.txt\n\nforeach ($server in $servers) {\n    try {\n        $null = Test-Connection -ComputerName $server -Count 1 -ErrorAction STOP\n        Write-Output \"$server - OK\"\n    }\n    catch {\n        Write-Output \"$server - $($_.Exception.Message)\"\n    }\n}\n\nGet-Content reads the server names from each line in the the server.txt file\nforeach iterates through the server names\ntry tests the connection and catch outputs an error message if a server fails.\nIf Test-Connection fails the error message is stored in the $null variable\nThe error message line has an interesting syntax\n\n$_ is an automatic variable that represents $null which contains the error message which is selected by .Exception-Message.\n$() evaluates the expression\n\n\nTake files from a directory and iterate them as inputs to a function.\n$directory = \"C:\\Users\\me\\Documents\\AnyCap Screen Recorder\"\n\n# Define the FFmpeg command\n$ffmpegCommand = '-i {0} ' +\n                 '-c:v libx265 ' +\n                 '-crf 28 ' +\n                 '-preset medium ' +\n                 '-vf scale=-1:720 ' +\n                 '-c:a copy ' +\n                 'C:\\Users\\me\\Documents\\temp-storage\\{1}'\n\n# Get all files in the directory\n$files = Get-ChildItem -Path $directory `\n                       -Filter \"*.mp4\" \n\n# Loop through each file and apply the FFmpeg command\nforeach ($file in $files) {\n  # Construct the full command with the current file path\n  $fullCommand = $ffmpegCommand -f \"`\"$($file.FullName)`\"\", $file.Name\n  # Execute the FFmpeg command\n  Start-Process -FilePath \"ffmpeg.exe\" `\n                -ArgumentList $fullCommand `\n                -Wait `\n                -NoNewWindow\n}\n\nWrite-Host \"Finished processing files!\"\n\n$ffmpegCommand variable is a concantenated string using multiple lines for readability. {0} and {1} are placeholders to be filled in later.\n\nNote the space included at the end of each argument before the single quote since there’s no space included during concantenation.\n\nGet-ChildItem retrieves files from the specified directory ($directory).\n\n-Filter filters files that match the pattern (e.g., *.mp4).\n\nThe foreach loop iterates through each file ($file) in the $files collection.\n\n-f flag stands for format. Says to replace {0} and {1} in the $ffmpegCommand template with these properties.\n\"`\"$($file.FullName)`\"\"\n\nSince the directory name has spaces in it, extra quotes must included in order for the path to be quoted within the output string. A quoted file path in necessary forffmpeg to be able to read a directory name with spaces in it.\n$file.FullName: This is the full path of the current file. It is enclosed in $() to ensure that the property is properly evaluated and its value is included in the string.\n\nIf there were no spaces in the directory name, then $file.FullName is only thing that would be required. Everything else in this description could be discarded\n\n`\"$($file.FullName)`\": The double quotes \"...\" are used to create a string literal. Placing the entire expression $($file.FullName) within these double quotes ensures that the value of $file.FullName is treated as a single string, even if it contains spaces or special characters.\n\nThe backticks are escape characters in PowerShell and indicate that the double quotes should be treated as literal characters and not as operators formatting a string.\n\n\"`\"\\$(\\$file.FullName)`\"\": The additional double quotes at the beginning and end are used to format the expression as string for when it’s used as an argument in Start-Process.\nThe resulting path in the ffmpeg argument will look like: \"&lt;full file path&gt;\".\n\nStart-Process launches ffmpeg.exe with the constructed $fullCommand arguments.\n\n-Wait ensures the command finishes before continuing.\n-NoNewWindow says run ffmpeg in the same console window and don’t open a new one.",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-batscri",
    "href": "qmd/cli-windows.html#sec-cli-win-batscri",
    "title": "Windows",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g. timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-wsl",
    "href": "qmd/cli-windows.html#sec-cli-win-wsl",
    "title": "Windows",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for –distribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html",
    "href": "qmd/confidence-and-prediction-intervals.html",
    "title": "Confidence & Prediction Intervals",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-misc",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-misc",
    "title": "Confidence & Prediction Intervals",
    "section": "",
    "text": "Also see Mathematices, Statistics &gt;&gt; Descriptive Statistics &gt;&gt; Understanding CI, sd, and sem Bars\nSE used for CIs of the difference in proportion\n\\[\n\\text{SE} = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\n\\]",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-terms",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-terms",
    "title": "Confidence & Prediction Intervals",
    "section": "Terms",
    "text": "Terms\n\nConfidence Intervals: A range of values within which we are reasonably confident the true parameter (e.g mean) of a population lies, based on a sample statistic (e.g. t-stat).\n\nFrequentist Interpretation: The confidence interval is constructed by a procedure, which, if you were to repeat the experiment and collecting samples many many times, in 95% of the experiments, the corresponding confidence intervals would cover the true value of the population mean. (link)\n\\[\n[100\\cdot(1-\\alpha)]\\;\\%\\: \\text{CI for}\\: \\hat\\beta_i = \\hat\\beta_i \\pm \\left[t_{(1-\\alpha/2)(n-k)} \\cdot \\text{SE}(\\hat\\beta_i)\\right]\n\\]\n\n\\(t\\) is the t-stat for\n\n\\(n-k\\) = sample size - number of predictors\n\\(1-\\alpha\\) for 2-sided; \\(1 - (\\alpha/2)\\) for 1 sided (I think)\n\n\\(\\text{SE}(\\beta_i)\\) is the sqrt of the corresponding value on the diagonal of the variance-covariance matrix for the coefficients.\n\nBayesian Interpretation: the true value is in that interval with 95% probability\n\nCoverage or Empirical Coverage: The level of coverage actually observed when evaluated on a dataset, typically a holdout dataset not used in training the model. Rarely will your model produce the Expected Coverage exactly\n\nAdaptive Coverage: Setting your Expected Coverage so that your Empirical Coverage = Target Coverage. A conformal prediction algorithm is adaptive if it not only achieves marginal coverage, but also (approximately) conditional coverage\n\nExample: 90% target coverage\n\nIf our model is slightly overfit, you might see that a 90% expected coverage leads to an 85% empirical coverage on a holdout dataset. To align your target and empirical coverage at 90%, may require setting expected coverage at something like 93%\n\n\nExpected Coverage: The level of confidence in the model for the prediction intervals.\nConditional Coverage: The coverage for each individual class of the outcome variable or subset of data specified by a grouping variable.\nMarginal Coverage: The overall average coverage across all classes of the outcome variable. All conformal methods achieve at or near the Expected Coverage averaged across classes but not necessarily for each individual class.\nTarget Coverage: The level of coverage you want to attain on a holdout dataset\n\ni.e. The proportion of observations you want to fall within your prediction intervals\n\n\nJeffrey’s Interval: Bayesian CIs for Binomial proportions (i.e. probability of an event)\n# probability of event\n# n_rain in the number of events (rainy days)\n# n is the number of trials (total days)\nmutate(pct_rain = n_rain / n, \n       # jeffreys interval\n       # bayesian CI for binomial proportions\n       low = qbeta(.025, n_rain + .5, n - n_rain + .5), \n       high = qbeta(.975, n_rain + .5, n - n_rain + .5))\nPrediction Interval: Used to estimate the range within which a future observation is likely to fall\n\nStandard Procedure for computing PIs for predictions (See link for examples and further details)\n\\[\n\\hat Y_0 \\pm t^{n-p}_{\\alpha/2} \\;\\hat\\sigma \\sqrt{1 + \\vec x_0'(X'X)^{-1}\\vec x_0}\n\\]\n\n\\(Y_0\\) is a single prediction\n\\(t\\) is the t-stat for\n\n\\(n-p\\) = sample size - number of predictors\n\\(1 - \\alpha\\) for 2-sided; \\(1 - (\\alpha/2)\\) for 1 sided (I think)\n\n\\(\\hat\\sigma\\) is the variance given by residual standard error, summary(Model1)$sigma\n\\[\nS^2 = \\frac{1}{n-p}\\;||\\;Y-X\\hat \\beta\\;||^2\n\\]\n\n\\(S = \\hat \\sigma\\)\nI think this is also the \\(\\operatorname{MSE}/\\operatorname{dof}\\) that you sometimes see in other formulas\n\n\\(x_0\\) is new data for the predictor variable values for the prediction (also would need to include a 1 for the intercept)\n\\((X'X)^{-1}\\) is the variance covariance matrix, vcov(model)",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-diag",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-diag",
    "title": "Confidence & Prediction Intervals",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nMean Interval Score (MIS)\n\n(Proper) Score of both coverage and interval width\n\nI don’t think there’s a closed range, so it’s meant for model comparison\nLower is better\n\ngreybox::MIS and (scaled) greybox::sMIS\n\nOnline docs don’t have these functions, but docs in RStudio do\n\nAlso scoringutils::interval_score\n\nDocs have formula\n\nThe actual paper is dense Need to take the mean of MIS\n\n\n\nCoverage\n\nExample: Coverage %\ncoverage &lt;- function(df, ...){\n  df %&gt;%\n    mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price pred_upper, 1, 0)) %&gt;% \n    group_by(...) %&gt;% \n    summarise(n = n(),\n              n_covered = sum(\n                covered\n              ),\n              stderror = sd(covered) / sqrt(n),\n              coverage_prop = n_covered / n)\n}\nrf_preds_test %&gt;% \n  coverage() %&gt;% \n  mutate(across(c(coverage_prop, stderror), ~.x * 100)) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt_number(\"stderror\", decimals = 2) %&gt;% \n  gt::fmt_number(\"coverage_prop\", decimals = 1)\n\nFrom Quantile Regression Forests for Prediction Intervals\nSale_Price is the outcome variable\nrf_preds_test is the resulting object from predict with a tidymodels model as input\n\nExample: Test consistency of coverage across quintiles\npreds_intervals %&gt;%  # preds w/ PIs\n  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;%  # quintiles\n  mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price &lt;= .pred_upper, 1, 0)) %&gt;% \n  with(chisq.test(price_grouped, covered))\n\np value &lt; 0.05 says coverage significantly differs by quintile\nSale_Price is the outcome variable\n\n\nInterval Width\n\nNarrower bands should mean a more precise model\nExample: Average interval width across quintiles\nlm_interval_widths &lt;- preds_intervals %&gt;% \n  mutate(interval_width = .pred_upper - .pred_lower,\n        interval_pred_ratio = interval_width / .pred) %&gt;% \n  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;% # quintiles\n  group_by(price_grouped) %&gt;% \n  summarize(n = n(),\n            mean_interval_width_percentage = mean(interval_pred_ratio),\n            stdev = sd(interval_pred_ratio),\n            stderror = stdev / sqrt(n)) %&gt;% \n  mutate(x_tmp = str_sub(price_grouped, 2, -2)) %&gt;% \n  separate(x_tmp, c(\"min\", \"max\"), sep = \",\") %&gt;% \n  mutate(across(c(min, max), as.double)) %&gt;% \n  select(-price_grouped)\n\nlm_interval_widths %&gt;% \n  mutate(across(c(mean_interval_width_percentage, stdev, stderror), ~.x*100)) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt_number(c(\"stdev\", \"stderror\"), decimals = 2) %&gt;% \n  gt::fmt_number(\"mean_interval_width_percentage\", decimals = 1)\n\nInterval width has actually been transformed into a percentage as related to the prediction (removes the scale of the outcome variable)",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-boot",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-boot",
    "title": "Confidence & Prediction Intervals",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nMisc\n\nDo NOT bootstrap the standard deviation\n\narticle\nbootstrap is “based on a weak convergence of moments”\nif you use an estimate based standard deviation of the bootstrap, you are being overly conservative (i.e. overestimate the sd)\n\nbootstrapping uses the original, initial sample as the population from which to resample, whereas Monte Carlo simulation is based on setting up a data generation process (with known values of the parameters of a known distribution). Where Monte Carlo is used to test drive estimators, bootstrap methods can be used to estimate the variability of a statistic and the shape of its sampling distribution\nPackages\n\n{ebtools::get_boot_ci}\n\n\nSteps\n\nResample with replacement\nCalculate statistic of resample\nStore statistic\nRepeat 10K or so times\nCalculate mean, sd, and quantiles for CIs across all collected statistics\n\nCIs\n\nPlenty of articles for means and models, see bkmks\nrsample::reg_intervals is a convenience function for lm, glm, survival models\n\nPIs\n\nBootstrapping PIs is a bit complicated\n\nSee Shalloway’s article (code included)\nonly use out-of-sample estimates to produce the interval\nestimate the uncertainty of the sample using the residuals from a separate set of models built with cross-validation",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-conf",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-conf",
    "title": "Confidence & Prediction Intervals",
    "section": "Conformal Prediction Intervals",
    "text": "Conformal Prediction Intervals\n\nMisc\n\nPackages\n\n{{mapie}} - Handles scikit-learn, tf, pytorch, etc. with wrappers. Computes conformal PIs for Regression, Classification, and Time Series models.\n\nRegression\n\nMethods: naive, split, jackknife, jackknife+, jackknife-minmax, jackknife-after-bootstrap, CV, CV+, CV-minmax, ensemble batch prediction intervals (EnbPI).\n“Since the typical coverage levels estimated by jackknife+ follow very closely the target coverage levels, this method should be used when accurate and robust prediction intervals are required.”\n“For practical applications where N is large and/or the computational time of each leave-one-out simulation is high, it is advised to adopt the CV+ method” even though the interval width will be slightly larger than jackknife+\n“The jackknife-minmax and CV-minmax methods are more conservative since they result in higher theoretical and practical coverages due to the larger widths of the prediction intervals. It is therefore advised to use them when conservative estimates are needed.”\n“The conformalized quantile regression method allows for more adaptiveness on the prediction intervals which becomes key when faced with heteroscedastic data.”\nEnbPI is for time series and residuals must be updated each time new observations are available\n\nClassification\n\nMethods: LAC, Top-K, Adaptive Prediction Sets (APS), Regularized Adaptive Prediction Sets (RAPS), Split and Cross-Conformal methods.\nThe difference between these methods is the way the conformity scores are computed\nLAC method is not adaptive: the coverage guarantee only holds on average (i.e. marginal coverage). Difficult classification cases may have prediction sets that are too small, and easy cases may have sets that are too large. (See below for details on process). Doesn’t seem like to great a task to manually make it adaptive though (See example below).\nAPS’ conformity score used to determine the threshold is a constrained sum of the predicted probabilities for that observation. Only the predicted probabilites \\(\\ge\\) the predicted probability of the true class are included in the sum. Everything else is the same as the LAC algorithm, although the default behavior is to keep the last class that crosses the threshold through the argument, include_last_label = [True, “randomized”, False]. The value of the argument can determine whether conditional coverage is (approximately) attained with True being the most liberal setting. Note that only “randomized” can produce empty predicted class sets. Algorithm tends to produce large predicted class sets when there are many classes in the outcome variable.\nRAPS attenuates the lengthier predicted class sets in APS through regularization. A penalty, \\(\\lambda\\), is added to predicted probabilities with ranks greater that some value, \\(k\\). Everything else is the same as APS.\nNot sure what Split is, but Cross-Conformal is CV applied to LAC and APS.\n\n\n\nNotes from\n\nHow to Handle Uncertainty in Forecasts: A deep dive into conformal prediction\n\nThe conformity score formula used in this article, \\(s_i = |\\;y_i - \\hat p_i(y_i\\;|\\;X_i)\\;|\\) where \\(y_i\\) is the observed class and \\(\\hat p\\) is the predicted probability, has the same results as to the one below, but it’s not workable in production since there is no observed class.\n\nConformal Prediction for Machine Learning Classification — From the Ground Up\n“MAPIE” Explained Exactly How You Wished Someone Explained to You\n\nResources\n\nIntroduction To Conformal Prediction With Python A Short Guide for Quantifying Uncertainty of Machine Learning Models\n\nSee R &gt;&gt; Documents &gt;&gt; Machine Learning\n\n\nNormal PIs require iid data while conformal PIs only require the “identically distributed” part (not independent) and therefore should provide more robust coverage.\nContinuous outcome (link) using quantile regression\n\n\n\nClassification\n\nLAC (aka Score Method) Process\n\nSplit data into Train, Calibration (aka Validation), and Test\nTrain the model on the training set\nOn the calibration (aka validation) set, compute the conformity scores only for the observed class (i.e. true label) for each observation\n\\[\ns_{i, j}  = 1 - \\hat p_{i,j}(y_i | X_i)\n\\]\n\nVariables\n\n\\(s_{i,j}\\): Conformity Score for the ith observation and class \\(j\\)\n\\(y_i\\): Observed Class\n\\(\\hat p_{i,j}\\): Predicted probability by the model for class \\(j\\)\n\\(X_i\\): Predictors\n\\(i\\): Index of the observed data\n\\(j\\): Class of the outcome variable\n\nRange: [0, 1]\nIn general, Low = good, High = bad\nIn R, the predicted probabilities for statistical models are always for the event (i.e. \\(y_i = 1\\)) in a binary outcome context, so when the observed class = 0, the score will be \\(s_{i,0} = 1-(1- \\hat p_{i, 1}(y_i | X_i)) = \\hat p_{i, 1}(y_i | X_i)\\) which is just the predicted probability.\n\nOrder the conformity scores from highest to lowest\nAdjust the chosen the \\(\\alpha\\) using a finite sample correction, \\(q_{\\text{level}} = 1- \\frac{ceil((n_{\\text{cal}}+1)\\alpha)}{n_{\\text{cal}}}\\) and calculate the quantile.\nCalculate the critical value or threshold for the quantile\n\n\nx-axis corresponds to an ordered set of conformity scores\nIf \\(\\alpha = 0.05\\), find the score value at the the 95th percentile (e.g. quantile(scores, 0.95))\nBlue: conformity scores are not statistically significant. They’re within our prediction interval.\nRed: Very large conformity scores indicate high divergence from the true label. These conformal scores are statistically significant and thereby outside of our prediction interval.\n\nPredict on the Test set and calculate conformity scores for each class\nFor each test set observation, select classes that have scores below the threshold score as the model prediction.\n\nAn observation could potentially have both classes or no classes selected. ( Not sure if this is true in a binary outcome situation)\n\n\nExample: LAC Method, Multinomial\n\nModel\nclassifier = LogisticRegression(random_state=42)\nclassifier.fit(X_train, y_train)\nScores calculated using only the predicted probability for the true class on the Validation set (aka Calibration set)\n# Get predicted probabilities for calibration set\ny_pred = classifier.predict(X_Cal)\ny_pred_proba = classifier.predict_proba(X_Cal)\nsi_scores = []\n# Loop through all calibration instances\nfor i, true_class in enumerate(y_cal):\n    # Get predicted probability for observed/true class\n    predicted_prob = y_pred_proba[i][true_class]\n    si_scores.append(1 - predicted_prob) \nThe threshold determines what coverage our predicted labels will have\nnumber_of_samples = len(X_Cal)\nalpha = 0.05\nqlevel = (1 - alpha) * ((number_of_samples + 1) / number_of_samples)\nthreshold = np.percentile(si_scores, qlevel*100)\nprint(f'Threshold: {threshold:0.3f}')\n#&gt; Threshold: 0.598\n\nFinite sample correction for the 95th quantile: multiply 0.95 by (n+1)/n\n\nThreshold is then used to get predicted labels of the test set\n# Get standard predictions for comparison\ny_pred = classifier.predict(X_test)\n# Calc scores, then only take scores in the 95% conformal PI\nprediction_sets = (1 - classifier.predict_proba(X_test) &lt;= threshold)\n\n# Get labels for predictions in conformal PI\ndef get_prediction_set_labels(prediction_set, class_labels):\n    # Get set of class labels for each instance in prediction sets\n    prediction_set_labels = [\n        set([class_labels[i] for i, x in enumerate(prediction_set) if x]) for prediction_set in \n        prediction_sets]\n    return prediction_set_labels\n\n# Compare conformal prediction with observed and traditional preds\nresults_sets = pd.DataFrame()\nresults_sets['observed'] = [class_labels[i] for i in y_test]\nresults_sets['conformal'] = get_prediction_set_labels(prediction_sets, class_labels)\nresults_sets['traditional'] = [class_labels[i] for i in y_pred]\nresults_sets.head(10)\n#&gt;    observed  conformal        traditional\n#&gt; 0  blue      {blue}           blue\n#&gt; 1  green     {green}          green\n#&gt; 2  blue      {blue}           blue\n#&gt; 3  green     {green}          green\n#&gt; 4  orange    {orange}         orange\n#&gt; 5  orange    {orange}         orange\n#&gt; 6  orange    {orange}         orange\n#&gt; 7  orange    {blue, orange}   blue\n#&gt; 8  orange    {orange}         orange\n#&gt; 9  orange    {orange}         orange\n\nconformity scores are calculated for each potential class using the predicted probabilities on the test set\nThe predicted class for an observation is determined by whether a class has a score below the threshold.\nTherefore, an observation may have 1 or more predicted classes or 0 predicted classes.\n\nStatistics (See Statistics section for functions)\n\nOverall\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n#&gt; Overall coverage: 0.947\n#&gt; Average set size: 1.035\n\nOverall coverage is very close to the target coverage of 95%, therefore, marginal coverage is achieved which is expected for this method\n\nPer Class\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n#&gt;         Class counts  Coverage   Average set size\n#&gt; blue    241           0.817427   1.087137\n#&gt; orange  848           0.954009   1.037736\n#&gt; green   828           0.977053   1.016908\n\nOverall coverage (i.e. for all labels) will be at or very near 95% but coverage for individual classes may vary.\n\nAn illustration of how this method lacks Conditional Coverage\nSolution: Get thresholds for each class. (See next example)\n\nNote that the blue class had substantially fewer observations that the other 2 classes.\n\n\n\nExample: LAC-adapted - Threshold per Class\n\nDon’t think {{mapie}} has this option.\nAlso possible do this for subgroups of data, such as ensuring equal coverage for a diagnostic across racial groups, if we found coverage using a shared threshold led to problems.\nCalculate individual class thresholds\n# Set alpha (1 - coverage)\nalpha = 0.05\nthresholds = []\n# Get predicted probabilities for calibration set\ny_cal_prob = classifier.predict_proba(X_Cal)\n# Get 95th percentile score for each class's s-scores\nfor class_label in range(n_classes):\n    mask = y_cal == class_label\n    y_cal_prob_class = y_cal_prob[mask][:, class_label]\n    s_scores = 1 - y_cal_prob_class\n    q = (1 - alpha) * 100\n    class_size = mask.sum()\n    correction = (class_size + 1) / class_size\n    q *= correction\n    threshold = np.percentile(s_scores, q)\n    thresholds.append(threshold)\nApply individual class thresholds to test set scores\n# Get Si scores for test set\npredicted_proba = classifier.predict_proba(X_test)\nsi_scores = 1 - predicted_proba\n\n# For each class, check whether each instance is below the threshold\nprediction_sets = []\nfor i in range(n_classes):\n    prediction_sets.append(si_scores[:, i] &lt;= thresholds[i])\nprediction_sets = np.array(prediction_sets).T\n\n# Get prediction set labels and show first 10\nprediction_set_labels = get_prediction_set_labels(prediction_sets, class_labels)\nStatistics\n\nOverall\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n#&gt; Overall coverage: 0.95\n#&gt; Average set size: 1.093\n\nSimilar to previous example\n\nPer Class\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n#&gt;         Class counts  Coverage   Average set size\n#&gt; blue    241           0.954357   1.228216\n#&gt; orange  848           0.956368   1.139151\n#&gt; green   828           0.942029   1.006039\n\nCoverages now very close to 95% and the average set sizes have increased, especially for Blue.\n\n\n\n\n\n\nContinuous\n\nConformalized Quantile Regression Process\n\nSplit data into Training, Calibration, and Test sets\n\nTraining data: data on which the quantile regression model learns.\nCalibration data: data on which CQR calibrates the intervals.\n\nIn the example, he split the data into 3 equal sets\n\nTest data: data on which we evaluate the goodness of intervals.\n\nFit quantile regression model on training data.\nUse the model obtained at previous step to predict intervals on calibration data.\n\nPIs are predictions at the quantiles:\n\n(alpha/2)*100) (e.g 0.025, alpha = 0. 05)\n(1-(alpha/2))*100) (e.g. 0.975)\n\n\nCompute conformity scores on calibration data and intervals obtained at the previous step.\n\nResiduals are calculated for the PI vectors\nScores are calculated by taking the row-wise maximum of both (upper/lower quantile) residual vectors (e.g s_i &lt;- pmax(lower_pi_res, upper_pi_res))\n\nGet 1-alpha quantile from the distribution of conformity scores (e.g threshold &lt;- quantile(s_i, 0.95)\n\nThis score value will be the threshold\n\nUse the model obtained at step 1 to make predictions on test data.\n\nCompute PI vectors (i.e. predictions at the previously stated quantiles) on Test set\ni.e. Same calculation as with the calibration data in step 2 where you use the model to predict at upper and lower PI quantiles.\n\nCompute lower/upper end of the interval by subtracting/adding the threshold from/to the quantile predictions (aka PIs)\n\nLower conformity interval: lower_pi &lt;- test_lower_pred  - threshold\nUpper conformity interval: upper_pi &lt;- test_upper_pred + threshold\n\n\nExample: Quantile Random Forest\nimport numpy as np\nfrom skgarden import RandomForestQuantileRegressor\n\nalpha = .05\n\n# 1. Fit quantile regression model on training data\nmodel = RandomForestQuantileRegressor().fit(X_train, y_train)\n\n# 2. Make prediction on calibration data\ny_cal_interval_pred = np.column_stack([\n    model.predict(X_cal, quantile=(alpha/2)*100), \n    model.predict(X_cal, quantile=(1-alpha/2)*100)])\n\n# 3. Compute conformity scores on calibration data\ny_cal_conformity_scores = np.maximum(\n    y_cal_interval_pred[:,0] - y_cal, \n    y_cal - y_cal_interval_pred[:,1])\n\n# 4. Threshold: Get 1-alpha quantile from the distribution of conformity scores\n#    Note: this is a single number\nquantile_conformity_scores = np.quantile(\n    y_cal_conformity_scores, 1-alpha)\n\n# 5. Make prediction on test data\ny_test_interval_pred = np.column_stack([\n    model.predict(X_test, quantile=(alpha/2)*100), \n    model.predict(X_test, quantile=(1-alpha/2)*100)])\n\n# 6. Compute left (right) end of the interval by\n#    subtracting (adding) the quantile to the predictions\ny_test_interval_pred_cqr = np.column_stack([\n    y_test_interval_pred[:,0] - quantile_conformity_scores,\n    y_test_interval_pred[:,1] + quantile_conformity_scores])\n\n\n\nStatistics\n\nAverage Set Size\n\nThe average number of predicted classes per observation since there can be more than 1 predicted class in the conformal PI\nExample:\n# average set size for each class\ndef get_average_set_size(prediction_sets, y_test):\n    average_set_size = []\n    for i in range(n_classes):\n        average_set_size.append(\n            np.mean(np.sum(prediction_sets[y_test == i], axis=1)))\n    return average_set_size   \n\n# Overall average set size (weighted by class size)\n# Get class counts\ndef get_class_counts(y_test):\n    class_counts = []\n    for i in range(n_classes):\n        class_counts.append(np.sum(y_test == i))\n    return class_counts\n\ndef get_weighted_set_size(set_size, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_set_size = np.sum((set_size * class_counts) / total_counts)\n    weighted_set_size = round(weighted_set_size, 3)\n    return weighted_set_size\n\nCoverage\n\nClassification: Percentage of correct classifications\nExample: Classification\n# coverage for each class\ndef get_coverage_by_class(prediction_sets, y_test):\n    coverage = []\n    for i in range(n_classes):\n        coverage.append(np.mean(prediction_sets[y_test == i, i]))\n    return coverage\n\n# overall coverage (weighted by class size)\n# Get class counts\ndef get_class_counts(y_test):\n    class_counts = []\n    for i in range(n_classes):\n        class_counts.append(np.sum(y_test == i))\n    return class_counts\n\ndef get_weighted_coverage(coverage, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_coverage = np.sum((coverage * class_counts) / total_counts)\n    weighted_coverage = round(weighted_coverage, 3)\n    return weighted_coverage\n\n\n\n\nVisualization\n\nConfusion Matrix\n\n\nBinary target where labels are 0 and 1\nInterpretation\n\nTop-left: predictions where both labels are not statistically significant (i.e. inside the “prediction interval”).\n\nThe model predicts both classes well since both labels have low scores.\nDepending the threshold, maybe the model could be relatively agnostic (e.g. predicted probabilites like 0.50-0.50, 0.60-0.40)\n\nBottom-right: predictions where both labels are statistically significant  (i.e. outside the “prediction interval”).\n\nModel totally whiffs. Confident it’s one label when it’s actually another.\n\nExample\n\n1 (truth) - low predicted probability = high score -&gt; Red and significant\n0 - high predicted probability = high score -&gt; Red and significant\n\n\n\nTop-right: predictions where all 0 labels are not statistically significant.\n\nModel predicted the 0=class well (i.e. low scores) but the 1-class poorly (i.e. high scores)\n\nBottom-left: predictions where all 1 labels are not statistically significant. Here, the model predicted that 1 is the true class.\n\nVice versa of top-right",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/big-data.html",
    "href": "qmd/big-data.html",
    "title": "Big Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-misc",
    "href": "qmd/big-data.html#sec-bgdat-misc",
    "title": "Big Data",
    "section": "",
    "text": "RcppArmadillo::fastLmPure Not sure what this does but it’s rcpp so maybe faster than lm for big data.\n.lm.fit is a base R lm function that is 30%-40% faster than lm.",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-hghperf",
    "href": "qmd/big-data.html#sec-bgdat-hghperf",
    "title": "Big Data",
    "section": "High Performance",
    "text": "High Performance\n\n{rpolars}: arrow product; uses SIMD which is a low-level vectorization that can be used to speed up simple operations like addition, subtraction, division, and multiplication\n\nResources\n\nCookbook Polars for R\n\nAlso see collapse &gt;&gt; vs arrow/polars\nExample: Read and Summarize\ndf &lt;- pl$scan_csv(file_name)$\n    group_by(\"state\")$\n    agg(\n        pl$\n          col(\"measurement\")$\n          min()$\n          alias(\"min_m\"),\n        pl$\n          col(\"measurement\")$\n          max()$\n          alias(\"max_m\"),\n        pl$\n          col(\"measurement\")$\n          mean()$\n          alias(\"mean_m\")\n    )$\n    collect()\n\nFastest at this operation according to this benchmark\n\nExample: groupby state + min, max, mean\n# polars sql\nlf &lt;- polars::pl$LazyFrame(D) \npolars::pl$SQLContext(frame = lf)$execute(\n  \"select min(measurement) as min_m, \n          max(measurement) as max_m, \n          avg(measurement) as mean_m \n  from frame \n  group by state\")$collect()\n\n# polars\npolars::pl$\n  DataFrame(D)$\n  group_by(\"state\")$\n  agg(polars::pl$\n        col(\"measurement\")$\n        min()$alias(\"min_m\"),\n      polars::pl$\n        col(\"measurement\")$\n        max()$alias(\"max_m\"),\n      polars::pl$\n        col(\"measurement\")$\n        mean()$alias(\"mean_m\"))\n\n{collapse}: Fast grouped & weighted statistical computations, time series and panel data transformations, list-processing, data manipulation functions, summary statistics and various utilities such as support for variable labels. Class-agnostic framework designed to work with vectors, matrices, data frames, lists and related classes i.e. xts, data.table, tibble, pdata.frame, sf.\n\noptions(collapse_mask = \"all\")\nlibrary(collapse)\n\nCode chunk above can optimize any script. No other changes necessary. Quick demo.\nvs arrow/polars (benchmark)\n\nDepends on the data/groups ratio\n\nIf you have “many groups and little data in each group” then use collapse\n\nIf your calculations involve “more complex statistics algorithms like the median (involving selection) or mode or distinct value count (involving hashing)(cannot, to my knowledge, benefit from SIMD)” then use collapse.\n\nExample: groupby state + min, max, mean\nD |&gt;\n  fgroup_by(state) |&gt; \n  fsummarise(min = fmin(measurement), \n             max = fmax(measurement), \n             mean = fmean(measurement)) |&gt;\n  fungroup()\n\n{r2c}: Fast grouped statistical computation; currently limited to a few functions, sometimes faster than {collapse}\n{data.table}: Enhanced data frame class with concise data manipulation framework offering powerful aggregation, extremely flexible split-apply-combine computing, reshaping, joins, rolling statistics, set operations on tables, fast csv read/write, and various utilities such as transposition of data.\n\nExample: groupby state + min, max, mean\nD[ ,.(mean = mean(measurement),\n      min = min(measurement),\n      max = max(measurement)),\n   by=state]\n\n# Supposedly faster\nrbindlist(lapply(unique(D$state), \n                 \\(x) data.table(state = x, \n                                 y[state == x, \n                                   .(mean(measurement), \n                                     min(measurement), \n                                     max(measurement))\n                                   ]\n                                 )))\n\n{rfast}: A collection of fast (utility) functions for data analysis. Column- and row- wise means, medians, variances, minimums, maximums, many t, F and G-square tests, many regressions (normal, logistic, Poisson), are some of the many fast functions\n\nThe vast majority of the functions accept matrices only, not data.frames.\nDo not have matrices or vectors with have missing data (i.e NAs). There are no checks and C++ internally transforms them into zeros (0), so you may get wrong results.\nExample: groupby state + min, max, mean\nlev_int &lt;- as.numeric(D$state)\nminmax &lt;- Rfast::group(D$measurement, lev_int, method = \"min.max\")\ndata.frame(\n    state = levels(D$state),\n    mean = Rfast::group(D$measurement, lev_int, method = \"mean\"),\n    min = minmax[1, ],\n    max = minmax[2, ]\n)\n\n{matrixStats}: Efficient row-and column-wise (weighted) statistics on matrices and vectors, including computations on subsets of rows and columns.\n{kit}: Fast vectorized and nested switches, some parallel (row-wise) statistics, and some utilities such as efficient partial sorting and unique values.\n{fst}: A compressed data file format that is very fast to read and write. Full random access in both rows and columns allows reading subsets from a ‘.fst’ file.",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-lgmem",
    "href": "qmd/big-data.html#sec-bgdat-lgmem",
    "title": "Big Data",
    "section": "Larger than Memory",
    "text": "Larger than Memory\n\nOnly work with a sample of the data\n\nRandom sample in CLI\n\nSee binder for code\nAlso this snippet from Healy for a zipped csv.\n\n\nImproved version\ngzip -cd giantfile.csv.gz | (read HEADER; echo $HEADER; perl -ne 'print if (rand() &lt; 0.001)’) &gt; sample.csv\n\nRemoves the need to decompress the file twice, adds the header row, and removes the risk of a double header row\n\n\n\nOnly read the first n lines\n\nset n_max arg in readr::read_*\n\n\ndatasette.io - App for exploring and publishing data. It helps people take data of any shape, analyze and explore it, and publish it as an interactive website and accompanying API.\n\nWell documented, many plugins\n\nRill - A tool for effortlessly transforming data sets into powerful, opinionated dashboards using SQL.\n\nDocs, Example Projects\nPowered by Sveltekit & DuckDB = conversation-fast, not wait-ten-seconds-for-result-set fast\nWorks with your local and remote datasets – imports and exports Parquet and CSV (s3, gcs, https, local)\nNo more data analysis “side-quests” – helps you build intuition about your dataset through automatic profiling\nNo “run query” button required – responds to each keystroke by re-profiling the resulting dataset\nRadically simple interactive dashboards – thoughtful, opinionated, interactive dashboard defaults to help you quickly derive insights from your data\nDashboards as code – each step from data to dashboard has versioning, Git sharing, and easy project rehydration\n\nOnline duckdb shell for parquet files (gist, https://shell.duckdb.org/)\nselect max(wind) \nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/gridwatch/gridwatch_2023-01-08.parquet';\n-- Takes 6 seconds on the first query, 200ms on subsequent similar queries\n\nselect * \nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/NSPL/NSPL.parquet' \nwhere pcd = 'SW1A1AA';\n-- Takes 13 seconds on the first query, 100ms on subsequent similar queries\nCSV Editors\n\nFor editing or reformatting cells\nPopular spreadsheet programs like googlesheets (100MB) and excel (25MB online) have file size limits and they’re slow to upload to. The following programs are free(-ish) local alternatives only limited by your RAM.\nSuggest for files over a few hundred MBs that you open as Read-Only\n\nOpening the files as “Editable” will probably balloon the memory cost to at least 5 times the file size. (e.g. 350MB csv \\(\\rightarrow\\) 2GB RAM)\n\nModern CSV - Nice modern interface, read-only mode that can open large csvs (100s of MBs) without making much of a dent in your RAM, fully featured (moreso if you pay a small-ish one time fee)\n\nDocs, Feature free/upgrade list\nStill has some functionality in read-only mode (e.g. search, sort)\n\nOpenRefine - Has read-only, Several add-ons, Completely open source.\n\nDocs, List of Extensions\nNo functionality when read-only (must create a project to do anything) — just reading\nStarts with a 1024 MB RAM usage limit which is proably fine for editing around a 100MB csv. Need to set the limit higher in a config file in order to edit larger files.\nOnce you create a project, I think it has some editing features that you’d have to pay for with Modern CV.\nOpens other file formats besides csv (e.g. xlsx, xml, json, etc)\n\n\ncsvkit - suite of command-line tools for converting to and working with CSV\n\nInstallation docs\n\nOne of the articles your terminal has to be a bash terminal but I dunno\n\nIf so, they recommend cmder or enabling the Linux subsystem with WSL2.\n\n\nNotes from\n\nArticle with additional examples and options\n\nFeatures\n\nPrint CSV files out nicely formatted\nCut out specific columns\nGet statistical information about columns\n\nConvert excel files to CSV files:\nin2csv excel_file.xlsx &gt; new_file.csv\n# +remove .xlsx file\nin2csv excel_file.xlsx &gt; new_file.csv && rm excel_file\nSearch within columns with regular expressions:\ncsvgrep -c county -m \"HOLT\" new_file.csv\n# subset of columns (might be faster) with pretty formatting\ncsvcut -c county,total_cost new_file.csv | csvgrep -c county -m \"HOLT\" | csvlook\n\nSearches for “HOLT” in the “county” column\n\nQuery with SQL\n\nsyntax csvsql --query \"ENTER YOUR SQL QUERY HERE\" FILE_NAME.csv\nExample\n\n\nView top lines: head new_file.csv\nView columns names: csvcut -n new_file.csv\nSelect specific columns: csvcut -c county,total_cost,ship_date new_file.csv\n\nWith pretty output: csvcut -c county,total_cost,ship_date new_file.csv | csvlook\nCan also use column indexes instead of names\n\nJoin 2 files: csvjoin -c cf data1.csv data2.csv &gt; joined.csv\n\n“cf” is the common column between the 2 files\n\nEDA-type stats:\ncsvstat new_file.csv\n# subset of columns\ncsvcut -c total_cost,ship_date new_file.csv | csvstat\n\nJSONata - a lightweight, open-source query and transformation language for JSON data, inspired by the ‘location path’ semantics of XPath 3.1.\n\nMisc\n\nNotes from: Hrbrmstr’s article\nJSONata also doesn’t throw errors for non-existing data in the input document. If during the navigation of the location path, a field is not found, then the expression returns nothing.\n\nThis can be beneficial in certain scenarios where the structure of the input JSON can vary and doesn’t always contain the same fields.\n\nTreats single values and arrays containing a single value as equivalent\nBoth JSONata and jq can work in the browser (JSONata embedding code, demo), but jq has a slight speed edge thanks to WASM. However, said edge comes at the cost of a slow-first-start\n\nFeatures\n\nDeclarative syntax that is pretty easy to read and write, which allows us to focus on the desired output rather than the procedural steps required to achieve it\nBuilt-in operators and functions for manipulating and combining data, making it easier to perform complex transformations without writing custom code in a traditional programming language like python or javascript\nUser-defined functions that let us extend JSONata’s capabilities and tailor it to our specific needs\nFlexible output structure that lets us format query results into pretty much any output type\n\n\njq + jsonlite - json files\njsoncrack.com - online editor/tool to visualize nested json (or regular json)\njj - cli tool for nested json. Full support for ndjson as well as setting/updating/deleting values. Plus it lets you perform similar pretty/ugly printing that jq does.\nsqlite3 - CLI utility allows the user to manually enter and execute SQL statements against an SQLite database or against a ZIP archive.\n\nalso directly against csv files (post)\n\ntextql - Execute SQL against structured text like CSV or TSV\n\nRequire Go language installed\nOnly for Macs or running a docker image\n\ncolumnq-cli - sql query json, csv, parquet, arrow, and more\nfread + CLI tools\n\nFor large csvs and fixing large csv with jacked-up formating see article, RBlogger version\n\n{arrow}\n\nconvert file into parquet files\n\npass the file path to open_dataset, use group_by to partition the Dataset into manageable chunks\nuse write_datasetto write each chunk to a separate Parquet file—all without needing to read the full CSV file into R\n\ndplyr support\n\nmultiplyr\n\nOption for data &gt; 10M rows and you only have access to one machine\nSpreads data over local cores\n\n{sparklyr}\n\nspin up a spark cluster\ndplyr support\nSet-up a cloud bucket and load data into it. Then, read into a local spark cluster. Process data.\n\n{h2o}\n\nh2o.import_file(path=path) holds data in the h2o cluster and not in memory\n\n{disk.frame}\n\nsupports many dplyr verbs\nsupports  future package to take advantage of multi-core CPUs but single machine focused\nstate-of-the-art data storage techniques such as fast data compression, and random access to rows and columns provided by the fst package to provide superior data manipulation speeds\n\nMatrix ops\n\nsee bkmks: mathematics &gt;&gt; packages\n\n{ff}\n\nsee bkmks: data &gt;&gt; loading/saving/memory\nThink it converts files to a ff file type, then you load them and use ffapply to perform row and column operations with base R functions and expressions\nmay not handle character and factor types but may work with {bit} pkg to solve this",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-viz",
    "href": "qmd/big-data.html#sec-bgdat-viz",
    "title": "Big Data",
    "section": "Viz",
    "text": "Viz\n\nScatter plots\n\n{scattermore}, {ggpointdensity}\n{ggrastr}\n\nRasterize only specific layers of a ggplot2 plot (for instance, large scatter plots with many points) while keeping all labels and text in vector format. This allows users to keep plots within a reasonable size limit without losing the vector properties of scale-sensitive information.\ngithub; tweet\n\n\nH2O\n\nh2o.aggregator Reduces data size to a representive sample, then you can visualize a clustering-based method for reducing a numerical/categorical dataset into a dataset with fewer rows A count column is added to show how many rows is represented by the exemplar row (I think)\n\nAggregator maintains outliers as outliers but lumps together dense clusters into exemplars with an attached count column showing the member points.\nFor cat vars:\n\nAccumulate the category frequencies.\nFor the top 1,000 or fewer categories (by frequency), generate dummy variables (called one-hot encoding by ML people, called dummy coding by statisticians).\nCalculate the first eigenvector of the covariance matrix of these dummy variables.\nReplace the row values on the categorical column with the value from the eigenvector corresponding to the dummy values.\n\ndocs; article\n\n\n{dbplot}\n\nplots data that are in databases\n\nAlso able to plot data within a spark cluster\n\ndocs\n\nObservableHQ\n\n{{{deepscatter}}}\n\nThread (using Arrow, duckdb)",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/bayes-workflow.html",
    "href": "qmd/bayes-workflow.html",
    "title": "Workflow",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Bayes",
      "Workflow"
    ]
  },
  {
    "objectID": "qmd/bayes-workflow.html#sec-bayes-wkflw-misc",
    "href": "qmd/bayes-workflow.html#sec-bayes-wkflw-misc",
    "title": "Workflow",
    "section": "",
    "text": "Also see Model Building, Concepts &gt;&gt; Misc &gt;&gt; Regression Workflow\nNotes from\n\nBayesian Workflow (Gelman, Vehtari) (arXiv link)\n\nResources\n\nVehtari Video: On Bayesian Workflow (2022) (Based on the paper, but I haven’t watched it, yet)\nNabiximols treatment efficiency: Vehtari’s example of applyijng his workflow in the context of comparing continuous and discrete observation models.\n\nCurrent Checklist\n\nCheck convergence diagnostics\nDo posterior predictive checking\nCheck residual plots\nModel comparison (if prediction)\n\nAnalysis Checklist (Thread)\n\nA suitably flexible Bayesian regression adjustment model,\nChosen by cross-validation/LOO,\nIncluding Gaussian processes for the unit-level effects over time (and space/network if relevant),\nImputation of missing data, and\nInformative priors for biases in the data collection process.",
    "crumbs": [
      "Bayes",
      "Workflow"
    ]
  },
  {
    "objectID": "qmd/glossary-ds-terms.html",
    "href": "qmd/glossary-ds-terms.html",
    "title": "Glossary: DS terms",
    "section": "",
    "text": "200 Status - An API serving an ML model returns a HTTP 200 OK success status response code indicates that the request has succeeded.\nAMI - amazon machine image. Thing that has R and the main packages you need to load onto the cloud server\nAnti-Patterns - certain patterns in software development that are considered bad programming practices.\n\nAs opposed to design patterns which are common approaches to common problems which have been formalized and are generally considered a good development practice, anti-patterns are the opposite and are undesirable.\n\nArm - a group of patients receiving a specific treatment (or no treatment). Trials involving several arms, or randomized trials, treat randomly-selected groups of patients with different therapies in order to compare their medical outcomes. Experimental arms, which receive an experimental drug, are compared with control arms. Single-arm or non-randomized trials, in which everyone enrolled in a trial receives the experimental therapy\nArtifacts - objects that are created as a result of a process. e.g. model objects, cleaned data sets, visuals, etc.\nAsynchronous Programming - code runs (or must run) after something else happens and also not sequentially (e.g. when a function calls a callback function in JS).\nAthena - amazon query service that works with S3. Best for analyses using kubernetes. ODBC drivers are best with interactive app\nB2C, B2B - business-to-consumer, business-to-business, describes a business that’s end-product is being sold to a consumer or a business.\nBalanced Design (aka orthogonal) has an equal number of observations for all possible level combinations. For example in an experiment where gender is an independent variable, an equal number of males receive the treatment as do females receive treatment. If the male/female counts were unequal, then the experiment is unbalanced.\n\nStat tests have greater power for balanced designs\nTest stat less susceptible to to small departures from the assumption of equal variances (homoscedasticity).\n\nBatch - collect a large number of data points, process them periodically and store results somewhere (contrasts with real-time in which a data input leads to an immediate prediction)\nBootstrapping (CS) - usually applies to a situation where a system depends on itself to start, sort of a chicken and egg problem. (e.g. How do you start an OS initialization process if you don’t have the OS running yet?) Typically a simple file that starts a large process.\nBounce, Email - When an email cannot be delivered to an email server.\n\nHard Bounce - indicates a permanent reason an email cannot be delivered (e.g. Recipient email address doesn’t exist; Recipient email server has completely blocked delivery)\nSoft Bounce - indicates a temporary delivery issue (for details on the reasons, see link)\n\nBounce Rate - the percentage of visitors to a particular website who navigate away from the site after viewing only one page. Low bounce rate can indicate the landing page needs improvement\nBPI - Business process improvement is a management exercise in which enterprise leaders use various methodologies to analyze their procedures to identify areas where they can improve accuracy, effectiveness and/or efficiency and then redesign those processes to realize the improvements.\nBLUE - best linear unbiased estimator, e.g. regression line\nCAC - customer acquisition cost - measures how much an organization spends to acquire new customers. The total cost of sales and marketing efforts, as well as property or equipment, needed to convince a customer to buy a product or service.\nCapEx - Capital Expenditure - 1 of 2 main forward budgeting mechanisms for a corporation (also see OpEx). Often used to undertake new projects or investments or large-scale asset acquisitions (buildings and vehicles)\nClinical Trial - research studies (e.g. RCT) performed in people that are aimed at evaluating a medical, surgical, or behavioral intervention\nCDI - Customer Data Infrastructure - built to collect behavioral data from primary or first-party data sources, but some solutions also support a handful of secondary data sources (third-party tools)\nCDP - Customer Data Platform - add-ons from CDI vendors; a layer on top of CDI that offers a set of capabilities to analyze data using a visual interface.\nCDN - content delivery network - a system of distributed servers (network) that deliver pages and other web content to a user, based on the geographic locations of the user, the origin of the webpage and the content delivery server.\nCLV/CLTV - Customer Lifetime Value - how much money a customer will bring your brand throughout their entire time as a paying customer.\nCOGS - Cost of goods sold (aka Cost of Sales) - refers to the direct costs of producing the goods sold by a company. This amount includes the cost of the materials and labor directly used to create the good. It excludes indirect expenses, such as distribution costs and sales force costs.\nComplete Factorial Design - a research study involving two or more independent variables in which every possible combination of the levels of each variable is represented. For instance, in a study of two drug treatments, one (A) having two dosages and the other (B) having three dosages, a complete factorial design would pair the dosages administered to different individuals or groups of participants as follows: A1 with B1, A1 with B2, A1 with B3, A2 with B1, A2 with B2, and A2 with B3.\nCPG - Consumer packaged goods are items used daily by average consumers that require routine replacement or replenishment, such as food, beverages, clothes, tobacco, makeup, and household products.\nCPC - Cost Per Click - refers to the cost an advertiser pays each time an online user clicks on his or her digital ad\nCRM - customer relationship management i.e. customer service. Salesforce tracks this data. Example: what features your salesperson promised, and when? How much revenue you have from each customer? Or which salesperson sold the most in the past year?\ncron- standard tool used on Unix and Unix-like systems to schedule the periodic execution in the background of a command or script (like a batch script)\nCrossed Factors - when every category of one factor co-occurs in the design with every category of the other factor. In other words, there is at least one observation in every combination of categories for the two factors. (in contrast to “nested factors”). As a consequence, interaction terms involving these two factors is allowed.\nCrossover Study - A type of clinical trial in which the study participants receive each treatment in a random order. With this type of study, every patient serves as his or her own control. Crossover studies are often used when researchers feel it would be difficult to recruit participants willing to risk going without a promising new treatment.\nCross-Section Data - randomly sampled data from a population. Like a survey. Aka observational data. See experimental data for comparison.\n\nPooled - differs from panel data in that it is observations of different subjects (instead of the same subjects) in different time periods.\nRolling - both the presence of an individual in the sample and the time at which the individual is included in the sample are determined randomly.\n\nCross-Tabs - section of survey analysis where the aggregated results are broken down by demography, party affiliation, etc.\nCTA - marketing term, call-to-action. any device designed to prompt an immediate response or encourage an immediate sale; words or phrases that can be incorporated into sales scripts, advertising messages or web pages that encourage consumers to take prompt action\nCTR - click through rate: the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement. It is commonly used to measure the success of an online advertising campaign for a particular website as well as the effectiveness of email campaigns.\nCRM - Customer Relationship Management - acquiring new customers but especially about retaining existing ones\nDAU - daily active users, ex: daily avg # of registered users of the site over past 30 days\nDBA - Database Administrator is an admin role that understands the particular database technology and how to get the best out of it. This includes improving performance, backups and recovery.\nDDL - Data definition or description language - Subset of SQL. Used to:\n\nKeep a snapshot of the database structure\nSet up a test system where the database acts like the production system but contains no data\nProduce templates for new objects that you can create based on existing ones. For example, generate the DDL for the Customer table, then edit the DDL to create the table Customer_New with the same schema.\n\nDesparate Impact Analysis - Analysis of the result of the application of a standard, requirement, test or other screening tool used for selection that—though appearing neutral—has an adverse effect on individuals who belong to a legally protected class Differential Dropout**]{style=‘color: #009499’} - Differing dropout rates between treatment arms\nDMA - Designated Market Area; a geographic region where Nielsen, the ratings company, analyzes and quantifies how television is viewed. Residents can receive the same local TV and radio stations\nDNS -  Domain Name System**]{style=‘color: #009499’} -  translates domain names to IP addresses so browsers can load Internet resources.\nDSL - domain-specific language - a computer language specialized to a particular application domain\nEMR - Amazon version of a spark cluster used for big data processing and analysis.\nEndogenous - A model variable is correlated with other variables excluded from the model (omitted variable bias). Determined by measuring the correlation between the variable and residuals of the model. If a predictor variable hasn’t been randomly assigned, it’s likely to be endogenous.\nEquitability - concept that says a dependence measure should give equal importance to linear and nonlinear relationships. Consistent strength measurements across different variable relationships that have similar amounts of noise.\nERP - enterprise resource planning, sort of a catch-all for manufacturing, supply-chain, etc, see the wiki\nETL - extract, transfer, load - usually refers to transferring data from one location to another\nEndpoint (biostats) - Outcome variable measured in a medical study. e.g. Death, stroke, or quality of life are good endpoints. Blood tests and images on scans are not good endpoints.\n\nA composite endpoint is one that consists of two or more events\n\nExample: death due to cardiovascular causes or hospitalization due to heart failure\n\nSo the binary outcome would be a 1 if either of those events took place or a 0 if they did not. Or in a survival model, time until either of those events.\n\n\n\nEOF - End of file - Input from a terminal never really “ends” (unless the device is disconnected), but it is useful to enter more than one “file” into a terminal, so a key sequence is reserved to indicate end of input.\nex ante - based on assumption and prediction and being essentially subjective and estimative\nex post - based on knowledge and retrospection and being essentially objective and factual\nExperimental Data - data from a RCE/RCT. Compare with observational data\nFaaS - Function as a service - type of cloud service for developing, running, and managing apps (e.g. AWS Lambda)\nFactorial Design - Experiment where you’re interested in the effect of two or more independent variables.\nFraud Rules - fraud scores are calculated based on rules, which add or subtract points. The user action may be a transaction, signup or login. Rules look at data points such as an email address, IP address, or social media presence.\nFraud Score - assigned values to how risky a user action is. Scoring determined by fraud rules.\nFuzzy Design - See Sharp Design\nGHA - Github Actions\nGMV - Gross merchandises value - the total value of merchandise sold over a given period of time through a customer-to-customer (C2C) exchange site\nGRP - Gross Rating Point. A standard measure in advertising, it measures advertising impact. You calculate it as a percent of the target market reached multiplied by the exposure frequency. Thus, if you get advertise to 30% of the target market and give them 4 exposures, you would have 120 GRP.\nHTE - Heterogeneous Treatment Effect - Also called differential treatment effect, includes difference of means, odds ratios, and Hazard ratios for time-to-event outcome vars\n\nAscertaining subpopulations for which a treatment is most beneficial (or harmful) is an important goal of many clinical trials.\nOutcome heterogeneity is due to wide distributions of baseline prognostic factors. When strong risk factors exist, there is hetergeneity in the outcome variable.\n\nSolution: add baseline predictors to your model that account for these strong risk factors.\n\nHeterogeneity of Treatment Effects - The degree to which different treatments have differential causal effects on each unit.\n\nHit Ratio - percent of records that were read in order to complete a query in a database. Cloud db providers often charge by the number of records searched\nHomogeneity of Treatment Effects - See Heterogeneity of Treatment Effects\nHPC - High Performance Computing\nHoneypot - data (for example, in a network site) that appears to be a legitimate part of the site, but is actually isolated and monitored, and that seems to contain information or a resource of value to attackers, who are then blocked.\nIaaS - infrastructure-as-a-service ( Hardware is provided by an external provider and managed for you)\nIAM - identity and access management, keys and passwords etc\nIRB - institutional review board, reviews studies ethical and moral issues\nITT - Intent-to-Treat analysis includes all randomized patients in the groups to which they were randomly assigned, regardless of their adherence with the entry criteria, regardless of the treatment they actually received, and regardless of subsequent withdrawal from treatment or deviation from the protocol. Avoids overoptimistic estimates of the efficacy of an intervention resulting from the removal of non-compliers by accepting that noncompliance and protocol deviations are likely to occur in actual clinical practice. So mimics likely situation in the real world, but not good for estimating the causal effect of a treatment.\nKernels - (article) - system kernels - the interface between the operating system, i.e. the software, and the hardware components in a device. It is used in all devices with an operating system, for example, computers, laptops, smartphones, smartwatches, etc.\n\nWhen we use a program on a computer, such as Excel, we handle it on the so-called Graphical User Interface (GUI). The program converts every button click or other action into machine code and sends it to the operating system kernel. If we want to add a new column in an Excel table, this call goes to the system core. This in turn passes the call on to the computer processing unit (CPU), which executes the action.\nJupyter Kernels - an engine that executes notebook code and is specific to a particular programming language (e.g. python kernel)\nKaggle Kernels - a free platform from Kaggle to run Jupyter notebooks in the browser. Advantage is that you don’t have to set-up an environment locally.\n\nKPI- key performance indicator\nKYC - Know-Your-Customer is info a company collects to verify your identity to combat fraud. Used by telecoms and financial services\nLazy Evaluation - ” never pulls data into R unless you explicitly ask for it. It delays doing any work until the last possible moment. It collects together everything you want to do and then sends it to the database in one step.”\nLikelihood - probability of seeing this data given a specific value for a distribution parameter (eg mean, sd). Goal is to search for parameter values until the likelihood is maximized.\nLOB - Line of Business is a general term which refers to a product or a set of related products that serve a particular customer transaction or business need. (i.e. product categories)\n\nExamples\n\nConsumer Banking: credit cards, line of credit or loan program, mortgages, and corporate, small business and personal bank accounts.\nFinancial services and brokerages: mergers and acquisitions or partnerships, real estate investments, and wealth management\nProperty and casualty insurance companies: property and casualty insurance (i.e., homeowners, car, boat, renters, etc.), life insurance, health insurance, and commercial business insurance.\n\nSub-lines of Business would be sub-categories within each LOB\n\nLongitudinal Data - see panel data\nLTV - see CLV/CLTV\nManual Review - A human is reviews the case to determine whether action is needed. In fraud, an model output may trigger a “manual review” to determine whether an event was indeed fraudulent.\nMLlib - Apache Spark machine learning library\nMVC - Minimum Viable Corpus - a data size threshold; such that below this threshold, the data simply isn’t useful/valuable. Used in data products business.\nMVP - minimum viable project, agile term. Version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort\nNamespace - allows you to use two functions with the same name but from different packages, e.g. dplyr::select or in general, package::function. https://stackoverflow.com/questions/3384204/what-are-namespaces/3384384#3384384\nNNH - Numbers Needed to Harm - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a particular adverse outcome. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNNT - Numbers Needed to Treat - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a favorable outcome such as treatment response. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNPS - Net Promoter Score - a measure of customer loyalty. Widely used market research metric that typically takes the form of a single survey question asking respondents to rate the likelihood that they would recommend a company, product, or a service to a friend or colleague.\nNRT - near real-time, aka streaming data\nObservational Data - see cross sectional data\nOEM - original equipment manufacturer\nOKR - Objectives and Key Results is a popular management strategy for goal setting within organizations. A framework for turning strategic intent into measurable outcomes for an organization.\nOnline Machine Learning - A method of machine learning where the model incrementally learns from a stream of data points in real-time. It’s a dynamic process that adapts its predictive algorithm over time, allowing the model to change as new data arrives.\nOn-Prem - on-premises — working with servers in the the building and not in the cloud.\nOOD - out-of-distribution - data which differ from the training data and on which a model might underperform\nOpen Cohort - subjects can leave or be added over time.\nOpEx - Operational Expenditures - 1 of 2 main forward budgeting mechanisms for a corporation (also see CapEx). Relates to day-to-day expenses (such as payroll and software subscriptions). Smaller payouts over time.\nOpportunity Sizing - Quantitative analysis to select a subset of ideas to which to devote resources in product development\nNested Factors - happens when all the levels of one factor only occur in combination with one level of another factor (in contrast to “crossed factors”). As a consequence, your model can’t have an interaction term involving these two variables.\nP&L - Profit and Loss Statement Panel data - cross section data with a time element. Repeated measures of the same subject over time. Synonym for Longitudinal Data\nParcel - a land record that defines the boundary of a piece of land. These boundaries are the basic administrative unit of local government in regards to land and property. Managing ownership and tax records are the primary reason local governments generate these files. So these are boundaries differentiating ownership of properties.\nPEP8 - style guide for python\nPI - principal investigator\nPivot Table - Excel name for a group_by %\\&gt;% summarize calculation\n\ne.g. from a table of individual fruit sales: group_by(fruit_type, country) %\\&gt;% summarize(total_amt = sum(amount))\n\nPLG - Product-led growth is an end user-focused growth model that relies on the product itself as the primary driver of customer acquisition, conversion, and expansion. e.g. open source a product, let the customer go through the documentation and use and experiment with the product on their own time. In contrast to sales pitching a product to a customer and letting them use it for a trial basis.\nPM - product manager\nPoC - Proof of Concept\nPOS - point of sale, The point of sale or point of purchase is the time and place where a retail transaction is completed. It can be in a physical store, where POS terminals and systems are used to process card payments or a virtual sales point such as a computer or mobile electronic device.\nRCE - randomized controlled experiment, subjects randomly assigned to two groups, treatment and control. Double blind means the researcher doesn’t know who is in which group.\nRCT - randomized clinical trial\nRDD - Regression discontinuity design\nRedis - REmote DIctionary Server - is an in-memory, key-value database, commonly referred to as a data structure server. Used when volume of read and write operations exceed the capabilities of traditional databases. With Redis’s capability to easily persist the data to disk, it is a superior alternative to the traditional memcached solution for caching.\nRefactoring - updating or optimizing code\nRegression Testing - checks if changes made to a system negatively impacted or broke any of the existing features. It is often performed right after each update or commit to the code base to identify new bugs and ensure that your system works properly.\nRFI - Request for Information - Used to collect written information about the capabilities of various suppliers. Normally it follows a format that can be used for comparative purposes. An RFI is primarily used to gather information to help make a decision on what steps to take next. RFIs are therefore seldom the final stage and are instead often used in combination with request for proposal (RFP), request for tender (RFT), and request for quotation (RFQ).\nRFM - recency, frequency, monetary value - method of estimating customer value; common in retail\nRFP - Request for Proposal - A document that an organization, often a government agency or large enterprise, posts to elicit a response – a formal bid – from potential vendors for a desired solution. The RFP specifies what the customer is looking for and describes each evaluation criterion on which a vendor’s proposal will be assessed.\n\nROAS - return on ad spend\nRUG - Regional User Group\nS3 - Amazon simple storage service, database\nSaaS - Software-as-a-service is a mechanism through which companies offer the functionality of their apps, which remain on their company servers, to other companies or customers.\nSCO - sales cycle optimization, active process of providing content on your site (and beyond) that speaks to each of the key phases\nSEO - Search engine optimization, generating high page rankings for key search terms\nSDK - software development kit\nSharp Design - Each individual or group receives the same “amount” of treatment (e.g. a state law or medication dosage). Opposite being fuzzy design (?)\nSKU - Stock Keeping Unit**]{style=‘color: #009499’} - Usually a bar code that has all the information to distinguish it from another product. These attributes can include manufacturer, description, material, size, color, packaging, and warranty terms. When a business takes inventory of its stock, it counts the quantity it has of each SKU.\nSLA - service level agreement - a contract between a service provider and its internal or external customers that documents what services the provider will furnish and defines the service standards this provider is obligated to meet. service. Important for holding prediction latency of an app to a certain standard or maintaining data reliability with vendors. (see link for more details on SLA, SLO, and SLI)\nSLI - service level indicators - metrics that measure compliance with an SLO (see link for more details on SLA, SLO, and SLI)\nSLO - service level objectives - objectives your team must meet in order to meet the conditions of the SLA (see link for more details on SLA, SLO, and SLI)\nSMB - (small to medium-sized business) generally defined as companies with fewer than 1000 employees and less than $1 billion in annual revenue.\nSME - Subject Matter Experts\nSPC - Statistical process control is a method of quality control which employs statistical methods to monitor and control a process\nSpill - missed opportunity metric, measures “lost trading days” on which flights or hotels filled too quickly (the result of pricing too low)\nSpoil - missed opportunity metric, measures empty seats or rooms (often the result of pricing too high)\nSSH - secure shell is a cryptographic Network protocol for operating Network Services securely over an unsecured Network. Typical applications include remote command line login in remote command execution\nstdout - standard output, which is the terminal by default\nTDD - Test-driven development is a style of programming where coding, testing, and design are tightly interwoven\nTF-IDF- stands for term frequency-inverse document frequency, and is often used in information retrieval and text mining.\nThroughput - the amount of material or items passing through a system or process.\ntx - treatment, seen as variable with different treatments as values\nURI - Uniform Resource Identifier - a string of characters that unambiguously identifies a particular resource. e.g. s3//bucket/path/to/folder or http://127.0.0.1:5000or c:\\Users\\me\\path\\to\\folder\nUTM - Urchin Traffic Monitor - used to identify marketing channels\n\ne.g. http://yourwebsite.com/your-post-title/?utm_source=google\n\nutm code = string after “?”\n\nThis person clicked a google ad to get to your site\n\nName comes from Urchin Tracker, a web analytics software that served as the base for Google Analytics.\n\nVPS - virtual private server\nWIP - Work-in-Progress\nWithin Person Study - multiple treatments on each person either all in the same period or different treatments in different periods\nYear-Over-Year - used to make comparisons between one time period and another that is one year earlier.\n\nFormula (percentage): (value_this_year / value_previous_year) - 1\nExample: (sales_Jul_2023 / sales_Jul_2022) - 1",
    "crumbs": [
      "Glossary: DS terms"
    ]
  },
  {
    "objectID": "qmd/python-general.html",
    "href": "qmd/python-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-misc",
    "href": "qmd/python-general.html#sec-py-gen-misc",
    "title": "General",
    "section": "",
    "text": "Tools (see article, article for installation and usage)\n\nruff - rust-based, linter and sorts imports\n\nFast, sensible default settings, focuses on more important things out of the box, and has less legacy burden\n\npydocstring - tool for checking compliance with Python docstring conventions\nblack - code formatter\nisort - sorts your imports\npytest, pytest-watch - unit tests\ncommitizen - guides you through a series of steps to create a commit message that conforms to the structure of a Conventional Commit\nnbQA - linting in jupyter notebooks\nmypy - type checker; good support and docs\npylance - checks type hinting in VSCode (see Functions &gt;&gt; Documentation &gt;&gt; Type Hinting)\ndoit - task runner; {targets}-like tool; tutorial\npre-commit - specify which checks you want to run against your code before committing changes to your git repository\nREADME templates - link\n\nPut as much config as possible into pyproject.toml. A lot of configurations tools will happily read from it, and it will give you one source of truth.\nAn underscore _ at the beginning is used to denote private variables in Python.\ndef set_temperature(self, value):\n        if value &lt; -273.15:\n            raise ValueError(\"Temperature below -273.15 is not possible.\")\n        self._temperature = value\n\nyou can still access “_temperature” but it’s just meant for internal use by the class and the underscore indicates this\n\n{{warnings::warnings.filterwarnings(‘ignore’)}}\nsys.getsizeof(obj) to get the size of an object in memory.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#terms",
    "href": "qmd/python-general.html#terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nclasses - code template for creating objects, we can think of it as a blueprint. It describes the possible states and behaviors that every object of a certain type could have.\nobject - data structure storing information about the state and behavior of a certain entity and is an instance of a class\nstub file - a file containing a skeleton of the public interface of that Python module, including classes, variables, functions – and most importantly, their types. (Source)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#base",
    "href": "qmd/python-general.html#base",
    "title": "General",
    "section": "Base",
    "text": "Base\n\nInfo method\n\nX.info()\nRemove an object: del\nCheck object type\n\ntype() : outputs the type of an object\nisinstance() : outputs type and inheritance of an object\nSee article for details on differences\n\nImport Libraries\nimport logging\nimport bentoml\nfrom transformers import (\n    SpeechT5Processor,\n    SpeechT5ForTextToSpeech,\n    SpeechT5HifiGan,\n    WhisperForConditionalGeneration,\n    WhisperProcessor,\n)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-fund",
    "href": "qmd/python-general.html#sec-py-gen-fund",
    "title": "General",
    "section": "Fundamentals",
    "text": "Fundamentals\n\nSlicing\n\nFormat my_list[start:stop:step]\n\n** start value is inclusive and the end value is exclusive\n\n1 element and more than 1 element\n\"Python\"[0] # P\n\"Python\"[0:1] # P\n\"Python\"[0:5] # Pytho\nillustrates how when using a range, the last element is exclusive\nNegative indexing my_list[0:-1]\n\nEverything but the last object\n\nSkip every second element\nmy_list = list(\"Python\")\nmy_list[0:len(my_list):2]\n&gt;&gt; ['P', 't', 'o']\nstart at 0, end at len(my_list), step = 2\nShortcuts\nmy_list[0:-1] == my_list[:-1]\nmy_list[0:len(my_list):2] == my_list[::2]\n\"Python\"[::-1] == \"Python\"[-1:-7:-1]\n\nDefaults\n\n0 for the start value\nlen(list) for the stop value\n1 for the step value\n\nDefaults for negative step value\n\n-1 for the start value\n-len(list) - 1 for the stop value\n\n\nAlias vs new object\nb = a # alias\nb = a[:] # new object\n\nWith the alias, changes to a will happen to b as well\n\nCommon use cases\n\n\n\n\n\n\n\nEvery element but the first and the last one\n[1:-1]\n\n\nEvery element in reverse order\n[::-1]\n\n\nEvery element but the first and the last one in reverse order\n[-2:0:-1]\n\n\nEvery second element but the first and the last one in reverse order\n[-2:0:-2]\n\n\n\nUsing slice function\nsequence = list(\"Python\")\nmy_slice = slice(None, None, 2) # equivalent to [::2]\nindices = my_slice.indices(len(sequence))\n&gt;&gt; (0, 6, 2)\n\nShows start = 0, stop = 6, step = 2\n\n\n\n\nF-Strings\n\nCheatsheet\nParameterize with {}\n&gt;&gt; x = 5\n&gt;&gt; f\"One icecream is worth [{x}]{style='color: #990000'} dollars\"\n'One icecream is worth 5 dollars'\n! - functions\n\n!r — Shows the string delimiter, calls the repr() method.\n\nrepr’s goal is to be unambiguous and str’s is to be readable. For example, if we suspect a float has a small rounding error, repr will show us while str may not\n\n!a — Shows the Ascii for the characters.\n!s — Converts the value to a string.\n\nGuessing this the str() method (see !r for details)\n\n\nfood2brand = \"Mcdonalds\"\nfood2 = \"French fries\"\nf\"I like eating {food2brand} {food2!r}\"\n\"I like eating Mcdonalds 'French fries'\"\nChange format with “:”\n&gt;&gt; import datetime\n&gt;&gt; date = datetime.datetime.utcnow()\n&gt;&gt; f\"The date is {date:%m-%Y %d}\"\n'The date is 02-2022 15'\nFormatting with “&gt;” and “&lt;”\n\n\n&lt;6 says width is 6 characters and text starts at the left edge\n&gt;10.2f says width is 10 characters, text starts the right hand edge, and number is rounded to 2 decimal places\n\n\n\n\nOperators\n\n(docs)\nExponential: 5**3\nInteger division: 5//3\nModulo: 5%3\nIdentity: is\nx = 5\ny = 3\nprint(\"The result for x is y is\", x is y)\nThe result for x is y is false\n\nThink you can also use == here too\n\nLogical: and and or\nprint(\"The result for 5 &gt; 3 and 6 &gt; 8 is\", 5 &gt; 3 and 6 &gt; 8)\nprint(\"The result for 5 &gt; 3 or 6 &gt; 8 is\", 5 &gt; 3 or 6 &gt; 8)\nThe result for 5 &gt; 3 and 6 &gt; 8 is False\nThe result for 5 &gt; 3 or 6 &gt; 8 is True\nSubset: in and not in\nprint(\"Is the number 3 in the list [1,2,3]?\", 3 in [1,2,3])\nIs the number 3 in the list [1,2,3]? True\n\nprint(\"Is the number 3 not in the list [1,2,3]?\", 3 not in [1,2,3])\nIs the number 3 not in the list [1,2,3]? False\nAssignment",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-dattyp",
    "href": "qmd/python-general.html#sec-py-gen-dattyp",
    "title": "General",
    "section": "Types",
    "text": "Types\n\nScalars\n\nCreate scalars by subsetting a list\ninputs = [1, 0.04, 0.9]\n# 1 numeric \nrmse = inputs[0] # rmse = 1 and is type 'float'\n# multiple numerics\nrmse, mape, rsq = inputs\n\nTuples\n\nLists are mutable and tuples are not\n\ni.e. we can add or remove elements to a list after we create it but we cannot do such thing to a tuple\n\nSyntax: name_of_tuple = (a, b)\n\nLists\n\nCreate list of objects (e.g. floats)\nacc_values = [rmse, mape, rsq]\n\nalt method: asterisk-comma notation\n*acc_names, = \"RMSE\", \"MAPE\", \"R-SQ\"\n\nasterisk is “unzipping operator”\n\n\nMake a copy\nold_list = [2, 3, 4]\nnew_list = list(old_list)\n\nDictionaries\n\n** if creating a simple dict, more performant to use curly braces **\n\nAvoid d = dict(1=1, x='x')\n\nJoin 2 dicts -  d.update(d2)\n\nIf d and d2 share keys, d2’s values for those keys will be used\n\nAccess a value from a key: sample_dict['key_name']\nMake a copy\nold_dict = {stuff: 2, more_stuff: 3}\nnew_dict = dict(old_dict)\nConvert list of tuples to a dict\nacc_dict = dict(acc_list)\n\nzip creates lists of tuples (See Loops &gt;&gt; zip section)\n\nAdd key, value pair to a dict\ntransaction_data['user_address'] = '221b Baker Street, London - UK'\n# or\ntransaction_data.update(user_address='221b Baker Street, London - UK')\nUnpack dict into separate tuples for key:value pairs\nrmse, mape, rsq = acc_dict.items()\nrmse\n('RMSE', 1)\n\n** fastest way to iterate over both keys and values in a dict **\ncan also use zip to unpack pairs into a list (see loops &gt;&gt; zip)\n\nUnpack dict into separate lists for keys and values\nacc_keys = list(acc_dict.keys()) \nacc_values = list(acc_dict.values())\n\n** fastest way to iterate over a dict’s keys or values **\n\nUnpack values from dicts into separate scalars\nrmse, mape, rsq = acc_dict.values()\nrmse\n1\nPull the value for a key (e.g. k) or return the default value - d.get(k, default)\n\nDefault is “None”. I think this can be set with d.setdefault(k, default)\n\nCheck for specific key (logical)\n‘send_currency’ in transaction_data\n‘send_currency’ in transaction_data.keys()\n‘send_currency’ not in transaction_data.keys()\n\nLike %in% in R\n\nCheck for specific value (logical)\n‘GBP’ in transaction_data.values()\nCheck for key, value pair\n(‘send_currency’, ‘GBP’) in transaction_data.items()\nPretty printing of dictionaries\n    _ = [print(k, \":\", f'{v:.1f}') for k,v in acc_dict.items()]\n    RMSE : 1.00\n    MAPE : 0.04\n    R-sq : 0.90\n\nfor-in loop format (see Loops &gt;&gt; Comprehension)\nprint returns “none” for each key:value at the bottom of the output for some reason. Assigning the print statement to a variable fixes it.\n\ndefaultdict\n\nCreates a key from a list element and groups the properties into a list of values where the value may also be a dict.\nFrom {{collections}}\nAlso see\n\nPybites video\nJSON &gt;&gt; Python &gt;&gt; Example: Parse Nested JSON into a dataframe\n\n\n\nSets\n\nIf performing set logic, always more performant to use sets instead of dicts or lists\n\n\nIf using numpy/pandas, using the .unique() syntax is more efficient for arrays/series’ with numeric values\nIf using strings, it’s more efficient to use list(set(my_array))\n\n\nStrings\n\nOperators\nOperator Description\n%d Signed decimal integer\n%u unsigned decimal integer\n%c Character\n%s String\n%f Floating-point real number\nExample\n\nname = \"india\"\nage = 19\nmarks = 20.56\nstring1 = 'Hey %s' % (name)\nprint(string1)\nstring2 = 'my age is %d' % (age)\nprint(string2)\nstring3= 'Hey %s, my age is %d' % (name, age)\nprint(string3)\nstring3= 'Hey %s, my subject mark is %f' % (name, marks)\nprint(string3)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-pip",
    "href": "qmd/python-general.html#sec-py-gen-pip",
    "title": "General",
    "section": "pip",
    "text": "pip\n\nLooks for packages on https://pypi.org, downloads, and installs it\nMisc\n\nIf you installed python using the app-store, replace python with python3.\nDon’t use sudo to install libraries, since it will install things outside of the virtual environment.\nNor should you use “–user”, since it’s made to install things outside of the virtual environment.\nDon’t mix pip, venv and Anaconda. Avoid Anaconda if you can. If you have to use Anaconda, don’t use pip and venv. Limit yourself to Anaconda tools.\nIf you get SSL errors (common if you are in a hotel or a company network) use the –trusted-host pypi.org –trusted-host files.pythonhosted.org options with pip to work around the problem.\n\ne.g. python -m pip install pendulum --trusted-host pypi.org --trusted-host files.pythonhosted.org\n\nIf you are behind a corporate proxy that requires authentication (common if you are in a company network), you can use the –proxy option with pip to give the proxy address and your credentials.\n\ne.g. python -m pip install pendulum --proxy http://your_username:yourpassword@proxy_address\nIt also works with the https_proxy environment variables\n\n\nInstall library\n$ python -m pip install &lt;library_name&gt;\n\n# inside ipython or a colab notebook, \"!\" signifies a shell command\n!pip install &lt;library_name&gt;\nInstall library from github\npython -m pip install git+https://github.com/bbalasub1/glmnet_python.git@1.0\n\n“@1.0” is the version number\n\nUninstall library\n$ python -m pip uninstall &lt;library_name&gt;\n\nWon’t uninstall the dependencies of this library.\nIf you wish to also uninstall the unused dependencies as well, take a look at pip-autoremove\n\nRemove all packages in environment\n$ python -m pip uninstall -y -r &lt;(pip freeze)\nRemove all packages in environment but write the names of the packages to a requirements.txt file first\n$ python -m pip freeze &gt; requirements.txt && python3 -m pip uninstall -r         requirements.txt -y\nInstall requirements.txt\n$ python -m pip install -r requirements.txt\nWrite names of all the packages in your environment to a requirement.txt file\n$ python -m pip freeze &gt; requirements.txt\n\nWrites the specific version of the packages that you have installed in your environment (e.g. pandas==1.0.0)\n\nThis may not be what you always want, so you’ll need to manually change to just the library name in that case (e.g. pandas)\n\nOnly aware of the packages installed using the pip install command\n\ni.e. any packages installed using a different approach such as peotry, setuptools, condaetc. won’t be included in the final requirements.txt file.\n\nDoes not account for dependency versioning conflicts\nSaves all packages in the environment including those that are not relevent to the project\nIf you are not using a virtual environment, pip freeze generates a requirement file containing all the libraries in including those beyond the scope of your project.\n\nList your installed libraries\n$ python -m pip list\nSee if you have a particular library installed\n$ python -m pip list | grep &lt;library_name&gt;\nGet library info (name, version, summary, license, dependencies and other)\n$ python -m pip show &lt;library_name&gt;\nCheck that all installed packages are compatible\n$ python -m pip check\nUpdate package\n$ python -m pip install package_name --upgrade\nSearch for PyPI libraries (pip source for libraries)\n$ python -m pip search &lt;search_term&gt;\n\nreturns all libraries matching search term\n\nDownload a package without installing it\npython -m pip download &lt;library name&gt;\n\nIt will download the package and all its dependencies in the current directory (the files, called, wheels, have a .whl extension).\nYou can then install them offline by doing python -m pip install on the wheels.\n\nBuild Wheel archives for the libraries and dependencies in your environment\n$ python -m pip wheel\n\nI think these are binaries, so they don’t need compiled if installed in a future environment\nReal Python Tutorial\n\nManage configuration\n$ python -m pip config &lt;action name&gt;\n\nActions: edit, get, list, set or unset\nExample\n$ python -m pip config set global.index-url https://your.global.index\n\nDisplay debug information specific to pip\n$ python -m pip debug",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-anac",
    "href": "qmd/python-general.html#sec-py-gen-anac",
    "title": "General",
    "section": "Anaconda",
    "text": "Anaconda\n\nCheck configuration of current environment\nconda list\n\nShows python version used, package names installed and their versions\n\nInstall packages\nconda install &lt;package1&gt; &lt;package2&gt;\nInstall a package from a specific channel\nconda install &lt;package_name&gt; -c &lt;channel_name&gt; -y # Short form\nconda install &lt;package_name&gt; --channel &lt;channel_name&gt; -y # Long form\nPackage installation channels (some packages not available in default channel)\n\nCheck current channels\nconda config --show channels\n\nThe order in which these channels are displayed shows the channel priority.\n\nWhen a package is installed, anaconda will the check the channel at the top of list first then work it’s way down\n\n\nAdd a channel\nconda config --add channels conda-forge\n\nAdds “conda-forge” to list of available channels\n\nRemove a channel\nconda config --remove channels conda-forge\n\nRemoves the “conda-forge” channel",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-env",
    "href": "qmd/python-general.html#sec-py-gen-env",
    "title": "General",
    "section": "Environments",
    "text": "Environments\n\nMisc\n\nWhen you’re in a virtual environment\n\nAnytime you use the “python” command while your virtual environment is activated, it will be only the one from this env.\nIf you start a Python shell now, it will see only the things in the current directory, and the things installed in the virtual environment.\nIf you run a script, it will see only the things in the current directory, and the things installed in the virtual environment.\nIf you run a command, the command will be taken from the virtual environment.\nAnd they will only use exactly the version of Python of the virtual environment.\n\nStore environment files with project\nBreakage\n\nYou cannot move a virtual environment, it will stop working. Create a “requirements.txt” file, delete the virtual environment and create a new one.\nDon’t rename a directory containing a virtual environment. Or if you do, prepare yourself to create a “requirements.txt” file, delete the virtual environment and create a new one.\nIf you change the Python used in the virtual environment, such as when uninstalling it, it will stop working.\n\nCreate one big virtual environment for all small scripts.\n\nIf you make a lot of venv, you may be tempted to install everything at the system level for convenience. After all, it’s a bore to create and activate a virtual environment each time you want to write a five liner. A good balance is one single virtual environment you use for all things quick and dirty.\n\nCreate several virtual environments per versions of python if your project needs to support several versions. You may need several requirements.txt files as well, one for each env.\nRecommendations for a stable dependency environment for your project (article)\n\nDon’t install the latest major version of Python\n\nMaximum: 1 version under the latest version\nMinimum: 4 versions under the latest version (e.g. latest = 3.11, min = 3.7)\n\nUse only the python.org installer on Windows and Mac, or official repositories on Linux.\nNever install or run anything outside of a virtual environment\nLimit yourself to the basics: “pip” and “venv”\nIf you run a command, use “-m”\n\nIt lets you run any importable Python module, no matter where you are. Because most commands are Python modules, we can use this to say, “run the module X of this particular python”.\nThere is currently no way for you to run any python command reliably without “-m”.\nExamples:\n# Don't do :\npip install\n# Do:\npython -m pip install\n\n# Don't do :\nblack\n# Do:\npython -m black\n\n# Don't do :\njupyter notebook\n# Do:\npython -m jupyter notebook\n\nWhen creating a virtual environment, be explicit about which Python you use\n\nGet current python versions installed: py --list-paths (windows)\n\n\n\n\n\npyenv\n\nJust a simple Python version manager — think {rig}\n{{pyenv}}, {{pyenv-win}}\nSet-up in RStudio (article)\nCompiles Python under the hood when you install it. But compiling can fail in a thousand ways\npyenv install --list: To see what python versions are available to install\npyenv install &lt;version number&gt;: To install a specific version\npyenv versions: To see what python versions are installed on your system\npyenv global &lt;version number&gt;: The set one python version as a global default\npyenv local &lt;version number&gt;: The set a python version to be used within a specific directory/project\\\n\n\n\npdm\n\nDocs\nPackage and dependency manager similar to npm. Doesn’t require virtual environments.\nFeatures: auto-updating pyproject.toml, isolating dependencies from dependencies-of-dependencies, active development and error handling\n\n\n\nvenv\n\nMisc\n\nShipped with Python\nDon’t mix pip, venv and Anaconda. Avoid Anaconda if you can. If you have to use Anaconda, don’t use pip and venv. Limit yourself to Anaconda tools.\n\nCreate\n\nWindows: py -&lt;py version&gt; -m venv &lt;env name&gt;\nMac/Linux: python3.8 -m venv .venv\n\nWhere the python version is 3.8 and the environment name is “.venv”\nMac and Linux hide folders with names that have preceding “.” by default, so make sure you have “display hiddent folders” activated or you won’t see it.\n\nNaming Environments\n\nName your environment directory “.venv”, because:\n\nSome editors check for this name and automatically load it.\nIt contains “venv”, so it’s explicit.\nIt has a dot, so it’s hidden on Linux.\n\nIf you have more than one environment directory, use a suffix to distinguish them.\n\ne.g. A project that must work with two different versions of Python (3.9 and 3.10), I will have a “.venv39” and a “.venv310”\n\nNaming enviroments for misc uses\n\n“.venv_test”: located in a personal directory to install new tools you want to play with. It’s disposable, and often broken, so you can delete it and recreate it regularly.\n“.venv_scripts”: Used for all the small scripts. You don’t want to create one virtual environment for each script, so centralize everything. It’s better than installing outside of a virtual environment, but is not a big constraint.\n\n\n\nActivate\n\nWindows: .venv\\Scripts\\activate\n\nWhere .venv is the name of the virtual environment\nMay need .bat as extension to activate\n\nMac/Linux: source .venv/bin/activate\n\nAfter that, you can use python -m pip install to install packages.\nDeactivate: deactivate\n\n\n\nvirtualenv\n\nDocs\nCreate a virtual environment\n python3 -m venv &lt;env_name&gt;\n\n-m venv tells python to run the virtual environment module, venv\nMake sure you’re in your projects directory\nRemember to add “&lt;env_name&gt;/” to .gitignore\n\nActivate environment\nsource venv/bin/activate # Mac or Linux\nvenv\\Scripts\\activate # Windows\n\n&gt;&gt; (&lt;env_name&gt;) $\n\nPrompt should change if the environment is activated\nAll pip  installed packages will now be installed into the “&lt;env_name&gt;/lib/python3.9/site-packages” directory\n\nUse the python contained within your virtual environment\npython main.py\n\nNot sure why you wouldn’t just activate the environment.\n\nDeactivate environment\ndeactivate\n\nno python  or env_name needed?\n\nReproducing environment\n\nDone using requirements.txt (see pip section for details on writing and installing)\n\nI don’t think the python version is included, so that will need to communicated manually\n\n\n\n\n\nAnaconda\n\nList environments\nconda env list # method 1\nconda info --envs # method 2\n\ndefault environment is called “base”\nActive environment will be in parentheses\nActive environment will be the one in the list with an asterix\n\nCreate a new conda environment\nconda create -n &lt;env name&gt;\nconda activate &lt;env name&gt;\nCreate a new conda environment with a specific python version\nconda create -n py310 python=3.10\nconda activate py310\nconda install jupyter jupyterlab\njupyter lab\n\nAlso install and launch jupyter lab\n\nCreate an environment from a yaml file\nconda env create -f environment.yml # Short form\nconda env create --file environment.yml # Long form\nRemove an environment\nconda deactivate &lt;env_name&gt; # Need to deactivate the environment first\nconda env remove -n &lt;env_name&gt;\n\nShould also delete environment folders (conda env list shows path to folders)\n\nClone an existing environment\nconda create -n testclone --clone test # Short form\nconda create --name testclone --clone test # Long form\n\n“testclone” is a copy of “test”\n\nActivate an environment\nconda activate &lt;env_name&gt;\nActivate environment with reticulate in R\nreticulate::use_python(\"/usr/local/bin/python\")   \nreticulate::use_condaenv(\"&lt;env name&gt;\", \"/home/jtimm/anaconda3/bin/conda\")\nDeactivate an environment\nconda activate # Option 1: activates base\nconda deactivate test # Option 2\nExport the specifications of the current environment into a YAML file into the current directory\nconda env export &gt; environment.yml # Option 1\nconda env export -f environment.yml # Option 2\nExample: Conda workflow\n\nCreate an environment that uses a specific python version\n\nWithout a specified python version, the environment will use the same version as “base”\n\nconda create -n anothertest python=3.9.7 -y\n\n-n is the name flag and “anothertest” is the name of the environment\nUses Python 3.9.7\nWithout the -y flag, there’d be a prompt you’d have to answer “yes” to\n\nActivate the environment\nconda activate anothertest\nInstall packages\n\n\nInstalling packages one at time can lead to dependency conflicts.\nConda’s official documentation recommends to install all packages at the same time so that the dependency conflicts are resolved\nconda install \"numpy&gt;=1.11\" nltk==3.6.2 jupyter -y # install specific versions\nconda install numpy nltk jupyter -y # install all latest versions\n\nDo work and deactivate environment\nconda deactivate anothertest\n\nExample Raschka workflow\n# create & activate\nconda create  --prefix ~/code/myproj python=3.8\nconda activate ~/code/myproj\n# export env\nconda env export &gt; myproj.yml\n# create new env from yaml\nconda env create --file myproj.yml --prefix ~/code/myproj2\n\n\n\nPoetry\n\nDocs (like renv)\nApparently buggy (article)\npip’s dependency resolver is more flexible and won’t die on you if the package specifies bad metadata, while poetry’s strictness may mean you can’t install some packages at all.\nCreate project\n\npoetry new &lt;project-dir-name&gt;\n\nautomatically creates a directory for your project with a skeleton\n“pyproject.toml” maintains dependencies for the project with the following sections:\n\ntool.poetry provides an area to capture information about your project such as the name, version and author(s).\ntool.poetry.dependencies lists all dependencies for your project.\ntool.poetry.dev-dependencies lists dependencies your project needs for development that should not be present in any version deployed to a production environment.\nbuild-system references the fact that Poetry has been used to manage the project.\n\n\nAdd library and create lock file: poetry add &lt;library name&gt;\n\nWhen the first library is added, a “poetry.lock” file wil be generated\n\nActivate environment: poetry shell\n\nDeactivate environment: exit\n\nRun script: poetry run python my_script.py\nPackage the project: poetry build\n\nCreates tar.gz and wheel files (.whl) in “dist” dir\n\nExample: poetry workflow (+pyenv, virtualenv)\n# Create a virtual environment called \"my-new-project\"\n# using Python 3.8.8\npyenv virtualenv 3.8.8 my-new-project\n# Activate the virtual environment\npyenv activate my-new-project\n\n{{pyenv}} - For managing the exact version of Python and activating the environment\nName your package the same name as the directory which is the same name as the virtual environment.\n\nDashes for the latter two and underscores for the package\n\nIntitialize the project and add packages (similar to renv) bash              poetry init     poetry add numpy\nReinstall dependencies\n# navigate to my project directory and run\npoetry install\nTurn off virtualenv management\n# right after installing poetry, run:\npoetry config virtualenvs.create false\n\nDefault poetry behavior is that it will manage your virtual environments for you. This may not be desirable because:\n\nCan’t just run a script from the command line. Instead, have to run poetry run my-script\n\nAwkward when you want to dockerize your code\n\nEnforces a virtual environment management framework on everybody in a shared codebase\nYour Makefile now needs to know about poetry",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-deps",
    "href": "qmd/python-general.html#sec-py-gen-deps",
    "title": "General",
    "section": "Dependencies",
    "text": "Dependencies\n\nMisc\n\nAfter mastering pip, {{pip-tools}} is recommended.\n\nGet a complete list of dependencies (e.g. dependencies of dependencies) with {{deptree}}\ndeptree\n# output\nFlask==2.2.2  # flask\n  Werkzeug==2.2.2  # Werkzeug&gt;=2.2.2\n    MarkupSafe==2.1.1  # MarkupSafe&gt;=2.1.1\n  Jinja2==3.1.2  # Jinja2&gt;=3.0\n    MarkupSafe==2.1.1  # MarkupSafe&gt;=2.0\n  itsdangerous==2.1.2  # itsdangerous&gt;=2.0\n  click==8.1.3  # click&gt;=8.0\n# deptree and pip trees\n\nFlask depends on Werkzeug which depends on MarkupSafe\n\nWerkzeug and MarkupSafe qualify as transitive dependencies for this project\n\nCommented part on the right is the compatible range\n\nrequirements.txt format\n# comment\npandas==1.0.0\npyspark\npip: write names of all the packages in your environment to a requirement.txt file\n$ python3 -m pip freeze &gt; requirements.txt\n\nSee pip section for issues with this method\n\n{{pipx}}\n\nA tool for installing Python CLI utilities that gives them their own hidden virtual environment for their dependencies\nAdds the tool itself to your PATH - so you can install stuff without worrying about it breaking anything else\nInstall\npipx install datasette\n\n{{pipreqs}}\n\nScans all the python files (.py) in your project, then generates the requirements.txt file based on the import statements in each python file of the project\nSet-up: pip install pipreqs\nGenerate requirements.txt file: pipreqs /&lt;your_project_root_path&gt;/\nUpdate requirements.txt:  pipreqs --force /&lt;your_project_root_path&gt;/ \nIgnore the libraries of some python files from a specific subfolder\npipreqs /&lt;your_project_root_path&gt;/ --ignore  /&lt;your_project_root_path&gt;/folder_to_ignore/\n\n{{pip-compile-multi}}\n\nNotes from:\n\nEnd Python Dependency Hell with pip-compile-multi\n\nCreates and nests multiple requirement files\n\ne.g. Able to keep dev environment from production environment separate\n\nAutoresolution of cross-requirement file conflicts\n\nDependency DAG (how all requirement files are connected) must have exactly one “sink” node\n\nOrganize your most ubiquitous dependencies into a single “core” set of dependencies that all other nodes require (a source node), and all of your development dependencies in a node that requires all others (directly or indirectly) require (a sink).\n\nSimplifies and allows use of autoresolution functionality\n\nExample: DAG (directionality of the arrows is opposite compared to library docs)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-loadsav",
    "href": "qmd/python-general.html#sec-py-gen-loadsav",
    "title": "General",
    "section": "Loading/Saving",
    "text": "Loading/Saving\n\nMisc\n\n{{pickle}} needs custom class(es) to be defined in another module/file and then imported. Otherwise, PicklingError will be raised.\n\n\n\nFile paths\n\nMisc\n\n{{pathlib}} is recommended\n\n{{os}}\n\nGet current working directory: os.getcwd()\nList all files and directories in working directory: os.listdir()\nList all files and directories from a subdirectory: os.listdir(os.getcwd()+'\\\\01_Main_Directory')\nUsing os.walk(): gathers paths, folders, and files\n\nPaths\n\n\npath = os.getcwd()+'\\\\01_Main_Directory'\nfor folder_path, folders, files in os.walk(path):\n    print(folder_path)\nFolders\n\n\nSimilar code, just replace print(folder_path) with print(folders)\n\nFiles\n\n\n{{glob}}\n\nGet a file path string\nimport glob\npath = os.getcwd()+'\\\\01_Main_Directory'\nfor filepath in glob.glob(path):\n    print(filepath)\n# C:\\Users\\Suraj\\Challenges\\01_Main_Directory\nList all files and subdirectories from a path\npath = os.getcwd()+'\\\\01_Main_Directory\\\\*'\nfor filepath in glob.glob(path):\n    print(filepath)\n\nNote the * wildcard\n\nList all files and subdirectories with a “1” in the name\npath = os.getcwd()+'\\\\01_Main_Directory\\\\*1.*'\nfor filepath in glob.glob(path):\n    print(filepath)\nGet a list of csv file paths from a directory: all_files = glob.glob(\"C:/Users/path/to/dir/*.csv\")\n\nNote that you don’t need a loop to save to an object\n\nList all files and subdirectories and files in those subdirectories\npath = os.getcwd()+'\\\\01_Main_Directory\\\\**\\\\*.txt'\nfor filepath in glob.glob(path, recursive=True):\n    print(filepath)\n#Output\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_2.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_3.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_4.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_5.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_1\\File_1_in_SubDict_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_1\\File_2_in_SubDict_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_2\\File_1_in_SubDict_2.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_2\\File_2_in_SubDict_2.txt\n\nComponents: “**” and “recursive=True”\n\n\n{{pathlib}}\n\nProvides a single Path class with a range of methods (instead of separate functions) that can be used to perform various operations on a path.\nCreate a path object for a directory\nfrom pathlib import Path\npath = Path('origin/data/for_arli')\nCheck if a folder or a file is available in a given path\nif path.exists():\n    print(f\"[{path}]{style='color: #990000'} exists.\")\n    if path.is_file():\n        print(f\"[{path}]{style='color: #990000'} is a file.\")\n    elif path.is_dir():\n        print(f\"[{path}]{style='color: #990000'} is a directory.\")\nelse:\n    raise ValueError(f\"[{path}]{style='color: #990000'} does not exists\")\n\nChecks if the path ‘origin/data/for_arli’ exists\n\nif it does, it will check whether it is a file or a directory.\nIf the path does not exist, it will print a raise an Error indicating that the path does not exist.\n\n\nList all files/folders in a path\nfor f in path.iterdir():\n    print(f)\n\nUse it in combination with the previous is_dir() and is_file()  methods to list either files or directories.\n\nDelete files/folders in a path\nfor f in path.iterdir():\n    f.unlink()\n\npath.rmdir()\n\nunlink deletes each file in the path\nrmdir deletes the directory.\n\ndirectory must be empty\n\n\nCreate a sequence of directories\n# existing directory: D:\\scripts\\myfolder\np = Path(\"D:\\scripts\\myfolder\\logs\\newfolder\")\np.mkdir(parents=True, exist_ok=True)\n\nCreate path object with desired sequence of directories (e.g. logs\\newfolder)\nmkdir with parents=True creates the sequence of directories\n\nW/exist_ok=True no error with occur if the directory already exists\n\n\nRename directory: path.rename('origin/data/new_name')\nConcatenate a path with string\npath = Path(\"/origin/data/for_arli\")\n# Join another path to the original path\nnew_path = path.joinpath(\"la\")\nprint(new_path) # prints 'origin/data/for_arli/bla'\n\nIt also handles the join between two Path objects\n\nDirectory stats\nprint(path.stat()) # print statistics \nprint(path.owner()) # print owner\n\ne.g. creation time, modification time, etc.\n\nWrite to a file\n# Open a file for writing\npath = Path('origin/data/for_arli/example.txt')\nwith path.open(mode='w') as f:\n    # Write to the file\n    f.write('Hello, World!')\n\nYou do not need to create manually example.txt.\n\nRead a file\npath = Path('example.txt')\nwith path.open(mode='r') as f:\n    # Read from the file\n    contents = f.read()\n    print(contents) # Output: Hello World!\n\n\n\n\nModels\n\nSaving and Loading an estimator as a binary using {{joblib}} (aside: pipelines are estimators)\nimport joblib\n#saving the pipeline into a binary file\njoblib.dump(pipe, 'wine_pipeline.bin')\n#loading the saved pipeline from a binary file\npipe = joblib.load('wine_pipeline.bin')\nSaving and loading a trained model as a pickle file\nimport pickle\n# open file connection\npickle_file = open('model.pkl', 'ab')\n# save the model\npickle.dump(model_obj, pickle_file)\n# close file connection                     \npickle_file.close()\n\n# Open conn and save\ntest_dict = {\"Hello\": \"World!\"}\nwith open(\"test.pickle\", \"wb\") as outfile:\n# \"wb\" argument opens the file in binary mode\npickle.dump(test_dict, outfile)\n\n# open file connection\npickle_file = open('model.pkl', 'rb')\n# load saved model\nmodel = pickle.load(pickle_file)\n\n# open conn and load\n# Deserialization\nwith open(\"test.pickle\", \"rb\") as infile:\n    test_dict_reconstructed = pickle.load(infile)\n\nCan serialize almost everything including classes and functions\n\n\n\n\nEnvironment Variables\n\n{{os}}\n\nCheck existence\nenv_var_exists = 'ENV' in os.environ\n# or\nenv_var_exists = os.environ.has_key('ENV')\nList environment variables: print(os.environ)\nLoading\nimport os\n# Errors when not present\nenv_var = os.environ['ENV'] # where ENV is the name of the environment variable\n# Returns None when not present\nenv_var = os.environ.get('ENV', 'DEFAULT_VALUE') # using default value is optional\nSet/Export or overwrite\nos.environ['ENV'] = 'dev'\nLoad or create if not present\ntry:\n    env_var = os.environ['ENV']\nexcept KeyError:\n    os.environ['ENV'] = 'dev'\nDelete\nif 'ENV' in os.environ:\n    del os.environ['ENV']\n\n{{python-decouple}}\n\nAccess environment variables from whatever environment it is running in.\nCreate a .env file in the project root directory: touch .env\nOpen .env in nano text editor: nano .env\n\nNano text editor is pre-installed on macOS and most Linux distros\nCheck if installed/version: nano --version\nBasic usage tutorial\n\nAdd environment variables to file\nUSER=alex\nKEY=hfy92kadHgkk29fahjsu3j922v9sjwaucahf\n\nSave: Ctrl+o\nExit: Ctrl+x\n\n* Add .env to your .gitignore file *\nAccess\nfrom decouple import config\nAPI_USERNAME = config('USER')\nAPI_KEY = config('KEY')\n\n{{python-dotenv}}\n\nReads .env files\nProbably more popular than {{python-decouple}}\nHas a companion R package, {dotenv}, so .env files can be used in projects that use both R and Python.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-funs",
    "href": "qmd/python-general.html#sec-py-gen-funs",
    "title": "General",
    "section": "Functions",
    "text": "Functions\n\nMisc\n\nBenchmarking a function\n\nUsing IPython function\n%time dat['col1001'] = some_function(dat['col1'], dat['col2'], dat['col3'])\n\n%%time prints the wall time for the entire cell whereas %time gives you the time for first line only\n\nUsing a decorator\n\n\nAssigning functions based on arg type\ndef process_data(data):\n    if isinstance(data, dict):\n        process_dict(data) \n    else:\n        process_list(data) \ndef process_dict(data: dict):\n    print(\"Dict is processed\")\ndef process_list(data: list):\n    print(\"List is processed\")\n\nAssigns data to a particular function depending on whether it’s a dict or a list\nisinstance checks that the passed argument is of the proper type or a subclass\n\nWrapping functions\nfrom functools import partial\nget_count_df = partial(get_count, df=df)\n\nWraps function to make df the default value for df arg\n\n\n\n\nDocumentation\n\nFunctions should at least include docstrings and type hinting\nDocstrings\n\nTypes: Google-style, Numpydoc, reStructured Text, EpyTex\nInformation to include\n\nFunction description, arg description, return value description, Description of errors, Optional extra notes or examples of usage.\n\nAccess functions docstring:\n\nprint(func_name.__doc__)\nFor large docstrings\nimport inspect\nprint(inspect.getdoc(func_name))\n\nExample: Google-style\ndef send_request(key: str, lat: float = 0, lon: float = 0):\n    \"\"\"Send a request to Climacell Weather API\n    to get weather info based on lat/lon.\n\n    Climacell API provides realtime weather\n    information which can be accessed using\n    their 'Realtime Endpoint'.\n\n    Args:\n      key (str): an API key with length of 32 chars.\n      lat (float, optional): value for latitude.\n        Default=0\n      lon (float, optional): value for longitude.\n        Default=0\n\n    Returns:\n      int: status code of the result \n      dict: Result of the call as a dict\n\n    Notes:\n      See https://www.climacell.co/weather-api/ \n      for more info on Weather API. You can get\n      API key from there, too.\n    \"\"\"\n\nFirst sentence should contain the purpose of the function\n\nExample: Numpydoc\ndef send_request(key: str, lat: float = 0, lon: float = 0):\n    \"\"\"\n    Send a request to Climacell Weather API\n    to get weather info based on lat/lon.\n\n    Climacell API provides realtime weather\n    information which can be accessed using\n    their 'Realtime Endpoint'.\n\n    Parameters\n    ----------\n      key (str): an API key with length of 32 chars.\n      lat (float, optional): value for latitude.\n        Default=0\n      lon (float, optional): value for longitude.\n        Default=0\n\n    Returns\n    -------\n      int: status code of the result \n      dict: Result of the call as a dict\n\n    Notes\n    -----\n      See https://www.climacell.co/weather-api/ \n      for more info on Weather API. You can get\n      API key from there, too.\n    \"\"\"\n\nType Hinting\n\nThis doesn’t check the type; it’s just metadata\n\nsee isinstance (see below), NotImplementedError (see below), or {{typecheck}} and {{mypy} (see bkmks) for type checking that will throw errors\n\nUsing type hints enables you to perform type checking. If you use an IDE like PyCharm or Visual Studio Code, you’ll get visual feedback if you’re using unexpected types:\nVariables: my_variable_name: tuple[int, ...]\n\nvariable should be a tuple that contains only integers. The ellipsis says the total quantity is unimportant.\n\nFunctions\ndef get_count(threshold: str, column: str, df: pd.DataFrame) -&gt; int:\n    return (df[column] &gt; threshold).sum()\n\n“threshold”, “column” should be strings (str)\n“df” should be a pandas dataframe (pd.DataFrame)\nOutput should be an integer (int)\n\nFunction as an arg: Callable[[Arg1Type, Arg2Type], ReturnType]\n\nExample:\nfrom collections.abc import Callable\ndef foo(bar: Callable[[int, int], int], a: int, b: int) -&gt; int:\n    return bar(a, b)\n\n“bar” is a function arg for the function, “foo”\n“bar” is supposed to take: 2 integer args ([int, int]) and return an integer (int)\n\nExample:\ndef calculate(i: int, action: Callable[..., int], *args: int) -&gt; int:\n    return action(i, *args)\n\n“action” takes any number and type of arguments but must return an integer.\nWith *args: int, you also allow a variable number of optional arguments, as long as they’re integers.\n\nExample: Lambda\nf: Callable[[int, int], int] = lambda x, y: 3*x + y\n\nMay not work\n\n\n\n\n\n\nArgs and Operators\n\nMisc\n\n** Args are not reset to default values after each call **\n\nExample:\n\ndef func(list1=[]):      # here l1 is a default argument set to []\n    list1.append(\"Temp\")\n    return list1\n\n“None” + conditional must be used to get the arg to reset back to the default value\n\ndef func(l1=None):     \n    if l1 is None: \n        l1 = []\n    l1.append(\"Temp\") \n    return l1\n\n*\n\nUnpacks Lists\n\nnum_list = [1,2,3,4,5]\nnum_list_2 = [6,7,8,9,10]\n\nprint(*num_list)\n# 1 2 3 4 5\nnew_list = [*num_list, *num_list_2] # merge multiple lists\n# [1,2,3,4,5,6,7,8,9,10]\n*args\n\nFunctions that can accept a varying number of values\n\ndef names_tuple(*args):\n    return args\n\nnames_tuple('Michael', 'John', 'Nancy')\n# ('Michael', 'John', 'Nancy')\nnames_tuple('Jennifer', 'Nancy')\n# ('Jennifer', 'Nancy')\n**\n\nUnpacks Dictionaries\n\nnum_dict = {‘a’: 1, ‘b’: 2, ‘c’: 3}\nnum_dict_2 = {‘d’: 4, ‘e’: 5, ‘f’: 6}\n\nprint(*num_dict) # only keys printed\n# a b c\nnew_dict = {**num_dict, **num_dict_2} # merge dictionaries\n# {‘a’: 1, ‘b’: 2, ‘c’: 3, ‘d’: 4, ‘e’: 5, ‘f’: 6}\n**kwargs\n\nFunctions that can accept a varying number of variable/value pairs (like a … in R)\n\ndef names_dict(**kwargs):\n    return kwargs\n\nnames_dict(Jane = 'Doe')\n# {'Jane': 'Doe'}\nnames_dict(Jane = 'Doe', John = 'Smith')\n# {'Jane': 'Doe', 'John': 'Smith'}\nFunction as an arg\ndef classic_boot(df, estimator, seed=1):\n    df_boot = df.sample(n=len(df), replace=True, random_state=seed)\n    estimate = estimator(df_boot)\n    return estimate\n\nBootstrap function with an “estimator” function (e.g. mean) as arg\nUsing a Callable\n\nClass as an arg\nfrom dataclasses import dataclass\n\n@dataclass\nclass Person:\n    name: str\n    age: int\n    address: str\n    phone: str\n    email: str\n\ndef process_data(person: Person):\n    print(f\"Processing data for {person.name}, {person.age}, living at {person.address}. Contact info: {person.phone}, {person.email}\")\n\nperson = Person(\"Alice\", 30, \"123 Main St\", \"555-1234\", \"alice@example.com\")\nprocess_data(person)\n\nMakes function more readable when the function requires a bunch of args\n\nClass as an arg (safer alternative)\nfrom typing import NamedTuple\n\nclass Person(NamedTuple):\n    name: str\n    age: int\n    address: str\n    phone: str\n    email: str\n\ndef process_data(person: Person):\n    print(f\"Processing data for {person.name}, {person.age}, living at {person.address}. Contact info: {person.phone}, {person.email}\")\n\nperson = Person(\"Alice\", 30, \"123 Main St\", \"555-1234\", \"alice@example.com\")\nprocess_data(person)\n\nUsing NamedTuple means that the attributes cannot be overridden\n\ne.g. Executing person.name = \"Bob\" will result in an error because tuples can’t be modified.\n\n\nMake an arg optional\nlass Address:\n    def __init__(self, street, city, state, zipcode, street2=''):\n        self.street = street\n        self.street2 = street2\n        self.city = city\n        self.state = state\n        self.zipcode = zipcode\n\n“street2” has default value of an empty string, so it’s optional\n\n\n\n\nLambda\n\nUseful if you just have 1 expression that you need to execute.\nBest Practices\n\nlambda is an anonymous function, hence it is not a good idea to store it in a variable for future use\nDon’t use lambdas for single functions (e.g. sqrt). Make sure it’s an expression.\n\nExample\n# bad\nsqrt_list = list(map(lambda x: math.sqrt(x), mylist))\n# good\nsqrt_list = list(map(math.sqrt, mylist))\n\nAffects performance\n\n\nDon’t use for complex expressions that require more than 1 line (meh)\n\nPer PEP8 guidelines, Limit all lines to a maximum of 79 characters\nExample\n# bad (118 characters)\ndf[\"FinalStatus\"] = df[\"Status\"].map(lambda x: 'Completed' if x ==\n'Delivered' or x == 'Shipped' else 'Not Completed')\n# instead\ndf[\"FinalStatus\"] = ''\ndf.loc[(df[\"Status\"] == 'Delivered') |\n      (df[\"Status\"] == 'Shipped'),\n      'FinalStatus'] = 'Completed'\ndf.loc[(df[\"Status\"] == 'Not Delivered') |\n      (df[\"Status\"] == 'Not Shipped'),\n      'FinalStatus'] = 'Not Completed'\n\n\nExample: 1 arg\n# py\nlambda x: np.sin(x / period * 2 * np.pi)\n# r\n~sin(.x / period * 2 * pi)\n# r\n\\(x) {sin(x / period * 2 * pi)}\nExample: 2 args\nGreater = lambda x, y : x if(x &gt; y) else y\nGreater(0.002, 0.5897)\nLambda-Filter\n\nFaster than a comprehension\n\nsee Loops &gt;&gt; Comprehensions\n\nFormat: filter(function, data_object)\n\nReturns a filter object, which needs to be converted into data structure such as list or set\n\nExample: Basic\nyourlist = list(np.arange(2,50,3))\nlist(filter(lambda x:x**2&lt;100, yourlist))\n# Output \n[2, 5, 8]\nExample: Filter w/logical\nimport pandas as pd\nimport datetime as dt\n# create a list of 10,000 dates\ndatlist = pd.date_range(dt.datetime.today(), periods=10000).tolist() \n# convert the dates to strings via list comprehension\ndatstrlist = [d.strftime(\"Day %d in %B of year %Y is a %A\") for d in datlist]\ndatstrlist[:4]\n['Day 21 in October of year 2021 is a Thursday', 'Day 22 in October of year 2021 is a Friday', 'Day 23 in October of year 2021 is a Saturday', 'Day 24 in October of year 2021 is a Sunday']\n\nstrLamb = filter(lambda d: ((d.endswith(\"urday\") or d.endswith(\"unday\")) and \"Oc\" in d), datstrlist)\n\nSearches for Saturdays and Sundays in the month of October of all years in list of strings\n\nExample: Nested Lists\ngroup1 = [1,2,3,43,23,42,8,3,7]\ngroup2 = [[3, 34, 23, 32, 42], [6, 11, 9], [1, 3,9,7,2,8]]\n[list(filter(lambda x: x in group1, sublist)) for sublist in group2]\n&gt;&gt; [[3, 23, 42], [], [1, 3, 7, 2, 8]]\n\nProbably useful for json\nfor-loop attached to the end of the list-filter combo\nEach sublist of group 2 is fed into the lambda-filter and compared to the group 1 list\n\n\nIterating over each element of a list\n\nExample: map\nlist(map(lambda x: x**2+x**3, yourlist))\n\nmap returns a map object that needs to be converted\n\nExample: 2 Lists\nmylist = list(np.arange(4,52,3))\nyourlist = list(np.arange(2,50,3))\nlist(map(lambda x,y: x**2+y**2, yourlist, mylist))\n\nLike a pmap\n\n\nNested lambdas\n\nExample: map\narr = [1,2,3,4,5]\nlist(map(lambda x: x*2, filter(lambda x: x%2 == 0, arr)))\n&gt;&gt; [4,8]\n\nWork inside out (locate where the data object, arr, appears)\n“arr” is filtered by the first lambda function for even numbers then iterated by map to be squared by the second lambda function\n\n\nIterate over rows of a column in a df\n\nExample: Using formula over rows\ngrade['evaluate']=grade['MathScore'].apply(lambda x: round((x**x)/2,2))\n\n“grade” is the df; “MathScore” is a numeric column; “evaluate” is the new column in the df\nFormula applied to each value of “MathScore” to generate each value of evaluate\n\nExample: Conditional over rows\ngrade['group']=grade['MathScore'].apply(lambda x: 'Excellent' if x&gt;=3.0 else 'Average')\n\n“grade” is the df; “MathScore” is a numeric column; “group” is the new column in the df\nConditional applied to each value of “MathScore” to generate each value of “group”\n\nUsing {{swifter}} for parallelization\nimport swifter\ndf['e'] = df.swifter.apply(lambda x: infer(x['a'], x['b'], x['c'], x['d']), axis = 1)\n\nIn a Pivot Table (like a crosstab)\n\nExample\n\ngrades_df\n\n2 names (“name”)\n6 scores (“score”)\nOnly 2 letter grades associated with these scores (“letter grade”)\n\nTask: drop lowest score for each letter grade, then calculate the average score for each letter grade\n\ngrades_df.pivot_table(index='name',\n                      columns='letter grade',\n                      values='score',\n                      aggfunc = lambda series : (sorted(list(series))[-1] + sorted(list(series))[-2]) / 2)\n\nletter grade    A    B\nname\nArif          96.5  87.0\nKayla        95.5  84.0\n\nindex: each row will be a “name”\ncolumns: each column will be a “letter grade”\nvalues: value in the cells will be from the “score” column according to each combination columns in the index and columns args\naggfunc: uses a lambda to compute the aggregated values\n\n“series” is used a the variable  in the lambda function\nsorts series (ascending), takes the top two values (using negative list indexing), and averages them\n\n\n\n\n\n\nScope\n\nPopulated objects within functions persist if you instantiate the object in the argument\n\n\n“all_numbers” retained it’s previous value when the 2nd call to the function was made\n\n\n\n\nClosures\n\nInner functions that can access values in the outer function, even after the outer function has finished its execution\nExample\n\n# closure way\ndef balanceOwed(roomN,rate,nights):\n    def increaseByMeals(extra):\n        amountOwned=rate*nights+extra\n        print(f\"Dear Guest of Room [{roomN}]{style='color: #990000'}, you have\", \n        \"a due balance:\", \"${:.2f}\".format(amountOwned))\n        return amountOwned\n    return increaseByMeals\n\nba = balanceOwned(201,400,3)\nba(200)\nba(150)\nba(180)\nba(190)\nDear Guest of Room 201, you have a due balance: $1400.00\nDear Guest of Room 201, you have a due balance: $1350.00\nDear Guest of Room 201, you have a due balance: $1380.00\nDear Guest of Room 201, you have a due balance: $1390.00\n\nTedious way: For each value of “extra” (e.g. meals), the function needs to be called even if the other values of the arguments don’t change.\nClosure way:\n\nincreaseByMeals() is a closure function, because it remembers the values of the outer function balanceOwed(), even after the execution of the latter\nbalanceOwed() is called with its three arguments only once and then after its execution, we call it four times with the meal expenses (“extra”).",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-mods",
    "href": "qmd/python-general.html#sec-py-gen-mods",
    "title": "General",
    "section": "Modules",
    "text": "Modules\n\n.py files are called “modules.”\nA directory with .py files in which one of the files is an “__init__.py” is called a package.\nMisc\n\nResource: Make your Python life easier by learning how imports find things\nsys.path contains the list of paths where Python is looking for things to import. Your virtual environment and the directory containing your entry point are automatically added to sys.path.\n\nsys.path is a list. Which means you can .append(). Any directory you add there will have its content importable. It’s a useful hack, but use it as a last resort.\n\nWhen using -m flag to run a script, if you pass a package instead of a module, the package must contain a “__main__.py” file for it to work. This __main__.py module will run.\nIf you have scripts in your projects, don’t run them directly. Run them using “-m”, and you can assume everything starts from the root.\n\nExample:\ntop_dir\n├── foo\n│   ├── bar.py\n│   ├── __init__.py\n│   └── blabla.py\n└── blabla.py\n\nRunning python foo/bar.py, “top_dir” is the current working directory, but “foo” is added to the sys.path.\nRunning python -m foo.bar, “top_dir” is the current working directory and added to sys.path.\n\nImports can all start from the root of the project and opened file paths as well.\n\n\n\n\nUsage\n\nProject Structure\n├── main.py\n├── packages\n│  └── __init__.py\n│  └── module_1.py\n│  └── module_2.py\n│  └── module_3.py\n└── └── module_4.py\n\n“__init__.py” contains only 1 line which declares all the functions (or classes?) that are in the modules\n__all__ = [\"func1\", \"func2\"]\n\nIf the module files contained classes with multiple functions, I think you’d just declare the classes and not every function in that class.\n\nIf using classes, each module should only have 1 class.\n\n\nScripts need to include “_main_” in order to used in other scripts\n# test_function.py\ndef function1(): \n    print(\"Hello world\") \nfunction1()\n\n# Define the __main__ script\nif __name__ == '__main__':   \n    # execute only if run as a script\n    function1()\n\nSays if this file is being run non-interactively (i.e. as a script), run this chunk\nAdd else: chunk, then that chunk will be run only if the file is imported as a module\nAllows you to allow or prevent parts of code from being run when the modules are imported\nImporting a module without _main_ in a jupyter notebook results in this\n\n\nLoading\n\nDO NOT USE from &lt;library&gt; import *\n\nThis will import anything and everything from that library and causes several problems:\n\nYou don’t know what is in that package, so you have no idea what you just imported, or even if what you want is in there.\nYou just filled your local namespace with an unknown quantity of mysterious names, and you don’t know what they will shadow.\nYour editor will have a hard time helping you since it doesn’t know what you imported.\nYour colleague will hate you because they have no idea what variables come from where.\n\nException: In the shell, it’s handy. Sometimes, you want to import all things in __init__.py and you have “__all__” defined (see above)\n\nFrom the working directory, it’s like importing from a library: from file1 import function1\nFrom a subdirectory, from subdirectory.file1 import function1\nFrom a directory outside the project, add the module to sys.path before importing it\nimport sys\nsys.path.append('/User/moduleDirectory')\n\nWhen a module is imported, it first searches for built-in modules, then the paths listed in sys.path\nThis appends the new path to the end of the sys.path\nimport sys\nsys.path.insert(1, '/User/moduleDirectory')\nPuts this path at the front of the sys.path directory list.\nimport sys\nsys.path.remove('/User/NewDirectory')\n\n*delete path from sys.path after you finish*\nPython will also search this path for future projects unless they are removed",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-cond",
    "href": "qmd/python-general.html#sec-py-gen-cond",
    "title": "General",
    "section": "Conditionals",
    "text": "Conditionals\n\nIf-Else\n\nSyntax\nif &lt;expression&gt;:\n    do something\nelse:\n    do something else\nExample\nregenerate = False\nif regenerate:\n    concepts_list = df2Graph(df, model='zephyr:latest')\n    dfg1 = graph2Df(concepts_list)\n    if not os.path.exists(outputdirectory):\n        os.makedirs(outputdirectory)\n\n    dfg1.to_csv(outputdirectory/\"graph.csv\", sep=\"|\", index=False)\n    df.to_csv(outputdirectory/\"chunks.csv\", sep=\"|\", index=False)\nelse:\n    dfg1 = pd.read_csv(outputdirectory/\"graph.csv\", sep=\"|\")\n\nTry-Except\n\nExample\nimport os\ntry:\n    env_var = os.environ['ENV']\nexcept KeyError:\n    # Do something\n\nIf “ENV” is not a present a KeyError is thrown. Then, except section executed.\n\n\nMatch (&gt; Python 3.10) (switch function)\nmatch object:\n    case &lt;pattern_1&gt;:\n        &lt;action_1&gt;\n    case &lt;pattern_2&gt;:\n        &lt;action_2&gt;\n    case &lt;pattern_3&gt;:\n        &lt;action_3&gt;\n    case _:\n        &lt;action_wildcard&gt;\n\n“object” is just a variable name; could be anything\n“case_” is the value used when none of the other cases are a match\nExample: function input inside user function\ndef http_error(status):\n    match status:\n        case 200:\n            return 'OK'\n        case 400:\n            return 'Bad request'\n        case 401 | 403 | 404:\n            return 'Not allowed'\n        case _:\n            return 'Something is wrong'\nExample: dict input inside a function\ndef get_service_level(user_data: dict):\n    match user_data:\n        case {'subscription': _, 'msg_type': 'info'}:\n            print('Service level = 0')\n        case {'subscription': 'free', 'msg_type': 'error'}:\n            print('Service level = 1')\n        case {'subscription': 'premium', 'msg_type': 'error'}:\n            print('Service level = 2')\nExample: inside a class\nclass ServiceLevel:\n    def __init__(self, subscription, msg_type):\n        self.subscription = subscription\n        self.msg_type = msg_type\n\n    def get_service_level(user_data):\n        match user_data:\n            case ServiceLevel(subscription=_, msg_type='info'):\n                print('Level = 0')\n            case ServiceLevel(subscription='free', msg_type='error'):\n                print('Level = 1')\n            case ServiceLevel(subscription='premium', msg_type='error'):\n                print('Level = 2')\n            case _:\n                print('Provide valid parameters')\n\nNote that inside the function, the change from “:” to “=”  and “()” following the class name in the “case” portion of the match\n\n\nAssert\n\nUsed to confirm a condition\n\nIncorrect: assert condition, message \n\nCorrect method: \nif not condition: \n    raise AssertionError\n\nassert is useful for debugging code because it lets you test if a condition in your code returns True, if not, the program will raise an AssertionError.\n** Do not use in production, because when code is executed with the -O (optimize) flag, the assert statements are removed from the bytecode. **",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-loops",
    "href": "qmd/python-general.html#sec-py-gen-loops",
    "title": "General",
    "section": "Loops",
    "text": "Loops\n\nMisc\n\nList Comprehensions vs Generators in terms of memory usage\n\n{{tqdm}} - progress bar for loops\nfrom tqdm import tqdm\nfor i in tqdm(range(10000))\n    ...\nbreak terminates the loop containing it\n\nIf in a nested loop, it will terminate the inner-most loop containing it\n\ncontinue is used to skip the remaining code inside a loop for the current iteration only; forces the start of the next iteration of the loop\npass does nothing\n\nused when a statement or a condition is required to be present in the program but we do not want any command or code to execute\n\n\n\n\nIterators\n\nRemembers values\nExample\nD = {\"123\":\"Y\",\"111\":\"PT\",\"313\":\"Y\",\"112\":\"Y\",\"201\":\"PT\"}\nff = filter(lambda e:e[1]==\"Y\", D.items())\n\nprint(next(ff))\n&gt;&gt; ('123', 'Y')\nprint(next(ff))\n&gt;&gt; ('313', 'Y')\napply\n\naxis\n\n0 or ‘index’: apply function to each column.\n1 or ‘columns’: apply function to each row.\n\nExample: Function applied to rows of a column of a dataframe (i.e. cells)\ndef df2Graph(dataframe: pd.DataFrame, model=None) -&gt; list:\n  # dataframe.reset_index(inplace=True)\n  results = dataframe.apply(\n    lambda row: graphPrompt(row.text, {\"chunk_id\": row.chunk_id}, model), axis=1\n  )\n\ntext and chunk_id are column names of the dataframe\nrow is the row of the dataframe since axis=1, and from that row, the columns text and chunk_id are subsetted in the arguments of user-defined function.\n\nExample: Formula applied to rows of a column of a dataframe (i.e. cells)\ngrade['evaluate']=grade['MathScore'].apply(lambda x: round((x**x)/2,2))\n\ngrade is the df; MathScore is a numeric column; evaluate is the new column in the df\n\n\n\n\n\nGenerators\n\nGenerators are iterators, a kind of iterable you can only iterate over once. (normal iterators like lists, strings, etc. can be repeatedly iterated over)\nGenerators do not store all the values in memory, they generate the values on the fly\n\nyield - Pauses the function saving all its states and later continues from there on successive calls.\n\nAllows you to consume one element at a time and work with it without requiring you to have every element in memory.\nProduces a generator\n\n\nMisc\n\n{{itertools}} islice can slice a generator.\nAlso see APIs &gt;&gt; {{requests}} for an example\n\nExample: Using a comprehension \nmygenerator = (x*x for x in range(3))\nfor i in mygenerator:\n...    print(i)\n\nProduce a list and ( ) produce a generator \n\nExample: Using a function\ndef create_generator():\n    mylist = range(3)\n    for i in mylist:\n        yield i*i\n\nfor i in mygenerator:\n    print(i)\n0\n1\n4\n\nThe first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then it’ll return the first value of the loop.\nThen, each subsequent call will run another iteration of the loop you have written in the function and return the next value.\nThis will continue until the generator is considered empty, which happens when the function runs without hitting yield.\n\nThat can be because the loop has come to an end, or because you no longer satisfy an “if/else”\n\n\nExample: Sending values to (yield)\ndef grep(pattern):\nprint \"Looking for %s\" % pattern\nwhile True:\n    line = (yield)\n    if pattern in line:\n        print line,\ng = grep(\"python\")  # instantiate with \"python\" pattern to search for\n\ng.next() # Prime it\n&gt;&gt; Looking for python\n\ng.send(\"A series of tubes\") # \"python\" not present so returns nothing\ng.send(\"python generators rock!\") # \"python\" present so returns line\n&gt;&gt; python generators rock!\ng.close() # closes coroutine\n\n(yield) receives the input of the .send method and creates a generator object which is assigned to “line”.\nAll coroutines must be “primed” by first calling .next() (or send(None))\n\nThis advances execution to the location of the first yield expression\n\n\nExample: Sending values to (yield)\ndef writer():\n    \"\"\"A coroutine that writes data *sent* to it to fd, socket, etc.\"\"\"\n    while True:\n        w = (yield)\n        print('&gt;&gt; ', w)\ndef writer_wrapper(coro):\n    # TBD\n    pass\nw = writer()\nwrap = writer_wrapper(w)\nwrap.send(None)  # \"prime\" the coroutine\nfor i in range(4):\n    wrap.send(i)\n&gt;&gt;  0\n&gt;&gt;  1\n&gt;&gt;  2\n&gt;&gt;  3\n\nA more complex framework if you want to break the workflow into multiple functions\n\n\n\nUsing yield from\n\nAllows for two-way usage (reading/sending) of generators\nExample (reading from a generator)\ndef reader():\n    \"\"\"A generator that fakes a read from a file, socket, etc.\"\"\"\n    for i in range(4):\n        yield '&lt;&lt; %s' % i\n\n# with yield\ndef reader_wrapper(g):\n    # Manually iterate over data produced by reader\n    for v in g:\n        yield v\n# OR with yield from\ndef reader_wrapper(g):\n    yield from g\nwrap = reader_wrapper(reader())\nfor i in wrap:\n    print(i)\n\nBasic; only eliminates 1 line of code\n\nExample (sending to a generator)\n# with (yield)\ndef writer_wrapper(coro):\n    coro.send(None)  # prime the coro\n    while True:\n        try:\n            x = (yield)  # Capture the value that's sent\n            coro.send(x)  # and pass it to the writer\n        except StopIteration:\n            pass\n# OR with yield from\ndef writer_wrapper(coro):\n    yield from coro\n\nNeed to see example 4 for the writer() code and the use case\nShows the other advantage of using “yield from”: it automatically includes the code to stop prime and stop the loop.\n\nReusable generator\n\n\nreading example using “yield from”\n\nSlicing a generator\nfrom itertools import islice\ndef gen():\n    yield from range(1,11)\ng = gen()\nmyslice = islice(g, 2)\n&gt;&gt; list(myslice)\n[1, 2]\n&gt;&gt; [i for i in g]\n[3,4,5,6,7,8,9,10]\n\n\n\n\nFor\n\n\nSyntax - for &lt;sequence&gt;: &lt;loop body&gt;\nNumeric Range\nfor i = 1 to 10\n    &lt;loop body&gt;\n\n# from 0 to 519\nfor i in range(520)\n    &lt;loop body&gt;\n\nres = 0\nfor idx in np.arange(0, 100000):\n  res += df.loc[idx, 'int']\n\nnp.arange() ran 8000 times faster than the same chunk using range()\n\nList\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\neven_numbers = []\nfor item in numbers:\n    if item % 2 == 0:\n        even_numbers.append(item)\nprint(even_numbers)\n\n# results: [2, 4, 6, 8]\nList: index and value\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nfor index, element in enumerate(numbers):\n    if element % 2 != 0:\n        numbers[index] = element * 2\n    else:\n        continue\nprint(numbers)\n# results: [2, 2, 6, 4, 10, 6, 14, 8, 18]\n\nenumerate also gets the index of the respective element at the same time\n\nWith three expressions\nfor (for i = 1; i &lt;= 10; i+=1)\n    &lt;loop body&gt;\nCollection-Based\n\nIf the collection is a dict, then this just iterates over the keys\n\nfor i in &lt;collection&gt;:\n    &lt;loop body&gt;\nIterate over a sliding window\n\nOver dictionary keys and values of a dict\nfor a,b in transaction_data.items():\n    print(a,’~’,b)\n\nThe .items method includes both key and value, so it iterates over the pairs.\n\nOver nested dictionaries\nfor k, v in transaction_data_n.items():\n    if type(v) is dict:\n        for nk, nv in v.items(): \n            print(nk,’ →’, nv)\n\nIf the item of the dict is itself a dict then another loop iterates through its items.\nnk and nv stand for nested key and nested value\n\nSelecting a specific item in a nested dictionary\nfor k, v in transaction_data_n.items():\n    if type(v) is dict and k == 'transaction_2':\n        for sk, sv in v.items():\n            print(sk,'--&gt;', sv)\n\nOnly transaction_2’ s items are printed\n\nRows of a data.frame\nres = 0\nfor row in df.itertuples():\n  res += getattr(row, 'int')\n\nitertuples()  is 30x faster than iterrows()\n\n\n\n\nzip\n\nCombine lists into 1 list of tuples\nacc_values = [1, 0.04, 0.9]\nacc_names = [\"RMSE\", \"MAPE\", \"R-sq\"]\nacc_list = list(zip(acc_names, acc_values))\nacc_list\n[('RMSE', 1), ('MAPE', 0.04), ('R-sq', 0.9)]\n\nzip does take lists of different lengths but will create shortest length list with corresponding elements\nCombine lists of unequal lengths but keep the non-paired elements\nfrom itertools import zip_longest\nacc_names3 = [\"RMSE\", \"MAPE\", \"R-sq\", \"MSE\"]\nacc_values3 = [rmse, mape, rsq] \nacc_list3 = list(zip_longest(acc_names3, acc_values3))\n\nUnzip list of tuples into separate lists\nnames, values = zip(*acc_list)\n\nAsterisk is the “unzipping operator”\n\nUnpack dict into a list of separate tuples for key:value pairs\nacc_tuples = list(zip(acc_dict.keys(), acc_dict.values()))\nacc_tuples\n[('RMSE', 1), ('MAPE', 0.04), ('R-sq', 0.9)]\n\n\n\nComprehensions\n\nMisc\n\n‘for — in’ construct within comprehensions is faster than the traditional for-loops\n\nnot faster than (all?) lambda-filters (see functions &gt;&gt; lambda)\n\nReturns lists or dicts (just change the bracket types)\n\nDicts\n\nSyntax: mydict = {key:val for key, val in zip(keys_list, vals_list)}\nCombine key:value lists into a dictionary\nacc_dict = {k:v for k,v in zip(acc_names, acc_values)}\nReturn value and output of expression\nmydict = {v: v**2 for v in numberslist}\nIf numberslist =[1,2,3], then mydict = {1:1, 2:4, 3:9}\n\nLists\n\nSyntax: newlist = [expression for item in iterable if condition == True]\nWith expression\nmylist = [x**2 for x in numberslist]\n\nif numberslist =[1,2,3], then mylist = [1,4,9]\n\nSet values in a list to uppercase\nnewlist = [x.upper() for x in fruits]\nWith conditional expression (if — else)\n\nAppend to the comprehension to filter the dictionary or list\nSyntax: mylist = [expressionA if (condition2==True) else expressionB for item in list if (condition1==True)]\nExample: newlist = [x if x != \"banana\" else \"orange\" for x in fruits]\n\nReturn “orange” instead of “banana”\n\nExample: new_list = [(x**2) if (x&gt;90) else (x**3) for x in old_list if (x%2==0)]\n\nSays\n\nSquare an argument if it exceeds 90, else cube it \nReturn all the exponentiated results only if the argument was an even number\n\n\nExample: c = [d for d in datstrlist if ((d.endswith(\"urday\") or d.endswith(\"unday\")) and \"Oc\" in d)]\n\nString filter than looks for strings with saturdays and sundays in october\n*Slower than a lamda-filter* (See Functions &gt;&gt; lambda)\n\n\n\nNested\n\nSyntax: myset = {{expression(itemA, itemB) for itemA in setA} for itemB in setB}\nExample: {j for i in range(2, int(N**0.5)+1) for j in range(i**2, N, i)}\n\nN = 100000\nCreates a set of all the integers from 2 to 100,000.\nPaces through all the integers i up to the square root of N\nDiscards from the set of 100,000 those numbers j which are equal or larger than the square of i\n\nExample: From link\n# Function to get set labels\ndef get_prediction_set_labels(prediction_set, class_labels):\n    # Get set of class labels for each instance in prediction sets\n    prediction_set_labels = [\n        set([class_labels[i] for i, x in enumerate(prediction_set) if x]) for prediction_set in \n        prediction_sets]\n    return prediction_set_labels\n\nReturns a list where each object in the list is a set object (e.g. {green}, {green, orange})",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-debug",
    "href": "qmd/python-general.html#sec-py-gen-debug",
    "title": "General",
    "section": "Debugging",
    "text": "Debugging\n\nMisc\nTerms\n\nException Errors - Raised when the syntax is correct but the program results in an error.\nSyntax Errors - Occur when the interpreter detects invalid syntax (relatively easier to fix)\n\ne.g. unmatched parenthesis\n\nTraceback - A report that helps us understand the reason for an exception.\n\nContains function calls made in the code along with their line numbers",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-errhand",
    "href": "qmd/python-general.html#sec-py-gen-errhand",
    "title": "General",
    "section": "Error Handling",
    "text": "Error Handling\n\ntry + except\n\nSays try the main code snippet, but if an exception (error) occurs, run the secondary code snippet, the workaround.\n\ndef pct_difference_error_handling(n1, n2):\n  '''Function that takes two numbers and return the percentual difference\n  between n1 and n2, being n1 the reference number'''\n\n  # Try the main code\n  try:\n    pct_diff = (n1-n2)/n1\n    return f'The difference between {n1} and {n2} is {n1-n2}, which is {pct_diff*100}% of {n1}'\n\n  # If you find an error, use this code instead\n  except:\n    pct_diff = (int(n1)-int(n2))/int(n1)\n    return f'The difference between {n1} and {n2} is {int(n1)-int(n2){style='color: #990000'}[}]{style='color: #990000'}, which is {pct_diff*100}% of {n1}'\n\n  # Optional\n  finally:\n    print(\"Code ended\")\n\nAssumes the error will be the user enters a string instead of a numeric. If errors, converts string to numeric and calcs.\nfinally: - This argument will always run, regardless if the try block raises an error or not. So it could be a completion message or a summary, for example.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/distributions.html",
    "href": "qmd/distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "Terms",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-terms",
    "href": "qmd/distributions.html#sec-distr-terms",
    "title": "Distributions",
    "section": "",
    "text": "Conditional Probability Distributions\n\nNotes from https://www.causact.com/joint-distributions-tell-you-everything.html#joint-distributions-tell-you-everything\nNotation: \\(P(Y | X) = P(Y \\;\\text{and}\\; X) / P(X) = P(Y, X) / P(X)\\)\n\ni.e. ratio of 2 marginal distributions\n\nExample: two tests for cancer are conducted to determine whether a biopsy should be performed\n\nConditional approach: Biopsy everyone at determined to be high risk from test 1; measure the genetic marker (aka test 2) for patients at intermediate risk and biopsy those with a probability of cancer past a certain level based on the marker\n\nWhen we perform regression analysis, we are essentially estimating conditional distributions. The conditional distribution, \\(P(Y|X_1, \\ldots, X_n)\\) represents the distribution of the response variable, \\(Y\\), given the specific values of the predictor variables, \\(X_1, \\ldots, X_n\\).\n\nEmpirical CDF\n\\[\nF_n (x) = \\frac {1}{n} \\sum_{i = 1}^n I(X_i \\leq x)\n\\]\n\nWhere \\(X_1, X_2,\\ldots,X_n\\) are from a population with CDF, \\(F_n (x)\\)\nProcess\n\nTake n samples from an unknown distribution. The more samples you take, the closer the empirical distribution will resemble the true distribution.\nSort these samples, and place them on the x-axis.\nStart plotting a ‘step-function’ style line — each time you encounter a datapoint on the x-axis, increase the step by 1/N.\n\nExample\n\n\nThe CDF of a normal distribution (green) and its empirical CDF (blue)\n\n\nJoint Probability Distribution - Assigns a probability value to all possible combinations of values for a set of random variables.\n\nNotation: \\(P(x_1, x_2, ... ,x_n)\\)\nPlugging in a value for each random variable returns a probability for that combination of values\nExample: Two tests for cancer are conducted to determine whether a biopsy should be performed\n\nJoint approach: biopsy anyone who is either at high risk of cancer (test 1) or who was determined to have a probability of cancer past a certain level, based on the marker from the genetic test (test 2)\nCompare with example in Conditional Probability Distributions\n\nIn the context of regression modeling: the joint distribution refers to the distribution of all the variables involved in the regression analysis. For example, if you have a regression model with a response variable \\(Y\\) and predictor variables \\(X_1, \\ldots, X_n\\), the joint distribution would describe the combined distribution of \\(Y, X_1, \\ldots, X_n\\).\n\nLocation - Distribution parameter determines the shift of the distribution\n\ne.g. mean, mu, of the normal distribution.\n\nMarginal Probability Distribution - Assigns a probability value to all possible combinations of values for a subset of random variables\n\nNotation: \\(P(x_1)\\)\n\n\\(P(x_1,x_2)\\) is sometimes called the Joint Marginal Probability Distribution\n\nThe marginal distribution, \\(P(Y)\\) where \\(Y\\) is a subset of random variables, is calculated from the joint distribution, \\(P(Y = y, Z = z)\\) where \\(Z\\) is the subset of random variables not in \\(Y\\) .\n\n\\(P(Y) = \\sum_{Z=z} P(Y = y, Z = z)\\)\n\nIf \\(Y\\) is just one variable\n\nSays sum all the joint probabilities for all the combinations of values for the variables in \\(Z\\) while holding \\(Y\\) constant\nRepeat for each value of \\(Y\\) to get this summed probability value\nThe marginal distribution is made up of all these values, one for each value of \\(Y\\) (or combination of values if \\(Y\\) is a subset of variables)\n\n\nWhen the joint probability distribution is in tabular form, one just sums up the probabilities in each row where \\(Y = y\\).\nIn the context of regression modeling, the marginal distribution of \\(Y\\) represents the distribution of \\(Y\\) alone, without considering the specific values of the predictor variables.\n\n\nScale - Distribution parameter; the larger the scale parameter, the more spread out the distribution\n\ne.g. s.d., sigma, \\(\\sigma\\) of the normal distribtution\nRate Parameter: the inverse of the scale parameter (see Gamma distribution)\n\nShape - Distribution parameter that affects the shape of a distribution rather than simply shifting it (as a location parameter does) or stretching/shrinking it (as a scale parameter does).\n\ne.g. “Peakedness” refers to how round the main peak is",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-tests",
    "href": "qmd/distributions.html#sec-distr-tests",
    "title": "Distributions",
    "section": "Tests",
    "text": "Tests\n\nWhy normality tests are great… as a teaching example and should be avoided in research\n\ntl;dr; KS test has very low power as a Normality test as compared to Shapiro-Wilk, and Shapiro-Wilk isn’t very good for n &lt; 100\nFor detecting moderate skew, you want at least n &gt; 75 to get 80% power for Shapiro-Wilk\nShapiro-Wilk can detect very fat tails at n &lt; 100, but would require larger sample sizes to detect more moderately thick tails.\nKS is worthless in detecting fat tails and near-worthless at detecting skew\nWhen n gets large (e.g. 1000s), these types of tests will almost always reject the null even when the practical deviation from normality is not practically significant.\n\nKolmogorov–Smirnov test (KS)\n\nUsed to compare distributions\n\nCan be used as a Normality test or any distribution test\nCan compare two samples\n\nMisc\n\nVectors may need to be standardized (e.g. normality test) first unless comparing two samples H0: Both distributions are from the same distribution\n\nPackages\n\n{KSgeneral} has tests to use for contiuous, mixed, and discrete distributions written in C++\n{stats} and {dgof} also have functions, ks.test\n\nBoth handle continuous and discrete distributions\n\nAll functions take a numeric vector and a base R density function (e.g. pnorm, pexp, etc.) as args\n\nKSgeneral docs don’t say you can supply your own comparison sample (2nd arg) only the density function but with stats and dgof, you can.\nAlthough they have function to compute the CDFs, so if you need speed, it might be possible to use their functions and do it manually\n\n\n2-sample test as the greatest distance between the CDFs (Cumulative Distribution Function) of each sample\n\nSpecifically, this test determines the distribution of your unknown data sample by constructing and comparing the sample’s empirical CDF  (see Terms) with the CDF you hypothesized. If the two CDFs are close, your unknown data sample likely follows the hypothesized distribution.\n\nKS statistic, \\(D_{n,m} = \\max|\\text{CDF}_1 - \\text{CDF}_2|\\) where \\(n\\) as the number of observations on Sample 1 and \\(m\\) as the number of observations in Sample 2\nCompare the KS statistic with the respective KS distribution based on parameter “en” to obtain the p-value of the test\n\n\\(en = (m \\times n) / (m + n)\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-beta",
    "href": "qmd/distributions.html#sec-distr-beta",
    "title": "Distributions",
    "section": "Beta",
    "text": "Beta\n\n\nDefined on the interval [0,1]\nThe key difference between the Binomial and Beta distributions is that for the Beta distribution the probability, x, is a random variable, however for the Binomial distribution the probability, x, is a fixed parameter.\nShape parameters are \\(\\alpha\\) and \\(\\beta\\), usually.\n\n\\(\\alpha\\) and \\(\\beta\\) are two positive parameters that appear as exponents of the random variable\n\npdf\n\\[\nf(x) = \\frac {x^{\\alpha - 1} (1-x)^{\\beta - 1}} {B(\\alpha, \\beta)}\n\\]\n\\(\\mathbb{E}(X) = \\frac {\\alpha} {\\alpha + \\beta}\\)\n\\(\\text{Var}(X) = \\frac {\\alpha \\cdot \\beta} {(\\alpha + \\beta)^2 \\cdot (\\alpha + \\beta + 1)}\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-betbin",
    "href": "qmd/distributions.html#sec-distr-betbin",
    "title": "Distributions",
    "section": "Beta-Binomial",
    "text": "Beta-Binomial\n\n\n\n\n\n\n\n\nWhere k is the number of events in n trials\n\n\n\n\n\n\n\nWhere \\(\\theta\\) is the probability of an event\n\n\n\n\n\n\n\nUsed when the probability of success, p, in a fixed number of Bernoulli trials is unknown or random and can change from trial to trial.\nShape parameters α and β define the probability of success (i.e. the success parameter is modeled by the Beta Distribution).\n\nFor large values of α and β, the distribution approaches a binomial distribution.\nWhen α and β both equal 1, the distribution equals a discrete uniform distribution from 0 to n\n\nAccuracy analysis data from psychology follow beta-binomial distributions (Jaeger, 2008; Kruschke, 2014)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-dirichlet",
    "href": "qmd/distributions.html#sec-distr-dirichlet",
    "title": "Distributions",
    "section": "Dirichlet",
    "text": "Dirichlet\n\nA family of continuous multivariate probability distributions parameterized by a vector α of positive reals",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-exp",
    "href": "qmd/distributions.html#sec-distr-exp",
    "title": "Distributions",
    "section": "Exponential",
    "text": "Exponential\n\n\nNotes from\n\nStatistical Rethinking &gt;&gt; Chapter 10\n\nConstrained to be zero or positive\nFundamental distribution of distance and duration, kinds of measurements that represent displacement from some point of reference, either in time or space.\nIf the probability of an event is constant in time or across space, then the distribution of events tends towards exponential.\nIts shape is described by a single parameter, the rate of events \\(\\lambda\\), or the average displacement \\(\\lambda −1\\) .\nThis distribution is the core of survival and event history analysis",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gamma",
    "href": "qmd/distributions.html#sec-distr-gamma",
    "title": "Distributions",
    "section": "Gamma",
    "text": "Gamma\n\n\nNotes from\n\nStatistical Rethinking &gt;&gt; Chapter 10\n\nConstrained to be zero or positive\nLike Exponential but can have a peak above zero\nIf an event can only happen after two or more exponentially distributed events happen, the resulting waiting times will be gamma distributed.\n\ne.g. age of cancer onset is approximately gamma distributed, since multiple events are necessary for onset.\n\nThe gamma can be viewed as the sum of iid n exponential random variables. Exponential random variables have a rate parameter, so it makes sense for the Gamma to inherit a rate parameterization. The rate parameter also happens to be related to a scale parameter, so it makes sense for the Gamma to have a scale parameterization.\nShape parameter \\(k\\) and a scale parameter \\(\\theta\\)\n\\(\\mathbb{E}[X] = k\\theta = \\frac{\\alpha}{\\beta}\\)\n\nShape parameter \\(\\alpha = k\\) and an\nInverse Scale parameter (aka Rate Parameter) \\(\\beta = \\frac {1}{\\theta}\\)\nTherefore if you want a gamma distributions with a certain “mean” and “standard deviation,” you’d:\n\nSet your mean to \\(\\mathbb{E}[X]\\), your standard deviation to \\(\\theta\\) (probably but maybe it’s \\(\\beta\\))\nCalculate \\(\\beta\\)\nCalculate \\(\\alpha\\)\nprior(gamma(alpha, beta))\n\n\nExample: Gamma distribution as the sums of random exponential variables\n\nn &lt;- 12\nbeta &lt;- 1.2\n\nrvs &lt;- replicate(1000, {\n  sum(rexp(n, beta))\n})\n\nhist(rvs, freq = F)\ncurve(dgamma(x, shape = n, rate = beta), col='red', add=T)\n\nGamma distribution density overlayed with a histogram of exponential variable sums\n\nUsed in Survival Regression",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gauss",
    "href": "qmd/distributions.html#sec-distr-gauss",
    "title": "Distributions",
    "section": "Gaussian",
    "text": "Gaussian\n\nSpecial case of Student’s t-distribution with the \\(\\nu\\) parameter (i.e. degree of freedom) set to infinity.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gumb",
    "href": "qmd/distributions.html#sec-distr-gumb",
    "title": "Distributions",
    "section": "Gumbel",
    "text": "Gumbel\n\n\nKnown as the type-I generalized extreme value distribution\n\nEVT says it is likely to be useful if the distribution of the underlying sample data is of the normal or exponential type.\n\nUsed to model the distribution of the maximum (or the minimum) of a number of samples of various distributions.\n\nTo model minimums, use the negative of the original data.\n\nUse Cases\n\nRepresent the distribution of the maximum level of a river in a particular year if there was a list of maximum values for the past ten years.\nPredicting the chance that an extreme earthquake, flood or other natural disaster will occur.\nDistribution of the residuals in Multinomial Logit and Nested Logit models\n\nParameters\n\nGumbel(\\(\\mu, \\beta\\)) (location, scale)\nMean: \\(\\mu + \\beta\\gamma\\) where \\(\\gamma\\) is Euler’s constant (\\(\\approx\\) 0.5772)\nMedian: \\(\\mu - \\beta \\ln(\\ln(2))\\)\nMode: \\(\\mu\\)\nVariance: \\(\\frac{\\pi^2}{6}\\beta^2\\)\nStandard Gumbel: When \\(\\mu = 0\\), mean = \\(\\gamma\\), median = \\(-\\ln(\\ln(2)) \\approx 0.3665\\) and the standard deviation = \\(\\pi/\\sqrt{6}\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-multgauss",
    "href": "qmd/distributions.html#sec-distr-multgauss",
    "title": "Distributions",
    "section": "Multivariate Gaussian",
    "text": "Multivariate Gaussian\n\nIf the random variable components in the vector are not normally distributed themselves, the result is not multivariate normally distributed.\nVariance-Covariance matrix must be semi-definite and therefore symmetric\n\nExample of not symmetric for two random variables",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-pareto",
    "href": "qmd/distributions.html#sec-distr-pareto",
    "title": "Distributions",
    "section": "Pareto",
    "text": "Pareto\n\nAlso see Extreme Value Theory &gt;&gt; Distribution Tail Classification\n“Gaussian distributions tend to prevail when events are completely independent of each other. As soon as you introduce the assumption of interdependence across events, Paretian distributions tend to surface because positive feedback loops tend to amplify small initial events.”\nPareto has similar relationship with the exponential distribution as lognormal does with normal \\[\nY_{exp} = \\log \\frac {X_{pareto}} {x_m}\n\\]\n\nWhere \\(X_{pareto} = x_m e^{Y_{\\text{exp}}}\\)\n\n\\(x_m\\) is the (positive) minimum of the randomly distributed pareto variable, X that has index α\n\\(Y_{exp}\\) is exponentially distributed with rate \\(\\alpha\\)\n\n\nSome theoretical statistical moments may not exist\n\nIf the theoretical moments do not exist, then calculating the sample moments is useless\nExample: Pareto (\\(\\alpha\\) = 1.5) has a finite mean and an infinite variance\n\nNeed \\(\\alpha &gt; 2\\) for a finite variance\nNeed \\(\\alpha &gt; 1\\) for a finite mean\nIn general you need \\(\\alpha &gt; p\\) for the pth moment to exist\nIf the nth moment is not finite, then the (n+1)th moment is not finite.\n\n\nFat Tails \\[\n\\bar{F} = x^{-\\alpha} L(x)\n\\]\n\n\\(L(x)\\) is just characterized as slowly varying function that gets dominated by the decaying inverse power law element, \\(x-\\alpha\\). as \\(x\\) goes to infinity\n\n\\(\\alpha\\) is a shape parameter, aka “tail index” aka “Pareto index”",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-poisson",
    "href": "qmd/distributions.html#sec-distr-poisson",
    "title": "Distributions",
    "section": "Poisson",
    "text": "Poisson\n\nObtained as the limit of the binomial distribution when the number of attempts is high and the success probability low. Or the Poisson distribution can be approximated by a normal distribution when λ is large\nProbability Mass Function \\[\n\\text{Pr}(Y = y) = f(y; \\lambda) = \\frac {e^{-\\lambda} \\cdot \\lambda^y} {y!}\n\\]\n\n\\(\\mathbb{E}[Y] = \\text{Var}(Y) = \\lambda\\)\n\n{distributions3}\n\nStats\nY &lt;- Poisson(lambda = 1.5) \nprint(Y) \n## [1] \"Poisson distribution (lambda = 1.5)\"\n\nmean(Y) \n## [1] 1.5 \nvariance(Y) \n## [1] 1.5 \npdf(Y, 0:5) \n## [1] 0.22313 0.33470 0.25102 0.12551 0.04707 0.01412 \ncdf(Y, 0:5) \n## [1] 0.2231 0.5578 0.8088 0.9344 0.9814 0.9955 \nquantile(Y, c(0.1, 0.5, 0.9)) \n## [1] 0 1 3 \nset.seed(0) \nrandom(Y, 5) \n## [1] 3 1 1 2 3\n\nVisualize\n\nplot(Poisson(0.5), main = expression(lambda == 0.5), xlim = c(0, 15)) \nplot(Poisson(2),   main = expression(lambda == 2),   xlim = c(0, 15)) \nplot(Poisson(5),   main = expression(lambda == 5),   xlim = c(0, 15)) \nplot(Poisson(10),  main = expression(lambda == 10),  xlim = c(0, 15))",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-studt",
    "href": "qmd/distributions.html#sec-distr-studt",
    "title": "Distributions",
    "section": "Student’s t-distribution",
    "text": "Student’s t-distribution\n\nStandard Deviation\n\\[\n\\text{sd} = \\sqrt {\\frac {\\nu} {\\nu - 2}}\n\\]\n\n\\(\\nu\\) = degrees of freedom\n\nWhen ν is small, the Student’s t-distribution is more robust to multivariate outliers\nThe smaller the degree of freedom, the more “heavy-tailed” it is\n\n\n-3 on the y-axis says that the probability of being in the tail is 1 in 103\n\nDon’t pay attention to the x-axis. Just note how much the probability of being in the tail gets larger as the dof get smaller\n\nAs the degrees of freedom goes to 1, the t distribution goes to the Cauchy distribution\nAs the degrees of freedom goes to infinity, it goes to the Normal distribution.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-tri",
    "href": "qmd/distributions.html#sec-distr-tri",
    "title": "Distributions",
    "section": "Triangular",
    "text": "Triangular\n\nTriangle shaped distribution\nUseful when you have a known min and max value\nextraDistr::rtriang(n, a, b, c) %\\&gt;% hist()\n\n# Discrete distribution\nextraDistr::rtriang(n, a, b, c) %\\&gt;% round() \\`\\`\\`\n\nn is the number of random values you wish to draw\na is the min value\nb is the max value\nc is the mode\n\nCan use to adjust the skew of the distribution\n\n\n\n\n\n\nWhere k is the number of events in n trials\nWhere \\(\\theta\\) is the probability of an event",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/mathematics-glossary.html",
    "href": "qmd/mathematics-glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "A priori - a type of knowledge that can be derived by reason alone\n\nA priori analyses are performed as part of the research planning process.\n\nA posteriori - a type of knowledge that expresses an empirical fact unknowable by reason alone.\n\nSame as post-hoc. Post-Hoc analysis is conducted after the experiment.\n\nbias - see unbiased estimator\nceteris paribus - latin for “all things being equal” or “other things held constant.”\nclosed form - a mathematical expression that uses a finite number of standard operations. It may contain constants, variables, certain well-known operations, and functions, but usually no limit, differentiation, or integration.\nconsistency - Requires that the outcome of the procedure with unlimited data should identify the underlying truth. Usage is restricted to cases where essentially the same procedure can be applied to any number of data items. In complicated applications of statistics, there may be several ways in which the number of data items may grow. For example, records for rainfall within an area might increase in three ways: records for additional time periods; records for additional sites with a fixed area; records for extra sites obtained by extending the size of the area. In such cases, the property of consistency may be limited to one or more of the possible ways a sample size can grow\ndegrees of freedom - When discussed about variable-sample size tradeoff, usually means n-p, where is the number of rows and p is the number of variables. The more variables used in the model the fewer degrees of freedom and therefore less power and precision.\nexchangeability - means we can swap around, or reorder, variables in the sequence without changing their joint distribution.\n\nEvery IID (independent, identically distributed) sequence is exchangeable - but not the other way around. Every exchangeable sequence is identically distributed, though\n\nExample: If you draw a sequence of red and blue marbles from a bag without replacement, the sample is exchangeable but not independent. e.g. drawing a red marble affects the probability of drawing a red or blue marble next.\n\n\nefficiency - A test, estimator, etc. is more efficient than another test, estimator, etc. if it requires fewer observation to obtain the same level of performance.\nergodicity - the idea that a point of a moving system, either a dynamical system or a stochastic process, will eventually visit all parts of the space that the system moves in, in a uniform and random sense\nexternal validity - Our estimates are externally valid if inferences and conclusions can be generalized from the population and setting studied to other populations and settings. (also see internal validity)\nidentifiable (aka point-indentifiable) - theoretically possible to learn the true values of this model’s underlying parameters after obtaining an infinite number of observations from it (see non-identifiability, partially-indentifiable)\nill-conditioned - In SVD decomposition, when there’s a huge difference between largest and smallest eigenvalue of the original matrix, A, the ratio of which is called condition number.\ninternal validity - our estimates are internally valid if statistical inferences about causal effects are valid for the population being studied. (also see external validity)\nintractable - problems for which there exist no efficient algorithms to solve them. Most intractable problems have an algorithm – the same algorithm – that provides a solution, and that algorithm is the brute-force search\nlocality - effects have causes and chains of cause and effect must be unbroken in space and time (not the case in ‘entanglement’)\nmarginalization - The process of eliminating one or more variables from a joint probability distribution or a multivariate statistical model to obtain the distribution or model for a subset of variables. The resulting distribution or model is called a marginal distribution or marginal model. It allows you to focus on the behavior of specific variables while considering the uncertainty associated with others.\n\nFor example, marginalizing over a joint distribution (i.e. many variables) gets you a marginal distribution (i.e. fewer variables). In other words, if you have a joint probability distribution for two variables \\(X\\) and \\(Y\\), the marginal distribution of \\(X\\) is obtained by summing or integrating over all possible values of \\(Y\\). Similarly, the marginal distribution of \\(Y\\) is obtained by summing or integrating over all possible values of \\(X\\).\nNotation: \\(P(X) = \\sum_Y P(X,Y) \\;\\text{or}\\; P(X) = \\int P(X,Y)\\;dY\\)\nOnce you have to the marginal distribution, this allows you compute conditional distributions. For example, after obtaining the marginal distribution, \\(P(X,Y)\\), from the joint distribution, \\(P(X,Y,Z)\\), you can compute the conditional distributions, \\(P(X|Y)\\) and \\(P(Y|X)\\).\nThe uncertainty associated with \\(Z\\) is indirectly considered in the sense that the marginal distribution \\(P(X,Y)\\) accounts for all possible values of \\(Z\\) by integrating over them. However, \\(P(X,Y)\\) itself doesn’t provide explicit information about the uncertainty associated with \\(Z\\).\n\nnon-identifiability - the structure of the data and model do not make it possible to estimate the parameter’s value. Multicollinearity is a type of non-identifiability problem. (i.e. two or more parametrizations of the model are observationally equivalent) (see identifiable, partially-indentifiable)\noverdetermined system - In linear regression, when there are more observations than features, n &gt; p\npartial coefficient - The coefficient of a variable in a multivariable regression. In a simple regression, the coefficient of the variable is just called the “regression coefficient.”\npartially-indentifiable (aka set identifiable) - non-identifiable but possible to learn the true values of a certain subset of the model parameters\nrobust - a “robust” estimator in statistics is one that is insensitive to outliers, whereas a “robust” estimator in econometrics is insensitive to heteroskedasticity and autocorrelation (hyndman)\nsupport (aka range) - the set of values that the random variable can take.\n\nFor discrete random variables, it is the set of all the realizations that have a strictly positive probability of being observed.\nFor continuous random variables, it is the set of all numbers whose probability density is strictly positive.\nSee link for examples\n\nunderspecification - In general, the solution to a problem is underspecified if there are many distinct solutions that solve the problem equivalently.\nAn unbiased estimator is an accurate statistic that’s used to approximate a population parameter.\n\n“Accurate” in this sense means that it’s neither an overestimate nor an underestimate. If an overestimate or underestimate does happen, the mean of the difference is called a “bias.”\n\nWeak Law of Large Numbers (Bernoulli’s theorem) - states that if you have a sample of independent and identically distributed random variables, as the sample size grows larger, the sample mean will tend toward the population mean",
    "crumbs": [
      "Mathematics",
      "Glossary"
    ]
  },
  {
    "objectID": "qmd/git-general.html",
    "href": "qmd/git-general.html",
    "title": "25  General",
    "section": "",
    "text": "25.1 Misc",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#misc",
    "href": "qmd/git-general.html#misc",
    "title": "25  General",
    "section": "",
    "text": "View HTML file in browser\n\nSyntax: “https://raw.githack.com/&lt;acct name&gt;/&lt;repo name&gt;/&lt;branch name&gt;/&lt;directory name&gt;/&lt;file name&gt;.html”\n\nInstalling from a git repo (From link)\n\nMake a fork of the repo and then clone it to your local machine.\nTo update, after setting an upstream remote (git remote add upstream git://github.com/benfulcher/hctsa.git) you can use git pull upstream main.\nTo update the submodule in the repo, git submodule update --init\n\nStart R project and Git repo in whichever order (I think)\n\nCreate R project in RStudio\n\nChoose “New Directory” for all the templated projects (e.g. quarto book, shiny, etc.). None of the other choices have them.\n\nIf you’ve already created a directory, it will NOT overwrite this directory or add to it. So you’ll either have alter the name of your old directory or choose a new name.\n\n\nCreate repo on Github\n\nAdd license and readme\n\nDo work\nTools &gt;&gt; Version Control &gt;&gt; Project Set-up &gt;&gt; Version Control System &gt;&gt; Select Git\nOpen terminal and go to working directory of project\ngit checkout -B main\ngit pull origin main --allow-unrelated-histories\ngit add .\ngit commit -m \"initial commit\"\ngit push --set-upstream origin main \n\nTurn off “LF will be replaced by CRLF the next time Git touches it”\n\nMessage spams terminal when committing changes from a window machines. Has to do with line endings in windows vs unix.\nTurn off: git config core.autocrlf true\nSee SO post for more details\n\nURL format to download files from repositories\n\nhttps://raw.githubusercontent.com/user/repository/branch/filename\n\n# Or evidently this way works too\n# adds ?raw=true to the end of the url\nfeat_all_url &lt;- url(\"https://github.com/notast/hierarchical-forecasting/blob/main/3feat_all.RData?raw=true\")\nload(feat_all_url)\nclose(feat_all_url)\nGet filelist from repo and download to a directory\n\n** Directory urls change as commits are made **\n\nlibrary(httr)\n\n# example: get url for the data dir of covidcast repo\nreq &lt;- httr::GET(\"https://api.github.com/repos/ercbk/Indiana-COVIDcast-Dashboard/git/trees/master?recursive=1\") %&gt;% \n  httr::content()\n# alphabetical order\ntrees &lt;- req$tree %&gt;% \n  map(., ~pluck(.x, 1)) %&gt;% \n  as.character()\n# returns 20 which is first instance, so 19 should the \"data\" folder\ndetect_index(trees, ~str_detect(., \"data/\"))\n# url for data dir\nreq$tree[[19]]$url\n\n# example\n# Get all the file paths from a repo\nreq &lt;- GET(\"https://api.github.com/repos/etiennebacher/tidytuesday/git/trees/master?recursive=1\")\n# any request errors get printed\nstop_for_status(req)\nfile_paths &lt;- unlist(lapply(content(req)$tree, \"[\", \"path\"), use.names = F)\n# file_path wanted &lt;- filter file path to file you want\n# gets the very last part of the path\nfile_wanted &lt;- basename(file_path_wanted)\norigin &lt;- paste0(\"https://raw.githubusercontent.com/etiennebacher/tidytuesday/master/\", file_wanted)\ndestination &lt;- \"output-path-with-filename-ext\"\n# if file doesn't already exist, download it from repo into destination\nif (!file.exists(destination)) {\n      # if root dir doesn't exist create it\n      if (!file.exists(\"_gallery/img\")) {\n        dir.create(\"_gallery/img\")\n      }\n      download.file(origin, destination)\nThe insides of .git",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#config-options",
    "href": "qmd/git-general.html#config-options",
    "title": "25  General",
    "section": "25.2 Config Options",
    "text": "25.2 Config Options\n\nNotes from: Popular git config options - More options listed that are not presented here.\nSetting Options\n\nAdd via CLI: git config --global &lt;name&gt; &lt;value&gt;\n\nExample: git config --global diff.algorithm histogram\n\nDelete by going into ~/.gitconfig and delete the parameter and value\n\nmerge.conflictstyle diff3 - Provides extra information on merge conflicts\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\ndef parse(input):\n    return input.split(\"\\n\")\n||||||| b9447fc\ndef parse(input):\n    return input.split(\"\\n\\n\")\n=======\ndef parse(text):\n    return text.split(\"\\n\\n\")\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; somebranch\n\nBelow &lt;&lt;&lt;&lt;&lt;&lt; HEAD: This is your local code that you’re trying to push\nBetween |||||||| b9447fc and =======: This is the original version of the code\nAbove &lt;&lt;&lt;&lt;&lt;&lt; somebranch: This is code from the branch that got merged before yours (I think)\nTherefore, the correct merge conflict resolution is return text.split(\"\\n\"), since that combines the changes from both sides.\n\nmerge.conflictstyle zdiff3 - A newer version of merge.conflictstyle diff3\nA\nB\nC\nD\nE\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; ours\nF\nG\n||||||| base\n# Add More Letters\n=======\nX\nY\nZ\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; theirs\n\nAbove &lt;&lt;&lt;&lt;&lt;&lt; ours: This is the original code plus the code that belongs to the branch that got merged that is not in conflict with your code\nBelow &lt;&lt;&lt;&lt;&lt;&lt; ours: This is the code that is in conflict with the branch (e.g. main) your merging into.\nBelow |||||||| base: This is the code that has been removed from the original code for both mergers\nAbove &lt;&lt;&lt;&lt;&lt;&lt; theirs: This is code for another branch that was merged before yours that is in conflict with your code.\n\npush.default current - Says that when using git push to always push the local branch to a remote branch with the same name.\n\npush.default simple is the default in Git. Means git push only works if your branch is already tracking a remote branch.\nI guess it’s possible to push a local branch to a remote branch of a different name.\n\ninit.defaultBranch main - Create a main branch instead of a master branch when creating a new repo. I normally do this on Github.\ncommit.verbose true - This adds the whole commit diff in the text editor where you’re writing your commit message, to help you remember what you were doing.\nrerere.enabled true - This enables rerere (”reuse recovered resolution”), which remembers how you resolved merge conflicts during a git rebase and automatically resolves conflicts for you when it can.\ncore.pager delta - The “pager” is what git uses to display the output of git diff, git log, git show, etc.\n\nValues:\n\ndelta: A fancy diff viewing tool with syntax highlighting\nless -x5,9 - Sets tabstops, which I guess helps if you have a lot of files with tabs in them?\nless -F -X - Not sure about this one, -F seems to disable the pager if everything fits on one screen if but her git seems to do that already anyway\ncat - To disable paging altogether\n\nDelta also suggests that you set up interactive.diffFilter delta –color-only to syntax highlight code when you run git add -p.\n\ndiff.algorithm histogram - Improves the Patience algorithm for presenting diffs. See link in article for more details.\n\nDefault (I think the default algorithm is Myers.)\n-.header {\n+.footer {\n     margin: 0;\n }\n\n-.footer {\n+.header {\n     margin: 0;\n+    color: green;\n }\n\nfooter didn’t actually have margin: 0 and color: green in the original code like this diff makes it seem. In reality, the two rules have switched order with header gaining the additional property, color: green.\n\nHistogram\n-.header {\n-    margin: 0;\n-}\n-\n .footer {\n     margin: 0;\n }\n\n+.header {\n+    margin: 0;\n+    color: green;\n+}\n\nThis shows header’s old rule without color: green at the top and being removed. footer is accurately depicted as unchanged. Then, it shows header with the addtional property, color: green, added below footer.\n\n\nincludeIf - Allows you to use different options depending which directory your project is in.\n\nExample: Use this config file only if you’re in the “work” directory\n[includeIf \"gitdir:~/code/&lt;work&gt;/\"]\n    path = \"~/code/&lt;work&gt;/.gitconfig\"\n\nGood if, for example, you want to have a work email set for work repos and personal email for set for personal repos\n\n\ninsteadOf - Useful to correct little mistakes often you make\n\nSee article for other usecases\nExample: If you accidently clone using http when you want to use SSH\n[url \"git@github.com:\"]\n    insteadOf = \"https://github.com/\"\n\nNow when you accidently clone a repo using the http address, it’ll change it to the ssh address in .git/config. Now you’ll be using ssh to push changes which is more secure.\n\n\nSubmodules\nstatus.submoduleSummary true\ndiff.submodule log\nsubmodule.recurse true\n\nSee thread for details\nThe top two “make git status and git diff display some more useful information on how things differ in submodules.”\nThe bottom one aids in the updating of submodules when switching branches\n\ndiff.colorMoved default - Uses different colours to highlight lines in diffs that have been “moved”\n\ndiff.colorMovedWS allow-indentation-change - With diff.colorMoved set, also ignores indentation changes\n\ngpg.format ssh - Allows you to sign commits with SSH keys\nmerge.tool meld (or nvim, or nvimdiff) - Enables use git mergetool to help resolve merge conflicts",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#optimizations",
    "href": "qmd/git-general.html#optimizations",
    "title": "25  General",
    "section": "25.3 Optimizations",
    "text": "25.3 Optimizations\n\nFor large repos, simple actions, like running git status or adding new commits can take many seconds. Cloning repos can take many hours.\nBenefits\n\nIt improves the overall performance of your development workflow, allowing you to work more efficiently. This is especially important when working with large organizations and open source projects, where multiple developers are constantly committing changes to the same repository. A faster repository means less time waiting for Git commands such as git clone or git push to finish. It helps to optimize the storage space, as large files are replaced by pointers which take up less space. This can help avoid storage issues, especially when working with remote servers.\n\nMisc\n\nSee How to Improve Performance in Git: The Complete Guide\n\nExplainer, config settings, advanced gc, checkout, and clone commands\n\n\nUse .gitignore\n\nGenerated files, like cache or build files\n\nThey will be modified at each different generation — and there’s no need to keep track of those changes.\n\nThird-party libraries\n\nInstead, aim for a list of the required dependencies (and the correct version) so that everyone can download and install them whenever the repo is cloned.\n\nFor example, with a package.json file for JavaScript projects you can (and should) exclude the /node_modules folder.\n.DS_Store files (which are automatically created by macOS) are another good candidate\n\n\n\nGit LFS\n\nDesigned specifically to handle large file versioning. LFS saves your local repositories from becoming unnecessarily big, preventing you from downloading unnessary data.\n\nGit LFS intercepts any large files and sends them to a separate server, leaving a smaller pointer file in the repository that links to the actual asset on the Git LFS server.\n\nThis is an extension to the standard Git feature set, so you will need to make sure that your code hosting provider supports it (all the popular ones do).\nAlso need to download and install the CLI extension on your machine before installing it in your repository.\nSet-Up\n$ git lfs install\n$ git lfs track \"*.wav\"\n$ git lfs track \"images/*.psd\"\n$ git lfs track \"videos\"\n$ git add .gitattributes\n\nTells Git LFS which file extensions it should manage.\n.gitattributes notes the file names and patterns in this text file and, just like any other change, it should be staged and committed to the repository.\nCan now add files and commit as normal\nList all file extensions being tracked: git lfs track\nList all files being managed: git lfs ls-files\n\n\nDon’t download the version history if you don’t need to\n\ngit clone –depth 1 gitj@github.com:name/repo.git",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#troubleshooting",
    "href": "qmd/git-general.html#troubleshooting",
    "title": "25  General",
    "section": "25.4 Troubleshooting",
    "text": "25.4 Troubleshooting\n\nDiverged Branches\n\n\nKeeps asking for username/password when pushing\n\nSolution: You (or if you used usethis::use_github/git) probably set-up a https connection when you need a ssh connection.\n\nsee https://docs.github.com/en/get-started/getting-started-with-git/managing-remote-repositories#changing-a-remote-repositorys-url to change from https to ssh.\n\n\nUndo a commit, but save changes made (e.g. you forgot to pull before you pushed)\n\nSteps\n\ngit log - Shows commit history. Copy the hash for your last commit\ngit diff &lt;last commit hash&gt; &gt; patch - save the diff of the latest commit to a file\ngit reset --hard HEAD^ to revert to the previous commit\n\n**After this, your changes will be lost locally **\n\ngit log - confirm that you are now at the previous commit\ngit pull - correct the mistake you made in first place\npatch -p1 &lt; patch - apply the changes you originally made\ngit diff - to confirm that the changes have been reapplied\nNow, you do the regular commit, push routine\n\n\nUndo uncommitted changes: git stash followed by git stash drop\n\n“but only use if you commit often” - guessing this is not good if your commit is somehow large and/or involves multiple files\n\nSearch commits by string: git log --grep &lt;string&gt;\nPinpoint bugs in your commit history\n\nInstead of sequentially searching each previous commit to look for the bad commit, git bisect helps you perform a bisect search for the commit which saves time.\nScenario: A bug is introduced in a codebase, but it is not discovered until later. The feature used to work, but now, it does not. The feature was definitely known to work 3 weeks ago.\nManual Workflow\n\nMake sure you’re in the current commit that’s bad and start git bisect\ngit bisect start\n1git bisect bad\n2git log --before=\"3 weeks\"\n3git checkout 3348b0\n\n1\n\nThis labels the current commit as bad (i.e. bug is present)\n\n2\n\nThis lists every commit for last 3 weeks\n\n3\n\nSwitch to the commit that’s the version of the project that was 3 weeks ago when supposedly the feature was working. The first commit listed (i.e. top) will be the commit closest to 3 weeks ago — with older commits below it. You only need to use the first 6 or so digits of the commit hash.\n\n\nRecompile code and test commit for bug\ndevtools::load_all()\n\nload_all will recompile your package using this current version’s code\nAfter recompiling code, use your reproducible examplet to see if the bug is present in this version\nIf the bug is stil present, then go to the next older commit and repeat process. Keep loading older commits until you find one that doesn’t have the bug.\n\nIf ths is the case and assuming you don’t have to go back too much further to find a “good” commit, then you can stop here since you’ll have found the bad commit that introduced the bug.\nIf you don’t find a good commit around this time period, then quit the current git bisect session using git bisect reset and choose whichever commit you stop at as the new starting point for a new git bisect session and repeat this whole workflow.\n\n\nGo to terminal and mark this commit as good\ngit bisect good\n\nGit will automatically switch you to commit that’s the midway point between the “start” commit and the commit you labeled as “good.”\nIt tells you how many commits that are currently between you and the “start” commit which is the same amount as between this midway commit and the commit you labelled as “good.\nIt also tells you how many more bisections (“steps”) you’ll have to go through to find the commit resposible for the bug.\n\nRepeat Step 2 and test verstion for the bug. Then label commit as good or bad\ngit bisect bad\n\nAfterwards, git will automatically checkout to the commit that is either midway between this commit and “start” or the end commit based on whether you label this current commit as good or bad.\n\nContinue labelling commits until git’s message is “&lt;some commit hash&gt; is the first bad commit.”\n\nGit will also show you the commit message and a list of files that were changed.\n\nUse git show &lt;commit hash&gt; to see the diff\n(Optional) Use git bisect log &gt; file-name to save the session to a file.\nUse git bisect reset to exit and return you to where you were at the start of this workflow (HEAD)\n\nAutomatic Workflow\n\nWrite script that includes you reproducible exaample and have it return an error code of 0 if it does not contain the bug or return an non-zero error if it does contain the bug.\n\nExample\ndevtools::load_all()\n\nif (nr != nrow(df)) {\n  stop(\"error\")\n}\n\nload_allwill recompile your package using this current version’s code\nReturns non-zero error code if condition is not triggered (i.e. False) and a 0 error code when the condition is triggered (i.e. True).\nCould also use stopifnot here.\n\n\n(Optional) If you already know the commit hash of commit from 3 weeks ago and that does not have the bug, you can bypass the step 3.\n# Make sure you're in the current commit that's got the bug\ngit bisect start\n1git bisect bad\n2git bisect good 3348b0\n\n1\n\nThis labels the current commit as bad (i.e. bug is present)\n\n2\n\nThis labels the commit from 3 weeks ago that you know doesn’t have the bug\n\n\nDo steps 1, 2, and 3 of the Manual Workflow\nRun auto-bisect\ngit bisect run Rscript test.R\n\ntest.R is the script from step 1 that determines whether the version (i.e. commit hash) of your code has the bug.\nThis run through all the steps of the Manual Workflow and determine with the version of the code is “good” or “bad” by whether the script returns an error code of zero or non-zero.\n\nRead final message to get the commit hash with bug in it.\n\nMessage will be “&lt;some commit hash&gt; is the first bad commit.”\nGit will also show you the commit message and a list of files that were changed.\n\nSee steps 6, 7, and 8 of the Manual Workflow",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#pulling",
    "href": "qmd/git-general.html#pulling",
    "title": "25  General",
    "section": "25.5 Pulling",
    "text": "25.5 Pulling\n\nSave your changes, pull in an update, apply your changes\ngit stash\ngit pull\ngit stash pop\n\ngit stash pop throws away the (topmost, by default) stash after applying it, whereas\ngit stash apply leaves it in the stash list for possible later reuse (or you can then git stash drop it).\n\nRe potential merge conflicts\n\n“For instance, say your stashed changes conflict with other changes that you’ve made since you first created the stash. Both pop and apply will helpfully trigger merge conflict resolution mode, allowing you to nicely resolve such conflicts… and neither will get rid of the stash, even though perhaps you’re expecting pop too. Since a lot of people expect stashes to just be a simple stack, this often leads to them popping the same stash accidentally later because they thought it was gone.”\n\nPulling is fetching + merging\n\nFetching just gets the info about the commits made to the remote repo\ngit fetch origin\nSome technical discussion for always using git pull –ff\n\nhttps://blog.sffc.xyz/post/185195398930/why-you-should-use-git-pull-ff-only-git-is-a\nhttps://megakemp.com/2019/03/20/the-case-for-pull-rebase/\nit’s still confusing but pull rebase sounds fine to me\n–global tag says do it for all my repos\nnot sure what the true and only are for\n\ngit pull –help will open doc in browser\n\n\nPulling by rebase\n\nLocal: using this method as default\ngit config pull.rebase true\ngit pull\nRemote\ngit pull --rebase\n\nPulling by fast-forward\n\nLocal: using this method as default\ngit config --global pull.ff only\ngit pull\nRemote\ngit pull --ff",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#branching",
    "href": "qmd/git-general.html#branching",
    "title": "25  General",
    "section": "25.6 Branching",
    "text": "25.6 Branching\n\nMisc\n\nCreate a new branch for each ticket you are working on or each data model. It can get sloppy when you put all your code changes on one branch.\nHEAD\n\nDetached HEAD\n\n\nCreate a branch (e.g. “testing”)\ngit branch testing\nWork in a branch\ngit checkout testing\nThe files in your working directory change to the version saved in that branch\nIt adds, removes, and modifies files automatically to make sure your working copy is what the branch looked like on your last commit to it.\nCreate and work in a branch\n# new way\ngit switch -c testing\nor\ngit checkout -b testing\nor\ngit branch testing\ngit checkout testing\ncreates the branch and switches you to working in that branch\nIf you did a bunch of changes in a codebase, only to realize that you’re working on `master`,  switch will bring those local changes with you to the new branch. So I guess they won’t affect master then.\n\nUnless If you already committed to main, then those changes are both in your new branch and in main. So you would still have to clean up the main branch.\n\nDeleting a branch\n\nlocal branch\ngit branch -d testing\n\nremote branch\ngit push &lt;remoteName&gt; --delete &lt;branchName&gt;\nSee existing branches\ngit branch\nSee what has been commited the remote repo branches\ngit fetch origin\ngit branch -vv\n“origin” is the name of the remote\nresult\ntesting    7e424c3 [origin/testing: ahead 2, behind 1] change abc \nmaster      1ae2a45 [origin/master] Deploy index fix\n* issue    f8674d9 [origin/issue: behind 1] should do it         \ncart        5ea463a Try something new\nformat: branch, last commit sha-1, local branch status vs remote branch status, commit message\nthe star indicates the HEAD pointer’s location (where you’re at, i.e. checkout)\ntesting branch\n\n“ahead 2” means  I committed twice to the local testing branch and this work has not been pushed to the remote testing branch repo yet.\n“behind 1” means someone has pushed a commit to the remote testing branch repo and we haven’t merged this work to our local testing branch\n\nGet the last 10 branches that you’ve committed to locally:\ngit branch --sort=-committerdate | head -n 10\nRename branch\n# change locally\ngit branch --move &lt;bad-branch-name&gt; &lt;corrected-branch-name&gt;\n# change remotely in repo\ngit push --set-upstream origin &lt;corrected-branch-name&gt;\n# confirm change\ngit branch --all\nHEAD determines to which branch new commits are added\n\nExample\n\n“testing” branch is created (not shown in above picture)\n\nHEAD points at “master” branch\n“master” branch and the new “testing” branch both point at commit, f30ab.\nf30ab commit points to previous commit 34ac2\n\nuser executes checkout to “testing” branch (not shown in picture)\n\nHEAD now points to testing branch\n\nuser commits 87ab2 (shown in pic)\n\n87ab2 is committed to the “testing” branch\n“testing” branch is now ahead of the “master” branch by 1 commit\n\n\nExample\n\nEverything above happens but now another user commits the master branch.\n\nBoth branches are in conflict. The testing branch is ahead and behind by 1 commit\n\n\n\nMerging\n\n\nNotes\n\nNEVER merge your branch locally on your machine with the master branch, ALWAYS merge online via pull request\n\nSteps\n\nPush final changes and use of a pull request\nSwitch to master branch locally and pull the merged changes\n\n\n\nUpdate branch with work that’s been done in master branch\n\nAfter updating your local branch, push to remote repo (no commit necessary)\n# while in branch\ngit merge master\n\n\nFast-Forward\n\nExample\n\nBefore the merge\n\nthe testing branch is 1 commit ahead of the master branch and the master branch doesnt have a new commit\n\nAfter the merge\n\nmaster is moved forward to the testing branch commit\n\n\nCode (merging work in branch with the master branch for production)\n# currently in test branch\ngit checkout master\ngit merge testing\n\nLines in file are marked\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.html\n# &lt;div id=\"footer\"&gt;contact : email.support@github.com&lt;/div&gt;\n# =======\n# &lt;div id=\"footer\"&gt;\n# please contact us at support@github.com\n# &lt;/div&gt;\n# &gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html\nAbove ======= is the master branch version of the code and below is the iss53 branch version\nMake necessary changes and save the file\ngit add . or git add &lt;resolved file&gt;\n\nTells git that conflict is resolved\n\nCheck status to confirm everything has been resolved\ngit status\n\n    On branch master\n    All conflicts fixed but you are still merging.\n      (use \"git commit\" to conclude merge)\n    Changes to be committed:\n      modified:  index.html\ngit commit\n\nno message required (there’s a default message) but you can add one if you want\n\nExample\n\niss53 branch ahead of master by 2 commits (c3, c5) and behind 1 commit (c2)\nSame code as Fast-Forward merge but git handles the merge a bit differently\ngit checkout master \ngit merge iss53\n\n\n\nC6 (right pic) is called a “merge commit.” Its created by git and points to two commits instead of one.\nNo need to merge with master (i.e. update local iss53 branch with c4 changes in master) before committing final changes\n\nIf there are changes in the same lines of code C4 and C5, then there will be a conflict (See below, Conflicts &gt;&gt; Example)\n\n\nConflicts\n\nExample\n\nChanged files in C4 (see above example) are in the same lines of the same files that you made changes to in C5\n\nRemember: you’re now in the master branch since you did checkout master as part of the merge code\nSteps\n\nCheck status to which files are causing the conflict (e.g. index.html)\ngit status\n  Unmerged paths:\n  (use \"git add &lt;file&gt;...\" to mark resolution) \n    both modified:      index.html\n\n\n\n\nMoving between branches\n\nfrom master to testing\ngit checkout testing\n\nlocal files are deleted and replaced with branch versions\n\nalternative: worktree\n\nExample\n\nWhat happens when you move from branch-a to branch-b\nBRANCH-A        BRANCH-B\nalpha.txt      alpha.txt\nbravo.txt\ncharlie.txt    charlie.txt\n                delta.txt\n\nbravo text is deleted from your local disc and delta.txt is added\nIf any changes to alpha.txt or charlie.txt have been made and no commit has been made, the checkout will be aborted\n\nSo either revert the changes or commit the changes\n\nUntracked files or newly created files\n\nIf you have branch-A checked out and you create a new file called echo.txt, Git will not touch this file when you checkout branch-B. This way, you can decide that you want to commit echo.txt against branch-B without having to go through the hassle of (1) move the file outside the repo, (2) checkout the correct branch, and (3) move the file back into the repo.",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#collaboration",
    "href": "qmd/git-general.html#collaboration",
    "title": "25  General",
    "section": "25.7 Collaboration",
    "text": "25.7 Collaboration\n\nAdd collaborators to your repository\nOne person invites the others and provides them with read/write access (github docs)\n\nSteps\n\nGo to the settings for your repository\nmanage access &gt;&gt; “invite a collaborator”\n\nSearch for each collaborator by full name, acct name, or email\nClick “Add &lt;name&gt; to &lt;repo&gt;”\n\nEach collaborator will need to accept the invitation\n\nSent by email",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/js.html",
    "href": "qmd/js.html",
    "title": "JS",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-misc",
    "href": "qmd/js.html#sec-js-misc",
    "title": "JS",
    "section": "",
    "text": "Resources\n\nLearn Just Enough JavaScript\n\nBasics: variables, objects, arrays, functions, conditionals, loops\n\nHow to run R code in the browser with webR\n\nNice breakdown of generic JS code to run scripts on a webpage\n\nJavaScript for Data Science\n\nhrbmstr: “javascript has the advantage over R/Python for both visualization speed — thanks to GPU integration — and interface creation — thanks to the ubiquity of HTML5 — means that people will increasingly bring their own data to websites for initial exploration first”\nconsole.log is the print method",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-basics",
    "href": "qmd/js.html#sec-js-basics",
    "title": "JS",
    "section": "Basics",
    "text": "Basics\n\nOperators\n\n// : comments\n... : If you want to copy all the values in your array, and add some new ones, you can use the {…} notation.\n${&lt;code&gt;} : Anything within the${} get ran as code\n\nExample:\n`${b.letter}: ${ (b.frequency*100).toFixed(2) }%`\n\nBackticks indicate it’s like a glue string or f string (i.e. uses code)\nb.letter and b.frequency are properties in an array\nto.Fixed is a method that rounds the value to to 2 decimal places\nThis was an example of a tooltip, so output would look like “F: 12.23%”\n\n\n\nVariables\nmyNumber = 10 * 1000\nvariableSetToCodeBlock = {\n  const today = new Date();\n  return today.getFullYear()\n}\nObject: myObject = ({name: \"Paul\", age: 25})\n\nContained within curly braces, { }\nSubset property, name:\n\nmyObject.name which returns value, Paul\nmyObject[\"name\"] which is useful if you have spaces, etc. in your property names\n\nTypes\n\nMap: Object holds key-value pairs and remembers the original insertion order of the keys\n\ne.g. See Stats &gt;&gt; By Group\nD3 Groups, Rollup, Index Docs\n\n\n\nArrays\n\nList of objects\n\nContained within brackets, [ ]\nEach row is an object and each column is a property of that object and that property has a value associated with it\n\nBasic examples\nmyArray = [1, 2, 3, 4]\nmyArray = [[1, 2], [3, 4]] // arrays within arrays\nmyArray = [1, 'cat', {name: 'kitty'}] // objects within arrays\nDF-like array\nmyData = [\n  {name: 'Paul', city: 'Denver'},\n  {name: 'Robert', city: 'Denver'},\n  {name: 'Ian', city: 'Boston'},\n  {name: 'Cobus', city: 'Boston'},\n  {name: 'Ayodele', city: 'New York'},\n  {name: 'Mike', city: 'New York'},\n]\n\nEquivalent Functions: Traditional vs Arrow\n// traditional\nfunction myFunctionWithParameters(firstName, lastName) {\n  return `My first name is ${firstName}, and my last name is ${lastName}.`\n}\n// arrow\nmyModernFunctionWithParameters = (firstName, lastName) =&gt; {\n  return `My first name is ${firstName}, and my last name is ${lastName}.`\n}\n\nArrow: Arguments are in the parentheses and the function is inside the curly braces\nString with variables needs to be surrounded by backticks\n\nFunctions Inside Methods: Traditional vs Arrow\n// traditional\n[1, 2, 3, 4, 5].filter(function(d) { return d &lt; 3 })\n// arrow\n[1, 2, 3, 4, 5].filter(d =&gt; d &lt; 3)\n\nThe argument is d but without parentheses and the function is d &lt; 3 without the curly braces\nThe function inputs each row/value of the array, so d is a row/value of the array. Then, the function does something to that row.\n\nConditionals\n\n== vs ===\n1 == '1' // true\n1 === '1' // false\n\n== is a logical test to see if two values are the same\n\n=== is a logical test to see if two values are the same and also checks if the value types are the same\n\nIf/Then\nif(1 &gt; 2) {                              // If this statement is true\n    return 'Math is broken'              // return this\n} else {                                // if the first statement was not true\n    return 'Math still works!'          // return this\n}\n\n// using ternary operator \"?\"\n\nUsing ternary operator “?”\n\nSyntax: condition ? exprIfTrue : exprIfFalse\nExample: d =&gt; d.frequency &gt;= minFreq ? \"steelblue\" : \"lightgray\"\n\nSays if the frequency property is &gt;= the variable, minFreq, value, then use steelblue otherwise use lightgray\n\n\n\n\nFor-Loop\nlet largestNumber = 0; // Declare a variable for the largest number\n\nfor(let i = 0; i &lt; myValues.length - 1; i++) {    // Loop through all the values in my array\n    if(myValues[i] &gt; largestNumber) {              // Check if the value in the array is larger that the largestNumber\n      largestNumber = myValues[i]                  // If so, assign the value as the new largest number\n    }\n}\n\nreturn largestNumber\n\nThe first statement sets a variable (let i = 0)\nThe second statement provides a condition for when the loop will run (whenever i &lt; myValues.length - 1)\nThe third statement says what to do each time the code block is executed (i++, which means to add 1 to i)\n\nWhile-Loop\nlet largestNumber = 0;                        // Create a variable for the largest number\nlet i = 0;\nwhile(i &lt; myValues.length - 1) {\n    if(myValues[i] &gt; largestNumber) {        // Check if the value in the array is larger that the largestNumber\n      largestNumber = myValues[i]            // If so, assign the value as the new largest number\n    }\n    i++;\n}\nreturn largestNumber",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-cleaning",
    "href": "qmd/js.html#sec-js-cleaning",
    "title": "JS",
    "section": "Cleaning",
    "text": "Cleaning\n\nMisc\n\nNotes from: Horst article\n\nFilter objects: myData.filter(d =&gt; d.city == 'Denver')\nSelect properties: myNewArray = salesData.map(d =&gt; ({ date: d.date, product: d.product, totalRevenue: d.totalRevenue }))\n\nIn some contexts, this, d =&gt; d[\"mileage (mpg)\"] , is also used to select columns\n\nArrange objects: salesData.sort((a, b) =&gt; a.totalRevenue - b.totalRevenue)\n\nReorders salesData by totalRevenue (low to high)\n\nMutate properties: salesData.map(d =&gt; ({...d, discountedPrice: 0.9 * d.unitPrice }))\n\nAdds a new column to salesData with a discountedPrice, which takes 10% off each unitPrice.\n\nGroup_By: d3.rollup(salesData, v =&gt; d3.sum(v, d =&gt; d.totalRevenue), d =&gt; d.region)\n\nReturn the sum of totalRevenue for each region in salesData.\nrollup might actually be a summarize and the group_by is handled in the syntax\n\nRename: salesData.map(d =&gt; ({...d, saleDate: d.date }))\n\nAdds a new column called saleDate by storing a version of the date with new name saleDate and keeping all other columns.\n\nSubset value: salesData.map(d =&gt; d.description)[3]\n\nAccess the fourth value from the description property in salesData\n\nUnite:\nsalesData.map(d =&gt; ({...d, fullDescription: `${d.product} ${d.description}`}))\n\nUnite the product and description columns into a single column called fullDescription, using a comma as a separator.\n\nLeft Join: *using {{{arquero}}} tables* salesData.join_left(productDetails, ['product', 'product_id'])\n\nJoin information from a productDetails table to salesData. Join on product in salesData and product_id in productDetails.",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-stats",
    "href": "qmd/js.html#sec-js-stats",
    "title": "JS",
    "section": "Stats",
    "text": "Stats\n\nMisc\n\nNotes from: Horst article\n\nIn examples, waterUsage is the array; waterGallons is the property.\n\n\nMean: d3.mean(waterUsage.map(d =&gt; d.waterGallons))\n\nReturns a Value\n\nStd.Dev: d3.deviation(waterUsage.map(d =&gt; d.waterGallons))\nMedian: d3.median(waterUsage.map(d =&gt; d.waterGallons))\nMin/Max: d3.min(waterUsage.map(d =&gt; d.waterGallons))\nTotal Observations (i.e. nrow ): waterUsage.length\nBy Group:\n\npropertyId is the discrete, grouping variable\nMean: waterMeans = d3.rollup(waterUsage, v =&gt; d3.mean(v, d =&gt; d.waterGallons), d =&gt; d.propertyId)\n// Returns a map object\nwaterMeans\n{\n  \"A001\" =&gt; 39.53389830508475\n  \"B002\" =&gt; 53.57627118644068\n  \"C003\" =&gt; 27.45762711864407\n  \"D004\" =&gt; 80.1864406779661\n}\n\n// View in a JS Table\n// ** Must be in a separate cell **\nInputs.table(waterMeans.map(([propertyId, meanWaterGallons]) =&gt; ({propertyId, meanWaterGallons})))\nCount: d3.rollup(waterUsage, v =&gt; d3.count(v, d =&gt; d.waterGallons), d =&gt; d.propertyId)\n\nConditional Counts: waterUsage.filter(d =&gt; d.waterGallons &gt; 90 && d.propertyId == \"B002\").length\n\nApplies two conditionals and counts the observations\n\nRanks\nwaterUsage.map((d, i) =&gt; ({...d, rank: d3.rank(waterUsage.map(d =&gt; d.waterGallons), d3.descending)[i] + 1}))\n\n1 is added so that ranks start at 1 instead of 0\n\nPercentiles: d3.quantile(waterUsage.map(d =&gt; d.waterGallons), 0.9) (e.g. 90th)",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-obs",
    "href": "qmd/js.html#sec-js-obs",
    "title": "JS",
    "section": "Observable",
    "text": "Observable\n\nA collaborative, online notebook platform that comes with libraries loaded to make it fairly straightforward to dive into ad hoc data analysis or produce complete reports.\nIn Observable, if you’re running a JavaScript cell that contains more than just a simple variable assignment (like myVariable = 'Hello World' ), you need to run a code block (i.e. bracket lines of code in curly braces, {}).\nYou can open your notebook in Safe Mode and edit your work without running it.\n\nGood for debugging (e.g. infinite while-loops)",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-def",
    "href": "qmd/js.html#sec-js-def",
    "title": "JS",
    "section": "Definitions",
    "text": "Definitions\n\nJSON vs R List\n{                              list(\n    boolean: true,                boolean = TRUE,\n    string: \"hello\",              string = \"hello\",\n    vector: [1,2,3]                vector = c(1,2,3)\n}                              )\n\n// Access                      # Access\njson.vector                    list$vector\nDependencies\nHTML                                                  R (shiny)\n&lt;head&gt;                                                tags$head(\n    &lt;!-- JavaScript --&gt;                                  tags$script(src = \"path/to/file.js\")\n    &lt;script src=\"path/to/file.js\"&gt;&lt;/script&gt;              tags$link(\n    &lt;!-- CSS --&gt;                                          rel = \"stylesheet\",\n    &lt;link rel=\"stylesheet\" href=\"path/to/file.css&gt;        href = \"path/to/file.css\n&lt;/head&gt;                                                  ))\nd is each row and =&gt; is function\n(d) =&gt; d.year === 2020\n\nSays for each row in your data, the year column must equal 2020\n\nCallback Function - A function that is passed to another function as a parameter. In other words, a function “calls back” to previously defined function.\nfunction print(callback) { \n    callback();\n}\n\ncallback is the callback function and is a parameter of the print function\nCallbacks make sure that a function is not going to run before a task is completed but will run right after the task has completed.\nExample:\n// \"Click here\" button in a web app\n&lt;button id=\"callback-btn\"&gt;Click here&lt;/button&gt;\ndocument.queryselector(\"#callback-btn\")\n    .addEventListener(\"click\", function() {   \n      console.log(\"User has clicked on the button!\");\n});\n\nFirst, button selected by its id, and then we add an event listener with the addEventListener method. It takes 2 parameters. The first one is its type, click, and the second parameter is a callback function, which logs the message when the button is clicked.\n\n\nAnonymous Function - Same as a callback but unnamed. It’s a  function that is defined within another function.\nsetTimeout(function() { \n    console.log(\"This message is shown after 3 seconds\");\n}, 3000);\n\n// if the function were named\nconst message = function() { \n    console.log(\"This message is shown after 3 seconds\");\n}\n\n// as an arrow function\nsetTimeout(() =&gt; { \n    console.log(\"This message is shown after 3 seconds\");\n}, 3000);\n\nThe function used as a parameter has no name. console.log is the contents of the function.",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-nfcd",
    "href": "qmd/js.html#sec-js-nfcd",
    "title": "JS",
    "section": "Notes From Covidcast Dashboard",
    "text": "Notes From Covidcast Dashboard\n\nNotes from\n\nCovidcast Dashboard: reactable + sparkline tooltip (link)\n\ndiv = vertical label or container , span = horizontal\nFormat: type, styling, value\n2 divs would result in a 2 element vertical label while 2 spans would be a 2 element horizontal label\nExample: A div container holding 2 spans which creates a “date value” horizontal label\n\"function (_ref) {\nvar datum = _ref.datum;\nreturn React.createElement(\n  'div',\n  null,\n  datum.date && React.createElement(\n      'span',\n      {style: {\n          backgroundColor: 'black', color: 'white',\n          padding: '3px', margin: '0px 4px 0px 0px', textAlign: 'center'\n        }},\n      datum.date[0].split('-').slice(1).join('/')\n  ),\n  React.createElement(\n      'span',\n      {style: {\n        fontWeight: 'bold', fontSize: '1.1em',\n        padding: '2px'\n      }},\n      datum.y ? datum.y.toLocaleString(undefined, {maximumFractionDigits: 0}) : '--'\n  )\n  );\n}\"\n\nCSS: margin, padding\n\nFormat is top, right, bottom, left (ordered like a clock)\nRequires units like “px”\nNo commas separate the values\n{margin: '0px 4px', padding: '0px 0px 0px 4px'}\n\nMaybe for 0s it doesn’t matter\nSee bkmk in css/definitions for explanations behind specifications with less than 4 numbers\n\ne.g. 2 is ‘top/bottom left/right’\n\n\n\nString manipulation\ndatum.endDate[0].split('-').slice(1).join('/')\n\nTreats variable as a string object\nLooks in data arg, finds endDate variable\nIts a list variable so requires the [0] (0 part an index?)\nDate format is ymd, so splits value by “-” separator, removes 1st value (year), joins the rest of the values (month, day) with “/”\n\nIf slice(2), removes first 2 values (left to right)\n\n\nConditional\nlabelPosition = htmlwidgets::JS(\"(d, i) =&gt; (i === 0 || i === 1 ? 'right' : 'left')\")\n\nSays that if index of data value, d, is 0 or 1 then label should be positioned on the right of the point, else place the label on the left of the point",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/simulation-data.html",
    "href": "qmd/simulation-data.html",
    "title": "Simulation, Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Simulation, Data"
    ]
  },
  {
    "objectID": "qmd/simulation-data.html#sec-sim-data-misc",
    "href": "qmd/simulation-data.html#sec-sim-data-misc",
    "title": "Simulation, Data",
    "section": "",
    "text": "Todo - for the “trtAssign” mess with ratio and the number of ratios\nAlso see\n\nBkmks Data &gt;&gt; Data Simulation\nPandas-Time Series-Simulation\n\nA Gaussian and Standard GARCH time-series that’s frequently encountered in econometrics\n\n\nPackages\n\n{structmcmc} - A set of tools for performing structural inference for Bayesian Networks using MCMC\n\nPapers\n\nGeneration and analysis of synthetic data via Bayesian networks: a robust approach for uncertainty quantification via Bayesian paradigm",
    "crumbs": [
      "Simulation, Data"
    ]
  },
  {
    "objectID": "qmd/simulation-data.html#sec-sim-data-simstudy",
    "href": "qmd/simulation-data.html#sec-sim-data-simstudy",
    "title": "Simulation, Data",
    "section": "{simstudy}",
    "text": "{simstudy}\n\nMisc\n\nDocs\n\n\n\nReference\n\nAvailable distributions (link)\n\nProbability Distributions\nnonrandom: For constants; can be a numeric or a string with a formula that defines a dependency on another variable\nclusterSize: For variable cluster sizes but a constant total sample size\n\nformula: The (fixed) total sample size\nvariance: A (non-negative) dispersion measure that represents the variability of size across clusters\n\nIf the dispersion is set to 0, then cluster sizes are constant\n\n\ntrtAssign: For treatment assignment\n\nformula: Ratio which is separated by semicolons and number of treatments\n\ne.g. 2 values = 2 groups and “1;2” says group 2 has twice as many units and group 1\n\nvariance: Stratification; ratio in formula is used as the stratification ratio (e.g. unbalanced treatment groups → unbalanced stratification)\nExample\ndef &lt;- \n  defData(def, \n          varname = \"rx\", \n          dist = \"trtAssign\",\n          formula = \"1;1;2\", \n          variance = \"male;over65\")\n\ncount(studytbl, rx)\n#&gt; # A tibble: 3 × 2\n#&gt;     rx    n\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1    1    84\n#&gt; 2    2    82\n#&gt; 3    3  164\n\ncount(studytbl, male, rx)\n#&gt; # A tibble: 6 × 3\n#&gt;   male    rx    n\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1    0    1    40\n#&gt; 2    0    2    39\n#&gt; 3    0    3    78\n#&gt; 4    1    1    44\n#&gt; 5    1    2    43\n#&gt; 6    1    3    86\n\ncount(studytbl, over65, rx)\n#&gt; # A tibble: 6 × 3\n#&gt;   over65    rx    n\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1      0    1    66\n#&gt; 2      0    2    65\n#&gt; 3      0    3  130\n#&gt; 4      1    1    18\n#&gt; 5      1    2    17\n#&gt; 6      1    3    34\n\n\nFunctions\n\ndefData(dtDefs = NULL, varname, formula, variance = 0, dist = \"normal\", link = \"identity\", id = \"id\") - Initially creates a data.table or adds a column to a data.table with instructions about creating a variable\n\nformula: Numeric constant or string formula for the mean, probability of event (binary), probability of success (binomial), etc.\n\ndefDataAdd(dtDefs = NULL, varname, formula, variance = 0, dist = \"normal\", link = \"identity\") - Creates a variable definition like defData but is used to augment a already generated dataset. Used as input to addColumns which will generate the variable data from the instructions in this object and add it as a column to the already generated dataset.\ngenCluster(dtClust, cLevelVar, numIndsVar, level1ID, allLevel2 = TRUE) - After generating cluster-level data, this function takes the number of clusters and the sizes of each cluster from that data, and does something like expand.grid to generate an individual-level dataset. Also, adds an id variable.\n\ndtClust: Cluster-Level Data\ncLevelVar: Cluster variable from the cluster-level data\nnumIndsvar: Variable with the number of units per cluster from the cluster-level data\nlevel1ID: Name you want for your individual-level ID variable\n\n\n\n\n\nVariable Dependence\n\nBinary depends on a Binary\n\nDefinitions\ndef &lt;- defData(varname = \"male\", dist = \"binary\",\n               formula = .5 , id=\"cid\")\ndef &lt;- defData(def, varname = \"over65\", dist = \"binary\",\n               formula = \"-1.7 + .8*male\", link=\"logit\")\nWhat’s happening\nmale &lt;- c(1,1,0,1,0,0,0,1,0,1)\nlogits &lt;- -1.7 + 0.8 * male\nprobabilities &lt;- boot::inv.logit(logits)\nover65 &lt;- rbinom(n = 10, size = 1, prob = probabilities)\n\nThe formula in the logits line defines the relationship between being male and being over 65yrs old.\nMales in this sample will have a higher probability (0.2890505) of being over 65yrs old than females (0.1544653)\nTo sample from a Bernoulli distribution, set size = 1\nover65 is an indicator where each value is determined by a separate probability parameter for a Bernoulli distribution\n\n\n\n\n\nClustered with Cluster-Level Random Effect\n\nExample: Fixed Cluster sizes; Balanced\n\nCluster Definitions\nd0 &lt;- defData(varname = \"n\", formula = 20, dist = \"nonrandom\")\nd0 &lt;- defData(d0, varname = \"a\", formula = 0, variance = 0.33)\nd0 &lt;- defData(d0, varname = \"rx\", formula = \"1;1\", dist = \"trtAssign\")\nd1 &lt;- defDataAdd(varname = \"y\", formula = \"18 + 1.6 * rx + a\",\n                 variance = 16, dist = \"normal\")\n\nn: sample size for the cluster\n\ndist = “nonrandom” and formula = 20 says use a constant for the cluster sizer\n\nrx: treatment indicator\n\ndist = “trtAssign” and formula = “1;1” says 2 treatment groups and they’re balanced\n\ny: the individual-level outcome is a function of the treatment assignment and the cluster effect, as well as random individual-level variation\na: random individual-level variation (i.e. random effect)\n\nRandom Effects are sampled from \\(\\mathcal{N}(0, \\sigma)\\) where the variance is typically estimated in a Mixed Effects model.\n\n\nGenerate Cluster-Level Data\nset.seed(2761)\ndc &lt;- genData(10, d0, \"site\")\ndc\n##    site  n      a rx\n##  1:    1 20 -0.3548  1\n##  2:    2 20 -1.1232  1\n##  3:    3 20 -0.5963  0\n##  4:    4 20 -0.0503  1\n##  5:    5 20  0.0894  0\n##  6:    6 20  0.5294  1\n##  7:    7 20  1.2302  0\n##  8:    8 20  0.9663  1\n##  9:    9 20  0.0993  0\n## 10:  10 20  0.6508  0\n\nGenerates 10 clusters labelled as site according to the instructions in d0\n\nGenerate Individual Level Data\ndd &lt;- genCluster(dc, \"site\", \"n\", \"id\")\ndd &lt;- addColumns(d1, dd)\ndd\n##      site  n      a rx  id    y\n##  1:    1 20 -0.355  1  1 17.7\n##  2:    1 20 -0.355  1  2 16.2\n##  3:    1 20 -0.355  1  3 19.2\n##  4:    1 20 -0.355  1  4 20.6\n##  5:    1 20 -0.355  1  5 14.7\n##  ---                           \n## 196:  10 20  0.651  0 196 25.3\n## 197:  10 20  0.651  0 197 22.1\n## 198:  10 20  0.651  0 198 13.2\n## 199:  10 20  0.651  0 199 15.6\n## 200:  10 20  0.651  0 200 13.8\n\ngenCluster performs an expand.grid to generate an individual-level dataset along with adding an ID variable\naddColumns uses individual-level data and outcome variable definition to generate the outcome variable and add it to the dataset.\n\n\nExample: Varying Cluster Sizes and therefore Varying Sample Size\nd0 &lt;- defData(varname = \"n\", formula = 20, dist = \"poisson\")\ngenData(10, d0, \"site\")\n##    site  n\n##  1:    1 13\n##  2:    2 18\n##  3:    3 21\n##  4:    4 26\n##  5:    5 25\n##  6:    6 27\n##  7:    7 23\n##  8:    8 30\n##  9:    9 23\n## 10:  10 20\n\nFormula sets the poisson distribution parameter, \\(\\lambda = 20\\). So sizes are sampled from poisson distribution with that mean/variance\nTo increase the variability between clusters, use the negative binomial distribution\nMost likely leads to an unbalanced design\n\nExample: Varying Cluster Sizes but Constant Sample Size\n# moderately varying cluster sizes\nd0 &lt;- defData(varname = \"n\", formula = 200, variance = 0.2, dist = \"clusterSize\")\ngenData(10, d0, \"site\")\n\n##    site  n\n##  1:    1 20\n##  2:    2 28\n##  3:    3 25\n##  4:    4 24\n##  5:    5 28\n##  6:    6 22\n##  7:    7  7\n##  8:    8 13\n##  9:    9 22\n## 10:  10 11\n\n# Very highly varying cluster sizes\nd0 &lt;- defData(varname = \"n\", formula = 200, variance = 5, dist = \"clusterSize\")\ngenData(10, d0, \"site\")\n##    site  n\n##  1:    1  10\n##  2:    2  2\n##  3:    3  17\n##  4:    4  2\n##  5:    5  49\n##  6:    6 110\n##  7:    7  1\n##  8:    8  4\n##  9:    9  1\n## 10:  10  4\n\nTotal sample size is fixed at 200 (formula), but individual cluster sizes are allowed to vary.\nvariance: A dispersion parameter that controls the amount of varying of the cluster sizes",
    "crumbs": [
      "Simulation, Data"
    ]
  },
  {
    "objectID": "qmd/association-general.html",
    "href": "qmd/association-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-misc",
    "href": "qmd/association-general.html#sec-assoc-gen-misc",
    "title": "General",
    "section": "",
    "text": "Also see\n\nEDA &gt;&gt; Correlation\nNotebook &gt;&gt; Statistical Inference &gt;&gt; Correlation\n\nE(υ|x)=0 is equivalent to Cov(x,υ)=0 or Cor(x,υ)=0\nA negative correlation between variables is also called anticorrelation or inverse correlation\nIndependence - Two random variables are independent if the product of their individual probability density functions equals the joint probability density function",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-partcor",
    "href": "qmd/association-general.html#sec-assoc-gen-partcor",
    "title": "General",
    "section": "Partial Correlation",
    "text": "Partial Correlation\n\nStatistical Formula\n\\[\n\\frac{\\mbox{Cov}(X, Y) - \\mbox{Cov}(X, Z) \\cdot \\mbox{Cov}(Y, Z)}{\\sqrt{\\mbox{Var}(X) - \\mbox{Cov}(X, Z)^2}\\cdot \\sqrt{\\mbox{Var}(Y) - \\mbox{Cov}(Y, Z)^2}}\n\\]\nMeasures the association (or correlation) between two variables when the effects of one or more other variables are removed from such a relationship.\n\nIn the above equation, I think it’s the partial correlation between x and y given z.\n\nMisc\n\nResources\n\nDealing with correlation in designed field experiments: part I\n\nExcellent tutorial on partial, joint correlations in block design\n\nppcor pkg: An R Package for a Fast Calculation to Semi-partial Correlation Coefficients\n\nExplainer for semi-partial, partial correlation\n\nAlso see notebook for a method using regression models\n\n\nExample: psych::partial.r(y ~ x - z, data)\nExample: {correlation}\nhead(correlation::correlation(mtcars, partial = TRUE))\n\n#&gt; # Correlation Matrix (pearson-method)\n\n#&gt; Parameter1 | Parameter2 |     r |         95% CI | t(30) |      p\n#&gt; -----------------------------------------------------------------\n#&gt; mpg        |        cyl | -0.02 | [-0.37,  0.33] | -0.13 | &gt; .999\n#&gt; mpg        |       disp |  0.16 | [-0.20,  0.48] |  0.89 | &gt; .999\n#&gt; mpg        |         hp | -0.21 | [-0.52,  0.15] | -1.18 | &gt; .999\n#&gt; mpg        |       drat |  0.10 | [-0.25,  0.44] |  0.58 | &gt; .999\n#&gt; mpg        |         wt | -0.39 | [-0.65, -0.05] | -2.34 | &gt; .999\n#&gt; mpg        |       qsec |  0.24 | [-0.12,  0.54] |  1.34 | &gt; .999\n#&gt; \n#&gt; p-value adjustment method: Holm (1979)\n#&gt; Observations: 32\n\nVisualization\n\npacman::p_load(see, ggraph)\ncorrelation::correlation(mtcars, partial = TRUE) |&gt; \n  plot()\n\nGraphical LASSO\n\nComputing covariance matrices are computationally expensive while computing its inverse can be less so. This algorithm calculates the inverse covariance matrix (ICT), aka Precision Matrix, and it’s based on an interplay between probability theory and graph theory, in which the properties of an underlying graph specify the conditional independence properties of a set of random variables.\n\nSee Statistical Learning With Sparsity (Hastie, Tibshirani, Wainright)\n\nMathematical introduction to graphical models and Graphical LASSO, pg 241 (252 in pdf), See R &gt;&gt; Documents &gt;&gt; Regression\n\n\nAssumes that the observations have a multivariate Gaussian distribution\nMisc\n\nPackages\n\n{glasso} - The original package by the authors of the algorithm. Estimation of a sparse inverse covariance matrix using a lasso (L1) penalty. Facilities are provided for estimates along a path of values for the regularization parameter. Can be slow or nonconvergent for large dimension datasets.\n{huge} - Provides functions for estimating high dimensional undirected graphs from data. Also provides functions for fitting high dimensional semiparametric Gaussian copula models (Vignette)\n{cglasso} - Conditional Graphical Lasso Inference with Censored and Missing Values (Vignette)\n\n\nPreprocessing: All variables should be standardized.\nThe terms in the ICT are not equivalent but are proportional to the partial correlation between the two corresponding variables\n\nTransform the ICT, \\(\\Omega\\) into a partial correlation matrix, \\(R\\)\n\\[\nR_{j,k} = \\frac{-\\Omega_{i,j}}{\\sqrt{\\Omega_{j,j}\\Omega_{k,k}}}\n\\]\nparr.corr &lt;- matrix(nrow=nrow(P), ncol=ncol(P))\nfor(k in 1:nrow(parr.corr)) {\n  for(j in 1:ncol(parr.corr)) {\n    parr.corr[j, k] &lt;- -P[j,k]/sqrt(P[j,j]*P[k,k])\n  }\n}\ncolnames(parr.corr) &lt;- colnames(P)\nrownames(parr.corr) &lt;- colnames(P)\ndiag(parr.corr) &lt;- 0\nSetting the terms on the diagonal to zero prevents variables from having connections with themselves in a network graph if you want to visualize the relationships\n\nWhere the nodes are variables and edges are the partial correlations.\n\n\nHyperparameter, \\(\\rho\\) , adjusts the sparsity of the matrix output\n\nHigher: Isolates the strongest relationships in your data (more sparse)\nLower: Preserving more tenuous connections, perhaps identifying variables with connections to multiple groups (less sparse)\n\nCheck symmetry. Assymmetry in the ICT can arise due to numerical computation and rounding errors, which can cause problems later depending on what you want to do with the matrix.\nExample: Stock Analysis using {glasso} (link)\nrho &lt;- 0.75\ninvcov &lt;- glasso(S, rho=rho)  \n\n# inverse covariance matrix\nP &lt;- invcov$wi\ncolnames(P) &lt;- colnames(S)\nrownames(P) &lt;- rownames(S)\n\n# check symmetry\nif(!isSymmetric(P)) {\n  P[lower.tri(P)] = t(P)[lower.tri(P)]  \n}\n\nGoal: Remove stocks relationship with market Beta and other confounding stocks to get the true relationsip between stock pairs.\nPost also has a network visualization. Data was put through PCA, then DBSCAN to get clusters. The cluster assignments were used to color the clusters in the network graph.\nPost also examines output from a lower \\(\\rho\\) and has an interesting analysis of the non-connected variables (i.e. no partial correlation).",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-cont",
    "href": "qmd/association-general.html#sec-assoc-gen-cont",
    "title": "General",
    "section": "Continuous",
    "text": "Continuous\n\nSpearman’s Rank\n\\[\n\\rho = 1 - \\frac{6\\sum_i d_i^2}{n(n^2-1)}\n\\]\n\n\\(d_i\\): The difference in ranks for the ith observation\nMeasures how well the relationship between the two variables can be described by a monotonic function\nRank correlation measures the similarity of the order of two sets of data, relative to each other (recall that PCC did not directly measure the relative rank).\n\nValues range from -1 to 1 where 0 is no association and 1 is perfect association\nNegative values don’t mean anything in ranked correlation, so just remove the negative\n\nLinear relationship is a specific type of monotonic relationship where the rate of increase remains constant — in other words, unlike a linear relationship, the amount of change (increase or decrease) in a monotonic relationship can vary.\nSee bkmks for CIs\nPackages\n\n{stats::cor.test(method = “spearman”)}\n{DescTools::SpearmanRho}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\n\nKendall’s Tau\n\nNon-parametric rank correlation\n\nNon-parametric because it only measures the rank correlation based on the relative ordering of the data (and not the specific values of the data).\n\nShould be pretty close to Sspearman’s Rank but a potentially faster calculation\nFlavors: a, b (makes adjustment for ties), c (for different sample sizes for each variable)\n\nUse Tau-b if the underlying scale of both variables has the same number of possible values (before ranking) and Tau-c if they differ.\ne.g. One variable might be scored on a 5-point scale (very good, good, average, bad, very bad), whereas the other might be based on a finer 10-point scale. In this case, Tau-c would be recommended.\n\nPackages\n\n{stats::cor.test(method = “kendall”)} - Doesn’t state specifically but I think it calculates a and b depending on whether ties are present or not\n{DescTools} - has all 3 flavors\n\n\nHoeffding’s D\n\nRank-based approach that measures the difference between the joint ranks of (X,Y) and the product of marginal ranks.(?) A non-parametric test of independence. the product of their marginal ranks.\nUnlike the Pearson or Spearman measures, it can pick up on nonlinear relationships.\nRange: [-.5,1]\nGuidelines: Larger values indicate a stronger relationship between the variables.\nPackages\n\n{Hmisc::hoeffd}\n{DescTools::HoeffD}\n\n\nBayesian\n\nSteps: {brms}\n\nList the variables you’d like correlations for within mvbind().\nPlace the mvbind() function within the left side of the model formula.\nOn the right side of the model formula, indicate you only want intercepts (i.e., ~ 1).\nWrap that whole formula within bf().\nThen use the + operator to append set_rescor(TRUE), which will ensure brms fits a model with residual correlations.\nUse non-default priors and the resp argument to specify which prior is associated with which criterion variable\n\nGaussian\n\nExample: multiple variables\nf9 &lt;- \n   brm(data = d,\n    family = gaussian,\n    bf(mvbind(x_s, y_s, z_s) ~ 0,\n       sigma ~ 0) +\n    set_rescor(TRUE),\n    prior(lkj(2), class = rescor),\n    chains = 4, cores = 4,\n    seed = 1)\n\n## Residual Correlations: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## rescor(xs,ys)    0.90      0.02    0.87    0.93 1.00    3719    3031\n## rescor(xs,zs)    0.57      0.07    0.42    0.69 1.00    3047    2773\n## rescor(ys,zs)    0.29      0.09    0.11    0.46 1.00    2839    2615\nStandardized data is used here but isn’t required\n\nWill need to set priors though (see article for further details)\n\nSince the data is standardized, the sd can be fixed at 1\n\nbrms models log of sd by default, hence sigma ~ 0 since log 1 = 0\n\nCorrelations are the estimates for rescor(xs,ys), rescor(xs,zs) rescor(ys,zs)\n\nStudent t-distribution\n\nIf the data has any outliers, pearson’s coefficient is substantially biased.\nExample: correlation between x and y\n\\\nf2 &lt;- \n    brm(data = x.noisy, \n    family = student,\n    bf(mvbind(x, y) ~ 1) + set_rescor(TRUE),\n    prior = c(prior(gamma(2, .1), class = nu),\n              prior(normal(0, 100), class = Intercept, resp = x),\n              prior(normal(0, 100), class = Intercept, resp = y),\n              prior(normal(0, 100), class = sigma, resp = x),\n              prior(normal(0, 100), class = sigma, resp = y),\n              prior(lkj(1), class = rescor)),\n    iter = 2000, warmup = 500, chains = 4, cores = 4, \n    seed = 210191)\n\n## Population-Level Effects: \n##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## x_Intercept    -2.07      3.59    -9.49    4.72 1.00    2412    2651\n## y_Intercept    1.93      7.20  -11.31    16.81 1.00    2454    2815\n## \n## Family Specific Parameters: \n##        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma_x    18.35      2.99    13.12    24.76 1.00    2313    2816\n## sigma_y    36.52      5.90    26.13    49.49 1.00    2216    3225\n## nu          2.65      0.99    1.36    4.99 1.00    3500    2710\n## nu_x        1.00      0.00    1.00    1.00 1.00    6000    6000\n## nu_y        1.00      0.00    1.00    1.00 1.00    6000    6000\n## \n## Residual Correlations: \n##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## rescor(x,y)    -0.93      0.03    -0.97    -0.85 1.00    2974    3366\n\nN = 40 simulated from a multivariate normal with 3 outliers\nCorrelation is the rescor(x,y) estimate -0.93; true value is -0.96\n\nUsing a pearson coefficient, cor = -0.6365649\nUsing brms::brm with family = gaussian, rescor(x,y) estimate -0.61",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-disc",
    "href": "qmd/association-general.html#sec-assoc-gen-disc",
    "title": "General",
    "section": "Discrete",
    "text": "Discrete\n\nMisc\n\nAlso see\n\nMultiple Correspondence Analysis (MCA) (see bkmks &gt;&gt; Features &gt;&gt; Reduction)\nDiscrete Analysis Notebook\n\nPackages\n\n{PAsso} - Assesses the Partial Association Between Ordinal Variables\n\nAllows users to perform a wide spectrum of assessments, including quantification, visualization, and hypothesis testing.\nVignette\n\n\nBinary vs Binary Similarity measures (paper)\n\nNote that a pearson correlation between binaries can be useful (see EDA &gt;&gt; Misc &gt;&gt; {correlationfunnel})\nTypes:\n\nJaccard-Needham\nDice\nYule\nRussell-Rao\nSokal-Michener\nRogers-Tanimoto\nKulzinsky\n\nPackages\n\n{{scipy}} - Also has other similarity measures\n\n\n\nPhi Coefficient - Used for binary variables when the categories are truly binary and not crudely measuring some underlying continuous variable (i.e. dichotomization of a continuous variable)\n\n“A Pearson correlation coefficient estimated for two binary variables will return the phi coefficient” (Phi coefficient wiki)\n(Contingency Table) Two binary variables are considered positively associated if most of the data falls along the diagonal cells. In contrast, two binary variables are considered negatively associated if most of the data falls off the diagonal\nAlso see StackExchange discussion on the difference between Phi Coefficient and Tetrachoric correlation\n{DescTools::Phi}\n\nCramer’s V - Association between two nominal variables\n\nSee Discrete Analysis notebook\n{DescTools::CramerV}\n\nPolychoric - Suppose each of the ordinal variables was obtained by categorizing a normally distributed underlying variable, and those two unobserved variables follow a bivariate normal distribution. Then the (maximum likelihood) estimate of that correlation is the polychoric correlation.\n\n{polycor}\n{psych::polychoric}\n\nFor correct=FALSE, the results agree perfectly with {polycor}\nFor very small data sets, the correction for continuity for the polychoric correlations can lead to difficulties, particularly if using the global=FALSE option, or if doing just one correlation at a time. Setting a smaller correction value (i.e., correct =.1) seems to help.\n\n{DescTools::CorPolychor}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\nTetrachoric - Used for binary variables when those variables are a sort of crude measure of an underlying continuous variable\n\nAlso see StackExchange discussion on the difference between Phi Coefficient and Tetrachoric correlation\nExample of appropriate use case: Suppose there are two judges who judge cakes, say, on some continuous scale, then based on a fixed, perhaps unknown, cutoff, pronounce the cakes as “bad” or “good”. Suppose the latent continuous metric of the two judges has correlation coefficient ρ.\n“the contingency tables are ‘balanced’ row-wise and col-wise, you get good correlation between the two metrics, but the tetrachoric tends to be a bit larger than the phi coefficient. When the cutoffs are somewhat imbalanced, you get slightly worse correlation between the metrics, and the phi appears to ‘shink’ towards zero.”\nThe estimation procedure is two stage ML.\n\nCell frequencies for each pair of items are found. Cells with zero counts are replaced with .5 as a correction for continuity (correct=TRUE).\nThe marginal frequencies are converted to normal theory thresholds and the resulting table for each item pair is converted to the (inferred) latent Pearson correlation that would produce the observed cell frequencies with the observed marginals\n\n{psych::tetrachoric}\n\nThe correlation matrix gets printed, but the correlations can also be extracted with $rho\nCan be sped up considerably by using multiple cores and using the parallel package. The number of cores to use when doing polychoric or tetrachoric may be specified using the options command. (e.g options(\"mc.cores\"=4);)\nsmooth = TRUE - For sets of data with missing data, the matrix will sometimes not be positive definite. Uses a procedure to transform the negative eigenvalues.\nFor relatively small samples with dichotomous data if some cells are empty, or if the resampled matrices are not positive semi-definite, warnings are issued. this leads to serious problems if using multi.cores. The solution seems to be to not use multi.cores (e.g., options(mc.cores =1)\n\nGoodman and Kruskal’s Gamma\n\nA measure of rank correlation, i.e., the similarity of the orderings of the data when ranked by each of the quantities. It measures the strength of association of the cross tabulated data when both variables are measured at the ordinal level.\nFor 2-way contingincy tables (i.e. 2x2 tables)\nIt makes no adjustment for either table size or ties.\nValues range from −1 (100% negative association, or perfect inversion) to +1 (100% positive association, or perfect agreement). A value of zero indicates the absence of association.\n{DescTools::GoodmanKruskalGamma}",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-mix",
    "href": "qmd/association-general.html#sec-assoc-gen-mix",
    "title": "General",
    "section": "Mixed",
    "text": "Mixed\n\nMisc\n\nAlso see\n\nPaper: JEL Ratio Test is non-parametric test that uses the categorical Gini covariance.\n\n{psych::mixedCor} - Finds Pearson correlations for the continous variables, polychorics for the polytomous items, tetrachorics for the dichotomous items, and the polyserial or biserial correlations for the various mixed variables (no polydi?)\n\nBiserial - correlation between a continuous variable and binary variable, which is assumed to have resulted from a dichotomized normal variable\n\n{psych::biserial}\n\nPolydi - correlation between multinomial variable and binary variable\n\n{psych::polydi}\n\nPolyserial - polychoric correlation between a continuous variable and ordinal variable\n\nBased on the assumption that the joint distribution of the quantitative variable and a latent continuous variable underlying the ordinal variable is bivariate normal\n{polycor}\n{psych::polyserial}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\nX2Y\n\nHandles types: continuous-continuous, continuous-categorical, categorical-continuous and categorical-categorical\nCalculates the % difference in prediction error after fitting a decision tree between two variables of interest and the mean (numeric) or most frequent (categorical)\nFunction is available through a script (Code &gt;&gt; statistical-testing &gt;&gt; correlation)\n\narticle with documentation and usage, https://rviews.rstudio.com/2021/04/15/an-alternative-to-the-correlation-coefficient-that-works-for-numeric-and-categorical-variables/\n\nAll x2y values where the y variable is continuous will be measuring a % reduction in MAE. All x2y values where the y variable is categorical will be measuring a % reduction in Misclassification Error. Is a 30% reduction in MAE equal to a 30% reduction in Misclassification Error? It is problem dependent, there’s no universal right answer.\n\nOn the other hand, since (1) all x2y values are on the same 0-100% scale (2) are conceptually measuring the same thing, i.e., reduction in prediction error and (3) our objective is to quickly scan and identify strongly-related pairs (rather than conduct an in-depth investigation), the x2y approach may be adequate.\n\nNot symmetric, but can average both scores to get a pseudo-symmetric value\nBootstrap CIs available\n\nCopulas\n\nlatentcor PKG: semi-parametric latent Gaussian copula models",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-nonlin",
    "href": "qmd/association-general.html#sec-assoc-gen-nonlin",
    "title": "General",
    "section": "Non-linear",
    "text": "Non-linear\n\nMisc\n\nAlso see General Additive Models &gt;&gt; Diagnostics for a method of determining a nonlinear relationship for either continuous or categorical outcomes.\n\nξ (xi) coefficient\n\nPaper: A New Coefficient of Correlation\nArticle: Exploring the XI Correlation Coefficient\nExcels at oscillatory and highly non-monotonic dependencies\nXICOR::xicor - calculates ξ and performs a significance test (H0: independent)\n\nXICOR::calculateXI just calculates the ξ coefficient\n\nProperties (value ranges; interpretation)\n\nIf y is a function of x, then ξ goes to 1 asymptotically as n (the number of data points, or the length of the vectors x and y) goes to Infinity.\nIf y and x are independent, then ξ goes to 0 asymptotically as n goes to Infinity.\n\nValues can be negative, but this negativity does not have any innate significance other than being close to zero\nn &gt; 20 necessary\n\nn larger than about 250 probably sufficient to get a good estimate\n\nFairly efficient (O(nlogn), compared to some more powerful methods, which are O(n2))\nIt measures dependency in one direction only (is y dependent on x not vice versa)\nDoesn’t tell you if the relationship is direct or inverse",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/base-r.html",
    "href": "qmd/base-r.html",
    "title": "Base R",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-misc",
    "href": "qmd/base-r.html#sec-baser-misc",
    "title": "Base R",
    "section": "",
    "text": "Magrittr + base\nmtcars %&gt;% {plot(.$hp, .$mpg)}\nmtcars %$% plot(hp, mpg)\n\nBy wrapping the RHS in curly braces, we can override the rule where the LHS is passed to the first argument ## Options {#sec-baser-opts .unnumbered}\n\nRemove scientific notation\noptions(scipen = 999)\nWide and long printing tibbles\n# in .Rprofile\nmakeActiveBinding(\".wide\", function() { print(.Last.value, width = Inf) }, .GlobalEnv)\n\nAfter printing a tibble, if you want to see it in wide, then just type .wide + ENTER.\nCan have similar bindings for `.long` and `.full`.\n\nHeredocs - Powerful feature in various programming languages that allow you to define a block of text within the code, preserving line breaks, indentation, and other whitespace.\ntext &lt;- r\"(\nThis is a\nmultiline string\nin R)\"\n\ncat(text)",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-usrfuns",
    "href": "qmd/base-r.html#sec-baser-usrfuns",
    "title": "Base R",
    "section": "User Defined Functions",
    "text": "User Defined Functions\n\nAnonymous (aka lambda) functions: \\(x) {} (&gt; R 4.1)\nfunction(x) {\n  x[which.max(x$mpg), ]\n}\n# equivalent to the above\n\\(x) {\n  x[which.max(x$mpg), ]\n}\nDots (…)\n\nMisc\n\n{ellipsis}: Functions for testing functions with dots so they fail loudly\n{rlang} dynamic dots: article\n\nSplice arguments saved in a list with the splice operator, !!! .\nInject names with glue syntax on the left-hand side of := .\n\n\nUser Defined Functions\nmoose &lt;- function(...) {\n    dots &lt;- list(...)\n    dots_names &lt;- names(dots)\n    if (is.null(dots_names) || \"\" %in% dots_names {\n        stop(\"All arguments must be named\")\n    }\n}\nNested Functions\nf02 &lt;- function(...){\n  vv &lt;- list(...)\n  print(vv)\n}\nf01 &lt;- function(...){\n  f02(b = 2,...)\n}\n\nf01(a=1,c=3)\n#&gt; $b\n#&gt; [1] 2\n#&gt; \n#&gt; $a\n#&gt; [1] 1\n#&gt; \n#&gt; $c\n#&gt; [1] 3\nSubset dots values\nadd2 &lt;- function(...) {\n    ..1 + ..2\n}\nadd2(3, 0.14)\n# 3.14\nSubset dots dynamically: ...elt(n)\n\nSet a value to n and get back the value of that argument\n\nNumber of arguments in … : ...length()",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-funs",
    "href": "qmd/base-r.html#sec-baser-funs",
    "title": "Base R",
    "section": "Functions",
    "text": "Functions\n\ndo.call - allows you to call other functions by constructing the function call as a list\n\nArgs\n\nwhat – Either a function or a non-empty character string naming the function to be called\nargs – A list of arguments to the function call. The names attribute of args gives the argument names\nquote – A logical value indicating whether to quote the arguments\nenvir – An environment within which to evaluate the call. This will be most useful if what is a character string and the arguments are symbols or quoted expressions\n\nExample: Apply function to list of vectors\nvectors &lt;- list(c(1, 2, 3), c(4, 5, 6), c(7, 8, 9))\ncombined_matrix &lt;- do.call(rbind, vectors)\n\ncombined_matrix\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    5    6\n## [3,]    7    8    9\nExample: Apply multiple functions\ndata_frames &lt;- list(\n  data.frame(a = 1:3), \n  data.frame(a = 4:6), \n  data.frame(a = 7:9)\n  )\nmean_results &lt;- do.call(\n  rbind, \n  lapply(data_frames, function(df) mean(df$a))\n  )\n\nmean_results\n##      [,1]\n## [1,]    2\n## [2,]    5\n## [3,]    8\n\nFirst the mean is calculated for column a of each df using lapply\n\nlapply is supplying the data for do.call in the required format, which is a list or character vector.\n\nSecond the results are combined into a matrix with rbind\n\n\nsink - used to divert R output to an external connection.\n\nUse Cases: exporting data to a file, logging R output, or debugging R code.\nArgs\n\nfile: The name of the file to which R output will be diverted. If file is NULL, then R output will be diverted to the console.\nappend: A logical value indicating whether R output should be appended to the file (TRUE) or overwritten (FALSE). The default value is FALSE.\ntype: A character string. Either the output stream or the messages stream. The name will be partially match so can be abbreviated.\nsplit: logical: if TRUE, output will be sent to the new sink and the current output stream, like the Unix program tee.\n\nExample: Logging output of code to file\nsink(\"r_output.log\")      # Redirect output to this file\n# Your R code goes here\nsink()                    # Turn off redirection\n\noutput file could also have an extension like “.txt”\n\nExample: Debugging\nsink(\"my_function.log\")   # Redirect output to this file\nmy_function()\nsink()                    # Turn off redirection\nExample: Appending output to a file\nsink(\"output.txt\", append = TRUE)  # Append output to the existing file\ncat(\"Additional text\\n\")  # Append custom text\nplain text\nsink()  # Turn off redirection\n\npmin and pmax\n\nFind the element-wise maximum and minimum values across vectors in R\nExample\nvec1 &lt;- c(3, 9, 2, 6)\nvec2 &lt;- c(7, 1, 8, 4)\npmax(vec1, vec2)\n#&gt; [1] 7 9 8 6\npmin(vec1, vec2)\n#&gt; [1] 3 1 2 4\nExample: With NAs\ndata1 &lt;- c(7, 3, NA, 12)\ndata2 &lt;- c(9, NA, 5, 8)\npmax(data1, data2, na.rm = TRUE)\n#&gt; [1] 9 3 5 12\n\nswitch\n\nExample:\nswitch(parallel,\n         windows = \"snow\" -&gt; para_proc,\n         other = \"multicore\" -&gt; para_proc,\n         no = \"no\" -&gt; para_proc,\n         stop(sprintf(\"%s is not one of the 3 possible parallel argument values. See documentation.\", parallel)))\n\nparallel is the function argument. If it doesn’t match one of the 3 values, then an error is thrown.\nIf the argument value is matched, then the quoted value is stored in para_proc\n\n\ndynGet\n\nLooks for objects in the environment of a function.\nWhen an object from the outer function is an input for a function nested around 3 layers deep or more, it may not be found by that most inner function. dynGet allows that function to find the object in the outer frame\nArguments\n\nminframe: Integer specifying the minimal frame number to look into (i.e. how far back to look for the object)\ninherits: Should the enclosing frames of the environment be searched?\n\nExample:\n1function(args) {\n  if (method == \"kj\") {\n      ncv_list &lt;- purrr::map2(grid$dat, \n                              grid$repeats, \n                              function(dat, reps) {\n         rsample::nested_cv(dat,\n                            outside = vfold_cv(v = 10, \n                                               repeats = dynGet(\"reps\")),\n                            inside = bootstraps(times = 25))\n      })\n  }\n}\n\n2function(data) {\n    if (chk::vld_used(...)) {\n        dots &lt;- list(...)\n        init_boot_args &lt;-\n          list(data = dynGet(\"data\"),\n               stat_fun = cles_boot, # internal function\n               group_variables = group_variables,\n               paired = paired)\n        get_boot_args &lt;-\n          append(init_boot_args,\n                 dots)\n    }\n    cles_booted &lt;-\n      do.call(\n        get_boot_ci,\n        get_boot_args\n      )\n}\n\n1\n\nExample from Nested Cross-Validation Comparison\n\n2\n\nExample from {ebtools::cles}\n\n\n\nmatch.arg\n\nPartially matches a function’s argument values to list of choices. If the value doesn’t match the choices, then an error is thrown\nExample:\nkeep_input &lt;- \"input_le\"\nkeep_input_val &lt;- \n  match.arg(keep_input,\n            choices = c(\"input_lags\",\n                        \"input_leads\",\n                        \"both\"),\n            several.ok = FALSE)\nkeep_input_val\n#&gt; [1] \"input_leads\"\n\nseveral.ok = FALSE says only 1 match is allowed otherwise an error is thrown.\nThe error message is pretty informative btw.\n\n\nmatch.fun\n\nExample\nf &lt;- function(a,b) {\n  a + b\n}\ng &lt;- function(a,b,c) {\n  (a + b) * c\n}\nh &lt;- function(d,e) {\n  d - e\n}\nyolo &lt;- function(FUN, ...) {\n  FUN &lt;- match.fun(FUN)\n  params &lt;- list(...)\n  FUN_formals &lt;- formals(FUN)\n  idx &lt;- names(params) %in% names(FUN)\n  do.call(FUN, params[idx])\n}\nyolo(h, d = 2, e = 3)\n#&gt; -1",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-pipe",
    "href": "qmd/base-r.html#sec-baser-pipe",
    "title": "Base R",
    "section": "Pipe",
    "text": "Pipe\n\n\nBenefits of base pipe\n\nMagrittr pipe is bloated with special features which may make it slower than the base pipe\nIf not using tidyverse, it’s one less dependency (maybe one day it will be deprecated in tidyverse)\n\nBase pipe with base and anonymous functions\n# verbosely\nmtcars |&gt; (function(.) plot(.$hp, .$mpg))()\n# using the anonymous function shortcut, emulating the dot syntax\nmtcars |&gt; (\\(.) plot(.$hp, .$mpg))()\n# or if you prefer x to .\nmtcars |&gt; (\\(x) plot(x$hp, x$mpg))()\n# or if you prefer to be explicit with argument names\nmtcars |&gt; (\\(data) plot(data$hp, data$mpg))()\nUsing “_” placeholder:\n\nmtcars |&gt; lm(mpg ~ disp, data = _)\nmtcars |&gt; lm(mpg ~ disp, data = _) |&gt; _$coef\n\nBase pipe .[ ]  hack\nwiki |&gt;\n  read_html() |&gt;\n  html_nodes(\"table\") |&gt;\n  (\\(.) .[[2]])() |&gt;\n  html_table(fill = TRUE) |&gt;\n  clean_names()\n# instead of\ndjia &lt;- wiki %&gt;%\n  read_html() %&gt;%\n  html_nodes(\"table\") %&gt;%\n  .[[2]] %&gt;%\n  html_table(fill = TRUE) %&gt;%\n  clean_names()\nMagrittr, base pipe differences\n\nmagrittr: %&gt;% allows you change the placement with a . placeholder.\n\nbase: R 4.2.0 added a _ placeholder to the base pipe, with one additional restriction: the argument has to be named\n\nmagrittr: With %&gt;% you can use . on the left-hand side of operators like “\\(\", \\[\\[, \\[ and use in multiple arguments (e.g. df %&gt;% {split(.\\)x, .$y))\n\nbase: can hack this by using anonymous function\n\nsee Base pipe with base and anonymous functions above\nsee Base pipe .[ ]  hack above\n\n\nmagrittr: %&gt;% allows you to drop the parentheses when calling a function with no other arguments (e.g. dat %&gt;% distinct)\n\nbase: |&gt; always requires the parentheses. (e.g. dat |&gt; distinct())\n\nmagrittr: %&gt;% allows you to start a pipe with . to create a function rather than immediately executing the pipe\n\nPurrr with base pipe\ndata_list |&gt;\n  map(\\(x) clean_names(x))\n# instead of\ndata_list %&gt;%\n  map( ~.x %&gt;% clean_names)\n# with split\nstar |&gt;\n  split(~variable) |&gt;\n  map_df(\\(.) hedg_g(., reading ~ value), .id = \"variable\")\n# instead of\nstar %&gt;%\n  split(.$variable) %&gt;%\n  map_df(. %&gt;% hedg_g(., reading ~ value), .id = \"variable\")",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-str",
    "href": "qmd/base-r.html#sec-baser-str",
    "title": "Base R",
    "section": "Strings",
    "text": "Strings\n\nsprintf\nx &lt;- 123.456               # Create example data\n\nsprintf(\"%f\", x)           # sprintf with default specification\n#&gt; [1] \"123.456000\"\n\nsprintf(\"%.10f\", x)        # sprintf with ten decimal places\n#&gt; [1] \"123.4560000000\"\n\nsprintf(\"%.2f\", x)         # sprintf with two rounded decimal places\n#&gt; [1] \"123.46\"\n\nsprintf(\"%1.0f\", x)        # sprintf without decimal places\n#&gt; [1] \"123\"\n\nsprintf(\"%10.0f\", x)       # sprintf with space before number\n#&gt; [1] \"       123\"\n\nsprintf(\"%10.1f\", x)       # Space before number & decimal places\n#&gt; [1] \"     123.5\"\n\nsprintf(\"%-15f\", x)        # Space on right side\n#&gt; [1] \"123.456000     \"\n\nsprintf(\"%+f\", x)          # Print plus sign before number\n#&gt; [1] \"+123.456000\"\n\nsprintf(\"%e\", x)           # Exponential notation\n#&gt; [1] \"1.234560e+02\"\n\nsprintf(\"%E\", x)           # Exponential with upper case E\n#&gt; [1] \"1.234560E+02\"\n\nsprintf(\"%g\", x)           # sprintf without decimal zeros\n#&gt; [1] \"123.456\"\n\nsprintf(\"%g\", 1e10 * x)    # Scientific notation\n#&gt; [1] \"1.23456e+12\"\n\nsprintf(\"%.13g\", 1e10 * x) # Fixed decimal zeros\n#&gt; [1] \"1234560000000\"\n\npaste0(sprintf(\"%f\", x),   # Print %-sign at the end of number\n       \"%\")\n#&gt; [1] \"123.456000%\"\n\nsprintf(\"Let's create %1.0f more complex example %1.0f you.\", 1, 4)\n#&gt; [1] \"Let's create 1 more complex example 4 you.\"\nstr2lang - Allows you to turn plain text into code.\ngrowth_rate &lt;- \"circumference / age\"\nclass(str2lang(growth_rate))\n#&gt; [1] \"call\"\n\nExample: Basic\neval(str2lang(\"2 + 2\"))\n#&gt; [1] 4\n\neval(str2lang(\"x &lt;- 3\"))\nx\n#&gt; [1] 3\nExample: Run formula against a df\ngrowth_rate &lt;- \"circumference / age\"\nwith(Orange, eval(str2lang(growth_rate)))\n\n#&gt;   [1] 0.25423729 0.11983471 0.13102410 0.11454183 0.09748172 0.10349854\n#&gt;   [7] 0.09165613 0.27966102 0.14256198 0.16716867 0.15537849 0.13972380\n#&gt;  [13] 0.14795918 0.12831858 0.25423729 0.10537190 0.11295181 0.10756972\n#&gt;  [19] 0.09341998 0.10131195 0.08849558 0.27118644 0.12809917 0.16867470\n#&gt;  [25] 0.16633466 0.14541024 0.15233236 0.13527181 0.25423729 0.10123967\n#&gt;  [31] 0.12198795 0.12450199 0.11535337 0.12682216 0.11188369",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-cond",
    "href": "qmd/base-r.html#sec-baser-cond",
    "title": "Base R",
    "section": "Conditionals",
    "text": "Conditionals\n\n&& and || are intended for use solely with scalars, they return a single logical value.\n\nSince they always return a scalar logical, you should use && and || in your if/while conditional expressions (when needed). If an & or | is used, you may end up with a non-scalar vector inside if (…) {} and R will throw an error.\n\n& and | work with multivalued vectors, they return a vector whose length matches their input arguments.\nAlternative way of negating a condition or set of conditions: if (!(condition))\n\nMakes it less readable IMO, but maybe for a complicated set of conditions if makes more sense in your head to do it this way\nExample\nif (!(nr == nrow(iris) || (nr == nrow(iris) - 2))) {print(\"moose\")}\n\nUsing else if\nif (condition1) {\n  expr1\n} else if (condition2) {\n  expr2\n} else {\n  expr3\n}\nstopifnot\npred_fn &lt;- function(steps_forward, newdata) {\n  stopifnot(steps_forward &gt;= 1)\n  stopifnot(nrow(newdata) == 1)\n  model_f = model_map[[steps_forward]]\n  # apply the model to the last \"before the test period\" row to get\n  # the k-steps_forward prediction\n  as.numeric(predict(model_f, newdata = newdata))\n}\n%||%\n\nCollapse operator which acts like:\n`%||%` &lt;- function(x, y) {\n   if (is_null(x)) y else x\n}\n\nSays if the first (left-hand) input x is NULL, return y. If x is not NULL, return the input\n\nUse Cases\n\nDetermine whether a function argument is NULL\ngithub_remote &lt;- \n  function(repo, username = NULL, ...) {\n    meta &lt;- parse_git_repo(repo)\n    meta$username &lt;- username %||%\n      getOption(\"github.user\") %||%\n      stop(\"Unknown username\")\n  }\nWithin the print argument collapse\nlibrary(rlang)\n\nadd_commas &lt;- function(x) {\n  if (length(x) &lt;= 1) {\n    collapse_arg &lt;- NULL\n  } else {\n    collapse_arg &lt;- \", \"\n  }\n  print(paste0(x, collapse = collapse_arg %||% \"\"))\n}\n\nadd_commas(c(\"apples\"))\n[1] \"apples\"\nadd_commas(c(\"apples\", \"bananas\"))\n[1] \"apples, bananas\"",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-sort",
    "href": "qmd/base-r.html#sec-baser-sort",
    "title": "Base R",
    "section": "Ordering Columns and Sorting Rows",
    "text": "Ordering Columns and Sorting Rows\n\nAscending: df[with(df, order(value)), ]\n\n“value” is the column used to sort the df by\n\nDescending: df[with(df, order(-value)), ]\nBy Multiple Columns\n\nDescending then Ascending: df[with(df, order(-value, id)), ]\n\nChange position of columns\n# Reorder column by index manually\ndf2 &lt;- df[, c(5, 1, 4, 2, 3)]\ndf3 &lt;- df[, c(1, 5, 2:4)]\n# Reorder column by name manually\nnew_order = c(\"emp_id\",\"name\",\"superior_emp_id\",\"dept_id\",\"dept_branch_id\")\ndf2 &lt;- df[, new_order]",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#set-operations",
    "href": "qmd/base-r.html#set-operations",
    "title": "Base R",
    "section": "Set Operations",
    "text": "Set Operations\n\nUnique values in A that are not in B\na &lt;- c(\"thing\", \"object\")\nb &lt;- c(\"thing\", \"gift\")\n\nunique(a[!(a %in% b)])\n#&gt; [1] \"object\"\n\nsetdiff(a, b)\n\nsetdiff is slower\n\nUnique values of the two vectors combined\nunique(c(a, b))\n#&gt; [1] \"thing\"  \"object\" \"gift\"\n\nunion(a, b)\n\nunion is just a wrapper for unique",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-subset",
    "href": "qmd/base-r.html#sec-baser-subset",
    "title": "Base R",
    "section": "Subsetting",
    "text": "Subsetting\n\nLists and Vectors\n\nRemoving Rows\n# Remove specific value from vector\nx[!x == 'A']\n\n# Remove multiple values by list\nx[!x %in% c('A', 'D', 'E')]\n\n# Using setdiff\nsetdiff(x, c('A','D','E'))\n\n# Remove elements by index\nx[-c(1,2,5)]\n\n# Using which\nx[-which(x %in% c('D','E') )]\n\n# Remove elements by name\nx &lt;- c(C1='A',C2='B',C3='C',C4='E',C5='G')\nx[!names(x) %in% c('C1','C2')]\n\nDataframes\n\nRemove specific Rows\ndf &lt;- df[-c(25, 3, 62), ]\nRemove column by name\ndf &lt;- df[, which(names(df) == \"col_name\")]\ndf &lt;- subset(df, select = -c(col_name))\ndf &lt;- df[, !names(df) %in% c(\"col1\", \"col2\"), drop = FALSE]\nFilter and Select\ndf &lt;- subset(df, subset = col1 &gt; 56, select = c(col2, col3))",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#joins",
    "href": "qmd/base-r.html#joins",
    "title": "Base R",
    "section": "Joins",
    "text": "Joins\n\nInner join: inner &lt;- merge(flights, weather, by = mergeCols)\nLeft (outer) join: left  &lt;- merge(flights, weather, by = mergeCols, all.x = TRUE)\nRight (outer) join: right &lt;- merge(flights, weather, by = mergeCols, all.y = TRUE)\nFull (outer) join: full &lt;- merge(flights, weather, by = mergeCols, all = TRUE)\nCross Join (Cartesian product): cross &lt;- merge(flights, weather, by = NULL)\nNatural join: natural &lt;- merge(flights, weather)",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-err",
    "href": "qmd/base-r.html#sec-baser-err",
    "title": "Base R",
    "section": "Error Handling",
    "text": "Error Handling\n\nstop\n\nExample:\nswitch(parallel,\n       windows = \"snow\" -&gt; para_proc,\n       other = \"multicore\" -&gt; para_proc,\n       no = \"no\" -&gt; para_proc,\n       stop(sprintf(\"%s is not one of the 3 possible parallel argument values. See documentation.\", parallel)))\n\nparallel is the function argument. If it doesn’t match one of the 3 values, then an error is thrown.\n\n\ntry\n\nIf something errors, then do something else\nExample\n\ncurrent &lt;- try(remDr$findElement(using = \"xpath\",\n                                '//*[contains(concat( \" \", @class, \" \" ),\n                                concat( \" \", \"product-price-value\", \" \" ))]'),\n                                silent = T)\n#If error : current price is NA\nif(class(current) =='try-error'){\n    currentp[i] &lt;- NA\n} else {\n    # do stuff\n}\ntryCatch\n\nRun the main code, but if it “catches” an error, then the secondary code (the workaround) will run.\n\npct_difference_error_handling &lt;- function(n1, n2) {\n# Try the main code\n  tryCatch(pct_diff &lt;- (n1-n2)/n1,\n        # If you find an error, use this code instead\n          error = return(\n            cat( 'The difference between', as.integer(n1), 'and', as.integer(n2), 'is',\n                  (as.integer(n1)-as.integer(n2)), 'which is',\n                  100*(as.integer(n1)-as.integer(n2))/as.integer(n1),\n                  '% of', n1 )#cat\n            ),\n          # finally = print('Code ended') # optional\n          )#trycatch\n  # If no error happens, return this statement\n  return ( cat('The difference between', n1, 'and', n2, 'is', n1-n2,\n              ', which is', pct_diff*100, '% of', n1) )\n}\n\nAssumes the error will be the user enters a string instead of a numeric. If errors, converts string to numeric and calcs.\n“finally” - This argument will always run, regardless if the try block raises an error or not. So it could be a completion message or a summary, for example.",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-mods",
    "href": "qmd/base-r.html#sec-baser-mods",
    "title": "Base R",
    "section": "Models",
    "text": "Models\n\nreformulate - Create formula sytax programmatically\n# Creating a formula using reformulate()\nformula &lt;- reformulate(c(\"hp\", \"cyl\"), response = \"mpg\")\n\n# Fitting a linear regression model\nmodel &lt;- lm(formula, data = mtcars)\n\nformula\n##&gt; mpg ~ hp + cyl\n\nCan also use as.formula\n\nDF2formula - Turns the column names from a data frame into a formula. The first column will become the outcome variable, and the rest will be used as predictors\nDF2formula(Orange)\n#&gt; Tree ~ age + circumference\nformula - Provides a way of extracting formulae which have been included in other objects\nrec_obj |&gt; prep() |&gt; formula()\n\nWhere “rec_obj” is a tidymodels recipe object\n\nData from Model Object\n\nmodel$model return the data dataframe\ndeparse(model$call$data) gives you that name of your data object as a string.\n\nmodel$call$data gives you the data as an unevaluated symbol;\n\neval(model$call$data) gives you back the original data object, if it is available in the current environment.",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/privacy.html",
    "href": "qmd/privacy.html",
    "title": "Privacy",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/privacy.html#sec-priv-misc",
    "href": "qmd/privacy.html#sec-priv-misc",
    "title": "Privacy",
    "section": "",
    "text": "Also see Simulation, Data",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/privacy.html#sec-priv-tag",
    "href": "qmd/privacy.html#sec-priv-tag",
    "title": "Privacy",
    "section": "Tags",
    "text": "Tags\n\nTag sensitive information in dataframes\nnames(df)\n[1] \"date\" \"first_name\" \"card_number\" \"payment\"\n# assign pii tags\nattr(df, \"pii\") &lt;- c(\"name\", \"ccn\", \"transaction\")\n\nPersonally Identifiable Information (PII)\n\nTag dataframes with the names of regulations that are applicable\nattr(df, \"regs\") &lt;- c(\"CCPA\", \"GDPR\", \"GLBA\")\n\nCCPA is the privacy regulation for California\nGDPR is the privacy regulation for the European Union\nGLBA is the financial regulation for the United States\n\nNeeded because df has credit card and financial information\n\nSaving objects as .rds files preserves tags",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/privacy.html#sec-priv-hash",
    "href": "qmd/privacy.html#sec-priv-hash",
    "title": "Privacy",
    "section": "Hashing",
    "text": "Hashing\n\n{digest}\n\nHash Function\n\nApply Hash Function to PII Fields",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#debugging",
    "href": "qmd/cli-linux.html#debugging",
    "title": "Linux",
    "section": "Debugging",
    "text": "Debugging\n\n\nAlso see set -o xtrace in Scripting &gt;&gt; Commands that should start your script",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html",
    "href": "qmd/regression-survival.html",
    "title": "43  Survival",
    "section": "",
    "text": "43.1 Misc",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-misc",
    "href": "qmd/regression-survival.html#sec-reg-surv-misc",
    "title": "43  Survival",
    "section": "",
    "text": "Model for estimating the time until a particular event occurs\n\ne.g. death of a patient being treated for a disease, failure of an engine part in a vehicle\n\nPrediction models for survival outcomes are important for clinicians who wish to estimate a patient’s risk (i.e. probability) of experiencing a future outcome. The term ‘survival’ outcome is used to indicate any prognostic or time-to-event outcome, such as death, progression, or recurrence of disease. Such risk estimates for future events can support shared decision making for interventions in high-risk patients, help manage the expectations of patients, or stratify patients by disease severity for inclusion in trials.1 For example, a prediction model for persistent pain after breast cancer surgery might be used to identify high risk patients for intervention studies\nOutcome variable: Time until event occurs\nPackages\n\nCRAN Task View\n{survival}\n{censored} - {tidymodels} for censored and survival modelling\n{quantreg} - Quantile Survival Regression\n{msm} - Multi-State Models\n\nVignette\nSee Multistate Models for Medical Applications\n\nTutorial using a heart transplant dataset\n\nStandard survival models only directly model two states: alive and dead. Multi-state models enable directly modeling disease progression where patients are observed to be in various states of health or disease at random intervals, but for which, except for death, the times of entering or leaving states are unknown.\nMulti-state models easily accommodate interval censored intermediate states while making the usual assumption that death times are known but may be right censored.\n\n{grf} - Generalized Random Forest; Causal forest with time-to-event data\n{partykit} - Conditional inference trees; Model-based recursive partitioning trees; can be used with {survival} to create random survival forests\n\n{bonsai}: tidymodels, partykit conditional trees, forests; successor to treesnip — Model Wrappers for Tree-Based Models\n\n{aorsf} - Optimized software to fit, interpret, and make predictions with oblique random survival forests (ORSFs)\n{{sklearn}} - Random Survival Forests, Survival Support Vector Machine\n{rmstbart} - Prognostic model that directly targets the RMST (see Terms) as a function of baseline covariates. The interpretation of each estimated RMST is transparent and does not require a proportional hazards assumption plus additional modeling of a baseline hazard function.\n\nFrom paper: Generalized Bayesian Additive Regression Trees for Restricted Mean Survival Time Inference. (Code)\n\n\nNotes from\n\nWhat is Cox’s proportional hazards model?\n\nWhy not use a standard regression model?\n\nUnits that “survive” until the end of the study will have a censored survival time.\n\ni.e. We won’t have an observed survival time for these units because they survive for an unknown time after the study is completed.\nWe don’t want to discard these units though, as they still have useful information.\n\n\nSample Size\nModels\n\nKaplan Meier model (i.e. K-M survival curve)\n\nOften used as a baseline in survival analysis\nCan not be used to compare risk between groups and compute metrics like the hazard ratio\n\nExponential model, the Weibull model, Cox Proportional-Hazards, Log-logistic and the Accelerated Failure Time (AFT)\nMulti-State Models\nHazard rates and Cumulative Hazard rates are typical quantities of interest\n\nLog-Rank Test (aka Mantel-Cox test) - tests if two groups survival curves are different\n\nNon-Parametric; a special case with one binary X\nThe intuition behind the test is that if the two groups have different hazard rates, the two survival curves (so their slopes) will differ.\nCompares the observed number of events in each group to what would be expected if the survival curves were identical (i.e., if the null hypothesis were true).\nExample\nlibrary(survival)\ndat &lt;- data.frame(\n  group = c(rep(1, 6), rep(2, 6)),\n  time = c(4.1, 7.8, 10, 10, 12.3, 17.2, 9.7, 10, 11.1, 13.1, 19.7, 24.1),\n  event = c(1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0)\n)\ndat\n##    group time event\n## 1      1  4.1    1\n## 2      1  7.8    0\n## 3      1 10.0    1\n## 4      1 10.0    1\n## 5      1 12.3    0\n## 6      1 17.2    1\n## 7      2  9.7    1\n## 8      2 10.0    1\n## 9      2 11.1    0\n## 10    2 13.1    0\n## 11    2 19.7    1\n## 12    2 24.1    0\nsurvdiff(Surv(time, event) ~ group,\n  data = dat\n)\n##        N Observed Expected (O-E)^2/E (O-E)^2/V\n## group=1 6        4    2.57    0.800      1.62\n## group=2 6        3    4.43    0.463      1.62\n## \n##  Chisq= 1.6  on 1 degrees of freedom, p= 0.2\n\n# plot curves with pval from test\nfit &lt;- survfit(Surv(time, event) ~ group, data = dat)\nggsurvplot(fit,\n  pval = TRUE,\n  pval.method = TRUE\n)\n\npval &gt; 0.05, so there isn’t enough evidence to that they’re different.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-terms",
    "href": "qmd/regression-survival.html#sec-reg-surv-terms",
    "title": "43  Survival",
    "section": "43.2 Terms",
    "text": "43.2 Terms\n\nCensoring Time (C): Time at which censoring occurs\n\nFor each unit, we observe Survival Time (T) or C.\n\nY = min(T, C)\n\nRight Censoring: occurs when the event has happened after the enrollment (but the time is unknown).\n\nThe patient does not experience the event for the whole duration of the study.\nThe patient withdraws from the study.\nThe patient is lost to follow-up.\n\nLeft Censoring: occurs when the event has happened before the enrollment (but the time is unknown).\n\nCumulative hazard function (aka Cumulative Hazard Rates)\n\nShows the total accumulated risk of an event occurring at time t\nThe area under the hazard function\n\nHazard Rate (aka Risk Score), \\(h(t \\;|\\; X)\\)\n\nThe hazard rate is the probability that a unit with predictors, X, will experience an event at time, t, given that the unit has survived just before time, t.\nThe formula for the Hazard Rate is the Hazard function.\n\nHazard Ratio (aka Relative Risk of an event): Risk of an event given category / risk of an event given by reference category\n\nThe ratio of two instantaneous event rates\nCoefficient of the Cox Proportional Hazards model (e.g. paper)\n\n\neβ &gt; 1 (or β &gt; 0) for an increased risk of event (e.g. death).\neβ &lt; 1 (or β &lt; 0) for a reduced risk of event.\nHR of 2 is equivalent to raising the entire survival curve for a control subject to the second power to get the survival curve for an exposed subject\n\nExample: if a control subject has 5y survival probability of 0.7 and the exposed:control HR is 2, the exposed subject has a 5y survival probability of 0.49\nIf the HR is 1/2, the exposed subject has a survival curve that is the square root of the control, so S(5) would be √0.7 = 0.837\n\n\n\nRestricted Mean Survival Time (RMST) - The expected survival duration up to a pre-specified truncation time, \\(\\tau\\). It has direct clinical interpretation, which holds regardless of the survival model used in an analysis. Changes in RMST are often cited as an alternative to hazard ratios in the context of treatment comparisons and in survival regression modeling.\n\nUnlike proportional hazards, the interpretation of change in RMST holds regardless of whether or not a particular survival regression model holds.\n\nStatus indicator, \\(\\delta\\)\n\nδ = 1, if T ≤ C (e.g. unit fails before study ends)\n\nTrue survival time is observed\n\nδ = 0, if T &gt; C (e.g. unit survives until end of study or has dropped out)\n\nCensoring time is observed\n\n\nSurvival function (aka Survival Rate), \\(S(T \\lt t)\\):\n\nOutputs the probability of a subject surviving (i.e., not experiencing the event) beyond time t\nMonotonically decreasing (i.e. level or decreasing)\nBaseline survival curve illustrates the survival function when all the covariates are set to their median value\n\nSurvival Time (T) (aka Death, Failure Time, Event Time): Time at which the event occurs",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-km",
    "href": "qmd/regression-survival.html#sec-reg-surv-km",
    "title": "43  Survival",
    "section": "43.3 Kaplan-Meir",
    "text": "43.3 Kaplan-Meir\n\nMisc\n\nUseful for validation of Proportional Hazards assumption. When lines cross the assumption, hazards are found to be non-proportional.\nHarrell RMS (Ch. 20.3):\n\nFor external validation: at least 200 events\nNeed 184 subjects with an event, or censored late, to estimate to within a margin of error of 0.1 everywhere, at the 0.95 confidence level\n\n\nOrder event times (T) of units from smallest to largest, t1 &lt; …. &lt; tk\nCalculate probability that a unit survives past event time, ti, given that they survived up until event time, ti (i.e. past ti-1) (conditional probability)\n\ne.g. for t1, it’s (n1 - d1) / n1\n\nn1 is the number of units that have survived at t1\nd1 is the number of units that have experienced the event (e.g. died) at t1\nSimilar for other t values\n\nMedian survival time is where the survival probability equals 0.5\n\nSurvival function[](./_resources/Regression,_Survival.resources/1-bp-FZyIs1WlqYgQMWn9Obw.gif]]\n\nThe survival function computes the products of these probabilities resulting in the K-M survival curve\nThe product of these conditional probabilities reflects the fact that to survive past event time, t, a unit must have survived all previous event times and the current event time.\n\nExample: 50 patients\n\n\nDotted lines represent 95% CI\nRed dots indicate time when patients died (aka event times)\nMedian survival time is ~ 13yrs",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-exp",
    "href": "qmd/regression-survival.html#sec-reg-surv-exp",
    "title": "43  Survival",
    "section": "43.4 Exponential",
    "text": "43.4 Exponential\n\nAssumes that the hazard rate is constant\n\ni.e. risk of the event of interest occurring remains the same throughout the period of observation\n\nSurvival function\n\nHazard function\n\n\nh(t) is the constand hazard rate\n\nEstimated parameter: λ",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-weibull",
    "href": "qmd/regression-survival.html#sec-reg-surv-weibull",
    "title": "43  Survival",
    "section": "43.5 Weibull",
    "text": "43.5 Weibull\n\nAssumes the change in hazard rate is linear.\nSurvival function\n\nHazard function\n\nEstimated parameters: λ and ρ\n\nλ parameter indicates how long it takes for 63.2% of the subjects to experience the event.\nρ parameter indicates whether the hazard rate is increasing, decreasing, or constant.\n\nIf ρ is greater than 1, the hazard rate is constantly increasing.\nIf ρ is less than 1, the hazard rate is constantly decreasing.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-coxph",
    "href": "qmd/regression-survival.html#sec-reg-surv-coxph",
    "title": "43  Survival",
    "section": "43.6 Cox’s Proportional Hazards",
    "text": "43.6 Cox’s Proportional Hazards\n\nMultivariable regression model\nAllows the hazard rate to fluctuate\nHarrell: “under PH [assumption] and absence of covariate interactions, HR is a good overall effect estimate for binary [treatment]”\nMisc\n\nPackages\n\n{glmnet} - Regularized Cox Regression\n{coxphf} - Cox Regression with Firth’s Penalized Likelihood\n\nSee Regression, Regularized &gt;&gt; Firth’s Estimator\n\n\nSample Size\n\nHarrell RMS (Ch. 20.3):\n\nTo achieve a Multiplicative Margin of Error (MMOE ) of 1.2 (?) in estimating eβ^ with equal numbers of events in the two groups (balanced, binary treatment variable) and α = 0.05 → requires a total of 462 events\n\n\n\nAssumes\n\nHazard ratios (ratio of hazard rates or exp(β) between groups/units remain constant\n\ni.e. no matter how the hazard rates of the subjects change during the period of observation, the hazard rate of one group relative to the other will always stay the same\n\nHazard Ratios are independent of time\nExample: Immunotherapy typically violates PH assumptions (post)\n\nThe survival probabilities between the treatment (blue) and the chemo (red) cross at around the 4.2 months\n\nThe distance between the lines should remain somewhat constant throughout the trial in order to adhere to the PH assumptions (1st assumption)\nAlso think the lines should be somewhat straight. (2nd assumption)\n\nPatients in immunotherapy drug trials often experience a period of toxicity, but if they survive this period, they have a much better outcome down the road.\n\nTests\n\nGrambsch and Therneau (G&T)\nSee Harrell RMS (Ch. 20.6.2)\n\nIf assumptions are violated,\n\nGelman says to try and “expand the model, at the very least by adding an interaction.” (post)\nSee Harrell RMS (Ch. 20.7)\nUse a different model\n\nAccelerated Failure Time (AFT) model (See ML &gt;&gt; Gradient Boosting Survival Trees)\nAdjusted Cox PH model with Time-Varying Coefficients\n\nMust choose a functional form describing how the effect of the treatment changes over time\n\nRecommended to use AIC criteria to guide one’s choice among a large number of candidates\n\n\n\n\n\nModels event time (T) outcome variable and outputs parameter estimates for treatment (X) effects\n\nProvides a way to have time-dependent (i.e. repeated measures) explanatory variables (e.g. age, health status, biomarkers)\nCan handle other types of censoring such as left or interval censoring.\nHas extensions such as lasso to handle high dimensional data\nDL and ML models also have versions of this method\n\nHazard function\n\nh(t |X) = h0(t)exβ\nThe hazard rate for a unit with predictors, X, is the product of a baseline hazard, h0(t) (corresponding to X = 0) and a factor that depends on X and the regression parameters, β\nOptimized to yield partial maximum likelihood estimates, β^.\n\nDoesn’t require the specification of h0(t) which makes the method flexible and robust\n\n\nHazard Ratio (aka relative risk) for a binary covariate (e.g. treatment)  = eᶜᵒᵉᶠ\nInterpretation:\n\nHazard Rate (risk of the event): Moving from the reference category to the other category changes the hazard rate by a factor of eᶜᵒᵉᶠ\n\ne.g. eᶜᵒᵉᶠ = 0.68 means the change in category results in a (1 - 0.68) = 0.32 = 32% decrease in the hazard rate on average.\n\nRelative Risk (of an event): Risk of an event given category / risk of an event given by reference category\nExample: Treatment = Smoking\n\nRisk Score (aka hazard rate) given by smoking: (xᵢ=1): h₀(t)exp(β⋅xᵢ) = h₀(t)exp(β*1) = h₀(t)exp(β)\nRisk Score (aka base hazard rate) given by not smoking: (xᵢ=0): h₀(t)exp(β⋅xᵢ) = h₀(t)exp(β*0) = h₀(t)\nRelative risk (aka hazard ratio) = risk given by smoking / risk given by not smoking: h₀(t)exp(β) / h₀(t) = exp(β)",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-ml",
    "href": "qmd/regression-survival.html#sec-reg-surv-ml",
    "title": "43  Survival",
    "section": "43.7 ML",
    "text": "43.7 ML\n\n43.7.1 Misc\n\nSplit data so partitions have the same censoring distribution.\n\nThe censoring distribution might be obtained from a Kaplan-Meier estimator applied to the data.\n\nDynamic AUC is a recommended metric\n\n\n\n43.7.2 Random Survival Forests\n\nThe main difference from a standard RF lies in the metric used to assess the quality of a split: log-rank (see Misc) which is typically used when comparing survival curves among two or more groups.\nPackages\n\n{{sklearn}}\n{aorsf} - Optimized software to fit, interpret, and make predictions with oblique random survival forests (ORSFs)\n\nInstead of using one variable to split the data, use a weighted combination of variables, i.e. \\(\\text{instead of}\\;\\; x_1 &lt; \\text{cutpoint (left), use}\\;\\; c_1x_1 + c_2x_2 &lt; \\text{cutpoint (right)}\\)\n\nPredictions of Standard RF vs Oblique RF\n\n\n\n\n\n\n\n\nStandard Random Forest\n\n\n\n\n\n\n\nOblique Random Forest\n\n\n\n\n\n\n\nIn the standard rf, the decision boundaries are essentially perpendicular while the oblique rf boundaries are more angular. This should make the oblique model more flexible.\n\nKaplan-Meir Curves are fit in the leaves of the trees\n\n\nTime is on the x-axis and probability of survival on the y-axis\n\n\n\nExample: {aorsf}\n\nFrom Machine Learning for Risk Prediction using Oblique Random Survival Forests (Video; Slides & Code)\nVia package\n# equivalent syntaxes\nfit_orsf &lt;- orsf(data = pbc_orsf, \n                 formula = Surv(time + status) ~ . - id)\nfit_orsf &lt;- orsf(data = pbc_orsf, \n                 formula = time + status ~ . - id)\n\nTop model is fit with the typical survival::coxph syntax\ntime: time to event\nstatus: dummy variable indicating whether event occurred\nid: unit or patient id which is excluded\n\nVia {tidymodels}\nlibrary(parsnip)\nlibrary(censored) # must be version 0.2.0 or higher\nrf_spec &lt;- \n  rand_forest(trees = 200) %&gt;%\n  set_engine(\"aorsf\") %&gt;% \n  set_mode(\"censored regression\") \nfit_tidy &lt;- \n  rf_spec %&gt;% \n  parsnip::fit(data = pbc_orsf, \n               formula = Surv(time, status) ~ . - id)\nEstimated Expected Risk via Partial Dependence (PD)\n\nPD and importance rank for variables\norsf_summarize_uni(fit_orsf, n_variables = 1)\n## \n## -- bili (VI Rank: 1) ----------------------------\n## \n##         |---------------- risk ----------------|\n##   Value      Mean    Median     25th %    75th %\n##  &lt;char&gt;     &lt;num&gt;     &lt;num&gt;      &lt;num&gt;     &lt;num&gt;\n##    0.80 0.2343668 0.1116206 0.04509389 0.3729834\n##     1.4 0.2547884 0.1363122 0.05985486 0.4103148\n##     3.5 0.3698634 0.2862611 0.16196924 0.5533383\n## \n##  Predicted risk at time t = 1788 for top 1 predictors\n\nComputes expected risk (predicted probability) at different quantiles as a predictor variable varies.\n\nValue is 3 values of the predictor which are the 25th, 50th, and 75th quantile.\n\nn_variables says how many “important” variables to look at\n\ne.g. n_variables = 2 would look at the top 2 variables in terms of variable importance.\nVI Rank: 1 indicates the bili is ranked first in variable importance\n\nbili: serum bilirubin (mg/dl); continuous predictor variable\nIt choses time = 1788 because that’s the median\nAlso see Diagnostics, Model Agnostic &gt;&gt; DALEX &gt;&gt; Dataset Level &gt;&gt; Partial Dependence Profiles\n\nPD at specified predictor values and time values\npd_by_gender &lt;- orsf_pd_oob(fit_orsf, \n                  pred_spec = list(sex = c(\"m\", \"f\")),\n                  pred_horizon = 365 * 1:5)\npd_by_gender %&gt;% \n  dplyr::select(pred_horizon, sex, mean) %&gt;% \n  tidyr::pivot_wider(names_from = sex, values_from = mean) %&gt;% \n  dplyr::mutate(ratio = m / f)\n\n## # A tibble: 5 x 4\n##   pred_horizon      m      f ratio\n##          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1          365 0.0768 0.0728  1.06\n## 2          730 0.125  0.111   1.13\n## 3         1095 0.230  0.195   1.18\n## 4         1460 0.298  0.251   1.19\n## 5         1825 0.355  0.296   1.20\n\norsf_pd_oob - Computes expected risk using out-of-bag only\n\nBoth values (m,f) for sex are specified\npred_horizon specifies the time values\n\nHere, time is in days, so these values specify expected risk (predicted probabilities) at years 1 through 5.\n\n\nratio is the risk ratio of males compared to females.\nOthers\n\norsf_pd_inb - Computes expected risk using all training data\norsf_pd_new - Computes expected risk using new data\n\n\n\n\n\n\n\n43.7.3 Gradient Boosting Survival Trees\n\nLoss Functions\n\nPartial likelihood loss of Cox’s proportional hazards model\nSquared regression loss\nInverse probability of censoring weighted least squares error.\n\nAllows the model to accelerate or decelerate the time to an event by a constant factor. It is known as the Accelerated Failure Time (AFT). It contrasts with the Cox proportional hazards model where only the features influence the hazard function.\n\n\nPackages\n\n{{sklearn}}\n\n\n\n\n43.7.4 Survival Support Vector Machine\n\nPredictions cannot be easily related to the standard quantities of survival analysis, that is, the survival function and the cumulative hazard function.\nPackages\n\n{{sklearn}}",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-diag",
    "href": "qmd/regression-survival.html#sec-reg-surv-diag",
    "title": "43  Survival",
    "section": "43.8 Diagnostics",
    "text": "43.8 Diagnostics\n\nMisc\n\nNotes from How to Evaluate Survival Analysis Models\nPackages\n\n{survex} - Explainable Machine Learning in Survival Analysis\n\nFrom Dalex group\n\n\n\nConcordance Index (C-Index, Harrell’s C)\n\nConsider a pair of patients (i, j). Intuitively, a higher risk should result in a shorter time to the adverse event. Therefore, if a model predicts a higher risk score for the first patient (ηᵢ &gt; ηⱼ), we also expect a shorter survival time in comparison with the other patient (Tᵢ &lt; Tⱼ).\nEach pair (i, j) that fulfills this expectation (ηᵢ &gt; ηⱼ : Tᵢ &lt; Tj or ηᵢ &lt; ηⱼ : Tᵢ &gt; Tⱼ) as concordant pair, and discordant otherwise.\n\nA high number of concordant pairs is an evidence of the quality of the model, as the predicted higher risks correspond to an effectively shorter survival time compared to lower risks\n\nFormula\n\n\nIf both patients i and j are censored, we have no information on Tᵢ and Tⱼ, hence the pair is discarded.\nIf only one patient is censored, we keep the pair only if the other patient experienced the event prior to the censoring time. Otherwise, we have no information on which patient might have experienced the event first, and the pair is discarded\n\nProgrammatic Formula\n\n\nWhere the variable Δⱼ indicates whether Tⱼ has been fully observed (Δⱼ = 1) or not (Δⱼ = 0). Therefore, the multiplication by Δⱼ allows to discard noncomparable pairs because the smaller survival time is censored (Δⱼ = 0).\n\nGuidelines\n\nC = 1: perfect concordance between risks and event times.\nC = 0: perfect anti-concordance between risks and event times.\nC = 0.5: random assignment. The model predicts the relationship between risk and survival time as well as a coin toss.\nDesirable values range between 0.5 and 1.\n\nThe closer to 1, the more the model differentiates between early events (higher risk) and later occurrences (lower risk).\n\n\nIssues\n\nThe C-index maintains an implicit dependency on time.\nThe C-index becomes more biased (upwards) the more the amount of censoring (see Uno’s C below)\n\n\nUno’s C\n\n** Preferable to Harrell’s C in the presence of a higher amount of censoring. **\nVariation of Harrell’s C that includes the inverse probability of censoring weighting\n\nWeights based on the estimated censoring cumulative distribution\nUses the Kaplan-Meier estimator for the censoring distribution\n\nSo the disribution of censored units should be independent of the covariate variables\n\nIn the paper, Uno showed through simulation this measure is still pretty robust even when the censoring is dependent on the covariates\n\n\n\n\nDynamic AUC\n\nAUC where the False Positive Rates (FPR) and True Positive Rates (TPR) are time-dependent\n\nSince a unit is a True Negative until the event then becomes a True Positive\nRecommended for tasks when you want to measure performance over a specific period of time (e.g. predicting churn in the first year of subscription).\n\nFormula\n\n\nf^s are predicted risk scores\nώ is the inverse probability of censoring weight (see Uno’s C)\n\ndisregard the freaking dash above the omega, microsoft deleted the regular one for some reason\n\nI(yi,j &gt; t) indicates whether the unit pair’s, i and j, event time is greater or less the time, t. (I think)\n\n\n\n\n\n\nStandard Random Forest\nOblique Random Forest",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/vs-code.html",
    "href": "qmd/vs-code.html",
    "title": "VS Code",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "VS Code"
    ]
  },
  {
    "objectID": "qmd/vs-code.html#sec-vsc-misc",
    "href": "qmd/vs-code.html#sec-vsc-misc",
    "title": "VS Code",
    "section": "",
    "text": "Press “.” inside any page in github to create a vscode instance that opens that file.",
    "crumbs": [
      "VS Code"
    ]
  },
  {
    "objectID": "qmd/vs-code.html#sec-vsc-shcts",
    "href": "qmd/vs-code.html#sec-vsc-shcts",
    "title": "VS Code",
    "section": "Shortcuts",
    "text": "Shortcuts\n\n\n\n\n\n\n\nShortcut\nDescription\n\n\n\n\nCTRL/Command + Enter\nInsert a new line directly below, regardless of where you are in the current line\n\n\nALT/Option +Shift + Up/Down\nDuplicate your current row up or down\n\n\nALT/Option + Up/Down\nMove the current row up or down\n\n\nALT/Option + Shift + Right\nHit this twice to select everything within a current bracket(this option is called smartSelect.grow , if it needs to be re-mapped)\n\n\nCTRL/Command + /\nComment out the current line\n\n\nCTRL/Command + [ or ]\nIndent lines inward or outward",
    "crumbs": [
      "VS Code"
    ]
  },
  {
    "objectID": "qmd/vs-code.html#preferences",
    "href": "qmd/vs-code.html#preferences",
    "title": "VS Code",
    "section": "Preferences",
    "text": "Preferences\n\nSetting shortcut for running a line of powershell code to ctrl + enter\n\nPress Ctrl+Shift+P, type “Preferences: Open Keyboard Shortcuts (JSON)”, and press Enter.\nAdd this code\n{\n    \"key\": \"ctrl+enter\", // Replace with your preferred shortcut\n    \"command\": \"workbench.action.terminal.runSelectedText\",\n    \"when\": \"editorTextFocus && editorLangId == 'powershell'\"\n}",
    "crumbs": [
      "VS Code"
    ]
  },
  {
    "objectID": "qmd/aws.html",
    "href": "qmd/aws.html",
    "title": "AWS",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-misc",
    "href": "qmd/aws.html#sec-aws-misc",
    "title": "AWS",
    "section": "",
    "text": "Notes from Skillshare: Absolute Beginners Introduction to Amazon Web Services\nHow to choose the right GPU for Deep Learning\nOptimizations\n\nIncreased the number of threads that the AWS CLI uses to some large number (the default is 10) with aws configure set default.s3.max_concurrent_requests 50\nIf downloading data is more of a bottleneck than cpu power, use a network speed optimized ec2 instance (“n” in the name) such as c5n.4xl.",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-creman",
    "href": "qmd/aws.html#sec-aws-creman",
    "title": "AWS",
    "section": "Create and Manage Account",
    "text": "Create and Manage Account\n\naws.amazon.com  – create a free account (top right)\n\ne: &lt;email&gt;\np: &lt;password&gt;\nacct name:\nall info needs entered:\n\ntick personal,\nfull name,phone number, country, address, city, state, postal code,\ntick you’ve read user agreement\n\n\nPayment info\n\ncredit card, exp date, name\n\nVerify phone\n\ncountry, phone number (again), the “i am not a robot” thing, click call me now\n\n4 digit code pops up on screen, computer calls you, and you enter code\nclick continue\n\n\nSelecting the Basic/free acct option (1-year),\nyou can skip the Personalize your experience stuff. Hit “sign into the console” (mid-right)\n\nenter email and password\n\nBasic account page set-up\n\nUpper right corner –&gt; name of the account –&gt; dropdown menu –&gt; My Account\n\nMain Body –&gt; Alternate Contact info –&gt; right side, click edit\nMain Body –&gt; Security Challenge Questions –&gt; click edit\nMain Body –&gt; IAM User and Role Access –&gt;  click edit –&gt; check Activate box –&gt; click update\n\nthink this is just for generating roles that have access to billing\n\nMain Body –&gt; Manage Communication preferences –&gt; edit\n\nnewsletters, services info, etc.\n\nMain Body –&gt; Manage AWS support plan\n\nalready selected free earlier\nwhere you can upgrade from free tier to a paid plan\n\n(nothing to fill out) left panel –&gt; Dashboard, Bills, Cost Explorer, Reports, allocation tags, credits, misc accounting stuff\n\nstats on your spending\ninvoice summary of this month’s usage\nVisual breakdown of the cost/services you’re incurring\nreports\ncreate tags to designate which departments/projects are using which services\nwhere to input promo codes\n\nCreate Spending Alerts\n\nleft panel –&gt; Budgets –&gt; Create Budget\nselect type: Cost, usage, reservation, utilization\n\nuse cost\n\n$20\n\n\nGive budget a name\nPeriod: monthly, etc.\n\nuse monthly, it’s the smallest period available\n\nselect start date and end date\nAmount\nCan set limits per service\nNotifications\n\nactual, greater than, 50% of budget amount\n\nother option besides actual is forecasted\n\nemail address\noption for SNS service but didn’t discuss what that is\ncreate new notification\n\nadd additional email alert for 75%\n\n\nclick create budget (bottom right)\n\nleft panel –&gt; Preferences\n\nset notifications for when you exceed the free tier services\nget emailed billing invoice\nBilling alerts, reports\n\n\n\nIdentity Access Management\n\nhome console – top search box under AWS Services, type “IAM”\n\nselect Identity and access management\n\nChange User Sign-in URL to something more memorable\n\nexample: root account name\n\nercbk\n\nclick customize (mid-right) and type name\nURL will be changed to .signin.aws.amazon.com/console\n\nSecurity Status section (main body\n\nActivate multi-factor authentication (MFA)\n\nThis is for root user, see change password section below if you’re a user that’s part of a group\nphysical MFA (e.g. usb drive) or virtual MFA\n\nchoose virtual\n\nclick next twice to the bar code\nuse authenticator app to scan bar code\n\nclick + in app, then click bar code, and scan bar code on screen\n\nAsks for 2 consecutive pins that display on the app\nClick finish\n(may need to refresh page on main screen in order to see green tick mark)\n\n\nCreate individual IAM users\n\nAllows you to distribute various permissions to people with different roles in your org\nNeed to set up an admin user to get access keys so you can use AWS programmatically\nleft panel –&gt; Users –&gt; add users –&gt; enter user name – select access types\n\nchoose access type\n\ntick programmatic and management console\n\nenter console password\nrequire password reset: nope (untick box)\nclick next (bottom right) to set permissions\n\nSet permissions\n\nif you’re creating user groups (left panel), you can skip this and set the permissions group-wide\nSelect Attach existing policies directly\n\ntick box “AdministratorAccess” –&gt; click next (bottom right)\n\ndude did this through groups and policies, so may have to go that route if AdministratorAccess isn’t available, then add the user to the group. Also there’s a button to attach additional “policies” later on if needed\n\n\n\nReview and click create User\na lot more to this… groups, policies  (see vid for details) \n\nSelect password policy\n\ntick allow users to set password and untick everything else –&gt; apply policy\nactivate/deactivate security token regions - he didn’t go into this but all US and EU regions were activated by default, so probably doesn’t need to be messed with.\n\nCopy URL, log out as root user, Now you can starting going to URL and sign in under the administrator acct you made\n\nTo change password\n\nleft panel –&gt; Users –&gt; select yourself –&gt; security credentials tab –&gt; Console password\nCan also create MFA in same tab\n\n\n\nCloud Trail\n\nlog of actions taken on account\ntype cloud trail in search box\nCreate trail -  lets you store the logs in a S3 bucket",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-basserv",
    "href": "qmd/aws.html#sec-aws-basserv",
    "title": "AWS",
    "section": "Overview of Basic Services",
    "text": "Overview of Basic Services\n\nCompute\n\nEC2 - Elastic Cloud Compute - Computer Clusters\n\nClosely related to on-premise set-ups\n\nElastic Container Service (ECS) - Spin up containers on top of EC2\nLambda - Serverless Architecture\n\nComputing resources can scale and descend automatically based on real-time demands.\nHandles security patches and OS updates automatically\nIssue with “timeouts” - not optimal for long running applications\n\nA timeout defines the maximum amount of time a Lambda function can execute before it’s terminated.\nDefault timeout is set at 3 seconds, but you can configure it in the AWS console in increments of 1 second, with a maximum of 15 minutes (900 seconds).\n\nAWS charges for Lambda usage in increments of 100 milliseconds (0.1 seconds). Setting the timeout in 1-second increments helps ensure you’re not billed for extra time if your function runs slightly longer than expected.\nPrice optimization entails tuning this timeout parameter to be as close to your run time as possible. Need to take into account timeouts of other services included in your lambda function. (Guide)\n\nMonitor your Lambda functions for timeouts using CloudWatch metrics.\n\nDependency limit at 50 MB, can add 512 MB more to a tmp file after function has executed\nSpins down when not in use, so you don’t pay for downtime, but takes 5 or more secs to spin back up\n\nCan ping server from time to time to keep it “warm”\n\nLess tweakable than EC2, if problems occur, less flexible in terms of your team handling it\nThe code you run on AWS Lambda is uploaded as a “Lambda function”. Each function has associated configuration information, such as its name, description, entry point, and resource requirements. The code must be written in a “stateless” style i.e. it should assume there is no affinity to the underlying compute infrastructure. Local file system access, child processes, and similar artifacts may not extend beyond the lifetime of the request, and any persistent state should be stored in Amazon S3, Amazon DynamoDB, or another Internet-available storage service. Lambda functions can include libraries, even native ones.\n\n\nStorage\n\nS3\n\nDatabase -automanaged by AWS\n\nrelational db service (RDS)\ndynamoDB - noSQL\nElasticCache - redis\nRedshift - cadillac data warehouse service\n\nManagement Tools\n\nCloudWatch\n\nmonitor cpu usage\nlatency\nset alarms around metrics",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-cli",
    "href": "qmd/aws.html#sec-aws-cli",
    "title": "AWS",
    "section": "CLI",
    "text": "CLI\n\ngoogle “download aws cli”\nGet keys\n\nlog into aws\nsearch IAM –&gt; goto iam\nleft panel –&gt; users –&gt; your usesrname –&gt; security credentials tab –&gt; create access key –&gt; 2 keys are created –&gt; click download csv file\n\naccess key id (kind of like a username)\nsecret access key (kind of like a password)\n\n\nSet-up\n\nConfigure profile\n\naws configure --profile &lt;name&gt;\n\nchoose a name, can be anything\n\nused ercbk\n\n\nYou’ll be asked for your\n\n“access key id” and “secret access key”, enter those\n\nused eb_admin values\n\noptional: default region: us-east-2 or hit enter to skip\n\nused us-east-2\n\noptional: default output format: just hit enter (he didn’t go into this)\n\n\n\ncommands\n\nHelp commands\n\ngives description, flags, inputs, etc.\naws help\naws &lt;service&gt; help\n\neg aws iam help\n\naws &lt;service&gt; &lt;command&gt; help\n\neg aws iam list-users help\n\nCan also go to AWS documentation at website",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-ec2",
    "href": "qmd/aws.html#sec-aws-ec2",
    "title": "AWS",
    "section": "EC2",
    "text": "EC2\n\nOn home screen, in search window, type “ec2”, click link\n\n\nEC2 Page\n\nleft panel – Dashboard\n\nMain Body – Under Resources\n\nInstances - which cluster instances you have running\nVolumes - which storage services you have\nkey pairs - you get a key pair for each running instance\nOther stuff…\n\n\nleft panel – Instances\n\ninstances\n\nwhere you launch on-demand instances\nno bidding, you pay full price\n\nlaunch templates\n\nstored instance configurations\nalternate method to launch instances\n\nspot instances (requests)\n\nbid for available instances for a cheaper price\nIf the Spot price increases above your bid price, capacity is no longer available, or the spot request has constraints that can’t be met, then the Spot Instance can be “interrupted.”\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/spot-instance-terminate/\nEBS volumes can be attached, snapshots taken, or results sent to s3 buckets to prepare for a potential terminations (see above link)\n\n\nreserved instances\n\nyou can make a reservation for an instance to guarantee it’s availability at the time you specify. Can be cheaper than on-demand.\n\ndedicated host\n\nWhen you buy instances you share hosts (servers) with other people. Here you can guarantee only you are on the host with your instances.\n\n\nleft panel – images\n\nAMI\n\nscenario: you launch an ec2 and load more packages onto it and want to save that AMI to reuse at a later time. Here you can take a snapshot of that image.\nyou can share these images with other users, sell the image in the marketplace\n\n\nleft panel – Elastic Block Store\n\nvolumes\n\nhard drives - hdd (cold or optimized, ssd, iops ssd )\n\nsnapshots\n\nback-up of the volume\ncan be used to launch another instance if the current one is terminated\n\n\nleft_panel – Network and Security\n\nSecurity Groups (also see docker notebook – aws – running a task)\n\nfirewall you place in front of an EC2 instance\nspecify allowed ports and allowed ip connections\ncan’t restrict to inbound or outbound traffic; open is open\n\nElastic IPs\n\nallows you to fix static ips (up to 5) to your instances\n\none less thing you’d have to configure if you’re starting and stopping ec2 instances a lot.\n\n\nPlacement Groups\n\ndecreases latency by placing all of your instances in the same or closely placed hosts\n\nKey Pairs\n\na form of tags that you can use to access your instance more easily,\n\nuse values as descriptors of use case for instance to organize and monitor\nCan use to SSH into your instance\npairs are key:value. Example: key= sales, value  = sales_forecast\n\n\nNetwork Interfaces\n\ngives a network card to your instance so it can connect to the internet\n\n\nleft panel – Load Balancing\n\nscaling managed by aws\nroutes traffic to different instances\nLoad Balancers (also see Docker notebook, aws create load balancer section)\n\ntypes: application, network\n\nTarget Groups\n\nleft panel – auto scaling\n\nadds instances if triggers set in load balancers (latency, cpu usage, memory)\nshuts down instances once load decreases\nkeeps cost lower\n\nleft panel – systems manager\n\nallows you to run a command across multiple instances by specifying tags, ids, etc\ncan use scripts that update packages\n\n\n\n\nLaunch\n\nSearch ec2 in the search window\nSteps\n\ntop-right header – choose a region\n\nwhere are the users located (latency)\n\nleft panel – Instances – instances (or Spot Instances)\n\nclick launch instances\nchoose an AMI, click select on right-hand side\nchoose compute option by ticking box on the left and click next on bottom right\n\neg t2 - micro (free tier option)\n\nconfigure instance details\n\nselect how many instances you want to launch\nnetwork, subnet, auto-assign public ip\n\ndefaults should be fine\n\nIAM role - mentions something about databases\nshutdown behavior: “stop” terminates instance when you stop it\nenable termination protection: protects against accidental termination\n\nthink this might have to do with spot instances being terminated\n\nmonitoring - enable CloudWatch ($)(see Overview of basic services section)\n\nfree version updates metrics every 5 min\ndetailed version updates metrics every 1 min\n\nT2 unlimited\n\nenabling says if your t2 instance cpu goes over 20% usage, then start using my credits, and once my credits are used, then bill me for the rest\nguessing if this is unticked then your t2 is throttled below 21% to remain totally free\n\n\nAdd storage\n\nsee Elastic Block Store above for details on available drives\ncomes with a “root” drive by default\n\nChoose HD type, size of storage, and whether to delete on termination (volume gets deleted)\nGeneral Purpose SSD and up to 30 GB available for free tier.\nAlso saw a snapshot ID I think but he didn’t discuss\n\nclick add volume to add multiple HDs\n\nAdd tags\n\nadd tag\nenter key and value (different from key pair below)\ntick which resources you want to use the tag for: instance and/or volume\n\ncan create unique tags for different resources\n\n\nConfigure security group\n\nAssign a new security group or select an existing group\n\ntick select existing –&gt; tick default\n\n\nClick review and launch\n\nshows all the options you selected above\nHit launch\n\nChoose existing or create new key pair\n\nfile that’s necessary to ssh (linux) or log into (windows)\nCreate new key pair –&gt; enter name –&gt; download key pair file\n\nor you can select previous key pair and use the same file\n\nLaunch instance\n\nLaunch Status\n\nClick View Instances\n\nWatch status column, usually takes a couple minutes to launch\nscreen is split in half, use divider to increase size of lower screen if you want to view the details of the instance you started\nI think you’re in left panel – instances – instances\n\n\n\n\n\n\n\nConnect to/ Terminate Instance\n\nAlso see Docker, AWS &gt;&gt; Create Cluster: EC2 with SSH Access\nleft panel – instances – instances\n\nhighlight instance you want to connect to\nIn bottom of split EC2 screen, copy IPv4 Public IP and save it in notebook++ or somewhere\n\nOpen an inbound port to instance\n\nleft panel – Network and Security – Security Groups\n\nclick on security group you selected during launch of instance\nlower half of screen – inbound tab\n\n“all traffic” means full communication allowed between all instances within this security group\nclick edit – add rule\n\nType – click dropdown – choose SSH\n\nafter choosing SSH, it also inputs port 22\n\nSource – ditto – choose “My IP”\n\nautomatically finds your ip  and inputs it\n\nDescription -  “SSH for  ip” or whatever you want\n\nfor Windows Server AMI instance\n\nType – click dropdown – choose RDP\n\nchooses port 3389\n\nSource – ditto – choose “My IP”\n\nautomatically finds your ip  and inputs it\n\nDescription - “RDP for  ip” or whatever you want\n\n\n\n\nIf on a linux machine (locally), click instance, choose connect, follow instructions\nIf on windows (locally):\n\nOpen Puttygen (key generator)\n\nClick load – find and select the key pair file (.pem) you downloaded before launching the instance\nClick Save private key\n\nIt’ll ask if you’re sure you don’t want to attach a passphrase – click yes\nenter name of .ppk file (no need to add extension) – Click Save\n\n\nOpen Putty\n\nConfiguration window opens\n\nIn Host Name (IP address) box – paste IPv4 Public IP (that you copied from earlier)\nleft panel – Connections – SSH – Auth\n\nOptions-for-controlling-SSH-authentication window opens\n\nprivate-key-for-authentication box – click browse\n\nselect the .ppk file you made\n\n\n\nClick Open (bottom right) to open connection\n\n Windows pop-up – click yes\n\n\n\nPutty CLI opens\n\n“login as” \n\ntype username for the AMI you used\n\nexample was a basic linux AMI with username “EC2-user”\nFor RStudio AMI, should be “ubuntu”\n\n\nHit enter and should be connected\n\n\nTerminate\n\nleft panel – instances – instances\n\nselect instance you want to terminate – right click instance_id (or anywhere on row) — instance state – terminate\n\n\nConnect to a Windows Server AMI instance\n\nWindows Search “Remote Desktop Connection”\n\nopen it\n\nFor “Computer:”, type in the IPv4 public ip – click connect\nFor base ami\n\nusername: administrator\npassword\n\nleft panel – instance – instance\n\nright click instance row – Get Windows Password\n\nclick Choose FIle – select key pair file .pem file (not .ppk)\nclick Decrypt Password (bottom right)\nCopy password, paste into Remote Desktop Connection\n\n\nClick OK\n\n\na window with opens up with Windows OS on it (takes a minute or two to completely load)\n\n\n\n\nConfigure Load Balancer and Application Ports\n\nAlso see Docker, AWS &gt;&gt; Create Application Load Balancer (ALB)\nleft panel – Network and Security – Security Groups (also see docker notebook – aws – running a task)\nExample: We want the Application Instances to only talk to the load balancer (inbound) and the load balancer talks to the public (inbound) and application instances (outbound) (Reminder all inbound ports are also outbound ports)\n\nCreate security group for EC2 Instance\n\nCreate Security Group (blue button, upper left)\n\nEnter name and description\nclick create\n\nInbound tab (lower half)\n\nclick edit\n\nType: HTTP (auto-inputs protocol TCP and port 80)\nSource: type name of Load Balancer security group (long box)\n\nautocomplete will help\nafter clicking name, a group id is entered into the field\n\nWeird, but id resembles the group id listed in the top half of screen but doesn’t exactly match. Should be correct though.\n\ndrop down should be “custom”\n\nclick add rule\nType: HTTPS\n\nsame thing but auto-inputs port 443\n\nClick Save\n\n\noutbound tab\n\nBy default all outbound traffic goes out on the inbound ports, but you can specify additional ports\nclick edit\n\nType: all traffic (auto-inputs protocol all, port range 0-65535)\nSource: same as for inbound\n\n\n\nCreate security group for Load Balancer\n\nCreate Security Group\n\nSame procedure as above, different name\n\nInbound tab\n\nclick edit\n\nType: HTTP\n\nkeep Source as is\n\n0.0.0.0 means accept traffic from everywhere\n\n\nclick add rule\nType: HTTPS\n\nsame\n\nclick Save\n\n\nOutbound tab\n\nclick edit\n\nType: HTTP (auto-inputs protocol TCP and port 80)\nSource: type the application security group name (long box)\n\nsee load balancer section for other details\n\nclick add rule\nType: HTTP\n\nsame\n\nclick Save",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-s3",
    "href": "qmd/aws.html#sec-aws-s3",
    "title": "AWS",
    "section": "S3",
    "text": "S3\n\nSearch S3\nautoscales,, replicates your data to prevent total loss, ability to version data, max file size 5TB\ncharged for what you use (GB/mo)\n3 different classes\n\nstandard\n\nhighest availability (99.999%), durability (replicated across hosts multiple times)\nmost expensive of the classes\n\ninfrequently accessed\n\nless durability (replicates), should be data that doesn’t end your world if lost\n\nglacial\n\nfor archiving purposes\n\n\nSecurity\n\nIAM\n\ngive certain persons or departments permissions\n\nS3 policies\n\nmake certain buckets public, private, etc.\n\n\nBuckets\n\nmust have unique name across all AWS\n\nassume this is taken care of by some auto-generated id\nCan’t contain periods (dns-compliant)\n\nregion-specific\ncross-region replication\n\ncopy objects from one bucket to another for a fee\n\n\nObjects\n\nfiles, artifacts, etc\nkey:value look-up\n\nthe keys are essentially just file paths\n\nbucket_name/folder/file.txt\nbucket_name/folder/*\n\ngets everything\n\n\nthe values are the objects\n\nYou can directly download files from the console but not folders\n\nselect file, click on “more” button (top left), select “download as”, follow directions\n\nTo make publicly available through a web link you have to specify that policy (see below)\n\n Create a basic bucket\n\nClick Create Bucket (top left)\n\nOpens Wizard\n\nName and Region\n\nEnter unique DNS compliant name (no periods)\nEnter Region\nClick next\n\nSet Properties\n\nkeep defaults\nclick next\n\nSet Permissions\n\nkeep defaults\nclick next\n\nReview\n\nclick create bucket\n\n\n\nUpload to bucket\n\nUpload via aws console\n\nClick bucket name\nclick upload button (top left)\nOpens Wizard\n\nSelect files\n\nclick add files or drag and drop folders into the window\nclick next\n\nSet Permissions\n\nkeep defaults\nclick next\n\nSet Properties\n\nStorage class\n\nstandard, standard-ia (infrequently accessed), reduced redundancy\n\nencryption\nkeep defaults\nclick next\n\nReview\n\nclick upload\n\n\n\nSee below for uploading via CLI\n\n\nCreate Bucket Policy to allow for public download (from a weblink)\n\nclick bucket name\nclick Permissions tab (top mid)\nBucket policy requires a json expression\n\nclick policy generator (bottom left)\n\nopens up new browser tab\n\nSelect type of policy\n\nS3 Bucket Policy\n\neffect (allow or deny)\n\nselect allow\n\nPrincipal\n\ntype “*” (asterisk) to give everyone access\n\nActions\n\ncan select more than one action if you want\nselect “GetObject”\n\nAmazon Resource Name\n\nThe format of the required expression is given below the box\nyour substituting your bucket_name/key_name into the expression\n\nfor key name he used * to signify “everything”, but I think this would be your path to the resources (not including bucket_name)\n\ne.g. folder1/folder2/file.csv or folder1/folder2/*\n\n\nClick add statement\nClick generate policy\ncopy json expression\n\n\nGo back to previous browser tab with the permissions tab and paste the expression into the window\nClick Save (mid right)\n\nThere’s also a DELETE button if you want to remove a policy from the bucket\n\nShould see a “public” tag on the permissions tab confirming the policy has been set\n\nDownload from bucket\n\nGet download link for a file that has a public permission set\n\ntick box of selected file\n\nwindow with info about the file should open on the right side\n\ncopy download link in overview section, paste in browser or use programatically\n\nDownload via console\n\nclick file name – overview tab\n\n various downloading options\n\n\nProgramatically: see section Upload to bucket via CLI below (uses copy command)\n\nGive bucket viewing (and downloading) access via IAM (need to have administrator permissions)\n\nsearch IAM\nleft panel – Policies\nclick create policy (top left)\nclick choose service (mid)\n\ntype or select “S3”\n\nclick select actions\n\nUnder Access Level\n\nList\n\nselect all (4)\n\nRead\n\nView policy\n\nGetBucketAcl, GetBucketCORS, GetBucketLocation, GetBucketLogging, GetBucketPolicy, GetBucketTagging, GetObjectAcl, ListBucketByTags, ListBucketVersions\n\nDownload policy (should be a separate policy from View)\n\nGetObject\n\n\nResources\n\nyou have to select resources (e.g. buckets, objects (i.e. folders, files) that the actions above affect\n\nView policy\n\nchoose all resources (which is buckets and objects)\n\nDownload policy\n\nchoose specific\nclick ARN\n\nEnter the bucket name you want to give access to (or tick “any” box to give access to all buckets)\nenter object name (or tick “any” box for access to all objects)\n\nClick add\n\n\n\nClick Review Policy (bottom right)\n\nEnter a Name\n\neg ListAllBucketsObjsS3\n\nClick Create Policy (bottom right)\nSelect or click newly created policy (should be back to left panel – policies console\n\nnew policy name should be in a clickable banner at top of screen\nOr choose it from list of policies that’s displayed\n\nGo to Attached entities tab\n\nclick attach\nselect users or groups you want the policy to apply to\nclick attach (bottom right)\n\n\n\nView buckets you have permissions with: aws s3 ls or aws s3api list-buckets --output text\nUpload to bucket via CLI\n\nAlso see above for uploading via aws console\naws s3 help, aws s3  help\naws s3 cp     can copy files via:\n\nbucket to bucket\nlocal to bucket\nbucket to local\n\nlocal to bucket\n\naws s3 cp &lt;C:\\\\Users\\\\path\\\\to\\\\file.csv&gt; &lt;s3://&lt;bucket/path/to/folder/name.csv&gt; --region &lt;bucket? region&gt; --profile &lt;profile name&gt;\nsee cli section above on how to create profile\nrefresh console if you already have it open to see the file\n\nbucket to local \n\naws s3 cp &lt;s3://&lt;bucket/path/to/folder/name.csv&gt; &lt;C:\\\\Users\\\\path\\\\to\\\\file.csv&gt; --region &lt;bucket? region&gt; --profile &lt;profile name&gt;\nsame thing as before, just reversing the  and  URIs\n\n\nVersioning\n\nIf you enable versioning and then disable it, then the versioned objects will remain but new objects won’t be versioned\n\n Would have to manually delete the versioned objects manually\n\nbucket name – properties tab\n\nclick on “Versioning” card\ntick Enable versioning \nclick save\n\nTo see the different file versions\n\nbucket name – folder\nclick versions: show button (upper left)\n\nOnce versions are displayed you can download and delete files/versions in various menus\n\nclick file name – “latest version” drop down (next to file name, top left)\n\nshows all versions with options to download or delete\n\nclick file name – overview tab\n\nvarious download options\n\ntick file name/version box – click more button (upper left) – select delete – click delete button",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html",
    "href": "qmd/cloud-services.html",
    "title": "4  Cloud Services",
    "section": "",
    "text": "4.1 Misc",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#misc",
    "href": "qmd/cloud-services.html#misc",
    "title": "4  Cloud Services",
    "section": "",
    "text": "See Cloud Costs Every Programmer Should Know for various service estimates in order to perform back-of-the-napkin calculations of project costs\nFor “Stead-State Workloads” requiring HPC, cloud compute doesn’t make economic sense\n\nSteady-State workloads are projects that are run near constantly\n\nSee Thread for discussion on scenarios, issues, and risks of your data center (DC) in the Cloud vs on-prem.\nExamples:\n\nAcademia: Where academics are in a queue to run experiments on the a HPC cluster\nWeather Forecasting: Forecasts are required nearly in real time, so these models run constantly\nFinancial transaction processing at a bank: The bank’s systems handle a constant stream of transactions\nInventory management system for a manufacturing plant: The system constantly receives updates on raw materials, production output, and finished goods.\nOthers: Week or two long analysis runs at hedge funds, genomic analysis jobs, a swath of AI training / fine tuning, Oil and Gas where they are plowing through seismic data constantly\n\n\n\nRStudio Server on your docker image allows you to access an ide connected to the server through a browser. Useful so you can make sure the correct packages are installed.\nServerless computing is a method of providing backend services on an as-used basis.\n\nA serverless provider allows users to write and deploy code without the hassle of worrying about the underlying infrastructure\nCharged based on their computation and do not have to reserve and pay for a fixed amount of bandwidth or number of servers, as the service is auto-scaling\ne.g. AWS Lambda (i.e. resources only get spun-up when an event is triggered)\n\nNVIDIA GPU Guide (thread)\n\nRTX 20-series or 30-series GPUs are forbidden from inclusion in data centers\nGeneral Recommendations (Oct 2022)\n\nA100 for model training\nT4 for inference workloads\n\nK80\n\nReleased in 2015, the K80 contained a lot of VRAM for the time (24 GB)\nCame before tensor cores and is relatively weak by today’s standards\nOnly okay for learning purposes\n\nP4\n\nReleased in 2016\nValue came from its low power consumption\nMay find it priced higher than its upgraded version (the T4), so recommended to avoid it\n\nT4\n\nReleased in 2018\nSignificant upgrade for inference workloads compared to the P4\nExtremely low power consumption, tensor cores, and plenty (16GB) of VRAM\nCheap, so if you have an inference workload, recommended to strongly consider a T4\n\nP100\n\nBig improvement for model training workloads over the K80 when released\nLess RAM (16GB) than K80\nWay more compute  than K80\n\nCan see memory savings from using mixed-precision training\n\nNo tensor cores\n\nV100\n\nHuge upgrade over the P100\nSame VRAM as P100 many but more CUDA cores\nIntroduces Tensor Cores\nMore cost-efficient than the P100\n\nA100\n\nnewest data center GPU\nupgraded tensor cores\nmost benchmarks show 3x+ faster training compared to the V100\n80GB VRAM\nPrice tag might be big, but it’s usually worth it over the V100",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#price-management",
    "href": "qmd/cloud-services.html#price-management",
    "title": "4  Cloud Services",
    "section": "4.2 Price Management",
    "text": "4.2 Price Management\n\nspot instances for cheaper machines\nautoscaling (kubernetes?) to handle peak usage times (spin-up more machines) while saving during slow times (spin down excess machines)\nUse opensource project management tools (dvc, airflow, etc)\nGoogle\n\nThe Google Kubernetes Engine (GKE) control plane is free, whereas Amazon’s (EKS) costs $0.20 an hour.\n\nAWS\n\nWith a well-defined framework of tag keys and values applied across different AWS resources, billing breakdowns by tag prove extremely useful for greater insight on the source of AWS charges — especially if resources are tagged by department, or team, or different layers of organizational granularity.\nReserved Instances - commit to specific configurations for one or three years at reduced cost\nSpot Instances - pay significantly lower costs but potential for applications to be interrupted\nSavings Plans\n\nEC2 Instance Savings Plans to reduce compute charges for specific instance types and AWS regions\n\nSavings of up to 72%\n\nCompute Savings Plans to reduce compute costs irrespective of type and region.\n\nSavings up to 66% and extends to ECS Fargate and Lambda functions.\n\n\nImage Management\n\nData Lifecycle Manager - automates the creation, retention, and deletion of images\n\nWill not manage images and snapshots created by other means, and it also excludes instance store-backed images.\nEC2 Recycle Bin - serves as a safety net to avoid the accidental deletion of resources — retaining images and snapshots for a configurable time where we may restore them before they are deleted permanently.\n\n\nLambda\n\nCloudwatch - Lambda automatically creates log groups for its functions, unless a group already exists matching the name /aws/lambda/{functionName}. These default groups do not configure a log retention period, leaving logs to accumulate indefinitely and increasing CloudWatch costs.\n\nExplicitly configure groups with matching names and a retention policy to maintain a manageable volume of logs.\n\nMemory Optimization - AWS Lambda Power Tuning can help to identify optimizations, albeit with notable initial costs given the underlying use of AWS Step Functions.\n\nLambda charges based on compute time in GB-seconds, where the duration in seconds is measured from when function code executes until it either returns or otherwise terminates, rounded up to the nearest millisecond. To reduce these times, we desire optimal memory configuration.\n\n\nS3 Lifecycle Configuration\n\nCharged for how much data stored, but also which S3 storage classes are utilized.\n\nStandard (default) class is the most expensive, permitting regular access to objects with high availability and short access times.\nInfrequent Access (IA) classes offer reduced cost for data which requires limited access (usually once per month)\nArchival options via Glacier deliver further cost reductions.\n\nConfiguring the lifecycle allows you to automatically transfer data to different storage classes and thereafter permanently delete it, X and Y days respectively after data creation",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#kaggle",
    "href": "qmd/cloud-services.html#kaggle",
    "title": "4  Cloud Services",
    "section": "4.3 Kaggle",
    "text": "4.3 Kaggle\n\nFree\n\n4-core CPU instances w/30 GB RAM\n2-core CPU, 2xT4 GPU w/13GB RAM\n\nT means tensor cores\n1 hour spent using 2xT4’s takes the same amount of your quota as a P100 (old free gpu offering)\n\nMeans 30-40 hours of free, multi-GPU compute per week",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#saturn-cloud",
    "href": "qmd/cloud-services.html#saturn-cloud",
    "title": "4  Cloud Services",
    "section": "4.4 Saturn Cloud",
    "text": "4.4 Saturn Cloud\n\nSaturn Cloud Recipes\n\nJSON files that specify your environment\nGood for keeping track of server dependencies (e.g. linux libraries)\n\nDunno about R packages",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#google-cloud-platform-gcp",
    "href": "qmd/cloud-services.html#google-cloud-platform-gcp",
    "title": "4  Cloud Services",
    "section": "4.5 Google Cloud Platform (GCP)",
    "text": "4.5 Google Cloud Platform (GCP)\n\nBigQuery sandbox is Google’s GCP free tier cloud SQL database. It’s free but your data only lasts 60 days at a time.\nGCP allows users to run deep learning workloads on TPUs\nSince data expires after 60 days, back-up the model coefficients and performance score tables to Google Sheets. Article suggested this is possible through WebUI.\nAs of Nov.19, regression, logistic regression, and k-nn are the only models available to be run with the sql query editor\nhttps://cloud.google.com/free/\n\n$300 credit for 12 months\nAlways free:\n\n2M requests for containers\n1 GB storage\n\nScalable NoSQL document database.\n50,000 reads, 20,000 writes, 20,000 deletes per day\n\nFunctions\n\n1 f1-micro instance per month (Available only in region: us-west1, Iowa: us-central1, South Carolina: us-east1)\n30 GB-months HDD\n5 GB-months snapshot in select regions\n1 GB network egress from North America to all region destinations per month (excluding China and Australia)\n\nKubernetes\n\nOne-click container orchestration via Kubernetes clusters, managed by Google.\nNo cluster management fee for clusters of all sizes\nEach user node is charged at standard Compute Engine pricing\n\nApp Engine\n\n28 instance hours per day\n5 GB Cloud Storage\nShared memcache\n1,000 search operations per day, 10 MB search indexing\n100 emails per day\n\nBigQuery\n\nFully managed, petabyte scale, analytics data warehouse.\n1 TB of querying per month\n10 GB of storage\n\nOther Stuff\n\nYour free trial credit applies to all GCP resources, with the following exceptions:\n\n* You can’t have more than 8 cores (or virtual CPUs) running at the same time.\n* You can’t add GPUs to your VM instances.\n* You can’t request a quota increase. For an overview of Compute Engine quotas, see Resource quotas.\n* You can’t create VM instances that are based on Windows Server images.\n\nYou must upgrade to a paid account to use GCP after the free trial ends. To take advantage of the features of a paid account (using GPUs, for example), you can upgrade before the trial ends. When you upgrade, the following conditions apply:\n\n* Any remaining, unexpired free trial credit remains in your account.\n* Your credit card on file is charged for resources you use in excess of what’s covered by any remaining credit.\nYou can upgrade your account at any time after starting the free trial. The following conditions apply depending on when you upgrade:\n* If you upgrade before the trial is over, your remaining credit is added to your paid account. You can continue to use the resources you created during the free trial without interruption.\n* If you upgrade within 30 days of the end of the trial, you can restore the resources you created during the trial.\n* If you upgrade more than 30 days after the end of the trial, your free trial resources are lost.\n\nSpot Instances (Preemptible VM)\n\nusage capped at 24 hrs\npricing is fixed and not market-driven\n\nGoogle price calculator: https://cloud.google.com/products/calculator/#id=3115f19f-4ff0-4c57-9028-69cb994fe7ca\nExample\n\ncreating a cluster with:\n\n1 x Dataproc cluster node with 30 GB of RAM\n3 x Dataproc worker nodes with 15 GB of RAM\nUsing less than 5 GB of disk space in a bucket\nAnd running the cluster for only 4 hrs\nWould cost only around $5 at the end of the month\n\n\nFree Tier\n\nincludes a 12-month free trial with $300 credit to use with any GCP services and an Always Free benefit, which provides limited access to many common GCP resources\nUse to test out, but KEEP EVERYTHING SMALL (data, hardware, etc). Need to upgrade it to see the true benefit. Free tier resources look like my desktop computer. Whatever cash is leftover should transfer to account.\nhttps://cloud.google.com/free/docs/gcp-free-tier#how-to-upgrade\nupgrade it from the free trial to a paid account through the GCP Console clicking the Upgrade button at the top of the page\n\n\n\nSteps for new project\n\nGo to interface https://console.cloud.google.com/\ncreate a project. “select a project” on top bar –&gt; “new project” on top right –&gt; choose name (optionally a folder/organization if you have one) –&gt; create\n(article wasn’t very reliable and went on talk about a python implementation so I stopped here\n\nTips\n\nApp Engine\n\nDon’t use App Engine Standard environments — big brother G wants you to use rather Flex environments, otherwise, they’ll punish you.\nReview cost analysis regularly to make sure there are no surprising costs.\nMake sure you clean up redundant App Engine application versions to prevent G from robbing you.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#microsoft-azure",
    "href": "qmd/cloud-services.html#microsoft-azure",
    "title": "4  Cloud Services",
    "section": "4.6 Microsoft Azure",
    "text": "4.6 Microsoft Azure\n\nhttps://azure.microsoft.com/en-us/free/?WT.mc_id=Revolutions-blog-davidsmi\nhttps://visualstudio.microsoft.com/dev-essentials/\n\nstarts azure trial but gives you free sql server developer edition\n\nWon’t be charged until you choose to upgrade.\n12 months access to $ services for free\n$200 credit for any service for 30 days\n\nAt the end of the 30 days, I think the remainder goes into your account after you change to a pay-to-play account\n\nAccess to the services that are always free\n\nAzure Kubernetes Service (AKS)\nFunctions\n\n1,000,000 requests per month\na solution for easily running small pieces of code in the cloud. You can write just the code you need for the problem at hand, without worrying about a whole application or the infrastructure to run it.\nExample use case: for handling WebAPI requests and sending the different data and results to where it needs to go.\n\nApp Service\n\n10 web, mobile, or API apps\n\nActive Directory B2C (identity)\n\n50,000 authentications per month\n\nMachine Learning Server\n\nDevelop and run R and Python models on your platform of choice.\n\nSQL Server 2017 Developer Edition\n\nBuild, test, and demostrate applications in a non-production environment.\n\nOther stuff\n\nBlob storage\n\nobject storage solution for the cloud\noptimized for storing massive amounts of unstructured data\n\nSpot Instances (Low Priority VM)\n\nnot time limit on instance usage\nno warning on termination by Azure\n\nTips\n\nIf you can’t create a service, because Azure servers are under maintenance for more than a couple of minutes — check out your permissions and registrations under the “Resource providers” panel.\nIf you see any strange errors on the Azure Portal — just change the filters’ values.\nIf you use Azure Machine Learning, and your scoring function cannot locate your source code — deliver the code as a Model and add it explicitly to the sys.path in the init function.\nIf you use Azure Machine Learning, don’t use Batch Endpoints — it looks like they are not ready yet — just use the regular Published Pipelines. In fact, “Batch endpoint” is just a wrapper around a published pipeline.\nDon’t include flask in your Azure conda environment specification.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#aws",
    "href": "qmd/cloud-services.html#aws",
    "title": "4  Cloud Services",
    "section": "4.7 AWS",
    "text": "4.7 AWS\n\nInstance types\n\nc-type instances are compute heavy\nr-type instances are RAM heavy\nm-type instances are balanced\n“Each thread is represented as a virtual CPU (vCPU) on the instance. An instance has a default number of CPU cores, which varies according to instance type. For example, an m5.xlarge instance type has two CPU cores and two threads per core by default—four vCPUs in total.”\nspot prices from 03/24/2020, all calculations over the previous month\ngen purpose\n\nm6g.8xlarge\n\ngen purpose, 32 vcpu, 128 gb\nnewer graviton, didn’t see any specs, but supposed to be much better than the xenon 1st gen\n\nm5.8xlarge\n\ngen purpose, 32 vcpu, 128 gb\nolder 3.1 ghz, xenon\non-demand $1.54/hr\n\nm5a.8xlarge\n\ngen purpose 32 vcpu, 128 gb\n2.4 ghz, slower processor speed than m5\n\nm5n.8xlarge\n\ngen purpose 32 vcpu, 128 gb\n3.1 ghz, xenon specialized for neural networks, ML tasks\nn.virg, 71% savings, &lt;5% interruption\nohio, 83% savings, &lt;5% interruption\non-demand $1.90/hr\npotential spot price = $0.32\n\nm5dn.8xlarge\n\nsame but with 2 ssd hard drives\n\nm4.10xlarge\n\ngen purpose 40 vcpu, 160 gb\n2.4 ghz\nsmaller write-up, get the sense these are older processors/instances\n\n\ncompute optimized\n\nRequires HVM AMIs that include drivers for ENA (network adaptor) and NVMe (ssd hard drives)\n\nseems standard on a lot of instances (gen purpose and here), shouldn’ t be an issue\n\nc5.9xlarge\n\n36 vcpu, 72 gb\n3.4 ghz\non-demand $1.53/hr\n\nc5d.9xlarge\n\nsame but with ssd\n\nc5n.9xlarge\n\n36 vcpu, 96 gb\n3.0 ghz, built for task needing high throughput for networking\non-demand, $1.94/hr\n\nc4.8xlarge\n\n36 vcpu, 60 gb\n2.9 ghz\n67% savings, &lt;5% interruption\non-demand $1.59/hr\npotential spot price = $0.52\n\n\nmemory optimized\n\nr5.8xlarge\n\n32 vcpu, 256 gb\n3.1 ghz\nn.virg, 72% savings, 5-10% interruption\nn.cal, 76% savings, &lt;5% interruption\non-demand $2.02/hr\npotential spot price = $0.48\n\nr5a.8xlarge\n\n32 vpu, 256 gb\n2.5 ghz\n\nr5n.8xlarge\n\n32 vcpu, 256 gb\n3.1 ghz, neural network optimized\nus.west. oregon 76% savings, 5-10% interruption\non-demand $2.38/hr\npotential spot price = $0.57\n\nr4.8xlarge\n\n32 vcpu, 244 gb\n2.3 ghz\n\nz1d.6xlarge\n\n24 vcpu, 192 gb\n4.0 ghz\non-demand $2.23\n\n\naccelerated computing\n\ninf1.6xlarge\n\n24 vcpu, 48 gb\nbuilt for ML\non-demand $1.91/hr\n\n\n\nFree Tier (12 months after sign-up)\n\naws.amazon.com – pricing (top) – free tier (mid) – create a free account (mid)\nEC2\n\n750 hrs/mo of t2-micro instance usage\n\nfor Linux, Windows, RHEL, SLES AMIs\n\n\nElastic Block Storage (EBS)\n\n30 GB\ncan be connected to an ec2\n\nElastic Container Registry\n\n500 MB per month\n\nfor storing and retrieving Docker images\nexample in course was a basic nginx image and it was 50MB\n\n\nS3\n\n5 GB of standard storage (high availability/ high durability)\n20,000 Get Requests, 2000 Put Requests per month\n\nElastic Load Balancing\n\n750 hrs per month shared between classic and application load balancers\n\nno idea what the differences are between classic and application\n\n\n\nPricing\n\nPrice per GPU as of 29-06-2023\n\n\nExamples\n\nr3.4xlarge 16 CPUs, 122 GB RAM, 1 x 320 SSD, Spot Price: $0.1517/h\n\nTrained H2O GBM, RF, XGBoost, DeepLearning. Cluster ran for 2 hr 40 min. Total Cost = around $0.42\nhttps://www.daeconomist.com/post/2019-01-15-partii/\n\n\nStorage\n\nS3\n\ncharged by amount stored\n\n$0.023/GB for standard (for first 50 TB)\n0.004/GB for glacier and 0.00099/GB deep glacier\n\ntakes longer to retrieve and not always available\n\n\nfree inbound transfer\nfree transfer between aws services (e.g. S3 to EC2) within the same region\n\nAurora\n\nstorage + inbound/outbound: $0.20 per million requests\n\n\nConsolidated Biling\n\na separate account. All company individual accounts (marketing, sales, etc.) bills are pooled into this account\nhas no access to services\nhas no permissions to access services in other accounts\npooled bill counted towards potential discount billing\n\nCalculators\n\nTotal Cost of Ownership (TCO) calculator\n\ncompares cost of running a project on-premises to aws cloud\n\naws pricing calculator\n\ncalculates price of running a cloud application\ncalculator.aws.com\nestimates cost per service, per service group, and total infrastructure\nhelps find right ec2 instance and region\n\n\nBilling and Cost Management console\n\ncost explorer\n\nview and analysis costs and usage\n\n\n\n\nSpot Instances\n\nSummary\n\nGo to spot advisor and find instances that fit budget and compute requirements\nPrepare strategy for interruption\nOther services\n\nAs of Jan 01, 2019, cloudyr’s aws.ec2 PKG didn’t support all spot instances.\nno time limit on instance usage\nAWS gives a 2 min warning when it decides it needs your spot instance\npricing is market driven depending on capacity levels at the time\nAvailable actions when Amazon “interrupts” your instance:\n\nHibernation:\n\n“like closing your laptop display”\nsaves data and memory and reboots once instance is available again\nRight before interruption, a daemon on the instance freezes the memory and stores it in Elastic Block Store (EBS) root volume\nYour EC2 will retain this root volume and any other EBS data volumes\nOnce market price falls below bid price, instance resumes with memory restored from disk to RAM\nYou aren’t charged while instance is in hibernation, but EBS volumes do cost $.\nAvailable for instance types: C3, C4, M4, R3, and R4 with &lt; 100 GB RAM on Amazon’s Linux, Ubuntu, and Windows\nAll this is done by something called the EC2 Hibernation Agent which sound like its just the name of the program on the servers\n\nStop\n\n“like shutting down your computer to be turned on later”\nlose whatever is in RAM but retain EBS data volumes ($)\nrestores once bid price &lt; market price\n\nTerminate\n\n***default option***\neverything deleted\n\n\nSpot Advisor\n\n**always use this before spinning up spot instances **\nhttps://aws.amazon.com/ec2/spot/instance-advisor\nInput\n\nvCPUs\nMemory size\nPlatform (linux?)\navailability zone (region?)\namount required (number of instances?)\n\noutput\n\ninstance type\nvCPUs\nMemory (GB)\nSavings over On-Demand (%)\nFrequency of termination (%)\n\nliklihood your instance will get terminated\n\n\n\nRunInstance API\n\nFor requesting a spot instance through CLI I think\nLooks like you send something that looks like a python dict with max price, type, region, etc. to this API\n\nSpot Blocks\n\nallows you to set a finite duration that your instance will run for\n\n1 to 6 hrs\nno interruption during that time\n\ntypically 30 to 45% cheaper than on-demand and maybe an additional 5% cheaper during non-peak hours for the region\nrecommended for batch runs\n\nStrategy\n\nUse regions with largest pools of spot instances\n\nLargest pools\n\nus.east.1 (north.virginia)\neu.west.1(ireland)\n\nThese regions have most types/most instances available\nTypically can go uninterrupted for weeks\nless price fluctuation = more certainty\n\n\nSmallest pools\n\neu.central.1 (frankfort)\nap.south.1 (mumbai)\nap.southeast.1 (singapore)\n\ntypically get interrupted within days\n\n\n\nRun groups of instances that come from multiple spot pools\n\nTo used different compute types, jobs/tasks need to be in containers\nspot pools are instances with same region, type, OS, etc.\napplications running on instances from a least 5 different pools can cut interruptions by up to 80%\n\n\nManaging/preparing for interruptions\n\nOnly use for jobs that are short lived\n\ndevelopment and staging environments, short data processing, proof-of-concept, etc.\n\nBuild internal management system that automatically handles interruptions\n\nlook at spot pool historical prices for past 90 days\n\nlooking for least volatile pools\nolder generation (e.g. c-family, m-family) tend to be most stable\n\n\nUse 3rd party platform that manages spot instances and interruptions\n\nSpotinst - uses ML to choose and manage instances that optimizes price and provide continuous activity for apps that are without a single point of failure.\n\nUses on-demand as a fall-back.\nSLA guarantees 99.9% availability.\nSnapshots volumes to migrate data to new instances in case of interruption.\nworks with other services and platforms (kubernetes, codedeploy, etc.)\n\nSpot Fleet - aws service, automanages groups of spot instances according to either of the following strategies:\n\nstrategy options\n\nlowest price - lowest price instances\ndiversified - spread instances across pools\n\nAfter receiving 2 min warning,\n\ntake snapshots of AMI and any attached EBS volumes and use them to launch a new instance.\n\nsnapshot of AMI\n\non EC2 dashboard – left panel – instances – instances\n\nright-click instance – image – create AMI\n\nimage is in left -panel – Images – AMIs\n\n\n\nActually both snapshots might be able to taken in left panel – spot requests\n\nsee AWS note – EC2 for further details\n\n\n\n\n\n\nneed to drain and detach instance from elastic load balancer if one is used\nIf using auto-scaling, need to create an on-demand group and a spot instance group\n\n\nKubernetes\n\nAfter receiving 2 minute interruption warning from AWS:\n\nDetach instance from elastic load balancer (ELB) is one is being used\nMark instance as unschedulable (?)\n\nprevents new pods (group of containers on an instance that performs a job) from being scheduled on that node\nunderlying compute capacity and scheduling of resources of the pods needs to be monitored. Compute capacity and pod resource requirements need to match.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#comparison",
    "href": "qmd/cloud-services.html#comparison",
    "title": "4  Cloud Services",
    "section": "4.8 Comparison",
    "text": "4.8 Comparison\n\nMisc\n\nNotes from\n\nThe Top Clouds Evaluated Such That You Don’t Need to Repeat Our Mistakes\nAWS vs GCP reliability is wildly different\n\nNo services for blockchain development, quantum computing, and graph databases in GCP (May 2022)\nhttps://cloud-gpus.com/ - tool for comparing gpu compute prices across vendors\n\nData centers\n\nCloser the resources are to your business, the less latency\n(May 2022) GCP has caught up and surpassed AWS in the number of data centers and regions that are available\n\nCompute\n\nCheapest vCPU\n\nGCP “e2-micro-preemptible” with 2 vCPU and 1 GB memory.\n\n48% lower than “t4g.nano” from AWS\n5 times lower than “A0” from Azure.\n\nAWS is in-between GCP and Azure in terms of price (i.e. Azure most expensive for cheap vCPUs)\n\nMore performant GCP instances usually cost approximately the same as their analogs from other cloud providers\n\nAzure servers cost the same or slightly less than AWS\n\nGCP: dedicated PostgreSQL server\n\nCheapest instances are 25% lower than the competitors\n\nGPU on-demand availability\n\nConclusion: Assuming you need on-demand boxes to succeed right when you need them, the consensus seems to clearly point to AWS. If you can stand to wait or be redundant to spawn failures, maybe Google’s hardware acceleration customizability can win the day.\nStats\n\nAWS consistently spawned a new GPU in under 15 seconds (average of 11.4s).\nGCP on the other hand took closer to 45 seconds (average of 42.6s).\nAWS encountered one valid launch error in these two weeks whereas GCP had 84\n\nCaveats\n\nGCP allows you to attach a GPU to an arbitrary VM as a hardware accelerator - you can separately configure quantity of the CPUs as needed.\nAWS only provisions defined VMs that have GPUs attached\n\n\n\nRecommendations\n\nAzure\n\nYou use the Microsoft Office stack (Word, Teams, OneDrive, SharePoint, etc.) and/or C# programming language.\nYou head neither for the cheapest servers nor for the most expensive ones — you need something in the middle.\nYou need a memory-optimized solution rather than a general-purpose or a compute-optimized one.\nYou read about the current bugs and inconsistencies in Azure, and it does not scare you.\n\nAWS\n\nYou are rich.\nYou have AWS experts in your team.\nYou build an enterprise-level long-term project.\nOR you just want to rent a cheap virtual machine, and you don’t care about all the other facilities.\n\nGCP\n\nYou are a start-up company.\nYou can’t invest much time in learning AWS and dealing with Azure bugs.\nYou don’t need much flexibility and configuration facilities from the cloud.\nYou are ready to accept the approaches dictated by the platform.\nYou need either a general-purpose or a compute-optimized solution, but not a memory-optimized one.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html",
    "href": "qmd/db-engineering.html",
    "title": "Engineering",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-misc",
    "href": "qmd/db-engineering.html#sec-db-eng-misc",
    "title": "Engineering",
    "section": "",
    "text": "If you’re developing an application, a good rule of thumb is to write your frequently run queries in such a way that they return a response within 500 ms\nColumn storage files (parquet) are more lightweight, as adequate compression can be made for each column. Row storage doesn’t work in that way, since a single row can have multiple data types.\n\n\n(See below) Apache Avro is smaller file size than most row format file types (e.g. csv)\n\n{pins}\n\nConvenient storage method\nUse when:\n\nObject is less than a 1 Gb\n\nUsed {butcher} for large model objects\n\nSome model objects store training data\n\n\n\nBenefits\n\nJust need the pins board name and name of pinned object\n\nThink the set-up is supposed to be easy\n\nEasy to share; don’t need to understand databases",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-terms",
    "href": "qmd/db-engineering.html#sec-db-eng-terms",
    "title": "Engineering",
    "section": "Terms",
    "text": "Terms\n\nACID - A database transaction, by definition, must be atomic, consistent, isolated and durable. These are popularly known as ACID properties.  These properties can ensure the concurrent execution of multiple transactions without conflict. Guarantees data validity despite errors and ensure that data does not become corrupt because of a failure of some sort.\n\nCrucial to business use cases that require a high level of data integrity such as transactions happening in banking.\n\nBatch processing - performing an action on data, such as ingesting it or transforming it, at a given time interval.\nBTEQ - Batch Teradata Query (like SQL) is simply a utility and query tool for Teradata which is a relational database system Creating a BTEQ script to load data from a flat-file.\nConcurrency - multiple computations are happening at the same time\nData Dump - A file or a table containing a significant amount of data to be analysed or transferred. A table containing the “data dump” of all customer addresses.\nData Mart - A subset of a data warehouse, created for a very specific business use case. Finance data mart storing all the relevant financial information required by the Accounting team to process their month-end cycles.\nData Integration - Usually, the hardest part of the project, where multiple sources of data are integrated into a singular application/data warehouse. Integrating finance and customer relationship systems integrating into an MS SQL server database.\nData Lake - A repository for all kinds of structured and unstructured data. Mainly based on Hadoop storage technology. Called a lake as it is flexible enough to store anything from raw data to unstructured email files. Hadoop Data Lake. Storing logs of all customers called into the inbound call centre including call duration.\nData Mesh - Decentralized design where data is owned and managed by teams across the organisation that understands it the most, known as domain-driven ownership. tl;dr - Each department controls they’re own data from ingestion to “data products.” This data product is then made a available to the other departments for them to use in their projects. Each department has their own engineers, scientists, and analysts.\n\nEach business unit or domain aims to infuse product thinking to create quality and reusable data products — a self-contained and accessible data set treated as a product by the data’s producers — which can then published and shared across the mesh to consumers in other domains and business units — called nodes on the mesh.\nEnables teams to work independently with greater autonomy and agility, while still ensuring that data is consistent, reliable and well-governed.\nYou don’t have to figure out who’s in charge of what data, who gets to access it, who needs to protect it and what controls and monitoring is in place to ensure things don’t go wrong.\nExample: Banking\n\nCredit risk domain’s own data engineers can independently create and manage their data pipelines, without relying on a centralised ingestion team far removed from the business and lacking in credit expertise. This credit team will take pride in building and refining high-quality, strategic, and reusable data products that can be shared to different nodes (business domains) across the mesh.\n\n\nData Models - A way of organising the data in a way that it can be understood in a real-world scenario. Taking a huge amount of data and logically grouping it into customer, product and location data.\nData Quality - A discipline of measuring the quality of the data to improve and cleanse it. Checking Customer data for completeness, accuracy and validity.\nData Replication - There are multiple ways to do this, but mainly it is a practice of replicating data to multiple servers to protect an organisation against data loss. Replicating the customer information across two databases, to make sure their core details are not lost.\nDenormalization - database optimization technique in which we add redundant data to one or more tables. Designers use it to tune the performance of systems to support time-critical operations. Done in order to avoid costly joins. Me: Seems like it’s kind of like a View except a View might have calculated columns in it.\nDimensions - A data warehousing term for qualitative information. Name of the customer or their country of residence.\nDistributed SQL -  a single logical database deployed across multiple physical nodes in a single data center or across many data centers if need be; all of which allow it to deliver elastic scale and resilience. Billions of transactions can be handled in a globally distributed database.\nEDW - The same as a data warehouse except it includes all the data within an organisation. This means that the entire enterprise can rely on this warehouse for their business decisions. Organising sales, customer, marketing and finance data in an enterprise data warehouse to be able to create several key management reports.\nEmbedded aka In-Process\n\nEmbedded database as in a database system particularly designed for the “embedded” space (mobile devices and so on.) This means they perform reasonably in tight environments (memory/CPU wise.)\nEmbedded database as in databases that do not need a server, and are embedded in an application (like SQLite.) This means everything is managed by the application.\n\nFacts - A data warehousing term for quantitative information. The number of orders placed by a customer.\nFlat File - Commonly used to transfer data due to their basic nature; flat files are a single table storing data in a plain text format. All customer order numbers stored in a comma-separated value (.csv) file\nHTAP - Hybrid Transactional Analytical Processing - System that attempts be good at both OLAP and OLTP\nMaster Data - This is data that is the best representation of a particular entity in the business. This gives you a 360 view of that data entity by generally consolidating multiple data sources. Best customer data representation from multiple sources of information.\nMulti-Master - allows data to be stored by a group of computers, and updated by any member of the group. All members are responsive to client data queries. The multi-master replication system is responsible for propagating the data modifications made by each member to the rest of the group and resolving any conflicts that might arise between concurrent changes made by different members.\n\nAdvantages\n\nAvailability: If one master fails, other masters continue to update the database.\nDistributed Access: Masters can be located in several physical sites, i.e. distributed across the network.\n\nDisadvantages\n\nConsistency: Most multi-master replication systems are only loosely consistent, i.e. lazy and asynchronous, violating ACID properties. (mysql’s multi-master is acid compliant)\nPerformance: Eager replication systems are complex and increase communication latency.\nIntegrity: Issues such as conflict resolution can become intractable as the number of nodes involved rises and latency increases.\n\nCan be contrasted with primary-replica replication, in which a single member of the group is designated as the “master” for a given piece of data and is the only node allowed to modify that data item. Other members wishing to modify the data item must first contact the master node. Allowing only a single master makes it easier to achieve consistency among the members of the group, but is less flexible than multi-master replication.\n\nNiFi - It is an open-source extract, transform and load tool (refer to ETL), this allows filter, integrating and joining data. Moving postcode data from a .csv file to HDFS using NiFi.\nNormalization - A method of organizing the data in a granular enough format that it can be utilised for different purposes over time. Organizing according to data attributes reduces or eliminates data redundancy (i.e. having the same data in multiple places). Usually, this is done by normalizing the data into different forms such as 1NF (normal form) or 3NF (3rd normal form) which is the most common. (See DB, Relational &gt;&gt; Normalization)\n\nTaking customer order data and creating granular information model; order in one table, item ordered in another table, customer contact in another table, payment of the order in another table. This allows for the data to be re-used for different purposes over time.\n\nNULL indexes - These are the indexes that contain a high ratio of NULL values\nObject-Relational Mapping (ORM) - Allows you to define your data models in Python classes, which are then used to create and interact with the database. See {{SQLAlchemy}}\nODS - Operational data store generally stores limited and current information to help simple queries. Unable to handle historical or complex data queries. An ODS for daily stock fluctuations in a warehouse help the warehouse manager decide what to prioritise in the next order delivery.\nOLAP - Online Analytical Processing - large chunks of tables are read to create summaries of the stored data\n\nUse chunked-columnar data representation\n\nOLTP - Online Transactional Processing - rows in tables are created, updated and removed concurrently\n\ntraditionally use a row-based data representation\npostgres excels at this type of processing\n\nRDBMS - Relational database management system. All of the above examples are RDBMS, meaning they store data in a structured format using rows and columns.\n\nA Microsoft SQL server database.\n\nReal-Time Processing (aka Event Streaming) - each new piece of data that is picked up triggers an event, which is streamed through the data pipeline continuously\nReverse ETL - Instead of ETL where data is transformed before it’s stored or ELT where data is stored and transformed while in storage, Reverse ETL performs transformations in the pipeline between Storage and the Data Product.\n\nSCD Type 1–6 - A method to deal with changes in the data over time in a data warehouse. Type 1 is when history is overwritten whereas Type 2 (most common) is when history is maintained each time a change occurs.\n\nWhen a customer changes their address; SCD Type 1 would overwrite the old address with the new one, whereas Type 2 would store both addresses to maintain history.\n\nSchemas - A term for a collection of database objects. These are generally used to logically separate data within the database and apply access controls.\n\nStoring HR data in HR schema allows logical segregation from other data in the organisation.\n\nSharding - Horizontal Partitioning — divides the data horizontally and usually on different database instances, which reduces performance pressure on a single server.\n\nStaging - The name of a storage area that is temporary in nature; to allow for processing of ETL jobs (refer to ETL). Typically data is loaded from a source database into the staging area database where it is transformed. Once transformed, it’s loaded into the production database where analytics can be performed on it.\n\nA staging area in an ETL routine to allow for data to be cleaned before loading into the final tables.\n\nTransactional Data - This is data that describes an actual event.\n\nOrder placed, a delivery arranged, or a delivery accepted.\n\nUnstructured Data - Data that cannot be nicely organised in a tabular format, like images, PDF files etc.\n\nAn image stored on a data lake cannot be retrieved using common data query languages.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-datqual",
    "href": "qmd/db-engineering.html#sec-db-eng-datqual",
    "title": "Engineering",
    "section": "Data Quality",
    "text": "Data Quality\n\nAlso see Production, Data Validation\nAccuracy - addresses the correctness of data, ensuring it represents real-world situations without errors. For instance, an accurate customer database should contain correct and up-to-date addresses for all customers.\nCompleteness - extent your datasets have all the required information on every record\n\nMonitor: missingness\n\nConsistency - extent that no contradictions in the data received from different sources. Data should be consistent in terms of format, units, and values. For example, a multinational company should report revenue data in a single currency to maintain consistency across its offices in various countries.\nTimeliness - Data should be available at the time it’s required in the system\nValidity - ensuring that data adheres to the established rules, formats, and standards.\n\nMonitor: variable types/classes, numeric variable: ranges, number of decimal places, categorical variable: valid categories, spelling\n\nUniqueness - no replication of the same information twice or more. They appear in two forms; duplicate records and information duplication in multiple places.\n\nMonitor: duplicate rows, duplicate columns in multiple tables",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-costopt",
    "href": "qmd/db-engineering.html#sec-db-eng-costopt",
    "title": "Engineering",
    "section": "Cost Optimization",
    "text": "Cost Optimization\n\nAlso see\n\npage 53 in notebook\nGoogle, BigQuery &gt;&gt; Optimization\n\nAvoid disk operations, make sure that you look out for hints & information in the EXPLAIN PLAN of your query. (e.g. using SORT without an index)\n\nWhen you see filesort, understand that it will try to fit the whole table in the memory in many chunks.\n\nIf the table is too large to fit in memory, it will create a temporary table on disk.\n\nLook out for a using filesort with or without a combination of using temporary.\n\nLoading data in chunks or streaming it record by record for ETL jobs helps to optimize memory usage.\nSplit tables with many columns Might be efficient to split the less-frequently used data into separate tables with a few columns each, and relate them back to the main table by duplicating the numeric ID column from the main table.\n\nEach small table can have a primary key for fast lookups of its data, and you can query just the set of columns that you need using a join operation.\n\nPrimary keys should be global integers.\n\nIntegers consume less memory than strings, and they are faster to compare and hash\n\nJoins\n\nWith correlated keys\n\nThe query planner won’t recognize the correlated keys and do nested loop join when a hash join is more efficient\nI don’t fully understand what correlated keys on a join are, but see SQL &gt;&gt; Terms &gt;&gt; Correlated/Uncorrelated queries\n\nIn the example below, a group of merge_commit_ids will only be from 1 repository id, so the two keys are associated in a sort of traditional statistical sense.\n\nSolutions\n\nUse LEFT_JOIN instead of INNER_JOIN\nUse extended statistics\nCREATE STATISTICS ids_correlation ON repository_id, merge_commit_id FROM pull_requests;\n\n“repository_id” and “merge_commit_id” are the correlated keys\nI’m not sure if “ids_correlation” is a function or just a user-defined name\nPostgreSQL ≥13 will recognize correlation and the query planner will make the correct calculation and perform a hash join\n\n\n\n\nPre-join data before loading it into storage\n\nIf a group of tables is frequently joined and frequently queried, then pre-joining will reduce query costs\ncan be done using an operational transform system such as Spark, Flow, or Flink (dbt can parallelize runs and work w/Spark)\n\nIndexes{#sec-db-eng-costopt-index}\n\nIndexes help in filtering data faster as the data is stored in a predefined order based on some key columns.\n\nIf the query uses those key columns, the index will be used, and the filter will be faster.\n\nSuitable for any combination of columns that are used in filter, group, order, or join\nMySQL Docs\nDon’t use indexes with LIKE\nCluster a table according to an index\n\nAlso see Google, BigQuery &gt;&gt; Optimization &gt;&gt; Partition and Cluster\nRearranges the rows of a table on the disk\nDoesn’t stay “clustered” if table is updated\n\nSee pg_repack for a solution\n\nExample\n-- create index\nCREATE INDEX pull_requests_repository_id ON pull_requests (repository_id, number)\n-- cluster table\nCLUSTER pull_requests USING pull_requests_repository_id\n\n\nUseful for queries such as\nSELECT *\nFROM pull_requests\nWHERE repository_id IN (...) AND number &gt; 1000\nBest Pactices\n\nAvoid too many indexes\n\nA copy of the indexed column + the primary key is created on disk\nIndexes add to the cost of inserts, updates, and deletes because each index must be updated\nBefore creating an index, see if you can repurpose an existing index to cater to an additional query\nCreate the least possible number of indexes to cover most of your queries (i.e. Covering Indexes).\n\nMakes effective use of the index-only scan feature\nAdd INCLUDE to the create index expression\nExample\n-- query\nSELECT y FROM tab WHERE x = 'key';\n-- covering index, x\nCREATE INDEX tab_x_y ON tab(x) INCLUDE (y);\n-- if the index, x, is unique\nCREATE UNIQUE INDEX tab_x_y ON tab(x) INCLUDE (y);\n\ny is called a non-payload column\n\nDon’t add too many non-payload columns to an index. Each one duplicates data from the index’s table and bloat the size of the index.\n\n\nExample: Query with function\n-- query\nSELECT f(x) FROM tab WHERE f(x) &lt; 1;\n-- covering index, x\nCREATE INDEX tab_f_x ON tab (f(x)) INCLUDE (x);\n\nWhere f() can be MEAN, MEDIAN, etc.\n\n\n\nFix unusable indexes\n\nIssues related to data types, collation (i.e. how it’s sorted), character set (how the db encodes characters), etc\nSometimes you can make the indexes work by explicitly forcing the optimizer to use them. (?)\n\nRepurpose or delete stale indexes\n\nIndexes are designed to serve an existing or a future load of queries on the database\nWhen queries change, some indexes originally designed to serve those queries might be completely irrelevant now\nAutomate stale index removal. Dbs keep statistics. Write a script to either notify you or just delete the index if it’s older and not been used past a certain threshold\n\nUse the most cost efficient index type\n\nExample: If your use case only needs a regular expression search, you’re better off having a simple index than a Full Text index.\n\nFull Text indexes occupy much more space and take much more time to update\n\n\nDon’t index huge tables (&gt; 100M rows), partition instead\n\nThen prune the partitions (partition pruning) you don’t need and create indexes for the partitioned tables you do keep.\n\n\nPartitioning\n\nAlso see Google, BigQuery &gt;&gt; Optimization &gt;&gt; Partition and Cluster\nSplits your table into smaller sub-tables under the hood\n\nNot viewable unless you check the table directory to see the multiple files that have been created\n\nThe same goes for indexes on that table.\n\n\nUse on tables with at least 100 million rows (BigQuery recommends &gt; 1 GB) Partitioning helps reduce table size and, in turn, reduces index size, which further speeds up the Data Warehouse (DWH) operations. But, partitioning also introduces complexity in the queries and increases the overhead of managing more data tables, especially backups. So try a few of the other performance techniques before getting to Sharding.\nPartition columns should always be picked based on how you expect to use the data, and not depending on which column would evenly split the data based on size.\n\nExample: partition on county because your analysis or transformations will largely be done by county even though since some counties may be much larger than others and will cause the partitions to be substantially imbalanced.\n\n\nUse ELT (e.g. load data from on-prem server to cloud, then transform) instead of ETL (transform data while on-prem, then load to cloud) for data pipelines\n\nMost of the time you have a lot of joins involved in the transformation step\n\nSQL joins are one of the most resource-intensive commands to run. Joins increase the query’s runtime exponentially as the number of joins increases.\nExample\n\nRunning 100+ pipelines with some pipelines having over 20 joins in a single query.\nEverything facilitated by airflow (see bkmk for code)\nETL: postgres on-prem server, sql queries with joins, tasks ran 12+ hours, then the transformed data is loaded to google storage\n\n13+ hrs for full pipeline completion\n\nELT: running the queries with the joins, etc. with bigquery sql on the data after it’s been loaded into google storage.\n\n6+ hrs for full pipeline completion\n\n\n\n\nUse Materialized Views\n\nA smaller data object that contains the subset of data resulting from a specific query\nWhereas a query happens after data is loaded, a materialized view is a precomputation\nThe computation is done once, and changes to the data are incorporated as they occur, making subsequent updates to the view much cheaper and more efficient than querying the entire database from scratch\n\nFetching a large table will be slower if you try to use multiple cores.\n\nYou have to divide up the table and recombine it. Plus setting up parallel network processes takes time.\nThe time used to fetch some data from the internet depends massively on the internet bandwidth available on your router/network.\n\nUse Random Access via http range header + sparse-hilbert index to optimize db for query searches\nCITEXT extension makes it so you don’t have use lower or upper which are huge hits on performance (at least they are in WHERE expressions) GIN custom indexes for LIKE and ILIKE\nCREATE EXTENSION IF NOT EXISTS btree_gin;\nCREATE EXTENSION IF NOT EXISTS pg_trgm;\nCREATE INDEX index_users_on_email_gin ON users USING gin (email gin_trgm_ops);\n\nCREATE EXTENSION adds btree and pg_trgm extensions\nindex_users_on_email_gin is the name of the index\nusers is the table\nUSING gin (email gin_trgm_ops)\n\ngin specifies that it’s a gin index\nemail is the field\ngin_trgm_ops is from the pg_trgm extension. It splits the index into trigrams which is necessary for the gin index to work with LIKE or ILIKE\n\nSlower to update than the standard ones. So you should avoid adding them to a frequently updated table.\n\nGiST indexes are very good for dynamic data and fast if the number of unique words (lexemes) is under 100,000, while GIN indexes will handle 100,000+ lexemes better but are slower to update.\n\n\nNULLS LASTputs the NULLS in a field in any sorting operations at the end\n\nThe default behavior of ORDER BY will put the NULLS first, so if you use LIMIT , you might get back a bunch of NULLS.\nUsing NULLS LAST fixes this behavior but its slow even on an indexed column\n\nExample: ORDER BY email DESC NULLS LAST LIMIT 10\n\nInstead use two queries\nSELECT *\nFROM users\nORDER BY email DESC\nWHERE email IS NOT NULL LIMIT 10;\n\nSELECT *\nFROM users\nWHERE email IS NULL LIMIT 10;\n\nThe first one would fetch the sorted non-null values. If the result does not satisfy the LIMIT, another query fetches remaining rows with NULL values.\n\n\nRebuild Null Indexes\nDROP INDEX CONCURRENTLY users_reset_token_ix;\nCREATE INDEX CONCURRENTLY users_reset_token_ix ON users(reset_token)\nWHERE reset_token IS NOT NULL;\n\nDrops and rebuilds an index to only include NOT NULL rows\nusers_reset_token_ix is the name of the index\nusers is the table\nI assume “reset_token has to be the field\n\nWrap multiple db update queries into a single transaction\n\nImproves the write performance unless the database update is VERY large.\nA large-scale update performed by a background worker process could potentially timeout web server processes and cause a user-facing app outage\nFor large db updates, add batching\n\nExample: db update has a 100K rows, so update 10K at a time.\nUPDATE messages SET status = 'archived'\n  WHERE id IN\n  (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 0);\nUPDATE messages SET status = 'archived'\n  WHERE id IN\n  (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 10000);\nUPDATE messages SET status = 'archived'\n  WHERE id IN\n  (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 20000);\n\nmessages is the table name\nI guess OFFSET is what’s key here.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-ets",
    "href": "qmd/db-engineering.html#sec-db-eng-ets",
    "title": "Engineering",
    "section": "Event Tracking Systems",
    "text": "Event Tracking Systems\n\nEvents are queued, then batch inserted into your db.\n\nStreaming events does not scale very well and is not fault tolerant.\n\nCommercial Services\n\nSegment\n\nMost popular option\nVery expensive\nSusceptible to ad blockers\nOnly syncs data once per hour or two\nMissing a few key fields in the schema it generates (specifically, session and page ids).\n\nFreshpaint is a newer commercial alternative that aims to solve some of these issues.\n\nOpen Source (each with a managed offering if you don’t feel like hosting it yourself)\n\nSnowplow is the oldest and most popular, but it can take a while to setup and configure.\nRudderstack is a full-featured Segment alternative.\nJitsu is a pared down event tracking library that is laser focused on just getting events into your warehouse as quickly as possible.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-stream",
    "href": "qmd/db-engineering.html#sec-db-eng-stream",
    "title": "Engineering",
    "section": "Streaming",
    "text": "Streaming\n\nStreaming or near real-time (i.e. micro-batch) data\nQuestions\n\nWhat would be the data flow rate in that pipeline?\nDo you require real-time analytics, or is near-real-time sufficient? \n\nData Characteristics\n\nIt is ingested near-real-time.\nUsed for real-time reporting and/or calculating near-real-time aggregates. Aggregation queries on it are temporal in nature so any aggregations defined on the data will be changed over time as the data comes.\nIt is append-only data but can have high ingestion rates so needs support for fast writes.\nHistorical trends can be analyzed to forecast future metrics.\n\nRelational databases can’t handle high ingestion rates and near-real-time aggregates without extensions.\nSteaming is the most expensive way to process the data in the majority of cases. Typically batch ingesting into warehouses is free, but streaming may not be.\nUse Cases: anomaly detection and fraud prevention, real-time personalized marketing and internet of things.\nTools:\n\nApache Kafka - Flexible, connects to app servers, other microservices, databases, sensor networks, financial networks, etc. and can feed the data to same types of systems including analytical tools.\n\nUtilizes a publish-subscribe model where producers (i.e. sources) publish data to topics and consumers (e.g. DBs, BI tools, Processing tools) subscribe to specific topics to receive relevant data.\nHighly scalable due to its distributed architecture, allowing data handling across multiple nodes.\nConfluent’s Kafka Connect - Open source and Commerical Connectors\n\nApache Flume - Similar to Kafka but easier to manage, more lightweight, and built to output to storage (but not as flexible as Kafka)\n\nLess scalable as data ingestion is handled by individual agents, limiting horizontal scaling.\nIts lightweight agents and simple configuration make it ideal for log collection\nCan also handle Batch workloads\nAble to perform basic preprocessing, e.g. filtering specific log types or converting timestamps to a standard format\n\nAmazon Kinesis - A managed, commercial alternative to Kafka. Charges based on data throughput and storage. Additional features include data firehose for delivery to data stores and Kinesis analytics for real-time analysis.\nApache Flink - Processes streaming data with lower latency than Spark Streaming, especially at high throughputs. Less likely to duplicate data. Uses SQL. Steeper learning curve given its more advanced features.\nApache Spark Streaming - See Apache, Spark &gt;&gt; Streaming\nGoogle Pub/Sub - Uses Apache Beam programming API to construct processing pipelines\n\nGoogle Dataflow can create processing pipelines using streaming data from Pub/Sub. Developers write their pipelines using Beam’s API, and then Beam translates them into specific instructions for Flink or Spark to execute.\n\n{{temporian}} can interact with Beam to perform various time series preprocessing\n\nIf you have existing workflows around Hadoop or Spark or expertise in those frameworks, then Google Dataproc allows you to reuse that code. It also allows you to used other libraries that aren’t available in Dataflow. Supports various languages like Java, Python, and Scala.\nFor short-lived batch jobs, Dataproc might be more cost-effective. Although, Dataflow’s serverless nature avoids idle resource charges while Dataproc clusters incur costs even when idle.\n\n\nArchitectures\n\nNotes from\n\nData Pipeline Design Patterns\n\nETL\n\n\nKinesis collects data from a server (e.g. app) and continuously feeds it to a lambda function for transformation. Transformed data is deposited into a S3 bucket, queried using Athena, and visualized using Quicksight.\n\nHybrid (Streaming and Batch)\n\n\nKinesis streams data to S3 and when a threshold is reached, a lambda trigger activates a transformation/batch load to the BQ warehouse\n\n\nTimeScale DB\n\nOpen source extension for postgresql\nSupport all things postgresql like relational queries, full SQL support(not SQL-like) as well as the support of real-time queries\nSupports an ingestion of 1.5M+ metrics per second per server\nNear-real-time aggregation of tables\nProvides integration with Kafka, kinesis, etc for data ingestion.\nCan be integrated with any real-time visualization tool such as Graphana\n\nPipeline DB\n\nOpen source extension for postgresql\nSimilar features as TimeScale DB\nEfficiency comes from it not storing raw data\n\nUsually, it’s recommended to store raw data",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-otools",
    "href": "qmd/db-engineering.html#sec-db-eng-otools",
    "title": "Engineering",
    "section": "Other Tools",
    "text": "Other Tools\n\nDataFold monitors your warehouse and alerts you if there are any anomalies (e.g. if checkout conversion rate drops suddenly right after a deploy).\nHightouch lets you sync data from your warehouse to your marketing and sales platforms.\nWhale is an open source tool to document and catalog your data. \nRetool lets you integrate warehouse data into your internal admin tools.\nGrowth Book that plugs into your data warehouse and handles all of the complicated querying and statistics required for robust A/B test analysis.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html",
    "href": "qmd/db-lakes.html",
    "title": "Lakes",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-misc",
    "href": "qmd/db-lakes.html#sec-db-lakes-misc",
    "title": "Lakes",
    "section": "",
    "text": "Data is stored in structured format or in its raw native format without any transformation at any scale.\n\nHandling both types allows all data to be centralized which means it can be better organized and more easily accessed.\n\nOptimal for fit for bulk data types such as server logs, clickstreams, social media, or sensor data.\nIdeal use cases\n\nBackup for logs\nRaw sensor data for your IoT application,\nText files from user interviews\nImages\nTrained machine learning models (with the database simply storing the path to the object)\n\nTools\n\nRclone - A command-line program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors’ web storage interfaces. Over 70 cloud storage products support rclone including S3 object stores\n\nLower storage costs due to their more open-source nature and undefined structure\nOn-Prem set-ups have to manage hardward and environments\n\nIf you wanted to separate stuff like test data from production data, you also probably had to set up new hardware.\nIf you had data in one physical environment that had to be used for analytical purposes in another physical environment, you probably had to copy that data over to the new replica environment.\n\nHave to keep a tie to the source environment to ensure that the stuff in the replica environment is still up-to-date, and your operational source data most likely isn’t in one single environment. It’s likely that you have tens — if not hundreds — of those operational sources where you gather data.\n\nWhere on-prem set-ups focus on isolating data with physical infrastructure, cloud computing shifts to focus on isolating data using security policies.\n\nObject Storage Systems\n\nCloud data lakes provide organizations with additional opportunities to simplify data management by being accessible everywhere to all applications as needed\nOrganized as collections of files within directory structures, often with multiple files in one directory representing a single table.\n\nPros: highly accessible and flexible\nMetadata Catalogs are used to answer these questions:\n\nWhat is the schema of a dataset, including columns and data types\nWhich files comprise the dataset and how are they organized (e.g., partitions)\nHow different applications coordinate changes to the dataset, including both changes to the definition of the dataset and changes to data\n\nHive Metastore (HMS) and AWS Glue Data Catalog are two popular catalog options\n\nContain the schema, table structure and data location for datasets within data lake storage\n\n\nIssues:\n\nDoes not coordinate data changes or schema evolution between applications in a transactionally consistent manner.\n\nCreates the necessity for data staging areas and this extra layer makes project pipelines brittle",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-brands",
    "href": "qmd/db-lakes.html#sec-db-lakes-brands",
    "title": "Lakes",
    "section": "Brands",
    "text": "Brands\n\nHadoop\n\nTraditional format for data lakes\n\nAmazon S3\n\nTry to stay &lt;1000 entries per level of hierarchy when designing the partitioning format. Otherwise there is paging and things get expensive.\nAWS Athena ($5/TB scanned)\n\nAWS Athena is serverless and intended for ad-hoc SQL queries against data on AWS S3\n\n\nMicrosoft Azure Data Lake Storage (ADLS)\nMinio\n\nOpen-Source alternative to AWS S3 storage.\nGiven that S3 often stores customer PII (either inadvertently via screenshots or actual structured JSON files), Minio is a great alternative to companies mindful of who has access to user data.\n\nOf course, AWS claims that AWS personnel doesn’t have direct access to customer data, but by being closed-source, that statement is just a function of trust.\n\n\nDatabricks Delta Lake -\nGoogle Cloud Storage\n\n5 GB of US regional storage free per month, not charged against your credits.\n\nApache Hudi - A transactional data lake platform that brings database and data warehouse capabilities to the data lake. Hudi reimagines slow old-school batch data processing with a powerful new incremental processing framework for low latency minute-level analytics.",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-iceb",
    "href": "qmd/db-lakes.html#sec-db-lakes-iceb",
    "title": "Lakes",
    "section": "Apache Iceberg",
    "text": "Apache Iceberg\n\nOpen source table format that addresses the performance and usability challenges of using Apache Hive tables in large and demanding data lake environments.\n\nOther currently popular open table formats are Hudi and Delta Lake.\n\nInterfaces\n\nDuckDB can query Iceberg tables in S3 with an extension, docs\nAthena can create Iceberg Tables\nGoogle Cloud Storage has something called BigLake that can create Iceberg tables\n\nFeatures\n\nTransactional consistency between multiple applications where files can be added, removed or modified atomically, with full read isolation and multiple concurrent writes\nFull schema evolution to track changes to a table over time\nTime travel to query historical data and verify changes between updates\nPartition layout and evolution enabling updates to partition schemes as queries and data volumes change without relying on hidden partitions or physical directories\nRollback to prior versions to quickly correct issues and return tables to a known good state\nAdvanced planning and filtering capabilities for high performance on large data volumes\nThe full history is maintained within the Iceberg table format and without storage system dependencies\n\nComponents\n\nIceberg Catalog - Used to map table names to locations and must be able to support atomic operations to update referenced pointers if needed.\nMetadata Layer (with metadata files, manifest lists, and manifest files) - Stores instead all the enriching information about the constituent files for every different snapshot/transaction\n\ne.g. table schema, configurations for the partitioning, etc.\n\nData Layer - Associated with the raw data files\n\nSupports common industry-standard file formats, including Parquet, ORC and Avro\nSupported by major data lake engines including Dremio, Spark, Hive and Presto\nQueries on tables that do not use or save file-level metadata (e.g., Hive) typically involve costly list and scan operations\nAny application that can deal with parquet files can use Iceberg tables and its API in order to query more efficiently\nComparison",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html",
    "href": "qmd/docker-aws.html",
    "title": "AWS",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-misc",
    "href": "qmd/docker-aws.html#sec-docker-aws-misc",
    "title": "AWS",
    "section": "",
    "text": "Notes from Linkedin Learning Docker on AWS\n\nThe example used in this class is for a web server",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-summ",
    "href": "qmd/docker-aws.html#sec-docker-aws-summ",
    "title": "AWS",
    "section": "Summary",
    "text": "Summary\n\nPush docker image to ECR\nPush app code and build instructions (buildspec yaml) to CodeCommit\nCreate CodeBuild project that executes image building instructions\nCreate a Pipeline that triggers CodeBuild (automatic image build when new code is committed)\nChoose a Cluster method (fargate or manual EC2), then create and start cluster instances\n\nonly able to specify number of instances to create with the EC2 method\nfargate handles most of the configuration (cost extra?)\n\nCreate a task or a service\n\nA task is for short running jobs, no load balancer or autoscaling. Its definition details the container configuration; how much of the resources you want your workloads (e.g. app) to be able to use; communication between containers, etc.\nA service is for long running jobs. Creates tasks and autoscales number of instances and load balances traffic \n\nAdd a storage container and update task definition to include a shared volume",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-gloss",
    "href": "qmd/docker-aws.html#sec-docker-aws-gloss",
    "title": "AWS",
    "section": "Glossary of AWS Services Used",
    "text": "Glossary of AWS Services Used\n\nECR stores your images that you build\nCodeCommit (CC) is like a github (code storage)\nCodeBuild sets up the process of using a yaml file in your CC repo as instructions to build the images\nPipeline is the CI/CD part. Triggers an image build every time there’s a new push to CodeCommit \nRoute 53 takes your domain name (www.store.com/app), creates a DNS ip address and reroutes traffic from that domain to your load balancer.\nContainer Networking Models\n\nHost - direct mapping to host networking (EC2)\n\nuse when performance is prime concern\nonly 1 container per task per port\n\nAwsvpc - ENI per task, required for fargate\n\nmost flexibility\nrecommended for most use cases\n\nBridge - “Classic” docker networking\n\ndidn’t get discussed\n\nNone - multi-container localhost and storage\n\nonly local connectivity (i.e. communication between containers within a task)\n\nSecurtiy groups available for Host and AWSvpc\n\nsecurity groups allow for tracking container performance and limiting access",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-meths",
    "href": "qmd/docker-aws.html#sec-docker-aws-meths",
    "title": "AWS",
    "section": "Two Methods For Running Containers on AWS",
    "text": "Two Methods For Running Containers on AWS\n\nManaging the EC2 instances yourself (see section below for set-up instructions)\n\nif you understand the capacity you need and want greater control, this might be better\nYou pay for unused capacity\nBilling is like the standard billing for using an EC2 instance \n\nFargate (see section below for set-up instructions)\n\nManaged by Amazon, less control, less to deal with\nYou don’t have to deal with starting, stopping, choosing compute sizes, capacities etc. of EC2 instances\nBilled by CPU/Memory and Time that’s used by your container",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ecr",
    "href": "qmd/docker-aws.html#sec-docker-aws-ecr",
    "title": "AWS",
    "section": "Elastic Container Registry (ECR)",
    "text": "Elastic Container Registry (ECR)\n\nCreate an ECR account\n\nLog into your account\nsearch ecr\nClick create “get started” under Create a Repository (mid right)\n\nSays you pay for the amount of data you store in the repository and data transferred to the internet.\n\nReason for doing this is latency. The repo is regional and you want your image/app to be close to the host\n\n\nAssign a name\n\nfirst part is a hash + region + amazon.com\nyou add a name. whatever you want\n\nmutable/immutable\n\nIf you’re going to be storing multiple versions of the same image, you should choose mutable.\n\nClick create repository (bottom right)\n\nPush image to ECR repo\n\ncopy the URI for your repo from the ecr console (ecr – left panel – repositories – images)\n\nsave it to registry-tag.txt file in your local image directory\nAlso include it as the tag to your docker image\n\ndocker build . -t &lt;URI&gt;\n\nauto-appends “:latest”\n\n\n\nIn terminal\n\n(aws ecr get-login --no-include-email --region &lt;region&gt;}\n\n*with parentheses\nregion is whatever you have in your profile e.g. us-east-2\ngets login from the aws profile you’ve already set-up\nprints some kind of warning, he didn’t act like it was meaningful\n\ndocker push &lt;tag&gt;\n\nwhich is your URI:\n\nin the example, the version is “latest”\n\n\nIf something doesn’t work, the instructions are at aws\n\nIn repository console\n\nclick repo name\nclick view push commands (top right)\nshows how to connect to repository and “push” instance\n\n\n\nIn console, hit refresh (mid-right) to see that the image is loaded into the repo",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-codcom",
    "href": "qmd/docker-aws.html#sec-docker-aws-codcom",
    "title": "AWS",
    "section": "CodeCommit",
    "text": "CodeCommit\n\nCreate a CodeCommit git repository - benefit is having (image/app) code live near hosting service, less latency for CI/CD processes\n\nDeveloper tools – CodeCommit – (maybe left panel – Source – Repositories)\nClick create repository (top right)\n\nEnter name\n\nDoesn’t have to match the name of the image repo, but might be worth doing\nalso a box for entering a description\n\nClick create\n\nConnection Steps\n\nhttps or ssh\n\nclick ssh\n\nfollow these directions to gitbash and create SSH keys for windows, enter them into config file, clone repository, etc. etc.\n\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-windows.html\nHe cloned the repo, one directory above the .ssh directory\n\n\n\nPush Container Code to CodeCommit repo\n\ncd to cloned repo directory\ncopy code files to that directory\ngit add *, git commit -m “blah blah”, git push -u origin master\n\n-u is for upstream\n“-u origin master”  necessary for a first push\n\nFiles should be present in developer tools – CodeCommit – Source – Repositories – repo",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-codcomproj",
    "href": "qmd/docker-aws.html#sec-docker-aws-codcomproj",
    "title": "AWS",
    "section": "Create a CodeBuild Project",
    "text": "Create a CodeBuild Project\n\nBuild the CodeCommit repo into a docker container\nbuildspec.yml (see hostname folder in exercise files)\n\nyaml script that automates building docker, logging into ECR, building an image, and pushing it to ECR\ncodebuild version used was 2.0 (which is at the top of the yaml script)\n\ncodebuild must be some aws tool you can use to do this\n\nadd, commit, push to CodeCommit repo\n\ndeveloper tools — codecommit – left panel – build – build projects\n\nclick create build project (upper right)\n\nanything not listed below, just used defaults\n\nenter project name\n\nhe gave same name as CC repo\n\nUnder Source, make sure it says CodeCommit, enter repo name in box\nMake sure Manage Image box is ticked\nOperating System\n\nhe used Ubuntu\n\nRuntime\n\nselect Standard\n\nImage\n\nstandard 2.0\n\nPriviledged\n\ntick box “Enable this flag if you want to build Docker images or want your builds to have elevated priviledges”\n\nLogs\n\nUsing cloudwatch\n\ngroup name - codebuild\nStream Name\n\nhe used the name of the CC repo\n\n\n\nClick Create build project (bottom right)\n\nGoto IAM console – left panel – Roles\n\nWhen the “build project” was created a role was also created\n\nUnder Role name -  click “codebuild--service-role”\nclick attach policy (mid left)\nSearch for AmazonEC2ContainerRegistryPowerUser\ntick box to select it\nclick attach policy (bottom right)\n\n\nGoto Developer tools – CodeBuild – left panel – Build – Build Project – project name\n\nClick Start Build (top right)\nkeep all defaults, Click Start Build (bottom right)\n\nProject builds and under Build Status, status should say “succeeded” when it finishes\n\nWhich means there are now two images in the ECR repo\n\noriginal push and image built from this project build process (duplicate)\n\n\nAutomate building container when new code is pushed (CI/CD)\n\ndeveloper tools – codebuild – left panel – pipeline – pipelines\nclick create pipeline\n\nenter pipeline name\n\nhe named it the CC repo name, hostname\nclick next\n\nAdd source stage\n\nchoices\n\nCodeCommit\nECR\nS3\nGithub\nChoose codecommit\n\nSelect repo name\n\nexample: “hostname”\n\nSelect branch\n\nexample “master”\n\nDetection option\n\nselect CloudWatch\n\nClick next\n\nAdd build stage\n\nCodeBuild or Jenkins\n\nchoose CodeBuild\n\nRegion\n\nexample US East - (Ohio)\n\nProject Name\n\nname of the build project from last section\nexample hostname\n\nClick next\n\nAdd deploy stage\n\nskipped, because something I didn’t understand. Sound like another level of automation that might be used in the future\nclick skip deploy stage\n\nReview\n\nClick create pipeline (bottom right)\n\n\nOnce created, it will start building the pipeline from the CodeCommit source\n\nProcess takes a few minutes\ndetects the buildspec.yml in CC and executes it\nunder Build Section, there will be a details link, you can right-click and open it in a new tab\n\nShould result in a 3rd image (duplicate images) in the ECR repo\n\nSo anytime a new commit is pushed to CodeCommit, an image will be built and stored in ECR",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ec2user",
    "href": "qmd/docker-aws.html#sec-docker-aws-ec2user",
    "title": "AWS",
    "section": "Create Cluster: EC2 (User-Managed)",
    "text": "Create Cluster: EC2 (User-Managed)\n\nCreate Cluster: Set-up instructions for running containers using EC2 method\n\nSearch for ECS\nleft panel – Under Amazon ECS: Clusters\n\nClick create cluster\nChoose Linux + Networking\n\nWindows + Networking and Networking-only (Fargate see below) options also available\nclick next (bottom right)\n\nConfigure Cluster\n\nEnter Cluster name\n\nexample: ecs-ec2\n\nProvisioning\n\nOn demand instance\nspot instance\n\nEC2 instance type (size of compute)\n\nexample: t2.medium\n\nNumber of instances\n\nhe chose 1\n\nEC2 AMI id\n\nLinux-1, linux-2\n\nhe chose linux-2; didn’t give a reason\n\n\nDefaults kept for Virtual Private Cloud (VPC), Security Group, storage, etc.\nCloudWatch container insights\n\ntick enable container insights\nso you can monitor stats in Cloudwatch and help you tune compute resources in the future\n\nClick create\n\ntakes a minute or two to spin up the instance",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ec2ssh",
    "href": "qmd/docker-aws.html#sec-docker-aws-ec2ssh",
    "title": "AWS",
    "section": "Create Cluster: EC2 with SSH Access",
    "text": "Create Cluster: EC2 with SSH Access\n\nCreate Cluster using EC2 method with SSH access (not possible with a Fargate cluster) and connect to it\nSteps\n\nFind your ssh public key\n\ngo into git bash and type “cat ~/.ssh/id_ed25519.pub”\n\noptions\n\nid_rsa.pub\nid_ecdsa.pub\nid_ed25519.pub\n\nI have 2, rsa that I created when linking rstudio to github and ed25519 when I created gitlab acct\n\nCopy everything (including the ssh-filename beginning part) all the way until your email (don’t include)\n\nGoto EC2 services page (open new tab)\n\nUnder Resources (mid), click Key Pairs\n\nClick import\npaste key into Public Key Contents box\nenter a name\n\nexample ecs-ec2-key\n\nclick import\n\n\nGo back to the ECS services page and create another cluster\n\nSame as before. (create cluster - EC2 method above) except:\n\ncluster name - ecs-ec2-ssh\nkey pair - chose newly imported key pair\nNetworking\n\nvpc\n\ndrop down\n\nchoose vpc created by prev. cluster (some big long hash)\n\n\nsubnets\n\ndropdown\n\nchoose subnet created by prev. cluster\nspawns another dropdown to add another subnet\n\ndropdown\n\nchoose second subnet created by prev.cluster\n\nShould only be 2, they’re names should gray-out after you choose them\n\nsecurity group\n\nchoose the one created by the prev. cluster\n\n\nHaving SSH available will allow us to go into the container and view docker ressource\n\n\nCopy public ip address and open SSH port (also see AWS notebook – EC2 – connect/terminate instance)\n\nClick on Cluster name\nclick on ECS instances tab (mid left)\nright-click EC2 Instance id and open in new tab\n\ncopy IPv4 Public IP (lower right)\nclick security group link (lower left)\n\nclick inbound tab (lower left)\nclick edit\n\nclick add rule\nunder Type, click dropdown and select SSH\n\nautomatically chooses port 22\n\nSource\n\nkept 0.0.0.0  (“v4 address”, guess 4 is for the 4 numbers in the address)\n\nDescription\n\nkept default\n\nclick save\n\n\n\n\nOpen terminal\n\nssh -i ~/.ssh/id_rsa ec2-user@\n\nasks if you’re sure, say yes\n\nCheck container status on instance\n\nsudu su -\n\nswitches to being a root user\n\ndocker ps\n\nshows container id and name, image, status, etc.\n\n\nexec into container\n\ndocker exec -it  sh\n\nonly works if linux image has a shell environment\ninstead of sh, can try bash\nor docker run  –rm –name linux -it alpine:latest sh\nctrl + d\n\nleave shell\n\nexit\n\nto exit as root user\n\nexit\n\nleaves instance, closes connection",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-farg",
    "href": "qmd/docker-aws.html#sec-docker-aws-farg",
    "title": "AWS",
    "section": "Create Cluster: Fargate (AWS-Managed)",
    "text": "Create Cluster: Fargate (AWS-Managed)\n\nSet-up instructions for running containers using Fargate method\n\nSearch for ECS\nleft panel – Under Amazon ECS: Clusters\n\nClick create cluster\n\nChoose Network-only (amazon fargate)\nEnter Cluster name\n\nexample ecs-fargate\n\ntick box for Enable Container insights (cloudwatch)\nclick create (bottom right)\n\n\nCluster created instantaneously\n\nclick view cluster",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-catd",
    "href": "qmd/docker-aws.html#sec-docker-aws-catd",
    "title": "AWS",
    "section": "Creating a Task Defintion",
    "text": "Creating a Task Defintion\n\nA task definition is a blueprint for your tasks, specifying what container image to use, how much CPU and memory is needed, and other configurations.\nIf using load balancer goto the “create appplication load balancer” and  “add ecs service and task” below\ndetails the images to use, the CPU and memory to allocate, environment variables, ports to expose, and how the containers interact.\n\nassociates the cluster created in the previous section with the app or workload\n\n1 task definition can be used in multiple containers\nsearch for ECS\nleft panel – Clusters – task definitions\n\nclick create new task definition\nselect cluster method (Fargate or EC2)\n\nchoose fargate\nclick next step\n\ncreate task-definition name\n\neg hostname-fargate\n\ntask role\n\nused if workload creates other resources inside aws\nleave blank\nsome kind of warning about the network settings, he ignored it.\n\ntask execution iam role\n\ngives permission for cluster to use task defintion\nkeep default\n\ntask size\n\nmemory size choice effects available choices for cpu\ndepends on your application needs, if running multiple containers with this definition, etc.\n\nhe chose the smallest for each just because this is for illustrative purposes\n\n\ncontainer definitions\n\nassigns which containers will be using this definition\nclick add container\n\ncontainer name\n\nwhatever you want, he chose hostname\n\nimage\n\ngoto services (top left) (open new tab) – left panel – ecr – left panel – repositories\n\nclick image repo name\ncopy image uri that you want to associate with the definition\ngo back to the task definitions tab\n\npaste uri into the box\n\nIf you want to always use the latest image and your uri has build version tag, replace the build tag with “latest”\n\nbuild tag starts at “build” and goes to the end of the uri\n\n\n\nauthentication only necessary if ecr repo is private\nsoft memory limit\n\nspecify a memory limit for the container\nleaving blank says only limit will be the memory size of the task definition (see 6.)\n\nport mappings\n\nwhatever is specified in dockerfile/image\nexample nginx image exposes 80 tcp\n\nAdvanced options\n\nhealthcheck\n\nsome cli code that allows you to check if your container is running properly at the container level\nhe already has this in his buildspec.yml code (see create CodeBuild project section)\n\nhealthcheckz is a check at the system level\n\n\nEnvironment, network settings, volumes\n\nseems like a bunch of stuff that would be used in a docker run command or in a docker-compose file\nall left blank\n\n\nclick add\n\n\nvolumes\n\nexternal volume to be shared\nleft blank\n\nclick create\n\nTakes you to launch status\n\nclick view task definition",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-utdtsdvbc",
    "href": "qmd/docker-aws.html#sec-docker-aws-utdtsdvbc",
    "title": "AWS",
    "section": "Update Task Definition to Share Data Volumes Between Containers",
    "text": "Update Task Definition to Share Data Volumes Between Containers\n\nFargate cluster example\n\nAlso see the Data Volumes, Sharing Data between containers, Docker-Compose sections of part 1 of this note\n\nthink a lot of what happens in those sections is automated by using the this task definition\n\nSearch ECS – Left panel – Task Definitions\n\nclick on fargate task definition\n\nclick on latest revision of the definition\n\ndefinitions are versioned\nclick on create new revision\n\nscroll down to click on add container\n\ncontainer name\n\nwhatever, fargate-storage (he called his hostname-v2)\n\nimage\n\nadd image uri (see creating task definition above)\nhe used the same image as the first container. This becomes a problem because he has two containers using the same port since both nginx containers are using 80. See troubleshooting section below. Also mentioned in part 1 – running containers – flags – p\nThink for data science we’d use a postgressql, redis, etc. image\n\nEnvironment\n\nThink this was for display purposes. He added one just so when he went to the webpage and it displayed the container names, we could tell the difference. The first container said version 1 and this one says version two.\nenvironment variables\n\nkey\n\nexample VERSION\n\nvalue\n\nexample versionTwo\n\n\n\nclick add\n\nVolumes\n\nclick add volume\n\nname\n\nwhatever\nexample shared-volume\n\nclick add\n\n\nGO BACK to container section\n\nDo this for each container: click on the container name\n\nStorage and Logging\n\nmount points\n\nsource volume\n\nclick dropdown and select volume name\n\nexample from above: shared-volume\n\n\ncontainer path\n\nhe added the shared folder path and it was the same for both containers\nsee part 1 Data Volumes and Sharing Data between containers sections\n\nI think using that example, we’d specify “/app/public/” (no quotes) for the app container. *** This dude said to add a trailing “/” to the paths ***\nfor storage container, example redis, it’d be “/data/” which is designated by the redis image authors.\n\n\n\n\nclick update\n\n\nClick create\n\nNote the revision number that’s given\n\n\n\nleft panel – Clusters\n\nclick on fargate cluster\n\nclick services tab (mid left)\n\nclick service name (example is hostname) using the task definition\n\nThis was created in the Add ECS service and task section below\nclick update button (top right)\n\nConfigure Service\n\nTask Definition\n\nrevision\n\nselect revision number of the updated definition\n\n\nclick next\n\nclick next all the way to review\n\nclick update service\n\nclick view service\n\n\n\nclick tasks tab (mid left)\n\nrefresh (mid right) and watch new task start up with “provisioning” status and then “running”\n\n\n\ncan go to ip address with volume path appended to the address to see that the volumes are up and running\n\nnot sure if this would work with a database example or not",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ratwaad",
    "href": "qmd/docker-aws.html#sec-docker-aws-ratwaad",
    "title": "AWS",
    "section": "Running a Task with an Available Definition",
    "text": "Running a Task with an Available Definition\n\nleft panel – Clusters\n\nClick Cluster your using for the task definition\n\nhe used the fargate one he created\n\nclick tasks tab (mid left)\nclick run new task\n\ntick fargate launch type\ncluster vpc, subnets\n\nclick dropdown boxes\nit’ll show the ones that were made during cluster creation\n\nchoose vpc and both subnets\n\n\nSecurity group\n\ncreates one for you with default rules which you can keep\n\nAlso can manipulate after created by going to EC2 – left panel – Network and Security – Security Groups\n\nOr click edit button to specify ports, choose existing security group, etc\n\nadd additional port\n\ntype\n\nselect custom with tcp protocol\n\nport range\n\n81-90\ncontainer must be configured to be able to listen on the range of ports\n\nSource\n\ncan choose a group that allows you to connect with other tasks in the environment and limit access\nhe kept Anywhere\n\nclick save\n\n\n\nclick run task (bottom right)\n\nclick the task hash under Task column\n\nat the bottom, you can watch the status turn from “pending” to “running”\n\nrefresh button (right)\n\ncopy the public ip under Network section\n\npaste into address bar + port\n\nexample 3.15.13.43:80\nexample: for his nginx server, it just displayed the private ip and the image name\n\n\nSimple way for a minor scale up the access to the application is to duplicate the task definition (also see autoscaling section below)\n\nleft panel – Clusters\n\nselect the same cluster\nclick tasks tab again\nclick task hash id again\n\nclick “run more like this” (top right)\n\ntick fargate again\nselect the same vpc and subnets again\nclick run task\n\nget the public ip the same way as before\nnow two ips are available for the app",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-calb",
    "href": "qmd/docker-aws.html#sec-docker-aws-calb",
    "title": "AWS",
    "section": "Create Application Load Balancer (ALB)",
    "text": "Create Application Load Balancer (ALB)\n\nAlso see AWS &gt;&gt; EC2 &gt;&gt; Configure Load Balancer and Application Ports\nsearch ec2\nleft panel – load balancing — load balancers\n\nclick create load balancer (top left)\n\nConfigure Load Balancer\n\nselect type\n\napplication, network or classic\n\napplication is for http, https\n\nguess this is for internet traffic coming into (and out of?) application\n\nnetwork is for tcp, tls, udp\n\nguess this would be for communication between containers\n\nclassic is for http, https, and tcp\n\nsomething about an app running on an ec2 classic network\n\nhe chose application\n\n\ngive it a name\n\nexample ecs-alb\n\nip address type\n\nipv4 (default)\n\nscheme\n\ninternal or internet facing\n\nkept internet-facing (default)\n\n\nListeners\n\nhttp, port 80\ncan add other ports if you want\n\nfor production should add a https, 80\n\n\nAvailability zones\n\nvpc, subnets\n\nselect those asscociated with the cluster\nsubnets have region specification (us-east-2a, b)\n\n\nclick next: configure security settings (bottom right)\n\nif you haven’t add https port, it’ll give you a warning\n\nclick next if don’t care about https\n\n\n\nConfigure Security Settings\n\ntick box that has the name of the security group that was created during the cluster creation\n\nExample: EC2ContainerServic-ecs-ec2-EcsSecurityGroup-somehash\n\ndescription: ECS Allowed Ports\n\n\nclick Next\n\nConfigure Routing\n\ntarget group (backend of load balancer)\n\nnew target group (default)\n\nName\n\n(literally) “default”\n\ntarget type\n\nInstance, IP, Lambda\nchose IP\n\nsomething about being able to use on EC2 and Fargate\n\n\nprotocol\n\nkept http\n\nport\n\nkept 80\n\nhealth check\n\nkept defaults\n\nclick next\n\nRegister targets\n\nkeep defaults\ngoing to specify this info through ecs in the next section\nclick next to review\n\nReview\n\nclick create\n\n\n\nEdit the default forwarding target\n\nA listener rule is comprised of a target (or group of targets) and conditions. When the load balancer receives a request, it checks it against the conditions in the listener rules. For whichever condition the request meets, the load balancer then sends the request to the target (e.g. ip address(instance) or lambda function (code scripts)) associated with that condition.\nSearch ec2 – left panel – load balancing – load balancer \n\ntick the load balancer you want\nclick listener tab (mid left)\nFor listener id = http 80 and under the Rules column it will say Default: forwarding to default\n\nclick view/edit rules\n\nunder the IF column (ie the condition) it says, Requests otherwise not routed. Which means any request that doesn’t meet any of the other conditions\nclick the edit pencil icon (top left) – click pencil icon next to http 80: default action\n\nUnder the THEN column – click the trash can to delete “forward to default”\nclick add action\nselect return fixed response\n\nkeep response code 503\n\nmeans server had an issue responding\n\nin Response body, type message\n\nexample: sorry, no one is home right now\nclick check mark\nclick update (top right)\n\n\n\nclick back arrow (top left)\n\n\ntest it out by clicking description tab (mid left)\n\nGet DNS (Domain Name Service) name\n\nExample blahblahamazonaws.com\nNormally, you take your domain name (www.store.com/app) and give it the Amazon Route 53 service which translates your domain name into an ip address.\nYou then reroute traffic from your domain ip address to this dns name.\n\npaste it in the browser and the error message displays",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-cesat",
    "href": "qmd/docker-aws.html#sec-docker-aws-cesat",
    "title": "AWS",
    "section": "Create ECS Service and Task",
    "text": "Create ECS Service and Task\n\nAlso see create task definition and run task sections above\nSteps\n\nsearch ecs – left panel – clusters – click fargate cluster\nclick tasks tab (mid left) – select task that was created in Create task section – click stop button (mid left)\nclick services tab (mid left) – click create service\n\nConfigure Service\n\nConfigure Service\n\nselect launch type\n\nchoose fargate\n\nTask Definiton and Cluster\n\nkept the ones created in sections above\n\nenter a service name\n\nexample hostname\n\nnumber of tasks\n\nexample 2\n\nclick next step\n\nDeployments\n\nkeep default, rolling update\n\nallows you to upgrade the task definition from version 1 to version 2 in a rolling fashion \n\ndon’t know what he’s talking about here with versions\n\n\n\nclick next step\n\nConfigure Network\n\nService\n\ncluster vpc, subnets\n\nselect the ones that are associated with this fargate cluster\n\n security group\n\nkeep default (allows traffic in)\n\nauto-assign public ip\n\nkeep default ENABLED\nwith a load balancer, we could choose to use only used private ips though\n\n\nHealth check grace period\n\nset to 60 (in seconds)\ngives the container/cluster a chance to get up an running before it tests it to see if everything is working\n\nLoad Balancing\n\ntick application load balancer\ncontainer to load balancer\n\nshows container name port:port\nclick add to load balancer\n\nproduction listener port\n\nclick dropdown – select 80 HTTP\n\n80 is our port of the container and we chose http when we created the load balancer\n\n\npath pattern\n\nit was /hostname but he changed it to /* which is every pattern\n\nI think /hostname would that “/hostname” would be ip address pattern associated with this container (i.e. the condition or rule)\nand /* means route any request from  no matter what pattern is attached to it.\n\n\nevaluation order\n\nas soon as the first rule/condition is matched, traffic goes to that target and no other rules are considered. Lower the evaluation order, the sooner the rule is considered\nhe chose 1\n\nhealth check path\n\ndefault was /hostname\nsince he’s using /*, he changed it to /hostname/ so the healthcheck will get a webpage (code 200) and not a redirect (code 300 error)\n\nService Discovery\n\nEnables Route 53 to create a local network dns address for your container.\nUseful for when you have multiple applications talking to each other\nuntick box for enable service discovery integation since this is only one application\n\n\n\n\nclick next step\n\nSet Autoscaling\n\nSee next section on adding autoscaling\nThis can be added after this service has been created by updating\nclick next step\n\nReview\n\nclick create service\n\nCreates target group, rule/condition\n\nclick view service\nshould see two tasks starting up\ngoto the dns address on a webpage (see end of create load balancer section)\n\nbefore it displayed the error message (default target), now it shows a webpage (like in the Create Task section above)\nrefresh and it shows the second dns address associated with having a second task\n\n2 tasks means it can handle more traffic (just like the end of the create task section above)",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-utstaa",
    "href": "qmd/docker-aws.html#sec-docker-aws-utstaa",
    "title": "AWS",
    "section": "Updating the Service to Add Autoscaling",
    "text": "Updating the Service to Add Autoscaling\n\nsearch ecs – left panel – clusters\nClick on your cluster that you want to update its service\n\nservices tab (mid left) – click service name or id\n\nclick update\n\nClick next until you get to Set Autoscaling\ntick configure service autoscaling\nminimum number of tasks\n\nhe chose 1\n\ndesired number of tasks\n\nhe chose 3\n\nmaximum number of tasks\n\nhe chose 5\n\nIAM role\n\nuse default ecsautoscalerole\nuse create new role if there isn’t already one available\n\nclick Add scaling policy\n\ntick step scaling\nenter policy name\n\nexample stepUp\n\nexecute policy when\n\ntick create new alarm\nalarm name\n\nexample upAlarm\n\nECS service metric\n\nCPU Utilization\n\nAlarm threshold\n\navg cpu utilization &gt; 10 (%)\nconsecutive period = 1\nperiod = 8 min\n\nhe chose 1 just for illustrative purposes\n\nclick save\n\n\nscaling action\n\nadd 1 task\nwhen cpu utilization &gt; 10\n\ncountdown period\n\namount of time it takes to make a decision\n30 sec\n\nclick save\n\nclick Add scaling policy (again)\n\nsame thing but for scaling down\navg cpu utilization\n\nhe chose &lt;= 10 but I’m not sure if that’s what you’d do in real life. I’d think you’d want some separation between the up and down scaling, but maybe not\n\nscaling action\n\nremove 1 task\n\n\nClick next to review\nclick update service\n\n\nClick tasks tab (mid left) to see how many task are currently running\nclick autoscaling tab to see the both upAlarm and downAlarm condition info\nTo see status of the targets (ip addresses of instances/containers)\n\nGoto EC2 – left panel – load balancing – target groups\n\ntick the target group name of the load balancer\nUnder Registered Targets (Bottom)\n\nshows ips, status\n\nstatus == draining when auto-scaling taking a resource offline",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-troub",
    "href": "qmd/docker-aws.html#sec-docker-aws-troub",
    "title": "AWS",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nNotes:\n\nnormal for an active cluster without any running services or tasks to have 1 active container instance. It’s called the container management instance.\n\nExample: you notice your cluster is running 5 containers when you only desire 3\n\nCan see this in Clusters – fargate – services tab, under the desired tasks and running tasks columns\n\nits says 5 for desired but he chose that for his max in the autoscaling section, so I don’t know if he adjusted it for demonstration purposes or if this something confusing that AWS does.\n\nAnswer: he had both containers trying to bind to port 80 (see logs below)\nService level\n\nclick service name\n\ntasks tab\n\ncan see the task ids and definitions that the various active tasks are using\n\nexample: tasks are alternating between running and provisioning. Why are some shutting down and others starting in their place? The container is running for some time and then being stopped for some reason.\n\nclick task id\n\nlogs tab – select container\n\nshows errors that have occurred\n\nDetails tab\n\nContainers (bottom)\n\nclick expand-arrow on desired container\n\nclick view logs in CloudWatch\n\ntakes you to CloudWatch console\n\nview the logs of the task\n\nable to filter log by events\n\ngo up one level to see log streams\n\ncan match containers and tasks to see if it might be a task issue\nend hash is the task id\n\n\n\n\n\n\n\n\nDetails tab\n\nLoad Balancing – click target group name\n\ntargets tab\n\nshows the individual targets (ip addresses), ports, statuses\nstatus by region (if you have resources in different zones)\n\nexample: there were 2 zones -  us.east.2a and 2b and one had all healthy and the other had zero healthy, but he didn’t mention anything about it. Think that’s just how nodes are taken on and offline and not that there’s a regional issue.\n\n\nhealth checks tab\n\nhealthy threshold\n\nnumber of code 200s i.e. healthy responses required in order for a node to be considered healthy\n\nunhealthy threshold\n\nnumber of code 300s i.e. error responses required in order for the node to be considered unhealthy\n\n\n\n\nLogs tab\n\nshows the aggregate of the logs for each container (all tasks included)\nselect a container from the dropdown\n\ntimestamp, message, task id\nexample: shows a “bind” error that says the container can’t a bind to port 80 because it’s already in use.  In the Update task definition to share data volumes section, the second container he added was a duplicate of the nginx container and both were trying to bind to port 80. Hence the error\n\n\n\n\nCluster metrics\n\nClusters – EC2 cluster – metrics tab\n\nonly useful for EC2 clusters\ncompute and memory resources being used\n\nshows time series of min, max, and average percent usage\n\n\nClusters – fargate cluster – services tab\n\nclick service name – metrics tab\n\nsame stuff as EC2 metrics tab\ncan click on the different metrics (cpu, memory utilization) and create custom metric functions, change period length, etc.\n\nleft panel has alarms (autoscaling trigger history), events, logs, and settings",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html",
    "href": "qmd/geospatial-processing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-misc",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-misc",
    "title": "Preprocessing",
    "section": "",
    "text": "Beware statistical computations of tibbles/sf_tibbles with geometry columns\n\nCould result in an expensive union operation over identical geometries and an R session crash\n\nExample with 100K rows crashed R.\n\nNotes from thread\nOption 1 (slower): Set do_union = FALSE in summarize\ntx_income_groups &lt;- \n  get_acs(\n    geography = \"tract\",\n    table = \"B19001\",\n    state = \"TX\",\n    year = 2020,\n    geometry = TRUE\n  ) |&gt; \n  filter(variable != \"B19001_001\") |&gt; \n  mutate(bracket = case_when(\n    variable &gt; \"B19001_012\" ~ \"Above $100k\",\n    TRUE ~ \"Below $100k\"\n  )) |&gt; \n  group_by(GEOID, bracket) |&gt; \n  summarize(n_households = sum(estimate, na.rm = TRUE),\n            do_union = FALSE)\nOption 2 (faster): Perform calculation without geometries then join\ntx_tracts &lt;- tracts(\"TX\", cb = TRUE, year = 2020) |&gt; \n  select(GEOID)\n\ntx_income_groups &lt;- \n  get_acs(\n    geography = \"tract\",\n    table = \"B19001\",\n    state = \"TX\",\n    year = 2020,\n    geometry = TRUE\n  ) |&gt; \n  filter(variable != \"B19001_001\") |&gt; \n  mutate(bracket = case_when(\n    variable &gt; \"B19001_012\" ~ \"Above $100k\",\n    TRUE ~ \"Below $100k\"\n  )) |&gt; \n  group_by(GEOID, bracket) |&gt; \n  summarize(n_households = sum(estimate, na.rm = TRUE))\n\ntx_income_groups &lt;- tx_tracts |&gt; \n  left_join(tx_income_groups, by = \"GEOID\")\n\n{tidycensus} has an arg to bypass d/ling the geometries, geometry = FALSE and a separate tracts function to get the census tract geometries",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-filtyp",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-filtyp",
    "title": "Preprocessing",
    "section": "File Types",
    "text": "File Types\n\nPMTiles - A single-file archive format for tiled data. A PMTiles archive can be hosted on a commodity storage platform such as S3, and enables low-cost, zero-maintenance map applications that are “serverless” - free of a custom tile backend or third party provider. (Docs)\n\nRun your interactive, smooth-zooming vector map from any storage like S3 that supports http requests; a Caddy server running on your Wi-Fi router, or even GitHub pages (if tiles &lt; 1GB).\nCloudflare R2 is the recommended storage platform for PMTiles because it does not have bandwidth fees, only per-request fees: see R2 Pricing.\n\nShape Files\n\nD/L and Load a shapefile\nMay need API key from Census Bureau (see {tigris} docs)\nExample: Counties in California\ntbl &lt;- tigris::counties(state = \"CA\") %&gt;%\n    st_set_crs(4326)\n{tigris} - US data\nlibrary(tigris)\n\nus_states &lt;- states(resolution = \"20m\", year = 2022, cb = TRUE)\n\nlower_48 &lt;- us_states %&gt;%\n  filter(!(NAME %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")))\n{rnaturalearth} - World data\n# Via URL\n# Medium scale data, 1:50m Admin 0 - Countries\n# Download from https://www.naturalearthdata.com/downloads/50m-cultural-vectors/\nworld_map &lt;- read_sf(\"ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp\") %&gt;%\n  filter(iso_a3 != \"ATA\")  # Remove Antarctica\n\n# Via Package\nlibrary(rnaturalearth)\n\n# rerturnclass = \"sf\" makes it so the resulting dataframe has the special\n# sf-enabled geometry column\nworld_map &lt;- ne_countries(scale = 50, returnclass = \"sf\") %&gt;%\n  filter(iso_a3 != \"ATA\")  # Remove Antarctica\n\nGeoJSON\n\nWrite data to geojson\ndata %&gt;%\n    st_write(\"mb_shapes.geojson\")",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-proj",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-proj",
    "title": "Preprocessing",
    "section": "Projections",
    "text": "Projections\n\nWGS 84\n\nGoogle “epsg code” + “your region name” to find a reasonable projection code to use\n\nStandard projection is 4326 aka WGS84 (required by leaflet)\nTransform shapefile\nmb_shapes &lt;- read_sf(download_folder)\nmb_shapes %&gt;%\n  st_transform(4326)\n\n\nTransform latitude and longitude then visualize\nnew_tbl &lt;- old_tbl # contains latitude and longitude variables\n    # convert to simple features object\n    sf::st_as_sf(\n        coords = c(\"&lt;longitude_var&gt;\", \"&lt;latitude_var&gt;\"), # order matters\n        crs = 4326 # standard crs\n    ) %&gt;%\n    mapviw::mapview()\nWGS 84 projection, which is what Google Maps (and all GPS systems) use\nus_states &lt;- us_states %&gt;% # df with geometries\n  sf::st_transform(st_crs(\"EPSG:4326\"))  # WGS 84\nNAD83, Albers, Mercator, Robinson\n\nlibrary(patchwork)\n\np1 &lt;- ggplot() +\n  geom_sf(data = lower_48, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"EPSG:4269\")) +  # NAD83\n  labs(title = \"NAD83 projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np2 &lt;- ggplot() +\n  geom_sf(data = lower_48, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  labs(title = \"Albers projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np3 &lt;- ggplot() +\n  geom_sf(data = world_map, fill = \"#FF4136\", color = \"white\", linewidth = 0.1) +\n  coord_sf(crs = st_crs(\"EPSG:3395\")) +  # Mercator\n  labs(title = \"Mercator projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np4 &lt;- ggplot() +\n  geom_sf(data = world_map, fill = \"#FF4136\", color = \"white\", linewidth = 0.1) +\n  coord_sf(crs = st_crs(\"ESRI:54030\")) +  # Robinson\n  labs(title = \"Robinson projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\n(p1 | p2) / (p3 | p4)",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-py",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-py",
    "title": "Preprocessing",
    "section": "Python",
    "text": "Python\n\nExample: Filter Data based on a polygon using latitude and longitude data\n\nGet California’s polygon\nimport osmnx\nimport geopandas as gpd\n\nplace = \"California, USA\"\ngdf = osmnx.geocode_to_gdf(place)\n# Get the target geometry\ngdf = gdf[[\"geometry\", \"bbox_north\", \"bbox_south\", \"bbox_east\", \"bbox_west\"]]\nFilter data according the polygon geometry\nfrom shapely.geometry import Point\n\n# Convert to a GeoDataFrame with Point geometry\ngeometry = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])]\nearthquake_gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:4326')\n\n# Filter to keep only points within the California bounding box\npoints_within_california = gpd.sjoin(earthquake_gdf, gdf, how='inner', predicate='within')\n\n# Select latitude, longitude etc. columns\ndf = points_within_california[['id', 'Latitude', 'Longitude', 'datetime', 'properties.mag']]\n\nLatitude and longitude are converted to point geometry to match the polygon point geometry\nAn inner join is used on the data and california polygon to get the points that are only in California.",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html",
    "href": "qmd/job-management-leadership.html",
    "title": "29  Management / Leadership",
    "section": "",
    "text": "29.1 Misc",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#misc",
    "href": "qmd/job-management-leadership.html#misc",
    "title": "29  Management / Leadership",
    "section": "",
    "text": "Try really hard not to send messages outside of work hours\nEmphasize unplugging during vacations\nProvide immediate feedback - positive and negative\nDedicate time to freeform exploration\n\nSometimes the rest of the business doesn’t know what to ask of your data org. That’s why you need to give your team time to explore.\nTeam members can come to new and exciting conclusions when they’re given time to explore the data for fun. They can apply their talents to looking for patterns that no one has requested, and have the space to uncover new discoveries. This freeform exploration can lead to game-changing innovations that no business stakeholder would have imagined were possible.\nHelps keep your most valuable team members engaged and satisfied in their work.\n\nWhen first starting, request documentation\n\nRelevant server locations & descriptions\nLocations of our documentation and dashboards\nA list of tools/software that are available to be used\nA list of relevant stakeholders/gatekeepers that I’d need to make contact with\n\nRemote Teams\n\nVideo calls too easily become transactional and with little time for the chitchat that builds a proper human relationship. Without those deeper bonds, misunderstandings fester into serious relationship difficulties, and teams can get tangled in situations that would be effectively resolved if everyone were able to talk in person.\nSome organizations may balk at the costs of travel and accommodation for a team assembly like this, but they should think of it as an investment in the team’s effectiveness. Neglecting these face-to-faces leads to teams getting stuck, heading off in the wrong direction, plagued with conflict, and people losing motivation. Compared to this, saving on airplanes and hotels is a false economy.\nFrequency\n\nGet together for a week every two or three months\nAfter the team has become seasoned they may then decide to reduce the frequency, but I would worry if a team isn’t having at least two face-to-face meetings a year.\nIf a team is all in the same city, but using a remote-first style to reduce commuting, then they can organize shorter gatherings, and do them more frequently.\n\nSchedule\n\nSet a full day of work, focusing on those tasks that benefit from the low-latency communication that comes from being together. tasks that require lots of input from many people with rapid feedback\nWe should then include what feels like too much time for breaks, informal chatter, and opportunities to step outside the office.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#terms",
    "href": "qmd/job-management-leadership.html#terms",
    "title": "29  Management / Leadership",
    "section": "29.2 Terms",
    "text": "29.2 Terms\n\nFocus Time - uninterrupted time, usually refers to a period of time (e.g. 2 hrs) where people can work without any distractions\nReport (aka Individual Contributor (IC))- People who report to the manager",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#getting-the-promotion",
    "href": "qmd/job-management-leadership.html#getting-the-promotion",
    "title": "29  Management / Leadership",
    "section": "29.3 Getting the Promotion",
    "text": "29.3 Getting the Promotion\n\nYou don’t get a promotion and THEN start to perform at the next level; you perform at the next level IN ORDER TO get a promotion.\n\nSo when you notice a gap somewhere, even if it doesn’t necessarily fall into your current role description, don’t be afraid to bring it up to your manager and discuss whether you can/should take initiative to help plug the gap.\nThe best way to notice gaps is to be a good listener and constantly communicate with your partners & stakeholders about their teams’ work and pain points.\n\nMentor a peer\n\nIf you have new members joining the team, offer to be an onboarding buddy to guide them through their first few weeks.\nbrainstorm with team members when then need help\n\nStep out of your immediate scope\nGet involved in team-level activities\n\nHelp out with things such as sprint planning, quarterly planning, etc.\n\nAllows you to gain knowledge about other team members’ work and other teams’ requests for your team\nGives you some exposure to the manager’s plan and vision for the team\n\nVolunteering for culture initiatives is a great way to practice thinking about the team as a whole\nTake on projects that help the whole team\n\nproduct design doc for the data product\nSLA agreement with partner teams (?)\nCodify the best practices you use in your own work\n\n\nHave open, timely feedback conversations with your manager\n\nAsk for the leveling guide when you have the initial career development conversation with your manager.\n\nAnd make sure you mention your aspiration to be a manager as soon as possible (don’t be shy) as well as your aspired timeline that you are working towards.\n\nAsk your manager for candid feedback with regards to their assessment of your readiness to become a manager, and any gaps that they think you need to address.\nIn followup career development check-ins, ask your manager to provide feedback for you against the leveling guide.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#managerial-hats",
    "href": "qmd/job-management-leadership.html#managerial-hats",
    "title": "29  Management / Leadership",
    "section": "29.4 Managerial Hats",
    "text": "29.4 Managerial Hats\n\nPeople manager - Learn what makes your direct reports (aka people that you manage) tick, identify their career aspirations, and point out opportunities for progress.\nResource manager - Determine what resources are needed and acquire them. Mostly this means recruiting, hiring, and onboarding, but it also means advocating for money for training and team activities.\nProject manager - Collect and triage projects and project requirements, set timetables and schedules, assigned tasks, and have the final say about when work was “done”.\nCommunications manager - Make sure the team’s work was being shared with the rest of the organization, and that everyone on the team knew what was going on outside.\nProcess manager - Help design the team’s processes to make sure we could identify, allot, do, and communicate work across the team.\nTechnical mentor and coach - A technical expert who reviews code, answers technical questions, and gives work feedback to my team.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#preparation-for-a-managerial-role",
    "href": "qmd/job-management-leadership.html#preparation-for-a-managerial-role",
    "title": "29  Management / Leadership",
    "section": "29.5 Preparation for a Managerial Role",
    "text": "29.5 Preparation for a Managerial Role\n\nTake notes on the time needed to do difficult tasks, easy quick-wins, common roadblocks, and their solutions.\n\nThis will help estimate deadlines for new projects\n\nPractice verbal and written communication\nGather information on “big picture” strategy of your company and that applies to data projects\nTake notes of every data team member’s strengths and weaknesses\nListen to your colleagues.\n\nComplaints on a day-to-day basis.\nPraise about the workplace in general.\nPay attention to pet projects of your teammates: these are the areas they actively pursue outside their usual work.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#meetings",
    "href": "qmd/job-management-leadership.html#meetings",
    "title": "29  Management / Leadership",
    "section": "29.6 Meetings",
    "text": "29.6 Meetings\n\nMeetings are bad when they:\n\nResult in calendar fragmentation.\n\nTry to schedule sometime after a Focus Time\nLimit size and number of meetings\n\n1-1s (1 on 1), team-wide update, or decision-making meetings\nLarge (&gt; 4 ppl) brainstorming meetings don’t work\n\nBetter to circulate a memo of come-up with options then debate those options during a meeting\n\n\n\nFeel useless to attendees\n\nKeep focus on the meetings agenda\n\nGroup meetings (manager’s agenda)\n1-1s (1 on 1) (report’s agenda)",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#tools-for-servant-leadership",
    "href": "qmd/job-management-leadership.html#tools-for-servant-leadership",
    "title": "29  Management / Leadership",
    "section": "29.7 Tools for Servant Leadership",
    "text": "29.7 Tools for Servant Leadership\n\nTeaching - As a leader you often have more context and more experience than your team members.\n\nTeach the team which situations different models work in, how those models are perceived in your organization and the red-flags to watch out for during development.\n\nReflecting - Make time to think back to events within your team.\n\nWhat caused success? What led to failure? Are we setting expectations appropriately for our models and analyses?\n\nDebate - Encourage debate.\n\nThe team is trying to use data to understand the world, and as in any form of science, there will be competing hypotheses\nTake advantage of the diversity (all forms) within our teams to minimize the impact of those personal biases\n\nProcess - Leaders will have to deal with ambiguity, but for the wider team we need to ensure there are steps to follow that support consistency across the team and alignment on the team’s over-arching goals.\nFeedback - To maintain team members’ morale, the balance between negative and positive feedback has to tilt heavily towards the positive\n\nIf you can’t find that balance, then you need to consider whether the team member should continue on your team. If you want them to remain, then you must figure out how to articulate their positives back to them, otherwise you can expect them to leave.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#nonviolent-communication-nvc",
    "href": "qmd/job-management-leadership.html#nonviolent-communication-nvc",
    "title": "29  Management / Leadership",
    "section": "29.8 Nonviolent Communication (NVC)",
    "text": "29.8 Nonviolent Communication (NVC)\n\nMisc\n\nNotes from How to deliver constructive feedback in difficult situations\nOther methods\n\nSBI (Situation-Behavior-Impact) Useful for giving better feedback by removing emotions from it and making it clear and concise\n\nSteps\n\nSituation - Start the feedback with a specific situation that occurred which serves as a common reference point and is specific.\nBehavior - Refer to a specific behavior that you observed and want to talk about. Make sure to not give any judgments and leave the interpretation out of it.\nImpact - Talk about the impact that behavior had and what you think and feel about it. Feel free to address what other people think and how it impacted things.\nIntent - Ask about the person’s intention and try to uncover if the person is aware of what he did and why he did it. Then, work together with the person to see how things can be made better and how to overcome issues.\n\n\n\n\nAt the core of NVC is a straightforward communication pattern:\n\n“When ____[observation], I feel ____[emotion] because I’m needing some ____[universal needs]. Would you be able to ____[request]?”\nExamples\n\nTo a co-founder: “When you said, ‘I’m not happy with your work,’ to me in front of the team, I felt embarrassed because it didn’t meet my need for trust and recognition. Please, could we set up a weekly one-on-one session to share feedback in private?”\nTo an investor: “I haven’t received any responses from the last three monthly updates. I’m feeling concerned because I need input. Please, would you mind getting back to me with responses to my questions in the last update?”\nTo a teammate: “You arrived 10 minutes late to the last three team meetings. I am frustrated because, as a team, we have a need for efficiency. Please, could you help me understand what’s happening?”\n\n\nObservations (vs evaluations)\n\nExamples\n\nEvaluation: “You are lazy” (which is a character attack). Observation: “You said that you’d send the document last week, and I haven’t received it.”\nEvaluation: “Your work is sloppy” (which is a criticism). Observation: “Three of the numbers in the report were inaccurate.”\nEvaluation: “You’re always late,” (which is a generalization). Observation: “You arrived 10 minutes late to the meeting this morning.”\nEvaluation: “You ignored me.” (which implies intent). Observation: “I sent you two emails, and I haven’t received a response.”\n\nCheck\n\nask yourself, “What did I actually see or hear?”\n\n\nEmotions (vs thoughts, vs evaluations)\n\nUsing an evaluation or thought instead of an emotion, can result in a defensive reply\nExamples\n\nEmotion: “I feel frustrated.” Thought: “I feel that you aren’t taking this seriously.”\nEvaluation: “I feel judged.” Impact: “I feel resentful.”\n\ndefensive reply: “I didn’t judge you.”\n\nEvaluation: “I feel misunderstood.” Impact: “I feel frustrated.”\nEvaluation: “I feel rejected.” Impact: “I feel hurt.”\n\nCheck\n\nFor thoughs, if you can substitute “I feel” with “I think” and the phrase still works — because it’s a thought, not an emotion.\n\n\nUniversal Need (vs strategy for obtaining a need)\n\nExamples\n\nStrategy: “I need you to copy me into every email.” Universal Need: “I need some transparency.”\nUniversal: ““I need support.” NOT Universal: “I need support from you.”\n\nNOT Universal is more easily interpreted as a veiled accusation and implication that “You aren’t supporting me.”\n\n\n\nRequests (vs demands)\n\nrequests are invitations for another person to meet our needs — but only if it doesn’t conflict with one of their needs.\nCharacteristics of a good request\n\nMake them specific\n\n“I request that you arrive to meetings on time.” instead of “I request that you be more respectful of everyone’s time.”\n\nSay what you want, not what you don’t want\n\nDon’t want: “I request that you don’t dismiss other people’s ideas straightaway”\nWant: “I request that when a team member shares an idea, you ask two or three probing questions before sharing your conclusion.”\n\nStay curious\n\nBe optimistic that everyone’s needs can be met.\nTreat “no” to a request or a defensive reply as an invitation to explore the needs stopping someone from saying “yes.”\nThink about how the other person is feeling and consider what unmet needs may be stopping them from saying “yes.”\n\nAre you feeling hurt because you need some understanding?\nAre you feeling angry because you need your hard work to be recognized?\nIs there more you’d like to say?\n\nSimilarly, if you’re on the receiving end of a request and have to say “no,” state the underlying need that stops you from saying “yes.”\n\n\n\nDiplomatically confirm communication if needed\n\n“Just so we know we’re on the same page, could you play back what I’m asking of you?”\n\n40-word rule\n\nDuring difficult conversations, it’s important to be extremely concise. Aim to describe your observations, feelings, needs, and requests in fewer than 40 words. Using more words suggests you’re justifying your needs, and that decreases their power.\n\nFace-to-Face is better\n\nNVC loses some of its power when it’s in an email.\n\nConsequences should be protective, not punitive\n\nAs a manager, you are responsible for the effectiveness of your team — and every team needs effectiveness. If deadlines continue to be missed (the boundary), you might have to switch their responsibilities or move them on (the consequence). It’s not personal, it’s just what you’ll do to protect your need for effectiveness.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#promoting-your-teams-projects",
    "href": "qmd/job-management-leadership.html#promoting-your-teams-projects",
    "title": "29  Management / Leadership",
    "section": "29.9 Promoting Your Team’s Projects",
    "text": "29.9 Promoting Your Team’s Projects\n\nAnnouncement emails\n\nUnlike “sharing” emails (brief description, link), announcement emails have more pomp associated with them\nCharacteristics\n\nUse catchy subject lines — e.g. ‘Retention Dashboard is here!’ or ‘Introducing Retention Dashboard’\nIn addition to stating what the dashboard contains, tie it to key insights, recommendations and next steps\nUse icons & visuals — Adding relevant icons and visuals makes the email easier to consume and provides a nice break from all the heavy text. Caution: Do not overuse!\n\nExample\n\n\nReadouts\n\nAn analysis that is packaged in a way that is easy to read through\nTypically a one-time analysis\n\n(deep dive) e.g. what drives customer retention\n(root cause analysis) e.g. why did top of funnel conversion decline or analyzing an experiment / launch / campaign performance\n\nAlso reoccurring\n\nCould be weekly or monthy, depending on topics important to your stakeholders\nActively sharing summarized findings from dashboards to the stakeholders can change perception that these dashboards are just another source of data\n\nMonthly or Quarterly Business Reviews\n\nPresentations where you review health of business based on trends in key metrics (month over month, quarter over quarter)\n\nAutomate frequent requests from Marketing managers, Product managers, and Operations managers\n\nProduce a readout that covers insights from multiple dashboards that managers are frequently asking about.\n\n\nNewletter\n\nHighlight goals for ongoing work-streams and outcomes for those completed, always connecting to business outcomes or stakeholder needs\nSample Layout\n\nSummary — Key Wins & What’s Coming\nDetailed updates by themes\nNewsletter FAQ’s —\n\nGoals: What is the goal of this newsletter. e.g. Providing visibility and aligning on prioritization\nCadence: Weekly / Bi-weekly / Monthly\nAudience: Sr. Leadership of company\nTeam members\nPOC: Who should they reach out to if they have questions\n\n\nExample",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html",
    "href": "qmd/production-tools.html",
    "title": "38  Tools",
    "section": "",
    "text": "38.1 Misc",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-misc",
    "href": "qmd/production-tools.html#sec-prod-tools-misc",
    "title": "38  Tools",
    "section": "",
    "text": "Overview of some 2021 tools Descriptions in article \nAWS Batch - Managed service for computational jobs. Alternative to having to maintain a kubernetes cluster\n\nTakes care of keeping a queue of jobs, spinning up EC2 instances, running code and shutting down the instances.\nScales up and down depending on how many jobs submitted.\nAllows you to execute your code in a scalable fashion and to request custom resources for compute-intensive jobs (e.g., instances with many CPUs and large memory) without requiring us to maintain a cluster\nSee bkmks: Hosting &gt;&gt; AWS &gt;&gt; Batch\nPackages:\n\n{crew.aws.batch}",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-stckcomprnk",
    "href": "qmd/production-tools.html#sec-prod-tools-stckcomprnk",
    "title": "38  Tools",
    "section": "38.2 Stack Component Rankings",
    "text": "38.2 Stack Component Rankings\n\nDB format\n\narrow files\n\nELT Operations\n\n*dbt\n\nGoogle’s alternative is Dataform\nAWS’s alternative is Databrew\n\n*Spark\n*Google Big Query SQL\n*AWS Athena\n\nOrchestration and monitoring\n\n*Targets\n\n+ {cronR} for orchestration + scheduling\n\n*Mage-AI\n*AWS Glue\nPrefect\nAirflow\n\nData Ingestion\n\nAirbyte (data ingestion)\nfivetran (data ingestion)\n\nCan “process atomic REST APIs to extract data out of SAAS silos and onto your warehouse”\n\nterraform (multi-cloud management)\n\nTracking/Versioning for Model Building\n\n*DVC\nMLFlow\n\nReporting\n\nblastula (email), xaringan (presentation), RMarkdown (reports), flexdashboard (dashboards),\nRStudio Connect (publishing platform to stakeholders)\n\ndashboards, apps\non-demand and scheduled reports\npresentations\nAPIs (?)\nPublish R and Python\nEnterprise security\nCan stay in RStudio\n\n\nVisualization Platforms\n\nLooker*\nPowerBI, DataStudio",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-depman",
    "href": "qmd/production-tools.html#sec-prod-tools-depman",
    "title": "38  Tools",
    "section": "38.3 Dependency Management",
    "text": "38.3 Dependency Management\n\nR\n\nr2u for linux installations\n\n“for Ubuntu 20.04 and 22.04 it provides _all_ of CRAN (and portion of BioConductor) as binary #Rstats packages with full, complete and automatic resolution of all dependencies for full system integration. If you use `bspm` along with it you can even use this via `install.packages()` and friends. Everything comes from a well connected mirror”",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#data-versioning",
    "href": "qmd/production-tools.html#data-versioning",
    "title": "38  Tools",
    "section": "38.4 Data versioning",
    "text": "38.4 Data versioning\n\nFlat Table by Github\n\nHas a Github action associated with it\nHas a datetime commit message\nLists as a feature that it tracks differences from one commit to the next, but doesn’t a normal data commit doe the same thing?\n\nLumberjack R package\n\nAdd functions to your processing script\ntracks using a log file\noptions for changes you want to track",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-dating",
    "href": "qmd/production-tools.html#sec-prod-tools-dating",
    "title": "38  Tools",
    "section": "38.5 Data Ingestion",
    "text": "38.5 Data Ingestion\n\nFiveTran\n\nFree-tier\nSync raw data sources\n\nevery 1hr for starter plan, every 15 minutes both standard plans, every 5 min for enterprise plan",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-orch",
    "href": "qmd/production-tools.html#sec-prod-tools-orch",
    "title": "38  Tools",
    "section": "38.6 Orchestration",
    "text": "38.6 Orchestration\n\n38.6.1 Airflow\n\nWebpage\nOpen-source platform for authoring, scheduling, and executing data pipelines.\n\nFeatures for managing and monitoring data pipelines, including integration with various data storage and processing technologies. Similar to the Unix cron utility — you write scripts and schedule them to run every X minutes.\nAirflow can be used for any sort of scheduling task, but is often used for scheduling data modeling. schedule, run and monitor the refresh of our data warehouse\nMonitoring on-prem checking Airflow logs is not user-friendly (better in AWS MWAA)\n\ndifferent types of logs for task, web server, scheduler, worker, and DAGs\nhave to SSH into the server and run commands which becomes more complicated when you want to use distributed servers for scalability.\n\nRequires you to create a central logging storage and make additional setup to make all servers write logs into that single place\n\n\nServer-based remains active even when not running jobs –&gt; continually incurring cost\n\nNo latency since servers are always running\n\nProblems\nLong feedback loop\n\nWhile programming, instant feedback of your DAG becomes crucial when you want a sanity check before your code goes too far.\nTo see the graph view, which is mainly for visualizing dependencies in DAGs, your code needs to be in the folder of an Airflow scheduler that can be picked up. The airflow scheduler also takes time to render and parse your DAG until it shows up.\nMakes debugging difficult during the development cycle, so some engineers write more lines of code and test them all together. If the lines of code become unmanageable on one screen, you might vaguely remember what to validate and what dependencies to check.\n\nDifficult with local development\n\na docker image can be used to inject as much production-related information as possible. But it’s still not 100% copy, and it takes tremendous effort to develop and maintain that docker image.\nEven if you set up dev, staging, and production environments for running Airflow, they aren’t totally isolated and developers can end-up interfering with one another. Services/Extensions Astronomer offers a managed Airflow service.\nAmazon Managed Workflows for Apache Airflow (MWAA) - managed Airflow service\n\nOrchestrate jobs in EMR, Athena, S3, or Redshift\n\nGlue\n\nAirflow has the glue operator\n\nCloudFormation can be used to configure and manage\n\nallows for autoscaling which saves on costs by scaling down when usage is low\nstill needs a server running even when not running jobs\nmonitoring much easier since all the logs are written into CloudWatch search certain logs using Logs Insights\n\nhave a dashboard that displays usage of server resources like CPU, memory, and network traffic.\nmonitor numerous other Airflow-specific metrics.\nset up alerts and manage notification recipients programmatically.\n\n\nCost factors\n\nInstance size\nAdditional worker instance\nAdditional scheduler instance\nMeta database storage\n\nPotential Issues:\n\nResources are shared on multiple jobs so performance can suffer if:\n\nDon’t distribute trigger times evenly\nMisconfigure your maximum worker count\n\n\nOperate through AWS SDK\n\nCan\n\ncreate, update, and delete MWAA environments and retrieve their environment information that includes logging policies, number of workers, schedulers\nrun Airflow’s internal commands to control DAGs\n\nCan’t\n\nSome of Airflow’s native commands like backfill (check this AWS document), dags list, dags list-runs, dags next-execution, and more\n\n\n\n\n\n\n\n38.6.2 AWS Glue\n\nCloud-based data integration service that makes it easy to move data between data stores.\n\nIncludes a data catalog for storing metadata about data sources and targets, as well as a ETL (extract, transform, and load) engine for transforming and moving data.\nIntegrates with other AWS services, such as S3 and Redshift, making it a convenient choice for users of the AWS ecosystem. Serverless (i.e. costs only incurred when triggered by event) Each job triggers separate resources, so if one job overloads resources, it doesn’t affect other jobs\nJobs experience latency since instances have to spin-up and install packages\nCost Charged by Data Processing Unit (DPU) multiplied by usage hours (Pricing)\n\nJob types:\n\nPython shell: you can choose either 0.0625 or 1 DPU.\nApache Spark: you can use 2 to 100 DPUs.\nSpark Streaming: you can use 2 DPUs to 100 DPUs.\n\n\n\nCan run Spark\nExpensive for longer running ETL tasks. So, setting up your own container and deploying it on ECS with Fargate makes sense, both in terms of efficiency and cost.\nMonitoring\n\nCloudwatch\nGlueStudio within Glue Clicking number sends you to Cloudwatch where you can drill down into jobs\nCloudFormation can be used to configure and manage\nGlue SDK available\n\n\n\n\n38.6.3 Prefect\n\nEasier to manage for smaller data engineer teams or a single data engineer\nmore user friendly than Airflow; Better UI; more easily discover location and time of errors\npurely python\nMisc\n\nadd slack webhook for notifications\nHas slack channel to get immediate help with issues or questions\n\nautomatic versioning for every flow, within every project\n\nalso document the models deployed with each version in the README they provide with every flow\n\nComponents\n\nTasks - individual jobs that do one unit of work\n\ne.g. a step that syncs Fivetran data or runs a dbt model\n\nFlows - functions that consist of a bunch of smaller tasks, or units of work, that depend on one another\n\ne.g. 1 flow could be multiple tasks running Fivetran syncs and dbt models\n\nExample:\nfrom prefect import flow, task\n@flow(name=\"Create a Report for Google Trends\")\ndef create_pytrends_report(\n    keyword: str = \"COVID\", start_date: str = \"2020-01-01\", num_countries: int = 10\n):\n\nThese flows are then scheduled and run by whatever types of agents you choose to set up.\n\nSome options include AWS ECS, GCP Vertex, Kubernetes, locally, etc.\n\nDeployments (docs)\n\nAlso see Create Robust Data Pipelines with Prefect, Docker, and GitHub\nDefintions\n\nSpecify the execution environment infrastructure for the flow run\nSpecify how your flow code is stored and retrieved by Prefect agents\nCreate flow runs with custom parameters from the UI\nCreate a schedule to run the flow\n\nSteps\n\nBuild the deployment definition file and optionally upload your flow to the specified remote storage location\nCreate the deployment by applying the deployment definition\n\nSyntax: prefect deployment build [OPTIONS] &lt;path-to-your-flow&gt;:&lt;flow-name&gt;\nExample:\nprefect deployment build src/main.py:create_pytrends_report \\\n  -n google-trends-gh-docker \\\n  -q test\n\nDeployment for the flow create_pytrends_report (see flow example) from the file, “src/main.py”\n-n google-trends-gh-docker specifies the name of the deployment to be google-trends-gh-docker.\n-q test specifies the work queue to be test . A work queue organizes deployments into queues for execution.\nOutput\n\n“create_pytrends_report-deployment.yaml” file and a “.prefectignore” created in the current directory.\n\n“create_pytrends_report-deployment.yaml”:  specifies where a flow’s code is stored and how a flow should be run.\n“.prefectignore”:  prevents certain files or directories from being uploaded to the configured storage location.\n\n\n\n\n\n38.6.4 Azure Data Factory\n\nAllows users to create, schedule, and orchestrate data pipelines for moving and transforming data from various sources to destinations.\nData Factory provides a visual designer for building pipelines, as well as a range of connectors for integrating with various data stores and processing technologies.\nExample: Demand Planning Project\n\n\n\n38.6.5 Mage-AI\n\nEnables users to define DAG regardless of the choice of languages (python/SQL/R)\nWeb-based IDE, so its mobility allows working from different devices, and sharing becomes more straightforward.\n\nUI layout feels like using RStudio. It has many sections divided into different areas.\nOne of the areas is the DAG visualization which provides instant feedback to the user on the task relationship.\n\nDAGs\n\nThe pipeline or DAG is constructed with modular blocks—a block maps to a single file.\nBlock Options\n\nExecution with upstream blocks: this triggers all upstream blocks to get the data ready for the current block to run\nExecute and run tests defined in the current block: this focuses on the current block to perform testing.\nSet block as dynamic: this changes the block type into the dynamic block, and it fits better to create multiple downstream blocks at runtime.\n\nManipulate dependencies via drag and drop\n\nmage-ai keeps track of the UI changes the user made and automatically builds the dependencies DAG into the YAML file. (./pipelines/{your_awesome_pipeline_name}/metadata.yaml)\n\nVisualize data in each block\n\nHelpful for inspecting your input data and further validating the transformation.\nOnce the chart has been created, it will also be attached to the current block as the downstream_blocks.\n\n\nR\n\nAllows users to write the main ETL (Extraction, Transformation, and Loading) blocks using R.\n\n\n\n\n38.6.6 kestra\n\nPopular orchestration libraries such as Airflow, Prefect, and Dagster require modifications to the Python code to use their functionalities. You may need to modify the data science code to add orchestration logic\nKestra, an open-source library, allows you to develop your Python scripts independently and then ​​seamlessly incorporate them into data workflows using YAML files.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-eltetl",
    "href": "qmd/production-tools.html#sec-prod-tools-eltetl",
    "title": "38  Tools",
    "section": "38.7 ELT/ETL Operations",
    "text": "38.7 ELT/ETL Operations\n\n38.7.1 Misc\n\ndbt - see DB, dbt\nGoogle Dataform - Docs, Best Practices\n\n\n\n38.7.2 AWS DataBrew\n\nFeatures to clean and transform the data to ready it for further processing or feeding to machine learning models\n\nNo coding; pay for what you use; scales automatically\nover 250 transformations\nAllows you to add custom transformations with lambda functions",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-modexptrk",
    "href": "qmd/production-tools.html#sec-prod-tools-modexptrk",
    "title": "38  Tools",
    "section": "38.8 Model Experimentation/Version Tracking",
    "text": "38.8 Model Experimentation/Version Tracking\n\n38.8.1 DVC\n\nTracks data and models while model building\nStore code and track changes in a Git repository while data/models are in AWS/GCP/Azure/etc. storage\nTracking changes\n\nSteps\n\nhashes every file in the directory data,\nadds it to .gitignore and\ncreates a small file data.dvc that is added to Git.\n\nBy comparing hashes, DVC knows when files change and which version to restore.\n\nInitial Steps\n\nGoto project directory -cd &lt;path to local github repo&gt;\nInitialize DVC - dvc init\nAdd a data path/uri - dvc remote add -d remote path/to/remote\n\ncan be Google Drive, Amazon S3, Google Cloud Storage, Azure Storage, or on your local machine\ne.g. Google Drive: dvc remote add -d remote gdrive://&lt;hash&gt;\n\nThe hash will the last part of the URL, e.g. “https://drive.google.com/drive/u/0/folders/1v1cBGN9vS9NT6-t6QhJG”\n\nConfirm data set-up: dvc config -l\n\nThe config file is located inside “.dvc/”\nTo version your config on github: git add .dvc/config\n\n\nAdd data/ to .gitignore\n\nExample showed adding every file in the repo manually but this seems easier\n\nAdd, commit, and push all files to repo\n\nMain differences to regular project initialization\n\ndata/ directory doesn’t get pushed to github\ndata.dvc file gets pushed to github\n\n\nSet-up DVC data cache\n\nCan be local directory/s3/gs/gdrive/etc\nExample: S3\n            dvc remote add -d myremote s3://mybucket/path\n            git add .dvc/config\n            git commit -m \"Configure remote storage\"\n            git push\n            dvc push\n\n\nI’m guessing .dvc/config is created with dvc remote add  and wasn’t there before. Otherwise in steps 3 and 4, I need to add the files manually.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-modmon",
    "href": "qmd/production-tools.html#sec-prod-tools-modmon",
    "title": "38  Tools",
    "section": "38.9 Model/Data Drift Monitoring",
    "text": "38.9 Model/Data Drift Monitoring\n\nArize AI\n\nDocs\nAccessed through Rest API, Python SDK, or Cloud Storage Bucket\n\nFiddler AI Monitoring: fiddler.ai has a suite of tools that help in making the AI explainable, aid in operating ML models in production, monitor ML models and yes data & model drift detection is one of them\nEvidently: EvidentlyAI is another open-source tool, which helps in evaluating and monitoring models in production. If you are not using Azure ML and looking for a non-commercial tool that is simple to use, evidentlyai is a good place to start.\nAzure ML\n\nMonitors data; uses wasserstein distance\n\nAWS Glue DataBrew\n\nmonitors features\ncalculates full suite of summary stats + entropy\n\nCan be exported to a bucket and then download to measure change over time\n\nAccessed through console or programmatically\nGenerates reports that can be viewed in console or be exported in html, pdf, etc.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-appclustmon",
    "href": "qmd/production-tools.html#sec-prod-tools-appclustmon",
    "title": "38  Tools",
    "section": "38.10 App/Cluster Monitoring",
    "text": "38.10 App/Cluster Monitoring\n\n38.10.1 Prometheus\n\nDon’t use for ML monitoring (from article)(maybe for apps?)\n\nNeed to use multiple Prometheus Metric types for cross-component monitoring\nNeed to define histogram buckets up front for single-component monitoring\nCorrectness of query results depending on scraping interval\nInability to handle sliding windows\nDisgusting-looking PromQL queries\nHigh latency for cross-component metrics (i.e., high-cardinality joins)\n\nMisc\n\nPrometheus is not a time series database (TSDB). It merely leverages a TSDB.\nBecause Prometheus scrapes values periodically, some Metric types (e.g., Gauges) can lose precision if the Metric value changes more frequently than the scraping interval. This problem does not apply to monotonically increasing metrics (e.g., Counters).\nMetrics can be logged with arbitrary identifiers such that at query time, users can filter Metrics by their identifier value.\nPromQL is flexible – users can compute many different aggregations (basic arithmetic functions) of Metric values over different window sizes, and these parameters can be specified at query time.\n\nMetric values (Docs):\n\nCounter: a cumulative Metric that monotonically increases. Can be used to track the number of predictions served, for example.\nGauge: a Metric that represents a single numerical value that can arbitrarily change. Can be used to track current memory usage, for example.\nHistogram: a Metric that categorizes observed numerical values into user-predefined buckets. This has a high server-side cost because the server calculates quantiles at query time.\nSummary: a Metric that tracks a user-predefined quantile over a sliding time window. This has a lower server-side cost because quantiles are configured and tracked at logging time. Also, the Summary Metric doesn’t generally support aggregations in queries.\n\nProcess\n\n\nUsers instrument their application code to log Metric values.\nThose values are scraped and stored in a Prometheus server.\nThe values can be queried using PromQL and exported to a visualization tool like Grafana\n\nThe are R packages that might make querying these metrics easier so you don’t have to learn PromQL",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-oth",
    "href": "qmd/production-tools.html#sec-prod-tools-oth",
    "title": "38  Tools",
    "section": "38.11 Other",
    "text": "38.11 Other\n\nTerraform\n\nProvision infrastructure across 300+ public clouds and services using a single workflow through yaml files\n\nAutomates and makes these workflows reproducible\nArticle on using it with R\n\n\nDatadog - Monitor servers in all cloud hosts in one place — alerts, metrics, logs, traces, security incidents, etc.\nPagerDuty - Automated incident management — alerts, notifications, diagnostics, logging, etc.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html",
    "href": "qmd/quarto-rmarkdown.html",
    "title": "Quarto",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-misc",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-misc",
    "title": "Quarto",
    "section": "",
    "text": "Packages\n\n{quarto}\n\nResources\n\nDocs\nReference\nTroubleshooting\n\nquarto --version - Must be in RStudio Terminal\nquarto check - Must be in RStudio Terminal - versions and engine checks\n$ quarto check\n[&gt;] Checking versions of quarto binary dependencies...\n      Pandoc version 3.1.1: OK\n      Dart Sass version 1.55.0: OK\n[&gt;] Checking versions of quarto dependencies......OK\n[&gt;] Checking Quarto installation......OK\n      Version: 1.3.340\n      Path: C:\\Users\\tbats\\AppData\\Local\\Programs\\Quarto\\bin\n      CodePage: 1252\n[&gt;] Checking basic markdown render....OK\n[&gt;] Checking Python 3 installation....OK\n      Version: 3.8.1 (Conda)\n      Path: C:/Users/tbats/Miniconda3/python.exe\n      Jupyter: 4.9.1\n      Kernels: python3\n(\\) Checking Jupyter engine render....2023-04-28 10:18:15,018 - traitlets - WARNING - Kernel\nProvisioning: The 'local-provisioner' is not found.  This is likely due to the presence of multiple jupyter_client distributions and a        previous distribution is being used as the source for entrypoints - which does not include 'local-provisioner'.  That distribution should     be removed such that only the version-appropriate distribution remains (version &gt;= 7).  Until then, a 'local-provisioner' entrypoint will     be automatically constructed and used.\nThe candidate distribution locations are: ['C:\\\\Users\\\\tbats\\\\Miniconda3\\\\lib\\\\site-packages\\\\jupyter_client-5.3.4.dist-info',                'C:\\\\Users\\\\tbats\\\\Miniconda3\\\\lib\\\\site-packages\\\\jupyter_client-7.0.6.dist-info']\n[&gt;] Checking Jupyter engine render....OK\n[&gt;] Checking R installation...........OK\n      Version: 4.2.3\n      Path: C:/PROGRA~1/R/R-42~1.3\n      LibPaths:\n        - C:/Users/tbats/AppData/Local/R/win-library/4.2\n        - C:/Program Files/R/R-4.2.3/library\n      knitr: 1.42\n      rmarkdown: 2.20\n[&gt;] Checking Knitr engine render......OK\nCLI\n\nquarto render to compile a document\nquarto preview to render a live preview that automatically updates when the source files are saved\n\nUsing a development verison of Quarto\n\nFirst Usage\n\nChange directories to where you want to store the dev version\nClone repo and change to the cloned directory\ngit clone https://github.com/quarto-dev/quarto-cli\ncd quarto-cli\nDisable Anti-Virus\nRun Configuration Script\n\nWindows Command Prompt\ncmd /k configure.cmd\n\n\\k keeps the window open in case it errors\n\nPowershell\nInvoke-Item configure.cmd\nLinux/MacOS\n./configure.sh\nThis will take a minute or two as it checks versions, installs dependencies like pandoc, etc.\n\nAdd path to quarto.cmd to PATH\n\nAfter the configuration file runs, it will output the path you need to put on PATH, e.g. \"C:\\Users\\erc\\Documents\\Quarto\\quarto-cli\\package\\dist\\bin\"\n\nEnable Anti-Virus\nShould be able to use in RStudio\n\nI was not able to use the RStudio terminal for quarto commands (e.g. quarto check) though.\nTo find the version, I just opened powershell and ran quarto –version just to make sure it was running and on PATH.\n\nNot sure if they use this every time but it was 99.9.9 instead of the verion in the changelog.\n\nI also rendered a qmd file using quarto-cmd from the root directory of quarto-cli to see if it matched the output from RStudio. (cd qmd then quarto preview forecasting-statistical.qmd --to html --no-watch-inputs --no-browse)\n\n\nSubsequent Development Versions\n\nChange directory to quarto-cli and git pull\n\n\nShortcuts\n\nNew R chunk: ctrl + alt + i\nBuild whole book: ctrl+shift b\nRender page and preview book: ctrl+shift k\n\nUsing yaml style for chunk options\n\nConvert Rmd chunk options to Quarto: knitr::convert_chunk_header(\"doc.rmd\", \"doc.qmd\")\nAnchor Link - A link, which allows the users to flow through a website page. It helps to scroll and skim-read easily. A named anchor can be used to link to a different part of the same page (like quickly navigating) or to a specific section of another page.\n\nThis is the “#sec-moose” id that can be added to headers which it allows to be referenced within the document or in other documents.\n\nMathJax commands\n\nFont Size: \\tiny{ }, \\scriptsize{ }, \\small{ }, \\normal{ }, \\large{ }, \\Large{ }, \\LARGE{ }, \\huge{ }, \\Huge{ }\n\nLightbox\n\nDocs\nGrouping images for lightbox carousel: ![A Lovely Image](mv-1.jpg){group=\"my-gallery\"}",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-syntax",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-syntax",
    "title": "Quarto",
    "section": "Syntax",
    "text": "Syntax\n\nInline code\n-   Total number of counties: **`{r} polling_places |&gt; filter(state == \"Alabama\") |&gt; distinct(county_name) |&gt; count()`**\n-   Total number of polling places: **`{r} polling_places |&gt; filter(state == \"Alabama\") |&gt; count()`**\n-   Election Day: **`{r} polling_places |&gt; filter(state == \"Alabama\") |&gt; pull(election_date) |&gt; unique()`**\n\nTo escape backticks in inline code, you have to use double-backticks instead of single backticks\n\nExample: To get \"`\"\\$(\\$file.FullName)`\"\"\n`` \"`\"\\$(\\$file.FullName)`\"\" ``\n\n\nAlign code chunk under bullet and add indented comment below chunk\n-   [Example]{.ribbon-highlight} (using a SQL Query; method 1)\n\n    ``` r\n    # open dataset\n    ds &lt;- arrow::open_dataset(dir_out, partitioning = \"species\")\n    # open connection to DuckDB\n    con &lt;- dbConnect(duckdb::duckdb())\n    # register the dataset as a DuckDB table, and give it a name\n    duckdb::duckdb_register_arrow(con, \"my_table\", ds)\n    # query\n    dbGetQuery(con, \"\n      SELECT sepal_length, COUNT(*) AS n\n      FROM my_table\n      WHERE species = 'species=setosa'\n      GROUP BY sepal_length\n    \")\n\n    # clean up\n    duckdb_unregister(con, \"my_table\")\n    dbDisconnect(con)\n    ```\n\n    -   filtering using a partition, the WHERE format is '\\&lt;partition_variable\\&gt;=\\&lt;partition_value\\&gt;'\n\nSpace between bullet and top ticks\nSpace between bottom ticks and bullet\nNote alignment of text\n\nAdd Code Annotations\n-   [Partition a large file and write to arrow format]{.underline}\n\n    ``` r\n    lrg_file &lt;- open_dataset(&lt;file_path&gt;, format = \"csv\") # &lt;1&gt;\n    lrg_file %&gt;%\n        group_by(var) %&gt;% # &lt;2&gt;\n        write_dataset(&lt;output_dir&gt;, format = \"feather\") # &lt;3&gt;\n    ```\n\n    1.  Pass the file path to `open_dataset()`\n\n    2.  Use `group_by()` to partition the Dataset into manageable chunks\n\n    3.  Use `write_dataset()` to write each chunk to a separate Parquet file---all without needing to read the full CSV file into R\n\n    -   `open_dataset` is fast because it only reads the metadata of the file system to determine how it can construct queries\nFootnote\nwords [^1]\n\n[^1]: Data from https://github.com/rfordatascience/tidytuesday\nFor PDF output, you need pagebreaks:\n{{&lt; pagebreak &gt;}}",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-yaml",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-yaml",
    "title": "Quarto",
    "section": "YAML",
    "text": "YAML\n\nSet global chunk options in yaml\n\n\nFor code cells\nexecute:\n  echo: false\n  message: false\n  warning: false\n\nEnable Margin Notes\n---\n# YAML front matter\nreference-location: margin\n---\n!expr to render code within chunk options\n\ne.g. figure caption: #| fig-cap: !expr glue::glue(\"The mean temperature was {mean(airquality$Temp) |&gt; round()}\")\n\ncolumn: screen-inset yaml markup is used to show a very wide table\nIf you haven’t set your Quarto document to be self-contained, then the images have also already been saved for you - probably in a folder called documentname_files/figure-html/\nformat: \n  html:\n    embed-resources: true\nDate first published and date modified using the current date:\n---\ndate: 2024-01-01\ndate-modified: today\n---\nYAML Examples\n\nExample",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-chunk",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-chunk",
    "title": "Quarto",
    "section": "Chunk Options",
    "text": "Chunk Options\n\nGraphics\n\nCode Chunk\n#| label: \"fig-statemap\"\n#| dpi: 300\n#| fig.height: 7.2\n#| fig.width: 3.6\n#| dev: \"png\"\n#| echo: false\n#| warning: false\n#| message: false\n\nExample shows settings for a graph for mobile\nfig.height and fig.width are always given in inches\n\nReference Figure\n1 See polling place locations in @fig-statemap.\n\nConditional Code Chunk Evaluation\n\nExample: document output type\n\nSet value in a code chunk\n```{r setup}\n# Include in first chunk of .qmd\n# Get output file type\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\n```\nUse !expr sytax to determine evaluation status\n\nExample: eval chunk based on output type\n```{r}\n#| eval: !expr out_type == \"html\"\n\n# code to create interactive {plotly}\n```\n\n```{r}\n#| eval: !expr out_type == \"docx\"\n\n# code to create static {ggplot2}\n```\n\n\nExample: Use parameterization to set value\n---\ntitle: \"test\"\nformat: html\nparams:\n  my_value: false\n---\n\nmy_value can then be used throughout the document to determine chunk evaluation status\n\n\nKnitr Hooks\n\nNotes from Writing knitr hooks\n\nAlso has a knitr hook example that alters cell output (e.g. only prints 4 lines of a vector)\n\nChunk Hooks\n\nChunk hooks get called twice: once before knitr executes the code in the chunk, and once again afterwards\nThe function can take up to four arguments, all of which are optional:\n\nbefore: A logical value indicating whether the function is being called before or after the code chunk is executed\noptions: The list of chunk options\nenvir: The environment in which the code chunk is executed\nname: The name of the code chunk option that triggered the hook function\n\nThe chunk hook is called for its side effects not the return value. However, if it returns a character output, knitr will add that output to the document output as-is.\nExample: Chunk Timer\n\nCode\ncreate_timer_hook &lt;- function() {\n  start_time &lt;- NULL\n  function(before, options) {\n    if (before) {\n      start_time &lt;&lt;- Sys.time()\n    } else {\n      stop_time &lt;- Sys.time()\n      elapsed &lt;- difftime(stop_time, start_time, units = \"secs\")\n      paste(\n        \"&lt;div style='font-size: 70%; text-align: right'&gt;\",\n        \"Elapsed time:\", \n        round(elapsed, 2), \n        \"secs\",\n        \"&lt;/div&gt;\"\n      )\n    }\n  }\n}\nknitr::knit_hooks$set(timer = create_timer_hook())\n\nThe hook is triggered the first time (with before = TRUE) to record the system time somewhere (e.g., in a variable called start_time). Then, when the hook is triggered the second time (with before = FALSE), it records the system time again (e.g., as stop_time), and computes the difference in time.\n\nUse in a cell\n```{r}\n#| timer: true\nrunif(10000)\n```\nOutput",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-rpy",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-rpy",
    "title": "Quarto",
    "section": "R and Python",
    "text": "R and Python\n\nIf only R or R and Python, the notebook is rendered by {knitr}\nIf only Python, the notebook is rendered by jupyter\nSet-up\n\n{reticulate} automatically comes loaded in Quarto and it knows to use it when it sees a python block, so you don’t need to load the package\nQuarto will select a version of Python using the Python Launcher on Windows or system PATH on MacOS and Linux. You can override the version of Python used by Quarto by setting the QUARTO_PYTHON environment variable.\n\nIn CLI on Windows, type py is see which version the Python Launcher , and therefore Quarto, is using and py –list to see which versions are installed.\n\n\nR\n```{r}\n#| label: read-data\n#| echo: true\n#| message: false\n#| cache: true\nlemurs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-08-24/lemur_data.csv')\n```\nPython\n```{python}\n#| label: modelling \n#| echo: true \n#| message: false\n\nlemur_data_py = r.lemur_data \nimport statsmodels.api as sm \ny = lemur_data_py[[\"Weight\"]] \nx = lemur_data_py[[\"Age\"]] \nx = sm.add_constant(x) \nmod = sm.OLS(y, x).fit() \nlemur_data_py[\"Predicted\"] = mod.predict(x) \nlemur_data_py[\"Residuals\"] = mod.resid`\n```\n\nUse r. to access the data in the R chunk\nThe first execution of a python cell starts reticulate::repl_python() in the terminal\n\n(back to) R\n```{r}\n#| label: plotting \n#| echo: true \n#| output-location: slide \n#| message: false \n#| fig-align: center \n#| fig-alt: \"Scatter plot of predicted and residual values for the fitted linear model.\" \n\nlibrary(reticulate) \nlibrary(ggplot2) \nlemur_residuals &lt;- py$lemur_data_py \nggplot(data = lemur_residuals, aes(x = Predicted, y = Residuals)) +\n  geom_point(colour = \"#2F4F4F\") +\n  geom_hline(yintercept = 0,\n            colour = \"red\") +\n  theme(panel.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"),\n        plot.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"))\n```\n\nUse py$ to access the data in the Python chunk *\nMust call library(reticulate) in order for Quarto to recognize py$",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-lay",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-lay",
    "title": "Quarto",
    "section": "Layouts",
    "text": "Layouts\n\n2 cols (1 col: text, 1 col: image)\n\n::: {layout=\"[50,50]\"}\n\n::: column\nEvery Quarto project starts with a Quarto file that has the extension `.qmd`.\n\n\nThis particular one analyzes children's early words, but every `.qmd` includes the same three basic elements inside:\n\n\n- A block of metadata at the top, between two fences of `---`s. This is written in [YAML](https://learnxinyminutes.com/docs/yaml/). \n- Narrative text, written in [Markdown](https://commonmark.org/help/tutorial/). \n- Code chunks in gray between two fences of ```` ``` ````, written with R or another programming language.\n\n\nYou can use all three elements to develop your code and ideas in one reproducible document.\n:::\n\n![](img/01-source.png)\n:::\n2 figures, 2 columns (i.e. side-by-side) with captions at the top\n---\nfig-cap-location: top\n---\n\n-   Words\n    -   Predictions of Standard RF vs Oblique RF\n\n        ::: {layout-ncol=\"2\"}\n        ![Standard Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-axpred-1.png){fig-align=\"left\" width=\"432\"}\n\n        ![Oblique Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-oblpred-1.png){fig-align=\"left\" width=\"432\"}\n        :::\n\n        -   Words  \n\nfig-cap-location: bottom is default;\nfig-cap-location: margin is buggy, at least in for project type book. Captions are added to the margins but bullet points mysteriously disappear during rendering to html\n\n2 charts side-by-side extending past body margins\n```{r}\n#| label: my-figure\n#| layout-ncol: 2\n#| column: page\nggplot() + ...\nggplot() + ...\n```\n\n“layout-ncol” says 2 side-by-side columns\n“column: page” says extend column width to the width of the page\n\nNested Tabs",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-auto",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-auto",
    "title": "Quarto",
    "section": "Automation",
    "text": "Automation\n\nIteration and Parameterization\n\nNotes from\n\nVelásquez R-Ladies Nairobi: Code, Slides, Video\n\nIt involves having a “child” document as a template and running it repeatedly with different parameters\nThe “main” document includes the output from the child document\nRendering Options\n\nCLI: e.g. quarto render polling-places-report.qmd -P state:'California'\n{quarto}:\nquarto::quarto_render(\n  input = here::here(\"polling-places-report.qmd\"),\n  execute_params = list(state = \"California\")\n)\n\nExample: Create a report for each parameter value. In each report, use the parameter value (e.g. state) to iterate through a template file that makes a tables (1 for each county) based on that value.\nMain Report Document\n---\ntitle: \"Polling Places Report - `r params$state`\"\nparams:\n  state: \"California\"\n---\n\n```{r}\n#| results: hide\n\nlibrary(dplyr)\n\ncounties &lt;- polling_places |&gt; \n  filter(state == params$state) |&gt; \n  distinct(county_name) |&gt; \n  pull()\n\nexpanded_child &lt;- \n  counties |&gt; \n    purrr::map(\\(county) {\n      knitr::knit_expand(\"../_template.qmd\", \n                         current_county = county))\n      }|&gt; \n    purrr::flatten()\n\nparsed_child &lt;- knitr::knit_child(text = unlist(expanded_child))\n```\n\n`{r} parsed_child`\n\nThe document that gets published, emailed, etc.\nparams specified in YAML\n\nValue can also be used in the title of the document via inline R code\n\nEach county is iterated through the child document (_template.qmd) via current_county variable and knit_expand\nparsed_child is a list of the template file outputs.\nThen, parsed_child is converted to a character vector by unlist and all the results are printed in the document by the inline R code\n\nChild Document (i.e. Template)\n### {{current_county}} COUNTY\n\n-   Total Polling Places: `{r} polling_places |&gt; filter(state == params$state, county_name == \"{{current_county}}\") |&gt; count()`\n-   Example Locations:\n\n```{r}\npolling_places |&gt; \n  filter(state == params$state, \n         county_name == \"{{current_county}}\") |&gt; \n  head(6) |&gt; \n  select(name, address.x) |&gt; \n  kbl(format = \"markdown\")\n```\n\nParameter value is used to get county data and create tables for each.\nNo YAML is necessary in child document\n\nparams values are automatically available through knitr::knit_expand that’s executed in the Main document\n\nThe county variable is utilized by the template file using the double curly braces, {{current_county}}\nkbl outputs in markdown format so the table is correctly rendered in the Main document.\n\nRendering Script\npolling_places &lt;-\n  readr::read_csv(here::here(\"data\", \"geocoded_polling_places.csv\"))\n\n# create quarto::render arguments df\npolling_places_reports &lt;-\n  polling_places |&gt;\n  dplyr::distinct(state) |&gt;\n  dplyr::slice_head(n = 5) |&gt;\n  dplyr::mutate(\n    output_format = \"html\",\n    output_file = paste0(tolower(state),\n                         \"-polling-places\"),\n    execute_params = purrr::map(state,\n                                \\(state) list(state = state))\n  ) |&gt;\n  # default output is html, so that variable not selected\n  dplyr::select(output_file, execute_params) \n\n# iterate through args and create reports\npurrr::pwalk(\n  .l = polling_places_reports,\n  .f = quarto::quarto_render,\n  input = here::here(\"main_report_document.qmd\"),\n  .progress = TRUE\n)\n\nCreates a report for each params value (e.g. state)\nGenerates a dataframe for each set of arguments to be fed to quarto::quarto_render.",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-webr",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-webr",
    "title": "Quarto",
    "section": "WebR",
    "text": "WebR\n\nSet-Up\n\nInstall the extension alongside your blog post by running quarto add coatless/quarto-webr\nAdd the extension to your blog by adding filters: [\"webr\"] to your post’s frontmatter\nInstead of {r} code chunks, use {webr-r} ones\n\nInstall CRAN packages on page load\nfilters:\n  - \"webr\"\nwebr:\n  packages:\n  - \"dplyr\"\n  - \"tidyr\"\n  - \"purrr\"\n  - \"tibble\"\n  - \"crayon\"\n\nAdd to frontmatter\n\nInstall R-Universe Package\n```{webr-r}\n#| context: setup\nwebr::install(\"collateral\", repos = c(\"https://jimjam-slam.r-universe.dev\"))\n```\n\nR-Universe packages must be installed in code cells",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html",
    "href": "qmd/clustering-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-misc",
    "href": "qmd/clustering-general.html#sec-clust-gen-misc",
    "title": "General",
    "section": "",
    "text": "Also see\n\nNotebook, pgs 57-58\nDiagnostics, Clustering\n\nFor static data, i.e., if the values do not change with time, clustering methods are usually divided into five major categories:\n\nPartitioning (or Partitional)\nHierarchical\nDensity-Based\nGrid-Based\nModel-Based Methods",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-terms",
    "href": "qmd/clustering-general.html#sec-clust-gen-terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nCluster Centroid - The middle of a cluster. A centroid is a vector that contains one number for each variable, where each number is the mean of a variable for the observations in that cluster. The centroid can be thought of as the multi-dimensional average of the cluster.\nHard (or Crisp) Clustering - each point belongs to a single cluster\nSoft (or Fuzzy) Clustering - each point is allowed to belong to multiple clusters",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-clustdesc",
    "href": "qmd/clustering-general.html#sec-clust-gen-clustdesc",
    "title": "General",
    "section": "Cluster Descriptions",
    "text": "Cluster Descriptions\n\nPackages\n\n{parameters} - provides various functions for describing, analyzing, and visualizing clusters for various methods\n{clustereval} - compute the statistical association between the features and the detected cluster labels and whether they are significant.\n\nCategorical: Chi-Square, Fisher’s Exact, or Hypergeometric tests\nContinuous: Mann-Whitney-U test\n\n\nExamine variable values at the centroids of each cluster\n\nA higher absolute value indicates that a certain variable characteristic is more pronounced within that specific cluster (as compared to other cluster groups with lower absolute mean values).\n\nDistributional statistics for each cluster\n\nNumeric variables: mean and sd for each variable in that cluster\nCategorical variables:\n\nbinary: percent where event = 1\nmultinomial: most prominent category\n\n\nRun a decision tree on clusters\n\n\nEach color (orange, blue, green, purple) represents a cluster\nExplains how clusters were generated\n{treeheatr}\n\n\nRadar charts\n\n\n3 clusters: blue (highlighted), red, green\nGuessing the mean values for each variable are the points\n\nScatter\n\nUse clustering variables of interest for a scatter plot then label the points with cluster id",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-gmm",
    "href": "qmd/clustering-general.html#sec-clust-gen-gmm",
    "title": "General",
    "section": "Gaussian Mixture Models (GMM)",
    "text": "Gaussian Mixture Models (GMM)\n\nMisc\n\nSoft clustering algorithm\nNotes from\n\nSerrano video: https://www.youtube.com/watch?v=q71Niz856KE&ab_channel=LuisSerrano\nPackages\n\n{otrimle}\n\nUses Improper Maximum Likelihood Estimator Clustering (IMLEC) method\nHyperparameters automatically tuned; Outliers removed\nRobust gaussian mixture clustering algorithm\nWebpage has links to paper, Coretto and Hennig, 2016\n\n\n\n\nComponents of the Algorithm\n\n“Color” points according to gaussians (clusters)\n\nThe closer a point is to the center of a gaussian the more intensely it matches the color of that gaussian\nPoints in between gaussians are a mixture or proportion of the colors of each gaussian\n\n\nFitting a Gaussian\n\nFind the center of mass\n\n2-dim: calculate the mean of x and the mean of y and that’s the coordinates of your center of mass\n\nFind the spread of the points\n\n2-dim: calculate the x-variance, y-variance, and covariance\n\n\nFirst Equation: Height of Gaussian (Multivariate Gaussian distribution equation).\nSecond Equation: 1-D gaussian equation that’s just being used for reference\n\nPartially “colored” points affect spread and center of mass calculations\n\nFully colored points “weigh” more than partially colored points and pull the center of mass and change the orientation\n\n\n\n\n\nSteps\n\nStart with random Gaussians\n\nEach gaussian has random means, variances\n\nColor points according to distance to the random gaussians\n\nThe heights in the distributions pic above\n\n(Forget about old gaussians) Calculate new gaussians based on the colored points\n(Forget about old colors) Color points according to distance to the new gaussians\nRepeat until some threshold is reached (i.e. gaussians or colors don’t change much)\n\nTuning\n\nInitial Conditions (i.e. Good starting points for the random gaussians at the beginning)\nLimits on the mean and variance calculations\nNumber of gaussians, k, can be chosen by minimizing the Davies-Bouldin score\n\nSee Diagnostics, Clustering &gt;&gt; Spherical/Centroid Based &gt;&gt; Davies-Bouldin Index\n\nRunning algorithm multiple times\n\nLike CV grid search algs or bootstrapping",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-lpa",
    "href": "qmd/clustering-general.html#sec-clust-gen-lpa",
    "title": "General",
    "section": "Latent Profile Analysis (LPA)",
    "text": "Latent Profile Analysis (LPA)\n\nSort of like k-means + GMM\nk number of profiles (i.e. clusters) are chosen\nModel outputs probabilities that an observation belongs to any particular cluster\nGOF metrics available\n“As with Exploratory Factor Analysis (EFA )(and other latent-variable models), the assumption of LPA is that the latent (unobserved) factor”causes” (I’m using the term loosely here) observed scores on the indicator variables. So, to refer back to my initial hypothetical example, a monster being a spell caster (the unobserved class) causes it to have high intelligence, low strength, etc. rather than the inverse. This is a worthwhile distinction to keep in mind, since it has implications for how the model is fit.”\nBin variables that might dominate the profile. This way the profiles will represent a latent variable and not gradations of the dominate variable (e.g. low, middle, high values of the dominate variable).\nCenter other variable observations according to dominant variable bin those observations are in. (e.g. subtract values in bin1 from bin1’s mean)\n# From D&D article where challenge_rating is a likely dominant variable\nmons_bin &lt;- mons_df %&gt;%\n  mutate(cr_bin = ntile(x = challenge_rating, n = 6))\nab_scores &lt;- c(\"strength\", \"dexterity\", \"constitution\", \"intelligence\", \"wisdom\", \"charisma\") \nmons_bin &lt;- mons_bin %&gt;%\n  group_by(cr_bin) %&gt;%\n  mutate(across(.cols = ab_scores, .fns = mean, .names = \"{.col}_bin_mean\")) %&gt;%\n  ungroup()",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-tsne",
    "href": "qmd/clustering-general.html#sec-clust-gen-tsne",
    "title": "General",
    "section": "tSNE",
    "text": "tSNE\n\nPackages\n\n{Rtsne}\n\nt-Distributed Stochastic Neighbor Embedding\nLooks at the local distances between points in the original data space and tries to reproduce them in the low-dimensional representation\n\nBoth UMAP and tSNE attempt to do this but fails (Lior Pachter paper thread, Doesn’t preserve local structure, No theorem says that it preserves topology)\n\nResults depend on a random starting point\nTuning parameters: perplexity",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-umap",
    "href": "qmd/clustering-general.html#sec-clust-gen-umap",
    "title": "General",
    "section": "UMAP",
    "text": "UMAP\n\nPackages:\n\n{umap}\n{scDEED} (article) - Detects Dubious t-SNE and UMAP Embeddings and Optimizes Hyperparameters\n\nscDEED assigns a reliability score to each 2D embedding to indicate how much the data point’s mid-range neighbors change in the 2D space. Observations whose 2D embedding neighbors have been drastically changed through the embedding process are called ‘dubious.’\n\n\nUniform Manifold Approximation and Projection\nSee tSNE section for Lior Pachter threads on why not to use tSNE or UMAP\nPreprocessing\n\nOnly for numeric variables\nStandardize\n\nProjects variables to a nonlinear space\nVariation of tSNE\n\nRandom starting point has less of an impact\n\nCan be supervised (give it an outcome variable)\nComputationally intensive\nLow-dimensional embedding cannot be interpreted\n\nNo rotation matrix plot like in PCA\n\nTry pca - linear method (fast)\n\nIf successful (good separation between categories), then prediction may be easier\nIf not, umap, tsne needed\n\nUMAP can taking training model and apply it to test data or new data (tSNE can’t)\nTuning parameter: neighbors\n\nExample used 500 iterations (n_epochs) as limit for convergence",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-kmeans",
    "href": "qmd/clustering-general.html#sec-clust-gen-kmeans",
    "title": "General",
    "section": "K-Means",
    "text": "K-Means\n\nSeeks to assign n points to k clusters and find cluster centers so as to minimize the sum of squared distances from each point to its cluster center.\nFor choosing the number of clusters, elbow method (i.e. WSS) is usually awful if there are more than few clusters. Recommended: Calinski-Harabasz Index and BIC then Silhouette Coefficient or Davies-Bouldin Index (See Diagnostics, Clustering &gt;&gt; Spherical/Centroid Based (article)\nBase R kmeans uses the Hartigan-Wong algorithm\n\nFor large k and larger n, the density of cluster centers should be proportional to the density of the points to the power (d/d+2). In other words the distribution of clusters found by k-means should be more spread out than the distribution of points. This is not in general achieved by commonly used iterative schemes, which stay stuck close to the initial choice of centers.\n\n{tidyclust}\n\nEngines\n\nstats and ClusterR run classical K-means\nlaR runs K-Modes models which are the categorical analog to K-means, meaning that it is intended to be used on only categorical data\nclustMixType to run K-prototypes which are the more general method that works with categorical and numeric data at the same time.\n\nExample: Mixed K-Means\nlibrary(tidymodels)\nlibrary(tidyclust)\n\ndata(\"ames\", package = \"modeldata\")\n\nkproto_spec &lt;- k_means(num_clusters = 3) %&gt;%\n  set_engine(\"clustMixType\")\n\nkproto_fit &lt;- kproto_spec %&gt;%\n  fit(~ ., data = ames)\n\nkproto_fit %&gt;%\n  extract_centroids() %&gt;%\n  select(11:20) %&gt;%\n  glimpse()\n#&gt; Rows: 3\n#&gt; Columns: 10\n#&gt; $ Lot_Config     &lt;fct&gt; Inside, Inside, Inside\n#&gt; $ Land_Slope     &lt;fct&gt; Gtl, Gtl, Gtl\n#&gt; $ Neighborhood   &lt;fct&gt; College_Creek, North_Ames, Northridge_Heights\n#&gt; $ Condition_1    &lt;fct&gt; Norm, Norm, Norm\n#&gt; $ Condition_2    &lt;fct&gt; Norm, Norm, Norm\n#&gt; $ Bldg_Type      &lt;fct&gt; OneFam, OneFam, OneFam\n#&gt; $ House_Style    &lt;fct&gt; Two_Story, One_Story, One_Story\n#&gt; $ Overall_Cond   &lt;fct&gt; Average, Average, Average\n#&gt; $ Year_Built     &lt;dbl&gt; 1989.977, 1953.793, 1998.765\n#&gt; $ Year_Remod_Add &lt;dbl&gt; 1995.934, 1972.973, 2003.035",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-ann",
    "href": "qmd/clustering-general.html#sec-clust-gen-ann",
    "title": "General",
    "section": "Approximate Nearest Neighbor (ANN)",
    "text": "Approximate Nearest Neighbor (ANN)\n\nkNN runs at O(N*K), where N is the number of items and K is the size of each embedding. Approximate nearest neighbor (ANN) algorithms typically drop the complexity of a lookup to O(log(n)).\nMisc\n\nAlso see Maximum inner product search using nearest neighbor search algorithms\n\nIt shows a preprocessing transformation that is performed before kNN to make it more efficient\nIt might already be implemented in ANN algorithms\n\n\nCommonly used in Recommendation algs to cluster user-item embeddings at the end. Also, any NLP task where you need to do a similarity search of one character embedding to other character embeddings.\nGenerally uses one of two main categories of hashing methods: either data-independent methods, such as locality-sensitive hashing (LSH); or data-dependent methods, such as Locality-preserving hashing (LPH)\nLocality-Sensitive Hashing (LSH)\n\nHashes similar input items into the same “buckets” with high probability.\nThe number of buckets is much smaller than the universe of possible input items\nHash collisions are maximized, not minimized, where a collision is where two distinct data points have the same hash.\n\nSpotify’s Annoy\n\nUses a type of LSH, Random Projections Method (RPM) (article didn’t explain this well)\nL RPM hashing functions are chosen. Each data point, p, gets hashed into buckets in each of the L hashing tables. When a new data point, q, is “queried,” it gets hash into buckets like p did. All the hashes in the same buckets of p are pulled and the hashes within a certain threshold, c*R, are considered nearest neighbors.\n\nWiki article on LSH and RPM clears it up a little, but I’d probably have to go to Spotify’s paper to totally make sense of this.\n\nAlso the Spotify alg might bring trees/forests into this somehow\n\nFacebook AI Similarity Search (FAISS)\n\nHierarchical Navigable Small World Graphs (HNSW)\nHNSW has a polylogarithmic time complexity (O(logN))\nTwo approximations available Embeddings are clustered and centroids are calculated. The k nearest centroids are returned.\n\nEmbeddings are clustered into veroni cells. The k nearest embeddings in a veroni cell or a region of veroni cells is returned.\n\nBoth types of approximations have tuning parameters.\n\nInverted File Index + Product Quantization (IVFPQ)(article)",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-dbscan",
    "href": "qmd/clustering-general.html#sec-clust-gen-dbscan",
    "title": "General",
    "section": "DBSCAN",
    "text": "DBSCAN\n\n\nMisc\n\nNotes from:\n\nUnderstanding DBSCAN and Implementation with Python\nClustering with DBSCAN, Clearly Explained video\n\nPackages\n\n{dbscan}\n{parameters}\n\nn_clusters_dbscan - Given a “min_size” (aka minPts?), the function estimates the optimal “eps”\ncluster_analysis - Shows Sum of Squares metrics and the (standardized) mean value for each variable within each cluster.\n\n\nHDBSCAN is the hierarchical density-based clustering algorithm\nUse Cases\n\nGeospatially Clustering Earthquakes\n\nEvents can occur in irregular shaped clusters (i.e., along faults of different orientations).\nEvents can occur in different densities (i.e. some fault zones are more active than others).\nEvents can occur far away from fault zones (i.e. outliers)\n\n\n\nTuning\n\neps - The maximum distance between two samples for one to be considered to be connected to the other\n\nLarge eps tend to include more points within a cluster,\nToo-large eps will include everything in the same single cluster\nToo-small eps will result in no clustering at all\n\nminPts (or min_samples) - The minimum number of samples in a neighborhood for a point to be considered as a core point\n\nToo-small minPts is not meaningful because it will regard every point as a core point.\nLarger minPts can be better to deal with noisy data\n\n\nAlgorithm\n\nFor each data point, find the points in the neighborhood within eps distance, and define the core points as those with at least minPts neighbors.\n\n\nThe orange circle represents the eps area\nIf minPts = 4, then the top 4 points are core points because they have at least 4 points overlapping the eps area\n\nDefine groups of connected core points as clusters.\n\n\nAll the green points have been labelled as core points\n\nAssign each non-core point to a nearby cluster if it’s directly reachable from a neighboring core point, otherwise define it as an outlier.\n\n\nThe black points are non-core points but are points that overlap the eps area for the outer-most core points.\nAdding these black points finalizes the first cluster\n\nThis process is repeated for the next group of core points and continues until all that’s left are outliers.\n\nAdvantages\n\nDoesn’t require users to specify the number of clusters.\nNot sensitive to outliers.\nClusters formed by DBSCAN can be any shape, which makes it robust to different types of data.\n\nExample: Nested Cluster Structure\n\nK-Means\n\n\nK-Means wants spherical clusters which makes it grab groups of points it shouldn’t\n\nDBSCAN\n\n\nAble correctly identify the oblong shaped cluster\n\n\n\n\nDisadvantages\n\nIf the data has a very large variation in densities across clusters because you can only use one pair of parameters, eps and MinPts, on one dataset\nIt could be hard to define eps without the domain knowledge of the data\nClusters not totally reproducible. Clusters are defined sequentially so depending on which group of core points the algorithm starts with and hyperparameter values, some non-core points that are within the eps area of multiple clusters may be assigned to different clusters on different runs of the algorithm.",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-lkhs",
    "href": "qmd/db-lakes.html#sec-db-lakes-lkhs",
    "title": "Lakes",
    "section": "Lakehouse",
    "text": "Lakehouse\n\n\nThe key idea behind a Lakehouse is to be able to take the best of a Data Lake and a Data Warehouse.\n\nData Lakes can in fact provide a lot of flexibility (e.g. handle structured and unstructured data) and low storage cost.\nData Warehouses can provide really good query performance and ACID guarantees.",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html",
    "href": "qmd/db-postgres.html",
    "title": "Postgres",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-misc",
    "href": "qmd/db-postgres.html#sec-db-pstgr-misc",
    "title": "Postgres",
    "section": "",
    "text": "Notes from\n\nCourse: linkedin.learning.postgresql.client.applications\nCourse: Linux.Academy.PostgreSQL.Administration.Deep.Dive\n\nResources\n\nPostgreSQL is Enough - Links to various applications/extensions resources\n\nEverything is case sensitive, so use lowercase for db and table names\nCheck postgres sql version - psql --version or -V\nSee flag options - psql --help\nIf there’s a “#” in the prompt after logging into a db, then that signifies you are a super-user\nMeta commands (i.e. commands once you’re logged into the db)\n\n\\du - list roles (aka users + permissions)\n\\c  - switches databases\n\\password  - assign a password to a user (prompt will ask for the password twice)\n\nCan also use ALTER ROLE for this but the password will then be in the log\n\n\nUnlogged Table - Data written to an unlogged table will not be logged to the write-ahead-log (WAL), making it ideal for intermediate tables and considerably faster. Note that unlogged tables will not be restored in case of a crash, and will not be replicated.",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-gstgr-ext",
    "href": "qmd/db-postgres.html#sec-db-gstgr-ext",
    "title": "Postgres",
    "section": "Extensions",
    "text": "Extensions\n\npg_analytics\n\nIntro, Repo\nArrow and Datafusion integrated with Postgres\nDelta Lake tables behave like regular Postgres tables but use a column-oriented layout via Apache Arrow and utilize Apache DataFusion, a query engine optimized for column-oriented data\nData is persisted to disk with Parquet\nThe delta-rs library is a Rust-based implementation of Delta Lake. This library adds ACID transactions, updates and deletes, and file compaction to Parquet storage. It also supports querying over data lakes like S3, which introduces the future possibility connecting Postgres tables to cloud data lakes.\n\npg_bm25\n\nIntro, Repo\nRust-based extension that significantly improves Postgres’ full text search capabilities\n\nBuilt to be an Elasticsearch inside of a postgres db\n\nPerformant on large tables, adds support for operations like fuzzy search, relevance tuning, or BM25 relevance scoring (same algo as Elasticsearch), real-time search — new data is immediately searchable without manual reindexing\n\nQuery times over 1M rows are 20x faster compared to tsquery and ts_ran (built-in search and sort)\n\n\npg_sparse\n\nIntro, Repo\nEnables efficient storage and retrieval of sparse vectors using HNSW\n\nSPLADE outputs sparse vectors with over 30,000 entries. Sparse vectors can detect the presence of exact keywords while also capturing semantic similarity between terms.\n\nFork of pgvector with modifications\nCompatible alongside both pg_bm25 and pgvector\n\npgvector\n\nRepo\nAlso see Databases, Vector Databases for alternatives and comparisons\nEnables efficient storage and retrieval of dense vectors using HNSW\n\nOpenAI’s text-embedding-ada-002 model outputs dense vectors with 1536 entries\n\nExact and Approximate Nearest Neighbor search\nL2 distance, Inner Product, and Cosine Distance\nSupported inside AWS RDS\n\npg_vectorize\n\nRepo\nWorkflows for both vector search and RAG\nIntegrations with OpenAI’s embeddings and chat-completion endpoints and a self-hosted container for running Hugging Face Sentence-Transformers\nAutomated creation of Postgres triggers to keep your embeddings up to date\nHigh level API - one function to initialize embeddings transformations, and another function to search\n\npgrx\n\nRepo\nFramework for developing PostgreSQL extensions in Rust\nTo install extensions built in Rust, you need to have this extension installed\n\nplprql\n\nRepo\nEnables you to run PRQL queries. PRQL has a syntax that is similar to {dplyr}\nBuilt in Rust so you have to have pgrx installed. Repo has directions.",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-dock",
    "href": "qmd/db-postgres.html#sec-db-pstgr-dock",
    "title": "Postgres",
    "section": "Docker",
    "text": "Docker\n\nSteps\n\nStart docker desktop\nStart powershell\ndocker run --name pg_database -p 5432:5432 -e POSTGRES_PASSWORD=ericb2022 -d postgres:latest\n\n1st 5432 is local computer port\n2nd 5432 is the required postgres image port\n-e is for defining an environment variable; here its the db password that I set to ericb2022\n-d\n\nRuns the container in the background\nAllows you to run commands in the same terminal window that you used the container run command in\n\n“postgres:latest” is the name of the image to build the container from\n\nClose powershell\nIn docker desktop, the “pg_database” container should be running\n\nConnect to the db\n\nSteps\n\npsql should be in your list of path environment variables\n\nRight-click Start &gt;&gt; System &gt;&gt; advanced settings (right panel) &gt;&gt; environment variables &gt;&gt; highlight path &gt;&gt; edit\n“C:\\Program Files\\PostgreSQL\\14\\bin”\n\n** Note the “14” in the path which is the current version. Therefore, when postgres is updated, this path will have to be updated **\n\n\npsql --host localhost --port 5432 --dbname postgres --username postgres\n\nNote these are all default values, so this is equivalent to psql -U postgres\n–host (-h) is the ip address or computer name that you want to connect to\n\nlocalhost is for the docker container that’s running\n\n5432 is the default –port (-p) for a postgres container\n–dbname (-d) is the name of the database on the server\n\n“postgres” is a db that ships with postgres\n\n–username (-U) is a username that has permission to access the db\n\n“postgres” is the default super-user name\n\n\nA prompt will then ask you for that username’s password\n\nThe container above has the password ericb2022\n\nThis didn’t work for me, needed to use my postgres password that I set-up when I installed postgres and pgAdmin.\nMy local postgres server and the container are listening on the same port, so maybe if I changed the first port number to something else, it would connect to the container.\n\n\nTo exit db, \\q\n\n\nCreate a db\n\nSteps\n\ncreatedb -h localhost -p 5432 -U postgres -O eric two_trees\n\n-U is the user account used to create the db\n-O is used to assign ownership to another user account\n\n“role” (i.e. user account) must already exist\n\n“two_trees” is the name of the new db\nYou will be prompted for user’s password\n\nList of dbs on the server\n\npsql -h localhost -p 5432 -U postgres -l\n\n-l lists all dbs on server\nYou will be prompted for user’s password\n\n\n\n\nRun a sql script\n\npsql -d acweb -f test.sql\n\n-d is for the database name (e.g. acweb)\n-f is for running a file (e.g. test.sql)\n\n\nAdd users\n\nCreate user/role (once inside db)\nCREATE USER &lt;user name1&gt;;\nCREATE ROLE &lt;user name2&gt;;\nALTER ROLE &lt;user name2&gt; LOGIN\n\nCREATE USER will give the user login attribute/permission while CREATE ROLE will not\n\nALTER ROLE gives the user attributes/permissions (e.g. login permission)\n\nCreate user/role (at the CLI) - createuser &lt;user name&gt;",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-pgadm",
    "href": "qmd/db-postgres.html#sec-db-pstgr-pgadm",
    "title": "Postgres",
    "section": "pgAdmin",
    "text": "pgAdmin\n\nCreate a server\n\nRight-click on servers &gt;&gt; create &gt;&gt; server\n\nGeneral tab &gt;&gt; enter name\nConnection tab\n\nHost name/address: computer name or ip address where the server is running\n\nlocal: localhost or 127.0.0.1\n\nPort: default = 5432\nMaintenance database: db you want to connect to\n\nIf you haven’t created it yet, just use default “postgres” which autmatically created during installation\n\nusername/password\n\nu: default is postgres\np: installation password\nTick Save password\n\n\nClick Save\n\n\nCreate a db\n\nRight-click databases &gt;&gt; create &gt;&gt; databases &gt;&gt; enter name (lowercase) and click save\n\nCreate a table\n\nVia gui\n\nClick db name &gt;&gt; schema &gt;&gt; public &gt;&gt; right-click tables &gt;&gt; create &gt;&gt; tables\nGeneral tab\n\nEnter the table name (lower case)\n\nColumns tab\n\nEnter name, data type, whether there should be a “Not Null” constraint, and whether it’s a primary key\nAdd additional column with “+” icon in upper right\nIf you’re going to fill the table with a .csv file, make sure the column names match\n\nClick save\nTable will be located at db name &gt;&gt; schema &gt;&gt; public &gt;&gt; tables\n\nVia sql\n\nOpen query tool\n\nRight-click  or Schemas or Tables &gt;&gt; query tool\nClick Tools menu dropdown (navbar) &gt;&gt; query tool\n\nRun CREATE TABLE statement\n\nIf you don’t include the schema as part of the table name, pgadmin automatically places it into the “public” schema directory (e.g. public.table_name)\n\n\n\nImport csv into an empty table\n\nMake sure the column names match\nRight-click table name &gt;&gt; import/export\nOptions tab\n\nMake sure import is selected\nSelect the file\nIf you have column names in your csv, select Yes for Header\nSelect “,” for the Delimiter\n\nColumns tab\n\nCheck to make sure all the column names are there\n\nClick OK\n\nQuery Table\n\nRight-click table &gt;&gt; query editor\nQuery editor tab\n\nType query &gt;&gt; click ▶ to run query",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-rds",
    "href": "qmd/db-postgres.html#sec-db-pstgr-rds",
    "title": "Postgres",
    "section": "AWS RDS",
    "text": "AWS RDS\n\nMisc\n\nNotes from Create an RDS Postgres Instance and connect with pgAdmin\n\nSteps\n\nSearch AWS services for “RDS” (top left navbar)\nCreate Database\n\nClick “Create Database”\n\nCreate Database\n\nChoose Standard create or Easy Create\n\nEasy Create - uses “best practices” settings\n\nSelect postgres\n\nAlso available: Amazon Aurora, MySQL, MariaDB, Oracle, Microsoft SQL Server\n\nTemplates\n\nProduction\n\nMulti-AZ Deployment - Multiple Availability Zones\nProvisioned IOPS Storage - Increased output\n\nDev/Test\nRree tier\n\n750 hrs of Amazon RDS in a Single-AZ db.t2.micro Instance.\n20 GB of General Purpose Storage (SSD).\n20 GB for automated backup storage and any user-initiated DB Snapshots.\n\nRDS pricing page\n\nSettings\n\nDB Instance Identifier - enter name\nSet master username, master username password\n\nDB Instance\n\ndb.t3.micro or db.t4g.micro for free tier\n\ndev/test, production has many other options\n\n\nStorage\n\nDefaults: SSD with 20GB\nAutoscaling can up the storage capacity to a default 1000GB",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-py",
    "href": "qmd/db-postgres.html#sec-db-pstgr-py",
    "title": "Postgres",
    "section": "Python",
    "text": "Python\n\n{{psycopg2}}\n\nMisc\n\nNotes from Fastest Way to Load Data Into PostgreSQL Using Python\ntl;dr\n\nLarge Data: use copy_to\nMedium to Small Data:\n\nTime and memory isn’t an issue: Use extract_values or maybe copy_to if you don’t have JSON.\n\n\n\nConnect to db\nimport psycopg2\n\nconnection = psycopg2.connect(\n    host=\"localhost\",\n    database=\"testload\",\n    user=\"haki\",\n    password=None,\n)\nconnection.autocommit = True\nCreate a table\ndef create_staging_table(cursor) -&gt; None:\n    cursor.execute(\"\"\"\n        DROP TABLE IF EXISTS staging_beers;\n        CREATE UNLOGGED TABLE staging_beers (\n            id                  INTEGER,\n            name                TEXT,\n            tagline             TEXT,\n            first_brewed        DATE,\n            description         TEXT,\n            image_url           TEXT,\n            abv                 DECIMAL,\n            ibu                 DECIMAL,\n            target_fg           DECIMAL,\n            target_og           DECIMAL,\n            ebc                 DECIMAL,\n            srm                 DECIMAL,\n            ph                  DECIMAL,\n            attenuation_level   DECIMAL,\n            brewers_tips        TEXT,\n            contributed_by      TEXT,\n            volume              INTEGER\n        );\n    \"\"\")\n\nwith connection.cursor() as cursor:\n  create_staging_table(cursor)\n\nThe function receives a cursor and creates a unlogged table called staging_beers.\n\nInsert many rows at once\n\nNotes from Fastest Way to Load Data Into PostgreSQL Using Python\nThe best way to load data into a database is using the copy command (last method in this section). The issue here is that copy needs a .csv file and not json.\n\nThis might be an issue just because of psycopg2 library doesn’t support json or that there is a postgres extension that isn’t supported by the library. This also might not be a problem in the future.\n\nData\nbeers = iter_beers_from_api()\nnext(beers)\n{'id': 1,\n 'name': 'Buzz',\n 'tagline': 'A Real Bitter Experience.',\n 'first_brewed': '09/2007',\n 'description': 'A light, crisp and bitter IPA brewed...',\n 'image_url': 'https://images.punkapi.com/v2/keg.png',\n 'abv': 4.5,\n 'ibu': 60,\n 'target_fg': 1010,\n...\n}\nnext(beers)\n{'id': 2,\n 'name': 'Trashy Blonde',\n 'tagline': \"You Know You Shouldn't\",\n 'first_brewed': '04/2008',\n 'description': 'A titillating, ...',\n 'image_url': 'https://images.punkapi.com/v2/2.png',\n 'abv': 4.1,\n 'ibu': 41.5,\n ...\n }\n\nData is from beers api\niter_beers_from_api is a udf that takes the json from the api and creates a generator object that iterates through each beer.\n\nInsert data in db using execute_values (low memory usage and still pretty fast)\ndef insert_execute_values_iterator(\n    connection,\n    beers: Iterator[Dict[str, Any]],\n    page_size: int = 100,\n) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        psycopg2.extras.execute_values(cursor, \"\"\"\n            INSERT INTO staging_beers VALUES %s;\n        \"\"\", ((\n            beer['id'],\n            beer['name'],\n            beer['tagline'],\n            parse_first_brewed(beer['first_brewed']),\n            beer['description'],\n            beer['image_url'],\n            beer['abv'],\n            beer['ibu'],\n            beer['target_fg'],\n            beer['target_og'],\n            beer['ebc'],\n            beer['srm'],\n            beer['ph'],\n            beer['attenuation_level'],\n            beer['brewers_tips'],\n            beer['contributed_by'],\n            beer['volume']['value'],\n        ) for beer in beers), page_size=page_size)\n\ninsert_execute_values_iterator(page_size=1000)\n\nparse_first_brewed is a udf that transforms a date string to datetime type.\nbeer[‘volume’][‘value’]: Data is in json and the value for volume is subsetted from the nested field.\nBenchmark: At page_size = 1000, 1.468s, 0.0MB of RAM used\nThe generator((bear['id'], … , bear['volume']['value'], for beer in beers) keeps data from being stored in memory during transformation\npage_size: maximum number of arglist items to include in every statement. If there are more items the function will execute more than one statement.\n\nHere arglist is the data in the form of generator\n\n\nInsert data in db using copy_from (Fast but memory intensive)\nimport io\n\ndef clean_csv_value(value: Optional[Any]) -&gt; str:\n    if value is None:\n        return r'\\N'\n    return str(value).replace('\\n', '\\\\n')\n\ndef copy_stringio(connection, beers: Iterator[Dict[str, Any]]) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        csv_file_like_object = io.StringIO()\n        for beer in beers:\n            csv_file_like_object.write('|'.join(map(clean_csv_value, (\n                beer['id'],\n                beer['name'],\n                beer['tagline'],\n                parse_first_brewed(beer['first_brewed']),\n                beer['description'],\n                beer['image_url'],\n                beer['abv'],\n                beer['ibu'],\n                beer['target_fg'],\n                beer['target_og'],\n                beer['ebc'],\n                beer['srm'],\n                beer['ph'],\n                beer['attenuation_level'],\n                beer['contributed_by'],\n                beer['brewers_tips'],\n                beer['volume']['value'],\n            ))) + '\\n')\n        csv_file_like_object.seek(0)\n        cursor.copy_from(csv_file_like_object, 'staging_beers', sep='|')\n\nclean_csv_value: Transforms a single value\n\nEscape new lines: some of the text fields include newlines, so we escape \\n -&gt; \\\\n.\nEmpty values are transformed to \\N: The string \"\\N\" is the default string used by PostgreSQL to indicate NULL in COPY (this can be changed using the NULL option).\n\ncsv_file_like_object: Generate a file like object using io.StringIO. A StringIO object contains a string which can be used like a file. In our case, a CSV file.\ncsv_file_like_object.write: Transform a beer to a CSV row\n\nTransform the data: transformations on first_brewed and volume are performed here.\nPick a delimiter: Some of the fields in the dataset contain free text with commas. To prevent conflicts, we pick “|” as the delimiter (another option is to use QUOTE).\n\n\nInsert data (streaming) in db using copy_from (Fastest and low memory but complicated, at least with json)\n\nBuffering function\nfrom typing import Iterator, Optional\nimport io\n\nclass StringIteratorIO(io.TextIOBase):\n    def __init__(self, iter: Iterator[str]):\n        self._iter = iter\n        self._buff = ''\n\n    def readable(self) -&gt; bool:\n        return True\n\n    def _read1(self, n: Optional[int] = None) -&gt; str:\n        while not self._buff:\n            try:\n                self._buff = next(self._iter)\n            except StopIteration:\n                break\n        ret = self._buff[:n]\n        self._buff = self._buff[len(ret):]\n        return ret\n\n    def read(self, n: Optional[int] = None) -&gt; str:\n        line = []\n        if n is None or n &lt; 0:\n            while True:\n                m = self._read1()\n                if not m:\n                    break\n                line.append(m)\n        else:\n            while n &gt; 0:\n                m = self._read1(n)\n                if not m:\n                    break\n                n -= len(m)\n                line.append(m)\n        return ''.join(line)\n\nThe regular io.StringIO creates a file-like object but is memory-heavy. This function creates buffer that will feed each line of the file into a buffer, stream it to copy, empty the buffer, and load the next line.\n\nCopy to db\ndef clean_csv_value(value: Optional[Any]) -&gt; str:\n    if value is None:\n        return r'\\N'\n    return str(value).replace('\\n', '\\\\n')\n\ndef copy_string_iterator(connection, beers: Iterator[Dict[str,\nAny]]) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        beers_string_iterator = StringIteratorIO((\n            '|'.join(map(clean_csv_value, (\n                beer['id'],\n                beer['name'],\n                beer['tagline'],\n                parse_first_brewed(beer['first_brewed']).isoformat(),\n                beer['description'],\n                beer['image_url'],\n                beer['abv'],\n                beer['ibu'],\n                beer['target_fg'],\n                beer['target_og'],\n                beer['ebc'],\n                beer['srm'],\n                beer['ph'],\n                beer['attenuation_level'],\n                beer['brewers_tips'],\n                beer['contributed_by'],\n                beer['volume']['value'],\n            ))) + '\\n'\n            for beer in beers\n        ))\n        cursor.copy_from(beers_string_iterator, 'staging_beers', sep='|')\n\nSimilar to other code above",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#distributed-architectures",
    "href": "qmd/db-postgres.html#distributed-architectures",
    "title": "Postgres",
    "section": "Distributed Architectures",
    "text": "Distributed Architectures\n\nMisc\n\nNotes from An Overview of Distributed PostgreSQL Architectures\nFeatures to achieve single node availability, durability, and performance - Replication - Place copies of data on different machines - Distribution - Place partitions of data on different machines - Decentralization - Place different DBMS activities on different machines\nIf transactions take on average 20ms, then a single (interactive) session can only do 50 transactions per second. You then need a lot of concurrent sessions to actually achieve high throughput. Having many sessions is not always practical from the application point-of-view, and each session uses significant resources like memory on the database server. Most PostgreSQL set ups limit the maximum number of sessions in the hundreds or low thousands, which puts a hard limit on achievable transaction throughput when network latency is involved.\n\nNetwork-Attached Block Storage (e.g. EBS)\n\n\nCommon technique in cloud-based architectures\nDatabase server typically runs in a virtual machine in a Hypervisor, which exposes a block device to the VM. Any reads and writes to the block device will result in network calls to a block storage API. The block storage service internally replicates the writes to 2-3 storage nodes.\nPros\n\nHigher durability (replication)\nHigher uptime (replace VM, reattach)\nFast backups and replica creation (snapshots)\nDisk is resizable\n\nCons\n\nHigher disk latency (~20μs -&gt; ~1000μs)\nLower IOPS (~1M -&gt; ~10k IOPS)\nCrash recovery on restart takes time\nCost can be high\n\nGuideline: The durability and availability benefits of network-attached storage usually outweigh the performance downsides, but it’s worth keeping in mind that PostgreSQL can be much faster.\n\nRead Replicas\n\n\nThe most common way of using a replica is to set it up as a hot standby that takes over when the primary fails in a high availability set up.\nHelps you scale read throughput when reads are CPU or I/O bottlenecked by load balancing queries across replicas, which achieves linear scalability of reads and also offloads the primary, which speeds up writes!\n\nThe primary usually does not wait for replication when committing a write, which means read replicas are always slightly behind. That can become an issue when your application does a read that, from the user’s perspective, depends on a write that happened earlier.\nFor example, a user clicks “Add to cart”, which adds the item to the shopping cart and immediately sends the user to the shopping cart page. If reading the shopping cart contents happens on the read replica, the shopping cart might then appear empty. Hence, you need to be very careful about which reads use a read replica.\n\nWhen load balancing between different nodes, clients might repeatedly get connected to different replica and see a different state of the database\nPowerful tool for scaling reads, but you should consider whether your workload is really appropriate for it.\nPros\n\nRead throughput scales linearly\nLow latency stale reads if read replica is closer than primary\nLower load on primary\n\nCons\n\nEventual read-your-writes consistency\nNo monotonic read consistency\nPoor cache usage\n\nGuideline: Consider using read replicas when you need &gt;100k reads/sec or observe a CPU bottleneck due to reads, best avoided for dependent transactions and large working sets.\n\nDBMS-Optimized Cloud Storage\n\n\nWhere DBMS is Database Management Software. (e.g. Aurora)\nPostgreSQL is not optimized for this architecture\nWhile the theory behind DBMS-optimized storage is sound. In practice, the performance benefits are often not very pronounced (and can be negative), and the cost can be much higher than regular network-attached block storage. It does offer a greater degree of flexibility to the cloud service provider, for instance in terms of attach/detach times, because storage is controlled in the data plane rather than the hypervisor.\nPros\n\nPotential performance benefits by avoiding page writes from primary\nReplicas can reuse storage, incl. hot standby\nCan do faster reattach, branching than network-attached storage\n\nCons\n\nWrite latency is high by default\nHigh cost / pricing\nPostgreSQL is not designed for it, not OSS\n\nGuideline: Can be beneficial for complex workloads, but important to measure whether price-performance under load is actually better than using a bigger machine.\n\nActive-Active (e.g. BDR)\n\n\nAny node can locally accept writes without coordination with other nodes.\nIt is typically used with replicas in multiple sites, each of which will then see low read and write latency, and can survive failure of other sites.\nActive-active systems do not have a linear history, even at the row level, which makes them very hard to program against.\nPros\n\nVery high read and write availability\nLow read and write latency\nRead throughput scales linearly\n\nCons\n\nEventual read-your-writes consistency\nNo monotonic read consistency\nNo linear history (updates might conflict after commit)\n\nGuideline: Consider only for very simple workloads (e.g. queues) and only if you really need the benefits.\n\nTransparent Sharding (e.g. Citus)\n\n\nTables distributed and/or replicated across multiple primary nodes using a “shard key .”\n\nEach node shows the distributed tables as if they were regular PostgreSQL tables and queries\n\nData are located in “shards” which are regular PostgreSQL tables. Joins and foreign keys that include the shard key can be performed locally.\nScaling out transactional workloads is most effective when queries have a filter on the shard key, such that they can be routed to a single shard group (e.g. single tenant in a multi-tenant app) or compute-heavy analytical queries that can be parallelized across the shards (e.g. time series / IoT).\nWhen loading data, use COPY, instead of INSERT, to avoid waiting for every row.\nPros\n\nScale throughput for reads & writes (CPU & IOPS)\nScale memory for large working sets\nParallelize analytical queries, batch operations\n\nCons\n\nHigh read and write latency\nData model decisions have high impact on performance\nSnapshot isolation concessions\n\nGuideline: Use for multi-tenant apps, otherwise use for large working set (&gt;100GB) or compute heavy queries.\n\nDistributed Key-Value Stores With SQL (e.g. Yugabyte)\n\n\nA bunch of complicated stuff I don’t understand 😅\nTables are stored in the key-value store, with the key being a combination of the table ID and the primary key.\nBetter to use PostgresSQL without this architecture.\nPros\n\nGood read and write availability (shard-level failover)\nSingle table, single key operations scale well\nNo additional data modeling steps or snapshot isolation concessions\n\nCons\n\nMany internal operations incur high latency\nNo local joins in current implementations\nNot actually PostgreSQL, and less mature and optimized\n\nGuideline: Just use PostgreSQL. For simple applications, the availability and scalability benefits can be useful.",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/missingness.html",
    "href": "qmd/missingness.html",
    "title": "Missingness",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-misc",
    "href": "qmd/missingness.html#sec-missing-misc",
    "title": "Missingness",
    "section": "",
    "text": "Also see\n\nEDA &gt;&gt; Missingness\nModel Building, tidymodels &gt;&gt; Recipe &gt;&gt; Imputation\n\nBagging and knn methods for cross-sectional data\nRolling method for time series data\n\nHarrell RMS 3.5 Strategies for Developing an Imputation Model\n\nPackages\n\n{mice} (Multivariate Imputation by Chained Equations) - Imputes mixes of continuous, binary, unordered categorical and ordered categorical data\n\nBased on Fully Conditional Specification, where each incomplete variable is imputed by a separate model.\nImpute continuous two-level data, and maintain consistency between imputations by means of passive imputation.\nMany diagnostic plots are implemented to inspect the quality of the imputations.\n\n{naniar} - Tidyverse compliant methods to summarize, visualize, and manipulate missing data.\n{simputation} - Model-based, multivariate, donar, and simple stat methods available\n{NPBayesImputeCat}: Non-Parametric Bayesian Multiple Imputation for Categorical Data\n\nProvides routines to i) create multiple imputations for missing data and ii) create synthetic data for statistical disclosure control, for multivariate categorical data, with or without structural zeros\nImputations and syntheses are based on Dirichlet process mixtures of multinomial distributions, which is a non-parametric Bayesian modeling approach that allows for flexible joint modeling\nVignette\n\n\n“But more precisely, even having the correct model of the analysis stage does not absolve the analyst of considering the relationship between the imputation stage variables, the causal model, and the missingness mechanism. It turns out that in this simple example, imputing with an analysis-stage collider is innocuous (so long as it is excluded at the analysis stage). But imputation-stage colliders can wreck MI even if they are excluded from the analysis stage.”\n\nSee Multiple Imputation with Colliders\n\n**Don’t impute missing values before your training/test split\nImputing Types full-information maximum likelihood\n\nMultiple imputation\nOne-Step Bayesian imputation\n\nMissness Types (MCAR, MAR, and MNAR)\n\nMultivariate Imputation with Chained Equation (MICE) assumes MAR\n\nMethod entails creating multiple imputations for each missing value as opposed to just one. The algorithm addresses statistical uncertainty and enables users to impute values for data of different types.\n\nStochastic Regression Imputation is problematic\n\nPopular among practitioners though\nIssues\n\nStochastic regression imputation might lead to implausible values (e.g. negative incomes).\nStochastic regression imputation has problems with heteroscedastic data\n\nBayesian PMM handles these issues\n\nMissingness in RCT due dropouts (aka loss to follow-up)\n\nNotes from To impute or not: the case of an RCT with baseline and follow-up measurements\n\n{mice} used for imputation\n\nBias in treatment effect due to missingness\n\nIf there are adjustment variables that affect unit dropout then bias increases as variation in treatment effect across units increases (aka hetergeneity)\n\nIn the example, a baseline measurement of the outcome variable, used an explanatory variable, was also causal of missingness. Greater values of this variable resulted in greater bias\nUsing multiple imputation resulted in less bias than just using complete cases, but still underestimated the treatment effect.\n\nIf there are no such variables, then there is no bias due to hetergeneous treatment effects\n\nComplete cases of the data can be used\n\n\nLast observation carried forward\n\nSometimes used in clinical trials because it tends to be conservative, setting a higher bar for showing that a new therapy is significantly better than a traditional therapy.\nMust assume that the previous value (e.g. 2008 score) is similar to the ahead value (e.g. 2010 score).\nInformation about trajectories over time is thrown away.\n\n\nAssessment of Imputations\n\nSee {naniar} vignette - Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations | Journal of Statistical Software",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-caim",
    "href": "qmd/missingness.html#sec-missing-caim",
    "title": "Missingness",
    "section": "Choosing an Imputation Method",
    "text": "Choosing an Imputation Method\n\n** Don’t use this. Just putting it here to be aware of **) Standard Procedure for choosing an imputation method\n\nIssues\n\nSome methods will be favored based on the metric used\n\nConditional means methods (RMSE)\nConditional medians methos (MAE) Chosen methods tend to artificially strengthen the association between variables. As a consequence, statistical estimation and inference techniques applied to the so-imputed data set can be invalid.\n\n\nSteps\n\nSelect some observations\nSet their status to missing\nImpute them with different methods\nCompare their imputation accuracy\n\nFor numeric variables, RMSE or MAE typically used\nFor categoricals, percentage of correct predictions (PCP)\n\n\n\nInitial Considerations\n\nIf a dataset’s feature has missing data in more than 80% of its records, it is probably best to remove that feature altogether.\nIf a feature with missing values is strongly correlated with other missing values, it’s worth considering using advanced imputation techniques that use information from those other features to derive values to replace the missing data.\nIf a feature’s values are missing not at random (MNAR), remove methods like MICE from consideration. I-Score {Iscores}, Paper\nA proper scoring rule metric\nConsistent for MCAR, but MAR requires additional assumptions\n\n“valid under missing at random (MAR) if we restrict the random projections in variable space to always include all variables, which in turn requires access to some complete observations”\n\nKinda complicated. I need to read the paper",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-bayes",
    "href": "qmd/missingness.html#sec-missing-bayes",
    "title": "Missingness",
    "section": "Bayesian",
    "text": "Bayesian\n\nPredictive Mean Matching (PMM)\n\nNotes from:\n\nPredictive Mean Matching Imputation (Theory & Example in R)\nPredictive Mean Matching Imputation in R (mice Package Example)\n\nUses a bayesian regression to predict a missing value, then randomly picks a value from a group of observed values that are closest to the predicted value.\nSteps\n\nEstimate a linear regression model:\n\nUse the variable we want to impute as Y.\nUse a set of good predictors as \\(X\\) (Guidelines for the selection of \\(X\\) can be found in van Buuren, 2012, p. 128).\nUse only the observed values of \\(X\\) and \\(Y\\) to estimate the model.\n\nDraw randomly from the posterior predictive distribution of \\(\\hat \\beta\\) and produce a new set of coefficients \\(\\beta^*\\).\n\nThis bayesian step is needed for all multiple imputation methods to create some random variability in the imputed values.\n\nCalculate predicted values for observed and missing \\(Y\\).\n\nUse \\(\\hat \\beta\\) to calculate predicted values for observed \\(Y\\).\nUse \\(\\beta^*\\) to calculate predicted values for missing \\(Y\\).\n\nFor each case where \\(Y\\) is missing, find the closest predicted values among cases where \\(Y\\) is observed.\n\nExample:\n\n\\(Y_i\\) is missing. Its predicted value is 10 (based on \\(\\beta^*\\)).\nOur data consists of five observed cases of \\(Y\\) with the values 6, 3, 22, 7, and 12.\nIn step 3, we predicted the values 7, 2, 20, 9, and 13 for these five observed cases (based on \\(\\hat \\beta\\)).\nThe predictive mean matching algorithm selects the closest observed values (typically three cases) to our missing value \\(Y_i\\). Hence, the algorithm selects the values 7, 9, and 13 (the closest values to 10).\n\n\nDraw randomly one of these three close cases and impute the missing value \\(Y_i\\) with the observed value of this close case.\n\nExample: Continued\n\nThe algorithm draws randomly from 6, 7, and 12 (the observed values that correspond to the predicted values 7, 9, and 13).\nThe algorithm chooses 12 and substitutes this value to \\(Y_i\\).\n\n\nIn case of multiple imputation (strongly advised), steps 1-5 are repeated several times.\n\nEach repetition of steps 1-5 creates a new imputed data set.\nWith multiple imputation, missing data is typically imputed 5 times.\n\n\nExample\ndata_imp &lt;- \n  complete(mice(data,\n           m = 5,\n           method = \"pmm\"))\n\nm is the number of times to impute the data\ncomplete formats the data into different shapes according to an action argument\nRunning parmice instead of mice imputes in parallel",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-multimp",
    "href": "qmd/missingness.html#sec-missing-multimp",
    "title": "Missingness",
    "section": "Multiple Imputation Fit",
    "text": "Multiple Imputation Fit\n\nAKA “multiply” imputed data\nFitting a regression model with multiply imputed data\n\nSee If you fit a model with multiply imputed data, you can still plot the line\nMethods\n\nPredict then Combine (PC)\nCombine then Predict (CP)",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-ts",
    "href": "qmd/missingness.html#sec-missing-ts",
    "title": "Missingness",
    "section": "Time Series",
    "text": "Time Series\n\nIf seasonality is present, mean, median, mode, random assignment, or previous value methods shouldn’t be used.",
    "crumbs": [
      "Missingness"
    ]
  }
]