[
  {
    "objectID": "qmd/causal-inference.html",
    "href": "qmd/causal-inference.html",
    "title": "Causal Inference",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-misc",
    "href": "qmd/causal-inference.html#sec-causinf-misc",
    "title": "Causal Inference",
    "section": "",
    "text": "Notes from\n\nhttps://fabiandablander.com/r/Causal-Inference.html\n\nStatistical models measure associations (e.g. linear, non-linear) which is mutual information among the variables\n\ne.g. wind and leaves moving in a tree (doesn’t answer whether the leaves moving creates the wind or the wind creates leaving moving)\n\nCausal inference predicts the conseqences after an intervention (i.e. action)\n\nYou must know the direction of causation in order to predict the conseqences of an intervention (unlike measuring associations)\nAnswers the question, “What happens if I do this?”\n\nCausal inference is able to reconstruct unobserved counterfactual outcomes.\n\nAnswers the question, “What happens if I had done something else?”\n\nCausal assumptions are necessary in order to make causal inferences\n\nmultiple regression does not distinguish causes from confounds\np-values are not causal statements\n\nDesigned to control type I error rate\n\nAIC, etc are purely predictive\n\nCausal Experiment Assumptions\n\nsee tlverse workshop notes and ebook for listing of assumptions and definitions,  https://tlverse.org/acic2019-workshop/intro.html#identifiability\n\nThe tlverse Project seeks to use ML models to calculate causal effects. Uses Super Learner ensembling and Targeted Maximum Likelihood Estimation (TMLE) which they call Targeted Learning.\n\nIgnorability - By randomly assigning treatment, researchers can ensure that the potential outcomes are independent of treatment assignment, so that the average difference in outcomes between the two groups can only be attributable to treatment\n\nEngineering outcome variables using potential adjustment variables does not automatically adjust for those variables in your model\n\nNotes from There Are No Magic Outcome Variables\nExample\n\n\nP is population density\nX is the variable of interest\nGDP and P have been used to create GDP/P\nP influences X and provides a backdoor path to GDP/P, so P must be adjusted for\nEven if P doesn’t influence X, the point is that constructing GDP/P using P doens’t automatically adjust for P\n\n\nRandomized experiments remove all paths from the treatment variable, X\n\n\nAdjusting for Z, B, and C can add precision to measurement of the treatment effect since they are causal to Y, but they aren’t necessary to get an unbiased estimate of the treatment effect.\n\nTable 2 fallacy (Notes from McElreath video, 2022 SR Lecture 6)\n\n\nThe 2nd table presented in a paper is usually a summary of all the effects of a regression. The fallacy is that the coefficient of each variable is treated as causal.\nExample: The effect of HIV on Stroke\n\nThe model is lm(Stroke ~ HIV + Smoke + Age)\n\nOnly the coefficient of the HIV variable should be treated as causal and none of the other adjustment variables (Smoke, Age)\n\nThe effects for Smoke and Age are only partial.\nThere are likely unobserved confounding variables, U, on the effect of Smoking on Stroke (e.g. other lifestyle variables).\n\nSmoke is confounded so it’s causal estimate is biased\nAge is also confounded since Smoke is now a collider and has been conditioned upon. This opens the non-causal path, Age-Smoke-U-Stroke.\n\nAge-Smoke is frontdoor, but the backdoor path, Smoke-U, also becomes a backdoor path for Age once Smoke is conditioned upon. (aka sub-backdoor path)\nSo any open path that contains a backdoor path must also be closed\n\n\n\nSolutions\n\nDon’t include effect estimates of adjustment variables\nExplicitly interpret each effect estimate according to the causal model\n\nSee 2022 SR at the end of Lecture 6 where McElreath breaks down the interpretation of each adjustment variable estimated effect.\n\n\n\nPartial Identification (Handling Unobserved Confounds)\n\nMisc\n\nAlso see\n\nPaper: Hidden yet quantifiable: A lower bound for confounding strength using randomized trials (code)\n\nUsing RCT results and Observational data, this paper proposes a statistical test and a method for determining the lower bound confounder strength.\nIn the context of pharmacuticals, RCT results are evidently often released after FDA approval, but this method can be used in any field where there’s a combination of RCT and observational studies..\n\n\n\nSometimes the confounding paths of a DAG model can be not be resolved.\n\nFor confounders that influence the treatment and outcome, see:\n\nStructural Causal Models &gt;&gt; Bayesian examples\nIf there’s a mediator, see Other Articles &gt;&gt; Frontdoor Adjustment\n\nMeasure proxies for the unobserved confound if it’s not practical/ethical to measure\n\ni.e. If the confound is ability, then test scores, letters of recommendation, etc. could be proxies.\n\nExample: 2022 SR Lecture 10 video, code\n\n\nA: Admitted to Grad School, G: Gender, D: Dept, u: Ability, T1,2,3: 3 Test Scores\n\nAbility is latent variable/unobserved confounder\nTest Scores are proxies for Ability\n\nBoth models are fit simultaneously\nCouldn’t find a way to use {brms} to code this and Kurz didn’t included it in his brms SR book.\n\n\n\nA biased estimate is better than no estimate. It can provide an upper bound\nFind a natural experiment or design one\nSensitivity Analysis\n\nAfter the analyis, you should be able to make the statement, “In order for the confound to be responsible for the entire causal effect, it was have to be .”\n\n\nPackages\n\n{tipr} - tools for tipping point sensitivity analyses\n\nSteps for using sensitivity analysis\n\nPerform a sensitivity analysis to determine plausibly how much of the causal effect is due to confounding paths\n\nAssume the confound exists, model it’s consequences for different strengths/kinds of influence\nExample: 2022 SR Lecture 10 video, code \n\nA: Admitted to Grad School, G: Gender, D: Dept, u: Unobserved Confounder\nBoth models are fit simultaneously\nValues for β and γ are specified and u is estimated as a parameter\nI think Gender (G) is an interaction in both models which I didn’t think was possible given there are no arrows of influence from gender to u.\n\nSince gender is a moderator it wouldn’t necessarily have to be an influence arrow, it would only need to be an arrow from G to the effect of u on D (see Moderator Analysis), so maybe this is kosher\nCould also be that I’m misunderstanding McElreath’s code he uses to specify his models with {Rethinking}.\n\nCouldn’t find a way to use {brms} to code this and Kurz didn’t included it in his brms SR book.\n\n\nUse previous studies that have effect strengths of those potential confounding variables\nCompare the strengths from the previous studies to the strength determined from the sensitivity analysis. The difference is a good guess for the strength of the causal effect of your treatment variable.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-causdes",
    "href": "qmd/causal-inference.html#sec-causinf-causdes",
    "title": "Causal Inference",
    "section": "Causal Design",
    "text": "Causal Design\n\nNotes from McElreath video\nMisc\n\nWhen trying to determine the relationship (e.g. linear, nonlinear) between variables and remove inconsequential variables, the Double Debiased ML procedure might be useful.\n\nDouble Debiased Machine Learning - basic concepts, links to papers, videos\nEconML (Microsoft) and causalml (Uber) has included the method in their libraries\n\n\nWhen trying to infer causal relationships, we should not blindly enter all variables into a regression in order to “control” for them, but think carefully about what the underlying causal DAG could look like. Otherwise, we might induce spurious associations (e.g. confounding such as collider bias).\nOverview\n\nMake a causal model (i.e. DAG)\n\nNeed background information in order to make the causal assumptions represented in the DAG\nDAGs only show whether or not a variable influences another, not how the influence occurs (e.g. DAGs can’t show interactions between variables or whether the association is non-linear)\n\nUse it to design data collection and statistical procedures\n\nSteps:\n\nDetermine two variables of interest (exposure, outcome) that you want to determine if a causal relationship exists and what effect the exposure has.\nUse domain knowledge or prior scholarship to determine the relevant variable and the likely associations between all variables in data\nCreate the DAG\n\nIdentify the direct causal path between exposure and outcome\nIdentify other explanatory variables and label their directions of influence with each other, the exposure, and the outcome variable\nConsider which variables (especially the exposure and the outcome) have unobserved variables influencing them.\n\nAnalyze the DAG\n\nIdentify colliders and use d-separation to determine conditional independencies\nIdentify additional paths (backdoor paths, sub-backdoor paths) between exposure and outcome\nUse the backdoor criterion to determine the set of variables that need to be adjusted for in order to block all backdoor paths with only the direct causal path remaining open.\nAdd additional adjustment variables that are causal to the outcome variable (but don’t confound the treatment effect) in order to add precision to the estimate of the treatment effect\n\nCreate simulated data that fits the DAG (i.e. a generative model)\nPerform statistical analysis (i.e. SCMs) on the simulated data  to make sure you can measure the causal effect.\nDesign experiment and collect the data\nRun the statistical analysis on the collected data and calculate the average causal effect (ACE) under the assumptions that your DAG and model specifications are correct.\nBased on your results, revise the DAG and SCM as necessary and repeat as necessary\n\nBad Adjustment Variables (Code and more details included in 2022 SR, Lecture 6)\n\nFor all examples, Z is the adjustment variable that’s being considered; X is the treatment and Y is the outcome\n\nIn each scenario, including Z produces a biased estimate of X, so the correct model is Y ~ X.\n\nM-bias\n\n\nZ doesn’t have a direct causal influence on the either X or Y, but when it’s conditioned upon it becomes a collider due to unobserved confounds that have a direct causal influence on X and Y.\nCommon issue in Political Science and network analysis\nExample\n\nY: Health of Person 2\nX: Health of Person 1\nZ: Friendship status\n\nPre-treatment variable (tend to be open to collider paths) since they could be friends before the exposure\n\nU: Hobbies of Person 1\nV: Hobbies of Person 2\n\n\nPost-Treatment Bias\n\n\nZ is a mediator and conditioning upon Z blocks the path from X to Y, but opens the backdoor path through the unobserved confound, U.\nCommon in medical studies  \nExample\n\nY: Lifespan\nX: Win Lottery\nZ: Happiness\nU: Contextual Confounds\n\n\nSelection Bias\n\n\nSame as collider bias\n\nThis version adds an unobserved confounder\n\nExample\n\nY: Income\nX: Education\nZ: Values\nU: Family\n\n\nCase-Control Bias\n\n\nZ is a descendent. Since Z has information about Y, conditioning on it will narrow the variation of Y and distort the measured effect of X.\nAlso see Association &gt;&gt; Single Path DAGs &gt;&gt; Descendent\nExample\n\nY: Occupation\nX: Education\nZ: Income\n\n\nPrecision Parasite\n\n\n2 versions: with and without U\n\nWithout U, conditioning on Z removes variation from X and lessens (but doesn’t bias) the precision of the estimated effect of X on Y (i.e. inflated std.error)\nWith U, the effect of X is biased and that bias is amplified when Z is included.\n\n\nPeer Bias\n\n\nClassic DAG of the Berkley Admission-Race-Department study\nAlso see Structural Causal Models &gt;&gt; Example (Bayesian Peer Bias)\nX is race, E is department, Q is unobserved (e.g. student quality), Y is Admission\nDepartment cannot be conditioned upon because it’s a collider with Q and would bias the estimate of X through a sub-backdoor path, X-E-Q-Y\nOnly the total effect of X on Y can be estimated (Y ~ X) since E cannot be conditioned upon but that’s not interesting and maybe not precise",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-terms",
    "href": "qmd/causal-inference.html#sec-causinf-terms",
    "title": "Causal Inference",
    "section": "Terms",
    "text": "Terms\n\nAverage Causal Efffect (ACE) - average population effect that’s calculated from an intervention (see Counterfactual definition for info on Individual Causal Effects)\n\nIf X is binary, then   is the average causal effect (see Simpson’s Paradox example)\n\nCalculated from a contingency table\n\nAlso, \n\nThis looks like the interpretation of the slope in a regression model.\n\n\nBackdoor Criterion - A valid causal estimate is available if it is possible to condition on variables such that all backdoor paths are closed\n\nGiven two nodes, X and Y, an adjustment set, L, fulfills the backdoor criterion if \n\nno member in L is a descendant of X and\nmembers in L block all backdoor paths (“shutting the backdoor”) between X and Y.\n\nAdjusting for L thus yields the causal effect of X→Y.\nAfter executing an intervention, the conditional distribution in the observational DAG (seeing) will correspond to the interventional distribution (doing) when blocking the spurious path. (see Simpson’s Paradox example)\n\nBackdoor Path - A non-causal path that enters a causal variable in a DAG rather than exits it.\n\ne.g. the path that connects a collider to a causal variable points from the collider to the causal variable\nSub-backdoor Path - this path begins with a frontdoor path but through conditioning on a variable, it opens a connecting backdoor path which biases the treatment effect\n\nsee Misc &gt;&gt; Table 1 Fallacy and Causal Design &gt;&gt; Bad Adjustment Variables &gt;&gt; Peer Bias\n\n\nThe causal effect is the distribution of Y when we change x, averaged over the distributions of the adjustment variables (Z)\nCausal Hierarchy (lowest to highest)\n\nAssociation\n\nassociated action: Seeing - observational; observing the value of Y when X = x\n\n , observational distribution; What values Y would likely take on if X happened to equal x.\n\n\nIntervention\n\nassociated action (do-Calculus): Doing -  experimental; observing the value of Y after setting X = x\n\n , interventional distribution; What values Y would likely take on if X would be set to x.\nUsing the do operator allows us to make inferences about the population but not individuals.\ndo(X) means to cut all of the backdoor paths into X, as if we did a manipulative experiment. The do-operator changes the graph, closing the backdoors.\nThe do-operator defines a causal relationship, because Pr(Y|do(X)) tells us the expected result of manipulating X on Y, given a causal graph.\n\nWe might say that some variable X is a cause of Y when Pr(Y|do(X)) &gt; Pr(Y|do(not-X)).\n\n(makes more sense to me with a binary outcome, Pr(Y = 1|do(X), but maybe Y as a continuous variable can be defined a subset. …I dunno)\n\n\nThe ordinary conditional probability comparison, Pr(Y|X) &gt; Pr(Y|not-X), is not the same. It does not close the backdoor.\nNote that what the do-operator gives you is not just the direct causal effect. It is the total causal effect through all forward paths.\n\nTo get a direct causal effect, you might have to close more backdoors.\n\nThe do-operator can also be used to derive causal inference strategies even when some backdoors cannot be closed.\n\n\nCounterfactual\n\nassociated action: Imagining - what would be the outcome if the alternative would’ve happened.\nIndividual Causal Effects can be calculated but it requires stronger assumptions and deeper understanding of the causal mechanisms\n\nNeed to research this part further.\nIf the underlying SCM is linear then the ICE = ACE.\n\n\n\nA collider along a path blocks that path. However, conditioning on a collider (or any of its descendants) unblocks that path\n\nWhen a collider is conditioned upon, the change in the association between the two nodes it separates is called collider bias.\n\ne.g. if Z is a collider between X and Y, conditioning upon Z will induce an association between X and Y.\n\n\nA conditioning set, \\(L\\), is the set of nodes we condition on (it can be empty).\nConfounding is the situation where a (possibly unobserved) common cause obscures the causal relationship between two or more variables.\n\nThere is more than one causal path between two nodes.\nA causal effect of X on Y is confounded if  \nCollider bias is a type of confounding. When a collider is controlled for, a second (or more) path opens, and the effect is confounded\n\nX and Y are d-separated by [L if conditioning on all members in [L blocks all paths between the nodes, X and Y.\n\nTool for checking the conditional independencies which are visualized in DAGs.\n\nA descendant is a node connected to a parent node by that parent node’s outgoing arrow.\nFrontdoor Adjustment - In a causal chain with three nodes X→Z→Y, we can estimate the effect of X on Y indirectly by combining two distinct quantities: (Useful for when unobserved confounders prevent direct causal estimation)\n\nThe estimate of the effect of X on Z, P(Z|do(X))\nThe estimate of the effect of Z on Y, P(Y|do(Z), X)\n\nFrontdoor Path - a path that exits a causal variable in a DAG rather than enters it.\n\ne.g. the path that connects a causal variable, X, to an outcome variable, Y, has an arrow that points from X to Y.\n\nMarkov Equivalence - A set of DAGs, each with the same conditional independencies\nMediation Analysis - seeks to identify and explain the mechanism or process that underlies an observed relationship between an independent variable and a dependent variable via the inclusion of a third hypothetical variable, known as a mediator variable (z-variable in the DAGs of “pipes” below)\n\nIncluding a mediator and the independent variable in a regression will result in the independent variable not being signficant and the mediator being significant.\n\nModeration Analysis - Like mediation analysis, it allows you to test for the influence of a third variable, Z, on the relationship between variables X and Y, but rather than testing a causal link between these other variables, moderation tests for when or under what conditions an effect occurs.\nA node is a parent of another node if it has an outgoing arrow to that node\nA path from X to Y is a sequence of nodes and edges such that the start and end nodes are X and Y, respectively.\nResidual Confounding occurs when a confounding variable is measured imperfectly or with some error and the adjustment using this imperfect measure does not completely remove the effect of the confounding variable.\n\nExample: Women who smoke during pregnancy have a decreased risk of having a Down syndrome birth.\n\nThis is puzzling, as smoking is not often thought of as a good thing to do. Should we ask women to start smoking during pregnancy?\nIt turns out that there is a relationship between age and smoking during pregnancy, with younger women being more likely to indulge in this bad habit. Younger women are also less likely to give birth to a child with Down syndrome. When you adjust the model relating smoking and Down syndrome for the important covariate of age, then the effect of smoking disappears. But when you make the adjustment using a binary variable (age&lt;35 years, age &gt;=35 years), the protective effect of smoking appears to remain.\n\n\nStructural Causal Models (SCMs) - relate causal and probabilistic statements; each equation is a causal statement\n\n\n\n“:=” is the assignment operator\nX is a direct cause of Y which it influences through the function f( )\n\nwhere f is a statistical model\n\nThe noise variables, ϵX and ϵY, are assumed to be independent.\n\nThere are Stochastic and Deterministic SCMs. Deterministic SCMs presented in article.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-assoc",
    "href": "qmd/causal-inference.html#sec-causinf-assoc",
    "title": "Causal Inference",
    "section": "Association",
    "text": "Association\n\n\n\nFar left: lm(Y ~ X); X and Y show a linear correlation when Z is NOT conditioned upon\nLeft: lm(Y ~ X + Z); X and Y show NO linear correlation when Z is conditioned upon\nRight: lm(Y ~ X); X and Y show NO linear correlation when Z is NOT conditioned upon\nFar Right:  lm(Y ~ X + Z); X and Y show a linear correlation when Z is conditioned upon",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-singpath",
    "href": "qmd/causal-inference.html#sec-causinf-singpath",
    "title": "Causal Inference",
    "section": "Single path DAGs",
    "text": "Single path DAGs\n\n\nFor each of these DAGs, Z would be the only member of the conditioning set.\nThe first 3 DAGs represent the scatter plots above\n\nZ only blocks the path between X and Y when it’s conditioned upon.\n\nX and Y are associated (e.g. linear correlation, mutual information, etc.) when Z is ignored\nConditioning on Z results in X and Y no longer being associated (i.e. conditional independence)\n\nThe first and second DAGs are elemental confounds or relations called “Pipes.”\n\nThe left one\n\nIn general, DO NOT add these variables to your model\n\nThese paths are causal so they shouldn’t be blocked\nIf your goal isn’t causal inference, then adding these variables might provide predictive information\ne.g. If there was a causal arrow from X to Y, the far left DAG would NOT have a backdoor path and therefore Z would not  be conditioned upon to block the path, X-Z-Y\n\nThe path from X to Z is a frontdoor path since the arrow exits X.\n\n\nSometimes you DO condition on these variables\n\nDuring mediation analysis, you condition on these variables as part of the process to determine how much of the effect goes through Z.\nThe mediation path can have an important interpretation depending on your research question\n\ne.g. indirect descrimination\n\nSee Statistical Rethinking &gt;&gt; Chapter 11 &gt;&gt; Conclusion of Berkeley Admissions example\n\nalso Lecture 9 2022 video\n\n\n\n\n\nThe right one is a backdoor path and should be conditioned on.\nEverything you can learn about Y from X (or vice versa) happens through Z, therefore learning about X separately provides no additional information\nZ is traditionally labelled a mediator\n\nThe third DAG is an elemental confound  or relation called a “Fork.”\n\nIn general, add these variables to your model\nThese are backdoor paths and are NOT causal\nX and Y have a common cause in Z and some of the mutual information about Z they each contain, overlaps, and creates an association (when Z isn’t conditioned upon).\n\n\nThe fourth DAG is an elemental confound or relation called a “Collider.”\n\n\nIn general, do NOT add these variables to your model\nZ blocks the path between X and Y unless conditioned upon.\nAn association between X and Y is induced  by conditioning on Z, lm(Y ~ X + Z)\n\nX and Y are independent causes of Z. Z contains information about both X and Y, but X doesn’t contain any information about Y and vice versa.\nA small X and a sufficiently large Y (and vice versa) can produce a Z = 1. So X and Y have compensatory relationship in causing Z.\n\ni.e. For a given value of Z, learning something about X tells us what Y might have been.\n\n\n\nThe last elemental confound or relation is called a “Descendent.”\n\n\nConditioning on a descendent variable, D, is like conditioning on the variable, Z itself, but weaker. A descendent is a variable influenced by another variable.\nControlling for D will also control, to a lesser extent, for Z. The reason is that D has some information about Z. This will (partially) open the path from X to Y, because Z is a collider. The same holds for non-colliders. If you condition on a descendent of Z in the pipe, it’ll still be like (weakly) closing the pipe.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-dualpath",
    "href": "qmd/causal-inference.html#sec-causinf-dualpath",
    "title": "Causal Inference",
    "section": "Dual path DAGs",
    "text": "Dual path DAGs\n\n\nCausal paths do not flow against arrows but associations can.\nTwo examples of DAGs representing confounding\n\nThese are the 2 middle DAGs above with an additional path from X to Y\nIf Z is NOT conditioned on (i.e. top path is not blocked), then the causal effect of X on Y would be confounded.\n\n\n\n\nThe paths from X to Y:\n\nThe path through Z matches the first DAG.\n\nTherefore X and Y are conditionally independent given Z.\n\nThe path through W matches the fourth DAG\n\nTherefore X and Y are conditionally dependent given W.\n\n\nThe path through W (collider) is blocked unless W is conditioned upon\nThe path through Z is open unless Z is conditioned upon\nIf Z and W are conditioned upon, then the path between X and Y is open through W and an association is present.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-interv",
    "href": "qmd/causal-inference.html#sec-causinf-interv",
    "title": "Causal Inference",
    "section": "Intervention",
    "text": "Intervention\n\n\nSince actual interventions are usually unfeasible, we want to be able to determine causality with observational data. This requires two assumptions:\n\nThe intervention occurs locally. Which means that only the variable we target is the one that receives the intervention.\nThe mechanism by which variables interact do not change through interventions; that is, the mechanism by which a cause brings about its effects does not change whether this occurs naturally or by intervention\n\nThe Doing row of DAGs (aka manipulated DAGs) represents setting X = x\n\nFor DAGs 1 and 4, Y is still affected\n\nMoving from seeing to doing didn’t change anything\n\n\nFor DAGs 2 and 3, Y is now UNaffected\n\nUsing the assumptions and some mathematical manipulation (See article for details):\n\n\n\nThus, the interventional distribution we care about is equal to the (observational) conditional distribution of Y given X when we adjust for Z\n\n\n\n\nThe rule: After an intervention, incoming arrows are cut from the node where the intervention took place.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-confound",
    "href": "qmd/causal-inference.html#sec-causinf-confound",
    "title": "Causal Inference",
    "section": "Confounding",
    "text": "Confounding\n\n\nThe backdoor criterion tells us which variable we need to adjust for in order to for our model to yield a causal relationship between two variables (i.e. graphically, nodes)\n\nBlocks all spurious, that is, non-causal paths between X and Y.\nLeaves all directed paths from X to Y unblocked\nCreates no spurious paths\n\nExample\n\nCausal effect of Z on U is confounded by X because in addition to the legitimate causal path Z→Y→W→U, there is also an unblocked path Z←X→W→U which confounds the causal effect\n\nSince X’s arrow enters the causal variable of interest, Z, it’s arrow is a backdoor path and needs to be blocked/closed\nThere are some descendant nodes that make the confounding a little difficult to parse out, but this graph is essentially\n\n\nwhich is the same as the second example DAG for confounding in the Association section\n\n\nThe backdoor criterion would have us condition on X, which blocks the spurious path and renders the causal effect of Z on U unconfounded.\n\nThe reduced, confounding DAG above is the same as the third DAG (without the path from Z to U) in the Association section. Conditioning on Z in that example blocked the path between X and Y, so it makes sense that conditioning on X in the reduced DAG would block the Z to X to U path. And therefore, the Z←X→W→U would also be blocked in the complete DAG.\n\nNote that conditioning on W would also block this spurious path; however, it would also block the causal path, Z→Y→W→U.\n\n\nIf we breakdown the complete DAG into the modular components involving W, we can see these are the same as the first example DAG in the Association section.\nW is also collider for X and Y, but I don’t think that has any bearing when discussing the causal effect of Z on U.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-appsimp",
    "href": "qmd/causal-inference.html#sec-causinf-appsimp",
    "title": "Causal Inference",
    "section": "Application: Simpson’s Paradox Example",
    "text": "Application: Simpson’s Paradox Example\n\nSex as the adjustment variable           \n\nPatients CHOOSE whether or not to take a drug to cure some disease.\nMen choosing to take the drug recover at a higher percentage that those that didn’t\nWomen choosing to take the drug recover at a higher percentage that those that didn’t\nBut overall, those that chose to take the drug recovered at a lower percentage than those that didn’t.\nSo should a doctor prescribe the drug or not?\nSuppose we know that women are more likely to take the drug, that being a woman has an effect on recovery more generally, and that the drug has an effect on recovery. \nCreate DAGs\n\n\nS=1 as being female,\nD=1 as having chosen to take the drug\nR=1 as having recovered\nThe right DAG indicates either forcing everyone to either take the drug or not take the drug\nNotice that   therefore our calculated effect will be confounded.\n\nBackdoor criterion says the manipulated DAG (right) will correspond to the observational DAG (left) if we condition on Sex.\n\n\nUse intervention formula from Intervention section\n\n\nAverage Causal Effect = 0.832 - 0.782 = 0.050. So the drug has a positive effect on average.\n\n\nBlood Pressure as the adjustment variable \n\nBlood Pressure instead of sex is used as the adjustment. Blood Pressure is a post-treatment variable.\nRelatively same observations as before. High or Low Blood Pressure with the drug produces better results than those that chose not to take the drug. Yet overall, those that chose the drug recovered at a lower percentage.\n\nSince Blood Pressure (B) is post-treatment, it has no effect on whether the patient takes the drug or not (D).\nTaking or not taking the drug (D) has an indirect effect on recovery (R) through Blood Pressure (B) along with a direct effect.   so our calculated effect will be unconfounded.\n\nSo with BP as the adjustment variable, the drug now has a small, negative effect (harmful), 0.78 - 0.83 = -0.05\n\nThe unconfounded, average causal effect for the population is negative, therefore the doctor should NOT prescribe the drug.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-scms",
    "href": "qmd/causal-inference.html#sec-causinf-scms",
    "title": "Causal Inference",
    "section": "Structural Causal Models (SCMs)",
    "text": "Structural Causal Models (SCMs)\n\nYou add additional assumptions to your DAG to derive a causal estimator\n“Full Luxury” Bayesian approach\n\n“Full Luxury” is just a term coined by McElreath; it’s just a bayesian model but bayesian models can fully model a DAG where standard regression approachs can fail (see examples)\nNo other approach will find something that the bayesian approach doesn’t\n\nMain disadvantage is that it can be computationally intensive (same with all baysian models)\n\nProvides ways to add “causes” for missingness and measurement error\n\nExample (2 Moms)\n\nNotes from McElreath video\nHypothesis: a mother’s family size is causal to her daughter’s family size\n\nTruth: no relationship\n\nVariables:\n\nM - Mother’s family size (i.e. number of children the birth)\nD - Daughter’s family size\nB1 - Mother’s birth order; binary, first born or not\nB2 - Daughters’ birth order; binary, first born or not\nU - unobserved confounds  (shown as curved dotted line)\n\n\n\nUnobserved confounds (economic status, education, cultural background, etc.) are causal to both Mother and Daughter (curved dotted line) which makes regression, D ~ M, impossible\n\nSee Baysian Two Moms example below for results of a typical regression\nStill possible to calculate the effect of M on D with SCMs\n\n\nAssumptions: Relationships are linear (i.e. linear system)\nCausal Effects\n\n\nWe want m which is the causal effect of M on D\nAssumes causal effect of birth order is the same on mother and daughter\nAside: There is no arrow/coefficient from M to B2 because it’s not germane to the calculation of m\n\nCalculate linear effect (i.e. regression coefficient) without a regression model using a linear system of equations\n\nNote: a regression coefficent, β = cov(X,Y) / var(X)\nWe can’t calculate the covariance of M and D directly because it depends on unobserved confounders but we can calculate the covariance between B1 and D and use that to get m.\nThe covariance for each path is the product of the path coefficients and the variance of the originating causal variable.\nPath B1 → M: cov(B1, M) = b*var(B1)\nPath B1 → D: cov(B1, D) = b*m*var(B1)\n2 equations and 2 unknowns, m and b\nSolve for b in the first equation, substitute b into the second equation, and solve for m\n\nm = cov(B1, D) / cov(B1, M)\n\nStill need an uncertainty of this value (e.g. bootstrap)\n\n\nExample (Bayesian 2 Moms)\n\nSee previous example for link, hypothesis, and definition of the variables\n\nFunctions (right side)\n\nEach variable’s function’s inputs are variables that are causal influences (i.e. have arrows pointing at the particular variable\n\ne.g. M has two arrows pointing at it in the DAG: B1 and u\n\n\nCode\n\nThe assumption is that this is a lineary system, so M and D have Normal distributions for their functions with means as linear regression equations\nB1 and B2 are binary so they get bernoulli distributions\nU gets a standard normal prior\n\nAside: evidently this is a typical prior for latent variables in psychology\n\np, intercepts, sd, k get typical priors for bayesian regressions\n\nResults\n\n\nTruth: no effect\n1st 3 lm models shows how the unobserved confound biases the estimate when using a typical regression model to estimate the causal effect\n\nIncluding B2 adds precision to the biased estimate since it is causal to the outcome D while adding B1 increases the bias\n\nBayesian model isn’t fooled because U is specified as an input to the functions for M and D\n\nInterpretation: There is no reliable estimate of an effect. The most likely effect is a moderately positive one but it could also be negative.\nAdding more simulated data to this example will move the point estimate towards zero\n\n\n\nExample (Bayesian Peer Bias)\n\nAlso see Causal Design &gt;&gt; Bad Adjustment Variables &gt;&gt; Peer Bias\nHypothesis: racial discrimination in acceptance of applicatioon to Berkeley grad schools\n\nTruth: moderate negative effect, -0.8\n\nVariables:\n\nX is race, E is department, Q is an unobserved confound (latent variable: student quality), Y is binary; Admission/No Admission\nR1 and R2 are proxy variables for Q (e.g. test scores, lab work, extracurriculars, etc.)\n\nAssumptions: System is linear\nDAG and Code\n\n\nXX is the race variable with X as the coefficient in the code\n\nThis code uses his {rethinking} package so some of this syntax is unfamiliar\n\nR1 and R2 are shown in the DAG to be influenced by student quality, Q\nEvery prior is normal except for Q’s coefficient\n\nResults\n\n\nTruth: -0.8\n1st 3 glm models shows how the unobserved confound, Q, biases the estimate when using a typical logistic regression model to estimate the causal effect\nBayesian model isn’t fooled because Q is specified as an input to the function for Y\n\nInterpretation: There is a reliably negative effect (no 0 in the CI). The most likely effect is a moderately negative one.\nNot quite equal to the truth but reliably negative and the point estimate is closer than the glms\n\n\n\nExample\n\nAssumptions: Relationships between variables are linear and error terms are independent\nEquations\n\n,  \n\n\nDAG 1 (left) shows the association DAG which represents the SCM\nmanipulated DAG 1 (middle) shows intervention where z is set to a constant\n\nincoming causal arrows get cutoff the intervening variable\n\nmanipulated DAG 1 (right) shows intervention where x is set to a constant\n\nSimulation of the SCM (n = 1000) (code in article)\n\n\nZ is more predictive of Y than X\n\nSimulate interventions (code in article)\n\n\nLeft - histogram of SCM for Y without an intervention\nMiddle - Intervention on Z\n\nconfirms the DAG which shows no effect on Y and Z is not causal\n\nRight - intervention on X\n\nconfirms the DAG which shows an intervention on X produces an effect on Y and X is causal\n\nAverage Causal Effect (ACE) can be determined by subtracting the expected values of interventions where  X = x +1 and  X = x",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-ctrfact",
    "href": "qmd/causal-inference.html#sec-causinf-ctrfact",
    "title": "Causal Inference",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nExample(code in article): Test whether Grandma’s home remedy can speed recovery time for the common cold\n\nSCM\n\n\nT is 1/0, i.e. whether patient receives Grandma’s treatment, with p = 0.5; \nR is recovery time\nμ is the intercept\nβ is the average causal effect, since\n\n\nwhere \n\n\nFrom fitting the model, we find μ = 7, β = -2, Τ = 0, ε1 = 0.78\n\nTherefore, the Individual Causal Effect for patient 1\n\n\nJust plug and chug where we substitute T = 1 into the SCM and we already have the T = 0 part from the model\n\n\nIn this case, the SCM is linear, so the ICE = ACE.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-medanal",
    "href": "qmd/causal-inference.html#sec-causinf-medanal",
    "title": "Causal Inference",
    "section": "Mediation Analysis",
    "text": "Mediation Analysis\n\n\nFigure\n\nc’ is the direct effect of X on the outcome after the indirect path has been removed (i.e. conditioned upon, outcome ~ X + mediator)\nc is the to total effect (outcome ~ X)\nc - c’ equals the indirect effect\nSee definitions below\n\nAllows you to test for the influence of a third variable, the mediator, on the relationship between (i.e. effect of) the treatment variable, X, and the Outcome variable, Y.\nMisc\n\nNotes from: Mediation Models\n\nOverview of packages (Aug 2020)\n\n{brms} very flexible in terms of models. You’ll just have to calculate the effects by hand unless some outside package (e.g. sjstats) takes a brms model and does it for you.\n\nSee below for formulas. {mediation} papers should have other formulas for other types of models (e.g. poisson, binomial)\n\n{mediation} handles a lot for you. Method isn’t bayesian but is very similar to it in a frequentist-bootstrappy-simulation way.\n\nPackage has been substantially updated since that article was written.\n\n\nAlso see\n\nOther Articles &gt;&gt; Frontdoor Adjustment\nStatistical Rethinking &gt;&gt; Chapter 11 &gt;&gt; Conclusion of Berkeley Admissions example\n\nalso Lecture 9 2022 video\n\nebook (w/brms) Introduction to Mediation, Moderation, and Conditional Process Analysis\n\nIncluding a mediator and the independent variable in a regression will result in the independent variable not being signficant and the mediator being significant.\n\nExample: Causal effect of education on income\n\nSay occupation is your mediator. Education has a big impact on your occupation, which in turn has a big impact on your income. You don’t want to control for a mediator if you are interested in the full effect of X on Y! Because a huge part of how X impacts on Y is precisely through the mediation of C, in our case choice of and access to occupation, given a certain level of education. If you ‘control’ for occupation you will be greatly underestimating the importance of education.\n\n\nWhen would you want to only measure the Direct Effect?\n\nExample: Determining the amount of remuneration for discrimination\n\nFrom Simulating confounders, colliders and mediators\nVariables\n\nOutcome: Pay Gap\nTreatment: Gender\n\nIn this case, this variable is actually “gender discrimination in the current workplace in making a pay decision” (for which we use actual, observed Gender as a proxy)\n\nMediators: Occupation and Experience\n\nWhen determining whether a type of descrimination exists, you don’t want to condtion on the mediators, because the effect of gender will be underestimated. So, you’d want the total effect. But here, discrimation is already determined and Gender is now a proxy variable. Under Gender’s new definition, Occupation and Experience might influence the amount of “gender discrimiation,” so they can’t be definitively labelled mediators any more.\nSo if you want to estimate that final “equal pay for equal work” step of the chain then yes it is legitimate to control for occupation and experience.\n\n\nShould always compare a mediation model to a model without mediation\n\nAn unnecessary mediation model will almost certainly be weaker and probably more confusing than the model you would otherwise have.\n\nAverage Causal Mediation Effect (ACME) (aka Indirect Effect)- the expected difference in the potential outcome when the mediator took the value that it would have under the treatment condition as opposed to the control condition, while the treatment status itself is held constant.\n\nIf this isn’t significant, there isn’t a mediation effect\nIt is possible that the ACME takes different values depending on the baseline treatment status. Shown by analyzing the interaction between the treatment variable and the mediator\nδ(t) = E[Y (t, M(t1)) − Y (t, M(t0))]\n\nwhere\n\nt, t1, t0 are particular values of the treatment T such that t1 ≠ t0,\nM(t) is the potential mediator\nY (t, m) is the potential outcome variable\n\n\n\nAverage Direct Effect (ADE) - the expected difference in the potential outcome when the treatment is changed but the mediator is held constant at the value that it would have if the treatment equals t.\n\nζ(t) = E[Y (t1, M(t)) − Y (t0, M(t))]\n\nThe Total Effect of the treatment on the outcome is ACME + ADE.\n\nConditions where you likely do NOT need mediation analysis :\n\nIf you cannot think of your model in temporal or physical terms, such that X necessarily leads to the mediator, which then necessarily leads to the outcome.\nIf you could see the arrows going either direction.\nIf when describing your model, everyone thinks you’re talking about an interaction (a.k.a. moderation).\nIf there is NO strong correlation between key variables (variables of interest) and mediator, and if there is NO strong correlation between mediator and the outcome.\n\nSobel test - tests whether the suspected mediator’s influence on the independent variable is significant.\n\nPerforming the test in R via bda::mediation.test - article\n\nMethods\n\nBaron & Kenny’s (1986) 4-step indirect effect method has low power\nProduct-of-Paths (or difference in coefficients)\n\nc - c’ = a*b (see figure at start of this section) where c - c’ is the indirect effect (aka ACME)\n\nif either a or b are nearly zero, then the indirect effect can only be nearly zero\nFormula only appropriate for the analysis of causal mediation effects when both the mediator and outcome models are linear regressions where treatment (IV) and moderator enter the models additively (e.g. without interaction)\n\nEffect formulas for models with an interaction between treatment and moderator (Paper)\n\nmediator: M = α2 + β2Ti + ξT2Xi + εi2(T~i`)\noutcome: Y = α~3 + β3Ti + γMi + κTiMi + ξT3Xi + εi3(Ti, Mi)\nACME = β2(γ + κt) where t = 0,1\nADE = β3 + κ{α2 + β2t + ξT2Ε(Xi)}\nATE = β2γ + β3 +κ{α2 + β2 + ξT2Ε(Xi)}\n\nAlternatively, fit Y = α1 + β1Ti + ξT1Xi + ηTTiXi + εi1\n\nThen ATE = β1 + ηTE(Xi)\n\n\nNotes\n\nVariables\n\nT is treatment, M is mediator, X is a set of adjustment variables\n\nThe exponentiated T in ξT is to let you know it can be a set of coefficients for a set of adjustment variables (I guess)\n\n\nCouldn’t figure out why curly braces are being used\nACME with have two estimates (t=0, t=1)\nATE (average total effect)\nΕ(Xi) is the sample average of each adjustment variable and it’s multiplied by its associated ξ2 coefficient\nSee paper for other types of models\n\n\n{lavaan}, {brms}\n\nTingley, Yamamoto, Hirose, Keele, & Imai, 2014\n\nQuasi-bayesian approach (paper ,esp Appendix D, for details)\n\nFits the mediation and outcome models (see 1st example)\nTakes the coefficients and vcov matrices from both models\n\nUses the coefs (means) and vcovs (variances) as inputs to a mvnorm function to simulate distributions for the coefficients.\nI do not understand what these are used for… would have to look at the code.\n\nSamples predictions of each model K times for treatment = 1, then for treatment = 0\nCalcs difference between predictions for each set of samples, then averages to get the ACME\n\nAssumes Sequential Ignorability\n\nRequires treatment randomization or an equivalent assignment mechanism\nmediator is also ignorable given the observed treatment and pre-treatment confounders. This additional assumption is quite strong because it excludes the existence of (measured or unmeasured) post-treatment confounders as well as that of unmeasured pretreatment confounders. This assumption, therefore, rules out the possibility of multiple mediators that are causally related to each other (see Section 6 for the method that is designed to deal with such a scenario).\nCan’t be tested but a sensitivity analysis can be conducted using mediation::medsens (see vignette)\n\n{mediation} (vignette)\n\nMultiple types of models for both mediator and outcome\n\nincluding multilevel model functions from {lme4} supported\n\nMethods for:\n\n‘moderated’ mediation\n\nthe magnitude of the ACME depends on (or is moderated by) a pre-treatment covariate. Such a pre-treatment covariate is called a moderator. (see Moderator Analysis)\nACME can depend on treatment status (i.e. interaction between treatment and mediator), but this situation is talking about a separate variable moderating the effect of the treatment on the mediator.\n\nmultiple mediators (which violates sequential ingnorability but can be handled)\nvarious experimental designs (e.g. parallel, crossover)\ntreatment non-compliance\n\nUses MASS (so may have conflicts with dplyr)\nNo latent variable capabilities\n\n\nEtsy article calculates generalized average causal mediation effect (GACME) and generalized average direct effect (GADE) and uses a known mediator to measure the direct causal effect even when the DAG has multiple unknown mediators (paper, video, R code linked in article)\n\nExample: Tingley, 2014 Method\n\nEquations\n\n\n\nPredictions for “job_seek” in the mediator model (top) are used as predictor values in the outcome model (bottom).\n\nData: data(jobs, package = 'mediation')\n\ndepress2: outcome, numeric: Measure of depressive symptoms post-treatment. The outcome variable.\ntreat: treatment, binary: whether participant was randomly selected for the JOBS II training program.\n\n1 = assignment to participation.\n\njob_seek: mediator, ordinal: measures the level of job-search self-efficacy with values from 1 to 5.\necon_hard: adjustment, ordinal: Level of economic hardship pre-treatment with values from 1 to 5.\nsex: adjustment, binary: 1 = female\nage: adjustment, numeric: Age in years\n\n{mediation}\nmodel_mediator &lt;- lm(job_seek ~ treat + econ_hard + sex + age, data = jobs)\nmodel_outcome  &lt;- lm(depress2 ~ treat + econ_hard + sex + age + job_seek, data = jobs)\n\n# Estimation via quasi-Bayesian approximation \nmediation_result &lt;- mediate(\n  model_mediator, \n  model_outcome, \n  sims = 500,\n  treat = \"treat\",\n  mediator = \"job_seek\"\n)\n\nSummary - summary(mediation_result)\n\n\nerror bar plot also available via plot(mediation_result)\nSays ACME isn’t significant, therefore no mediation effect detected.\n“Prop Mediated” is supposed to be the ratio of the indirect effect to the total.\n\nHowever this is not a proportion, and can even be negative, and so “it is mostly a meaningless number.”\n\n\n\n\nExample: product-of-paths (or difference in coefficients)\n\n{lavaan}\nsem_model = '\n  job_seek ~ a*treat + econ_hard + sex + age\n  depress2 ~ c*treat + econ_hard + sex + age + b*job_seek\n  # direct effect\n  direct := c\n  # indirect effect\n  indirect := a*b\n  # total effect\n  total := c + (a*b)\n'\nmodel_sem = sem(sem_model, data=jobs, se='boot', bootstrap=500)\nsummary(model_sem, rsq=T)  # compare with ACME in mediation\nDefined Parameters:\n                  Estimate  Std.Err  z-value  P(&gt;|z|)\n    direct          -0.040    0.045  -0.904    0.366\n    indirect        -0.016    0.012  -1.324    0.185\n    total            -0.056    0.046  -1.224    0.221\n\nAlso outputs the typical summary regression estimates, std.errors, pvals, R2 etc.\nBootstraps std.errors\nSame results for “indirect” here as with {mediation} ACME estimate\nR2s are poor for both regression models which could be why no mediation effect is detected.\n\n{brms}\nmodel_mediator &lt;- bf(job_seek ~ treat + econ_hard + sex + age)\nmodel_outcome  &lt;- bf(depress2 ~ treat + job_seek + econ_hard + sex + age)\nmed_result = brm(\n  model_mediator + model_outcome + set_rescor(FALSE), \n  data = jobs\n)\nsummary(med_result) # regression results\n# using brms we can calculate the indirect effect as follows\nhypothesis(med_result, 'jobseek_treat*depress2_job_seek = 0')\n\nExact same brms syntax (except priors are specified) as in Statistical Rethinking &gt;&gt; Chapter 5 &gt;&gt; Counterfactual Plots\nExample has a mediator DAG as well.\nhypothesis tests H0: a*b == 0\n\npval &lt; 0.05 says there is a mediation effect.\n\n\n{sjstats}\n\nsjstats::mediation(med_result) %&gt;% kable_df()\n\nmediator (b): the effect of “job_seek” on “depress2”\nindirect (c-c’): ACME\ndirect (c’): ADE\nproportion mediated: See {mediation} example",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-modanal",
    "href": "qmd/causal-inference.html#sec-causinf-modanal",
    "title": "Causal Inference",
    "section": "Moderation Analysis",
    "text": "Moderation Analysis\n\n\nMisc\n\nAlso see Introduction to Mediation, Moderation, and Conditional Process Analysis\n\nLike mediation analysis, it allows you to test for the influence of a third variable, Z (moderator), on the relationship between variables X and Y, but rather than testing a causal link between these other variables, moderation tests for when or under what conditions an effect occurs.\n\nModerators are conceptually different from mediators (“when” (moderator) vs “how/why” (mediator)).\n\nThere can be moderated mediation effect though. (see Mediation Analysis &gt;&gt; Methods &gt;&gt; {mediation})\n\nModerators can stengthen, weaken, or reverse the nature of a relationship.\nSome variables may be a moderator or a mediator depending on your question.\n\nAssumption: assumes that there is little to no measurement error in the moderator variable and that the DV did not CAUSE the moderator.\n\nIf moderator error is likely to be high, researchers should collect multiple indicators of the construct and use SEM to estimate latent variables.\nThe safest ways to make sure your moderator is not caused by your DV are to experimentally manipulate the variable or collect the measurement of your moderator before you introduce your IV.\n\nModeration can be tested by interacting variables of interest (moderator x IV) and plotting the simple slopes of the interaction, if present.\n\nSee Regression, Interactions for simple slopes/effects analysis\nMean center both your moderator and your IV to reduce multicolinearity and make interpretation easier. (“c” in variable names indicates variable was centered)\n\nExample: academic self-efficacy (moderator)(confidence in own’s ability to do well in school) moderates the relationship between task importance (independent variable (IV)) and the amount of test anxiety (outcome) a student feels (Nie, Lau, & Liau, 2011).\n\nStudents with high self-efficacy experience less anxiety on important tests (task importance) than students with low self-efficacy while all students feel relatively low anxiety for less important tests.\nSelf-efficacy (Z) is considered a moderator in this case because it interacts with task importance (X), creating a different effect on test anxiety (Y) at different levels of task importance.\n\nExample: What is the relationship between the number of hours of sleep (X, independent variable (IV)) a graduate student receives and the attention that they pay to this tutorial (Y, outcome) and is this relationship influenced by their consumption of coffee (Z, moderator)\nmod &lt;- lm(Y ~ Xc + Zc + Xc*Zc)\nsummary(mod)\n## Coefficients:\n##            Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept) 48.54443    1.17286  41.390  &lt; 2e-16 ***\n## Xc          5.20812    0.34870  14.936  &lt; 2e-16 ***\n## Zc          1.10443    0.15537  7.108 2.08e-10 ***\n## Xc:Zc        0.23384    0.04134  5.656 1.59e-07 ***\n\nSince we have significant interactions in this model, there is no need to interpret the separate main effects of either our IV or our moderator\nPlot the simple slopes (1 SD above and 1 SD below the mean) of the moderating effect\n\n\nFor details on this plot and analysis, see Regression, Interactions &gt;&gt; OLS &gt;&gt; numeric:numeric &gt;&gt; Calculate simple slopes for the IV at 3 representative values for the moderator variable\nInterpretation\n\nThose who drank less coffee (moderator, black line) paid more attention (outcome) with the more sleep (IV) that they got last night but paid less attention overall than average (the red line).\nThose who drank more coffee (moderator, green line) paid more attention (outcome) when they slept more (IV) as well and paid more attention than average.\nThe difference in the slopes for those who drank more or less coffee (moderator) shows that coffee consumption moderates the relationship between hours of sleep and attention paid",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-sr",
    "href": "qmd/causal-inference.html#sec-causinf-sr",
    "title": "Causal Inference",
    "section": "Statistical Rethinking",
    "text": "Statistical Rethinking\n\nMisc\n\nArrows indicate directions of influence\nArrows in DAGs “create” correlations\n\ni.e. if arrow, then correlation\nThe direction it points determines whether its association is causal or not.\n\nUnlike a statistical model, a DAG, if it is correct, will tell you the consequences of intervening to change a variable.\n** The data alone can never tell us when a DAG is right. But the data can tell us when a DAG is wrong. **\nMany dynamical systems cannot be usefully represented by DAGs, because they have complex behavior that is sensitive to initial conditions. But these models can still be analyzed and causal interventions designed from them.\nA DAG path means any series of variables you could walk through to get from one variable to another, ignoring the directions of the arrows.\nThe variable, U, in DAGs represents one or more unobserved variables\n\nUsually has circle around the U or is just represented by a dashed line\n\n“Conditioned upon,” “adjusted for,” or “controlled for” is all the same thing\n“a” or “α” is used in bayesian formulas to represent the intercept\nNotation\n\nX is not independent of Y, i.e \nconditional independence: Y is not associated with some variable X, after conditioning on some other variable Z, i.e. \n\nthey are statements of which variables should be associated with one another (or not) in the data.\nthey are statements of which variables become dis-associated when we condition on some other set of variables.\nThere is no other path of influence from X to Y except through Z\n\n\n(Total ) Causal Effect and Direct Causal Effect\n\n\nWeight (W) is the outcome, Height (H) and Sex (S) are explanatory\n(Total) Causal Effect is simply, W ~ S\nDirect Causal Effect shuts the backdoor paths, W ~ S + H\n\nSometimes we want the total causal effect and not the direct causal effect. (e.g. if H is a post-treatment variable, see SR, Ch.6)\n\n\n\n\n\nTestable Implications\n\nDiffering associations between plausible DAGs that are testable through statistical models\nAny DAG may imply that some variables are independent of others under certain conditions.\nNO conditional independencies → NO testable implications\nwww.dagitty.net - Enter DAG and it will give you the Adjustment Set and Testable Implications\nExample\n\nQuestion: What is the causal relationship between Divorce Rate (D), Marriage Rate (M), and Median Age at Marriage (A)\nData:\n\n2 regressions are fit\n\nD ~ α + βM\n\nShows that M is positively correlated with D\n\nD ~ α + βA\n\nShows that A is negatively correlated with D\n\n\n\nPlausible DAGs (note: marriage cannot influence your age… technically)\n\n\n\nA directly influences D\nM directly influences D\nA directly influences M\nReasoning: First, Age can have a direct effect, perhaps because younger people change faster than older people and are therefore more likely to grow incompatible with a partner. Second, it can have an indirect effect by influencing the marriage rate. If people get married earlier, then the marriage rate may rise, because there are more young people. Consider for example if an evil dictator forced everyone to marry at age 65. Since a smaller fraction of the population lives to 65 than to 25, forcing delayed marriage will also reduce the marriage rate. If marriage rate itself has any direct effect on divorce, maybe by making marriage more or less normative, then some of that direct effect could be the indirect effect of age at marriage.\n\n\n\nSimilar to 1 except M does not directly influence D\nReasoning This DAG is plausible even though there’s a correlation between M and D (regression 1). It could be that M derives it’s correlation with D through it’s association with A.\n\nThe direction of influence doesn’t prevent a correlation between M and D\n\n\n\nTestable implications\n\nDAG 1\n\nThe DAG shows all three are associated to each other, i.e. \nIt would be natural to think about measuring correlation and if a pair shows no correlation you could discard the DAG, but it is NOT a good test since there are many ways two variables can show correlation yet not be directly associated. (see reasoning under DAG 2 above and under DAG2 below)\nDAG1 has NO conditional independencies and therefore, NO testable implications\n\nDAG 2\n\nThis DAG also shows all three variables are associated with each other.\nD and M are associated with one another, because A influences them both. They share a cause, and this leads them to be correlated with one another through that cause. But suppose we condition on A. All of the information in M that is relevant to predicting D is in A. So once we’ve conditioned on A, M tells us nothing more about D\nThe testable implication is that D is independent of M, conditional on A, i.e. \n\n(Conditioning on A does not make D independent of M, because M really influences D all by itself in this model.)\n\ni.e A and M are marginally dependent\n\n\n\nOnly difference between both DAGs is the conditional independence in DAG2.\n\nTest\n\nRun a multiple regression D ~ α + βMM + βAA\nIf the effect measured from regression 1 disappears in the multiple regression, then we can discard DAG 1. If the effect remains, then we discard DAG 2.\n\n\nDAGs that are consistent with the data associations (M & N are associated but the causal relationship isn’t known)\n\nwhere U is an unknown variable. Unobserved variables are circled.\n\nAll three DAGs have no conditional independencies and therefore not testable implications\n\nA set of DAGs, each with the same conditional independencies known as a Markov Equivalence\n\nData cannot eliminate any of these DAGS. Domain knowledge must be used to reduce the number of Markov Equivalent DAGs.\n\n\n\n\n“Shutting the backdoor” to potential confounding paths\n\nSection 6.4\nwww.dagitty.net - Enter DAG and it will give you the Adjustment Set and Testable Implications\nRecipe\n\nList all of the paths connecting X (the potential cause of interest) and Y (the outcome).\nClassify each path by whether it is open or closed. A path is open unless it contains a collider.\nClassify each path by whether it is a backdoor path. A backdoor path has an arrow entering X.\nIf there are any backdoor paths that are also open, decide which variable(s) to condition on to close it.\n\nIf you have a choice between two variables where conditioning on either will close a backdoor path and one of them is causal to the outcome variable, then condition on the variable that is causal to the outcome variable. It will add precision to the estimate of the treatment effect.\nAny frontdoor paths that lead to backdoor paths must also be closed (see Misc &gt;&gt; Table 2 fallacy)\n\n\nExamples:\n\n\n\nProblem: We want to measure the causal effect of X –&gt; Y\nPotential confounding paths: XUAC, XUBC\n\nXUAC doesn’t have a collider so a variable needs conditioned on (aka adjusted for)\n\nU is unobserved, so either A or C. C directly influences Y, so it’s more efficient and will “aid in precision.”\n\nXUBC has a collider, B. So, no need to condition on any variable\n\nSolution: Y ~ a + X + C\n\nlibrary(dagitty)\ndag_6.1 &lt;- dagitty( \"dag { \n    U [unobserved]\n    X -&gt; Y\n    X &lt;- U &lt;- A -&gt; C -&gt; Y\n    U -&gt; B &lt;- C\n}\")\nadjustmentSets( dag_6.1 , exposure=\"X\" , outcome=\"Y\" )\n#&gt; { C }\n#&gt; { A }\n\n\nProblem: We want to measure the causal effect of the number of Waffle Houses, W, on Divorce, D.\nPotential confounding paths: WSM, WSA, WSMA (Also WSAM but McElreath on says there are 3. Maybe a combo of same letters is equivalent?)\n\nWSM doesn’t have a collider and therefore either S or M needs conditioned on\nWSA doesn’t have a collider and therefor either S or A needs conditioned on\nWSMA has a collider, M. So that path is blocked\nM is a choice for WSM but it’s a collider so it’s out. S is in both WSM and WSA, so conditioning on it kills two birds.\n\nSolution: D ~ a + W + S\n\nlibrary(dagitty)\ndag_6.2 &lt;- dagitty( \"dag {\n    A -&gt; D\n    A -&gt; M -&gt; D\n    A &lt;- S -&gt; M\n    S -&gt; W -&gt; D\n}\")\nadjustmentSets( dag_6.2 , exposure=\"W\" , outcome=\"D\" )\n#&gt; { A, M }\n#&gt; { S }\n\nEvidently conditioning on A and M is also a solution\n\nConditioning on M does close WSM but would then open WSMA. So, by then conditioning on A which is on a fork (or pipe depending on the path) it closes WSMA.\n\nIn his brms ebook, Kurz fits these regressions and a couple others for comparison. There wasn’t a consensus point estimate for W in the regressions that adjust for S and A + M.\n\nMcElreath mentions, “This DAG is obviously not satisfactory–it assumes there are no unobserved confounds, which is very unlikely for this sort of data.”\nThe inconsistent point estimates are probably do to an omitted variable(s) that is confounding the regression.\n\nConditional independencies:\nimpliedConditionalIndependencies( dag_6.2 )\n#&gt; A _||_ W | S\n#&gt; D _||_ S | A, M, W\n#&gt; M _||_ W | S",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-othart",
    "href": "qmd/causal-inference.html#sec-causinf-othart",
    "title": "Causal Inference",
    "section": "Other Articles",
    "text": "Other Articles\n\nFrontdoor Adjustment\n\nFrom http://arelbundock.com/posts/frontdoor/\nUseful when an unobserved confounder creates a backdoor path that prevents direct causal estimation\nIn a causal chain with three nodes X→Z→Y, we can estimate the effect of X on Y indirectly by combining two distinct quantities:\n\nThe estimate of the effect of X on Z, P(Z|do(X))\nThe estimate of the effect of Z on Y, P(Y|do(Z), X)\n\nAssumptions\n\nFull mediation: there is no direct path from X to Y, except through Z.\nUn-confoundedness 1: There is no open backdoor from X to Z.\nUn-confoundedness 2: All backdoors from Z to Y are blocked by X\n\nExample: 1\n\nOur goal is to estimate P(Y|do(X)). Unfortunately, this relationship between X and Y is confounded by the unobserved variable U, via this backdoor path: X←U→Y. Therefore, we cannot estimate the causal quantity of interest directly.\n\n\ncause X, a mediator Z, an outcome Y, and an unobserved confounder U\n\nlibrary(data.table)\nset.seed(731460) \nN = 1e5\nU = rbinom(N, 1, prob = .2)\nX = rbinom(N, 1, prob = .1 + U * .6)\nZ = rbinom(N, 1, prob = .3 + X * .5)\nY = rbinom(N, 1, prob = .1 + U * .3 + Z * .5)\ndat = data.table(X, Z, Y)\n\n# truth\ncoef(lm(Y ~ X + U))[\"X\"]\n## 0.2549541\nEstimate the effect of X on Z, P(Z|do(X))\nstep1 = lm(Z ~ X, dat)\nEstimate the effect of Z on Y, P(Y|do(Z), X)\nstep2 = lm(Y ~ Z + X, dat)\nCombine both estimates by multiplication\ncoef(step1)[\"X\"] * coef(step2)[\"Z\"]\n## 0.2496002\n\nExample 2\n\nSame as first example but using {dosearch} package\nlibrary('dosearch')\n   data1 &lt;- \"P(X, Y, Z)\"\nquery1 &lt;- \"P(Y | do(X))\"\ngraph1 &lt;- \"U -&gt; X\n          U -&gt; Y\n          X -&gt; Z\n          Z -&gt; Y \"\n   # compute\n   frontdoor &lt;- dosearch(data1, query1, graph1)\n   frontdoor\n\nOutput:\n\nEstimate the causal effect\ndat[, `P(X)`    := fifelse(X == 1, mean(X), 1 - mean(X)) ][\n    , `P(Z|X)`  := mean(Z), by = X                      ][\n    , `P(Y|Z,X)` := mean(Y), by = .(Z, X)                ][\n    , `P(Z|X)`  := mean(Z), by = X                      ][\n    , Y := NULL                                          ]\ndat = unique(dat)\ndat[, `P(Y|do(Z))` := sum(`P(Y|Z,X)` * `P(X)`), by = Z]\n`P(Y|do(X=0))` = with(dat[X == 0], \n  `P(Z|X)`          [Z == 1] * \n  `P(Y|do(Z))`      [Z == 1] +\n  (1 - `P(Z|X)`)    [Z == 0] * \n  `P(Y|do(Z))`      [Z == 0]\n)\n`P(Y|do(X=1))` = with(dat[X == 1], {\n  `P(Z|X)`          [Z == 1] * \n  `P(Y|do(Z))`      [Z == 1] +\n  (1 - `P(Z|X)`)    [Z == 0] * \n  `P(Y|do(Z))`      [Z == 0]\n})\n`P(Y|do(X=1))` - `P(Y|do(X=0))`\n## 0.249766\nComparison\n\nTruth: 0.2549541\nlm: 0.2496002\ndosearch: 0.249766",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/business-plots.html",
    "href": "qmd/business-plots.html",
    "title": "Business Plots",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Business Plots"
    ]
  },
  {
    "objectID": "qmd/business-plots.html#sec-bizplots-misc",
    "href": "qmd/business-plots.html#sec-bizplots-misc",
    "title": "Business Plots",
    "section": "",
    "text": "{modelplotr}\n\nGithub, Vignette\nNice implementations but package is not maintained\nNotes for marketing and financial graphs taken from articles and vignettes introducing that package.",
    "crumbs": [
      "Business Plots"
    ]
  },
  {
    "objectID": "qmd/business-plots.html#sec-bizplots-market",
    "href": "qmd/business-plots.html#sec-bizplots-market",
    "title": "Business Plots",
    "section": "Marketing Plots",
    "text": "Marketing Plots\n\nTL;DR - Most useful/popular are the Cumulative Gains and Cumulative Response graphs.\nThe example objective is to select the customers of a bank that are most likely to respond to an offer to purchase a “term deposit”. The outcome is binary: “term deposit” or “no”\nInformation from models used in these plots\n\nPredicted probability for the target class\nX-Axis: Equally sized groups based on this predicted probability\n\ne.g. Splitting observations into deciles. Top 10% in predicted probability for target class would be in the first decile.\n\nNumber of observed target class observations in these groups\n\nThe test dataset is used for the plots to get a realistic idea of what a marketing campaign in the would would produce.\n\nResponse Plot has some GOF capability so I could maybe see using the validation set with that plot to compare models with.\n\n\n\nCumulative Gains\n\n\nAKA Gains Plot\nAnswers the question: “When we apply the model and select the best X quantiles, what % of the actual target class observations can we expect to target?”\n\ny-axis = % of positive events (1s in binary classification) out of the entire dataset\n\nHow to apply:\n\nChoose a probability threshold (i.e. the corresponding quantile on the x-axis). The graph shows the percentage of observations on the y-axis that are within that threshold\nChoose the percentage of customers that you can afford to target with your campaign. The corresponding quantile on the x-axis shows the quantile and therefore the associated probability of positive result.\n\n“When we select 20% with the highest probability according to gradient boosted trees, this selection holds 87% of all term deposit cases in test data.”\n\nSays using the top 20% will include 87% of all the 1s (in binary classification) in the entire dataset.\n\n\nIf the gains is 87%, then there are potentially 13% of the total 1s that won’t be included in the campaign if we only target the top 20% percent.\n\nwizard model (perfect model) line - line takes steepest route to 100% on y-axis as possible, depending on the percentage of your outcome variable is the target level.\n\nFor the graph above, it looks like around 12% of the outcome variable values are the positive event case since the line reaches the 100% on the y-axis a little past the 1st decile. So the perfect model predicts all those values as being the positive class.\n\n\n\n\nCumulative Lift\n\n\nAKA Index or Lift Plot\nEspecially useful for companies with little to no experience with data models\nAnswers the question: “When we apply the model and select the best X quantiles, how many times better is that than using no model at all?”\n“no model at all” (i.e. coin flip) is a random model (also seen in the gains plot) is represented by a horizontal line at y = 1 or 100% depending on how the y-axis is specified. It is the ratio of the % of actual target category observations in each quantile to the overall % of actual target category observations after randomization of the rows of the data set.\nThe amount of lift can’t be generalized to all models and all data sets. So there aren’t guidelines as to what is a “good” lift score and what isn’t. If 50% of your data belongs to the target (positive) class of interest, a perfect model would ‘only’ do twice as good (lift: 2) as a random selection. If 10% of the data belong to the positive class, then lift = 10 or 1000% is the best possible lift score.\nHow to apply:\n\nChoose a quantile (x-axis) and the corresponding y value can be used to explain to stakeholders how many times or what percent better this model is at selecting the top prospects than random selection.\n\n“A term deposit campaign targeted at a selection of 20% of all customers based on our gradient boosted trees model can be expected to have a 4 times higher response (434%) compared to a random sample of customers.”\n\n\n\n\n\nResponse Plot\n\n\nPlots the percentage of *target class* observations per quantile\n\nnote: the cumulative gains y-axis is total observations where this plot’s y-axis is just positive class (1s in a binary classification model)\n\nAnswers the question: “When we apply the model and select quantile X, what is the expected % of target class observations in that quantile?” but also gives information about the model fit.\nHow to apply:\n\nThis plot is more important in what it tells about the model fit than what it says about how many observations are in a particular quantile\n\nA good fitting model will have a sharp sloping line with the highest response % in the lower quantiles. This says that the model is giving high probability scores to the vast majority of the positive class observations\nFor model comparison: the earlier the line crosses the horizontal (random model) line should indicate a steeper slope and therefore a better fit.\n\n“When we select decile 1 (10th percentile) according to model gradient boosted trees in dataset test data the % of term deposit cases in the selection is 51%.”\nThe horizontal line represents a random model (i.e. the % of target class cases in the total set)\n\nFrom the quantile where the line intersects the horizontal dashed-line and onwards, the % of target class cases is lower than a random selection of cases would hold.\n\n\n\n\n\nCumulative Response\n\n\nAnswers the question: “When we apply the model and select up until quantile X, what is the expected % of target class observations in the selection?\n\nOften used to decide - together with business colleagues - up until what decile to select for a marketing campaign\n\nHow to apply:\n\n“When we select quantiles 1 until 30 according to model gradient boosted trees in dataset test data, the % of term deposit cases in the selection is 36%.”\n\nIn other words, targeting these customers should produce a response rate (percent of customers purchasing a term deposit) of 35% on average as compared to randomly selecting the same number of customers which is 12% (term deposits/total obs for the test set).\nThe y-axis is the percentage of 1s (in binary classification) in that subset (quantiles from 1 to 30). Different from cumulative gains where the y-axis is the percentage of 1s in the entire dataset.\n\nIs that response big enough to have a successfull campaign, given costs and other expectations? Will the absolute number of sold term deposits meet the targets? Or do we lose too much of all potential term deposit buyers by only selecting the top 30%? To answer that question, we can go back to the cumulative gains plot.\nThe dashed horizontal is the same as in the Response Plot",
    "crumbs": [
      "Business Plots"
    ]
  },
  {
    "objectID": "qmd/business-plots.html#sec-bizplots-fin",
    "href": "qmd/business-plots.html#sec-bizplots-fin",
    "title": "Business Plots",
    "section": "Financial Plots",
    "text": "Financial Plots\n\nExample objective is to select the customers of a bank that are most likely to respond to an offer to purchase a “term deposit”. The outcome is binary: “term deposit” or “no”\n\nfixed costs = $75,000 (a tv commercial and some glossy print material)\nvariable costs per unit = $50 (customers are given an incentive to buy)\nprofit per unit = $250\n\nInformation from models used in these plots\n\nSame stuff as Marketing Plots\nFixed Costs (e.g. sales force expenses, advertising campaigns, sales promotion, and distribution costs)\nVariable Costs per unit (e.g.sales commission, bonuses, and performance allowances)\nProfit per Sale\n\nThe test dataset is used for the plots to get a realistic idea of what a marketing campaign in the would would cost and return. A validation set could be used on the Revenue and Costs Plot and models could be compared based risk of nonprofitability.\n\n\nProfit Plot\n\n\nAnswers the question: “When we apply the model and select up until quantile X, what is the expected profit of the campaign?”\nHow to apply:\n\nThe most profitable quantile is the one directly under the apex of the curve.\nThe most profitable quantile is highlighted by default, but this can be specified if so desired\nannotation means?\n\n\n\n\nCosts and Revenues Plot\n\n\nAnswers the question: “When we apply the model and select up until decile X, what are the expected revenues and investments of the campaign?”\nThe costs are the cumulative costs of selecting up until a given decile and consist of both fixed costs and variable costs.\nThe revenues take into account the expected response % - as plotted in the cumulative response plot - as well as the expected revenue per response.\nSolid curve is the revenue and the dashed diagonal line is the total costs\nHow to apply:\n\nThe campaign is profitable in the plot area where revenues exceed costs.\nGives an idea of the range of spending that can be considered while the campaign remains profitable. Ranges could be associated with risk. The smaller the range, the greater the risk given the uncertainty of the models. Various campaign ranges could be compared based on this risk.\nSee profits plot for optimal quantile.\n\n\n\n\nROI Plot\n\n\nAnswers the question: “When we apply the model and select up until decile X, what is the expected % return on investment of the campaign?”\nThe quantile at which the campaign profit is maximized is not necessarily the same as the quantile where the campaign ROI is maximized\n\nIt can be the case that a bigger selection (higher decile) results in a higher profit, however this selection needs a larger investment (cost), impacting the ROI negatively.\nSo maximum ROI can be considered the most effficient use of resources, but it takes money to make (the most) money.\n\nBasic formula for ROI = Net Profit / Total Investment * 100\nHow to apply:\n\nThe quantile directly underneath the apex of the curve is where the ROI is maximized.",
    "crumbs": [
      "Business Plots"
    ]
  },
  {
    "objectID": "qmd/web-design.html",
    "href": "qmd/web-design.html",
    "title": "Web Design",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Web",
      "Web Design"
    ]
  },
  {
    "objectID": "qmd/web-design.html#sec-webdes-misc",
    "href": "qmd/web-design.html#sec-webdes-misc",
    "title": "Web Design",
    "section": "",
    "text": "Using “Brand” to help choose font, palette, and imagery\n\nNotes from Erik Kennedy Video\nBrand is just adjectives to describe your business, organization, etc.\n\ne.g. Trustworthy, Geeky, Casual, Precise, Fun, Technical, etc.\n\nCommon Brands\n\n\n“Neat, modern”, “Luxury, formal”, etc. are more of what I’d consider brand adjectives\n“Clean & Simple”,“Fancy”, “Techie”, etc. are how I’d describe the sites that epitomize those brands, but they could also be brand descriptors\n\nBlending Brands\n\n\nShows names of company websites that most represent the brand/website types\ne.g. The Apple website is a blend of Techie and Fancy.\n\n\nWebsites should be under 14kb (article)\n\nMost web servers TCP slow start algorithm starts by sending 10 TCP packets which works out to 14kb\n\n404 pages\n\nGuidelines\n\nbe brief: the message on the 404 page should be straightforward and easy to understand, informing the user that the page they were trying to access is not available.\nbe contrite: the tone of the 404 page should be friendly and apologetic, acknowledging the user’s inconvenience and expressing empathy.\nbe helpful: provide links to other areas of the website or a search box that can help users find what they’re looking for quickly and easily.\nbe informative: include contact information, such as a feedback form, social media account, or email address to give users an alternative way to reach out to you for assistance.\nbe you: incorporate your brand’s visual identity, including logos and colors, to help reinforce brand recognition and create a cohesive user experience.\n\nExamples\n\n404 Page SVG Animations That Maximize Visitor Retention\n21 Stunning 404 Pages to Convert Lost Visitors 2023\nguinslym/awesome-404: A curated list of awesome 404 web pages greynoise’s ‘404’ equivalent and hrbmstr’s.",
    "crumbs": [
      "Web",
      "Web Design"
    ]
  },
  {
    "objectID": "qmd/git-general.html",
    "href": "qmd/git-general.html",
    "title": "26  General",
    "section": "",
    "text": "26.1 Misc",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#misc",
    "href": "qmd/git-general.html#misc",
    "title": "26  General",
    "section": "",
    "text": "View HTML file in browser\n\nSyntax: “https://raw.githack.com/&lt;acct name&gt;/&lt;repo name&gt;/&lt;branch name&gt;/&lt;directory name&gt;/&lt;file name&gt;.html”\n\nInstalling from a git repo (From link)\n\nMake a fork of the repo and then clone it to your local machine.\nTo update, after setting an upstream remote (git remote add upstream git://github.com/benfulcher/hctsa.git) you can use git pull upstream main.\nTo update the submodule in the repo, git submodule update --init\n\nStart R project and Git repo in whichever order (I think)\n\nCreate R project in RStudio\n\nChoose “New Directory” for all the templated projects (e.g. quarto book, shiny, etc.). None of the other choices have them.\n\nIf you’ve already created a directory, it will NOT overwrite this directory or add to it. So you’ll either have alter the name of your old directory or choose a new name.\n\n\nCreate repo on Github\n\nAdd license and readme\n\nDo work\nTools &gt;&gt; Version Control &gt;&gt; Project Set-up &gt;&gt; Version Control System &gt;&gt; Select Git\nOpen terminal and go to working directory of project\ngit checkout -B main\ngit pull origin main --allow-unrelated-histories\ngit add .\ngit commit -m \"initial commit\"\ngit push --set-upstream origin main \n\nTurn off “LF will be replaced by CRLF the next time Git touches it”\n\nMessage spams terminal when committing changes from a window machines. Has to do with line endings in windows vs unix.\nTurn off: git config core.autocrlf true\nSee SO post for more details\n\nURL format to download files from repositories\n\nhttps://raw.githubusercontent.com/user/repository/branch/filename\n\n# Or evidently this way works too\n# adds ?raw=true to the end of the url\nfeat_all_url &lt;- url(\"https://github.com/notast/hierarchical-forecasting/blob/main/3feat_all.RData?raw=true\")\nload(feat_all_url)\nclose(feat_all_url)\nGet filelist from repo and download to a directory\n\n** Directory urls change as commits are made **\n\nlibrary(httr)\n\n# example: get url for the data dir of covidcast repo\nreq &lt;- httr::GET(\"https://api.github.com/repos/ercbk/Indiana-COVIDcast-Dashboard/git/trees/master?recursive=1\") %&gt;% \n  httr::content()\n# alphabetical order\ntrees &lt;- req$tree %&gt;% \n  map(., ~pluck(.x, 1)) %&gt;% \n  as.character()\n# returns 20 which is first instance, so 19 should the \"data\" folder\ndetect_index(trees, ~str_detect(., \"data/\"))\n# url for data dir\nreq$tree[[19]]$url\n\n# example\n# Get all the file paths from a repo\nreq &lt;- GET(\"https://api.github.com/repos/etiennebacher/tidytuesday/git/trees/master?recursive=1\")\n# any request errors get printed\nstop_for_status(req)\nfile_paths &lt;- unlist(lapply(content(req)$tree, \"[\", \"path\"), use.names = F)\n# file_path wanted &lt;- filter file path to file you want\n# gets the very last part of the path\nfile_wanted &lt;- basename(file_path_wanted)\norigin &lt;- paste0(\"https://raw.githubusercontent.com/etiennebacher/tidytuesday/master/\", file_wanted)\ndestination &lt;- \"output-path-with-filename-ext\"\n# if file doesn't already exist, download it from repo into destination\nif (!file.exists(destination)) {\n      # if root dir doesn't exist create it\n      if (!file.exists(\"_gallery/img\")) {\n        dir.create(\"_gallery/img\")\n      }\n      download.file(origin, destination)\nThe insides of .git",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#optimizations",
    "href": "qmd/git-general.html#optimizations",
    "title": "26  General",
    "section": "26.2 Optimizations",
    "text": "26.2 Optimizations\n\nFor large repos, simple actions, like running git status or adding new commits can take many seconds. Cloning repos can take many hours.\nBenefits\n\nIt improves the overall performance of your development workflow, allowing you to work more efficiently. This is especially important when working with large organizations and open source projects, where multiple developers are constantly committing changes to the same repository. A faster repository means less time waiting for Git commands such as git clone or git push to finish. It helps to optimize the storage space, as large files are replaced by pointers which take up less space. This can help avoid storage issues, especially when working with remote servers.\n\nMisc\n\nSee How to Improve Performance in Git: The Complete Guide\n\nExplainer, config settings, advanced gc, checkout, and clone commands\n\n\nUse .gitignore\n\nGenerated files, like cache or build files\n\nThey will be modified at each different generation — and there’s no need to keep track of those changes.\n\nThird-party libraries\n\nInstead, aim for a list of the required dependencies (and the correct version) so that everyone can download and install them whenever the repo is cloned.\n\nFor example, with a package.json file for JavaScript projects you can (and should) exclude the /node_modules folder.\n.DS_Store files (which are automatically created by macOS) are another good candidate\n\n\n\nGit LFS\n\nDesigned specifically to handle large file versioning. LFS saves your local repositories from becoming unnecessarily big, preventing you from downloading unnessary data.\n\nGit LFS intercepts any large files and sends them to a separate server, leaving a smaller pointer file in the repository that links to the actual asset on the Git LFS server.\n\nThis is an extension to the standard Git feature set, so you will need to make sure that your code hosting provider supports it (all the popular ones do).\nAlso need to download and install the CLI extension on your machine before installing it in your repository.\nSet-Up\n$ git lfs install\n$ git lfs track \"*.wav\"\n$ git lfs track \"images/*.psd\"\n$ git lfs track \"videos\"\n$ git add .gitattributes\n\nTells Git LFS which file extensions it should manage.\n.gitattributes notes the file names and patterns in this text file and, just like any other change, it should be staged and committed to the repository.\nCan now add files and commit as normal\nList all file extensions being tracked: git lfs track\nList all files being managed: git lfs ls-files\n\n\nDon’t download the version history if you don’t need to\n\ngit clone –depth 1 gitj@github.com:name/repo.git",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#troubleshooting",
    "href": "qmd/git-general.html#troubleshooting",
    "title": "26  General",
    "section": "26.3 Troubleshooting",
    "text": "26.3 Troubleshooting\n\nKeeps asking for username/password when pushing\n\nSolution: You (or if you used usethis::use_github/git) probably set-up a https connection when you need a ssh connection.\n\nsee https://docs.github.com/en/get-started/getting-started-with-git/managing-remote-repositories#changing-a-remote-repositorys-url to change from https to ssh.\n\n\nUndo a commit, but save changes made (e.g. you forgot to pull before you pushed)\n\nSteps\n\ngit log - Shows commit history. Copy the hash for your last commit\ngit diff &lt;last commit hash&gt; &gt; patch - save the diff of the latest commit to a file\ngit reset --hard HEAD^ to revert to the previous commit\n\n**After this, your changes will be lost locally **\n\ngit log - confirm that you are now at the previous commit\ngit pull - correct the mistake you made in first place\npatch -p1 &lt; patch - apply the changes you originally made\ngit diff - to confirm that the changes have been reapplied\nNow, you do the regular commit, push routine\n\n\nUndo uncommitted changes: git stash followed by git stash drop\n\n“but only use if you commit often” - guessing this is not good if your commit is somehow large and/or involves multiple files\n\nSearch commits by string: git log --grep &lt;string&gt;",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#pulling",
    "href": "qmd/git-general.html#pulling",
    "title": "26  General",
    "section": "26.4 Pulling",
    "text": "26.4 Pulling\n\nSave your changes, pull in an update, apply your changes\ngit stash\ngit pull\ngit stash pop\n\ngit stash pop throws away the (topmost, by default) stash after applying it, whereas\ngit stash apply leaves it in the stash list for possible later reuse (or you can then git stash drop it).\n\nRe potential merge conflicts\n\n“For instance, say your stashed changes conflict with other changes that you’ve made since you first created the stash. Both pop and apply will helpfully trigger merge conflict resolution mode, allowing you to nicely resolve such conflicts… and neither will get rid of the stash, even though perhaps you’re expecting pop too. Since a lot of people expect stashes to just be a simple stack, this often leads to them popping the same stash accidentally later because they thought it was gone.”\n\nPulling is fetching + merging\n\nFetching just gets the info about the commits made to the remote repo\ngit fetch origin\nSome technical discussion for always using git pull –ff\n\nhttps://blog.sffc.xyz/post/185195398930/why-you-should-use-git-pull-ff-only-git-is-a\nhttps://megakemp.com/2019/03/20/the-case-for-pull-rebase/\nit’s still confusing but pull rebase sounds fine to me\n–global tag says do it for all my repos\nnot sure what the true and only are for\n\ngit pull –help will open doc in browser\n\n\nPulling by rebase\n\nLocal: using this method as default\ngit config pull.rebase true\ngit pull\nRemote\ngit pull --rebase\n\nPulling by fast-forward\n\nLocal: using this method as default\ngit config --global pull.ff only\ngit pull\nRemote\ngit pull --ff",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#branching",
    "href": "qmd/git-general.html#branching",
    "title": "26  General",
    "section": "26.5 Branching",
    "text": "26.5 Branching\n\nMisc\n\nCreate a new branch for each ticket you are working on or each data model. It can get sloppy when you put all your code changes on one branch.\n\nCreate a branch (e.g. “testing”)\ngit branch testing\nWork in a branch\ngit checkout testing\nThe files in your working directory change to the version saved in that branch\nIt adds, removes, and modifies files automatically to make sure your working copy is what the branch looked like on your last commit to it.\nCreate and work in a branch\n# new way\ngit switch -c testing\nor\ngit checkout -b testing\nor\ngit branch testing\ngit checkout testing\ncreates the branch and switches you to working in that branch\nIf you did a bunch of changes in a codebase, only to realize that you’re working on `master`,  switch will bring those local changes with you to the new branch. So I guess they won’t affect master then.\n\nUnless If you already committed to main, then those changes are both in your new branch and in main. So you would still have to clean up the main branch.\n\nDeleting a branch\n\nlocal branch\ngit branch -d testing\n\nremote branch\ngit push &lt;remoteName&gt; --delete &lt;branchName&gt;\nSee existing branches\ngit branch\nSee what has been commited the remote repo branches\ngit fetch origin\ngit branch -vv\n“origin” is the name of the remote\nresult\ntesting    7e424c3 [origin/testing: ahead 2, behind 1] change abc \nmaster      1ae2a45 [origin/master] Deploy index fix\n* issue    f8674d9 [origin/issue: behind 1] should do it         \ncart        5ea463a Try something new\nformat: branch, last commit sha-1, local branch status vs remote branch status, commit message\nthe star indicates the HEAD pointer’s location (where you’re at, i.e. checkout)\ntesting branch\n\n“ahead 2” means  I committed twice to the local testing branch and this work has not been pushed to the remote testing branch repo yet.\n“behind 1” means someone has pushed a commit to the remote testing branch repo and we haven’t merged this work to our local testing branch\n\nGet the last 10 branches that you’ve committed to locally:\ngit branch --sort=-committerdate | head -n 10\nRename branch\n# change locally\ngit branch --move &lt;bad-branch-name&gt; &lt;corrected-branch-name&gt;\n# change remotely in repo\ngit push --set-upstream origin &lt;corrected-branch-name&gt;\n# confirm change\ngit branch --all\nHEAD determines to which branch new commits are added\n\nExample\n\n“testing” branch is created (not shown in above picture)\n\nHEAD points at “master” branch\n“master” branch and the new “testing” branch both point at commit, f30ab.\nf30ab commit points to previous commit 34ac2\n\nuser executes checkout to “testing” branch (not shown in picture)\n\nHEAD now points to testing branch\n\nuser commits 87ab2 (shown in pic)\n\n87ab2 is committed to the “testing” branch\n“testing” branch is now ahead of the “master” branch by 1 commit\n\n\nExample\n\nEverything above happens but now another user commits the master branch.\n\nBoth branches are in conflict. The testing branch is ahead and behind by 1 commit\n\n\n\nMerging\n\n\nNotes\n\nNEVER merge your branch locally on your machine with the master branch, ALWAYS merge online via pull request\n\nSteps\n\nPush final changes and use of a pull request\nSwitch to master branch locally and pull the merged changes\n\n\n\nUpdate branch with work that’s been done in master branch\n\nAfter updating your local branch, push to remote repo (no commit necessary)\n# while in branch\ngit merge master\n\n\nFast-Forward\n\nExample\n\nBefore the merge\n\nthe testing branch is 1 commit ahead of the master branch and the master branch doesnt have a new commit\n\nAfter the merge\n\nmaster is moved forward to the testing branch commit\n\n\nCode (merging work in branch with the master branch for production)\n# currently in test branch\ngit checkout master\ngit merge testing\n\nLines in file are marked\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.html\n# &lt;div id=\"footer\"&gt;contact : email.support@github.com&lt;/div&gt;\n# =======\n# &lt;div id=\"footer\"&gt;\n# please contact us at support@github.com\n# &lt;/div&gt;\n# &gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html\nAbove ======= is the master branch version of the code and below is the iss53 branch version\nMake necessary changes and save the file\ngit add . or git add &lt;resolved file&gt;\n\nTells git that conflict is resolved\n\nCheck status to confirm everything has been resolved\ngit status\n\n    On branch master\n    All conflicts fixed but you are still merging.\n      (use \"git commit\" to conclude merge)\n    Changes to be committed:\n      modified:  index.html\ngit commit\n\nno message required (there’s a default message) but you can add one if you want\n\nExample\n\niss53 branch ahead of master by 2 commits (c3, c5) and behind 1 commit (c2)\nSame code as Fast-Forward merge but git handles the merge a bit differently\ngit checkout master \ngit merge iss53\n\n\n\nC6 (right pic) is called a “merge commit.” Its created by git and points to two commits instead of one.\nNo need to merge with master (i.e. update local iss53 branch with c4 changes in master) before committing final changes\n\nIf there are changes in the same lines of code C4 and C5, then there will be a conflict (See below, Conflicts &gt;&gt; Example)\n\n\nConflicts\n\nExample\n\nChanged files in C4 (see above example) are in the same lines of the same files that you made changes to in C5\n\nRemember: you’re now in the master branch since you did checkout master as part of the merge code\nSteps\n\nCheck status to which files are causing the conflict (e.g. index.html)\ngit status\n  Unmerged paths:\n  (use \"git add &lt;file&gt;...\" to mark resolution) \n    both modified:      index.html\n\n\n\n\nMoving between branches\n\nfrom master to testing\ngit checkout testing\n\nlocal files are deleted and replaced with branch versions\n\nalternative: worktree\n\nExample\n\nWhat happens when you move from branch-a to branch-b\nBRANCH-A        BRANCH-B\nalpha.txt      alpha.txt\nbravo.txt\ncharlie.txt    charlie.txt\n                delta.txt\n\nbravo text is deleted from your local disc and delta.txt is added\nIf any changes to alpha.txt or charlie.txt have been made and no commit has been made, the checkout will be aborted\n\nSo either revert the changes or commit the changes\n\nUntracked files or newly created files\n\nIf you have branch-A checked out and you create a new file called echo.txt, Git will not touch this file when you checkout branch-B. This way, you can decide that you want to commit echo.txt against branch-B without having to go through the hassle of (1) move the file outside the repo, (2) checkout the correct branch, and (3) move the file back into the repo.",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#collaboration",
    "href": "qmd/git-general.html#collaboration",
    "title": "26  General",
    "section": "26.6 Collaboration",
    "text": "26.6 Collaboration\n\nAdd collaborators to your repository\nOne person invites the others and provides them with read/write access (github docs)\n\nSteps\n\nGo to the settings for your repository\nmanage access &gt;&gt; “invite a collaborator”\n\nSearch for each collaborator by full name, acct name, or email\nClick “Add &lt;name&gt; to &lt;repo&gt;”\n\nEach collaborator will need to accept the invitation\n\nSent by email",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html",
    "href": "qmd/db-normalization.html",
    "title": "Normalization",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html#sec-db-norm-misc",
    "href": "qmd/db-normalization.html#sec-db-norm-misc",
    "title": "Normalization",
    "section": "",
    "text": "Organizing according to data attributes to reduce or eliminate data redundancy (i.e. having the same data in multiple places).\n\nIt gives you a set of rules to be able to start categorizing your data and forming a layout\n\nBy establishing structure in a database, you are able to help establish a couple of important things: data integrity and scalability.\n\nIntegrity ensures that data is entered correctly and accurately.\nScalability ensures you have organized the data in a way that it is more computationally efficient when you start to run SQL queries.\n\nNotes from When Spreadsheets Aren’t Good Enough: A Lesson in Relational Databases\n\nGives an example of normalizing a dataset through a MySQL analysis\n\nPackages\n\n{{autonormalize}} - analyzes transaction df and creates relational tables - python library for automated dataset normalization",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html#sec-db-norm-terms",
    "href": "qmd/db-normalization.html#sec-db-norm-terms",
    "title": "Normalization",
    "section": "Terms",
    "text": "Terms\n\nDimension Tables - Contains data about how the data in Fact Table is being analyzed. They facilitate the fact table in gathering different dimensions on the measures which are to be taken.\nFact Tables - Contain data corresponding to any business process. Every row represents any event that can be associated with any process. It stores quantitative information for analysis",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html#sec-db-norm-form",
    "href": "qmd/db-normalization.html#sec-db-norm-form",
    "title": "Normalization",
    "section": "Forms",
    "text": "Forms\n\nDatabases are often considered as “normalized” if they meet the third normal form\nSee A Complete Guide to Database Normalization in SQL for details on the other 4 forms.\n\nAlso gives an example of normalizing a dataset through a posgresSQL analysis\n\nFirst normal form (1NF)\n\nEvery value in each column of a table must be reduced to its most simple value, also known as atomic.\n\nAn atomic value is one where there are no sets of values within a column. (i.e. 1 value per cell)\n\nThere are no repeating columns or rows within the database.\nEach table should have a primary key which can be defined as a non-null, unique value that identifies each row insertion. Second normal form (2NF)\nConforms to first normal form rules.\nAdjust columns so that each table only contains data relating to the primary key.\nForeign keys are used to establish relationships between tables. Third normal form (3NF)\nConforms to both first and second normal form rules.\nNecessary to shift or remove columns (attributes) that are transitively dependent, which means they rely on other columns that aren’t foreign or primary keys.",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html#sec-db-norm-schema",
    "href": "qmd/db-normalization.html#sec-db-norm-schema",
    "title": "Normalization",
    "section": "Schema",
    "text": "Schema\n\nMisc\n\nFactors that influence normalizing dimension tables\n\nData redundancy concerns: If minimizing redundancy is crucial, normalization might be preferred.\nQuery performance priorities: If query performance is paramount, denormalization often offers advantages.\nData consistency requirements: High consistency needs might favor normalization.\nMaintenance complexity: Denormalized dimensions can be simpler to maintain in some cases.\n\nDon’t use external IDs as primary keys\n\nSince you don’t control those IDs, they can change the format and break your queries.\n\n\nStar\n\n\nExample: A Star schema of sales data with dimensions such as customer, product & time.\nIn a star schema, as the structure of a star, there is one fact table in the middle and a number of associated dimension tables.\nThe fact table consists of primary information. It surrounds the smaller dimension lookup tables which will have details for different fact tables. The primary key which is present in each dimension is related to a foreign key which is present in the fact table.\nThe fact tables are in 3NF form and the dimension tables are in denormalized form. Every dimension in star schema should be represented by the only one-dimensional table.\n\nSnowflake\n\n\nSnowflake schema acts like an extended version of a star schema. There are additional subdimensions added to dimensions.\nUnlike the Star schema, dimensions are normalized.\nCan be slower than star schemas due to complex joins across multiple tables, but achieves better storage efficiency compared to star schemas due to reduced data redundancy.\nThere are hierarchical relationships and child tables involved that can have multiple parent tables.\nThe advantage of snowflake schema is that it uses small disk space. The implementation of dimensions is easy when they are added to this schema.\n\nFact Constellation or Galaxy\n\n\nA fact constellation can consist of multiple fact tables. These are more than two tables that share the same dimension tables — like connected Star schema.\nThe shared dimensions in this schema are known as conformed dimensions. Denormalization in shared dimension tables might increase storage size compared to fully normalized schemas.\nDimensions can be normalized but is rare in this schema due the level of complexity already present.\nUseful when aggregation of fact tables is necessary. Fact constellations are considered to be more complex than star or snowflake schemas. Therefore, more flexible but harder to implement and maintain. Joins across multiple fact and dimension tables can lead to complex queries with potential performance impacts.",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html#sec-db-norm-dsgn",
    "href": "qmd/db-normalization.html#sec-db-norm-dsgn",
    "title": "Normalization",
    "section": "Design",
    "text": "Design\n\nMisc\n\nOracle Data Model Documentation\n\nConsiderations\n\n7 Vs\n\nVolume: How big is the incoming data stream and how much storage is needed?\nVelocity: Refers to speed in which the data is generated and how quickly it needs to be accessed.\nVariety: What format the data needs to be stored? Structured such as tables or Unstructured such as text, images, etc.\nValue: What value is derived from storing all the data?\nVeracity:How trustworthy the data source, type and its processing are?\nViscosity: How the data flows through the stream and what is the resistance and the processability?\nVirality: Ability of the data to be distributed over the networks and its dispersion rate across the users_\n\nData Quality (See Database, Engineering &gt;&gt; Data Quality) completeness, uniqueness, timeliness, validity, accuracy, and consistency\n\nComponents\n\n\nMetamodeling:\n\nDefines how the conceptual, logical, and physical models are consistently linked together.\nProvides a standardized way of defining and describing models and their components (i.e. grammar, vocabulary), which helps ensure consistency and clarity in the development and use of these models.\nData ownership should be assigned based on a mapping of data domains to the business architecture domains (i.e. market tables to the marketing department?)\n\nConceptual Modeling - Involves creating business-oriented views of data that capture the major entities, relationships, and attributes involved in particular domains such as Customers, Employees, and Products.\nLogical Modeling - Involves refining the conceptual model by adding more detail, such as specifying data types, keys, and relationships between entities, and by breaking conceptual domains out into logical attributes, such as Customer Name, Employee Name, and Product SKU.\nPhysical Data Modeling - Involves translating the logical data model into specific database schemas that can be implemented on a particular technology platform\n\nProcess (article, article, article)\n\nUnderstand the Core Business Requirements\n\nCreate a catalogue of reporting stories for each stakeholder to an idea of the reports that each will want generated\n\nThese will inform you of the data requirements\ne.g. “As a marketing manager, I need to know the number of products the customer bought last year in order to target them with an upsell offer.”\n\nFrom the story above, I can determine that we will need to aggregate the number of products per customer based on sales from the previous year.\n\n\n\nSelect the tools and technologies:\n\nUsed to build and manage the data warehouse. This may include selecting a database management system (DBMS), data integration and extraction tools, and analysis and visualization tools.\nWarehouses - See Brands\nSee Production, Tools &gt;&gt;\n\nOrchestration\nELT/ETL Operations\n\n\nChoose a data model\n\nIdentify Business Processes\n\nFocus on business process and not business departments as many departments share the same business process\nIf we focus on department, we might end up with multiple copies of models and have different sources of truth.\n\nChoose a data model from the Business Process\n\nStart with the most impactful model with the lowest risk\n\nConsult with the stakeholders\n\nShould be used frequently and be critical to the business and also it must be built accurately\n\nDecide on the data granularity\n\nMost atomic level is the safest choice since all the types of queries is typically unknown\nNeed to consider the size and complexity of the data at the various granularities, as well as the resources available/costs for storing and processing it.\nExamples\n\nCustomer Level - easy to answer questions about individual customers, such as their purchase history or demographic information.\nTransaction Level - easy to answer questions about individual transactions, such as the products purchased and the total amount spent.\nDaily or Monthly?\n\n\n\nCreate Conceptual Data Models (Tables)\n\nThese represent abstract relationships that are part of your business process.\nExplains at the highest level what respective domains or concepts are, and how they are related.\nThe elements within the reporting stories should be consistent with these models\n\nExample: Retail Sales\n\nTime, Location, Product, and Customer.\n\nTime might be used to track sales data over different time periods (e.g. daily, monthly, yearly).\nLocation might be used to track sales data by store or region.\nProduct might be used to track sales data by product category or specific product.\nCustomer might be used to track sales data by customer demographics or customer loyalty status.\n\n\n\nExample:\n\n\nTransactions form a key concept, where each transaction can be linked to the Products that were sold, the Customer that bought them, the method of Payment, and the Store the purchase was made in — each of which constitute their own concept.\nConnectors show that each individual transaction can have at most one customer, store, or employee associated with it, but these in turn can be associated with many transactions (multi-prong connector into Transactions)\n\nExample:\n\n\nEach Customer (1 prong connector) can have 0 or more Orders (multi-prong connector)\nEach Order can have 1 or more Products\nEach Product can have 0 or more Orders\n\n\nCreate Logical Data Models\n\nBreakdown each entity of the conceptual model into attributes\n\nExample:\n\nExample:\n\n\n\nCreate Physical Data Models\n\nDetails are added on where exactly (e.g., in what table), and in what format, these data attributes exist.\n\ne.g. finalizing table names, column names, data types, indexes, constraints, and other database objects\n\nTranslate the logical data model into specific database schemas that can be implemented on a particular technology platform\n\ne.g. dimensional modelling in a star schema or normalisation in a 3rd normal form in a snowflake model.\n\nExample:\n\nExample: Dimension model in a star schema\n\n\nfact_ (quantitative) and dim_ (qualitative)\n\n\nMake Design and Environment decisions\n\nDecide on:\n\nPhysical data models\nHistory requirements\nEnvironment provisions & set up\n\n\nBuild a prototype (aka wireframe) of the end product\n\nThe business end-user may have a vision, they couldn’t coherently articulate at the requirement phase.\nThe prototype need not use real-world data or be in the reporting tool.\n\n** Profile known sources data **\n\nLearn about the data quality issues, and try and remediate those issues before designing your data pipelines.\n\nIf an issue cannot be resolved, you will have to handle it in your data pipeline\n\n\nBuild, Test, and Iterate\n\nCreate ETL jobs or data pipelines\n\nIteratively need to unit test the individual components of the pipeline.\n\nThe data will need to be moved from the source system into our physical warehouse\nProfile data\n\nData types, and if conversion is required\nThe amount of history that needs to be pulled\n\nValidate the model’s output numbers with the business end-user\n\nProgress towards Data Maturity (see Job, Organizational and Team Development &gt;&gt; Data Maturity)",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/cli.html",
    "href": "qmd/cli.html",
    "title": "CLI",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-misc",
    "href": "qmd/cli.html#sec-cli-misc",
    "title": "CLI",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly’s suggestions are prioritized in real time with a small neural network\n\nPath to a folder that’s above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder\n\nDebian vs. Ubuntu (from ChatGPT)\n\nStability vs. Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it’s known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu’s.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as “Debian spins,” catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-r",
    "href": "qmd/cli.html#sec-cli-r",
    "title": "CLI",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n      janitor::clean_names() %&gt;%\n      select(date, geo, county = name, negative, positive) %&gt;%\n      filter(geo == \"County\") %&gt;%\n      mutate(date = lubridate::as_date(date)) %&gt;%\n      select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it’ll search automatically\n35548",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-awk",
    "href": "qmd/cli.html#sec-cli-awk",
    "title": "CLI",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‘$13 &gt; 98’ adult_t.csv|head\nFilter lines with “Doctorate” and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-bash",
    "href": "qmd/cli.html#sec-cli-bash",
    "title": "CLI",
    "section": "Bash",
    "text": "Bash\n\nMisc\n\nNotes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn’t take notes on\n\n\nResources\n\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n“&gt;” redirects the output from a program to a file.\n\n“&gt;&gt;” does the same thing, but it’s appending to an existing file instead of overwriting it, if it already exists.\n\n\n\n\n\nCommands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you’re using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you’re about to move does already exist under the new directory,\n\nThis way you don’t accidentally overwrite files that you didn’t mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\n\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\nOutput to file: echo “This is an example for redirect” &gt; file1.txt\nAppend line to file: echo “This is the second line of the file” &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to “tmp” directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name “my_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n“rwx” stand for read, write, and execute rights, respectively\nThe 3 “rwx” blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a “d” for directory or “l” for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - “super user” - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‘error’, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo “No such directory exist.Check”\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn’t exist, then the message “No such directory exists. check” message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for “error” and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for “error” in each line. Lines with “error” get written to “error_file.txt”\n\nFilter lines\ngrep -i “Doctorate” adult_t.csv |grep -i “Husband”|grep -i “Black”|csvlook\n# -i, --ignore-case-Ignore  case  distinctions,  so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i “Doctorate” adult_t.csv | wc -l\n\nCounts how many rows have “Doctorate”\n\n-wc is “word count”\n\n\n\n\n\n\nVariables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via “$”\n# local\nev_car=’Tesla’\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=’Tesla’\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it’s okay not to is on the left-hand-side of an [[ ]] condition. But even there I’d recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or –help or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this\n\n\n\n\n\nScripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like –silent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script’s directory close to the start of the script.\n\nAnd it’s usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck. Heed its warnings.\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don’t give executable permission to the script file.\nStarts with “#!” the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory “/bin”\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you’re good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n    #!/bin/bash\n    echo ‘Hello World’\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n    set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n    echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n    exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n    echo do awesome stuff\n}\nmain \"$@\"\n\n\n\nJob Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.\n\n\n\n\ntmux (‘terminal multiplexer’)\n\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‘moose’\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named “moose”\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you’ve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‘moose’\n\n\nPane Creation and Navigation\n\ncontrol+b then press ” (i.e. shift+’): add another terminal pane below\ncontrol+b then press % (i.e. shift+5) : add another terminal pane to the right\ncontrol+b then press → : move to the terminal pane on the right (similar for left, up, down)\n\n\n\n\nSSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n“ssh-keygen” is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named “id_rsa.pub” and a private key named “id_rsa”, and place both into your “~/.ssh” directory\nYou’ll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: “~/.ssh/config”\nContents\nHost dev\n  HostName remote\n  IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist\n\n\n\n\nVim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you’ll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you’re in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file\n\n\n\n\nPackages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to “upgrading” the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and “updates” them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade\n\n\n\n\nExpressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-powsh",
    "href": "qmd/cli.html#sec-cli-powsh",
    "title": "CLI",
    "section": "Powershell",
    "text": "Powershell\n\nComments: &lt;# comment #&gt;\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you’re already in the directory that you want the folder in. You can also use a path, e.g. \"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it’s a backslash (\\), but in Powershell, it’s a backtick ( ` )\n*Don’t forget that there’s a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it’s open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-batscri",
    "href": "qmd/cli.html#sec-cli-batscri",
    "title": "CLI",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g. timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-wsl",
    "href": "qmd/cli.html#sec-cli-wsl",
    "title": "CLI",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for –distribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html",
    "href": "qmd/cloud-services.html",
    "title": "4  Cloud Services",
    "section": "",
    "text": "4.1 Misc",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#misc",
    "href": "qmd/cloud-services.html#misc",
    "title": "4  Cloud Services",
    "section": "",
    "text": "See Cloud Costs Every Programmer Should Know for various service estimates in order to perform back-of-the-napkin calculations of project costs\nFor “Stead-State Workloads” requiring HPC, cloud compute doesn’t make economic sense\n\nSteady-State workloads are projects that are run near constantly\n\nSee Thread for discussion on scenarios, issues, and risks of your data center (DC) in the Cloud vs on-prem.\nExamples:\n\nAcademia: Where academics are in a queue to run experiments on the a HPC cluster\nWeather Forecasting: Forecasts are required nearly in real time, so these models run constantly\nFinancial transaction processing at a bank: The bank’s systems handle a constant stream of transactions\nInventory management system for a manufacturing plant: The system constantly receives updates on raw materials, production output, and finished goods.\nOthers: Week or two long analysis runs at hedge funds, genomic analysis jobs, a swath of AI training / fine tuning, Oil and Gas where they are plowing through seismic data constantly\n\n\n\nRStudio Server on your docker image allows you to access an ide connected to the server through a browser. Useful so you can make sure the correct packages are installed.\nServerless computing is a method of providing backend services on an as-used basis.\n\nA serverless provider allows users to write and deploy code without the hassle of worrying about the underlying infrastructure\nCharged based on their computation and do not have to reserve and pay for a fixed amount of bandwidth or number of servers, as the service is auto-scaling\ne.g. AWS Lambda (i.e. resources only get spun-up when an event is triggered)\n\nNVIDIA GPU Guide (thread)\n\nRTX 20-series or 30-series GPUs are forbidden from inclusion in data centers\nGeneral Recommendations (Oct 2022)\n\nA100 for model training\nT4 for inference workloads\n\nK80\n\nReleased in 2015, the K80 contained a lot of VRAM for the time (24 GB)\nCame before tensor cores and is relatively weak by today’s standards\nOnly okay for learning purposes\n\nP4\n\nReleased in 2016\nValue came from its low power consumption\nMay find it priced higher than its upgraded version (the T4), so recommended to avoid it\n\nT4\n\nReleased in 2018\nSignificant upgrade for inference workloads compared to the P4\nExtremely low power consumption, tensor cores, and plenty (16GB) of VRAM\nCheap, so if you have an inference workload, recommended to strongly consider a T4\n\nP100\n\nBig improvement for model training workloads over the K80 when released\nLess RAM (16GB) than K80\nWay more compute  than K80\n\nCan see memory savings from using mixed-precision training\n\nNo tensor cores\n\nV100\n\nHuge upgrade over the P100\nSame VRAM as P100 many but more CUDA cores\nIntroduces Tensor Cores\nMore cost-efficient than the P100\n\nA100\n\nnewest data center GPU\nupgraded tensor cores\nmost benchmarks show 3x+ faster training compared to the V100\n80GB VRAM\nPrice tag might be big, but it’s usually worth it over the V100",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#price-management",
    "href": "qmd/cloud-services.html#price-management",
    "title": "4  Cloud Services",
    "section": "4.2 Price Management",
    "text": "4.2 Price Management\n\nspot instances for cheaper machines\nautoscaling (kubernetes?) to handle peak usage times (spin-up more machines) while saving during slow times (spin down excess machines)\nUse opensource project management tools (dvc, airflow, etc)\nGoogle\n\nThe Google Kubernetes Engine (GKE) control plane is free, whereas Amazon’s (EKS) costs $0.20 an hour.\n\nAWS\n\nWith a well-defined framework of tag keys and values applied across different AWS resources, billing breakdowns by tag prove extremely useful for greater insight on the source of AWS charges — especially if resources are tagged by department, or team, or different layers of organizational granularity.\nReserved Instances - commit to specific configurations for one or three years at reduced cost\nSpot Instances - pay significantly lower costs but potential for applications to be interrupted\nSavings Plans\n\nEC2 Instance Savings Plans to reduce compute charges for specific instance types and AWS regions\n\nSavings of up to 72%\n\nCompute Savings Plans to reduce compute costs irrespective of type and region.\n\nSavings up to 66% and extends to ECS Fargate and Lambda functions.\n\n\nImage Management\n\nData Lifecycle Manager - automates the creation, retention, and deletion of images\n\nWill not manage images and snapshots created by other means, and it also excludes instance store-backed images.\nEC2 Recycle Bin - serves as a safety net to avoid the accidental deletion of resources — retaining images and snapshots for a configurable time where we may restore them before they are deleted permanently.\n\n\nLambda\n\nCloudwatch - Lambda automatically creates log groups for its functions, unless a group already exists matching the name /aws/lambda/[{functionName}]{style='color: #990000'}. These default groups do not configure a log retention period, leaving logs to accumulate indefinitely and increasing CloudWatch costs.\n\nExplicitly configure groups with matching names and a retention policy to maintain a manageable volume of logs.\n\nMemory Optimization - AWS Lambda Power Tuning can help to identify optimizations, albeit with notable initial costs given the underlying use of AWS Step Functions.\n\nLambda charges based on compute time in GB-seconds, where the duration in seconds is measured from when function code executes until it either returns or otherwise terminates, rounded up to the nearest millisecond. To reduce these times, we desire optimal memory configuration.\n\n\nS3 Lifecycle Configuration\n\nCharged for how much data stored, but also which S3 storage classes are utilized.\n\nStandard (default) class is the most expensive, permitting regular access to objects with high availability and short access times.\nInfrequent Access (IA) classes offer reduced cost for data which requires limited access (usually once per month)\nArchival options via Glacier deliver further cost reductions.\n\nConfiguring the lifecycle allows you to automatically transfer data to different storage classes and thereafter permanently delete it, X and Y days respectively after data creation",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#kaggle",
    "href": "qmd/cloud-services.html#kaggle",
    "title": "4  Cloud Services",
    "section": "4.3 Kaggle",
    "text": "4.3 Kaggle\n\nFree\n\n4-core CPU instances w/30 GB RAM\n2-core CPU, 2xT4 GPU w/13GB RAM\n\nT means tensor cores\n1 hour spent using 2xT4’s takes the same amount of your quota as a P100 (old free gpu offering)\n\nMeans 30-40 hours of free, multi-GPU compute per week",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#saturn-cloud",
    "href": "qmd/cloud-services.html#saturn-cloud",
    "title": "4  Cloud Services",
    "section": "4.4 Saturn Cloud",
    "text": "4.4 Saturn Cloud\n\nSaturn Cloud Recipes\n\nJSON files that specify your environment\nGood for keeping track of server dependencies (e.g. linux libraries)\n\nDunno about R packages",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#google-cloud-platform-gcp",
    "href": "qmd/cloud-services.html#google-cloud-platform-gcp",
    "title": "4  Cloud Services",
    "section": "4.5 Google Cloud Platform (GCP)",
    "text": "4.5 Google Cloud Platform (GCP)\n\nBigQuery sandbox is Google’s GCP free tier cloud SQL database. It’s free but your data only lasts 60 days at a time.\nGCP allows users to run deep learning workloads on TPUs\nSince data expires after 60 days, back-up the model coefficients and performance score tables to Google Sheets. Article suggested this is possible through WebUI.\nAs of Nov.19, regression, logistic regression, and k-nn are the only models available to be run with the sql query editor\nhttps://cloud.google.com/free/\n\n$300 credit for 12 months\nAlways free:\n\n2M requests for containers\n1 GB storage\n\nScalable NoSQL document database.\n50,000 reads, 20,000 writes, 20,000 deletes per day\n\nFunctions\n\n1 f1-micro instance per month (Available only in region: us-west1, Iowa: us-central1, South Carolina: us-east1)\n30 GB-months HDD\n5 GB-months snapshot in select regions\n1 GB network egress from North America to all region destinations per month (excluding China and Australia)\n\nKubernetes\n\nOne-click container orchestration via Kubernetes clusters, managed by Google.\nNo cluster management fee for clusters of all sizes\nEach user node is charged at standard Compute Engine pricing\n\nApp Engine\n\n28 instance hours per day\n5 GB Cloud Storage\nShared memcache\n1,000 search operations per day, 10 MB search indexing\n100 emails per day\n\nBigQuery\n\nFully managed, petabyte scale, analytics data warehouse.\n1 TB of querying per month\n10 GB of storage\n\nOther Stuff\n\nYour free trial credit applies to all GCP resources, with the following exceptions:\n\n* You can’t have more than 8 cores (or virtual CPUs) running at the same time.\n* You can’t add GPUs to your VM instances.\n* You can’t request a quota increase. For an overview of Compute Engine quotas, see Resource quotas.\n* You can’t create VM instances that are based on Windows Server images.\n\nYou must upgrade to a paid account to use GCP after the free trial ends. To take advantage of the features of a paid account (using GPUs, for example), you can upgrade before the trial ends. When you upgrade, the following conditions apply:\n\n* Any remaining, unexpired free trial credit remains in your account.\n* Your credit card on file is charged for resources you use in excess of what’s covered by any remaining credit.\nYou can upgrade your account at any time after starting the free trial. The following conditions apply depending on when you upgrade:\n* If you upgrade before the trial is over, your remaining credit is added to your paid account. You can continue to use the resources you created during the free trial without interruption.\n* If you upgrade within 30 days of the end of the trial, you can restore the resources you created during the trial.\n* If you upgrade more than 30 days after the end of the trial, your free trial resources are lost.\n\nSpot Instances (Preemptible VM)\n\nusage capped at 24 hrs\npricing is fixed and not market-driven\n\nGoogle price calculator: https://cloud.google.com/products/calculator/#id=3115f19f-4ff0-4c57-9028-69cb994fe7ca\nExample\n\ncreating a cluster with:\n\n1 x Dataproc cluster node with 30 GB of RAM\n3 x Dataproc worker nodes with 15 GB of RAM\nUsing less than 5 GB of disk space in a bucket\nAnd running the cluster for only 4 hrs\nWould cost only around $5 at the end of the month\n\n\nFree Tier\n\nincludes a 12-month free trial with $300 credit to use with any GCP services and an Always Free benefit, which provides limited access to many common GCP resources\nUse to test out, but KEEP EVERYTHING SMALL (data, hardware, etc). Need to upgrade it to see the true benefit. Free tier resources look like my desktop computer. Whatever cash is leftover should transfer to account.\nhttps://cloud.google.com/free/docs/gcp-free-tier#how-to-upgrade\nupgrade it from the free trial to a paid account through the GCP Console clicking the Upgrade button at the top of the page\n\n\n\nSteps for new project\n\nGo to interface https://console.cloud.google.com/\ncreate a project. “select a project” on top bar –&gt; “new project” on top right –&gt; choose name (optionally a folder/organization if you have one) –&gt; create\n(article wasn’t very reliable and went on talk about a python implementation so I stopped here\n\nTips\n\nApp Engine\n\nDon’t use App Engine Standard environments — big brother G wants you to use rather Flex environments, otherwise, they’ll punish you.\nReview cost analysis regularly to make sure there are no surprising costs.\nMake sure you clean up redundant App Engine application versions to prevent G from robbing you.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#microsoft-azure",
    "href": "qmd/cloud-services.html#microsoft-azure",
    "title": "4  Cloud Services",
    "section": "4.6 Microsoft Azure",
    "text": "4.6 Microsoft Azure\n\nhttps://azure.microsoft.com/en-us/free/?WT.mc_id=Revolutions-blog-davidsmi\nhttps://visualstudio.microsoft.com/dev-essentials/\n\nstarts azure trial but gives you free sql server developer edition\n\nWon’t be charged until you choose to upgrade.\n12 months access to $ services for free\n$200 credit for any service for 30 days\n\nAt the end of the 30 days, I think the remainder goes into your account after you change to a pay-to-play account\n\nAccess to the services that are always free\n\nAzure Kubernetes Service (AKS)\nFunctions\n\n1,000,000 requests per month\na solution for easily running small pieces of code in the cloud. You can write just the code you need for the problem at hand, without worrying about a whole application or the infrastructure to run it.\nExample use case: for handling WebAPI requests and sending the different data and results to where it needs to go.\n\nApp Service\n\n10 web, mobile, or API apps\n\nActive Directory B2C (identity)\n\n50,000 authentications per month\n\nMachine Learning Server\n\nDevelop and run R and Python models on your platform of choice.\n\nSQL Server 2017 Developer Edition\n\nBuild, test, and demostrate applications in a non-production environment.\n\nOther stuff\n\nBlob storage\n\nobject storage solution for the cloud\noptimized for storing massive amounts of unstructured data\n\nSpot Instances (Low Priority VM)\n\nnot time limit on instance usage\nno warning on termination by Azure\n\nTips\n\nIf you can’t create a service, because Azure servers are under maintenance for more than a couple of minutes — check out your permissions and registrations under the “Resource providers” panel.\nIf you see any strange errors on the Azure Portal — just change the filters’ values.\nIf you use Azure Machine Learning, and your scoring function cannot locate your source code — deliver the code as a Model and add it explicitly to the sys.path in the init function.\nIf you use Azure Machine Learning, don’t use Batch Endpoints — it looks like they are not ready yet — just use the regular Published Pipelines. In fact, “Batch endpoint” is just a wrapper around a published pipeline.\nDon’t include flask in your Azure conda environment specification.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#aws",
    "href": "qmd/cloud-services.html#aws",
    "title": "4  Cloud Services",
    "section": "4.7 AWS",
    "text": "4.7 AWS\n\nInstance types\n\nc-type instances are compute heavy\nr-type instances are RAM heavy\nm-type instances are balanced\n“Each thread is represented as a virtual CPU (vCPU) on the instance. An instance has a default number of CPU cores, which varies according to instance type. For example, an m5.xlarge instance type has two CPU cores and two threads per core by default—four vCPUs in total.”\nspot prices from 03/24/2020, all calculations over the previous month\ngen purpose\n\nm6g.8xlarge\n\ngen purpose, 32 vcpu, 128 gb\nnewer graviton, didn’t see any specs, but supposed to be much better than the xenon 1st gen\n\nm5.8xlarge\n\ngen purpose, 32 vcpu, 128 gb\nolder 3.1 ghz, xenon\non-demand $1.54/hr\n\nm5a.8xlarge\n\ngen purpose 32 vcpu, 128 gb\n2.4 ghz, slower processor speed than m5\n\nm5n.8xlarge\n\ngen purpose 32 vcpu, 128 gb\n3.1 ghz, xenon specialized for neural networks, ML tasks\nn.virg, 71% savings, &lt;5% interruption\nohio, 83% savings, &lt;5% interruption\non-demand $1.90/hr\npotential spot price = $0.32\n\nm5dn.8xlarge\n\nsame but with 2 ssd hard drives\n\nm4.10xlarge\n\ngen purpose 40 vcpu, 160 gb\n2.4 ghz\nsmaller write-up, get the sense these are older processors/instances\n\n\ncompute optimized\n\nRequires HVM AMIs that include drivers for ENA (network adaptor) and NVMe (ssd hard drives)\n\nseems standard on a lot of instances (gen purpose and here), shouldn’ t be an issue\n\nc5.9xlarge\n\n36 vcpu, 72 gb\n3.4 ghz\non-demand $1.53/hr\n\nc5d.9xlarge\n\nsame but with ssd\n\nc5n.9xlarge\n\n36 vcpu, 96 gb\n3.0 ghz, built for task needing high throughput for networking\non-demand, $1.94/hr\n\nc4.8xlarge\n\n36 vcpu, 60 gb\n2.9 ghz\n67% savings, &lt;5% interruption\non-demand $1.59/hr\npotential spot price = $0.52\n\n\nmemory optimized\n\nr5.8xlarge\n\n32 vcpu, 256 gb\n3.1 ghz\nn.virg, 72% savings, 5-10% interruption\nn.cal, 76% savings, &lt;5% interruption\non-demand $2.02/hr\npotential spot price = $0.48\n\nr5a.8xlarge\n\n32 vpu, 256 gb\n2.5 ghz\n\nr5n.8xlarge\n\n32 vcpu, 256 gb\n3.1 ghz, neural network optimized\nus.west. oregon 76% savings, 5-10% interruption\non-demand $2.38/hr\npotential spot price = $0.57\n\nr4.8xlarge\n\n32 vcpu, 244 gb\n2.3 ghz\n\nz1d.6xlarge\n\n24 vcpu, 192 gb\n4.0 ghz\non-demand $2.23\n\n\naccelerated computing\n\ninf1.6xlarge\n\n24 vcpu, 48 gb\nbuilt for ML\non-demand $1.91/hr\n\n\n\nFree Tier (12 months after sign-up)\n\naws.amazon.com – pricing (top) – free tier (mid) – create a free account (mid)\nEC2\n\n750 hrs/mo of t2-micro instance usage\n\nfor Linux, Windows, RHEL, SLES AMIs\n\n\nElastic Block Storage (EBS)\n\n30 GB\ncan be connected to an ec2\n\nElastic Container Registry\n\n500 MB per month\n\nfor storing and retrieving Docker images\nexample in course was a basic nginx image and it was 50MB\n\n\nS3\n\n5 GB of standard storage (high availability/ high durability)\n20,000 Get Requests, 2000 Put Requests per month\n\nElastic Load Balancing\n\n750 hrs per month shared between classic and application load balancers\n\nno idea what the differences are between classic and application\n\n\n\nPricing\n\nPrice per GPU as of 29-06-2023\n\n\nExamples\n\nr3.4xlarge 16 CPUs, 122 GB RAM, 1 x 320 SSD, Spot Price: $0.1517/h\n\nTrained H2O GBM, RF, XGBoost, DeepLearning. Cluster ran for 2 hr 40 min. Total Cost = around $0.42\nhttps://www.daeconomist.com/post/2019-01-15-partii/\n\n\nStorage\n\nS3\n\ncharged by amount stored\n\n$0.023/GB for standard (for first 50 TB)\n0.004/GB for glacier and 0.00099/GB deep glacier\n\ntakes longer to retrieve and not always available\n\n\nfree inbound transfer\nfree transfer between aws services (e.g. S3 to EC2) within the same region\n\nAurora\n\nstorage + inbound/outbound: $0.20 per million requests\n\n\nConsolidated Biling\n\na separate account. All company individual accounts (marketing, sales, etc.) bills are pooled into this account\nhas no access to services\nhas no permissions to access services in other accounts\npooled bill counted towards potential discount billing\n\nCalculators\n\nTotal Cost of Ownership (TCO) calculator\n\ncompares cost of running a project on-premises to aws cloud\n\naws pricing calculator\n\ncalculates price of running a cloud application\ncalculator.aws.com\nestimates cost per service, per service group, and total infrastructure\nhelps find right ec2 instance and region\n\n\nBilling and Cost Management console\n\ncost explorer\n\nview and analysis costs and usage\n\n\n\n\nSpot Instances\n\nSummary\n\nGo to spot advisor and find instances that fit budget and compute requirements\nPrepare strategy for interruption\nOther services\n\nAs of Jan 01, 2019, cloudyr’s aws.ec2 PKG didn’t support all spot instances.\nno time limit on instance usage\nAWS gives a 2 min warning when it decides it needs your spot instance\npricing is market driven depending on capacity levels at the time\nAvailable actions when Amazon “interrupts” your instance:\n\nHibernation:\n\n“like closing your laptop display”\nsaves data and memory and reboots once instance is available again\nRight before interruption, a daemon on the instance freezes the memory and stores it in Elastic Block Store (EBS) root volume\nYour EC2 will retain this root volume and any other EBS data volumes\nOnce market price falls below bid price, instance resumes with memory restored from disk to RAM\nYou aren’t charged while instance is in hibernation, but EBS volumes do cost $.\nAvailable for instance types: C3, C4, M4, R3, and R4 with &lt; 100 GB RAM on Amazon’s Linux, Ubuntu, and Windows\nAll this is done by something called the EC2 Hibernation Agent which sound like its just the name of the program on the servers\n\nStop\n\n“like shutting down your computer to be turned on later”\nlose whatever is in RAM but retain EBS data volumes ($)\nrestores once bid price &lt; market price\n\nTerminate\n\n***default option***\neverything deleted\n\n\nSpot Advisor\n\n**always use this before spinning up spot instances **\nhttps://aws.amazon.com/ec2/spot/instance-advisor\nInput\n\nvCPUs\nMemory size\nPlatform (linux?)\navailability zone (region?)\namount required (number of instances?)\n\noutput\n\ninstance type\nvCPUs\nMemory (GB)\nSavings over On-Demand (%)\nFrequency of termination (%)\n\nliklihood your instance will get terminated\n\n\n\nRunInstance API\n\nFor requesting a spot instance through CLI I think\nLooks like you send something that looks like a python dict with max price, type, region, etc. to this API\n\nSpot Blocks\n\nallows you to set a finite duration that your instance will run for\n\n1 to 6 hrs\nno interruption during that time\n\ntypically 30 to 45% cheaper than on-demand and maybe an additional 5% cheaper during non-peak hours for the region\nrecommended for batch runs\n\nStrategy\n\nUse regions with largest pools of spot instances\n\nLargest pools\n\nus.east.1 (north.virginia)\neu.west.1(ireland)\n\nThese regions have most types/most instances available\nTypically can go uninterrupted for weeks\nless price fluctuation = more certainty\n\n\nSmallest pools\n\neu.central.1 (frankfort)\nap.south.1 (mumbai)\nap.southeast.1 (singapore)\n\ntypically get interrupted within days\n\n\n\nRun groups of instances that come from multiple spot pools\n\nTo used different compute types, jobs/tasks need to be in containers\nspot pools are instances with same region, type, OS, etc.\napplications running on instances from a least 5 different pools can cut interruptions by up to 80%\n\n\nManaging/preparing for interruptions\n\nOnly use for jobs that are short lived\n\ndevelopment and staging environments, short data processing, proof-of-concept, etc.\n\nBuild internal management system that automatically handles interruptions\n\nlook at spot pool historical prices for past 90 days\n\nlooking for least volatile pools\nolder generation (e.g. c-family, m-family) tend to be most stable\n\n\nUse 3rd party platform that manages spot instances and interruptions\n\nSpotinst - uses ML to choose and manage instances that optimizes price and provide continuous activity for apps that are without a single point of failure.\n\nUses on-demand as a fall-back.\nSLA guarantees 99.9% availability.\nSnapshots volumes to migrate data to new instances in case of interruption.\nworks with other services and platforms (kubernetes, codedeploy, etc.)\n\nSpot Fleet - aws service, automanages groups of spot instances according to either of the following strategies:\n\nstrategy options\n\nlowest price - lowest price instances\ndiversified - spread instances across pools\n\nAfter receiving 2 min warning,\n\ntake snapshots of AMI and any attached EBS volumes and use them to launch a new instance.\n\nsnapshot of AMI\n\non EC2 dashboard – left panel – instances – instances\n\nright-click instance – image – create AMI\n\nimage is in left -panel – Images – AMIs\n\n\n\nActually both snapshots might be able to taken in left panel – spot requests\n\nsee AWS note – EC2 for further details\n\n\n\n\n\n\nneed to drain and detach instance from elastic load balancer if one is used\nIf using auto-scaling, need to create an on-demand group and a spot instance group\n\n\nKubernetes\n\nAfter receiving 2 minute interruption warning from AWS:\n\nDetach instance from elastic load balancer (ELB) is one is being used\nMark instance as unschedulable (?)\n\nprevents new pods (group of containers on an instance that performs a job) from being scheduled on that node\nunderlying compute capacity and scheduling of resources of the pods needs to be monitored. Compute capacity and pod resource requirements need to match.\n\n\n\n\n\nComparison\n\nMisc\n\nNotes from\n\nThe Top Clouds Evaluated Such That You Don’t Need to Repeat Our Mistakes\nAWS vs GCP reliability is wildly different\n\nNo services for blockchain development, quantum computing, and graph databases in GCP (May 2022)\nhttps://cloud-gpus.com/ - tool for comparing gpu compute prices across vendors\n\nData centers\n\nCloser the resources are to your business, the less latency\n(May 2022) GCP has caught up and surpassed AWS in the number of data centers and regions that are available\n\nCompute\n\nCheapest vCPU\n\nGCP “e2-micro-preemptible” with 2 vCPU and 1 GB memory.\n\n48% lower than “t4g.nano” from AWS\n5 times lower than “A0” from Azure.\n\nAWS is in-between GCP and Azure in terms of price (i.e. Azure most expensive for cheap vCPUs)\n\nMore performant GCP instances usually cost approximately the same as their analogs from other cloud providers\n\nAzure servers cost the same or slightly less than AWS\n\nGCP: dedicated PostgreSQL server\n\nCheapest instances are 25% lower than the competitors\n\nGPU on-demand availability\n\nConclusion: Assuming you need on-demand boxes to succeed right when you need them, the consensus seems to clearly point to AWS. If you can stand to wait or be redundant to spawn failures, maybe Google’s hardware acceleration customizability can win the day.\nStats\n\nAWS consistently spawned a new GPU in under 15 seconds (average of 11.4s).\nGCP on the other hand took closer to 45 seconds (average of 42.6s).\nAWS encountered one valid launch error in these two weeks whereas GCP had 84\n\nCaveats\n\nGCP allows you to attach a GPU to an arbitrary VM as a hardware accelerator - you can separately configure quantity of the CPUs as needed.\nAWS only provisions defined VMs that have GPUs attached\n\n\n\nRecommendations\n\nAzure\n\nYou use the Microsoft Office stack (Word, Teams, OneDrive, SharePoint, etc.) and/or C# programming language.\nYou head neither for the cheapest servers nor for the most expensive ones — you need something in the middle.\nYou need a memory-optimized solution rather than a general-purpose or a compute-optimized one.\nYou read about the current bugs and inconsistencies in Azure, and it does not scare you.\n\nAWS\n\nYou are rich.\nYou have AWS experts in your team.\nYou build an enterprise-level long-term project.\nOR you just want to rent a cheap virtual machine, and you don’t care about all the other facilities.\n\nGCP\n\nYou are a start-up company.\nYou can’t invest much time in learning AWS and dealing with Azure bugs.\nYou don’t need much flexibility and configuration facilities from the cloud.\nYou are ready to accept the approaches dictated by the platform.\nYou need either a general-purpose or a compute-optimized solution, but not a memory-optimized one.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html",
    "href": "qmd/db-engineering.html",
    "title": "Engineering",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-misc",
    "href": "qmd/db-engineering.html#sec-db-eng-misc",
    "title": "Engineering",
    "section": "",
    "text": "If you’re developing an application, a good rule of thumb is to write your frequently run queries in such a way that they return a response within 500 ms\nAWS Athena ($5/TB scanned)\n\nAWS Athena is serverless and intended for ad-hoc SQL queries against data on AWS S3\n\nParquet format supports indexing such that every pg has min-max stat\nColumn storage files (parquet) are more lightweight, as adequate compression can be made for each column. Row storage doesn’t work in that way, since a single row can have multiple data types.\n\n\n(See below) Apache Avro is smaller file size than most row format file types (e.g. csv)\n\nAthena doesn’t support indexed parquet formats\n{pins}\n\nConvenient storage method\nUse when:\n\nObject is less than a 1 Gb\n\nUsed {butcher} for large model objects\n\nSome model objects store training data\n\n\n\nBenefits\n\nJust need the pins board name and name of pinned object\n\nThink the set-up is supposed to be easy\n\nEasy to share; don’t need to understand databases",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-terms",
    "href": "qmd/db-engineering.html#sec-db-eng-terms",
    "title": "Engineering",
    "section": "Terms",
    "text": "Terms\n\nACID - A database transaction, by definition, must be atomic, consistent, isolated and durable. These are popularly known as ACID properties.  These properties can ensure the concurrent execution of multiple transactions without conflict. Guarantees data validity despite errors and ensure that data does not become corrupt because of a failure of some sort.\n\nCrucial to business use cases that require a high level of data integrity such as transactions happening in banking.\n\nBatch processing - performing an action on data, such as ingesting it or transforming it, at a given time interval.\nBTEQ - Batch Teradata Query (like SQL) is simply a utility and query tool for Teradata which is a relational database system Creating a BTEQ script to load data from a flat-file.\nConcurrency - multiple computations are happening at the same time\nData Dump - A file or a table containing a significant amount of data to be analysed or transferred. A table containing the “data dump” of all customer addresses.\nData Mart - A subset of a data warehouse, created for a very specific business use case. Finance data mart storing all the relevant financial information required by the Accounting team to process their month-end cycles.\nData Integration - Usually, the hardest part of the project, where multiple sources of data are integrated into a singular application/data warehouse. Integrating finance and customer relationship systems integrating into an MS SQL server database.\nData Lake - A repository for all kinds of structured and unstructured data. Mainly based on Hadoop storage technology. Called a lake as it is flexible enough to store anything from raw data to unstructured email files. Hadoop Data Lake. Storing logs of all customers called into the inbound call centre including call duration.\nData Mesh - Decentralized design where data is owned and managed by teams across the organisation that understands it the most, known as domain-driven ownership. tl;dr - Each department controls they’re own data from ingestion to “data products.” This data product is then made a available to the other departments for them to use in their projects. Each department has their own engineers, scientists, and analysts.\n\nEach business unit or domain aims to infuse product thinking to create quality and reusable data products — a self-contained and accessible data set treated as a product by the data’s producers — which can then published and shared across the mesh to consumers in other domains and business units — called nodes on the mesh.\nEnables teams to work independently with greater autonomy and agility, while still ensuring that data is consistent, reliable and well-governed.\nYou don’t have to figure out who’s in charge of what data, who gets to access it, who needs to protect it and what controls and monitoring is in place to ensure things don’t go wrong.\nExample: Banking\n\nCredit risk domain’s own data engineers can independently create and manage their data pipelines, without relying on a centralised ingestion team far removed from the business and lacking in credit expertise. This credit team will take pride in building and refining high-quality, strategic, and reusable data products that can be shared to different nodes (business domains) across the mesh.\n\n\nData Models - A way of organising the data in a way that it can be understood in a real-world scenario. Taking a huge amount of data and logically grouping it into customer, product and location data.\nData Quality - A discipline of measuring the quality of the data to improve and cleanse it. Checking Customer data for completeness, accuracy and validity.\nData Replication - There are multiple ways to do this, but mainly it is a practice of replicating data to multiple servers to protect an organisation against data loss. Replicating the customer information across two databases, to make sure their core details are not lost.\nDenormalization - database optimization technique in which we add redundant data to one or more tables. Designers use it to tune the performance of systems to support time-critical operations. Done in order to avoid costly joins. Me: Seems like it’s kind of like a View except a View might have calculated columns in it.\nDimensions - A data warehousing term for qualitative information. Name of the customer or their country of residence.\nDistributed SQL -  a single logical database deployed across multiple physical nodes in a single data center or across many data centers if need be; all of which allow it to deliver elastic scale and resilience. Billions of transactions can be handled in a globally distributed database.\nEDW - The same as a data warehouse except it includes all the data within an organisation. This means that the entire enterprise can rely on this warehouse for their business decisions. Organising sales, customer, marketing and finance data in an enterprise data warehouse to be able to create several key management reports.\nEmbedded aka In-Process\n\nEmbedded database as in a database system particularly designed for the “embedded” space (mobile devices and so on.) This means they perform reasonably in tight environments (memory/CPU wise.)\nEmbedded database as in databases that do not need a server, and are embedded in an application (like SQLite.) This means everything is managed by the application.\n\nFacts - A data warehousing term for quantitative information. The number of orders placed by a customer.\nFlat File - Commonly used to transfer data due to their basic nature; flat files are a single table storing data in a plain text format. All customer order numbers stored in a comma-separated value (.csv) file\nHTAP - Hybrid Transactional Analytical Processing - System that attempts be good at both OLAP and OLTP\nMaster Data - This is data that is the best representation of a particular entity in the business. This gives you a 360 view of that data entity by generally consolidating multiple data sources. Best customer data representation from multiple sources of information.\nMulti-Master - allows data to be stored by a group of computers, and updated by any member of the group. All members are responsive to client data queries. The multi-master replication system is responsible for propagating the data modifications made by each member to the rest of the group and resolving any conflicts that might arise between concurrent changes made by different members.\n\nAdvantages\n\nAvailability: If one master fails, other masters continue to update the database.\nDistributed Access: Masters can be located in several physical sites, i.e. distributed across the network.\n\nDisadvantages\n\nConsistency: Most multi-master replication systems are only loosely consistent, i.e. lazy and asynchronous, violating ACID properties. (mysql’s multi-master is acid compliant)\nPerformance: Eager replication systems are complex and increase communication latency.\nIntegrity: Issues such as conflict resolution can become intractable as the number of nodes involved rises and latency increases.\n\nCan be contrasted with primary-replica replication, in which a single member of the group is designated as the “master” for a given piece of data and is the only node allowed to modify that data item. Other members wishing to modify the data item must first contact the master node. Allowing only a single master makes it easier to achieve consistency among the members of the group, but is less flexible than multi-master replication.\n\nNiFi - It is an open-source extract, transform and load tool (refer to ETL), this allows filter, integrating and joining data. Moving postcode data from a .csv file to HDFS using NiFi.\nNormalization - A method of organizing the data in a granular enough format that it can be utilised for different purposes over time. Organizing according to data attributes reduces or eliminates data redundancy (i.e. having the same data in multiple places). Usually, this is done by normalizing the data into different forms such as 1NF (normal form) or 3NF (3rd normal form) which is the most common. (See DB, Relational &gt;&gt; Normalization)\n\nTaking customer order data and creating granular information model; order in one table, item ordered in another table, customer contact in another table, payment of the order in another table. This allows for the data to be re-used for different purposes over time.\n\nNULL indexes - These are the indexes that contain a high ratio of NULL values\nObject-Relational Mapping (ORM) - Allows you to define your data models in Python classes, which are then used to create and interact with the database. See {{SQLAlchemy}}\nODS - Operational data store generally stores limited and current information to help simple queries. Unable to handle historical or complex data queries. An ODS for daily stock fluctuations in a warehouse help the warehouse manager decide what to prioritise in the next order delivery.\nOLAP - Online Analytical Processing - large chunks of tables are read to create summaries of the stored data\n\nUse chunked-columnar data representation\n\nOLTP - Online Transactional Processing - rows in tables are created, updated and removed concurrently\n\ntraditionally use a row-based data representation\npostgres excels at this type of processing\n\nRDBMS - Relational database management system. All of the above examples are RDBMS, meaning they store data in a structured format using rows and columns.\n\nA Microsoft SQL server database.\n\nReal-Time Processing (aka Event Streaming) - each new piece of data that is picked up triggers an event, which is streamed through the data pipeline continuously\nReverse ETL - Instead of ETL where data is transformed before it’s stored or ELT where data is stored and transformed while in storage, Reverse ETL performs transformations in the pipeline between Storage and the Data Product.\n\nSCD Type 1–6 - A method to deal with changes in the data over time in a data warehouse. Type 1 is when history is overwritten whereas Type 2 (most common) is when history is maintained each time a change occurs.\n\nWhen a customer changes their address; SCD Type 1 would overwrite the old address with the new one, whereas Type 2 would store both addresses to maintain history.\n\nSchemas - A term for a collection of database objects. These are generally used to logically separate data within the database and apply access controls.\n\nStoring HR data in HR schema allows logical segregation from other data in the organisation.\n\nSharding - Horizontal Partitioning — divides the data horizontally and usually on different database instances, which reduces performance pressure on a single server.\n\nStaging - The name of a storage area that is temporary in nature; to allow for processing of ETL jobs (refer to ETL).\n\nA staging area in an ETL routine to allow for data to be cleaned before loading into the final tables.\n\nTransactional Data - This is data that describes an actual event.\n\nOrder placed, a delivery arranged, or a delivery accepted.\n\nUnstructured Data - Data that cannot be nicely organised in a tabular format, like images, PDF files etc.\n\nAn image stored on a data lake cannot be retrieved using common data query languages.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-datqual",
    "href": "qmd/db-engineering.html#sec-db-eng-datqual",
    "title": "Engineering",
    "section": "Data Quality",
    "text": "Data Quality\n\nAlso see Production, Data Validation\nAccuracy - addresses the correctness of data, ensuring it represents real-world situations without errors. For instance, an accurate customer database should contain correct and up-to-date addresses for all customers.\nCompleteness - extent your datasets have all the required information on every record\n\nMonitor: missingness\n\nConsistency - extent that no contradictions in the data received from different sources. Data should be consistent in terms of format, units, and values. For example, a multinational company should report revenue data in a single currency to maintain consistency across its offices in various countries.\nTimeliness - Data should be available at the time it’s required in the system\nValidity - ensuring that data adheres to the established rules, formats, and standards.\n\nMonitor: variable types/classes, numeric variable: ranges, number of decimal places, categorical variable: valid categories, spelling\n\nUniqueness - no replication of the same information twice or more. They appear in two forms; duplicate records and information duplication in multiple places.\n\nMonitor: duplicate rows, duplicate columns in multiple tables",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-costopt",
    "href": "qmd/db-engineering.html#sec-db-eng-costopt",
    "title": "Engineering",
    "section": "Cost Optimization",
    "text": "Cost Optimization\n\nAlso see\n\npage 53 in notebook\nGoogle, BigQuery &gt;&gt; Optimization\n\nAvoid disk operations, make sure that you look out for hints & information in the EXPLAIN PLAN of your query. (e.g. using SORT without an index)\n\nWhen you see filesort, understand that it will try to fit the whole table in the memory in many chunks.\n\nIf the table is too large to fit in memory, it will create a temporary table on disk.\n\nLook out for a using filesort with or without a combination of using temporary.\n\nSplit tables with many columns Might be efficient to split the less-frequently used data into separate tables with a few columns each, and relate them back to the main table by duplicating the numeric ID column from the main table.\n\nEach small table can have a primary key for fast lookups of its data, and you can query just the set of columns that you need using a join operation.\n\nPrimary keys should be global integers.\n\nIntegers consume less memory than strings, and they are faster to compare and hash\n\nJoins\n\nWith correlated keys\n\nThe query planner won’t recognize the correlated keys and do nested loop join when a hash join is more efficient\nI don’t fully understand what correlated keys on a join are, but see SQL &gt;&gt; Terms &gt;&gt; Correlated/Uncorrelated queries\n\nIn the example below, a group of merge_commit_ids will only be from 1 repository id, so the two keys are associated in a sort of traditional statistical sense.\n\nSolutions\n\nUse LEFT_JOIN instead of INNER_JOIN\nUse extended statistics\nCREATE STATISTICS ids_correlation ON repository_id, merge_commit_id FROM pull_requests;\n\n“repository_id” and “merge_commit_id” are the correlated keys\nI’m not sure if “ids_correlation” is a function or just a user-defined name\nPostgreSQL ≥13 will recognize correlation and the query planner will make the correct calculation and perform a hash join\n\n\n\n\nPre-join data before loading it into storage\n\nIf a group of tables is frequently joined and frequently queried, then pre-joining will reduce query costs\ncan be done using an operational transform system such as Spark, Flow, or Flink (dbt can parallelize runs and work w/Spark)\n\nIndexes{#sec-db-eng-costopt-index}\n\nIndexes help in filtering data faster as the data is stored in a predefined order based on some key columns.\n\nIf the query uses those key columns, the index will be used, and the filter will be faster.\n\nSuitable for any combination of columns that are used in filter, group, order, or join\nMySQL Docs\nDon’t use indexes with LIKE\nCluster a table according to an index\n\nAlso see Google, BigQuery &gt;&gt; Optimization &gt;&gt; Partition and Cluster\nRearranges the rows of a table on the disk\nDoesn’t stay “clustered” if table is updated\n\nSee pg_repack for a solution\n\nExample\n-- create index\nCREATE INDEX pull_requests_repository_id ON pull_requests (repository_id, number)\n-- cluster table\nCLUSTER pull_requests USING pull_requests_repository_id\n\n\nUseful for queries such as\nSELECT *\nFROM pull_requests\nWHERE repository_id IN (...) AND number &gt; 1000\nBest Pactices\n\nAvoid too many indexes\n\nA copy of the indexed column + the primary key is created on disk\nIndexes add to the cost of inserts, updates, and deletes because each index must be updated\nBefore creating an index, see if you can repurpose an existing index to cater to an additional query\nCreate the least possible number of indexes to cover most of your queries (i.e. Covering Indexes).\n\nMakes effective use of the index-only scan feature\nAdd INCLUDE to the create index expression\nExample\n-- query\nSELECT y FROM tab WHERE x = 'key';\n-- covering index, x\nCREATE INDEX tab_x_y ON tab(x) INCLUDE (y);\n-- if the index, x, is unique\nCREATE UNIQUE INDEX tab_x_y ON tab(x) INCLUDE (y);\n\ny is called a non-payload column\n\nDon’t add too many non-payload columns to an index. Each one duplicates data from the index’s table and bloat the size of the index.\n\n\nExample: Query with function\n-- query\nSELECT f(x) FROM tab WHERE f(x) &lt; 1;\n-- covering index, x\nCREATE INDEX tab_f_x ON tab (f(x)) INCLUDE (x);\n\nWhere f() can be MEAN, MEDIAN, etc.\n\n\n\nFix unusable indexes\n\nIssues related to data types, collation (i.e. how it’s sorted), character set (how the db encodes characters), etc\nSometimes you can make the indexes work by explicitly forcing the optimizer to use them. (?)\n\nRepurpose or delete stale indexes\n\nIndexes are designed to serve an existing or a future load of queries on the database\nWhen queries change, some indexes originally designed to serve those queries might be completely irrelevant now\nAutomate stale index removal. Dbs keep statistics. Write a script to either notify you or just delete the index if it’s older and not been used past a certain threshold\n\nUse the most cost efficient index type\n\nExample: If your use case only needs a regular expression search, you’re better off having a simple index than a Full Text index.\n\nFull Text indexes occupy much more space and take much more time to update\n\n\nDon’t index huge tables (&gt; 100M rows), partition instead\n\nThen prune the partitions (partition pruning) you don’t need and create indexes for the partitioned tables you do keep.\n\n\nPartitioning\n\nAlso see Google, BigQuery &gt;&gt; Optimization &gt;&gt; Partition and Cluster\nSplits your table into smaller sub-tables under the hood\n\nNot viewable unless you check the table directory to see the multiple files that have been created\n\nThe same goes for indexes on that table.\n\n\nUse on tables with at least 100 million rows (BigQuery recommends &gt; 1 GB) Partitioning helps reduce table size and, in turn, reduces index size, which further speeds up the Data Warehouse (DWH) operations. But, partitioning also introduces complexity in the queries and increases the overhead of managing more data tables, especially backups. So try a few of the other performance techniques before getting to Sharding.\nPartition columns should always be picked based on how you expect to use the data, and not depending on which column would evenly split the data based on size.\n\nExample: partition on county because your analysis or transformations will largely be done by county even though since some counties may be much larger than others and will cause the partitions to be substantially imbalanced.\n\n\nUse ELT (e.g. load data from on-prem server to cloud, then transform) instead of ETL (transform data while on-prem, then load to cloud) for data pipelines\n\nMost of the time you have a lot of joins involved in the transformation step\n\nSQL joins are one of the most resource-intensive commands to run. Joins increase the query’s runtime exponentially as the number of joins increases.\nExample\n\nRunning 100+ pipelines with some pipelines having over 20 joins in a single query.\nEverything facilitated by airflow (see bkmk for code)\nETL: postgres on-prem server, sql queries with joins, tasks ran 12+ hours, then the transformed data is loaded to google storage\n\n13+ hrs for full pipeline completion\n\nELT: running the queries with the joins, etc. with bigquery sql on the data after it’s been loaded into google storage.\n\n6+ hrs for full pipeline completion\n\n\n\n\nUse Materialized Views\n\nA smaller data object that contains the subset of data resulting from a specific query\nWhereas a query happens after data is loaded, a materialized view is a precomputation\nThe computation is done once, and changes to the data are incorporated as they occur, making subsequent updates to the view much cheaper and more efficient than querying the entire database from scratch\n\nFetching a large table will be slower if you try to use multiple cores.\n\nYou have to divide up the table and recombine it. Plus setting up parallel network processes takes time.\nThe time used to fetch some data from the internet depends massively on the internet bandwidth available on your router/network.\n\nUse Random Access via http range header + sparse-hilbert index to optimize db for query searches\nCITEXT extension makes it so you don’t have use lower or upper which are huge hits on performance (at least they are in WHERE expressions) GIN custom indexes for LIKE and ILIKE\nCREATE EXTENSION IF NOT EXISTS btree_gin;\nCREATE EXTENSION IF NOT EXISTS pg_trgm;\nCREATE INDEX index_users_on_email_gin ON users USING gin (email gin_trgm_ops);\n\nCREATE EXTENSION adds btree and pg_trgm extensions\nindex_users_on_email_gin is the name of the index\nusers is the table\nUSING gin (email gin_trgm_ops)\n\ngin specifies that it’s a gin index\nemail is the field\ngin_trgm_ops is from the pg_trgm extension. It splits the index into trigrams which is necessary for the gin index to work with LIKE or ILIKE\n\nSlower to update than the standard ones. So you should avoid adding them to a frequently updated table.\n\nGiST indexes are very good for dynamic data and fast if the number of unique words (lexemes) is under 100,000, while GIN indexes will handle 100,000+ lexemes better but are slower to update.\n\n\nNULLS LASTputs the NULLS in a field in any sorting operations at the end\n\nThe default behavior of ORDER BY will put the NULLS first, so if you use LIMIT , you might get back a bunch of NULLS.\nUsing NULLS LAST fixes this behavior but its slow even on an indexed column\n\nExample: ORDER BY email DESC NULLS LAST LIMIT 10\n\nInstead use two queries\nSELECT *\nFROM users\nORDER BY email DESC\nWHERE email IS NOT NULL LIMIT 10;\n\nSELECT *\nFROM users\nWHERE email IS NULL LIMIT 10;\n\nThe first one would fetch the sorted non-null values. If the result does not satisfy the LIMIT, another query fetches remaining rows with NULL values.\n\n\nRebuild Null Indexes\nDROP INDEX CONCURRENTLY users_reset_token_ix;\nCREATE INDEX CONCURRENTLY users_reset_token_ix ON users(reset_token)\nWHERE reset_token IS NOT NULL;\n\nDrops and rebuilds an index to only include NOT NULL rows\nusers_reset_token_ix is the name of the index\nusers is the table\nI assume “reset_token has to be the field\n\nWrap multiple db update queries into a single transaction\n\nImproves the write performance unless the database update is VERY large.\nA large-scale update performed by a background worker process could potentially timeout web server processes and cause a user-facing app outage\nFor large db updates, add batching\n\n[Example]{.ribbon-highlight: db update has a 100K rows, so update 10K at a time.\nUPDATE messages SET status = 'archived'\n  WHERE id IN\n  (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 0);\nUPDATE messages SET status = 'archived'\n  WHERE id IN\n  (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 10000);\nUPDATE messages SET status = 'archived'\n  WHERE id IN\n  (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 20000);\n\nmessages is the table name\nI guess OFFSET is what’s key here.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-ets",
    "href": "qmd/db-engineering.html#sec-db-eng-ets",
    "title": "Engineering",
    "section": "Event Tracking Systems",
    "text": "Event Tracking Systems\n\nEvents are queued, then batch inserted into your db.\n\nStreaming events does not scale very well and is not fault tolerant.\n\nCommercial Services\n\nSegment\n\nMost popular option\nVery expensive\nSusceptible to ad blockers\nOnly syncs data once per hour or two\nMissing a few key fields in the schema it generates (specifically, session and page ids).\n\nFreshpaint is a newer commercial alternative that aims to solve some of these issues.\n\nOpen Source (each with a managed offering if you don’t feel like hosting it yourself)\n\nSnowplow is the oldest and most popular, but it can take a while to setup and configure.\nRudderstack is a full-featured Segment alternative.\nJitsu is a pared down event tracking library that is laser focused on just getting events into your warehouse as quickly as possible.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-stream",
    "href": "qmd/db-engineering.html#sec-db-eng-stream",
    "title": "Engineering",
    "section": "Streaming",
    "text": "Streaming\n\nStreaming or near real-time data\nData Characteristics\n\nIt is ingested near-real-time.\nUsed for real-time reporting and/or calculating near-real-time aggregates. Aggregation queries on it are temporal in nature so any aggregations defined on the data will be changed over time as the data comes.\nIt is append-only data but can have high ingestion rates so needs support for fast writes.\nHistorical trends can be analyzed to forecast future metrics. Algorithms like ARIMA are used to do time series forecasting.\n\nRelational databases can’t handle high ingestion rates and near-real-time aggregates without extensions.\nArchitectures\n\n\nTimeScale DB\n\nOpen source extension for postgresql\nSupport all things postgresql like relational queries, full SQL support(not SQL-like) as well as the support of real-time queries\nSupports an ingestion of 1.5M+ metrics per second per server\nNear-real-time aggregation of tables\nProvides integration with Kafka, kinesis, etc for data ingestion.\nCan be integrated with any real-time visualization tool such as Graphana\n\nPipeline DB\n\nOpen source extension for postgresql\nSimilar features as TimeScale DB\nEfficiency comes from it not storing raw data\n\nUsually, it’s recommended to store raw data",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-otools",
    "href": "qmd/db-engineering.html#sec-db-eng-otools",
    "title": "Engineering",
    "section": "Other Tools",
    "text": "Other Tools\n\nDataFold monitors your warehouse and alerts you if there are any anomalies (e.g. if checkout conversion rate drops suddenly right after a deploy).\nHightouch lets you sync data from your warehouse to your marketing and sales platforms.\nWhale is an open source tool to document and catalog your data. \nRetool lets you integrate warehouse data into your internal admin tools.\nGrowth Book that plugs into your data warehouse and handles all of the complicated querying and statistics required for robust A/B test analysis.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-relational.html",
    "href": "qmd/db-relational.html",
    "title": "7  Relational",
    "section": "",
    "text": "7.1 Misc",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Relational</span>"
    ]
  },
  {
    "objectID": "qmd/db-relational.html#sec-db-rel-misc",
    "href": "qmd/db-relational.html#sec-db-rel-misc",
    "title": "7  Relational",
    "section": "",
    "text": "Packages\n\n{dplyr}\n\ncompute stores results in a remote temporary table\ncollect retrieves data into a local tibble.\ncollapse doesn’t force computation, but instead forces generation of the SQL query.\n\nsometimes needed to work around bugs in dplyr’s SQL generation.\n\n\n{dm}\n\nCan join multiple tables from a db, but keeps the meta info such as table names, primary and foreign keys, size of original tables etc.\n\n\nRelational databases do not keep all data together but split it into multiple smaller tables. That separation into sub-tables has several advantages:\n\nAll information is stored only once, avoiding repetition and conserving memory\nAll information is updated only once and in one place, improving consistency and avoiding errors that may result from updating the same value in multiple locations\nAll information is organized by topic and segmented into smaller tables that are easier to handle\n\nOptimized for a mix of read and write queries that insert/select a small number of rows at a time and can handle up to 1TB of data reasonably well.\nThe main difference between a “relational database” and a “data warehouse” is that the former is created and optimized to “record” data, whilst the latter is created and built to “react to analytics”.\nTypes\n\nEmbedded aka In-Process (see Databases, Engineering &gt;&gt; Terms): DuckDB (analytics) and SQLite (transactional)\nServer-based: postgres, mysql, SQL Server\n\nMix of transactional and analytical\nDistributed SQL (database replicants across regions or hybrid (on-prem + cloud)\n\nmysql, postgres available for both in AWS Aurora (See below)\npostgres available using yugabytedb\nSQL Server on Azure SQL Database\nCloud Spanner on GCP\n\n\n\nWrapper for db connections (e.g. con_depA &lt;- connect_databaseA(username = ..., password = ...) )\n# ... other stuff including code for \"connect_odbc\" function\n\n# connection attempt loop\nwhile(try &lt; retries) {\n    con &lt;- connect_odbc(source_db = \"&lt;database name&gt;\"\n                        username = username,\n                        password = password)\n    if(class(con) == \"NetexxaSQL\") {\n        try &lt;- retries + 1\n    } else if (!\"NetezzaSQL\" %in% class(con) & try &lt; retries {\n        warning(\"&lt;database name&gt; connection failed. Retrying...\")\n        try &lt;- try + 1\n        Sys.sleep(retry_wait)\n    } else {\n        try &lt;- try + 1\n        warning(\"&lt;database name&gt; connection failed\")\n    }\n}\n\nGuessing “NetezzaSQL” is some kind of error code for a failed connection to the db\n\nBenchmarks\n\nExample\n\nData\n\n~54,000,000 rows and 6 columns\n10 .rds files with gz compression is 220MB total,\n\nIf they were .csv, 1.5 GB\n\nSQLite file is 3 GB\nDuckDB file is 2.5 GB\nArrow creates a structure of directories, 477 MB total\n\nOperation: read, filter, group_by, summarize\nResults\n##  format          median_time mem_alloc\n##  &lt;chr&gt;              &lt;bch:tm&gt; &lt;bch:byt&gt;\n## 1 R (RDS)              1.34m    4.08GB\n## 2 SQL (SQLite)          5.48s    6.17MB\n## 3 SQL (DuckDB)          1.76s  104.66KB\n## 4 Arrow (Parquet)      1.36s  453.89MB\n\nTradional relational db solutions balloon up the file size\n\nSQLite 2x, DuckDB 1.66x (using csv size)",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Relational</span>"
    ]
  },
  {
    "objectID": "qmd/db-relational.html#sec-db-rel-brands",
    "href": "qmd/db-relational.html#sec-db-rel-brands",
    "title": "7  Relational",
    "section": "7.2 Brands",
    "text": "7.2 Brands\n\nSQLite vs MySQL as transactional dbs (article)\n\nSQLite:\n\nembedded, size ~600KB\nlimited data types\nbeing self-contained, other clients on a network would not have access to the database (no multi-users) unlike with MySQL\nno built-in authentication that is supported\nmultiple processes are able to access the database at the same time, but making changes at the same time is not something supported\nUse Cases\n\nData being confined in the files of the device is not a problem\nNetwork access to the db is not needed\nApplications that will minimally access the database and not require heavy calculations\n\n\nMySQL:\n\nopposites of the sqlite stuff\nSize ~600MB\nsupports replication and scalability\nSecurity is a large; built-in features to keep unwanted people from easily accessing data\nUse cases\n\ntransactions are more frequent like on web or desktop applications\nif network capabilities are a must\nmulti-user access and therefore security and authentication\nlarge amounts of data\n\n\n\nMySQL\n\nInstallation docs\nBasic intro\nSee SQL notebook\n\nCloud SQL - Google service to provide hosting services for relational dbs (see Google, BigQuery &gt;&gt; Misc). Can use postgres, mysql, etc. on their machines.\n\nCloud SQL Insights - good query optimization tool\n\nAWS RDS for db instances (see Database, postgres &gt;&gt; AWS RDS)\n\nAvailable: Amazon Aurora, MySQL, MariaDB, postgres, Oracle, Microsoft SQL Server\nRDS (Relational Database Service)\n\nBenefits over hosting db on EC2: AWS handles scaling, availability, backups, and software and operating system updates\n\nS3 is like googledrive or dropbox\n\nCon: only contains data about the files, not what’s inside them, i.e. no querying\nIdeal use cases\n\nbackup for logs,\nraw sensor data for your IoT application,\ntext files from user interviews\nimages\ntrained machine learning models (with the database simply storing the path to the object)\n\nAlternative: Minio\n\nOpen-Source alternative to AWS S3 storage.\nGiven that S3 often stores customer PII (either inadvertently via screenshots or actual structured JSON files), Minio is a great alternative to companies mindful of who has access to user data.\n\nOf course, AWS claims that AWS personnel doesn’t have direct access to customer data, but by being closed-source, that statement is just a function of trust.\n\n\n\n\nAWS Aurora - MySQL- and PostgreSQL-compatible enterprise-class database\n\nstarting at &lt;$1/day.\nsupports up to 64TB of auto-scaling storage capacity, 6-way replication across three availability zones, and 15 low-latency read replicas.\nCreate MySQL and Postgres instances using AWS Cloudformation\n\nAWS DynamoDB - for creating and querying NoSQL databases.\nSQLite\n\n{RSQLite}\n\nApache Avro\n\nrow storage file format unlike parquet\na single Avro file contains a JSON-like schema for data types and the data itself in binary format\n4x slower reading than csv but 1.5x faster writing than csv\n1.7x smaller file size than csv",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Relational</span>"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html",
    "href": "qmd/db-warehouses.html",
    "title": "Warehouses",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-misc",
    "href": "qmd/db-warehouses.html#sec-db-ware-misc",
    "title": "Warehouses",
    "section": "",
    "text": "The main difference between a “relational database” and a “data warehouse” is that the former is created and optimized to “record” data, whilst the latter is created and built to “react to analytics.”\nOptimized for read-heavy workloads that scan a small number of columns across a very large number of rows and can easily scale to petabytes of data\nCons\n\nCan become expensive when an organization needs to scale them\nDo not perform well when handling unstructured or complex data formats.\n\nPros\n\nIntegrating multiple data sources in a single database for single queries\nMaintaining data history, improving data quality, and keeping data consistency\nProviding a central view for multiple source system across the enterprise\nRestructuring data for fast performance on complex queries",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#olap-vs-oltp",
    "href": "qmd/db-warehouses.html#olap-vs-oltp",
    "title": "Warehouses",
    "section": "OLAP vs OLTP",
    "text": "OLAP vs OLTP\n\n\nOLAP (Online Analytical Processing)(aka the Cube)(Data Warehouses)\n\ndb designed to optimize performance in analysis-intensive applications\nAggregates transactions to be less frequent but more complex\nExamples: Snowflake, Bigquery\n\nOLTP (Online Transaction Processing) db designed for frequent, small transactions\n\nExecutes a number of transactions occurring concurrently (i.e. at the same time)\nUse cases: online banking, shopping, order entry, or sending text messages\n\nData model: OLTP systems typically use a normalized data model, which means that data is stored in multiple tables and relationships are defined between the tables. This allows for efficient data manipulation and ensures data integrity. OLAP systems, on the other hand, often use a denormalized data model, where data is stored in a single table or a small number of tables. This allows for faster querying, but can make data manipulation more difficult.\nData volume: OLTP systems typically deal with smaller amounts of data, while OLAP systems are designed to handle large volumes of data.\nQuery complexity: OLTP systems are designed to handle simple, short queries that involve a small number of records. OLAP systems, on the other hand, are optimized for more complex queries that may involve aggregating and analyzing large amounts of data.\nData updates: OLTP systems are designed to support frequent data updates and insertions, while OLAP systems are optimized for read-only access to data.\nConcurrency: OLTP systems are designed to support high levels of concurrency and handle a large number of transactions simultaneously. OLAP systems, on the other hand, are optimized for batch processing and may not perform as well with high levels of concurrency.",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-brands",
    "href": "qmd/db-warehouses.html#sec-db-ware-brands",
    "title": "Warehouses",
    "section": "Brands",
    "text": "Brands\n\nAmazon Redshift, DynamoDB, RDS, S3\n\nRedshift is best when you have data engineers who want control over infrastructure costs and tuning.\nRDS (Relational Database Service)\n\nBenefits over hosting db on EC2: AWS handles scaling, availability, backups, and software and operating system updates\n\nS3 is like googledrive or dropbox\n\nCon: only contains data about the files, not what’s inside them, i.e. no querying\nIdeal use cases\n\nBackup for logs,\nRaw sensor data for your IoT application,\nText files from user interviews\nImages\nTrained machine learning models (with the database simply storing the path to the object)\n\nAlternative: Minio\n\nOpen-Source alternative to AWS S3 storage.\nGiven that S3 often stores customer PII (either inadvertently via screenshots or actual structured JSON files), Minio is a great alternative to companies mindful of who has access to user data.\n\nOf course, AWS claims that AWS personnel doesn’t have direct access to customer data, but by being closed-source, that statement is just a function of trust.\n\n\n\n\nGoogle BigQuery\n\nBest when you have very spiky workloads.\n\nSnowflake\n\nA cloud data warehouse for analytics. It’s columnar, which means that data is stored (under the hood) in entire columns instead of rows; this makes large analytical queries faster, so it’s a common choice for how to build analytical DBs.\nBest when you have a more continuous usage pattern\nSupport for semi-structured data, data sharing, and data lake integration\nResource: Snowflake Data Warehouse Tutorials\n\nAzure Synapse Analytics\n\nFully managed, cloud-based data warehousing service offered by Microsoft Azure. It offers integration with Azure Machine Learning and support for real-time analytics.\n\nData Bricks\n\nCompany behind spark technology and have built a cloud-based data warehousing service.\n\nTeradata\nSAP HANA\nClickHouse\n\nOpensource, built by Yandex (Russian search engine)\n\nApache Hadoop running Apache Hive\n\nHive: an open-source data warehouse solution for Hadoop infrastructure. It is used to process structured data of large datasets and provides a way to run HiveQL queries.\n\nResource: Apache Hive Tutorial with Examples",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-strat",
    "href": "qmd/db-warehouses.html#sec-db-ware-strat",
    "title": "Warehouses",
    "section": "Strategies",
    "text": "Strategies\n\nInmon\n\n\nPrioritizes accuracy and consistency of data above all else.\nQuerying is pretty fast (data marts)\nTends to be a lot of upfront work, however subsequent modifications and additions are quite efficient.\nRecommended if:\n\nData accuracy is the most important characteristic of your warehouse\nYou have time/resources to do a lot of upfront work\n\n\nKimball\n\n\nLess structured approach, which speeds up the initial development cycle.\nFuture iterations require the same amount of work, which can be costly if you’re constantly updating the warehouse Fast querying but very few quality checks\nRecommended if:\n\nIf you’re business requirements are well-defined and stable\nYou are querying lots of data often\n\n\nData Vault\n\n\nTrys to fix disadvantages of Kimball and Inmon strategies by waiting to the last minute to develop any kind of structure\nWorkflow: Sources –&gt; unstructured storage (data lake) –&gt; Staging which supports operations such as batch and streaming processes –&gt; data vault which stores all raw data virtually untouched (non-relational db?)\nAdvantages: efficient, fast to implement, and highly dynamic\nDisadvantages: querying can be quite slow\n\nUh doesn’t seem to be much cleaning either\n\nRecommended if:\n\nYour business goals change often\nYou need cheap server and storage costs",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-dsgn",
    "href": "qmd/db-warehouses.html#sec-db-ware-dsgn",
    "title": "Warehouses",
    "section": "Design",
    "text": "Design\n\nMisc\n\nBest practice to keep only a portion of data in the RAW database and use it to update our “BASE” or “PROD” database tables.\n\nDatabases Inside the Warehouse:\n\nRaw or Source: Raw data is inported into this db; source of truth\nBase or Prod: Where data is imported from Source db and has had basic field transformations; storage\nAnalytics: Where data is ready to be queried; ad-hoc analytics and materialized queries and views\n\nEnvironments\n\nEach db will have development and production branches\n\nDevelopment: staging, mocking and developing data transformations\nProduction: Data is validated and transformations applied on a schedule\n\n\nExample",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-trig",
    "href": "qmd/db-warehouses.html#sec-db-ware-trig",
    "title": "Warehouses",
    "section": "Database Triggers",
    "text": "Database Triggers\n\n\nA database trigger is a function that gets triggered every time a record is created or updated (or even deleted) in the source table (in this case, a transactional table)\nDatabase triggers provide an effective, solution to extracting data from the transactional system and seamlessly integrating it into the data warehouse while also not adversely impacting that system.\nUse case — You see a couple of data points in your transactional system’s tables that you would require for your reporting metrics but these data points are not being provided by your transactional system’s API endpoints. So, there is no way you can write a script in Python or Java to grab these data points using the API. You cannot use direct querying on your transactional system as it can negatively impact its performance.\nMisc\n\nNotes from Harnessing Triggers in the Absence of API Endpoints\n\nProvides a detailed step-by-step\n\nIf your transactional system does not have a lot of traffic (or) is not directly used by end-user applications, then it can be set up as a synchronous process. In that case, the lambda or the Azure functions would need to have the trigger event as the transactional database’s staging table. The appropriate database connection information would also need to be provided.\n\nDatabase Triggers\n\nDDL Triggers - Set up whenever you want to get notified of structural changes in your database\n\nUseful when you wish to get alerted every time a new schema is defined; or when a new table is created or dropped. Hence, the name DDL (Data Definition Language) triggers.\n\nDML Triggers - Fired when new records are inserted, deleted, or updated\n\ni.e. You’re notified anytime a data manipulation change happens in a system.\n\n\nSyntax: &lt;Timing&gt; &lt;Event&gt;\n\nTrigger Event - The action that should activate the trigger.\nTrigger Timing - Whether you need the trigger to perform an activity before the event occurs or after the event occurs.\n\nSpecialized triggers provided by cloud services\n\nAWS\n\nLambda Triggers: These triggers help initiate a lambda function when a specified event happens. Events can be internal to AWS, or external in nature. Internal events can be related to AWS services such as Amazon S3, Amazon DynamoDB streams, or Amazon Kinesis. External events can come in from the database trigger of a transactional system outside of AWS or an IoT event.\nCloudwatch Events: If you have used standalone relational databases such as Microsoft SQL Server and SQL Server Management Studio (SSMS), you may have used SQL Server Agent to notify users of a job failure. Cloudwatch is specific to AWS and is used not only to notify users of a job failure but also to trigger Lambda functions and to respond to events. The important difference between a CloudWatch Event and a Lambda Trigger is that while Lambda triggers refer to the capability of AWS Lambda to respond to events, CloudWatch Events is a broader event management service that can handle events from sources beyond Lambda. On a side note, while SQL Server Agent requires an email server to be configured, Cloudwatch has no such requirement.\n\nAzure\n\nBlob Trigger: Azure blobs are similar to S3 buckets offered by AWS. Similar to how Amazon S3 notifications can be used to get alerts about changes in S3 buckets; blob triggers can be used to get notified of changes in Azure blob containers.\nAzure Function Trigger: These are the Azure equivalent of AWS Lambda Function Triggers. These triggers can be used to initiate an Azure function in response to an event within Azure or an external event, such as an external transactional database trigger, an HTTP request, or an IoT event hub stream. Azure functions can also be initiated based on a pre-defined schedule using a Timer Trigger.\n\n\nExample: Transfer data from a transactional database to a warehouse (See article for further details)\n\nIdentify table in transactional db with data you want\nCreate a staging table that’s exactly like the transaction table\n\nEnsure that you don’t have any additional constraints copied over from the source transactional table. This is to ensure as minimal impact as possible on the transactional system.\nFor a bulk data transfer of historical transaction data:\n\nCREATE TABLE AS SELECT (SELECT * INTO in SQL Server) while creating the staging table. This will create the staging table pre-populated with all the data currently available in the transaction table.\nDo an empty UPDATE on all the records in the transaction table\n\ne.g. UPDATE TABLE Pricing_info SET OperationDate=OperationDate\nThis is not a recommended approach as it could bog down the transactional system due to the number of updates and undo statements generated. Moreover, the transaction table will also be locked during the entire update operation and will be unavailable for other processes thus impacting the transactional system. This method is okay to use if your transaction table is extremely small in size.\n\n\nIn addition to that, also have a column to indicate the operation performed such as Insert, Update, Delete).\n\nSet up a DML trigger directly on the transaction table\n\nAll DML events namely Insert, Delete, and Update in the transaction table should have a separate trigger assigned to them.\n\nThe below example shows the trigger for Insert. The rest of the triggers are created similarily — just by substituting 2 INSERTs (trigger event, select statement) for DELETE or UPDATE (See article for code) and using a different name in CREATE\n\nInsert trigger in (SQL Server)\n-- Create the trigger\nCREATE TRIGGER TransactionTrigger_pricing_Insert\nON Pricing_info\n--Trigger Event\nAFTER INSERT\nAS\nBEGIN\n    -- Insert new records into the staging table\n    INSERT INTO StagingTable_pricing (ID, Column1, Column2, OperationType)\n    SELECT ID, Column1, Column2, 'INSERT'\n    FROM inserted\nEND;\n\n“Pricing_info” is the name of transactional table with the data you want\n“StagingTable_pricing” is the name of the staging table\nAFTER INSERT where AFTER is the trigger timing and INSERT is the trigger event\nIn the SELECT statement, “INSERT” is the value for that extra column in the staging table that tells us which type of operation this was.\n\n\nSet-up the specialized trigger in the warehouse\n\nAWS \n\nA database DML trigger in the transactional system’s database. Whenever a new record comes into the transactional database table, the trigger would insert the new data into a staging table within the transactional database.\n\nIf you based it on a schedule (using AWS Cloudwatch events), the Lambda trigger would trigger a lambda function to grab the data from the staging table to a table in the datawarehouse (Redshift)\n\n\nAzure \n\nWhen the timer trigger activates, it would run the Azure Function which would then pick up the new/updated/deleted records from the staging table.",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/misc.html",
    "href": "qmd/misc.html",
    "title": "Misc",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-misc",
    "href": "qmd/misc.html#sec-misc-misc",
    "title": "Misc",
    "section": "",
    "text": "Windows\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + Tab\nChange application\n\n\nCtrl + ~\nChange window within an application\n\n\n\nBrowser\n\n\n\n\n\n\n\nAction\nShortcut\n\n\n\n\nTo Address Bar\nCtrl + L\n\n\nOpen a new window\nCtrl + n\n\n\nOpen a new window in Incognito mode\nCtrl + Shift + n\n\n\nOpen a new tab, and jump to it\nCtrl + t\n\n\nReopen previously closed tabs in the order they were closed\nCtrl + Shift + t\n\n\nJump to the next open tab\nCtrl + Tab or Ctrl + PgDn\n\n\nJump to the previous open tab\nCtrl + Shift + Tab or Ctrl + PgUp\n\n\nJump to a specific tab\nCtrl + 1 through Ctrl + 8\n\n\nJump to the rightmost tab\nCtrl + 9\n\n\nOpen your home page in the current tab\nAlt + Home\n\n\nOpen the previous page from your browsing history in the current tab\nAlt + Left arrow\n\n\nOpen the next page from your browsing history in the current tab\nAlt + Right arrow\n\n\nClose the current tab\nCtrl + w or Ctrl + F4\n\n\nClose the current window\nCtrl + Shift + w or Alt + F4\n\n\nMinimize the current window\nAlt + Space then n\n\n\nMaximize the current window\nAlt + Space then x\n\n\nQuit Google Chrome\nAlt + f then x\n\n\nMove tabs right or left\nCtrl + Shift + PgUp or Ctrl + Shift + PgDn\n\n\n\nR-devel (&gt;= 4.4.0) gained a command-line option to adjust the limit connections (previous limit was 128 parallel workers)\n$ R\n&gt; parallelly::availableConnections()\n[1] 128\n\n$ R --max-connections=512\n&gt; parallelly::availableConnections()\n[1] 512",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-rstud",
    "href": "qmd/misc.html#sec-misc-rstud",
    "title": "Misc",
    "section": "RStudio",
    "text": "RStudio\n\nJob: Run script in the background\nlibrary(rstudioapi)\njobRunScript(\"wfsets_desperation_tune.R\", name = \"tune\", exportEnv = \"R_GlobalEnv\")\n\nNeed to look up args\nI think exportEnv takes the variables in your current environment and runs the script with them as inputs\n\nShortcuts\n\n\n\n\n\n\n\nShortcut\nDescription\n\n\n\n\nAlt + Shift + k\nKeyboard Shortcuts\n\n\nCtrl + Shift + p\nCommand Palette\n\n\nCtrl + Shift + f\nFind in Files\n\n\nCtrl + Alt + up/down\nMultiple Cursors\n\n\nCtrl + Shift + z\nReverse Undo\n\n\nCtrl + Shift + a\nFormat highlighted code (style/linter the code)\n\n\nCtrl + d\nDelete current line\n\n\nAlt + up/down\nYank line up or down\n\n\nCtrl + Alt + up/down\nCopy the above line (or selected lines) down or up\n\n\nCtrl + .\nGo to file/function name\n\n\nAlt + Shift + m\nFocus on Terminal\n\n\n\n\nCustomizing Shortcuts in RStudio\n{shrtcts} - Make anything a shortcut in RStudio",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-hack",
    "href": "qmd/misc.html#sec-misc-hack",
    "title": "Misc",
    "section": "Hackathon Criteria",
    "text": "Hackathon Criteria",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-update",
    "href": "qmd/misc.html#sec-misc-update",
    "title": "Misc",
    "section": "Update R",
    "text": "Update R\n\nMisc\n\n{rig} - r version management system\nupdate.packages(checkBuilt = TRUE, ask = FALSE) is supposed to search for packages in other R versions and update them in the new R version, but I haven’t tried it, yet.\nErrors when compiling from source may require installing libraries and they’ll supply code to install via “pacman”\n\nOpen Start &gt;&gt; scroll down to RTools40 &gt;&gt; RTools Bash\nPaste pacman code and hit enter to install\n\nProblem packages in the past\n\n{brms} dependency, {igraph}, didn’t have a binary on CRAN and wouldn’t compile from source even with correct libraries installed.\n\nSol’n: install.packages(\"igraph\", repos = 'https://igraph.r-universe.dev')\n\ninstalls dev version from r-universe\n\n\nSome {easystats} packages had gave {pak} some problems. No difficulties using install.packages with default repo or if they had a r-universe repo though.\n\n\nSteps\n\nCopy user installed packages in current R version\n\nIn R:\nsquirrel &lt;- names(installed.packages(priority = \"NA\")[, 1]) # user installed packages\nreadr::write_rds(squirrel, \"packages.rds\")\n\nThen, close RStudio\n\n\nRTools: Check to see if you have the latest because you’ll need it to compile some of newest versions of packages.\n\nYour rtools folder has the version in it’s folder name.\nrtools website has the latest version and an .exe to download\n\nCheck/Update rig version\n\nIn powershell: rig --version\nCheck current rig release: link\nDownload and install if your version isn’t current\n\nInstall new version of R\n\nClose R if not already closed\nrig add release installs the latest version of R.\nrig default &lt;new_r_version&gt; sets that version as the default\n\nAdd R and RTools to path\n\nRight-click Windows &gt;&gt; System &gt;&gt; (right panel) Advanced System Settings &gt;&gt; Environment Variables &gt;&gt; Under User Variables, highlight Path, click Edit &gt;&gt; Click Add\n\nR: Add path to directory with all the RScript, R exe, etc. e.g. “C:\\Program Files\\R\\R-4.2.3\\bin\\x64”\nRTools: e.g. “C:\\rtools43\\usr\\bin”\n\n\nOpen R and confirm new version\n\nIf RStudio\n\nThe setting of the new version to the “default” version of R in rig should result in RStudio loading the new version.\nIf not, Tools &gt;&gt; Global Options &gt;&gt; General\n\nUnder “R version”, click “change” button; choose new R version\nQuit session and restart RStudio\n\n\n\nInstall “high maintenance” packages\n\nI’ve had issues with {pak} installing packages that need to be compiled. Maybe be worth trying {pak} first to see if they’ve fixed it.\n{cmdstanr} doesn’t live on CRAN, so you have to use: install.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\n\nCheck for latest cmdstan version\n\nAfter loading the package, library(cmdstanr) , it should run a check on your cmdstan version and tell you if there’s a newer version.\nTo update, first check toolchain: check_cmdstan_toolchain()\n\nMight tell you to update RTools or that you need some C++ library added\n\nFix C++ toolchain with check_cmdstan_toolchain(fix = TRUE)\nUpdate cmdstan: install_cmdstan()\nMay need to install {rstudioapi} and run rstudioapi::restartSession() (programmatically) or just ctrl + shift + f10 so that this package can be used as a dependency for other packages that need to be installed.\n\n\n{rstanarm}: install.packages(\"rstanarm\")\n\nInstall other packages\nmoose &lt;- readRDS(\"packages.rds\")\nmoose &lt;- moose[!moose %in% c(\"cmdstanr\", \"rstanarm\", \"ebtools\", \"translations\", \"&lt;RStudio add-ins&gt;\")]\n\n# Next time, add a try/catch? or maybe purrr::safely, so that it continues through errors. Also, need to log pkgs that do error.\nfor (i in seq_len(length(moose))) {\n  print(moose[i])\n  pak::pkg_install(moose[i])\n}\n\nfs::file_delete(\"packages.rds\")\n\n{ebtools} is my personal helper package.\n{translations} is a system package that shouldn’t have been included when I saved the packages from previous version, but was when I recently updated. Might not be necessary to include it in the excuded packages in the future.\n\nCheck for updates of RStudio (link)\n\nCurrent version under Help &gt;&gt; About Rstudio\nPossible to check for updates under Help &gt;&gt; Check for Updates, but that’s failed me before.",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html",
    "href": "qmd/spreadsheets.html",
    "title": "Spreadsheets",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-misc",
    "href": "qmd/spreadsheets.html#sec-spdsht-misc",
    "title": "Spreadsheets",
    "section": "",
    "text": "Some Excel files are binaries and in order to use download.file, you must set mode = “wb”\ndownload.file(url, \n              destfile = glue(\"{rprojroot::find_rstudio_root_file()}/data/cases-age.xlsx\"), \n              mode = \"wb\")\nIndustry studies show that 90 percent of spreadsheets containing more than 150 rows have at least one major mistake.",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-cats",
    "href": "qmd/spreadsheets.html#sec-spdsht-cats",
    "title": "Spreadsheets",
    "section": "Catastrophes",
    "text": "Catastrophes\n\nReleasing confidential information\n\nIrish police accidently handed out officers private information when sharing sheets with statistics due to a freedom of information request. (link)\n\nErrors when combining sheets\n\nWales dismissed anaesthesiologists after mistakenly deeming them “unappointable.” Spreadsheets from different areas lacked standardization in formatting, naming conventions, and overall structure. To make matters worse, data was manually copied and pasted between various spreadsheets, a time-consuming and error-prone process. (link)\nWhen consolidating assets from different spreadsheets, the spreadsheet data was not “cleaned” and formatted properly. The Icelandic bank’s shares were subsequently undervalued by as much as £16 million. (link)\n\nData entry errors\n\nCryto.com accidentally transferred $10.5 million instead of $100 into the account of an Australian customer due to an incorrect number being entered on a spreadsheet. (link)",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-bprac",
    "href": "qmd/spreadsheets.html#sec-spdsht-bprac",
    "title": "Spreadsheets",
    "section": "Best Practices",
    "text": "Best Practices\n\nNotes from Data organization in spreadsheets\n\nBe consistent\nWrite dates like YYYY-MM-DD\nDon’t leave any cells empty\nPut just one thing in a cell\nOrganize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row)\nCreate a data dictionary\nDon’t include calculations in the raw data files\nDon’t use font color or highlighting as data\nChoose good names for things\nMake backups\nUse data validation to avoid data entry errors\nSave the data in plain text files.",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-transspr",
    "href": "qmd/spreadsheets.html#sec-spdsht-transspr",
    "title": "Spreadsheets",
    "section": "Transitioning from Spreadsheet to DB",
    "text": "Transitioning from Spreadsheet to DB\n\nMisc\n\nWhen you start to have multiple datasets or when you want to make use of several columns in one table and other columns in another table you should consider going the local database route.\nUse db “normalization” to figure out a schema\nAlso see\n\nDatabases, Engineering &gt;&gt; Schema\nDatabases, Warehouses &gt;&gt; Design a Warehouse\n\n\nDB advantages over spreadsheets:\n\nEfficient analysis: Relational databases allow information to be retrieved quicker to then be analyzed with SQL (Structured Query Language), to then run queries.\n\nOnce spreadsheets get large, they can lag or freeze when opening, editing, or performing simple analyses in them.\n\nCentralized data management: Since relational databases often require a certain type or format of data to be input into each column of a table, it’s less likely that you’ll end up with duplicate or inconsistent data.\nScalability: If your business is experiencing high growth, this means that the database will expand, and a relational database can accommodate an increased volume of data.\n\nStart documenting the spreadsheets\n\nfile names, file paths\nUnderstand where values are coming from\n\nsource (e.g. department, store, sensor), owner\n\nHow rows of data are being generated\n\nwho/what is inputting the data\n\nHow does each spreadsheet/notebooks/set of spreadsheets fit in the company’s business model\n\nHow are they being used and by whom\n\nMap the spreadsheets relationships to one another\n\nSee Databases, Warehouses &gt;&gt; Design a Warehouse",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/sql.html",
    "href": "qmd/sql.html",
    "title": "SQL",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-misc",
    "href": "qmd/sql.html#sec-sql-misc",
    "title": "SQL",
    "section": "",
    "text": "Resources\n\nPublicly Available SQL Databases: Need to email administrator to gain access\n\ndplyr::show_query can convert a dplyr expression to SQL for db object (e.g. dbplyr,  duckdb, arrow)\nQueries in examples\n\nWindow Functions\n\nAverage Salary by Job Title\nAverage Unit Price for each CustomerId\nRank customers by amount spent\nCreate a new column that ranks Unit Price in descending order for each CustomerId\nCreate a new column that provides the previous order date’s Quantity for each ProductId\nCreate a new column that provides the very first Quantity ever ordered for each ProductId\nCalculate a cumulative moving average UnitPrice for each CustomerId\nRank customers for each department by amount spent\nFind the model and year of the car that been on the lot the longest\nCreate a subset (CTE)\n\nCalculate a running monthly total (aka cumsum)\n\nAlso running average\n\nCalculate a running monthly total for each account id\nCalculate a 3 months rolling running total using a window that includes the current month.\nCalculate a 7 months rolling running total using a window where the current month is always the middle month\nCalculate the number of consecutive days spent in each country\n\n\nCTE\n\nAverage monthly cost per campaign for the company’s marketing efforts\nCount the number of interactions of new users\nThe average top Math test score for students in California\n\nBusiness Queries\n\n7-day Simple Moving Average (SMA)\nRank product categories by shipping cost for each shipping address\nDaily counts of open jobs (where “open” is an untracked daily status)\nGet the latest order from each customer\nOverall median price\nMedian price for each product\nOverall median price and quantity\n\nProcessing Expressions\n\nProvide subtotals for a hierarchical group of fields (e.g. family, category, subcategory)\n\nSee NULLs &gt;&gt; COALESCE\n\n\n\nOrder of Operations\n\n\nHigher ranked functions can be inserted inside lower ranked functions\n\ne.g a window function can be inside a SELECT function but not inside a WHERE clause\nThere are exceptions and hacks around this in some cases\n\n\nTypes of Commands\n\nData Query Language (DQL) - used to find and view data without making any permanent changes to the database.\nData Manipulation Language (DML) - used to make permanent changes to the data, such as updating values or deleting them.\nData Definition Language (DDL) - used to make permanent changes to the table, such as creating or deleting a table.\nData Control Language (DCL) - used for administrative commands, such as adding or removing users of different tables and databases.\nTransact Control Language (TCL) - advanced SQL that deals with transaction level statements.\n\nMicrosoft SQL Server format for referencing a table:\n\n[database].[schema].[tablename]\nAlternative\nUSE my_data_base\nGO\n\nCheck if a table is updatable\nSELECT table_name, is_updatable\nFROM information_schema.views\n\nUseful if some of the tables you are working with are missing values that you need to add\nIf not updatable, then you’ll need to contact the database administrator to request permission to update that specific table\nShow all tables\nSHOW FULL TABLES -- mysql",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-setup",
    "href": "qmd/sql.html#sec-sql-setup",
    "title": "SQL",
    "section": "Set-up",
    "text": "Set-up\n\npostgres\n\nDownload postgres\npgAdmin is an IDE commonly used with postgres\n\nOpen pgAdmin and click on “Add new server.”\n\nSets up connection to existing server so make sure postgres is installed beforehand\n\nCreate Tables\n\nhome &gt;&gt; Databases (1) &gt;&gt; postgres &gt;&gt; Query Tool\n\nIf needed, give permission to pgAdmin to access data from a folder\n\nMight be necessary to upload csv files\n\nImport csv file\n\nright-click the table name &gt;&gt; Import/Export\nOptions tab\n\nSelect import, add file path to File Name, choose csv for format, select Yes for Header, add , for Delimiter\n\nColumns tab\n\nuncheck columns not in the csv (probably the primary key)\n\nWonder if NULLs will be automatically inserted for columns in the table that aren’t in the file.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-terms",
    "href": "qmd/sql.html#sec-sql-terms",
    "title": "SQL",
    "section": "Terms",
    "text": "Terms\n\nBatch - a set of sql statements e.g. statements between BEGIN and END\nCompiled object - you can create a function written in C/C++ and load into the database (at least in postgres) to achieve high performance.\nCorrelated Columns - tells how good the match between logical and physical ordering is.\nCorrelated/Uncorrelated Subqueries\n\ncorrelated- a type of query, where inner query depends upon the outcome of the outer query in order to perform its execution\n\nA correlated subquery can be thought of as a filter on the table that it refers to, as if the subquery were evaluated on each row of the table in the outer query\n\n\nuncorrelated - a type of sub-query where inner query doesn’t depend upon the outer query for its execution.\n\nIt is an independent query, the results of which are returned to and used by the outer query once (not per row).\n\n-- Uncorrelated subquery:\n-- inner query, c1, only depends on table2\nselect c1, c2\n  from table1 where c1 = (select max(x) from table2);\n\n-- Correlated subquery:\n-- inner query, c1, depends on table1 and table2\nselect c1, c2\n  from table1 where c1 = (select x from table2 where y = table1.c2);\n\nFunctions execute at a different level of priority and are handled differently than Views. You will likely see better performance.\nIndex - a quick lookup table (e.g. field or set of fields) for finding records users need to search frequently. An index is small, fast, and optimized for quick lookups. It is very useful for connecting the relational tables and searching large tables. (also see DB, Engineering &gt;&gt; Cost Optimizations)\nMigrations (schema) - version control system for your database schema. Management of incremental, reversible changes and version control to relational database schemas. A schema migration is performed on a database whenever it is necessary to update or revert that database’s schema to some newer or older version.\nPhysical Ordering - A PostgreSQL table consists of one or more files of 8KB blocks (or “pages”). The order in which the rows are stored in the file is the physical ordering.\nPredicate - defines a logical condition being applied to rows in a table. (e.g. IN, EXISTS, BETWEEEN, LIKE, ALL, ANY)\nScalar/Non-Scalar Subqueries\n\nA scalar subquery returns a single value (one column of one row). If no rows qualify to be returned, the subquery returns NULL.\nA non-scalar subquery returns 0, 1, or multiple rows, each of which may contain 1 or multiple columns. For each column, if there is no value to return, the subquery returns NULL. If no rows qualify to be returned, the subquery returns 0 rows (not NULLs).\n\nSelectivity - the fraction of rows in a table or partition that is chosen by the predicate\n\nRefers to the quality of a filter in its ability to reduce the number of rows that will need to be examined and ultimately returned\n\nWith a high selectivity, using the primary key or indexes to get right to the rows of interest\nWith a low selectivity, a full table scan would likely be needed to get the rows of interest.\n\nHigher selectivity means: more unique data; fewer duplicates; fewer number of rows for each key value\nUsed to estimate the cost of a particular access method; it is also used to determine the optimal join order. A poor choice of join order by the optimizer could result in a very expensive execution plan.\n\nSoft-deleted - An operation in which a flag is used to mark data as unusable, without erasing the data itself from the database\nSurrogate key - very similar to a primary key in that it is a unique value for an object in a table. However, rather than being derived from actual data present in the table, it is a field generated by the object itself. It has no business value like a primary key does, but is rather only used for data analysis purposes. Can be generated using different columns that already exist in your table or more often from two or more tables. dbt function definition\n\nExamples: PostgreSQL serial column, Oracle sequence column, or MySQL auto_increment column, Snowflake _file + _line columns\nExample: Each employee id is concatenated with a department id (e.g. marketing or finance)\n\n\nTransaction - a set of queries tied together such that if one query fails, the entire set of queries are rolled back to a pre-query state if the situation dictates.\n\nA database transaction, by definition, must be atomic, consistent, isolated and durable. These are popularly known as ACID properties.  These properties can ensure the concurrent execution of multiple transactions without conflict.\n\nViews - database objects that represent saved SELECT queries in “virtual” tables.\n\nContains a query plan.  For each query executed, the planner has to evaluate what’s being asked and calculate an optimal path. Views already have this plan calculated so it allows subsequent queries to be returned with almost no friction in processing aside from data retrieval.\nSome views are updateable but under certain conditions (1-1 mapping of rows in view to underlying table, no group_by, etc.)",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-basics",
    "href": "qmd/sql.html#sec-sql-basics",
    "title": "SQL",
    "section": "Basics",
    "text": "Basics\n\nCreate Tables\n\nIf you don’t include the schema as part of the table name (e.g. schema_name.table_name), pgadmin automatically places it into the “public” schema directory\nField Syntax: name, data type, constraints\nExample: Create table as select (CTAS)\nCREATE TABLE new_table AS \nSELECT * \nFROM old_table \nWHERE condition;\nExample: Table 1 (w/primary key)\nDROP TABLE IF EXISTS classrooms CASCADE;\nCREATE TABLE classrooms (\n    id INT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,\n    teacher VARCHAR(100)\n    );\n -- OR\nCREATE TABLE classrooms ( \n    id INT GENERATED ALWAYS AS IDENTITY, \n    teacher VARCHAR(100)\n    PRIMARY KEY(id) \n    );   \n\n“classrooms” is the name of the table; “id” and “teacher” are the fields\nCASCADE - postgres won’t delete the table if other tables point to it, so cascade will override measure.\nGENERATED ALWAYS AS IDENTITY - makes it so you don’t have to keep track of which “id” values have been used when adding rows. You can ommit the value for “id” and just add the values for the other fields\nSee tutorial for options, usage, removing, adding, etc. this constraint\nINSERT INTO classrooms\n    (teacher)\nVALUES\n    ('Mary'),\n    ('Jonah');\n\nAlso see Add Data &gt;&gt; Example: chatGPT\n\n\nExample: Table 2 (w/foreign key)\nDROP TABLE IF EXISTS students CASCADE;\nCREATE TABLE students (\n    id INT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,\n    name VARCHAR(100),\n    classroom_id INT,\n    CONSTRAINT fk_classrooms\n        FOREIGN KEY(classroom_id)\n        REFERENCES classrooms(id)\n);\n\n“students” is the name of the table; “id”, “name”, and “classroom_id” are the fields\nCreate a foreign key that points to the “classrooms” table\n\nExpression\n\nfk_classrooms is the name of the CONSTRAINT\n“classroom_id” is the field that will be the FOREIGN KEY\nREFERENCES points the foreign key to the classrooms table’s primary key, “id”\n\nforeign keys can point to any table\n\n\nPostgres won’t allow you to insert a row into students with a “classroom_id” that doesn’t exist in the “id” field of classrooms but will allow you to use a NULL placeholder\n-- Explicitly specify NULL\nINSERT INTO students\n    (name, classroom_id)\nVALUES\n    ('Dina', NULL); \n\n-- Implicitly specify NULL\nINSERT INTO students\n    (name)\nVALUES\n    ('Evan');\n\nAlso see Add Data &gt;&gt; Example: chatGPT\n\n\n\nExample\nCREATE TABLE members (\n    id serial primary key,\n    second_name character varying(200) NOT NULL,\n    date_joined date NOT NULL DEFAULT current_date,\n    member_id integer references members(id),\n    booking_start_time timestamp without timezone NOT NULL\n\nThe “serial” data type does the same thing as GENERATED ALWAYS AS IDENTITY (see first example), but is NOT compliant with the SQL standard. Use GENERATED ALWAYS AS IDENTITY\n“references” seems to be another old way to create foreign keys (see 2nd example for proper way)\n“character varying” - variable-length with limit (e.g limit of 200 characters)\n\ncharacter(n), char(n) are for fixed character lengths; text is for unlimited character lengths\n\n“current_date” is a function that will insert the current date as a value\n“timestamp without timezone” is literally that\n\nalso available: time with/without timezone, date, interval (see Docs for details)\n\n\nExample: MySQL\nCREATE DATABASE products;\n\nCREATE TABLE `products`.`prices` (\n  `pid` int(11) NOT NULL AUTO_INCREMENT,\n  `category` varchar(100) NOT NULL,\n  `price` float NOT NULL,\n  PRIMARY KEY (`pid`)\n);\n\nINSERT INTO products.prices\n    (pid, category, price)\nVALUES\n    (1, 'A', 2),\n    (2, 'A', 1),\n    (3, 'A', 5),\n    (4, 'A', 4),\n    (5, 'A', 3),\n    (6, 'B', 6),\n    (7, 'B', 4),\n    (8, 'B', 3),\n    (9, 'B', 5),\n    (10, 'B', 2),\n    (11, 'B', 1)\n;\n\n\n\nAdd Data\n\nExample: Copy/Paste table values into chatGPT to get the query\n\nExample: Add data via .csv\nCOPY assignments(category, name, due_date, weight)\nFROM 'C:/Users/mgsosna/Desktop/db_data/assignments.csv'\nDELIMITER ','\nCSV HEADER;\n\n“assignments” is the table; “category”, “name”, “due_date”, “weight” are fields that you want to import from the csv file\n** The order of the columns must be the same as the ones in the CSV file **\nHEADER keyword to indicate that the CSV file contains a header\nMight need to have superuser access in order to execute the COPY statement successfully\n\n\n\n\nUpdate Table\n\nUpdate Target Table with Source Data\nMERGE INTO target_table tgt\nUSING source_table src \n ON tgt.customer_id = src.customer_id\nWHEN MATCHED THEN\n UPDATE SET\n   tgt.is_active = src.is_active,\n   tgt.updated_date = '2024-04-01'::DATE\nWHEN NOT MATCHED THEN\n INSERT\n   (customer_id, is_active, updated_date)\n VALUES\n (src.customer_id, src.is_active, '2024-04-01'::DATE)\n; \n\nThe statement uses the MERGE keyword to conditionally update or insert rows into a target table based on a source table.\nIt matches rows between the tables using the ON clause and the customer_id column.\nThe WHEN MATCHED THEN clause specifies the update actions for matching rows.\nThe WHEN NOT MATCHED THEN clause specifies the insert actions for rows that don’t have a match in the target table.\nThe ::DATE cast ensures that the updated_date value is treated as a date.\nThis statement works for\n\nMicrosoft SQL Server (Transact-SQL)\nOracle Database\nPostgreSQL (version 9.5 and later)\nSQLite (version 3.24.0 and later)\n\n\n\n\n\nSubqueries\n\n**Using CTEs instead of subqueries make code more readable**\n\nSubqueries make it difficult to understand their context in the larger query\nThe only way to debug a subquery is by turning it into a CTE or pulling it out of the query entirely.\nCTEs and subqueries have a similar runtime, but subqueries make your code more complex for no reason.\n\nNotes from How to Use SubQueries in SQL\n\nAlso shows the alt method of creating a temporary table to compute the queries\n\nUse cases\n\nFiltering rows from a table with the context of another.\nPerforming double-layer aggregations such as average of averages or an average of sums.\nAccessing aggregations with a subquery.\n\nTables used in examples\n\nStore A (store_a)\n\n\nStore B is similar\n\n\nExample: Filtering rows\nselect * \nfrom sandbox.store_b\nwhere product_id IN (\n    select product_id\n    from sandbox.store_b\n    group by product_id \n    having count(product_id) &gt;= 3\n);\n\nfilters the rows with products that have been bought at least three times in store_b\n\nExample: Multi-Layer Aggregation\nselect avg(average_price.total_value) as average_transaction from (\n  select transaction_id, sum(price_paid) as total_value\n  from sandbox.store_a\n  group by transaction_id\n  ) as average_price\n;\n\ncomputes the average of all transactions\ncan’t apply an average directly, as our table is oriented to product_ids and not to transaction_ids\n\nExample: Filtering the table based on an Aggregation\nselect @avg_transaction:= avg(agg_table.total_value)\nfrom (\n  select transaction_id, sum(price_paid) as total_value\n  from sandbox.store_a\n  group by transaction_id\n) as agg_table;\n\nselect * \nfrom sandbox.store_a\nwhere transaction_id in (\n  select transaction_id\n  from sandbox.store_a\n  group by transaction_id\n  having sum(price_paid) &gt; @avg_transaction\n)\n\nfilters transactions that have a value higher than the average (where the output must retain the original product-oriented row)\n\n\n\n\nJoins\n\n\n\nCross Join -  acts like an expand_grid; where each value in the join key column gets all combinations of rows in both tables (also see above pic)\n\n\nEfficient join\nWhen you add the where clause, the cross join acts similarly to an inner join, except you aren’t joining it on any specified column\nExample:\nSELECT\n  schedule.event,\n  calendar.number_of_days\nFROM schedule\nCROSS JOIN calendar\nWHERE schedule.total_number &lt; calendar.number_of_days\n\nOnly join the row in the “schedule” table with the rows in the “calendar” table that meet the specified condition\n\n\nNatural Join - don’t need to specify join columns; need to have two columns in each table with the same name\n\nUse cases\n\nThere are a lot of common columns with the same name across multiple tables\n\nThey will all be used as joining keys.\n\nYou don’t want to type out all of the common columns in select just to avoid outputting the same columns multiple times.\n\n\nselect *\nfrom table_a\nnatural join table_b\n;\n\n-- natural + outer\nselect *\nfrom table_a\nnatural outer join table_b\n;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-r",
    "href": "qmd/sql.html#sec-sql-r",
    "title": "SQL",
    "section": "R",
    "text": "R\n\nGet query from dplyr code\ntbl_to_sql &lt;- function(tbl) {\n  dplyr::show_query(tbl) |&gt; \n    capture.output() |&gt; \n    purrr::discard_at(1) |&gt; \n    paste(collapse = \" \")\n}\n\nTransforms query into a string\nAlso see Generating SQL with {dbplyr} and sqlfluff\n\nConnect to or Create a SQLite database\ncon &lt;- DBI::dbConnect(drv = RSQLite::SQLite(),\n                      here::here(\"db_name.db\"),\n                      timeout = 10)\nConnect to Microsoft SQL Server\ncon &lt;- DBI::dbConnect(odbc::odbc(), \n                      Driver = \"SQL Server\", \n                      Server = \"SERVER\", \n                      Database = \"DB_NAME\", \n                      Trusted_Connection = \"True\", \n                      Port = 1433)\nClose connection: dbDisconnect(con)\nCreate a table from a data source: df &lt;- dbplyr::tbl(con, \"&lt;table name&gt;\")\n\nAllows you to use dplyr verbs with a remote database table then collect\n\nCancel a running query (postgres)\n# Store PID\npid &lt;- DBI::dbGetInfo(conn)$pid\n\n# Cancel query and get control of IDE back\n# SQL command\nSELECT pg_cancel_backend(&lt;PID&gt;)\n\nUseful if query is running too long and you want control of your IDE back\n\nCreate single tables from a list of tibbles to a database\npurrr::map2(table_names, list_of_tbls, ~ dbWriteTable(con, .x, .y))\nLoad all tables from a database into a list\ntables &lt;- dbListTables(con) \nall_data &lt;- map(tables, dbReadTable, conn = con)\nCan use map_dfr if all the tables have the same columns\nDynamic queries with {glue}\n\nExample: MS SQL Server\nvars &lt;- c(\"columns\", \"you\", \"want\", \"to\", \"select\")\ndate_var &lt;- 'date_col'\nstart_date &lt;- as.Date('2022-01-01')\ntoday &lt;- Sys.Date()\ntablename &lt;- \"yourtablename\"\nschema_name &lt;- \"yourschema\"\nquery &lt;- glue_sql(.con = con, \"SELECT TOP(10) {`vars`*} FROM {`schema_name`}.{`tablename`} \")\nDBI::dbGetQuery(con, query)\n\nvars format collapses the vars vector, separated by commas, so that it resembles a SELECT statement\n\n\nPRQL\n\nDocs\n{prqlr}\nA dplyr + SQL hybrid language\nUsing with DuckDB\nlibrary(prqlr); library(duckdb)\ncon &lt;- dbConnect(duckdb(), dbdir = \":memory\")\ndbWriteTable(con, \"mtcars\", mtcars)\n\"from mtcars | filter cyl &gt; 6 | select {cyl, mpg}\" |&gt; \n  prql_compile() |&gt; \n  dbGetQuery(conn = con)",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-bestp",
    "href": "qmd/sql.html#sec-sql-bestp",
    "title": "SQL",
    "section": "Best Practices",
    "text": "Best Practices\n\nMisc\n\nResources\n\nSQL Style Guide\n\nUse aliases only when table names are long enough so that using them improves readability (but choose meaningful aliases)\nDo not use SELECT *. Explicitly list columns instead\nUse comments to document business logic\nA comment at the top should provide a high-level description\nUse an auto-formatter\nGeneral Optimizations\n\nRemoving duplicates or filtering out null values at the beginning of your model will speed up queries\nReplace complex code with window functions\n\nExample: Replace GROUP_BY + TOP with a partition + FIRST_VALUE()\nFIRST_VALUE(test_score) OVER(PARTITION BY student_name ORDER BY test_score DESC)\nExample: AVG(test_score) OVER(PARTITION BY student_name)\n\n\n\nCTEs\n\nBreak down logic in CTEs using WITH … AS\nThe SELECT statement inside each CTE must do a single thing (join, filter or aggregate)\nThe CTE name should provide a high-level explanation\nThe last statement should be a SELECT statement querying the last CTE\n\nJoins\n\nUse WHERE for filtering, not for joining\nFavor LEFT JOIN over INNER JOIN; in most cases, it’s essential to know the distribution of NULLs\nAvoid using”Self-Joins.” Use window functions instead (see Google, BigQuery &gt;&gt; Optimization for details on self-joins)\nWhen doing equijoins (i.e., joins where all conditions have the something=another form), use the USING keyword\nBreak-up joins using OR into UNION because SQL uses nested operations for JOIN + OR queries which slow things.\n\nBad\n\nGood\n\nUNION simply joins the outputs of two separate SELECT statements and retains only one occurrence of duplicated rows if there are any.\n\n\nStyle Guide",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-index",
    "href": "qmd/sql.html#sec-sql-index",
    "title": "SQL",
    "section": "Indexes",
    "text": "Indexes\n\nMisc\n\nAn index may consist of up to 16 columns\nThe first column of the index must always be present in the query’s filter, order , join or group operations to be used\n\nCreate Index on an existing table (postgres)\nCREATE INDEX\n    score_index ON grades(score, student);\n\n“score_index” is the name of the index\n“grades” is the name of the table\n“score” and “student” are fields to be used as the indexes\n\nCreate Index that only uses a specific character length\n/* mysql */\nCREATE TABLE test (blob_col BLOB, INDEX(blob_col(10)));\n\nindex only uses the first 10 characters of the column value of a BLOB column type\n\nCreate index with multiple columns\n/* mysql */\nCREATE TABLE test (\n    id        INT NOT NULL,\n    last_name  CHAR(30) NOT NULL,\n    first_name CHAR(30) NOT NULL,\n    PRIMARY KEY (id),\n    INDEX name (last_name,first_name)\n)\nUsage of multiple column index (** order of columns is important **)\nSELECT * FROM test WHERE last_name='Jones';\nSELECT * FROM test\n  WHERE last_name='Jones' AND first_name='John';\nSELECT * FROM test\n  WHERE last_name='Jones'\n  AND (first_name='John' OR first_name='Jon');\nSELECT * FROM test\n  WHERE last_name='Jones'\n  AND first_name &gt;='M' AND first_name &lt; 'N';\n\nIndex is used when both columns are used as part of filtering criteria or when only the left-most column is used\nif you have a three-column index on (col1, col2, col3), you have indexed search capabilities on (col1), (col1, col2), and (col1, col2, col3).\n\nInvalid usage of multiple column index\nSELECT * FROM test WHERE first_name='John';\nSELECT * FROM test\n  WHERE last_name='Jones' OR first_name='John';\n\nThe “name” index won’t be used in these queries since\n\nfirst_name is NOT the left-most column specified in the index\nOR is used instead of AND\n\n\nCreate index with DESC, ASC\n/* mysql */\nCREATE TABLE t (\n  c1 INT, c2 INT,\n  INDEX idx1 (c1 ASC, c2 ASC),\n  INDEX idx2 (c1 ASC, c2 DESC),\n  INDEX idx3 (c1 DESC, c2 ASC),\n  INDEX idx4 (c1 DESC, c2 DESC)\n);\n\nUsed by ORDER BY\n\nSee Docs to see what operations and index types support Descending Indexes\n\nNote: idx_a on column_p, column_q desc is not the same as an * Index idx_a on column_q desc, column p or, * Index idx_b on column_p desc, column q\n\nUsage of Descending Indexes\nORDER BY c1 ASC, c2 ASC    -- optimizer can use idx1\nORDER BY c1 DESC, c2 DESC  -- optimizer can use idx4\nORDER BY c1 ASC, c2 DESC  -- optimizer can use idx2\nORDER BY c1 DESC, c2 ASC  -- optimizer can use idx3\n\nSee previous example for definition of idx* names\nSee Docs to see what operations and index types support Descending Indexes",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-part",
    "href": "qmd/sql.html#sec-sql-part",
    "title": "SQL",
    "section": "Partitioning",
    "text": "Partitioning\n\nMisc\n\nAlso see\n\nMySQL Docs\nGoogle, BigQuery &gt;&gt; Optimization &gt;&gt; Partitioning and Clustering\nDB, Engineering &gt;&gt; Cost Optimization &gt;&gt; Partitioning\n\nall of your queries to the partitioned table must contain the partition_key in the WHERE clause",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-views",
    "href": "qmd/sql.html#sec-sql-views",
    "title": "SQL",
    "section": "Views",
    "text": "Views\n\nA smaller data object that contains the subset of data resulting from a specific query\nWhereas a query happens after data is loaded, a materialized view is a precomputation\nThe computation is done once, and changes to the data are incorporated as they occur, making subsequent updates to the view much cheaper and more efficient than querying the entire database from scratch.\nCreate a View\n\nExample: Create view as select (CVAS)\nCREATE VIEW high_earner AS \nSELECT p.id AS person_id, j.salary\nFROM People p\nJOIN Job j \nON p.job = j.title\nWHERE j.salary &gt;= 200000;\n\nQuery a view (same as a table): SELECT * FROM high_earner\nUpdate view\nCREATE OR REPLACE VIEW high_earner AS \nSELECT p.id AS person_id, j.salary\nFROM People p\nJOIN Job j \nON p.job = j.title\nWHERE j.salary &gt;= 150000;\n\nExpects the query output to retain the same number of columns, column names, and column data types. Thus, any modification that results in a change in the data structure will raise an error.\n\nList views\n\nSELECT * \nFROM information_schema.views\nWHERE table_schema NOT IN ('pg_catalog', 'information_schema');\n\n“table_name” has the names of the views\n“view_definition” shows the query stored in the view\nWHERE command is included to omit built-in views from PostgreSQL.\n\nDelete view: DROP VIEW high_earner;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-vars",
    "href": "qmd/sql.html#sec-sql-vars",
    "title": "SQL",
    "section": "Variables",
    "text": "Variables\n\nMisc\n\nAlso see Business Queries &gt;&gt; Medians\n\nUser-defined\n\nDECLARE and SET\n-- Declare your variables\nDECLARE @start date\nDECLARE @stop date\n-- SET the relevant values for each variable\nSET @start = '2021-06-01'\nSET @stop = GETDATE()\n\nDECLARE sets the variable type (e.g. date)\nSET assigns a value\n\nOr just use DECLARE\nDECLARE @Iteration Integer = 0;\nExamples\n\nExample: Exclude 3 months of data from the query\nSELECT t1.[DATETIME], COUNT(*) AS vol\nFROM Medium.dbo.Earthquakes t1\nWHERE t1.[DATETIME] BETWEEN @start AND DATEADD(MONTH, -3, @stop)\nGROUP BY t1.[DATETIME]\nORDER BY t1.[DATETIME] DESC;\n\nSee above for the definitions of @start and @stop\n\nExample: Apply a counter\n-- Declare the variable (a SQL Command, the var name, the datatype)\nDECLARE @counter INT;\n-- Set the counter to 20\nSET @counter = 20;\n-- Print the initial value\nSELECT @counter AS _COUNT;\n-- Select and increment the counter by one\nSELECT @counter = @counter + 1;\n-- Print variable\nSELECT @counter AS _COUNT;\n-- Select and increment the counter by one\nSELECT @counter += 1;\n-- Print the variable\nSELECT @counter AS _COUNT;\n\n\nSystem\n\nROWCOUNT - returns the number of rows affected by the last previous statement\n\nExample\nBEGIN\n    SELECT\n        product_id,\n        product_name\n    FROM\n        production.products\n    WHERE\n        list_price &gt; 100000;\n    IF @@ROWCOUNT = 0\n        PRINT 'No product with price greater than 100000 found';\nEND",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-funs",
    "href": "qmd/sql.html#sec-sql-funs",
    "title": "SQL",
    "section": "Functions",
    "text": "Functions\n\n“||” Concantenate strings. e.g ‘Post’ || ‘greSQL’ –&gt; PostgreSQL\nBEGIN…END - defines a compound statement or statement block. A compound statement consists of a set of SQL statements that execute together. A statement block is also known as a batch\n\nA compound statement can have a local declaration for a variable, a cursor, a temporary table, or an exception\n\nLocal declarations can be referenced by any statement in that compound statement, or in any compound statement nested within it.\nLocal declarations are invisible to other procedures that are called from within a compound statement\n\n\nCOMMIT - a transaction control language that is used to permanently save the changes done in the transaction in tables/databases. The database cannot regain its previous state after its execution of commit.\nDATEADD - adds units of time to a variable or value\n\ne.g. DATEADD(month, -3, '2021-06-01')\n\nsubtracts 3 months from 2021-06-01\n\n\nDATE_TRUNC - pulls a component of a date object.\n\ne.g. date_trunc('month', date_var) as month\n\nDENSE_RANK- similar to the RANK , but it does not skip any numbers even if there is a tie between the rows.\n\nValues are ranked by the column specified in ORDER BY expression of the window function\n\nEXPLAIN - a means of running your query as a what-if to see what the planner thinks about it. It will show the process the system goes through to get to the data and return it.\nEXPLAIN\nSELECT\n    s.id AS student_id,\n    g.score\nFROM\n    students AS s\nLEFT JOIN\n    grades AS g\n    ON s.id = g.student_id\nWHERE\n    g.score &gt; 90\nORDER BY\n    g.score DESC;\n/*\nQUERY PLAN\n----------\nSort (cost=80.34..81.88 rows=617 width=8)\n[...] Sort Key: g.score DESC\n[...] -&gt; Hash Join (cost=16.98..51.74 rows=617 width=8)\n[...] Hash Cond: (g.student_id = s.id)\n[...] -&gt; Seq Scan on grades g (cost=0.00..33.13 rows=617 width=8)\n[...] Filter: (score &gt; 90)\n[...] -&gt; Hash (cost=13.10..13.10 rows=310 width=4)\n[...] -&gt; Seq Scan on students s (cost=0.00..13.20 rows=320 width=4)\n*/\n\nSequentially scanning (“Seq Scan”) the grades and students tables because the tables aren’t indexed\n\nAny Seq Scan, parallel or not, is sub-optimal\n\nEXPLAIN (BUFFERS) also shows how may data pages the database had to fetch using slow disk read operations (“read”), and how many of them were cached in memory (“shared hit”)\n\nEXPLAIN ANALYZE - tells the planner to not only hypothesize on what it would do, but actually run the query and show the results.\n\nshows where indexes are being hit — or not hit as it may be. You can step through and re-optimize your basic and complex queries.\n\nGETDATE() - Gets the current date\nGO - Not a sql function. Used by some interpreters as a reset.\n\ni.e. any variables set before the GO statement will now not be recognized by the interpreter.\nHelps to separate code into different sections\n\nISDATE - boolean - checks that a variable is a date type\nQUALIFY - clause filters the results of window functions.\n\nuseful when answering questions like fetching the most XXX value of each category\nQUALIFY does with window functions as what HAVING does with GROUP BY. As a result, in the order of execution, QUALIFY is evaluated after window functions.\nSee Business Queries &gt;&gt; Get the latest order from each customer\n\nUNNEST - BigQuery - takes an ARRAY and returns a table with a row for each element in the ARRAY (docs)\n\nGoogle Analytics, Analysis &gt;&gt; Example 17",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-udfs",
    "href": "qmd/sql.html#sec-sql-udfs",
    "title": "SQL",
    "section": "User Defined Functions (UDF)",
    "text": "User Defined Functions (UDF)\n\nMisc\n\nAvailable in SQL Server (Docs1, Docs2), Postgres (Docs), BigQuery (docs), etc.\nKeep a dictionary with the UDFs you’ve created and make sure to share it with any collaborators.\nCan be persistent or temporary\n\nPersistent UDFs can be used across multiple queries, while temporary UDFs only exist in the scope of a single query\n\n\nCreate\n\nExample: temporary udf (BQ)\nCREATE TEMP FUNCTION AddFourAndDivide(x INT64, y INT64)\nRETURNS FLOAT64\nAS (\n  (x + 4) / y\n);\nSELECT\n  val, AddFourAndDivide(val, 2)\nFROM\n  UNNEST([2,3,5,8]) AS val;\nExample: persistent udf (BQ)\nCREATE FUNCTION mydataset.AddFourAndDivide(x INT64, y INT64)\nRETURNS FLOAT64\nAS (\n  (x + 4) / y\n);\n\nSELECT\n  val, mydataset.AddFourAndDivide(val, 2)\nFROM\n  UNNEST([2,3,5,8,12]) AS val;\n\nDelete persistent udf: DROP FUNCTION &lt;udf_name&gt;\nWith Scalar subquery (BQ)\nCREATE TEMP FUNCTION countUserByAge(userAge INT64)\nAS (\n  (SELECT COUNT(1) FROM users WHERE age = userAge)\n);\nSELECT\n  countUserByAge(10) AS count_user_age_10,\n  countUserByAge(20) AS count_user_age_20,\n  countUserByAge(30) AS count_user_age_30;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-loops",
    "href": "qmd/sql.html#sec-sql-loops",
    "title": "SQL",
    "section": "Loops",
    "text": "Loops\n\nMisc\n\nposgres docs for loops\n\nWHILE\n\nExample: Incrementally add to a counter variable\n-- Declare the initial value\nDECLARE @counter INT;\nSET @counter = 20;\n-- Print initial value\nSELECT @counter AS _COUNT;\n-- Create a loop\nBEGIN;\n-- Loop code starting point\nWHILE @counter &lt; 30\nSELECT @counter = @counter + 1;\n-- Loop finish\nEND;\n-- Check the value of the variable\nSELECT @counter AS _COUNT;\n\nCursors (Docs)\n\nRather than executing a whole query at once, it is possible to set up a cursor that encapsulates the query, and then read the query result a few rows at a time.\n\nOne reason for doing this is to avoid memory overrun when the result contains a large number of rows. (However, PL/pgSQL users do not normally need to worry about that, since FOR loops automatically use a cursor internally to avoid memory problems.)\nA more interesting usage is to return a reference to a cursor that a function has created, allowing the caller to read the rows. This provides an efficient way to return large row sets from functions.\n\nExample (article (do not pay attention dynamic sql. it’s for embedding sql in C programs))\nDECLARE\n    cur_orders CURSOR FOR \n        SELECT order_id, product_id, quantity\n        FROM order_details\n        WHERE product_id = 456;\n    product_inventory INTEGER;\nBEGIN\n    OPEN cur_orders;\n    LOOP\n        FETCH cur_orders INTO order_id, product_id, quantity;\n        EXIT WHEN NOT FOUND;\n        SELECT inventory INTO product_inventory FROM products WHERE product_id = 456;\n        product_inventory := product_inventory - quantity;\n        UPDATE products SET inventory = product_inventory WHERE product_id = 456;\n    END LOOP;\n    CLOSE cur_orders;\n    -- do something after updating the inventory, such as logging the changes\nEND;\n\nA table called “products” that contains information about all products, including the product ID, product name, and current inventory. You can use a cursor to iterate through all orders that contain a specific product and update its inventory.\nA cursor called “cur_orders” that selects all order details that contain a specific product ID. We then define a variable called “product_inventory” to store the current inventory of the product.\nInside the loop, we fetch each order ID, product ID, and quantity from the cursor, subtract the quantity from the current inventory and update the products table with the new inventory value.\nFinally, we close the cursor and do something after updating the inventory, such as logging the changes.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-winfun",
    "href": "qmd/sql.html#sec-sql-winfun",
    "title": "SQL",
    "section": "Window Functions",
    "text": "Window Functions\n\nUnlike GROUP BY, keeps original columns after an aggregation\n\n\nAllows you to work with both aggregate and non-aggregate values all at once\n\nBetter performance than using GROUP BY + JOIN to get the same result\n\n\nDespite the order of operations, if you really need to have a window function inside a WHERE clause or GROUP BY clause, you may get around this limitation by using a subquery or a WITH query\n\nExample: Remove duplicate rows\nWITH temporary_employees as\n(SELECT \n  employee_id,\n  employee_name,\n  department,\n  ROW_NUMBER() OVER(PARTITION BY employee_name,\n                                department,\n                                employee_id) as row_count\nFROM Dummy_employees)\n\nSELECT *\nFROM temporary_employees\nWHERE row_count = 1\n\n3 Types of Window Functions\n\n\nLEAD() will give you the row AFTER the row you are finding a value for.\nLAG() will give you the row BEFORE the row you are finding a value for.\nFIRST_VALUE() returns the first value in an ordered, partitioned data output.\n\nGeneral Syntax\n\n\nwindow_function is the name of the window function we want to use (e.g. see above)\n\nexpression is the name of the column that we want the window function operated on.\n\nMay not be necessary depending on what window_function is used\n\nOVER is just to signify that this is a window function\n\nPARTITION BY divides the rows into partitions so we can specify which rows to use to compute the window function\n\npartition_list is the name of the column(s) we want to partition by (i.e. group_by)\n\nORDER BY is used so that we can order the rows within each partition. This is optional and does not have to be specified\n\norder_list is the name of the column(s) we want to order by\n\nROWS (optional; typically not used) used to subset the rows within each partition.\n\nframe_clause defines how much to offset from our current row\nSyntax: ROWS BETWEEN &lt;starting_row&gt; AND &lt;ending_row&gt;\n\nOptions for starting and ending row\n\nUNBOUNDED PRECEDING — all rows before the current row in the partition, i.e. the first row of the partition\n[some #] PRECEDING — # of rows before the current row\nCURRENT ROW — the current row\n[some #] FOLLOWING — # of rows after the current row\nUNBOUNDED FOLLOWING — all rows after the current row in the partition, i.e. the last row of the partition\n\nExamples\n\nROWS BETWEEN 3 PRECEDING AND CURRENT ROW — this means look back the previous 3 rows up to the current row.\nROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING — this means look from the first row of the partition to 1 row after the current row\nROWS BETWEEN 5 PRECEDING AND 1 PRECEDING — this means look back the previous 5 rows up to 1 row before the current row\nROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING — this means look from the first row of the partition to the last row of the partition\n\n\n\n\nExample: Average Salary by Job Title\n\n\nTables for Examples\n\nExample: Average Unit Price for each CustomerId\n\nSELECT CustomerId, \n      UnitPrice, \n      AVG(UnitPrice) OVER (PARTITION BY CustomerId) AS “AvgUnitPrice”\nFROM [Order] \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Average Unit Price for each group of CustomerId AND EmployeeId\n\nSELECT CustomerId, \n      EmployeeId, \n      AVG(UnitPrice) OVER (PARTITION BY CustomerId, EmployeeId) AS “AvgUnitPrice”\nFROM [Order] \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Create a new column that ranks Unit Price in descending order for each CustomerId\n\nSELECT CustomerId, \n      OrderDate, \n      UnitPrice, \n      ROW_NUMBER() OVER (PARTITION BY CustomerId ORDER BY UnitPrice DESC) AS “UnitRank”\nFROM [Order] \nINNER JOIN OrderDetail \nON [Order].Id = OrderDetail.OrderId\n\nSubstituting RANK in place of ROW_NUMBER should produce the same results\nNote that ranks are skipped (e.g. rank 3 for ALFKI) when there are rows with the same rank\n\nIf you don’t want ranks skipped, use DENSE_RANK for the window function\n\n\nExample: Create a new column that provides the previous order date’s Quantity for each ProductId\n\nSELECT ProductId, \n      OrderDate, \n      Quantity, \n      LAG(Quantity) OVER (PARTITION BY ProductId ORDER BY OrderDate) AS \"LAG\"\nFROM [Order] \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\n\nUse LEAD for the following quantity\n\nExample: Create a new column that provides the very first Quantity ever ordered for each ProductId\n\nSELECT ProductId, \n      OrderDate, \n      Quantity, \n      FIRST_VALUE(Quantity) OVER (PARTITION BY ProductId ORDER BY OrderDate) AS \"FirstValue\"\nFROM [Order] \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Calculate a cumulative moving average UnitPrice for each CustomerId\n\nSELECT CustomerId, \n      UnitPrice, \n      AVG(UnitPrice) OVER (PARTITION BY CustomerId \n      ORDER BY CustomerId \n      ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS “CumAvg”\nFROM [Order]\nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Rank customers for each department by amount spent\nSELECT\n    customer_name,\n    customer_id,\n    amount_spent,\n    department_id,\n    RANK(amount_spent) OVER(ORDER BY amount_spent DESC PARTITION BY department_id) AS spend_rank\nFROM employees\nExample: Find the model and year of car that been on lot the longest\nSELECT \nFIRST_VALUE(name) OVER(PARTITION BY model, year ORDER BY date_at_lot ASC) AS oldest_car_name\nmodel,\nyear\nFROM cars\nRunning Totals/Averages (Cumulative Sums)\n\nUses SUM as the window function\n\nJust replace SUM with AVG to get running averages\n\nExample\n\n\nGenerate a new dataset grouped by month, instead of timestamp. (CTE)\n\nOnly include three fields: account_id, occurred_month and total_amount_usd\nOnly computed for the following accounts: 1041 , 1051, 1061, 10141.\n\nCompute a running total ordered by occurred_month, without collapsing the rows in the result set.\n\nDisplay 2 columns: occurred_month and cum_amnt_usd_by_month\n\n\nBecause no partition was specified, the running total is applied on the full dataset and ordered by (ascending) occurred_month\n\nExample running total by grouping variable\n\nUsing previous CTE\n\nCompute a running total by account_id, ordered by occurred_month, and account_id (i.e. a separate running total for each account_id.)\n\nDisplay 3 columns: account_id, occurred_month, and cum_mon_amnt_usd_by_account\n\n\n\nSame as previous example except a partition column (account_id) is added\n\nExample Running total over various window lengths\n\nUsing previous CTE\n\nCompute a 3 months rolling running total using a window that includes the current month.\nCompute a 7 months rolling running total using a window where the current month is always the middle month.\n\n\nFirst case uses 2 PRECEDING rows and the CURRENT_ROW\nSecond case uses 3 PRECEDING rows and 3 FOLLOWING rows and the CURRENT_ROW\n\nExample: Calculate the number consecutive days spent in each country (sqlite)\nwith ordered as (\n  select \n    created,\n    country,\n    lag(country) over (order by created desc)\n      as previous_country\n  from \n    raw\n),\ngrouped as (\n  select \n    country, \n    created, \n    count(*) filter (\n      where previous_country is null\n      or previous_country != country\n    ) over (\n      order by created desc\n      rows between unbounded preceding\n      and current row\n    ) as grp\n  from \n    ordered\n)\nselect\n  country,\n  date(min(created)) as start,\n  date(max(created)) as end,\n  cast(\n    julianday(date(max(created))) -\n    julianday(date(min(created))) as integer\n  ) as days\nfrom \n  grouped\ngroup by\n  country, grp\norder by\n  start desc;\n\nPost\n\nGoes over the code and thought process step-by-step with shows original data and results during intermediate steps\n\nThread\n\nEvidently only sqlite and postgres support filter. Someone in the thread suggest an alternate method.\n\nOutput:\ncountry         start         end           days\nUnited Kingdom  2023-06-08  2023-06-08  0\nUnited States   2019-09-02  2023-05-11  1347\nFrance          2019-08-25  2019-08-31  6\nMadagascar      2019-07-31  2019-08-07  7\nFrance          2019-07-25  2019-07-25  0\nUnited States   2019-05-04  2019-06-30  57\nUnited Kingdom  2018-08-29  2018-09-10  12\nUnited States   2018-08-05  2018-08-10  5",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-ctes",
    "href": "qmd/sql.html#sec-sql-ctes",
    "title": "SQL",
    "section": "Common Table Expressions (CTE)",
    "text": "Common Table Expressions (CTE)\n\nThe result set of a query which exists temporarily and for use only within the context of a larger query. Much like a derived table, the result of a CTE is not stored and exists only for the duration of the query.\nAlso see\n\nWindow Functions &gt;&gt; Running Totals &gt;&gt; Examples\nGoogle, Google Analytics, Analysis &gt;&gt; Examples 12-15, 18, 19\n\nUse Cases\n\nNeeding to reference a derived table multiple times in a single query\nAn alternative to creating a view in the database\nPerforming the same calculation multiple times over across multiple query components\n\nImproves readability and usually no performance difference\n\nPrior to PostgreSQL 12, https://hakibenita.com/be-careful-with-cte-in-postgre-sql , something with the caching mechanism created a bottleneck. Currently, version 13 is the latest, so hopefully not a common problem anymore.\n\nSteps\n\nInitiate a CTE using “WITH”\nProvide a name for the result soon-to-be defined query\nAfter assigning a name, follow with “AS”\nSpecify column names (optional step)\nDefine the query to produce the desired result set\nIf multiple CTEs are required, initiate each subsequent expression with a comma and repeat steps 2-4.\nReference the above-defined CTE(s) in a subsequent query\n\nSyntax\nWITH\nexpression_name_1 AS\n(CTE query definition 1)\n[, expression_name_X AS\n  (CTE query definition X)\n, etc ]\nSELECT expression_A, expression_B, ...\nFROM expression_name_1\nExample\n\nComparison with a “derived” query\n“What is the average monthly cost per campaign for the company’s marketing efforts?”\nUsing CTE workflow\n-- define CTE:\nWITH Cost_by_Month AS\n(SELECT campaign_id AS campaign,\n      TO_CHAR(created_date, 'YYYY-MM') AS month,\n      SUM(cost) AS monthly_cost\nFROM marketing\nWHERE created_date BETWEEN NOW() - INTERVAL '3 MONTH' AND NOW()\nGROUP BY 1, 2\nORDER BY 1, 2)\n\n-- use CTE in subsequent query:\nSELECT campaign, avg(monthly_cost) as \"Avg Monthly Cost\"\nFROM Cost_by_Month\nGROUP BY campaign\nORDER BY campaign\nUsed derived query\n-- Derived\nSELECT campaign, avg(monthly_cost) as \"Avg Monthly Cost\"\nFROM\n    -- this is where the derived query is used\n    (SELECT campaign_id AS campaign,\n      TO_CHAR(created_date, 'YYYY-MM') AS month,\n      SUM(cost) AS monthly_cost\n    FROM marketing\n    WHERE created_date BETWEEN NOW() - INTERVAL '3 MONTH' AND NOW()\n    GROUP BY 1, 2\n    ORDER BY 1, 2) as Cost_By_Month\nGROUP BY campaign\nORDER BY campaign\n\nExample\n\nCount the number of interactions of new users\nSteps\n\nGet new users\nCount interactions\nGet interactions of new users\n\n\nWITH new_users AS (\n    SELECT id\n    FROM users\n    WHERE created &gt;= '2021-01-01'\n),\ncount_interactions AS (\n    SELECT id,\n        COUNT(*) n_interactions\n    FROM interactions\n    GROUP BY id\n),\ninteractions_by_new_users AS (\n    SELECT id,\n        n_interactions\n    FROM new_users\n        LEFT JOIN count_interactions USING (id)\n)\n\nSELECT *\nFROM interactions_by_new_users\nExample\n\nFind the average top Math test score for students in California\nSteps\n\nGet a subset of students (California)\nGet a subset of test scores (Math)\nJoin them together to get all Math test scores from California students\nGet the top score per student\nTake the overall average\n\nDerived Query (i.e. w/o CTE)\nSELECT AVG(score)\nFROM \n  (SELECT students.id, MAX(test_results.score) as score\n  FROM students \n  JOIN schools ON (\n    students.school_id = schools.id AND schools.state = 'CA'\n  )\n  JOIN test_results ON (\n    students.id = test_results.student_id\n    AND test_results.subject = 'math'\n  )\n  GROUP BY students.id) as tmp\nUsing CTE\nWITH\n  student_subset as (\n    SELECT students.id \n    FROM students \n    JOIN schools ON (\n      students.school_id = schools.id AND schools.state = 'CA'\n    )\n  ),\n  score_subset as (\n    SELECT student_id, score \n    FROM test_results \n    WHERE subject = 'math'\n  ),\n  student_scores as (\n    SELECT student_subset.id, score_subset.score\n    FROM student_subset \n    JOIN score_subset ON (\n        student_subset.id = score_subset.student_id\n    )\n  ),\n  top_score_per_student as (\n    SELECT id, MAX(score) as score \n    FROM student_scores \n    GROUP BY id\n  )\n\nSELECT AVG(score) \nFROM top_score_per_student",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-str",
    "href": "qmd/sql.html#sec-sql-str",
    "title": "SQL",
    "section": "Strings",
    "text": "Strings\n\nConcatenate\n\nAlso see Processing Expressions &gt;&gt; NULLs\n“||”\nSELECT 'PostgreSQL' || ' ' || 'Databases' AS result;\n\n    result\n--------------\nPostgreSQL Databases\nCONCAT\nSELECT CONCAT('PostgreSQL', ' ', 'Databases') AS result;\n\n    result\n--------------\nPostgreSQL Databases\nWith NULL values\nSELECT CONCAT('Harry', NULL, 'Peter');\n\n--------------\nHarryPeter\n\n“||” won’t work with NULLs\n\nColumns\nSELECT first_name, last_name, \nCONCAT(first_name,' ' , last_name) \"Full Name\" \nFROM candidates;\n\nNew column, “Full Name”, is created with concatenated columns\n\n\nSplitting (BQ)\nSELECT\n*,\nCASE WHEN ARRAY_LENGTH(SPLIT(page_location, '/')) &gt;= 5 \n          AND\n          CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+')\n          AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN \n                                      ('accessories','apparel','brands','campus+collection','drinkware',\n                                        'electronics','google+redesign',\n                                        'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                        'office','shop+by+brand','small+goods','stationery','wearables'\n                                        )\n                OR\n                LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN \n                                      ('accessories','apparel','brands','campus+collection','drinkware',\n                                        'electronics','google+redesign',\n                                        'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                        'office','shop+by+brand','small+goods','stationery','wearables'\n                                        )\n          )\n          THEN 'PDP'\n          WHEN NOT(CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+'))\n          AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                        'electronics','google+redesign',\n                                        'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                        'office','shop+by+brand','small+goods','stationery','wearables'\n                                        )\n                OR \n                LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                          'electronics','google+redesign',\n                                          'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                          'office','shop+by+brand','small+goods','stationery','wearables'\n                                          )\n          )\n          THEN 'PLP'\n      ELSE page_title\n      END AS page_title_adjusted \nFROM \n  unnested_events\n\nFrom article, gist\nQuery is creating a new categorical column, “page_title_adjusted,” that is “PDP” when a substring in “page_location” is one of a set of words, and “PLP” when it’s not, and the value of page_title otherwise.\nSPLIT splits the string by separator, ‘/’\nCONTAINS_SUBSTR is looking for substring with a “+”\n[SAFE_OFFSET(3)] pulls the 4th substring (think this indexes by 0?)\nAfter it’s been reversed via ARRAY_REVERSE (?)\nELSE says use the value for page_title when length of the substrings after splitting page_location is 5 or less\n“unnested_events” is a CTE",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-arr",
    "href": "qmd/sql.html#sec-sql-arr",
    "title": "SQL",
    "section": "Arrays",
    "text": "Arrays\n\nMisc\n\nPostGres\n\nIndexing Arrays starts at 1, not at 0\n\n\nCreate Array (BQ)\nSELECT ARRAY\n  (SELECT 1 UNION ALL\n  SELECT 2 UNION ALL\n  SELECT 3) AS new_array;\n+-----------+\n| new_array |\n+-----------+\n| [1, 2, 3] |\n+-----------+\n\nSELECT\n  ARRAY\n    (SELECT AS STRUCT 1, 2, 3\n    UNION ALL SELECT AS STRUCT 4, 5, 6) AS new_array;\n+------------------------+\n| new_array              |\n+------------------------+\n| [{1, 2, 3}, {4, 5, 6}] |\n+------------------------+\n\nSELECT ARRAY\n  (SELECT AS STRUCT [1, 2, 3] UNION ALL\n  SELECT AS STRUCT [4, 5, 6]) AS new_array;\n+----------------------------+\n| new_array                  |\n+----------------------------+\n| [{[1, 2, 3]}, {[4, 5, 6]}] |\n+----------------------------+\nCreate a table with Arrays (Postgres)\n\nCREATE TEMP TABLE shopping_cart (\n  cart_id serial PRIMARY KEY,\n  products text ARRAY\n  );\nINSERT INTO\n  shopping_cart(products)\nVALUES\n  (ARRAY['product_a', 'product_b']),\n  (ARRAY['product_c', 'product_d']),\n  (ARRAY['product_a', 'product_b', 'product_c']),\n  (ARRAY['product_a', 'product_b', 'product_d']),\n  (ARRAY['product_b', 'product_d']);\n\n-- alt syntax w/o ARRAY\nINSERT INTO\n  shopping_cart(products)\nVALUES\n  ('{\"product_a\", \"product_d\"}');\n\nAlso see Basics &gt;&gt; Add Data &gt;&gt; Example: chatGPT\n\nSubset an array (postgres)\n\nSELECT\n  cart_id,\n  products[1] AS first_product -- indexing starts at 1\nFROM\n  shopping_cart;\nSlice an array (postgres)\n\nSELECT\n  cart_id,\n  products [1:2] AS first_two_products\nFROM\n  shopping_cart\nWHERE\n  CARDINALITY(products) &gt; 2;\nUnnest an array (postgres)\n\nSELECT\n  cart_id,\n  UNNEST(products) AS products\nFROM\n  shopping_cart\nWHERE\n  cart_id IN (3, 4);\n\nUseful if you want to perform a join\n\nFilter according to items in arrays (postgres)\nSELECT\n  cart_id,\n  products\nFROM\n  shopping_cart\nWHERE\n  'product_c' = ANY (products);\n\nOnly rows with arrays that have “product_c” will be returned\n\nChange array values using UPDATE, SET\n-- update arrays \nUPDATE\n  shopping_cart\nSET\n  products = ARRAY['product_a','product_b','product_e']\nWHERE\n  cart_id = 1;\n\nUPDATE \n  shopping_cart\nSET\n  products[1] = 'product_f'\nWHERE\n  cart_id = 2;\nSELECT\n  *\nFROM\n  shopping_cart\nORDER BY cart_id;\n\nFirst update: all arrays where cart_id == 1 are set to [‘product_a’,‘product_b’,‘product_e’]\nSecond update: all array first values where cart_id == 2 are set to ‘product_f’\n\nInsert array values\n\nARRAY_APPEND - puts value at the end of the array\nUPDATE\n  shopping_cart\nSET\n  products = ARRAY_APPEND(products, 'product_x')\nWHERE\n  cart_id = 1;\n\narrays in product column where cart_id == 1 get “product_x” appended to the end of their arrays\n\nARRAY_PREPEND - puts value at the beginning of the array\nUPDATE \n  shopping_cart\nSET\n  products = ARRAY_PREPEND('product_x', products)\nWHERE\n  cart_id = 2;\n\narrays in product column where cart_id == 2 get “product_x” prepended to the beginning of their arrays\n\n\nARRAY_REMOVE - remove array item\nUPDATE\n  shopping_cart\nSET\n  products = array_remove(products, 'product_e')\nWHERE cart_id = 1;\n\narrays in product column where cart_id == 1 get “product_e” removed from their arrays\n\nARRAY_CONCAT(BQ), ARRAY_CAT(postgres) - Concantenate\nSELECT ARRAY_CONCAT([1, 2], [3, 4], [5, 6]) as count_to_six;\n+--------------------------------------------------+\n| count_to_six                                    |\n+--------------------------------------------------+\n| [1, 2, 3, 4, 5, 6]                              |\n+--------------------------------------------------+\n\n-- postgres\nSELECT\n  cart_id,\n  ARRAY_CAT(products, ARRAY['promo_product_1', 'promo_product_2'])\nFROM shopping_cart\nORDER BY cart_id;\nARRAY_TO_STRING - Coerce to string (BQ)\nWITH items AS\n  (SELECT ['coffee', 'tea', 'milk' ] as list\n  UNION ALL\n  SELECT ['cake', 'pie', NULL] as list)\nSELECT ARRAY_TO_STRING(list, '--') AS text\nFROM items;\n+--------------------------------+\n| text                          |\n+--------------------------------+\n| coffee--tea--milk              |\n| cake--pie                      |\n+--------------------------------+\n\nWITH items AS\n  (SELECT ['coffee', 'tea', 'milk' ] as list\n  UNION ALL\n  SELECT ['cake', 'pie', NULL] as list)\nSELECT ARRAY_TO_STRING(list, '--', 'MISSING') AS text\nFROM items;\n+--------------------------------+\n| text                          |\n+--------------------------------+\n| coffee--tea--milk              |\n| cake--pie--MISSING            |\n+--------------------------------+\nARRAY_AGG - gather values of a group by variable into an array (doc)\n\nMakes the output more readable\nExample: Get categories for each brand\n-- without array_agg\nselect\n    brand,\n    category\nfrom order_item\ngroup by brand, category\norder by brand, category\n;\nResults:\n| brand  | category  | \n| ------ | ---------- | \n| Arket  | jacket    |\n| COS    | shirts    |\n| COS    | trousers  | \n| COS    | vest      |\n| Levi's | jacket    |\n| Levi's | jeans      |\n\n-- with array_agg\nselect\n  brand,\n  array_agg(distinct category) as all_categories\nfrom order_item\ngroup by brand\norder by brand\n;\nResults:\n| brand  | all_categories              | \n| ------ | ---------------------------- | \n| Arket  | ['jacket']                  |\n| COS    | ['shirts','trousers','vest'] |\n| Levi's | ['jacket','jeans']          |\n| Uniqlo | ['shirts','t-shirts','vest'] |\n\nARRAY_SIZE - function takes an array or a variant as input and returns the number of items within the array/variant (doc)\n\nExample: How many categories does each brand have?\nselect\n  brand,\n  array_agg(distinct category) as all_categories,\n  array_size(all_categories) as no_of_cat\nfrom order_item\ngroup by brand\norder by brand\n;\nResults:\n| brand  | all_categories              | no_of_cat |\n| ------ | ---------------------------  | --------- |\n| Arket  | ['jacket']                  | 1        |\n| COS    | ['shirts','trousers','vest'] | 3        |\n| Levi's | ['jacket','jeans']          | 2        |\n| Uniqlo | ['shirts','t-shirts','vest'] | 3        |\n\n-- postgres using CARDINALITY to get array_size\nSELECT\n  cart_id,\n  CARDINALITY(products) AS num_products\nFROM\n  shopping_cart;\n\nARRAY_CONTAINS checks if a variant is included in an array and returns a boolean value. (doc)\n\nVariant is just a specific category\nNeed to cast the item you’d like to check as a variant first\nSyntax: ARRAY_CONTAINS(variant, array)\nExample: What brands have jackets?\nselect\n  brand,\n  array_agg(distinct category) as all_categories,\n  array_size(all_categories) as no_of_cat,\n  array_contains('jacket'::variant,all_categories) as has_jacket\nfrom order_item\ngroup by brand\norder by brand\n;\nResults:\n| brand  | all_categories              | no_of_cat | has_jacket |\n| ------ | ---------------------------  | --------- | ---------- |\n| Arket  | ['jacket']                  | 1        | true      |\n| COS    | ['shirts','trousers','vest'] | 3        | false      |\n| Levi's | ['jacket','jeans']          | 2        | true      |\n| Uniqlo | ['shirts','t-shirts','vest'] | 3        | false      |\n\n-- postgres contains_operator, @&gt;\nSELECT\n  cart_id,\n  products\nFROM\n  shopping_cart\nWHERE\n  products  @&gt; ARRAY['product_a', 'product_b'];\n\n“@&gt;” example returns all rows with arrays containing product_a and product_b",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-bizq",
    "href": "qmd/sql.html#sec-sql-bizq",
    "title": "SQL",
    "section": "Business Queries",
    "text": "Business Queries\n\nSimple Moving Average (SMA)\n\nExample: 7-day SMA including today\nSELECT\n  Date, Conversions,\n  AVG(Conversions) OVER (ORDER BY Date ROWS BETWEEN 6 PRECEDING AND\n  CURRENT ROW) as SMA\nFROM daily_sales\n\nExample: 3-day SMA not including today\nselect\n  date,\n  sales,\n  avg(sales) over (order by date\n        rows between 3 preceding and current row - 1) as moving_avg\nfrom table_daily_sales\nExample: Rank product categories by shipping cost for each shipping address\n\nSELECT Product_Category,\n  Shipping_Address,\n  Shipping_Cost,\n  ROW_NUMBER() OVER\n              (PARTITION BY Product_Category,\n                            Shipping_Address\n              ORDER BY Shipping_Cost DESC) as RowNumber,\n  RANK() OVER \n        (PARTITION BY Product_Category,\n                      Shipping_Address\n        ORDER BY Shipping_Cost DESC) as RankValues,\n  DENSE_RANK() OVER \n              (PARTITION BY Product_Category,\n                            Shipping_Address \n              ORDER BY Shipping_Cost DESC) as DenseRankValues\nFROM Dummy_Sales_Data_v1\nWHERE Product_Category IS NOT NULL\nAND Shipping_Address IN ('Germany','India')\nAND Status IN ('Delivered')\n\nRANK() retrieves ranked rows based on the condition of ORDER BY clause. As you can see there is a tie between 1st two rows i.e. first two rows have same value in Shipping_Cost column (which is mentioned in ORDER BY clause).\nDENSE_RANK is similar to the RANK , but it does not skip any numbers even if there is a tie between the rows. This you can see in Blue box in the above picture.\nRank resets to 1 when “Shipping_Address” changes location\n\nExample: Total order quantity for each month\nSELECT strftime('%m', OrderDate) as Month,\n      SUM(Quantity) as Total_Quantity\nfrom Dummy_Sales_Data_v1\nGROUP BY strftime('%m', OrderDate)\n\nstrftime extracts the month (%m) from the datetime column, “OrderDate”\n\nExample: Daily counts of open jobs\n\nThe issue is that there aren’t rows for transactions that remain in a type of holding status\n\ne.g. Job Postings website has date columns for the date the job posting was created, the date the job posting went live on the website, and the date the job posting was taken down (action based timestamps), but no dates for the status between “went live” and “taken down”.\n\n\n-- create a calendar column\nSELECT parse_datetime('2020–01–01 08:00:00', 'yyyy-MM-dd H:m:s') + (interval '1' day * d) as cal_date from \nFROM ( SELECT\nROW_NUMBER() OVER () -1 as d\nFROM\n(SELECT 0 as n UNION SELECT 1) p0,\n(SELECT 0 as n UNION SELECT 1) p1,\n(SELECT 0 as n UNION SELECT 1) p2,\n(SELECT 0 as n UNION SELECT 1) p3,\n(SELECT 0 as n UNION SELECT 1) p4,\n(SELECT 0 as n UNION SELECT 1) p5,\n(SELECT 0 as n UNION SELECT 1) p6,\n(SELECT 0 as n UNION SELECT 1) p7,\n(SELECT 0 as n UNION SELECT 1) p8,\n(SELECT 0 as n UNION SELECT 1) p9,\n(SELECT 0 as n UNION SELECT 1) p10\n)\n\n-- left-join your table to the calendar column\nSelect\n    c.cal_date,\n    count(distinct opp_id) as \"historical_prospects\"\nFrom calendar c\nLeft Join\n    opportunities o\n    on\n        o.stage_entered ≤ c.cal_date \n        and (o.stage_exited is null or o.stage_exited &gt; c.cal_date)\n\nCalendar column should probably be a CTE\nNotes from Using SQL to calculate trends based on historical status\nSome flavours of SQL have a generate_series function, which will create this calendar column for you\nFor one particular month, then create an indicator column with “if posting_publish_date ≤ 2022–01–01 and (posting_closed_date is null or posting_closed_date &gt; 2022–01–31) then True” and then filter for True and count.\n\nExample: Get the latest order from each customer\n-- Using QUALIFY\nselect\n    date,\n    customer_id,\n    order_id,\n    price\nfrom customer_order_table\nqualify row_number() over (partition by customer_id order by date desc) = 1\n;\n\n-- CTE w/window function\nwith order_order as\n(\nselect\n    date,\n    customer_id,\n    order_id,\n    price,\n    row_number() over (partition by customer_id order by date desc)   \n    as order_of_orders\nfrom customer_order_table \n)\n\nselect\n    *\nfrom order_order\nwhere order_of_orders = 1\n;\nResults:\n| date      | customer_id | order_id | price |\n|------------|-------------|----------|-------|\n| 2022-01-03 | 002        | 212      | 350  |\n| 2022-01-06 | 005        | 982      | 300  |\n| 2022-01-07 | 001        | 109      | 120  |\nMedians\n\nNotes from How to Calculate Medians with Grouping in MySQL\n\nVariables:\n\npid: unique id variable\ncategory: A or B\nprice: random value between 1 and 6\n\n\nExample: Overall median price\nSELECT AVG(sub.price) AS median\nFROM ( \n    SELECT @row_index := @row_index + 1 AS row_index, p.price\n    FROM products.prices p, (SELECT @row_index := -1) r\n    WHERE p.category = 'A'\n    ORDER BY p.price \n) AS sub\nWHERE sub.row_index IN (FLOOR(@row_index / 2), CEIL(@row_index / 2))\n;\n\nmedian|\n------+\n   3.0|\n\n@row_index is a SQL variable that is initiated in the FROM statement and updated for each row in the SELECT statement.\nThe column whose median will be calculated (the price column in this example) should be sorted. It doesn’t matter if it’s sorted in ascending or descending order.\nAccording to the definition of median, the median is the value of the middle element (total count is odd) or the average value of the two middle elements (total count is even). In this example, category A has 5 rows and thus the median is the value of the third row after sorting. The values of both FLOOR(@row_index / 2) and CEIL(@row_index / 2) are 2 which is the third row. On the other hand, for category B which has 6 rows, the median is the average value of the third and fourth rows.\n\nExample: Median price for each product\nSELECT\n    sub2.category,\n    CASE WHEN MOD(sub2.total, 2) = 1 THEN sub2.mid_prices\n         WHEN MOD(sub2.total, 2) = 0 THEN (SUBSTRING_INDEX(sub2.mid_prices, ',', 1) + SUBSTRING_INDEX(sub2.mid_prices, ',', -1)) / 2\n    END AS median    \nFROM \n    (\n        SELECT \n            sub1.category,\n            sub1.total,\n            CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', CEIL(sub1.total/2)), ',', '-1')\n                 WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', sub1.total/2 + 1), ',', '-2')\n            END AS mid_prices\n        FROM \n            (\n                SELECT\n                    p.category,\n                    GROUP_CONCAT(p.price ORDER BY p.price) AS prices,\n                    COUNT(*) AS total\n                FROM products.prices p\n                GROUP BY p.category\n            ) sub1\n    ) sub2\n;\n\ncategory|median|\n--------+------+\nA       |3     |\nB       |3.5   |\n\nBreaking down the subqueries\n\nSort prices per category\nSELECT\n    category,\n    GROUP_CONCAT(price ORDER BY p.price) AS prices,\n    COUNT(*) AS total\nFROM products.prices p\nGROUP BY p.category\n;\n\ncategory|prices     |total|\n--------+-----------+-----+\nA       |1,2,3,4,5  |    5|\nB       |1,2,3,4,5,6|    6|\n\nIf your table has a lot of data, GROUP_CONCAT would not contain all the data. In this case, you increase the limit for GROUP_CONCAT by: SET GROUP_CONCAT_MAX_LEN = 100000;\n\nGet middle prices according to whether the total count is an odd or even number\nSELECT \n    sub1.category,\n    sub1.total,\n    CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', CEIL(sub1.total/2)), ',', '-1')\n         WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', sub1.total/2 + 1), ',', '-2')\n    END AS mid_prices\nFROM \n    (\n        SELECT\n            p.category,\n            GROUP_CONCAT(p.price ORDER BY p.price) AS prices,\n            COUNT(*) AS total\n        FROM products.prices p\n        GROUP BY p.category\n    ) sub1\n;\n\ncategory|total|mid_prices|\n--------+-----+----------+\nA       |    5|3         |\nB       |    6|3,4       |\n\nWe use the MOD function (modulo) to check if the total count is an odd or even number.\nThe SUBSTRING_INDEX function is used twice to extract the middle elements.\n\n\n\nExample: Overall median of price and quantity\nSELECT\n    CASE WHEN MOD(sub2.total, 2) = 1 THEN sub2.mid_prices\n         WHEN MOD(sub2.total, 2) = 0 THEN (SUBSTRING_INDEX(sub2.mid_prices, ',', 1) + SUBSTRING_INDEX(sub2.mid_prices, ',', -1)) / 2\n    END AS median_of_price,\n    CASE WHEN MOD(sub2.total, 2) = 1 THEN sub2.mid_quantities\n         WHEN MOD(sub2.total, 2) = 0 THEN (SUBSTRING_INDEX(sub2.mid_quantities, ',', 1) + SUBSTRING_INDEX(sub2.mid_prices, ',', -1)) / 2\n    END AS median_of_quantity\nFROM \n    (\n        SELECT \n            sub1.total,\n            CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', CEIL(sub1.total/2)), ',', '-1')\n                 WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', sub1.total/2 + 1), ',', '-2')\n            END AS mid_prices,\n            CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.quantities, ',', CEIL(sub1.total/2)), ',', '-1')\n                 WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.quantities, ',', sub1.total/2 + 1), ',', '-2')                 \n            END AS mid_quantities\n        FROM \n            (\n                SELECT\n                    COUNT(*) AS total,\n                    GROUP_CONCAT(o.price ORDER BY o.price) AS prices,\n                    GROUP_CONCAT(o.quantity ORDER BY o.quantity) AS quantities\n                FROM products.orders o\n            ) sub1\n    ) sub2\n;\n\n\nmedian_of_price|median_of_quantity|\n---------------+------------------+\n3              |30                |\n\nSimilar to previous example\nVariables: order_id, price, quantity",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-trans",
    "href": "qmd/sql.html#sec-sql-trans",
    "title": "SQL",
    "section": "Transactions",
    "text": "Transactions\n\nMisc\n\nAlso see\n\nTerms &gt;&gt; Transaction\nDatabase, Warehouses &gt;&gt; Database Triggers - Shows how to efficiently transfer data from a transactional database to a warehouse/relational database by setting up event triggers and staging tables.\n\nWhen the transaction is successful, COMMIT is applied. When the transaction is aborted, incorrect execution, system failure ROLLBACK occurs.\n\nOnly used with INSERT, UPDATE and DELETE\nBEGIN TRANSACTION: It indicates the start point of an explicit or local transaction.\n\nRepresents a point ast which the data referenced by a connection is logically and physically consistent.\nIf errors are encountered, all data modifications made after the BEGIN TRANSACTION can be rolled back to return the data to this known state of consistency\nSyntax: BEGIN TRANSACTION transaction_name ;\n\nSET TRANSACTION: Places a name on a transaction.\n\nSyntax: SET TRANSACTION [ READ WRITE | READ ONLY ];\n\nCOMMIT: used to permanently save the changes done in the transaction in tables/databases. The database cannot regain its previous state after its execution of commit.\n\nIf everything is in order with all statements within a single transaction, all changes are recorded together in the database is called committed. The COMMIT command saves all the transactions to the database since the last COMMIT or ROLLBACK command\nExample: Delete records\nDELETE FROM Student WHERE AGE = 20;\nCOMMIT;\n\nDeletes those records from the table which have age = 20 and then commits the changes in the database.\n\n\nROLLBACK: used to undo the transactions that have not been saved in the database. The command is only been used to undo changes since the last commit\n\nIf any error occurs with any of the SQL grouped statements, all changes need to be aborted. The process of reversing changes is called rollback. This command can only be used to undo transactions since the last COMMIT or ROLLBACK command was issued.\nSyntax: ROLLBACK;\n\nSAVEPOINT: creates points within the groups of transactions in which to ROLLBACK.\n\nSyntax: SAVEPOINT &lt;savepoint_name&gt;;\nA savepoint is a point in a transaction in which you can roll the transaction back to a certain point without rolling back the entire transaction.\nRemove a savepoint: RELEASE SAVEPOINT &lt;savepoint_name&gt;\nExample: Rollback a deletion\nSAVEPOINT SP1;\n//Savepoint created.\nDELETE FROM Student WHERE AGE = 20;\n//deleted\nSAVEPOINT SP2;\n//Savepoint created.\nROLLBACK TO SP1;\n//Rollback completed.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-procexp",
    "href": "qmd/sql.html#sec-sql-procexp",
    "title": "SQL",
    "section": "Processing Expressions",
    "text": "Processing Expressions\n\nUse multiple conditions in a WHERE expression\nselect\n    *\nfrom XXX_table\nwhere 1=1\n    (if condition A) and clause 1 \n    (if condition B) and clause 2 \n    (if condition C) and clause 3\n;\n\nThe “1=1” prevents errors that would occur when the first condition doesn’t apply to any rows.\n\nCan also use “true”\n\n\nSelect unique rows without using DISTINCT\n\nUsing UNION\nSELECT employee_id,\n      employee_name,\n      department\nFROM Dummy_employees\nUNION\nSELECT employee_id,\n      employee_name,\n      department\nFROM Dummy_employees\n\nthere must be same number and order of columns in both the SELECT statements\n\nUsing INTERSECT\nSELECT employee_id,\n      employee_name,\n      department\nFROM Dummy_employees\nINTERSECT\nSELECT employee_id,\n      employee_name,\n      department\nFROM Dummy_employees\n\nThere must be same number and order of columns in both the SELECT statements\n\nUsing ROW_NUMBER\nWITH temporary_employees as (\n  SELECT\n    employee_id,\n    employee_name,\n    department,\n    ROW_NUMBER() OVER(PARTITION BY employee_name,\n                                  department,\n                                  employee_id) as row_count\n  FROM Dummy_employees\n)\n\nSELECT *\nFROM temporary_employees\nWHERE row_count = 1\nUsing GROUP BY\nSELECT employee_id,\n      employee_name,\n      department\nFROM Dummy_employees\nGROUP BY employee_id,\n        employee_name,\n        department\n\nJust need to group by all the columns. Useful to use in conjunction with aggregate functions.\n\n\nCASE WHEN\nSELECT OrderID,\n      OrderDate,\n      Sales_Manager,\n      Quantity,\n      CASE WHEN Quantity &gt; 51 THEN 'High'\n            WHEN Quantity &lt; 51 THEN 'Low'\n            ELSE 'Medium' \n      END AS OrderVolume\nFROM Dummy_Sales_Data_v1\n\nEND AS specifies the name of the new column, “OrderVolume”\nELSE specifies the value when none of the conditions are met\n\nIf you did not mention ELSE clause and no condition is satisfied, the query will return NULL for that specific record\n\n\nPivot Wider\n\nSELECT Sales_Manager,\n      COUNT(CASE WHEN Shipping_Address = 'Singapore' THEN OrderID\n            END) AS Singapore_Orders,\n\n      COUNT(CASE WHEN Shipping_Address = 'UK' THEN OrderID\n            END) AS UK_Orders,\n\n      COUNT(CASE WHEN Shipping_Address = 'Kenya' THEN OrderID\n            END) AS Kenya_Orders,\n\n      COUNT(CASE WHEN Shipping_Address = 'India' THEN OrderID\n            END) AS India_Orders\nFROM Dummy_Sales_Data_v1\nGROUP BY Sales_Manager\n\nDepending on your use-case you can also use different aggregation such as SUM, AVG, MAX, MIN with CASE statement.\n\n\n\nNULLs\n\nDivision and NULLS\n\nAny division with NULL values with have a result of NULL.\nisNull allows to get a different resulting value\nSELECT IsNull(&lt;column&gt;, 0) / 45\n\nAll NULL values in the column will replaced with 0s during the division operation.\n\n\nCOALESCE\n\nSubstitute a default value in place of NULLs\nSELECT COALESCE(column_name, 'Default Value') AS processed_column\nFROM table_name;\n\nSELECT COALESCE(order_date, current_date) AS processed_date\nFROM orders;\n\nSELECT\n  product ||' - '||\n  COALESCE(subcategory, category, family, 'no product description ')\n    AS product_and_subcategory\nFROM stock\n\n3rd Expression: If there is a NULL in subcategory, then it looks in category, then into family, and finally if all those fields have NULLs, it uses “no product description” as the value.\n\nConcantenating Strings where NULLs are present\nSELECT COALESCE(first_name, '') || ' ' || COALESCE(last_name, '') AS full_name\nFROM employees;\n\nNULLs are replaced with an empty string so transformation doesn’t break\n\nPerforming calculations involving numeric columns where there are NULLs\nSELECT COALESCE(quantity, 0) * COALESCE(unit_price, 0) AS total_cost\nFROM products;\n\nSELECT product,\n  quantity_available,\n  minimum_to_have,\n  COALESCE(minimum_to_have, quantity_available * 0.5) AS threshold\nFROM stock\n\nNULLs are substituted with 0s so the calcuation doesn’t break\n\nAs part of a join in case keys have missing values\nSELECT *\nFROM employees e\nLEFT JOIN departments d ON COALESCE(e.department_id, 0) = COALESCE(d.id, 0);\nWith Aggregate Functions\nSELECT department_id, COALESCE(SUM(salary), 0) AS total_salary\nFROM employees\nGROUP BY department_id;\nMake hierarchical subtotals output more readable\n\nSELECT COALESCE(family,'All Families') AS family,\n COALESCE(category,'All Categories') AS category,\n COALESCE(subcategory,'All Subcategories') AS subcategory,\n SUM(quantity_available) as quantity_in_stock\nFROM stock\nGROUP BY ROLLUP(family, category, subcategory)\nORDER BY family, category, subcategory\n\nROLLUP clause assumes a hierarchy among the columns family, category, and subcategory. Thus, it generates all the grouping sets that make sense considering the hierarchy: GROUP BY family, GROUP BY family, category and GROUP BY family, category, subcategory.\n\nThis is the reason why ROLLUP is often used to generate subtotals and grand totals for reports.\n\nWithout COALESCE , the text in the unused columns for the subtotals would be NULLs.\n\n\n\n\n\nDuplicated Rows\n\nRemove duplicated rows with window function\nWITH temporary_employees as \n(SELECT \n  employee_id, \n  employee_name, \n  department, \n  ROW_NUMBER() OVER(PARTITION BY employee_name, \n                                department, \n                                employee_id) as row_count \nFROM Dummy_employees)\n\nSELECT * \nFROM temporary_employees \nWHERE row_count = 1\nUse a hash column as id column, then test for duplicates, remove them or investigate them (BigQuery)\n\nWITH\n    inbound_zoo_elephants AS (\n        SELECT *\n        FROM flowfunctions.examples.zoo_elephants\n    ),\n    add_row_hash AS (\n        SELECT\n            *,\n            TO_HEX(MD5(TO_JSON_STRING(inbound_zoo_elephants))) AS hex_row_hash\n        FROM inbound_zoo_elephants\n    )\n\nSELECT\n    COUNT(*) AS records,\n    COUNT(DISTINCT hex_row_hash) AS unique_records\nFROM add_row_hash\n\nNo duplicate records found, since “records” = 9 and “unique_records” = 9\n\nif records &gt; unique_records, duplicates exist\n\nCan select distinct hex_row_hash if you want to remove duplicates\nCan count hex_row_hash then filter where hex_row_hash &gt; 1 to find which rows are duplicates\nNotes from link\nDescription\n\nflowfunctions is the project name\nexamples is a directory (?)\nzoo_elephants is the dataset\n\nSteps\n\nTO_JSON_STRING - creates column with json string for each row\nMD5 hashes that string\nTO_HEX makes it alpha-numeric and gets rid of the symbols in the hash\n\nEasier to deal with in BigQuery\nAssume this is still unique (?)\n\n\nNote: By adding “true” value, TO_JSON_STRING(inbound_zoo_elephants, true) , TO_JSON_STRING adds line breaks to the json string for easier readability.\nHashing function options\n\nMD5 -  shortest one (16 characters), fine for this use case\n\ncryptographically broken, returns 16 characters and suffices for our use-case. Other options are\n\nFARM_FINGERPRINT - returns a signed integer of variable length\nSHA1, SHA256 and SHA512, which return 20, 32 and 64 bytes respectively and are more secure for cryptographic use cases.\n\n\n\n\n\nNested Data\n\nRecursive CTE\n\nRecursive CTEs are used primarily when you want to query hierarchical data or graphs. This could be a company’s organizational structure, a family tree, a restaurant menu, or various routes between cities\nAlso see\n\nWhat Is a Recursive CTE in SQL?\n\nTutorial, 3 examples, and links to other articles\n\n\nSyntax\nWITH RECURSIVE cte_name AS (\n    cte_query_definition (the anchor member)\n    UNION ALL\n    cte_query_definition (the recursive member)\n)\n\nSELECT *\nFROM  cte_name;\nExample: : postgres\nWITH RECURSIVE category_tree(id, name, parent_id, depth, path) AS (\n  SELECT id, name, parent_id, 1, ARRAY[id]\n  FROM categories\n  WHERE parent_id IS NULL\n  UNION ALL\n  SELECT categories.id, categories.name, categories.parent_id, category_tree.depth + 1, path || categories.id\n  FROM categories\n  JOIN category_tree ON categories.parent_id = category_tree.id\n)\n\nSELECT id, name, parent_id, depth, path\nFROM category_tree;\n\nCTE (WITH) + RECURSIVE says it’s a recursive query.\nUNION ALLcombines the results of both statements.\n\nExample is defined by 2 Select statements\n\nAnchor Member: First SELECT statement selects the root nodes of the category tree (nodes with no parent)\n\nRoot node is indicated by “parent_id” = NULL\n\nRecursive member: Second SELECT statement selects the child nodes recursively\n\nAlso see Arrays for further examples of the use of UNION ALL\n\nThe “depth” column is used to keep track of the depth of each category node in the tree.\n\n“1” in the first statement\n“category_tree.depth + 1” in the second statement\n\nWith every recursion, the CTE will add 1 to the previous depth level, and it will do that until it reaches the end of the hierarchy\n\n\nThe “path” column is an array that stores the path from the root to the current node.\n\n“ARRAY[id]” in the first statement\n“path || categories.id” in the second statement\n\n“||” concatenates “path” and “id” columns (See Strings)\n\n\n\n\n\n\n\nBinning\n\nCASE WHEN\n\nSELECT\n Name, \n Grade,\n CASE\n  WHEN Grade &lt; 10 THEN '0-9'\n  WHEN Grade BETWEEN 10 and 19 THEN '10-19'\n  WHEN Grade BETWEEN 20 and 29 THEN '20-29'\n  WHEN Grade BETWEEN 30 and 39 THEN '30-39'\n  WHEN Grade BETWEEN 40 and 49 THEN '40-49'\n  WHEN Grade BETWEEN 50 and 59 THEN '50-59'\n  WHEN Grade BETWEEN 60 and 69 THEN '60-69'\n  WHEN Grade BETWEEN 70 and 79 THEN '70-79'\n  WHEN Grade BETWEEN 80 and 89 THEN '80-89'\n  WHEN Grade BETWEEN 90 and 99 THEN '90-99'\n  END AS Grade_Bucket\n FROM students\n\nBETWEEN is inclusive of the end points\nFlexible for any size of bin you need\n\nFLOOR\nSELECT\n Name,\n Grade,\n FLOOR(Grade / 10) * 10 AS Grade_Bucket\nFROM students\n\nCan easily scale up the number of bins without having to increase the lines of code\nOnly useful for evenly spaced bins\n\nLEFT JOIN on preformatted table\nCREATE OR REPLACE TABLE bins (\n    Lower_Bound INT64,\n    Upper_Bound INT64,\n    Grade_Bucket STRING\n);\n\nINSERT bins (Lower_Bound, Upper_Bound, Grade_Bucket)\nVALUES\n (0, 9, '0-9')\n (10, 19, '10-19')\n (20, 29, '20-29')\n (30, 39, '30-39')\n (40, 49, '40-49')\n (50, 59, '50-59')\n (60, 69, '60-69')\n (70, 79, '70-79')\n (80, 89, '80-89')\n (90, 99, '90-99');\n\nSELECT\n A.Name, \n A.Grade,\n B.Grade_Bucket\nFROM students AS A\nLEFT JOIN bins AS B\nON A.Grade BETWEEN B.Lower_Bound AND B.Upper_Bound\n\n“bins” table acts a template that funnels the values from your table into the correct bins\n\n\n\n\nTime Series\n\nExtract components from date-time columns\n/* MySQL */\nEXTRACT(part_of_date FROM date_time_column_name)\nYEAR(date_time_column_name)\nMONTH(date_time_column_name)\nMONTHNAME(date_time_column_name)\nDATE_FORMAT(date_time_column_name)\n\n/* SQLte */\nSELECT strftime('%m', OrderDate) as Month\n\nstrftime codes\n\n\nPreprocess Time Series with 4 Lags (article)\nWITH top_customers as (\n    --- select the customter ids you want to track\n),\ntransactions as (\n    SELECT \n      cust_id, \n      dt, \n      date_trunc('hour', cast(event_time as timestamp)) as event_hour, \n      count(*) as transactions\n    FROM ourTable\n    WHERE\n        dt between cast(date_add('day', -7, current_date) as varchar) \n        and cast(current_date as varchar)\n    GROUP BY 1,2,3 Order By event_hour asc\n)\n\nSELECT transactions.cust_id,\n      transactions.event_hour,\n      day_of_week(transactions.event_hour) day_of_week,\n        hour(transactions.event_hour) hour_of_day,\n        transactions.transactions as transactions,\n        LAG(transactions,1) OVER \n          (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag1,\n        LAG(transactions,2) OVER \n          (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag2,\n        LAG(transactions,3) OVER \n          (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag3,\n        LAG(transactions,4) OVER \n          (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag4\nFROM transactions \n    join top_customers \n      on transactions.cust_id = top_customers.cust_id\n\n/* output */\n\"cust_id\", \"event_hour\", \"day_of_week\", \"hour_of_day\", \"transactions\", \"lag1\", \"lag2\", \"lag3\", \"lag4\"\n\"Customer-123\",\"2023-01-14 00:00:00.000\",\"6\",\"0\",\"4093\",,,,,,\n\"Customer-123\",\"2023-01-14 01:00:00.000\",\"6\",\"1\",\"4628\",\"4093\",,,,,\n\"Customer-123\",\"2023-01-14 02:00:00.000\",\"6\",\"2\",\"5138\",\"4628\",\"4093\",,,,\n\"Customer-123\",\"2023-01-14 03:00:00.000\",\"6\",\"3\",\"5412\",\"5138\",\"4628\",\"4093\",,,\n\"Customer-123\",\"2023-01-14 04:00:00.000\",\"6\",\"4\",\"5645\",\"5412\",\"5138\",\"4628\",\"4093\",\n\"Customer-123\",\"2023-01-14 05:00:00.000\",\"6\",\"5\",\"5676\",\"5645\",\"5412\",\"5138\",\"4628\",\n\"Customer-123\",\"2023-01-14 06:00:00.000\",\"6\",\"6\",\"6045\",\"5676\",\"5645\",\"5412\",\"5138\",\n\"Customer-123\",\"2023-01-14 07:00:00.000\",\"6\",\"7\",\"6558\",\"6045\",\"5676\",\"5645\",\"5412\",\n\nDataset contains number of transactions made per customer per hour.\n2 WITH clauses: the first just extracts a list of customers we are interested in. Here you can add any condition that is supposed to filter in or out specific customers (perhaps you want to filter new customers or only include customers with sufficient traffic). The second WITH clause simply creates the first data set — Dataset A, which pulls a week of data for these customers and selects the customer id, date, hour, and number of transactions.\nFinally, the last and most important SELECT clause generates Dataset B, by using SQL lag() function on each row in order to capture the number of transactions in each of the hours that preceded the hour in the row.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-tools",
    "href": "qmd/sql.html#sec-sql-tools",
    "title": "SQL",
    "section": "Tools",
    "text": "Tools\n\nChatSQL: Convert plain text to MySQL query by ChatGPT\n{{sqlglot}} - no dependency Python SQL parser, transpiler, optimizer, and engine\n\nFormat SQL or translate between nearly twenty different SQL dialects.\n\nIt doesn’t just transpile active SQL code, too. Moves comments from one dialect to another.\n\nThe parser itself can be customized\nCan also help you analyze queries, traverse parsed expression trees, and incrementally (and, programmatically) build SQL queries.\nsupport for optimizing SQL queries, and performing semantic diffs.\nCan be used to unit test queries through mocks based on Python dictionaries.\nExample: : translate duckdb to hive\nimport sqlglot\nsqlglot.transpile(\n  \"SELECT EPOCH_MS(1618088028295)\", \n  read = \"duckdb\", \n  write = \"hive\"\n)[0]\n---\n'SELECT FROM_UNIXTIME(1618088028295 / 1000)'",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/bayes-reporting.html",
    "href": "qmd/bayes-reporting.html",
    "title": "Reporting",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Bayes",
      "Reporting"
    ]
  },
  {
    "objectID": "qmd/bayes-reporting.html#sec-bayes-rep-misc",
    "href": "qmd/bayes-reporting.html#sec-bayes-rep-misc",
    "title": "Reporting",
    "section": "",
    "text": "Also see Mathematics, Statistics &gt;&gt; Descriptive &gt;&gt; Understanding CI, sd, and sem Bars\n{posterior} rvars class\n\nobject class that’s designed to interoperate with vectorized distributions in {distributional}, to be able to be used inside data.frame()s and tibble()s, and to be used with distribution visualizations in the {ggdist}.\nDocs\n\nRemember CIs of parameter estimates including zero are not evidence of the null hypothesis (i.e. β = 0).\n\nEspecially if CIs are broad and most of the posterior probability distribution is massed away from zero\n\nVisualization for differences (Thread)",
    "crumbs": [
      "Bayes",
      "Reporting"
    ]
  },
  {
    "objectID": "qmd/bayes-reporting.html#sec-bayes-rep-sdfe",
    "href": "qmd/bayes-reporting.html#sec-bayes-rep-sdfe",
    "title": "Reporting",
    "section": "Significant Digits for Estimates",
    "text": "Significant Digits for Estimates\n\nMisc\n\nNotes from: Bayesian workflow book - Digits\n\nBefore we can answer how many chains and iterations we need to run, we need to know how many significant digits we want to report\nMCMC in general doesn’t produce independent draws and the effect of dependency affects how many draws are needed to estimate different expectations\nGuidelines in general\n\nIf the posterior would be close to a normal(μ,1), then\n\nFor 2 significant digit accuracy,\n\n2000 independent draws from the posterior would be sufficient for that 2nd digit to only sometimes vary.\n4 chains with 1000 iterations after warmup is likely to give near two significant digit accuracy for the posterior mean. The accuracy for 5% and 95% quantiles would be between one and two significant digits.\nWith 10,000 draws, the uncertainty is 1% of the posterior scale which would often be sufficient for two significant digit accuracy.\n\nFor 1 significant digit accuracy, 100 independent draws would be often sufficient, but reliable convergence diagnostics may need more iterations than 100.\nFor posterior quantiles, more draws may be needed (need more draws to get values towards the tails of the posterior)\n\nSome quantities of interest may have posterior distribution with infinite variance, and then the ESS and MCSE are not defined for the expectation.\n\nIn such cases, use median instead of mean and mean absolute deviation (MAD) instead of standard deviation.\nVariance of parameter posteriors\nas_draws_rvars(brms_fit) %&gt;%\n    summarise_draws(var = distributional::variance) \n#&gt;    variable  var\n#&gt;    &lt;chr&gt;    &lt;dbl&gt;\n#&gt;  1 mu        11.6\n#&gt;  2 tau      12.8\n#&gt;  3 theta[1]  39.7\n#&gt;  4 theta[2]  21.5\n\n\nSteps\n\nCheck convergence diagnostics for all parameters\n\ne.g. RHat, ESS, autocorrelation plots (see Diagnostics, Bayes)\n\nLook at the posterior for quantities of interest and decide how many significant digits is reasonable taking into account the posterior uncertainty (using SD, MAD, or tail quantiles)\n\nYou want to be able to distinguish you upper or lower CI from the point estimate\n\ne.g. Point estimate is 2.1 and you upper CI is 2.1 then you want at least another significant digit.\n\n\nCheck that MCSE is small enough for the desired accuracy of reporting the posterior summaries for the quantities of interest.\n\nCalculate the range of variation due to MC sampling for your paramter (See MCSE example)\n\nMC sampling error is the average amount of variation that’s expected from changing seeds and re-running the analysis\n\nIf the accuracy is not sufficient (i.e. range is too wide), report less digits or run more iterations.\n\n\nMonte Carlo standard error (MCSE) - uncertainty about a parameter estimate due to MCMC sampling error\n\nPackages\n\n{posterior} is the preferred package for brms objects\n{mcmcse} - methods to calculate MCMC standard errors for means and quantiles using sub-sampling methods. (Different calculation than used by Stan)\nbayestestR::mcse uses Kruschke 2015 method of calculation\n\nExample: brms, MCSE quantiles\n# Coefficient and CI estimates for the \"beta100\" variable\nas_draws_rvars(brms_fit) %&gt;%\n  subset_draws(\"beta100\") %&gt;%\n  summarize_draws(mean, ~quantile(.x, probs = c(0.05, 0.95)))\n#&gt; variable  mean      5%   95%\n#&gt; beta100   1.966  0.673 3.242\n\nas_draws_rvars(brms_fit) %&gt;%\n  subset_draws(\"beta100\") %&gt;% # select variable\n  summarize_draws(mcse_mean, ~mcse_quantile(.x, probs = c(0.05, 0.95)))\n#&gt; variable  mcse_mean  mcse_q5 mcse_q95\n#&gt; beta100       0.013    0.036    0.033\n\nSpecification\n\n“mcse_mean” and “mean” are available as preloaded functions that summary_draws can use out of the box\n“mcse_quantile” (also in {posterior}) and “quantile” are not preloaded functions so they’re called as lambda functions\n\nThese are MCSE values for\n\nthe summary estimate (aka point estimate) which is the mean of the posterior in this case\nAnd the CI values of that summary estimate\n\nTail quantiles will have greater amounts of error sampling in the tails of the posterior than in the bulk (i.e. less accurate tail estimates)\nFewer points, more uncertainty\n\n\nCalculate the range of variation due to Monte Carlo\n\nMultiply the MCSE values by 2, the likely range of variation due to Monte Carlo is ±0.02 for mean and ±0.07 for 5% and 95% quantiles\n\nMultiplying by 2, since I guess they’re assuming a normal distribution posterior, therefore estimate ± 1.96 * SE\n\n\nConclusion for “beta100” coefficient\n\nIf the mean estimate for beta100 is reported as 2 (rounded up from 1.966), then there is unlikely to be any variation in that estimate due to MCMC sampling. (i.e. okay to report the estimate as 2)\n\nThis is because\n\n1.966 + 0.02 = 1.986 which would still be rounded up to 2\n1.966 - 0.02 = 1.946 which would still be rounded up to 2\n\n\n\nDraws and iterations\n\nWith an MCSE in the 100ths (e.g. 0.07), 4 times more iterations would halve the MCSEs\nWith an MCSE in the 1000ths (e.g. 0.007), 64 times more iterations would halve the MCSEs\nMCSEs depend on the quantity type. Continuous quantities (e.g. parameter estimates) have more information than discrete quantities (e.g. indicator values used to calculate probabilities).\n\nFor example, above, the estimate for whether the temperature increase is larger than 4 degrees per century has high ESS, but the indicator variable contains less information (than continuous values) and thus much higher ESS would be needed for two significant digit accuracy.",
    "crumbs": [
      "Bayes",
      "Reporting"
    ]
  },
  {
    "objectID": "qmd/bayes-reporting.html#probabilistic-inference-of-estimates",
    "href": "qmd/bayes-reporting.html#probabilistic-inference-of-estimates",
    "title": "Reporting",
    "section": "Probabilistic Inference of Estimates",
    "text": "Probabilistic Inference of Estimates\n\nMisc\n\nNotes from: Bayesian workflow book - Digits\n\nExample: probability that an estimate is positive\nas_draws_rvars(brms_fit) %&gt;%\n  # binary 1/0, posterior samples &gt; 0\n  mutate_variables(beta0p = beta100 &gt; 0) %&gt;% \n  subset_draws(\"beta0p\") %&gt;% # select variable\n  summarize_draws(\"mean\", mcse = mcse_mean)\n\n#&gt; variable  mean  mcse\n#&gt;   beta0p 0.993 0.001\n99.3% probability the estimate is above zero +/- 0.2% (= 2*MCSE)\nMCSE indicates that we have enough MCMC iterations for practically meaningful reporting that the probability that the variable (e.g. temperature) is increasing (i.e. slope is positive) is larger than 99%\nExample: probability that an estimate &gt; 1,2,3,4\nas_draws_rvars(brms_fit) %&gt;%\n  subset_draws(\"beta100\") %&gt;%\n  # binary 1/0 variable\n  mutate_variables(beta1p = beta100 &gt; 1,\n                  beta2p = beta100 &gt; 2,\n                  beta3p = beta100 &gt; 3,\n                  beta4p = beta100 &gt; 4) %&gt;%\n  subset_draws(\"beta[1-4]p\", regex=TRUE) %&gt;%\n  summarize_draws(\"mean\", mcse = mcse_mean, ESS = ess_mean)\n\n#&gt; variable  mean mcse  ESS\n#&gt;  beta1p 0.896 0.006 3020\n#&gt;  beta2p 0.487 0.008 4311\n#&gt;  beta3p 0.088 0.005 3188\n#&gt;  beta4p 0.006 0.001 3265\nTaking into account MCSEs given the current posterior sample, we can summarise these as\n\np(beta100&gt;1) = 88%–91%,\np(beta100&gt;2) = 46%–51%,\np(beta100&gt;3) = 7%–10%,\np(beta100&gt;4) = 0.2%–1%.\n\nTo get these probabilities estimated with 2 digit accuracy would again require more iterations (16-300 times more iterations depending on the quantity), but the added iterations would not change the conclusion radically.perature in the center of the time range (instead defining prior for temperature at year 0).",
    "crumbs": [
      "Bayes",
      "Reporting"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html",
    "href": "qmd/post-hoc-analysis-anova.html",
    "title": "ANOVA",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-misc",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-misc",
    "title": "ANOVA",
    "section": "",
    "text": "Packages\n\n{car} - Anova function that computes all 3 types of ANOVA table\n\nCan also be applied to glm models to produce Analysis of Deviance tables (e.g. logistic, poisson, etc.)\nThink the other packages wrap this function, so they can be used instead in order to advantage of their plotting, testing conveniences.\n\n{grafify} - ANOVA wrappers, plotting, wrappers for {emmeans}\n{afex} - Analysis of Factorial EXperiments\n\nANOVA helper functions that fit the lm, center, apply contrasts, etc. in one line of code\n\nExample: afex::aov_car(Y ~ group * condition + Error(id), data = d)\nType III used, Factor variables created, Sum-to-Zero contrast is applied\n\nEffect plotting functions\n\n\nNotes from\n\nEverything You Always Wanted to Know About ANOVA\n\nANOVA vs. Regression (GPT-3.5)\n\nDifferent Research Questions:\n\nANOVA is typically used when you want to compare the means of three or more groups to determine if there are statistically significant differences among them. It’s suited for situations where you’re interested in group-level comparisons (e.g., comparing the average test scores of students from different schools).\nRegression, on the other hand, is used to model the relationship between one or more independent variables and a dependent variable. It’s suitable for predicting or explaining a continuous outcome variable.\n\nData Type:\n\nANOVA is traditionally used with categorical independent variables and a continuous dependent variable. It helps assess whether the categorical variable has a significant impact on the continuous variable.\n\nThere are other variants such as ANCOVA (categorical and continuous IVs) and Analysis of Deviance (discrete outcome)\n\nRegression can be used with both categorical and continuous independent variables to predict a continuous dependent variable or to examine the relationship between variables.\n\nMultiple Factors:\n\nANOVA is designed to handle situations with multiple categorical independent variables (factors) and their interactions. It is useful when you are interested in understanding the combined effects of several factors.\nRegression can accommodate multiple independent variables as well, but it focuses on predicting the value of the dependent variable rather than comparing groups.\n\nHypothesis Testing:\n\nANOVA tests for differences in means among groups and provides p-values to determine whether those differences are statistically significant.\nRegression can be used for hypothesis testing, but it’s more often used for estimating the effect size and making predictions.\n\nAssumptions:\n\nANOVA assumes that the groups are independent and that the residuals (the differences between observed values and group means) are normally distributed and have equal variances.\nRegression makes similar assumptions about residuals but also assumes a linear relationship between independent and dependent variables.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-gen",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-gen",
    "title": "ANOVA",
    "section": "General",
    "text": "General\n\nFamily of procedures which summarizes the relationship between the underlying model and the outcome by partitioning the variation in the outcome into components which can be uniquely attributable to different sources according to the law of total variance.\nEssentially, each of the model’s terms is represented in a line in the ANOVA table which answers the question how much of the variation in Y can be attributed to the variation in X?\n\nWhere applicable, each source of variance has an accompanying test statistic (oftenF), sometimes called the omnibus test, which indicates the significance of the variance attributable to that term, often accompanied by some measure of effect size.\n\nOne-Way ANOVA - 1 categorical, independent variable\n\nDetermines whether there is a statistically significant difference in the means of the dependent variable across the different levels of the independent variable.\nExample: A researcher wants to compare the average plant height grown using three different types of fertilizer. They would use a one-way ANOVA to test if there is a significant difference in height between the groups fertilized with each type.\n\nTwo-Way ANOVA - 2 categorical, independent variables\n\nExample: 3 treatments are given to subjects and the researcher thinks that females and males will have different responses in general.\n\nTest whether there are treatment differences after accounting for sex effects\nTest whether there are sex differences after accounting for treatment effects\nTest whether the treatment effect is different for females and males if you allow the treatment \\(\\times\\) sex interaction to be in the model\n\n\nTypes\n\nTL;DR;\n\nI don’t see a reason not to run type III every time.\nType I: Sequential Attribution of Variation\nType II: Simultaneous Attribution of Variation\n\nFor interactions: Sequential-Simultaneous Attribution of Variation\n\nType III: Simultaneous Attribution of Variation for Main Effects and Interactions\nIf the categorical explanatory variables in the analysis are balanced, then all 3 types will give the same results. The results for each variable will be it’s unique contribution.\n\nExample:\n# balanced\ntable(d$Rx, d$condition)\n#&gt;           Ca Cb\n#&gt;   Placebo  5  5\n#&gt;   Dose100  5  5\n#&gt;   Dose250  5  5\n\n# imbalanced\ntable(d$group, d$condition)\n#&gt;      Ca Cb\n#&gt;   Gb  6  6\n#&gt;   Ga  5  6\n#&gt;   Gc  4  3\n\n\nType I: Sequential Sum of Squares\n\nVariance attribution is calculated sequentially so the order of variables in the model matters. Each term is attributed with a portion of the variation (represented by its SS) that has not yet been attributed to any of the previous terms.\nRarely used in practice because the order in which variation is attributed isn’t usually important\nExample: Order of terms matters\nanova(lm(Y ~ group + X, data = d))\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Response: Y\n#&gt;           Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \n#&gt; group      2    8783    4391  0.0918 0.912617   \n#&gt; X          1  380471  380471  7.9503 0.009077 **\n#&gt; Residuals 26 1244265   47856                    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm(Y ~ X + group, data = d))\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Response: Y\n#&gt;           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \n#&gt; X          1  325745  325745  6.8067 0.01486 *\n#&gt; group      2   63509   31754  0.6635 0.52353  \n#&gt; Residuals 26 1244265   47856                  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSum of Squares values change based on the order of the terms in the model\nIn the first model,\n\nThe effect of group does not represent its unique contribution to Y’s variance, but instead its total contribution.\n\nThis reminds me of a dual path DAG where group is influenced by X. Here X’s variance contribution is included in group’s contribution since X is not conditioned upon. (See Causal Inference &gt;&gt; Dual Path DAGs)\n\nThe effect of X represents only what X explains after removing the contribution of group — the variance attributed to X is strictly the variance that can be uniquely attributed to X, controlling for group\n\n\n\nType II: Simultaneous Sum of Squares\n\nThe variance attributed to each variable is its unique contribution — variance after controlling for the other variables. Order of terms does not matter.\nExample\ncar::Anova(m, type = 2)\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: Y\n#&gt;            Sum Sq Df F value   Pr(&gt;F)   \n#&gt; group       63509  2  0.6635 0.523533   \n#&gt; X          380471  1  7.9503 0.009077 **\n#&gt; Residuals 1244265 26                    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSum of Squares values are equal to values of the Type 1 results when each variable is last.\nNote that factor variables, e.g. group, are treated as 1 term and not broken down into dummy variables for each level.\n\nWith interactions, the method of calculation could be called, Sequential-Simultaneous.\n\nTerms are evaluated simultaneously in groups based on type of term, e.g. main effects, 2-way interactions, 3-way interactions, etc., but sequentially according to the order of that term where the order of main effects &lt; 2-way interactions &lt; 3-way interactions, etc.\nAll main effects (1st order) are tested simultaneously (accounting for one another), then all 2-way interactions (2nd order) are tested simultaneously (accounting for the main effects and one another), and finally the 3-way interaction is tested (accounting for all main effects and 2-way interactions).\nSo, if you use this way to test a model with interactions, only the highest order term’s Sum of Squares represents a unique variance contribution.\n\n\nType III: Simultaneous-Simultaneous Sum of Squares\n\nThe Sum-of-Squares for each main effect and interaction is calculated as its unique contribution (i.e. takes into account all other terms of the model).\nUnlike Type II, it allows you compare variance contributions for every term in your model.\nWithout centering continuous variables and applying sum-to-zero contrasts to categorical variables, tests results can change depending on the categorical level of the moderator. (Also see Regression, Linear &gt;&gt; Contrasts &gt;&gt; Sum-to-Zero)\n\nExample\n\nNo Centering, No Sum-to-Zero Contrasts\nm_int &lt;- lm(Y ~ group * X, data = d)\n\nd$group &lt;- relevel(d$group, ref = \"Gb\")\nm_int2 &lt;- lm(Y ~ group * X, data = d)\n\ncar::Anova(m_int, type = 3)\n#&gt;             Sum Sq Df F value    Pr(&gt;F)    \n#&gt; (Intercept) 538630  1 22.9922 6.994e-05 ***\n#&gt; group       738108  2 15.7536 4.269e-05 ***\n#&gt; X           101495  1  4.3325   0.04823 *  \n#&gt; group:X     682026  2 14.5566 7.246e-05 ***\n#&gt; Residuals   562240 24     \n\ncar::Anova(m_int2, type = 3)\n#&gt;             Sum Sq Df F value    Pr(&gt;F)    \n#&gt; (Intercept) 219106  1  9.3528  0.005402 ** \n#&gt; group       738108  2 15.7536 4.269e-05 ***\n#&gt; X           910646  1 38.8722 1.918e-06 ***\n#&gt; group:X     682026  2 14.5566 7.246e-05 ***\n#&gt; Residuals   562240 24  \n\nThe sum of squares and p-value change for X when the categorical variable’s reference level changed which shouldn’t matter given this is an omnibus test (i.e. the categorical variable is treated as 1 entity and not set of dummy variables).\n\nCentered, Sum-to-Zero Contrasts Applied\n# center, contr.sum\nd_contr_sum &lt;- d |&gt; \n  mutate(X_c = scale(X, scale = FALSE))\ncontrasts(d_contr_sum$group) &lt;- contr.sum\nm_int_cont_sum &lt;- lm(Y ~ group * X_c, data = d_contr_sum)\ncar::Anova(m_int_cont_sum, type = 3)\n#&gt;              Sum Sq Df  F value    Pr(&gt;F)    \n#&gt; (Intercept) 4743668  1 202.4902 3.401e-13 ***\n#&gt; group         19640  2   0.4192   0.66231    \n#&gt; X_c          143772  1   6.1371   0.02067 *  \n#&gt; group:X_c    682026  2  14.5566 7.246e-05 ***\n#&gt; Residuals    562240 24\n\n# change reference level\nd_rl &lt;- d_contr_sum |&gt; \n  mutate(group_rl = relevel(group, ref = \"Gb\"))\ncontrasts(d_rl$group_rl) &lt;- contr.sum\ncar::Anova(lm(Y ~ group_rl * X_c, data = d_rl),\n           type = 3)\n#&gt;               Sum Sq Df  F value    Pr(&gt;F)    \n#&gt; (Intercept)  4743668  1 202.4902 3.401e-13 ***\n#&gt; group_rl       19640  2   0.4192   0.66231    \n#&gt; X_c           143772  1   6.1371   0.02067 *  \n#&gt; group_rl:X_c  682026  2  14.5566 7.246e-05 ***\n#&gt; Residuals     562240 24   \n\nNow when the reference level is changed, the sum-of-squares and p-value for X remain the same.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#assumptions",
    "href": "qmd/post-hoc-analysis-anova.html#assumptions",
    "title": "ANOVA",
    "section": "Assumptions",
    "text": "Assumptions\n\nEach group category has a normal distribution.\nEach group category is independent of each other and identically distributed (iid)\nGroup categories have of similar variance (i.e. homoskedastic variance)\n\nIf this is violated\n\nIf the ratio of the largest variance to the smallest variance is less than 4, then proceed with one-way ANOVA (robust to small differences)\nIf the ratio of the largest variance to the smallest variance is greater than 4, perform a Kruskal-Wallis test. This is considered the non-parametric equivalent to the one-way ANOVA. (example)\n\nEDA\ndata %&gt;%\n  group_by(program) %&gt;%\n  summarize(var=var(weight_loss))\n#&gt; A tibble: 3 x 2\n#&gt;   program  var   \n#&gt; 1 A      0.819\n#&gt; 2 B      1.53 \n#&gt; 3 C      2.46\nPerform a statisical test to see if these variables are statistically significant (See Post-Hoc Analysis, Difference-in-Means &gt;&gt; EDA &gt;&gt; Tests for Equal Variances)",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-math",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-math",
    "title": "ANOVA",
    "section": "Mathematics",
    "text": "Mathematics\n\nAsides:\n\nThis lookd like the variance formula except for not dividing by the sample size to get the “average” squared distance\nSSA formula - the second summation just translates to multiplying by ni, the group category sample size, since there is no j in that formula\n\nCalculate SSA and SSE\n\\[\n\\begin{align}\n\\text{SST} &= \\text{SSA} + \\text{SSE} \\\\\n&= \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (x_{i,j} - \\mu)^2 \\\\\n&= \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (\\bar x_i - \\mu)^2 + \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (x_{i,j} - \\bar x_i)^2\n\\end{align}\n\\]\n\n\\(\\text{SST}\\): Sum of Squares Total\n\\(\\text{SSA}\\): Sum of Squares between categories, treatments, or factors\n\n“A” stands for attributes (i.e. categories)\n\n\\(\\text{SSE}\\): Sum of Squares of Errors; randomness within categories, treatments, or factors\n\\(x_{ij}\\): The jth observation of the ith category\n\\(\\bar x_i\\): The sample mean of category i\n\\(\\mu\\): The overall sample mean\n\\(n_i\\): The group category sample size\n\\(a\\): The number of group categories\n\nCalculate MSA and MSE\n\\[\n\\begin{align}\n\\text{MSE} &= \\frac{\\text{SSE}}{N-a} \\\\\n\\text{MSA} &= \\frac{\\text{SSA}}{a-1}\n\\end{align}\n\\]\n\nWhere N is the total sample size\n\nCalculate the F statistic and P-Value\n\\[\nF = \\frac{\\text{MSA}}{\\text{MSE}}\n\\]\n\nFind the p-value (need a table to look it up)\nIf our F statistic is less than the critical value F statistic for a \\(\\alpha = 0.05\\) than we cannot reject the null hypothesis (no statistical difference between categories)\n\nDiscussion\n\nIf there is a group category that has more variance than the others’ attribute error (SSA), we should then pick that up when we compare it to the random error (SSE)\n\nIf a group is further away from the overall mean, then it will increase SSA and thus influence the overall variance but might not always increase random error",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#diagnostics",
    "href": "qmd/post-hoc-analysis-anova.html#diagnostics",
    "title": "ANOVA",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nEta Squared\n\nMetric to describe the effect size of a variable\nRange: [0, 1]; values closer to 1 indicating that a specific variable in the model can explain a greater fraction of the variation\nlsr::etaSquared(anova_model) (use first column of output)\nGuidelines\n\n0.01: Effect size is small.\n0.06: Effect size is medium.\nLarge effect size if the number is 0.14 or above\n\n\nPost-ANOVA Tests\n\nAssume approximately Normal distributions\nFor links to more details about each test, https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/post-hoc/\nDuncan’s new multiple range test (MRT)\n\nWhen you run Analysis of Variance (ANOVA), the results will tell you if there is a difference in means. However, it won’t pinpoint the pairs of means that are different. Duncan’s Multiple Range Test will identify the pairs of means (from at least three) that differ. The MRT is similar to the LSD, but instead of a t-value, a Q Value is used.\n\nFisher’s Least Significant Difference (LSD)\n\nA tool to identify which pairs of means are statistically different. Essentially the same as Duncan’s MRT, but with t-values instead of Q values.\n\nNewman-Keuls\n\nLike Tukey’s, this post-hoc test identifies sample means that are different from each other. Newman-Keuls uses different critical values for comparing pairs of means. Therefore, it is more likely to find significant differences.\n\nRodger’s Method\n\nConsidered by some to be the most powerful post-hoc test for detecting differences among groups. This test protects against loss of statistical power as the degrees of freedom increase.\n\nScheffé’s Method\n\nUsed when you want to look at post-hoc comparisons in general (as opposed to just pairwise comparisons). Scheffe’s controls for the overall confidence level. It is customarily used with unequal sample sizes.\n\nTukey’s Test\n\nThe purpose of Tukey’s test is to figure out which groups in your sample differ. It uses the “Honest Significant Difference,” a number that represents the distance between groups, to compare every mean with every other mean.\n\nDunnett’s Test\n\nLike Tukey’s this post-hoc test is used to compare means. Unlike Tukey’s, it compares every mean to a control mean.\n{DescTools::DunnettTest}\n\nBenjamin-Hochberg (BH) Procedure\n\nIf you perform a very large amount of tests, one or more of the tests will have a significant result purely by chance alone. This post-hoc test accounts for that false discovery rate.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-oneway",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-oneway",
    "title": "ANOVA",
    "section": "One-Way",
    "text": "One-Way\n\nMeasures if there’s a difference in means between any group category\nExample: 1 control, 2 Test groups\n\nData\ndata &lt;- data.frame(Group = rep(c(\"control\", \"Test1\", \"Test2\"), each = 10),\nvalue = c(rnorm(10), rnorm(10),rnorm(10)))\ndata$Group&lt;-as.factor(data$Group)\nhead(data)\n#&gt;   Group      value\n#&gt; 1 control  0.1932123\n#&gt; 2 control -0.4346821\n#&gt; 3 control  0.9132671\n#&gt; 4 control  1.7933881\n#&gt; 5 control  0.9966051\n#&gt; 6 control  1.1074905\nFit model\nmodel &lt;- aov(value ~ Group, data = data)\nsummary(model)\n#&gt;             Df    Sum Sq   Mean Sq  F value  Pr(&gt;F) \n#&gt; Group        2     4.407    2.2036     3.71  0.0377 *\n#&gt; Residuals   27    16.035    0.5939\n\n# or\nlm_mod &lt;- lm(value ~ Group, data = data)\nanova(lm_mod)\n\nP-Value &lt; 0.05 says at least 1 group category has a statistically significant different mean from another category\n\nDunnett’s Test\nDescTools::DunnettTest(x=data$value, g=data$Group)\n\n#&gt; Dunnett's test for comparing several treatments with a control : \n#&gt;     95% family-wise confidence level\n#&gt; $control\n#&gt;                     diff    lwr.ci      upr.ci  pval   \n#&gt; Test1-control -0.8742469 -1.678514 -0.06998022 0.0320 * \n#&gt; Test2-control -0.7335283 -1.537795  0.07073836 0.0768 .\n\nMeasures if there is any difference between treatments and the control\nThe mean score of the test1 group was significantly higher than the control group. The mean score of the test2 group was not significantly higher than the control group.\n\nTukey’s HSD\nstats::TukeyHSD(model, conf.level=.95)\n\nMeasures difference in means between all categories and each other",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ancova",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ancova",
    "title": "ANOVA",
    "section": "ANCOVA",
    "text": "ANCOVA\n\nAnalysis of Covariance is used to measure the main effect and interaction effects of categorical variables on a continuous dependent variable while controlling the effects of selected other continuous variables which co-vary with the dependent variable.\nMisc\n\nAnalysis of covariance is classical terminology for linear models but we often use the term for nonlinear models (Harrell)\nSee also\n\nHarrell - Biostatistics for Biomedical Research Ch. 13\n\n\nAssumptions\n\nIndependent observations (i.e. random assignment, avoid is having known relationships among participants in the study)\nLinearity: the relation between the covariate(s) and the dependent variable must be linear.\nNormality: the dependent variable must be normally distributed within each subpopulation. (only needed for small samples of n &lt; 20 or so)\nHomogeneity of regression slopes: the beta-coefficient(s) for the covariate(s) must be equal among all subpopulations. (regression lines for these individual groups are assumed to be parallel)\n\nFailure to meet this assumption implies that there is an interaction between the covariate and the treatment.\nThis assumption can be checked with an F test on the interaction of the independent variable(s) with the covariate(s).\n\nIf the F test is significant (i.e., significant interaction) then this assumption has been violated and the covariate should not be used as is.\nA possible solution is converting the continuous scale of the covariate to a categorical (discrete) variable and making it a subsequent independent variable, and then use a factorial ANOVA to analyze the data.\n\n\nThe covariate (adjustment variable) and the treatment are independent\nmodel &lt;- aov(grade ~ technique, data = data)\nsummary(model)\n\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; technique    2    9.8    4.92    0.14  0.869\n#&gt; Residuals  87 3047.7  35.03\n\nH0: variables are independent\n\n\nHomogeneity of variance: variance of the dependent variable must be equal over all subpopulations (only needed for sharply unequal sample sizes)\n# response ~ treatment\nleveneTest(exam ~ technique, data = data)\n\n#&gt;       Df F value    Pr(&gt;F)   \n#&gt; group  2  13.752 6.464e-06 ***\n#&gt;       87\n\n# alt test\nfligner.test(size ~ location, my.dataframe)\n\nH0: Homogeneous variance\nThis one fails\n\nFit\nancova_model &lt;- aov(exam ~ technique + grade, data = data)\ncar::Anova(ancova_model, type=\"III\")\n\n#&gt;                 Sum Sq Df F value    Pr(&gt;F)   \n#&gt;     (Intercept) 3492.4  1 57.1325 4.096e-11 ***\n#&gt;     technique  1085.8  2  8.8814 0.0003116 ***\n#&gt;     grade          4.0  1  0.0657 0.7982685   \n#&gt;     Residuals  5257.0 86\n\nWhen adjusting for current grade (covariate), study technique (treatment) has a significant effect on the final exam score (response).\n\nDoes the effect differ by treatment\npostHocs &lt;- multicomp::glht(ancova_model, linfct = mcp(technique = \"Tukey\"))\nsummary(postHocs)\n\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   \n#&gt; B - A == 0   -5.279      2.021  -2.613  0.0284 * \n#&gt; C - A == 0    3.138      2.022   1.552  0.2719   \n#&gt; C - B == 0    8.418      2.019   4.170  &lt;0.001 ***\n\nAlso see Post-Hoc Analysis, Multilevel &gt;&gt; Tukey’s Test\n\\(A\\), \\(B\\), and \\(C\\) are the study techniques (treatment)\nSignificant differences between \\(B\\) and \\(A\\) and a pretty large difference between \\(B\\) and \\(C\\).\n\nExample: RCT\n\\[\n\\begin{align}\n\\text{post}_i &\\sim \\mathcal{N}(\\mu_i, \\sigma_\\epsilon)\\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{tx}_i + \\beta_2 \\text{pre}_i\n\\end{align}\n\\]\nw2 &lt;- glm(\n  data = dw,\n  family = gaussian,\n  post ~ 1 + tx + pre)\n\nSpecification\n\npost, pre: The post-treatment and pre-treatment measurement of the outcome variable\ntx: The treatment indicator variable\n\\(\\beta_0\\): Population mean for the outcome variable in the control group\n\\(\\beta_1\\): Parameter is the population level difference in pre/post change in the treatment group, compared to the control group.\n\nAlso a causal estimate for the average treatment effect (ATE) in the population, τ\n\nBecause pre is added as a covariate, both \\(\\beta_0\\) and \\(\\beta_1\\) are conditional on the outcome variable, as collected at baseline before random assignment.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html",
    "href": "qmd/visualization-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-misc",
    "href": "qmd/visualization-general.html#sec-vis-gen-misc",
    "title": "General",
    "section": "",
    "text": "Notes from\n\nFriends Don’t Let Friends\n\nMicrosoft Paint 3D\n\nLocation: Start &gt;&gt; All Programs &gt;&gt; Paint 3D\nHightlight Text\n\nClick 2D Shapes (navbar) &gt;&gt; Select square (side panel)\nLeft click and hold &gt;&gt; Extend area around text you want to highlight &gt;&gt; Release\nChoose Line Type color and Sticker Opacity level (37%)\nOn area surrrounding text\n\nIf needed, make area size adjustment dragging little box-shaped icons that are along the outside\nOn the right side, click the check mark icon to finalize\n\nClick Menu (left-side on navbar) &gt;&gt; save as &gt;&gt; Image\n\nIt adds a png extension, but you just need to type the name.\n\n\n\nAlt Text\n\nThe guiding principle is to write alt text that gives disabled readers as close to the same experience as nondisabled readers as possible.\n\nggplot2\n\nDon’t use stat calculating geoms and set axis limits with scale_y_continuous\n\n\nSee examples of the behavior in this thread\n\nDefaults for any {ggplot2} geom using the default_aes field (i.e. GeomBlah$default_aes )\n\nFractional Data\n\nUse Stacked Bars instead of Pie or Circular or Donut\n\nHumans are better at judging lengths than angles (article)\n\n\nFactorial Experiments\n\nDon’t use bars factorial experiments\n\nCheck outcome ranges by group when facetting",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-concepts",
    "href": "qmd/visualization-general.html#sec-vis-gen-concepts",
    "title": "General",
    "section": "Concepts",
    "text": "Concepts\n\nExploration and Analysis\n\nGoal: explore a new dataset, gertan overview, find answers to specific questions\nFast iteration of many generic charts, don’t customize or worry about color schemes, etc.\n\nExplanation\n\nGoal: help others understand a relationship in the data\nUse as few charts as possible, carefully chosen\nSequence so that they are easy to understand\nAdd interaction to help people get a better understanding\n\nPresentation\n\nGoal: walk your audience through an argument, help them come to a decision\nFocus on polishing charts: colors, legends, titles, etc.\nHighlighting of key elements (which might be considered biasing in Exploration)\nPossibly use of unusual charts for memorability\nSequence to make a specific point",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-svg",
    "href": "qmd/visualization-general.html#sec-vis-gen-svg",
    "title": "General",
    "section": "SVG",
    "text": "SVG\n\nBetter for doing post-processing in Inkscape and gimp\nSVGs won’t be pixelated when you zoom in like PNGs are\nD3 outputs SVG\nsvglite PKG\n\nusing svglite instead of base::svg( ) allows you alter text in Inkscape or Illustrator\nrequires the used fonts to be present on the system it is viewed on.\n\nThe vast majority of interactive data visualizations on the web are now based on D3.js which often renders to SVG and it all seems to behave. Still, this is something to be mindful of, and a reason to use svg() if exactness of the rendered text is of prime importance\n\nFile size will be dramatically smaller",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-layout",
    "href": "qmd/visualization-general.html#sec-vis-gen-layout",
    "title": "General",
    "section": "Layout",
    "text": "Layout\n\nFacetting vs Single Graph\n\nLayout based on experiement design\n\nAlign title ALL the way to the left (ggplot: plot.title.position = “plot”)\nremove legends\n\nuse colored text in title (ggtext)\nlabel points or lines\nlast resort: place legend underneath title/subtitle\n\ngrid lines\n\nremove if possible\nsparse and faint if needed\n\naxis labels\n\nremove if obvious (e.g brands of cars)\ncreate a title that informs about the axis labels\nshould always be horizontal\n\nflip axis, don’t angle them 45 degrees\n\n\ntext\n\nleft-align most text\ncan center a subtitle if it helps with making the graph more symmetrical\nsome labels can be right-aligned\n\nRemove all borders\nMaximize white space\n\ndon’t cram visuals together\n\nWorking memory. A cognitive limitation that affects plot comprehension is the limit on working memory. Typically, working memory is limited to approximately seven (plus or minus two) items, or chunks. In practice, this means that categorical scales with more than seven categories decrease readability, increase comprehension time, and require significant attentional resources, because it is not possible to hold the legend mapping in working memory.\nThe use of redundant aesthetics that activate the same gestalt principles (such as color and shape in a scatter plot, which both activate similarity) results in higher identification of corresponding data features. In addition, dual encoding increases the accessibility of a chart to individuals who have impaired color vision or perceptual processing (e.g., dyslexia, dysgraphia). This experimental evidence directly contradicts the guidelines popularized by Tufte (1991), which suggest the elimination of any feature that is not dedicated to representing the core data, including redundant encoding and other unnecessary graphical elements.\nggplot themes\n\nCedric Sherer (article)\ntheme_set(theme_minimal(base_size = 15, base_family = \"Anybody\"))\ntheme_update(\n  axis.title.x = element_text(margin = margin(12, 0, 0, 0), color = \"grey30\"),\n  axis.title.y = element_text(margin = margin(0, 12, 0, 0), color = \"grey30\"),\n  panel.grid.minor = element_blank(),\n  panel.border = element_rect(color = \"grey45\", fill = NA, linewidth = 1.5),\n  panel.spacing = unit(.9, \"lines\"),\n  strip.text = element_text(size = rel(1)),\n  plot.title = element_text(size = rel(1.4), face = \"bold\", hjust = .5),\n  plot.title.position = \"plot\"\n)",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-ar",
    "href": "qmd/visualization-general.html#sec-vis-gen-ar",
    "title": "General",
    "section": "Aspect Ratio",
    "text": "Aspect Ratio\n\nMisc\n\nGolden Rectangle\n```{{{r, fig.width = 6, fig.asp = 1.618}}}\n```\nGet consistent outputs\n\nRStudio pane displays in 72dpi which can mislead you on what your output looks like.\nThink {ragg} is supposed to have taken care of the inconsistency in terms of printing on different OSes\nUsing {camcorder}\n\nStart “recording” plots\ncamcorder::gg_record(\n  dir = \"imgs\",\n  width = 12,\n  height = 12*9/16,\n  dpi = 300,\n  bg = \"white\"  # Makes sure background is actually white an not transparent\n)\n\nAll plots will immediately be exported as a .png-file to the directory specified\nAll plots will be displayed in the viewer with dimensions and resolution that you specified and not in the plots pane in RStudio\n300 dpi is pretty standard and default of ggsave\n\nDo work. Export final png file in directory when done and delete the rest\nRegarding Fonts\n\nIf using {ragg}, then all is fine.\nIf using {showtext}, then you have to set resolution in options, showtext_opts(dpi = 300)\n\n\n\n\nTwitter\n\nVideo: 1105 x 1920\n\nLine Charts\n\nMatters most if two different line charts are being compared\n\nThe core idea of “banking” is that the slopes in a line chart are most readable if they average to 45°.\nUse ggthemes::bank_slopes(x, y, method = c(\"ms\", \"as\"))\n\n2 methods (that req. no optimization) from Jeer, Maneesh who followed Cleveland’s 45° guideline\ndocs\n\n“The problem with banking is that sometimes you need the chart in a certain aspect ratio to fit into a page layout. Especially if banking produces portrait sized charts. But why not let the optimal chart ratio define your layout? For instance, you can put the additional information to the side of the chart. Remember that the main goal of banking is to increase the readability of the line slopes. In the following example, the slopes for Nuclear and Renewables would have been much more difficult to see, if the chart would have been ‘squeezed’ to a landscape aspect.” (article)",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-typo",
    "href": "qmd/visualization-general.html#sec-vis-gen-typo",
    "title": "General",
    "section": "Typography",
    "text": "Typography\n\nCSS Length Units\n\nAbsolute Lengths\n\n* Pixels (px) are relative to the viewing device. For low-dpi devices, 1px is one device pixel (dot) of the display. For printers and high resolution screens 1px implies multiple device pixels.\n\n\ncm\ncentimeters\n\n\nmm\nmillimeters\n\n\nin\ninches (1in = 96px = 2.54cm)\n\n\npx*\npixels (1px = 1/96th of 1in)\n\n\npt\npoints (1pt = 1/72 of 1in)\n\n\npc\npicas (1pc = 12 pt)\n\n\n\nRelative Lengths\n\nThe em and rem units are practical in creating perfectly scalable layout! * Viewport = the browser window size. If the viewport is 50cm wide, 1vw = 0.5cm.\n\n\n\n\n\n\nem\nRelative to the font-size of the element (2em means 2 times the size of the current font)\n\n\nex\nRelative to the x-height of the current font (rarely used)\n\n\nch\nRelative to the width of the “0” (zero)\n\n\nrem\nRelative to font-size of the root element\n\n\nvw\nRelative to 1% of the width of the viewport*\n\n\nvh\nRelative to 1% of the height of the viewport*\n\n\nvmin\nRelative to 1% of viewport’s* smaller dimension\n\n\nvmax\nRelative to 1% of viewport’s* larger dimension\n\n\n%\nRelative to the parent element\n\n\n\n\nFont Weight\n\n400 is the same as normal, and 700 is the same as bold\n\nFonts\n\nAdelle\n\nA serif font that doesn’t go overboard. Good for short paragraphs.\n\nAlegreya\nBarlow\n\nSlender font\n\nFira Code Retina\n\ncode syntax highlighting\n@import url(“https://cdn.rawgit.com/tonsky/FiraCode/1.205/distr/fira_code.css”);\n\nLora\n\nbody\nUsed in COVID-19 project &gt;&gt; Static Charts, Hospitals\n@import url(‘https://fonts.googleapis.com/css2?family=Lora&display=swap’);\n\nMerriweather\n\nSimilar to Adelle, but has a bit more pronounced hooks\n\nMontserrat\n\nSimple design that can handle long lines of text. I like it for minimal plots.\n\nPrata\n\nheader\nUsed in ericbook-distill\n@import url(‘https://fonts.googleapis.com/css2?family=Cinzel&display=swap’);\n\nReforma family\n\nonly one I have is Roboto, need to import and load the rest using extrafont pkg\n\nRoboto family\n\nDancho shiny apps\n\np, body: 100 wt\nHeaders, (h1, h2, etc.): 400 wt\n\nRoboto Slab\n\nNot sure if this is exact font used but it’s very similar. Only difference I spotted was the “3.”\n\n\nTitillium Web Bold\n\nheaders\nUsed in ebtools\n@import url(‘https://fonts.googleapis.com/css?family=Titillium+Web&display=swap’);\n\n\nNumbers\n\nshould all have the same height (Lining)\nshould all have the same width (Tabular)\n\nUsing {showtext}\nlibrary(showtext)\n#load font\nfont_add_google(name = \"Metal Mania\", family = \"metal\")\nfont_add_google(name = \"Montserrat\", family = \"montserrat\")\nshowtext_auto()",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-annot",
    "href": "qmd/visualization-general.html#sec-vis-gen-annot",
    "title": "General",
    "section": "Annotation",
    "text": "Annotation\n\nPeople love annotations (thread, paper). More text, the better.\n\nTheir takeaway from the chart is more likely to resemble the annotation if it takes the form of L2 and/or L4 and is close to the data\n\nExample: Financial Times\n\n\nTitle (L2) is used for part of the takaway message\n\nSubtitle used to describe the Y-Axis\n\nChart annotation paragraph (L4) gives contextual information\n\n\nWhen to annotate\n\na design element in your visualization that needs explaining\na data point or series that you want readers to see, like an outlier\nreaders should know something to better understand why certain data points look the way they do\n\nRemove the color key/legend and directly label your categories\n\nIf the screen is small (e.g. mobile), then it’s better to keep the legend\n\nMake it obvious which units your data uses.\n\nDon’t just put units in the description, but also in axis labels, tooltips, and annotations\n\nFor large numbers (e.g. 20 million), try to use B, M, K instead of an annotation somewhere that says something like “in thousand”\nTooltips\n\nConsider not just stating the numbers in tooltips, but also the category\n\ne.g. “3.4% unemployed” instead of “3.4%,” or “+16% revenue” instead of “+16%”\n\nUse a transparent background by setting the alpha channel of CSS background-colorto a number less than 1\n\ne.g. 0.3 using rgba(255, 255, 255, 0.3)\n\nWith a transparent background, text behind the tooltip can interfere with the text in the tooltip, so also apply backdrop-filter\n\nExample:\n.tooltip {\n  background-color: rgba(255, 255, 255, 0.3);\n  -webkit-backdrop-filter: blur(2px);\n  backdrop-filter: blur(2px);\n}\n@media (prefers-contrast: more) {\n  .tooltip {\n    background-color: white;\n    -webkit-backdrop-filter: none;\n    backdrop-filter: none;\n  }\n}\n\nExample shows a tooltip that has an HTML class of “tooltip”.\nblur is measured in pixels and the image size varies with screen width, so the optimal blur size here may vary for you depending on the dimensions of your browser window.\n\nApplies a Gaussian blur to the target element’s background with the standard deviation specified as the argument (e.g. two pixels).\n\nAs of Mar 2023, doesn’t work on Safari, so adding -webkit-backdrop-filter allows it to work on Safari\n@media (prefers-contrast: more)checks if your user has informed their operating system or browser that they prefer increased contrast. When they do, this chunk then overrides the applied styles.\n\n\n\nTransparent backgrounds might work better with thematic maps and less with scatter plots\nDon’t center-align your text\nUse straightforward phrasings\nMove axis labels nearest the most important chart objects (e.g. bars)\n\n\nIf the higher bars are what’s most important and they’re on the right, then usea right-side axis\n\nFonts for annotation\n\n\nUse what readers are most used to (e.g. sans-serif regular, &gt;12px, (almost) black text\nIf you need to need a lot of words and they don’t fit, don’t use smaller font, use a tooltip instead\n\nOn mobile screens you can also hide the least important annotations, or move them below the visualization\n\n\nLead the eye with font sizes, styles, and colors\n\n\nThe biggest and boldest text with the highest contrast against the background should be reserved for the most important information.\n\nDon’t overdo it though\n\nUse only two levels of hierarchy that are clearly different from each other — like a 12px gray and a 14px black\nEmphasize within the annotations using boldness\n\nKeep labels horizontal\n\n\nUse a text outline\n\n\nSet the stroke around your letters, using the background color of your chart.\n\nBe conversational first and precise later",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-color",
    "href": "qmd/visualization-general.html#sec-vis-gen-color",
    "title": "General",
    "section": "Color",
    "text": "Color\n\nMisc\n\nWhen choosing bg and fg colors, keep in mind that it’s generally a good idea to pick colors with a similar hue but a large difference in their luminance.\nDatawrapper guide\nWhen using several subplots together to tell a story and they each have their own color scheme. Blend a color into each color scheme to produce a more unified look\n\nExample: Blending blue into a plot with green color scheme.\n\n\nBreakpoints for scales\n\nHow to choose an interpolation for your color scale\n\nCharts (see prismatic PKG to do this manipulation within ggplot)\nPalette composition methods\n\nComplimentary\n\nopposite sides of the color wheel (2 colors)\ncontrast\n\nAnalogous\n\nsame side of the color wheel (multiple)\ngradient\n\nTriadic\n\nforms triangle on the color wheel\nvibrant, contrast\n\nOthers\n\nsplit complimentary (popular)\n\nComprised of one color and two colors symmetrically placed around it. This strategy adds more variety than complementary color schemes by including three hues without being too jarring or bold. Using this method, we end up with combinations that include warm and cool hues that are more easily balanced than the complementary color schemes\n\nquadratic\n\n\nAdjustments once you chosen a color (hue) to create variations\n\nMove brightness up for lighter variations and down for darker variations\nThen, move saturation in the opposite way you moved brightness\n\nSave colors you find attractive\n\ninstant eyedropper (windows)\nThen use HSL (hue, saturation, lightness) slider for adjustments\n\nBackgrounds\n\nWhite\n\nbright, used a lot\ntry ivory or a light gray\nshades of eggshell, link\n\nAvoid black (or REALLY dark) unless situation calls for it\n\ndark is fine\n\n\nLightest and darkest colors should have meaning (e.g. min, max, mean, zero) and not just some arbitrary numbers\n\nWhat to do when you have a lot of categories\n\nSimply don’t show different colors Does your chart work without colors?\n\n1 color and a discrete axis with the categories\n\nShow shades, not hues Can you make the chart less confetti-like?\n\nAlthough, consider not using shades when the parts are as or more important than the totals\n\nEmphasize Can you only use color for your most important categories?\nLabel directly Can you use the same or similar colors but label them?\nMerge categories Can you put categories together?\nGroup categories, but keep showing them Can strokes help to tell categories apart?\n\nChange the chart type Will another chart type rely less on colors?\n“Small multiply” it Can you split the categories into multiple charts? (i.e. facet by category)\nAdd other indicators Can you add symbols, patterns, line widths, or dashes?\n\n\nDoesn’t use any color — just opacity, thickness, and dotted lines.\n\nUse tooltips and hover effects Can smaller categories be hidden with them?\n\nColor scales should be chosen to best match the data values and plot type: If the goal is to show magnitude, a univariate color scheme is typically preferable, while a double-ended color scale is typically more effective when showing data that differ in sign and magnitude. Where possible, color scales should use a minimal number of hues, varying intensity or lightness of the color to show magnitude, and transitioning through neutral colors (white, light yellow) when utilizing a gradient. Cognitive load can also be reduced by selecting colors with cultural associations that match the data display, such as the use of blue for men and red (or pink) for women, or the use of blue for cold temperatures and red/orange for warm temperatures.\n\nIt is also important to consider the human perceptual system, which does not perceive hues uniformly: We can distinguish more shades of green than any other hue, and fewer shades of yellow, so green univariate color schemes will provide finer discriminability than other colors because the human perceptual system evolved to work in the natural world, where shades of green are plentiful.\n\n\nFigure above shows the International Commission on Illumination (CIE) 1931 color space, which maps the wavelength of a color to a physiologically based perceptual space; a significant portion of the color space is dedicated to greens and blues, while much smaller regions are dedicated to violet, red, orange, and yellow colors. This unevenness in mapping color is one reason that the multi-hued rainbow color scheme is suboptimal—the distance between points in a given color space may not be the same as the distance between points in perceptual space. As a result of the uneven mapping between color space and perceptual space, multi-hued color schemes are not recommended.",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-types",
    "href": "qmd/visualization-general.html#sec-vis-gen-types",
    "title": "General",
    "section": "Chart Types",
    "text": "Chart Types\n\nBar Graphs\n\nDon’t use bar graphs for anything except counts. Audiences have trouble with the abstraction.\nFor averages, used errorbar charts or use median + raincloud.\nGuide\n\n\nStacked Bar\n\nReplacement for pie charts et al when dealing with fractional data\nAlways reorder stacks\n\n\nRmd tutorial for reordering optimation\n\n\nBox Plots\n\n\nSmall data - emphasize the points\nLarge data - emphasize the box\n\nLine Charts\n\nSometimes it’s appropriate not to use zero as the baseline\nHaving the y-axis not intersect the x-axis can minimize the risk of confusing the readers with a non-zero baseline chart\nTime Series of ordinal discrete data by category\n\nordinal data has 3 levels\n\n\nHeatmaps\n\nReorder rows and columns to produce a more meaningful visualization\n\n\nGuide on reordering heatmaps\nIf order is important, then this may not be possible\n\nReorder by clustering\n\n\nNetwork Graphs\n\nAlways try different multiple layout methodologies",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-maps",
    "href": "qmd/visualization-general.html#sec-vis-gen-maps",
    "title": "General",
    "section": "Maps",
    "text": "Maps\n\nAbove rules also apply\nRemove as many extraneous elements as possible\n\nHard because maps have so many necessary elements\n\nBorders, Labels, etc.\n\nIn cloropleths, remove unnecessary borders (e.g. along coastlines)\n\n\n“Borders as lines” is much less cluttered\nArticle, rmapshaper::ms_innerlines() keeps only the necessary inner borders in the “geometry” column of the spatial dataset.\n\n\nPay close attention to typography hierarchy\n\nBold, Font size, etc\n\nUse iconography to help users identify what you want them to see\nNumeric values (thread)\n\nPalettes: use a sequential (top row) or diverging (bottom row)\n\n\nFor diverging palettes\n\n\nThe middle value should be light on a light background (top left) or dark on a dark background (bottom left)\n\n\nBackgrounds:\n\n\nLight background: darker color on the value of interest (usually the higher value) (top left)\nDark background: lighter color on the value of interest (usually the higher value) (bottom left)\n\n\nTry not to use Rainbow palettes, because they are misleading\n\nThe rainbow and jet colors are problematic as the change in color is not perceptually uniform, leading to distinct ‘bands’ of certain colors. This causes misleading jumps and emphasizes certain values, most likely without the intention to highlight them. (Cedric Scherer)\n(acceptable) rainbow called “Turbo” if you need one (article)\n\nCode - see comments for links to R scripts and improved versions of Turbo\n\nOther Alternatives",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-area",
    "href": "qmd/visualization-general.html#sec-vis-gen-area",
    "title": "General",
    "section": "Area",
    "text": "Area\n\nIn general, these charts aren’t good for noisy data and data with many categories\n\nHave issues when values increase sharply (see video. around 50:13)\n\nExperiment with the order of the groups\n\nEvents that you’re looking for are probably only visable when there’s a particular order\nMost of the time, putting the most stable groups at the bottom produces the best results",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-ts",
    "href": "qmd/visualization-general.html#sec-vis-gen-ts",
    "title": "General",
    "section": "Time Series",
    "text": "Time Series\n\nHorizon Charts\n\nSee Anomaly Detection &gt;&gt; Charts\nEspecially useful for showing data with large amplitudes in a short vertical space",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-uncert",
    "href": "qmd/visualization-general.html#sec-vis-gen-uncert",
    "title": "General",
    "section": "Uncertainty",
    "text": "Uncertainty\n\nVisualizing only inferential uncertainty can lead to significant overestimates of treatment effects\n\n\nWhen possible, plot individual data points alongside statistical estimates\n\nTranslate percentages into counts (e.g. “a 1 out of 5 chance” rather than “a 20% chance”)\n\n{riskyr} - icon arrays and less sophisticated viz for the above chart\nicon arrays\n\nExamples\n\nbase rates and error rates (paper)\nrelative risks (paper)\n\n\nWaffle plots are similar to icon arrays\n\nquantile dotplots\n\n{ggdist} (many examples and flavors)\n\nhypothetical outcome plots\n\nConsists of multiple individual plots (frames), each of which depicts one draw from a distribution (use case for animation)\nBest suited for multivariate judgments like how reliable a perceived difference between two random variables is\nIllustration of the process\n\nYou create a distribution to sample from or using known distribution and parameters or bootstrapping the sample and sample from each bootstrap.\nEach sample/draw is presented on the right side of the distribution plot (fig 1) (final product)\n\nI think it would be better if after each draw the previous draw remained but was de-emphasized (i.e. turned light gray)\nAnother example would McElreath’s lecture video on posterior prediction distribution.\n\nFigs 2 and 3 show a sequence of draws from a joint distribution of uncorrelated variables (fig 2) and correlated variables (fig 3)\n\nExample: NYT on interpreting jobs reports\n\n2 facets: accelerating job growth (left), steady job growth (right)\nFor each facet,\n\nthe left plot is static, and the right plot is animated showing different noisy samples of the same underlying dgp\nthe left plot shows what normals perceive the distribution to look like for the given interpretation (e.g. accelerating job growth), and the right plot shows what real (i.e. noisy) data with the same interpretion looks like.\n\n\n\n\nFan charts\n\n\nshows a 90% interval broken divided into 30% increments (left) or 10% increments (right)\n\nShow previous forecasts\n\n\nTruth is in dark blue with light blue branches showing previous forecasts",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-mob",
    "href": "qmd/visualization-general.html#sec-vis-gen-mob",
    "title": "General",
    "section": "Mobile",
    "text": "Mobile\n\nMisc\n\nRStudio plots are displayed in 96 dpi and ggsave uses 300 dpi as default\n\ni.e. viewed plots won’t look the same as the saved plots using default settings\n\n\nUse sharp color contrasts when highlighting\nMinimal readable size is 16, but 22 is recommended\nAspect ratio of 4:3 or 1024 x 768 pixels\n\nAnother article say 1:2\n\nBar Charts should be horizontal to make charts with many categories readable\n\nMobile screens are more tall than wide so labels on the y-axis makes more sense than on the x-axis\n\nR\n\nSet-up external window with aspect ratio (e.g. 1:2)\ndev.new(width=1080, height=2160, unit=\"px\", noRStudioGD = TRUE)\n\nnoRStudioGD = TRUE says any new plots appear in the new graphics window rather than the RStudio graphics device\nCan also use windows(), x11(), or png() from {ragg}\n\n\nUse Quarto (or Rmd) for developement\n#| dpi: 300     \n#| fig.height: 7.2     \n#| fig.width: 3.6     \n#| dev: \"png\"     \n#| echo: false     \n#| warning: false     \n#| message: false`\n\nThis way your dpi and aspect ratio are set and you can view the final output without having to save the png and viewing it separately to see how it looks\nfig.height and fig.width are always given in inches\n\nIf you haven’t set your Quarto document to be self-contained, then the images have also already been saved for you - probably in a folder called documentname_files/figure-html/",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html",
    "href": "qmd/quarto-rmarkdown.html",
    "title": "Quarto",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-misc",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-misc",
    "title": "Quarto",
    "section": "",
    "text": "Resources\n\nDocs\nReference\nTroubleshooting\n\nquarto --version - Must be in RStudio Terminal\nquarto check - Must be in RStudio Terminal - versions and engine checks\n$ quarto check\n[&gt;] Checking versions of quarto binary dependencies...\n      Pandoc version 3.1.1: OK\n      Dart Sass version 1.55.0: OK\n[&gt;] Checking versions of quarto dependencies......OK\n[&gt;] Checking Quarto installation......OK\n      Version: 1.3.340\n      Path: C:\\Users\\tbats\\AppData\\Local\\Programs\\Quarto\\bin\n      CodePage: 1252\n[&gt;] Checking basic markdown render....OK\n[&gt;] Checking Python 3 installation....OK\n      Version: 3.8.1 (Conda)\n      Path: C:/Users/tbats/Miniconda3/python.exe\n      Jupyter: 4.9.1\n      Kernels: python3\n(\\) Checking Jupyter engine render....2023-04-28 10:18:15,018 - traitlets - WARNING - Kernel\nProvisioning: The 'local-provisioner' is not found.  This is likely due to the presence of multiple jupyter_client distributions and a        previous distribution is being used as the source for entrypoints - which does not include 'local-provisioner'.  That distribution should     be removed such that only the version-appropriate distribution remains (version &gt;= 7).  Until then, a 'local-provisioner' entrypoint will     be automatically constructed and used.\nThe candidate distribution locations are: ['C:\\\\Users\\\\tbats\\\\Miniconda3\\\\lib\\\\site-packages\\\\jupyter_client-5.3.4.dist-info',                'C:\\\\Users\\\\tbats\\\\Miniconda3\\\\lib\\\\site-packages\\\\jupyter_client-7.0.6.dist-info']\n[&gt;] Checking Jupyter engine render....OK\n[&gt;] Checking R installation...........OK\n      Version: 4.2.3\n      Path: C:/PROGRA~1/R/R-42~1.3\n      LibPaths:\n        - C:/Users/tbats/AppData/Local/R/win-library/4.2\n        - C:/Program Files/R/R-4.2.3/library\n      knitr: 1.42\n      rmarkdown: 2.20\n[&gt;] Checking Knitr engine render......OK\nUsing a development verison of Quarto\n\nFirst Usage\n\nChange directories to where you want to store the dev version\nClone repo and change to the cloned directory\ngit clone https://github.com/quarto-dev/quarto-cli\ncd quarto-cli\nDisable Anti-Virus\nRun Configuration Script\n\nWindows Command Prompt\ncmd /k configure.cmd\n\n\\k keeps the window open in case it errors\n\nPowershell\nInvoke-Item configure.cmd\nLinux/MacOS\n./configure.sh\nThis will take a minute or two as it checks versions, installs dependencies like pandoc, etc.\n\nAdd path to quarto.cmd to PATH\n\nAfter the configuration file runs, it will output the path you need to put on PATH, e.g. \"C:\\Users\\erc\\Documents\\Quarto\\quarto-cli\\package\\dist\\bin\"\n\nEnable Anti-Virus\nShould be able to use in RStudio\n\nI was not able to use the RStudio terminal for quarto commands (e.g. quarto check) though.\nTo find the version, I just opened powershell and ran quarto –version just to make sure it was running and on PATH.\n\nNot sure if they use this every time but it was 99.9.9 instead of the verion in the changelog.\n\nI also rendered a qmd file using quarto-cmd from the root directory of quarto-cli to see if it matched the output from RStudio. (cd qmd then quarto preview forecasting-statistical.qmd --to html --no-watch-inputs --no-browse)\n\n\nSubsequent Development Versions\n\nChange directory to quarto-cli and git pull\n\n\nShortcuts\n\nNew R chunk: ctrl + alt + i\nBuild whole book: ctrl+shift b\nRender page and preview book: ctrl+shift k\n\nUsing yaml style for chunk options\n\nConvert Rmd chunk options to Quarto: knitr::convert_chunk_header(\"doc.rmd\", \"doc.qmd\")\nAnchor Link - A link, which allows the users to flow through a website page. It helps to scroll and skim-read easily. A named anchor can be used to link to a different part of the same page (like quickly navigating) or to a specific section of another page.\n\nThis is the “#sec-moose” id that can be added to headers which it allows to be referenced within the document or in other documents.\n\nMathJax commands\n\nFont Size: \\tiny{ }, \\scriptsize{ }, \\small{ }, \\normal{ }, \\large{ }, \\Large{ }, \\LARGE{ }, \\huge{ }, \\Huge{ }\n\nLightbox\n\nDocs\nGrouping images for lightbox carousel: ![A Lovely Image](mv-1.jpg){group=\"my-gallery\"}",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-quarto",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-quarto",
    "title": "Quarto",
    "section": "Syntax",
    "text": "Syntax\n\nAlign code chunk under bullet and add indented comment below chunk\n-   [Example]{.ribbon-highlight} (using a SQL Query; method 1)\n\n    ``` r\n    # open dataset\n    ds &lt;- arrow::open_dataset(dir_out, partitioning = \"species\")\n    # open connection to DuckDB\n    con &lt;- dbConnect(duckdb::duckdb())\n    # register the dataset as a DuckDB table, and give it a name\n    duckdb::duckdb_register_arrow(con, \"my_table\", ds)\n    # query\n    dbGetQuery(con, \"\n      SELECT sepal_length, COUNT(*) AS n\n      FROM my_table\n      WHERE species = 'species=setosa'\n      GROUP BY sepal_length\n    \")\n\n    # clean up\n    duckdb_unregister(con, \"my_table\")\n    dbDisconnect(con)\n    ```\n\n    -   filtering using a partition, the WHERE format is '\\&lt;partition_variable\\&gt;=\\&lt;partition_value\\&gt;'\n\nSpace between bullet and top ticks\nSpace between bottom ticks and bullet\nNote alignment of text\n\nAdd Code Annotations\n-   [Partition a large file and write to arrow format]{.underline}\n\n    ``` r\n    lrg_file &lt;- open_dataset(&lt;file_path&gt;, format = \"csv\") # &lt;1&gt;\n    lrg_file %&gt;%\n        group_by(var) %&gt;% # &lt;2&gt;\n        write_dataset(&lt;output_dir&gt;, format = \"feather\") # &lt;3&gt;\n    ```\n\n    1.  Pass the file path to `open_dataset()`\n\n    2.  Use `group_by()` to partition the Dataset into manageable chunks\n\n    3.  Use `write_dataset()` to write each chunk to a separate Parquet file---all without needing to read the full CSV file into R\n\n    -   `open_dataset` is fast because it only reads the metadata of the file system to determine how it can construct queries",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#chunk-options-and-yaml",
    "href": "qmd/quarto-rmarkdown.html#chunk-options-and-yaml",
    "title": "Quarto",
    "section": "Chunk Options and YAML",
    "text": "Chunk Options and YAML\n\nSet global chunk options in yaml\n\nEnable Margin Notes\n---\n# YAML front matter\nreference-location: margin\n---\n!expr to render code within chunk options\n\ne.g. figure caption: #| fig-cap: !expr glue::glue(\"The mean temperature was {mean(airquality$Temp) |&gt; round()}\")\n\nConditional Code Chunk Evaluation\n\nExample: document output type\n\nSet value in a code chunk\n```{r setup}\n# Include in first chunk of .qmd\n# Get output file type\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\n```\nUse !expr sytax to determine evaluation status\n\nExample: eval chunk based on output type\n```{r}\n#| eval: !expr out_type == \"html\"\n\n# code to create interactive {plotly}\n```\n\n```{r}\n#| eval: !expr out_type == \"docx\"\n\n# code to create static {ggplot2}\n```\n\n\nExample: Use parameterization to set value\n---\ntitle: \"test\"\nformat: html\nparams:\n  my_value: false\n---\n\nmy_value can then be used throughout the document to determine chunk evaluation status\n\n\ncolumn: screen-inset yaml markup is used to show a very wide table\nCLI\n\nquarto render to compile a document\nquarto preview to render a live preview that automatically updates when the source files are saved\n\nGraphics\n\nCode Chunk\n#| dpi: 300\n#| fig.height: 7.2\n#| fig.width: 3.6\n#| dev: \"png\"\n#| echo: false\n#| warning: false\n#| message: false\n\nExample shows settings for a graph for mobile\nfig.height and fig.width are always given in inches\n\n\nIf you haven’t set your Quarto document to be self-contained, then the images have also already been saved for you - probably in a folder called documentname_files/figure-html/\nformat: \n  html:\n    embed-resources: true\nYAML Example\n\nNested Tabs\n\nKnitr Hooks\n\nNotes from Writing knitr hooks\n\nAlso has a knitr hook example that alters cell output (e.g. only prints 4 lines of a vector)\n\nChunk Hooks\n\nChunk hooks get called twice: once before knitr executes the code in the chunk, and once again afterwards\nThe function can take up to four arguments, all of which are optional:\n\nbefore: A logical value indicating whether the function is being called before or after the code chunk is executed\noptions: The list of chunk options\nenvir: The environment in which the code chunk is executed\nname: The name of the code chunk option that triggered the hook function\n\nThe chunk hook is called for its side effects not the return value. However, if it returns a character output, knitr will add that output to the document output as-is.\nExample: Chunk Timer\n\nCode\ncreate_timer_hook &lt;- function() {\n  start_time &lt;- NULL\n  function(before, options) {\n    if (before) {\n      start_time &lt;&lt;- Sys.time()\n    } else {\n      stop_time &lt;- Sys.time()\n      elapsed &lt;- difftime(stop_time, start_time, units = \"secs\")\n      paste(\n        \"&lt;div style='font-size: 70%; text-align: right'&gt;\",\n        \"Elapsed time:\", \n        round(elapsed, 2), \n        \"secs\",\n        \"&lt;/div&gt;\"\n      )\n    }\n  }\n}\nknitr::knit_hooks$set(timer = create_timer_hook())\n\nThe hook is triggered the first time (with before = TRUE) to record the system time somewhere (e.g., in a variable called start_time). Then, when the hook is triggered the second time (with before = FALSE), it records the system time again (e.g., as stop_time), and computes the difference in time.\n\nUse in a cell\n```{r}\n#| timer: true\nrunif(10000)\n```\nOutput",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#r-and-python",
    "href": "qmd/quarto-rmarkdown.html#r-and-python",
    "title": "Quarto",
    "section": "R and Python",
    "text": "R and Python\n\nIf only R or R and Python, the notebook is rendered by {knitr}\nIf only Python, the notebook is rendered by jupyter\nSet-up\n\n{reticulate} automatically comes loaded in Quarto and it knows to use it when it sees a python block, so you don’t need to load the package\nQuarto will select a version of Python using the Python Launcher on Windows or system PATH on MacOS and Linux. You can override the version of Python used by Quarto by setting the QUARTO_PYTHON environment variable.\n\nIn CLI on Windows, type py is see which version the Python Launcher , and therefore Quarto, is using and py –list to see which versions are installed.\n\n\nR\n```{r}\n#| label: read-data\n#| echo: true\n#| message: false\n#| cache: true\nlemurs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-08-24/lemur_data.csv')\n```\nPython\n```{python}\n#| label: modelling \n#| echo: true \n#| message: false\n\nlemur_data_py = r.lemur_data \nimport statsmodels.api as sm \ny = lemur_data_py[[\"Weight\"]] \nx = lemur_data_py[[\"Age\"]] \nx = sm.add_constant(x) \nmod = sm.OLS(y, x).fit() \nlemur_data_py[\"Predicted\"] = mod.predict(x) \nlemur_data_py[\"Residuals\"] = mod.resid`\n```\n\nUse r. to access the data in the R chunk\nThe first execution of a python cell starts reticulate::repl_python() in the terminal\n\n(back to) R\n```{r}\n#| label: plotting \n#| echo: true \n#| output-location: slide \n#| message: false \n#| fig-align: center \n#| fig-alt: \"Scatter plot of predicted and residual values for the fitted linear model.\" \n\nlibrary(reticulate) \nlibrary(ggplot2) \nlemur_residuals &lt;- py$lemur_data_py \nggplot(data = lemur_residuals, aes(x = Predicted, y = Residuals)) +\n  geom_point(colour = \"#2F4F4F\") +\n  geom_hline(yintercept = 0,\n            colour = \"red\") +\n  theme(panel.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"),\n        plot.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"))\n```\n\nUse py$ to access the data in the Python chunk *\nMust call library(reticulate) in order for Quarto to recognize py$",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#layouts",
    "href": "qmd/quarto-rmarkdown.html#layouts",
    "title": "Quarto",
    "section": "Layouts",
    "text": "Layouts\n\n2 cols (1 col: text, 1 col: image)\n\n::: {layout=\"[50,50]\"}\n\n::: column\nEvery Quarto project starts with a Quarto file that has the extension `.qmd`.\n\n\nThis particular one analyzes children's early words, but every `.qmd` includes the same three basic elements inside:\n\n\n- A block of metadata at the top, between two fences of `---`s. This is written in [YAML](https://learnxinyminutes.com/docs/yaml/). \n- Narrative text, written in [Markdown](https://commonmark.org/help/tutorial/). \n- Code chunks in gray between two fences of ```` ``` ````, written with R or another programming language.\n\n\nYou can use all three elements to develop your code and ideas in one reproducible document.\n:::\n\n![](img/01-source.png)\n:::\n2 figures, 2 columns (i.e. side-by-side) with captions at the top\n---\nfig-cap-location: top\n---\n\n-   Words\n    -   Predictions of Standard RF vs Oblique RF\n\n        ::: {layout-ncol=\"2\"}\n        ![Standard Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-axpred-1.png){fig-align=\"left\" width=\"432\"}\n\n        ![Oblique Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-oblpred-1.png){fig-align=\"left\" width=\"432\"}\n        :::\n\n        -   Words  \n\nfig-cap-location: bottom is default;\nfig-cap-location: margin is buggy, at least in for project type book. Captions are added to the margins but bullet points mysteriously disappear during rendering to html\n\n2 charts side-by-side extending past body margins\n```{r}\n#| label: my-figure\n#| layout-ncol: 2\n#| column: page\nggplot() + ...\nggplot() + ...\n```\n\n“layout-ncol” says 2 side-by-side columns\n“column: page” says extend column width to the width of the page",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#webr",
    "href": "qmd/quarto-rmarkdown.html#webr",
    "title": "Quarto",
    "section": "WebR",
    "text": "WebR\n\nSet-Up\n\nInstall the extension alongside your blog post by running quarto add coatless/quarto-webr\nAdd the extension to your blog by adding filters: [\"webr\"] to your post’s frontmatter\nInstead of {r} code chunks, use {webr-r} ones\n\nInstall CRAN packages on page load\nfilters:\n  - \"webr\"\nwebr:\n  packages:\n  - \"dplyr\"\n  - \"tidyr\"\n  - \"purrr\"\n  - \"tibble\"\n  - \"crayon\"\n\nAdd to frontmatter\n\nInstall R-Universe Package\n```{webr-r}\n#| context: setup\nwebr::install(\"collateral\", repos = c(\"https://jimjam-slam.r-universe.dev\"))\n```\n\nR-Universe packages must be installed in code cells",
    "crumbs": [
      "Quarto"
    ]
  }
]