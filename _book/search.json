[
  {
    "objectID": "qmd/surveys-census-data.html",
    "href": "qmd/surveys-census-data.html",
    "title": "Census Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "title": "Census Data",
    "section": "",
    "text": "Notes from\n\nTidycensus Workshop 2024\n\nFIPS GEOID\n\npopular variable calculations from variables in ACS\nCensus Geocoder (link)\n\nEnter an address and codes for various geographies are returned\nBatch geocoding available for up to 10K records\n\nCodes for geographies returned in a .csv file\n\n\nTIGERweb (link)\n\nAllows you to get geography codes by searching for an area on a map\nOnce zoomed-in on your desired area, you turn on geography layers to find the geography code for your area.\n\nUS Census Regions",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "title": "Census Data",
    "section": "Geographies",
    "text": "Geographies\n\n\nMisc\n\n{tidycensus} docs on various geographies, function arguments, and which surveys (ACS, Census) they‚Äôre available in.\nACS Geography Boundaries by Year (link)\n\nTypes\n\nLegal/Administrative\n\nCensus gets boundaries from outside party (state, county, city, etc.)\ne.g.¬†election areas, school districts, counties, county subdivisions\n\nStatistical\n\nCensus creates these boundaries\ne.g.¬†regions, census tracts, ZCTAs, block groups, MSAs, urban areas\n\n\nNested Areas\n\n\nCensus Tracts\n\nAreas within a county\nAround 1200 to 8000 people\nSmall towns, rural areas, neighborhoods\n** Census tracts may cross city boundaries **\n\nBlock Groups\n\nAreas within a census tract\nAround 600 to 3000 people\n\nCensus Blocks\n\nAreas within a block group\nNot for ACS, only for the 10-yr census\n\n\nPlaces\n\nMisc\n\nOne place cannot overlap another place\nExpand and contract as population or commercial activity increases or decreases\nMust represent an organized settlement of people living in close proximity.\n\nIncorporated Places\n\ncities, towns, villages\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\nCensus Designated Places (CDPs)\n\nAreas that can‚Äôt become Incorporated Places because of state or city regulations\nConcentrations of population, housing, commericial structures\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\n\nCounty Subdivisions\n\nMinor Civil Divisions (MCDs)\n\nLegally defined by the state or county, stable entity. May have elected government\ne.g.¬†townships, charter townships, or districts\n\nCensus County Divisions (CCDs)\n\nno population requirment\nSubcounty units with stable boundaries and recognizable names\n\n\nZip Code Tabulation Areas (ZCTAs)\n\n\nMisc\n\n{crosswalkZCTA} - Contains the US Census Bureau‚Äôs 2020 ZCTA to County Relationship File, as well as convenience functions to translate between States, Counties and ZIP Code Tabulation Areas (ZCTAs)\n\nApproximate USPS Code distribution for housing units\n\nThe most frequently occurring zip code within an census block is assigned to a census block\nThen blocks are aggregated into areas (ZCTAs)\n\nZCTAs do NOT nest within any other geographies\n\nI guess the aggregated ZCTA blocks can overlap block groups\n\n2010 ZCTAs exclude large bodies of water and unpopulated areas",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "title": "Census Data",
    "section": "American Community Survey (ACS)",
    "text": "American Community Survey (ACS)\n\nAbout\n\nYearly estimates based on samples of the population over a 5yr period\n\nTherefore a Margin of Error (MoE) is included with the estimates.\n\nDetailed social, economic, housing, and demographic characteristics\ncensus.gov/acs\n\nACS Release Schedule (releases)\n\nSeptember - 1-Year Estimates (from previous year‚Äôs collection)\n\nEstimates for areas with populations of &gt;65K\n\nOctober - 1-Year Supplemental Estimates\n\nEstimates for areas with populations between 20K-64999\n\nDecember - 5-Year Estimates\n\nEstimates for areas including census tract and block groups\n\n\nData Collected\n\nPopulation\n\nSocial\n\nAncestry, Citizenship, Citizen Voting Age¬† Population, Disability, Education Attainment, Fertility, Grandparents, Language, Marital Status, Migration, School Enrollment, Veterans\n\nDemographic\n\nAge, Hispanic Origin, Race, Relationship, Sex\n\nEconomic\n\nClass of worker, Commuting, Employment Status, Food Stamps (SNAP), Health Insurance, Hours/Week, Weeks/Year, Income, Industry & Occupation\n\n\nHousing\n\nComputer & Internet Use, Costs (Mortgage, Taxes, Insurance), Heating Fuel, Home Value, Occupancy, Plumbing/Kitchen Facilities, Structure, Tenure (Own/Rent), Utilities, Vehicles, Year Built/Year Movied In",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "title": "Census Data",
    "section": "Dicennial US Census",
    "text": "Dicennial US Census\n\nMisc\n\nA complete count ‚Äî not based on samples like the ACS\nApplies differential privacy to preserve respondent confidentiality\n\nAdds noise to data. Greater effect at lower levels (i.e.¬†block level)\nThe exception is that is no differetial privacy for household-level data.\n\n\n\n\nPL94-171\n\nPopulation data which the government needs for redistricting\nsumfile = ‚Äúpl‚Äù\nState Populations\npop20 &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"P1_001N\",\n    year = 2020\n  )\n\nFor 2020, default is sumfile = ‚Äúpl‚Äù\n\n\n\n\nDHC\n\nAge, Sex, Race, Ethnicity, and Housing Tenure (most popular dataset)\nsumfile = ‚Äúdhc‚Äù\nCounty\ntx_population &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\nCensus Block (analogous to a city block)\nmatagorda_blocks &lt;- \n  get_decennial(\n    geography = \"block\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    county = \"Matagorda\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\n\n\n\nDemographic Profile\n\nPretabulated percentages from dhc\nsumfile = ‚Äúdp‚Äù\n\nTabulations for 118th Congress and Island Areas (i.e.¬†Congressional Districts)\n\nsumfile = ‚Äúcd118‚Äù\n\n\nC suffix variables are counts while P suffix variables are percentages\n\n0.4 is 0.4% not 40%\n\nExample: Same-sex married and partnered in California by County\nca_samesex &lt;- \n  get_decennial(\n    geography = \"county\",\n    state = \"CA\",\n    variables = c(married = \"DP1_0116P\",\n                  partnered = \"DP1_0118P\"),\n    year = 2020,\n    sumfile = \"dp\",\n    output = \"wide\"\n  )\n\n\n\nDetailed DHC-A\n\nDetailed demographic data; Thousands of racial and ethnic groups; Tabulation by sex and age.\nDifferent groups are in different tables, so specific groups can be hard to locate.\nAdaptive design means the demographic group (i.e.¬†variable) will only be available in certain areas. For privacy, data gets supressed when the area has low population.\n\nThere‚Äôs typically considerable sparsity especially when going down census tract\n\nArgs\n\nsumfile = ‚Äúddhca‚Äù\npop_group - Population group code (See get_pop_groups below)\n\n‚Äúall‚Äù for all groups\npop_group_label = TRUE - Adds group labels\n\n\nget_pop_groups(2020, \"ddhca\") - Gets group codes for ethnic groups\n\nFor various groups there could be at least two variables (e..g Somaili, Somali and any combination)\nFor time series analysis, analagous groups to 2020‚Äôs for 2000 is SF2/SF4 and for 2010 is SF2. (SF stands for Summary File)\n\ncheck_ddhca_groups - Checks which variables are available for a specific group\n\nExample: Somali\ncheck_ddhca_groups(\n  geography = \"county\", \n  pop_group = \"1325\", \n  state = \"MN\", \n  county = \"Hennepin\"\n)\n\nExample: Minnesota group populations\nload_variables(2020, \"ddhca\") %&gt;% \n  View()\nmn_population_groups &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"all\", # for all groups\n    pop_group_label = TRUE\n  )\n\nIncludes aggregate categories like European Alone, Other White Alone, etc., so you can‚Äôt just aggregate the value column to get the total population in Minnesota.\n\nSo, in order to calculate ethnic group ratios of the total state or county, etc. population, you need to get those state/county totals from other tables (e.g.¬†PL94-171)\n\n\nUse dot density and not chloropleths to visualize these sparse datasets\n\nExample: Somali populations by census tract in Minneapolis\n\nhennepin_somali &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    county = \"Hennepin\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"1325\", # somali\n    pop_group_label = TRUE,\n    geometry = TRUE\n  )\n\nsomali_dots &lt;- \n  as_dot_density(\n    hennepin_somali,\n    value = \"value\", # column name which is by default, \"value\"\n    values_per_dot = 25\n  )\n\nmapview(somali_dots, \n        cex = 0.01, \n        layer.name = \"Somali population&lt;br&gt;1 dot = 25 people\",\n        col.regions = \"navy\", \n        color = \"navy\")\n\nvalues_per_dot = 25 says make each dot worth 25 units (e.g.¬†people or housing units)\n\n\n\n\n\nTime Series Analysis\n\n{tidycensus} only has 2010 and 2020 censuses\n\nSee https://nhgis.org for older census data\n\nIssue: county names and boundaries change over time (e.g.¬†Alaska redraws a lot)\n\nCensus gives a different GeoID to counties that get renamed even though they‚Äôre the same county.\nNA values showing up after you calculate how the value changes over time is a good indication of this problem. Check for NAs: filter(county_change, is.na(value10))\n\nExample: Join 2010 and 2020 and Calculate Percent Change\ncounty_pop_10 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P001001\", \n    year = 2010,\n    sumfile = \"sf1\"\n  )\n\ncounty_pop_10_clean &lt;- \n  county_pop_10 %&gt;%\n    select(GEOID, value10 = value) \n\ncounty_pop_20 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    year = 2020,\n    sumfile = \"dhc\"\n  ) %&gt;%\n    select(GEOID, NAME, value20 = value)\n\ncounty_joined &lt;- \n  county_pop_20 %&gt;%\n    left_join(county_pop_10_clean, by = \"GEOID\") \n\ncounty_joined\n\ncounty_change &lt;- \n  county_joined %&gt;%\n    mutate( \n      total_change = value20 - value10, \n      percent_change = 100 * (total_change / value10) \n    ) \nExample: Age distribution over time in Michigan\n\n\nCode available in the github repo or R/Workshops/tidycensus-umich-workshop-2024-main/census-2020/bonus-chart.R\nDistribution shape remains pretty much the same, but decreasing for most age cohorts, i.e.¬†people are leaving the state across most age groups.\n\ne.g.¬†The large hump representing the group of people in there mid-40s in 2000 steadily decreases over time.",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "title": "Census Data",
    "section": "tidycensus",
    "text": "tidycensus\n\nGet an API key\n\nRequest a key, then activate the key from the link in your email.(https://api.census.gov/data/key_signup.html)\nSet as an environment variable: census_api_key(\"&lt;api key&gt;\", install = TRUE)\n\nOr add this line to .Renviron file, CENSUS_API_KEY=‚Äò&lt;api key‚Äô\n\n\nSearch Variables\n\nColumns\n\nName - ID of the variable (Use this in the survey functions)\nLabel - Detailed description of the variable\nContext - Subject of the table that the variable is located in.\n\nPrefixes (Variables can have combinations of prefixes)\n\nP: i.e.¬†Person; Data available at the census block and larger\nCT: Data available at the census track and larger\nH: Data available at the Housing Unit level\n\nI think housing unit is an alternatve unit. So instead of the unit being a person, which I assume is the typical unit, it‚Äôs a housing unit (~family).\nNot affected by Differential Privacy (i.e.¬†no noise added; true value)\nExample: Total Deleware housing units at census block level\ndp_households &lt;- \n      get_decennial(\n            geography = \"block\",\n            variables = \"H1_001N\",\n            state = \"DE\",\n            sumfile = \"dhc\",\n            year = 2020\n      )\n\n\nExample: DHC data in census for 2020\n\nvars &lt;- load_variables(2020, \"dhc\")\n\nView(vars)\n\nView table, click filter, and then search for parameters (e.g.¬†Age, Median, etc.) with the Label, Context boxes, and overall search box\n\n\nsummary_var - Argument for supplying an additional variable that you need to calculate some kind of summary statistic\n\nExample: Race Percentage per Congressional District\n\nrace_vars &lt;- c(\n  Hispanic = \"P5_010N\", # all races identified as hispanic\n  White = \"P5_003N\", # white not hispanic\n  Black = \"P5_004N\", # black not hispanic\n  Native = \"P5_005N\", # native american not hispanic\n  Asian = \"P5_006N\", # asian not hispanic\n  HIPI = \"P5_007N\" # hawaiian, islander not hispanic\n)\n\ncd_race &lt;- \n  get_decennial(\n    geography = \"congressional district\",\n    variables = race_vars,\n    summary_var = \"P5_001N\", # total population for county\n    year = 2020,\n    sumfile = \"cd118\"\n)\n\ncd_race_percent &lt;- \n  cd_race %&gt;%\n    mutate(percent = 100 * (value / summary_value)) %&gt;% \n    select(NAME, variable, percent)\n\ngeometry = TRUE- Joins shapefile with data and returns a SF (Simple Features) dataframe for mapping\n\nMisc\n\nYou can create a discrete color palette with the at argument in the mapview function.\n\nExample\n# check min and max of your data to select range of bins\nmin(iowa_over_65, na.rm = TRUE) # 0\nmax(iowa_over_65, na.rm = TRUE) # 38.4\n\nm1 &lt;- \n  mapview(iowa_over_65, \n          zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1),\n          at = c(0, 10, 20, 30, 40))\n\nThis will result in a discrete palette with bins of 0-10, 10-20, etc. Looks like an overlap, so I‚Äôm sure which bin contains the endpoints.\n\n\n\nExample: Over 65 in Iowa by census tract\n\nlibrary(mapviw); library(viridisLite)\n\niowa_over_65 &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"DP1_0024P\",\n    state = \"IA\",\n    geometry = TRUE,\n    sumfile = \"dp\",\n    year = 2020\n  )\nm1 &lt;- \n  mapview(iowa_over_65, zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1))\nExport as an HTML file\nhtmlwidgets::saveWidget(m1@map, \"iowa_over_65.html\")\n\nCan embed it elsewhere (html report or website) by adding it as an asset",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html",
    "href": "qmd/model-building-concepts.html",
    "title": "Concepts",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html#sec-modbld-misc",
    "href": "qmd/model-building-concepts.html#sec-modbld-misc",
    "title": "Concepts",
    "section": "",
    "text": "Packages\n\n{multiverse} - makes it easy to specify and execute all combinations of reasonable analyses of a dataset\n\n\n\nPaper, Summary of it‚Äôs usage\nLots of vignettes\n\n\nRegression Workflow (Paper)\n\nMake ML model pipelines reusable and reproducible\n\n\nNotes from 7 Tips to Future-Proof Machine Learning Projects\nModularization - Useful for debugging and iteration\n\nDon‚Äôt used declarative programming. Create functions/classes for preprocessing, training, tuning, etc., and keep in separate files. You‚Äôll call these functions in the main script\n\nHelper function\n## file preprocessing.py ##\ndef data_preparation(data):\n    data = data.drop(['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am'], axis=1)\n    numeric_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']\n    data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)\n    return data\nMain script\nfrom preprocessing import data_preparation \ntrain_preprocessed = data_preparation(train_data)\ninference_preprocessed = data_preparation(inference_data)\n\nKeep parameters in a separate config file\n\nConfig file\n## parameters.py ##\nDROP_COLS = ['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am']\nNUM_COLS = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']\nProprocessing script\n## preprocessing.py ##\nfrom parameters import DROP_COLS, NUM_COLS\ndef data_preparation(data):\n    data = data.drop(DROP_COLS, axis=1)\n    data[NUM_COLS] = data[NUM_COLS].fillna(data[NUM_COLS].mean())\n    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)\n    return data\n\n\nVersioning Code, Data, and Models - Useful for investigating drift\n\nSee tools like DVC, MLFlow, Weights and Biases, etc. for model and data versioning\n\nImportant to save data snapshots throughout the project lifecycle, for example: raw data, processed data, train data, validation data, test data and inference data.\n\nGithub and dbt for code versioning\n\nConsistent Structures - Consistency in project structures and naming can reduce human error, improve communication, and just make things easier to find.\n\nNaming examples:\n\n&lt;model-name&gt;-&lt;parameters&gt;-&lt;model-version&gt;\n&lt;model-name&gt;-&lt;data-version&gt;-&lt;use-case&gt;\n\nExample: Reduced project template based on {{cookiecutter}}\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ output      &lt;- The output data from the model. \n‚îÇ   ‚îú‚îÄ‚îÄ processed      &lt;- The final, canonical data sets for modeling.\n‚îÇ   ‚îî‚îÄ‚îÄ raw            &lt;- The original, immutable data dump.\n‚îÇ\n‚îú‚îÄ‚îÄ models             &lt;- Trained and serialized models, model predictions, or model summaries\n‚îÇ\n‚îú‚îÄ‚îÄ notebooks          &lt;- Jupyter notebooks. \n‚îÇ\n‚îú‚îÄ‚îÄ reports            &lt;- Generated analysis as HTML, PDF, LaTeX, etc.\n‚îÇ   ‚îî‚îÄ‚îÄ figures        &lt;- Generated graphics and figures to be used in reporting\n‚îÇ\n‚îú‚îÄ‚îÄ requirements.txt   &lt;- The requirements file for reproducing the analysis environment, e.g.\n‚îÇ                         generated with `pip freeze &gt; requirements.txt`\n‚îÇ\n‚îú‚îÄ‚îÄ code              &lt;- Source code for use in this project.\n    ‚îú‚îÄ‚îÄ __init__.py    &lt;- Makes src a Python module\n    ‚îÇ\n    ‚îú‚îÄ‚îÄ data           &lt;- Scripts to generate and process data\n    ‚îÇ   ‚îú‚îÄ‚îÄ data_preparation.py\n    ‚îÇ   ‚îî‚îÄ‚îÄ data_preprocessing.py\n    ‚îÇ\n    ‚îú‚îÄ‚îÄ models         &lt;- Scripts to train models and then use trained models to make\n    ‚îÇ   ‚îÇ                 predictions\n    ‚îÇ   ‚îú‚îÄ‚îÄ inference_model.py\n    ‚îÇ   ‚îî‚îÄ‚îÄ train_model.py\n    ‚îÇ\n    ‚îî‚îÄ‚îÄ analysis  &lt;- Scripts to create exploratory and results oriented visualizations\n        ‚îî‚îÄ‚îÄ analysis.py\n\n\nModel is performing well on the training set but much worse on the validation/test set\n\n\nAndrew Ng calls the validation set the ‚ÄúDev Set‚Äù üôÑ\nTest: Random sample the training set and use that as your validation set. Score your model on this new validation set\n\n‚ÄúTrain-Dev‚Äù is the sampled validation set\nPossibilities\n\nVariance: The data distribution of the training set is the same as the validation/test sets\n\n\nThe model has been overfit to the training data\n\nData Mismatch: The data distribution of the training set is NOT the same as the validation/test sets\n\n\nUnlucky and the split was bad\n\nSomething maybe is wrong with the splitting function\n\nSplit ratio needs adjusting. Validation set isn‚Äôt getting enough data to be representative.\n\n\n\n\nModel is performing well on the validation/test set but not in the real world\n\nInvestigate the validation/test set and figure out why it‚Äôs not reflecting real world data. Then, apply corrections to the dataset.\n\ne.g.¬†distributions of your validation/tests sets should look like the real world data.\n\nChange the metric\n\nConsider weighting cases that your model is performing extremely poorly on.\n\n\nSplits\n\nHarrell: ‚Äúnot appropriate to split data into training and test sets unless n&gt;20,000 because of the luck (or bad luck) of the split.‚Äù\nIf your dataset is over 1M rows, then having a test set of 200K might be overkill (e.g.¬†ratio of 60/20/20).\n\nMight be better to use a ratio of 98/1/1 for big data projects and 60/20/20 for smaller data projects\n\nlink\n\nShows that simple data splitting does not give valid confidence intervals (even asymptotically) when one refits the model on the whole dataset. Thus, if one wants valid confidence intervals for prediction error, we can only recommend either data splitting without refitting the model (which is viable when one has ample data), or nested CV.",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/cli.html",
    "href": "qmd/cli.html",
    "title": "CLI",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-misc",
    "href": "qmd/cli.html#sec-cli-misc",
    "title": "CLI",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly‚Äôs suggestions are prioritized in real time with a small neural network\n\nPath to a folder that‚Äôs above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder\n\nDebian vs.¬†Ubuntu (from ChatGPT)\n\nStability vs.¬†Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it‚Äôs known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu‚Äôs.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as ‚ÄúDebian spins,‚Äù catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-r",
    "href": "qmd/cli.html#sec-cli-r",
    "title": "CLI",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n¬† ¬† ¬† janitor::clean_names() %&gt;%\n¬† ¬† ¬† select(date, geo, county = name, negative, positive) %&gt;%\n¬† ¬† ¬† filter(geo == \"County\") %&gt;%\n¬† ¬† ¬† mutate(date = lubridate::as_date(date)) %&gt;%\n¬† ¬† ¬† select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it‚Äôll search automatically\n35548",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-awk",
    "href": "qmd/cli.html#sec-cli-awk",
    "title": "CLI",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‚Äò$13 &gt; 98‚Äô adult_t.csv|head\nFilter lines with ‚ÄúDoctorate‚Äù and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-bash",
    "href": "qmd/cli.html#sec-cli-bash",
    "title": "CLI",
    "section": "Bash",
    "text": "Bash\n\nMisc\n\nNotes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn‚Äôt take notes on\n\nBash Scripting on Linux: The Complete Guide - video course\n\nResources\n\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n‚Äú&gt;‚Äù redirects the output from a program to a file.\n\n‚Äú&gt;&gt;‚Äù does the same thing, but it‚Äôs appending to an existing file instead of overwriting it, if it already exists.\n\n\n\n\n\nCommands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you‚Äôre using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you‚Äôre about to move does already exist under the new directory,\n\nThis way you don‚Äôt accidentally overwrite files that you didn‚Äôt mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\n\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\nOutput to file: echo ‚ÄúThis is an example for redirect‚Äù &gt; file1.txt\nAppend line to file: echo ‚ÄúThis is the second line of the file‚Äù &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to ‚Äútmp‚Äù directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name ‚Äúmy_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n‚Äúrwx‚Äù stand for read, write, and execute rights, respectively\nThe 3 ‚Äúrwx‚Äù blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a ‚Äúd‚Äù for directory or ‚Äúl‚Äù for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - ‚Äúsuper user‚Äù - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‚Äòerror‚Äô, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo ‚ÄúNo such directory exist.Check‚Äù\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn‚Äôt exist, then the message ‚ÄúNo such directory exists. check‚Äù message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for ‚Äúerror‚Äù and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for ‚Äúerror‚Äù in each line. Lines with ‚Äúerror‚Äù get written to ‚Äúerror_file.txt‚Äù\n\nFilter lines\ngrep -i ‚ÄúDoctorate‚Äù adult_t.csv |grep -i ‚ÄúHusband‚Äù|grep -i ‚ÄúBlack‚Äù|csvlook\n# -i, --ignore-case-Ignore¬† case¬† distinctions,¬† so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i ‚ÄúDoctorate‚Äù adult_t.csv | wc -l\n\nCounts how many rows have ‚ÄúDoctorate‚Äù\n\n-wc is ‚Äúword count‚Äù\n\n\n\n\n\n\nVariables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via ‚Äú$‚Äù\n# local\nev_car=‚ÄôTesla‚Äô\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=‚ÄôTesla‚Äô\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it‚Äôs okay not to is on the left-hand-side of an [[ ]] condition. But even there I‚Äôd recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or ‚Äìhelp or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this\n\n\n\n\n\nScripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like ‚Äìsilent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script‚Äôs directory close to the start of the script.\n\nAnd it‚Äôs usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck. Heed its warnings.\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don‚Äôt give executable permission to the script file.\nStarts with ‚Äú#!‚Äù the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory ‚Äú/bin‚Äù\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you‚Äôre good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n¬† ¬† #!/bin/bash\n¬† ¬† echo ‚ÄòHello World‚Äô\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n¬† ¬† set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n¬† ¬† echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n¬† ¬† exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n¬† ¬† echo do awesome stuff\n}\nmain \"$@\"\n\n\n\nJob Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.\n\n\n\n\ntmux (‚Äòterminal multiplexer‚Äô)\n\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‚Äòmoose‚Äô\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named ‚Äúmoose‚Äù\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you‚Äôve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‚Äòmoose‚Äô\n\n\nPane Creation and Navigation\n\ncontrol+b then press ‚Äù (i.e.¬†shift+‚Äô): add another terminal pane below\ncontrol+b then press % (i.e.¬†shift+5) : add another terminal pane to the right\ncontrol+b then press ‚Üí : move to the terminal pane on the right (similar for left, up, down)\n\n\n\n\nSSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n‚Äússh-keygen‚Äù is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named ‚Äúid_rsa.pub‚Äù and a private key named ‚Äúid_rsa‚Äù, and place both into your ‚Äú~/.ssh‚Äù directory\nYou‚Äôll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: ‚Äú~/.ssh/config‚Äù\nContents\nHost dev\n¬† HostName remote\n¬† IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist\n\n\n\n\nVim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you‚Äôll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you‚Äôre in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file\n\n\n\n\nPackages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to ‚Äúupgrading‚Äù the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and ‚Äúupdates‚Äù them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade\n\n\n\n\nExpressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-powsh",
    "href": "qmd/cli.html#sec-cli-powsh",
    "title": "CLI",
    "section": "Powershell",
    "text": "Powershell\n\nMisc\n\nDocs, Sample Scripts\nUse Get-Help &lt;cmdlet-name&gt;¬†to access help information for specific cmdlets.\n\n$_ (Docs)\n\nAutomatic Variable; Alias for $PSItem\n\nDocs for automatic variables\n\nUseful for looping through objects in a directory. See ‚ÄúChange Name (or Extensions) of Multiple Files‚Äù for an example of usage\nGet the properties of an object\n\nExample: Using Get-ItemProperty (Docs)\nGet-ItemProperty scrapsheet.txt | Format-List\n\nProperty names will be on the left side of the output\n\nExample: Using Get-Member (Docs)\n Get-ChildItem *.txt | Select-Object -First 1 | Get-Member\n\nSelects the first text file in the directory and gets the Properties and a bunch of other stuff like methods which also can be used with $_\nDescriptions are awful for a lot of the stuff, but for the most part, you can guess what the property is.\n\n\nSome properties that can be used with $_\n\nFile System Objects:\n\n.Name: Returns the name component of a file\n.FullName: Returns the full path of the file.\n.Length: Returns the size of the file in bytes.\n.CreationTime: Returns the date and time the file was created.\n.LastWriteTime: Returns the date and time the file was last modified.\n.IsReadOnly: Returns $true if the file is read-only, $false otherwise.\n\nProcess Objects:\n\n.Id: Returns the unique identifier (PID) of the process.\n.Name: Returns the name of the process.\n.MainWindowTitle: Returns the title of the process‚Äôs main window (if applicable).\n.WorkingDirectory: Returns the working directory of the process.\n.CPU: Returns the CPU usage of the process.\n\nRegistry Objects:\n\n.Name: Returns the name of the registry key.\n.Hive: Returns the hive of the registry key (e.g., HKLM, HKCU).\n.Value: Returns the value of the registry key (if applicable).\n.Path: Returns the full path of the registry key.\n\n\n\nComments: &lt;# comment #&gt;\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you‚Äôre already in the directory that you want the folder in. You can also use a path, e.g.¬†\"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nChange Name (or Extensions) of Multiple Files\nGet-ChildItem *.md | Rename-Item -NewName { $_.Name -replace '.md','.qmd' }\n\nGet-ChildItem: cmdlet gets all the files in the current folder that have a .mc file extension\nResults piped to Rename-Item\n\nNewName: Has a value that‚Äôs a script block that runs before the value is submitted to the NewName parameter.\n\n$_: (automatic variable) It represents each file object as it comes to the command through the pipeline.\n-replace: Operator replaces the file extension of each file with .qmd.\n\nNotice that matching using the -replace operator is not case sensitive.\n\n\n\n\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it‚Äôs a backslash (\\), but in Powershell, it‚Äôs a backtick ( ` )\n*Don‚Äôt forget that there‚Äôs a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it‚Äôs open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-batscri",
    "href": "qmd/cli.html#sec-cli-batscri",
    "title": "CLI",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g.¬†timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-wsl",
    "href": "qmd/cli.html#sec-cli-wsl",
    "title": "CLI",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for ‚Äìdistribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/json.html",
    "href": "qmd/json.html",
    "title": "JSON",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-misc",
    "href": "qmd/json.html#sec-json-misc",
    "title": "JSON",
    "section": "",
    "text": "Packages\n\n{yyjsonr} - A fast JSON parser/serializer, which converts R data to/from JSON and NDJSON. It is around 2x to 10x faster than jsonlite at both reading and writing JSON.\n{RcppSimdJson} - Comparable to {yyjsonr} in performance.\n\nAlso see\n\nBig Data &gt;&gt; Larger than Memory\nSQL &gt;&gt; Processing Expressions &gt;&gt; Nested Data\nDatabases &gt;&gt; DuckDB &gt;&gt; Misc\n\nhrbmstr recommends trying duckdb before using the cli tools in ‚ÄúBig Data‚Äù\n\n\nTools\n\n{listviewer}: Allows you to interactively explore and edit json files through the Viewer in the IDE. Docs show how it can be embedded into a Shiny app as well.\n\nExample\nlibrary(listviewer)\nmoose &lt;- jsonlite::read_json(\"path/to/file.json\")\njsonedit(moose)\nreactjson(moose)\n\nI‚Äôve also used this a .config file which looked like a json file when I opened in a text editor, so this seems to work on anything json-like.\nreactjson has a copy button which is nice so that you can paste your edited version into a file.\njsonedit seems like it has more features, but I didn‚Äôt see a copy button. But there‚Äôs a view in which you can manually select everything a copy it via keyboard shortcut.",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-jsonlite",
    "href": "qmd/json.html#sec-json-jsonlite",
    "title": "JSON",
    "section": "{jsonlite}",
    "text": "{jsonlite}\n\nRead",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-py",
    "href": "qmd/json.html#sec-json-py",
    "title": "JSON",
    "section": "Python",
    "text": "Python\n\nExample: Parse Nested JSON into a dataframe (article)\n\nRaw JSON\n\n\n‚Äúentry‚Äù has the data we want\n‚Äú‚Ä¶‚Äù at the end indicates there are multiple objectss inside the element, ‚Äúentry‚Äù\n\nProbably other root elements other than ‚Äúfeed‚Äù as well\n\n\nRead a json file from a URL using {{requests}} and convert to list\n\nimport requests\n\nurl = \"https://itunes.apple.com/gb/rss/customerreviews/id=1500780518/sortBy=mostRecent/json\"\n\nr = requests.get(url)\n\ndata = r.json()\nentries = data[\"feed\"][\"entry\"]\n\nIt looks like the list conversion also ordered the elements alphabetically\nThe output list is subsetted by the root element ‚Äúfeed‚Äù and the child element ‚Äúentry‚Äù\n\nGet a feel for the final structure you want by hardcoding elements into a df\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    parsed_data[\"author_uri\"].append(entry[\"author\"][\"uri\"][\"label\"])\n    parsed_data[\"author_name\"].append(entry[\"author\"][\"name\"][\"label\"])\n    parsed_data[\"author_label\"].append(entry[\"author\"][\"label\"])\n    parsed_data[\"content_label\"].append(entry[\"content\"][\"label\"])\n    parsed_data[\"content_attributes_type\"].append(entry[\"content\"][\"attributes\"][\"type\"])\n    ... \nGeneralize extracting the properties of each object in ‚Äúentry‚Äù with a nested loop\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    for key, val in entry.items():\n        for subkey, subval in val.items():\n            if not isinstance(subval, dict):\n                parsed_data[f\"{key}_{subkey}\"].append(subval)\n            else:\n                for att_key, att_val in subval.items():\n                    parsed_data[f\"{key}_{subkey}_{att_key}\"].append(att_val)\n\ndefaultdict creates a key from a list element (e.g.¬†‚Äúauthor‚Äù) and groups the properties into a list of values where the value may also be a dict.\n\nSee Python, General &gt;&gt; Types &gt;&gt; Dictionaries\n\nFor each item in ‚Äúentry‚Äù, it looks at the first key-value pair knowing that value is always a dictionary (object in JSON)\nThen handles two different cases\n\nFirst Case: The value dictionary is flat and does not contain another dictionary, only key-value pairs.\n\nCombine the outer key with the inner key to a column name and take the value as column value for each pair.\n\nSecond Case: Dictionary contains a key-value pair where the value is again a dictionary.\n\nAssumes at most two levels of nested dictionaries\nIterates over the key-value pairs of the inner dictionary and again combines the outer key and the most inner key to a column name and take the inner value as column value.\n\n\n\nRecursive function that handles json elements with deeper structures\n\ndef recursive_parser(entry: dict, data_dict: dict, col_name: str = \"\") -&gt; dict:\n    \"\"\"Recursive parser for a list of nested JSON objects\n\n    Args:\n        entry (dict): A dictionary representing a single entry (row) of the final data frame.\n        data_dict (dict): Accumulator holding the current parsed data.\n        col_name (str): Accumulator holding the current column name. Defaults to empty string.\n    \"\"\"\n    for key, val in entry.items():\n        extended_col_name = f\"{col_name}_{key}\" if col_name else key\n        if isinstance(val, dict):\n            recursive_parser(entry[key], data_dict, extended_col_name)\n        else:\n            data_dict[extended_col_name].append(val)\n\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    recursive_parser(entry, parsed_data, \"\")\n\ndf = pd.DataFrame(parsed_data)\n\nNotice the check for a deeper structure with isinstance. If there is one, then the function is called again.\nFunction outputs a dict which is coerced into dataframe\nTo get rid of ‚Äúlabel‚Äù in column names: df.columns = [col if not \"label\" in col else \"_\".join(col.split(\"_\")[:-1]) for col in df.columns]\nobject types can be cast into more efficient types: df[\"im:rating\"] = df[\"im:rating\"].astype(int)",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html",
    "href": "qmd/cli-linux.html",
    "title": "Linux",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-misc",
    "href": "qmd/cli-linux.html#sec-cli-lin-misc",
    "title": "Linux",
    "section": "",
    "text": "Notes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn‚Äôt take notes on\n\nBash Scripting on Linux: The Complete Guide - video course\n\nResources\n\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n‚Äú&gt;‚Äù redirects the output from a program to a file.\n\n‚Äú&gt;&gt;‚Äù does the same thing, but it‚Äôs appending to an existing file instead of overwriting it, if it already exists.\n\n\nDebian vs.¬†Ubuntu (from ChatGPT)\n\nStability vs.¬†Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it‚Äôs known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu‚Äôs.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as ‚ÄúDebian spins,‚Äù catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-com",
    "href": "qmd/cli-linux.html#sec-cli-lin-com",
    "title": "Linux",
    "section": "Commands",
    "text": "Commands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you‚Äôre using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you‚Äôre about to move does already exist under the new directory,\n\nThis way you don‚Äôt accidentally overwrite files that you didn‚Äôt mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\n\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\nOutput to file: echo ‚ÄúThis is an example for redirect‚Äù &gt; file1.txt\nAppend line to file: echo ‚ÄúThis is the second line of the file‚Äù &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to ‚Äútmp‚Äù directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name ‚Äúmy_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n‚Äúrwx‚Äù stand for read, write, and execute rights, respectively\nThe 3 ‚Äúrwx‚Äù blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a ‚Äúd‚Äù for directory or ‚Äúl‚Äù for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - ‚Äúsuper user‚Äù - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‚Äòerror‚Äô, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo ‚ÄúNo such directory exist.Check‚Äù\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn‚Äôt exist, then the message ‚ÄúNo such directory exists. check‚Äù message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for ‚Äúerror‚Äù and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for ‚Äúerror‚Äù in each line. Lines with ‚Äúerror‚Äù get written to ‚Äúerror_file.txt‚Äù\n\nFilter lines\ngrep -i ‚ÄúDoctorate‚Äù adult_t.csv |grep -i ‚ÄúHusband‚Äù|grep -i ‚ÄúBlack‚Äù|csvlook\n# -i, --ignore-case-Ignore¬† case¬† distinctions,¬† so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i ‚ÄúDoctorate‚Äù adult_t.csv | wc -l\n\nCounts how many rows have ‚ÄúDoctorate‚Äù\n\n-wc is ‚Äúword count‚Äù",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-var",
    "href": "qmd/cli-linux.html#sec-cli-lin-var",
    "title": "Linux",
    "section": "Variables",
    "text": "Variables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via ‚Äú$‚Äù\n# local\nev_car=‚ÄôTesla‚Äô\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=‚ÄôTesla‚Äô\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it‚Äôs okay not to is on the left-hand-side of an [[ ]] condition. But even there I‚Äôd recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or ‚Äìhelp or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-script",
    "href": "qmd/cli-linux.html#sec-cli-lin-script",
    "title": "Linux",
    "section": "Scripting",
    "text": "Scripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like ‚Äìsilent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script‚Äôs directory close to the start of the script.\n\nAnd it‚Äôs usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck. Heed its warnings.\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don‚Äôt give executable permission to the script file.\nStarts with ‚Äú#!‚Äù the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory ‚Äú/bin‚Äù\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you‚Äôre good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n¬† ¬† #!/bin/bash\n¬† ¬† echo ‚ÄòHello World‚Äô\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n¬† ¬† set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n¬† ¬† echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n¬† ¬† exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n¬† ¬† echo do awesome stuff\n}\nmain \"$@\"",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-jobm",
    "href": "qmd/cli-linux.html#sec-cli-lin-jobm",
    "title": "Linux",
    "section": "Job Management",
    "text": "Job Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-tmux",
    "href": "qmd/cli-linux.html#sec-cli-lin-tmux",
    "title": "Linux",
    "section": "tmux (terminal multiplexer)",
    "text": "tmux (terminal multiplexer)\n\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‚Äòmoose‚Äô\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named ‚Äúmoose‚Äù\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you‚Äôve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‚Äòmoose‚Äô\n\n\nPane Creation and Navigation\n\ncontrol+b then press ‚Äù (i.e.¬†shift+‚Äô): add another terminal pane below\ncontrol+b then press % (i.e.¬†shift+5) : add another terminal pane to the right\ncontrol+b then press ‚Üí : move to the terminal pane on the right (similar for left, up, down)",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-ssh",
    "href": "qmd/cli-linux.html#sec-cli-lin-ssh",
    "title": "Linux",
    "section": "SSH",
    "text": "SSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n‚Äússh-keygen‚Äù is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named ‚Äúid_rsa.pub‚Äù and a private key named ‚Äúid_rsa‚Äù, and place both into your ‚Äú~/.ssh‚Äù directory\nYou‚Äôll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: ‚Äú~/.ssh/config‚Äù\nContents\nHost dev\n¬† HostName remote\n¬† IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-vim",
    "href": "qmd/cli-linux.html#sec-cli-lin-vim",
    "title": "Linux",
    "section": "Vim",
    "text": "Vim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you‚Äôll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you‚Äôre in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-pkg",
    "href": "qmd/cli-linux.html#sec-cli-lin-pkg",
    "title": "Linux",
    "section": "Packages",
    "text": "Packages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to ‚Äúupgrading‚Äù the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and ‚Äúupdates‚Äù them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-expr",
    "href": "qmd/cli-linux.html#sec-cli-lin-expr",
    "title": "Linux",
    "section": "Expressions",
    "text": "Expressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-general.html",
    "href": "qmd/cli-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-misc",
    "href": "qmd/cli-general.html#sec-cli-gen-misc",
    "title": "General",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly‚Äôs suggestions are prioritized in real time with a small neural network\n\nPath to a folder that‚Äôs above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-r",
    "href": "qmd/cli-general.html#sec-cli-gen-r",
    "title": "General",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n¬† ¬† ¬† janitor::clean_names() %&gt;%\n¬† ¬† ¬† select(date, geo, county = name, negative, positive) %&gt;%\n¬† ¬† ¬† filter(geo == \"County\") %&gt;%\n¬† ¬† ¬† mutate(date = lubridate::as_date(date)) %&gt;%\n¬† ¬† ¬† select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it‚Äôll search automatically\n35548",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-awk",
    "href": "qmd/cli-general.html#sec-cli-gen-awk",
    "title": "General",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‚Äò$13 &gt; 98‚Äô adult_t.csv|head\nFilter lines with ‚ÄúDoctorate‚Äù and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html",
    "href": "qmd/cli-windows.html",
    "title": "Windows",
    "section": "",
    "text": "PowerShell",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-powsh",
    "href": "qmd/cli-windows.html#sec-cli-win-powsh",
    "title": "Windows",
    "section": "",
    "text": "Misc\n\nDocs, Sample Scripts\nUse Get-Help &lt;cmdlet-name&gt; to access help information for specific cmdlets.\nCheck version: $PSVersionTable\n\nFor a breakdown of the version number (e.g.¬†build, revison, etc.): $PSVersionTable.PSVersion\n\nUpdate to latest stable version: github\nComments: &lt;# comment #&gt;\nClear terminal: clear or cls or Clear-Host\nBefore you‚Äôll be able to run a script, you need to open PowerShell as administrator and execute this command: Set-ExecutionPolicy RemoteSigned\n\n\n\nLoops\n\nIterables\n\nArrays : $folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n$_ (Docs)\n\nAutomatic Variable; Alias for $PSItem\n\nDocs for automatic variables\n\nUseful for looping through objects in a directory. See ‚ÄúChange Name (or Extensions) of Multiple Files‚Äù for an example of usage\nGet the properties of an object\n\nExample: Using Get-ItemProperty (Docs)\nGet-ItemProperty scrapsheet.txt | Format-List\n\nProperty names will be on the left side of the output\n\nExample: Using Get-Member (Docs)\n Get-ChildItem *.txt | Select-Object -First 1 | Get-Member\n\nSelects the first text file in the directory and gets the Properties and a bunch of other stuff like methods which also can be used with $_\nDescriptions are awful for a lot of the stuff, but for the most part, you can guess what the property is.\n\n\nSome properties that can be used with $_\n\nFile System Objects:\n\n.Name: Returns the name component of a file\n.FullName: Returns the full path of the file.\n.Length: Returns the size of the file in bytes.\n.CreationTime: Returns the date and time the file was created.\n.LastWriteTime: Returns the date and time the file was last modified.\n.IsReadOnly: Returns $true if the file is read-only, $false otherwise.\n\nProcess Objects:\n\n.Id: Returns the unique identifier (PID) of the process.\n.Name: Returns the name of the process.\n.MainWindowTitle: Returns the title of the process‚Äôs main window (if applicable).\n.WorkingDirectory: Returns the working directory of the process.\n.CPU: Returns the CPU usage of the process.\n\nRegistry Objects:\n\n.Name: Returns the name of the registry key.\n.Hive: Returns the hive of the registry key (e.g., HKLM, HKCU).\n.Value: Returns the value of the registry key (if applicable).\n.Path: Returns the full path of the registry key.\n\n\n\n\n\n\nforeach\n\nIterate over an array\n# Create an array of folders\n$folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n\n# Perform iteration to create the same file in each folder\nforeach ($i in $folders) {\n    Add-Content -Path \"$i\\SampleFile.txt\" -Value \"This is the content of the file\"\n}\nIterate over the output of Get-ChildItem\n# Define the directory containing the files\n$directory = \"C:\\Path\\To\\Files\"\n\n# Define the FFmpeg argument string\n$ffmpegArg = \"-i {0}.mp4 -c:v libx265 -crf 28 -preset medium -vf scale=-1:720 -c:a copy Documents\\temp-storage\\{1}.mp4\"\n\n# Get all files in the directory\n$files = Get-ChildItem -Path $directory -Filter \"*.mp4\"\n\n# Loop through each file and apply the FFmpeg command\nforeach ($file in $files) {\n  # Construct the full argument string with the current file path\n  $fullArg = $ffmpegArg -f [string]::Format($file.FullName, $file.Name)\n\n  # Execute the FFmpeg command\n  Start-Process -FilePath \"ffmpeg.exe\" -ArgumentList $fullArg -Wait -NoNewWindow\n}\n\nWrite-Host \"Finished processing files!\"\n\nDefine variables:\n\n$directory: Replace with the actual path to your directory.\n$ffmpegCommand: Replace with your desired FFmpeg command. Use curly braces {} to represent the placeholder for the file path.\n\nGet files:\n\nGet-ChildItem retrieves files from the specified directory ($directory).\nAdjust the -Filter parameter to match your desired file type (e.g., *.mp4).\n\nLoop through files:\n\nThe foreach loop iterates through each file ($file) in the $files collection.\n\nConstruct full command:\n\nstring.Format inserts the current file‚Äôs full path ($file.FullName) into the $ffmpegCommand template.\n\nExecute FFmpeg:\n\nStart-Process launches ffmpeg.exe with the constructed $fullCommand arguments.\n-Wait ensures the command finishes before continuing.\n-NoNewWindow hides the ffmpeg console window.\n\nPrint confirmation:\n\nAfter processing all files, a message is displayed.\n\n\n\n\n\n\nCommands\n\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you‚Äôre already in the directory that you want the folder in. You can also use a path, e.g.¬†\"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nChange Name (or Extensions) of Multiple Files\nGet-ChildItem *.md | Rename-Item -NewName { $_.Name -replace '.md','.qmd' }\n\nGet-ChildItem: cmdlet gets all the files in the current folder that have a .mc file extension\nResults piped to Rename-Item\n\nNewName: Has a value that‚Äôs a script block that runs before the value is submitted to the NewName parameter.\n\n$_: (automatic variable) It represents each file object as it comes to the command through the pipeline.\n-replace: Operator replaces the file extension of each file with .qmd.\n\nNotice that matching using the -replace operator is not case sensitive.\n\n\n\n\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it‚Äôs a backslash (\\), but in Powershell, it‚Äôs a backtick ( ` )\n*Don‚Äôt forget that there‚Äôs a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it‚Äôs open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-batscri",
    "href": "qmd/cli-windows.html#sec-cli-win-batscri",
    "title": "Windows",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g.¬†timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-wsl",
    "href": "qmd/cli-windows.html#sec-cli-win-wsl",
    "title": "Windows",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for ‚Äìdistribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html",
    "href": "qmd/confidence-and-prediction-intervals.html",
    "title": "Confidence & Prediction Intervals",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-misc",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-misc",
    "title": "Confidence & Prediction Intervals",
    "section": "",
    "text": "Also see Mathematices, Statistics &gt;&gt; Descriptive Statistics &gt;&gt; Understanding CI, sd, and sem Bars\nSE used for CIs of the difference in proportion\n\\[\n\\text{SE} = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\n\\]",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-terms",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-terms",
    "title": "Confidence & Prediction Intervals",
    "section": "Terms",
    "text": "Terms\n\nConfidence Intervals: A range of values within which we are reasonably confident the true parameter (e.g mean) of a population lies, based on a sample statistic (e.g.¬†t-stat).\n\nFrequentist Interpretation: The confidence interval is constructed by a procedure, which, if you were to repeat the experiment and collecting samples many many times, in 95% of the experiments, the corresponding confidence intervals would cover the true value of the population mean. (link)\n\\[\n[100\\cdot(1-\\alpha)]\\;\\%\\: \\text{CI for}\\: \\hat\\beta_i = \\hat\\beta_i \\pm \\left[t_{(1-\\alpha/2)(n-k)} \\cdot \\text{SE}(\\hat\\beta_i)\\right]\n\\]\n\n\\(t\\) is the t-stat for\n\n\\(n-k\\) = sample size - number of predictors\n\\(1-\\alpha\\) for 2-sided; \\(1 - (\\alpha/2)\\) for 1 sided (I think)\n\n\\(\\text{SE}(\\beta_i)\\) is the sqrt of the corresponding value on the diagonal of the variance-covariance matrix for the coefficients.\n\nBayesian Interpretation: the true value is in that interval with 95% probability\n\nCoverage or Empirical Coverage: The level of coverage actually observed when evaluated on a dataset, typically a holdout dataset not used in training the model. Rarely will your model produce the Expected Coverage exactly\n\nAdaptive Coverage: Setting your Expected Coverage so that your Empirical Coverage = Target Coverage. A conformal prediction algorithm is adaptive if it not only achieves marginal coverage, but also (approximately) conditional coverage\n\nExample: 90% target coverage\n\nIf our model is slightly overfit, you might see that a 90% expected coverage leads to an 85% empirical coverage on a holdout dataset. To align your target and empirical coverage at 90%, may require setting expected coverage at something like 93%\n\n\nExpected Coverage: The level of confidence in the model for the prediction intervals.\nConditional Coverage: The coverage for each individual class of the outcome variable or subset of data specified by a grouping variable.\nMarginal Coverage: The overall average coverage across all classes of the outcome variable. All conformal methods achieve at or near the Expected Coverage averaged across classes but not necessarily for each individual class.\nTarget Coverage: The level of coverage you want to attain on a holdout dataset\n\ni.e.¬†The proportion of observations you want to fall within your prediction intervals\n\n\nJeffrey‚Äôs Interval: Bayesian CIs for Binomial proportions (i.e.¬†probability of an event)\n# probability of event\n# n_rain in the number of events (rainy days)\n# n is the number of trials (total days)\nmutate(pct_rain = n_rain / n,¬†\n¬† ¬† ¬† ¬†# jeffreys interval\n¬† ¬† ¬† ¬†# bayesian CI for binomial proportions\n¬† ¬† ¬† ¬†low = qbeta(.025, n_rain + .5, n - n_rain + .5),¬†\n¬† ¬† ¬† ¬†high = qbeta(.975, n_rain + .5, n - n_rain + .5))\nPrediction Interval: Used to estimate the range within which a future observation is likely to fall\n\nStandard Procedure for computing PIs for predictions (See link for examples and further details)\n\\[\n\\hat Y_0 \\pm t^{n-p}_{\\alpha/2} \\;\\hat\\sigma \\sqrt{1 + \\vec x_0'(X'X)^{-1}\\vec x_0}\n\\]\n\n\\(Y_0\\) is a single prediction\n\\(t\\) is the t-stat for\n\n\\(n-p\\) = sample size - number of predictors\n\\(1 - \\alpha\\) for 2-sided; \\(1 - (\\alpha/2)\\) for 1 sided (I think)\n\n\\(\\hat\\sigma\\) is the variance given by residual standard error, summary(Model1)$sigma\n\\[\nS^2 = \\frac{1}{n-p}\\;||\\;Y-X\\hat \\beta\\;||^2\n\\]\n\n\\(S = \\hat \\sigma\\)\nI think this is also the \\(\\operatorname{MSE}/\\operatorname{dof}\\) that you sometimes see in other formulas\n\n\\(x_0\\) is new data for the predictor variable values for the prediction (also would need to include a 1 for the intercept)\n\\((X'X)^{-1}\\) is the variance covariance matrix, vcov(model)",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-diag",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-diag",
    "title": "Confidence & Prediction Intervals",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nMean Interval Score (MIS)\n\n(Proper) Score of both coverage and interval width\n\nI don‚Äôt think there‚Äôs a closed range, so it‚Äôs meant for model comparison\nLower is better\n\ngreybox::MIS and (scaled) greybox::sMIS\n\nOnline docs don‚Äôt have these functions, but docs in RStudio do\n\nAlso scoringutils::interval_score\n\nDocs have formula\n\nThe actual paper is dense Need to take the mean of MIS\n\n\n\nCoverage\n\nExample: Coverage %\ncoverage &lt;- function(df, ...){\n¬† df %&gt;%\n¬† ¬† mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price pred_upper, 1, 0)) %&gt;%¬†\n¬† ¬† group_by(...) %&gt;%¬†\n¬† ¬† summarise(n = n(),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† n_covered = sum(\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† covered\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† stderror = sd(covered) / sqrt(n),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† coverage_prop = n_covered / n)\n}\nrf_preds_test %&gt;%¬†\n¬† coverage() %&gt;%¬†\n¬† mutate(across(c(coverage_prop, stderror), ~.x * 100)) %&gt;%¬†\n¬† gt::gt() %&gt;%¬†\n¬† gt::fmt_number(\"stderror\", decimals = 2) %&gt;%¬†\n¬† gt::fmt_number(\"coverage_prop\", decimals = 1)\n\nFrom Quantile Regression Forests for Prediction Intervals\nSale_Price is the outcome variable\nrf_preds_test is the resulting object from predict with a tidymodels model as input\n\nExample: Test consistency of coverage across quintiles\npreds_intervals %&gt;%¬† # preds w/ PIs\n¬† mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;%¬† # quintiles\n¬† mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price &lt;= .pred_upper, 1, 0)) %&gt;%¬†\n  with(chisq.test(price_grouped, covered))\n\np value &lt; 0.05 says coverage significantly differs by quintile\nSale_Price is the outcome variable\n\n\nInterval Width\n\nNarrower bands should mean a more precise model\nExample: Average interval width across quintiles\nlm_interval_widths &lt;- preds_intervals %&gt;%¬†\n¬† mutate(interval_width = .pred_upper - .pred_lower,\n¬† ¬† ¬† ¬† interval_pred_ratio = interval_width / .pred) %&gt;%¬†\n¬† mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;% # quintiles\n¬† group_by(price_grouped) %&gt;%¬†\n¬† summarize(n = n(),\n¬† ¬† ¬† ¬† ¬† ¬† mean_interval_width_percentage = mean(interval_pred_ratio),\n¬† ¬† ¬† ¬† ¬† ¬† stdev = sd(interval_pred_ratio),\n¬† ¬† ¬† ¬† ¬† ¬† stderror = stdev / sqrt(n)) %&gt;%¬†\n¬† mutate(x_tmp = str_sub(price_grouped, 2, -2)) %&gt;%¬†\n¬† separate(x_tmp, c(\"min\", \"max\"), sep = \",\") %&gt;%¬†\n¬† mutate(across(c(min, max), as.double)) %&gt;%¬†\n¬† select(-price_grouped)\n\nlm_interval_widths %&gt;%¬†\n¬† mutate(across(c(mean_interval_width_percentage, stdev, stderror), ~.x*100)) %&gt;%¬†\n¬† gt::gt() %&gt;%¬†\n¬† gt::fmt_number(c(\"stdev\", \"stderror\"), decimals = 2) %&gt;%¬†\n¬† gt::fmt_number(\"mean_interval_width_percentage\", decimals = 1)\n\nInterval width has actually been transformed into a percentage as related to the prediction (removes the scale of the outcome variable)",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-boot",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-boot",
    "title": "Confidence & Prediction Intervals",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nMisc\n\nDo NOT bootstrap the standard deviation\n\narticle\nbootstrap is ‚Äúbased on a weak convergence of moments‚Äù\nif you use an estimate based standard deviation of the bootstrap, you are being overly conservative (i.e.¬†overestimate the sd)\n\nbootstrapping uses the original, initial sample as the population from which to resample, whereas Monte Carlo simulation is based on setting up a data generation process (with known values of the parameters of a known distribution). Where Monte Carlo is used to test drive estimators, bootstrap methods can be used to estimate the variability of a statistic and the shape of its sampling distribution\nPackages\n\n{ebtools::get_boot_ci}\n\n\nSteps\n\nResample with replacement\nCalculate statistic of resample\nStore statistic\nRepeat 10K or so times\nCalculate mean, sd, and quantiles for CIs across all collected statistics\n\nCIs\n\nPlenty of articles for means and models, see bkmks\nrsample::reg_intervals is a convenience function for lm, glm, survival models\n\nPIs\n\nBootstrapping PIs is a bit complicated\n\nSee Shalloway‚Äôs article (code included)\nonly use out-of-sample estimates to produce the interval\nestimate the uncertainty of the sample using the residuals from a separate set of models built with cross-validation",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-conf",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-conf",
    "title": "Confidence & Prediction Intervals",
    "section": "Conformal Prediction Intervals",
    "text": "Conformal Prediction Intervals\n\nMisc\n\nPackages\n\n{{mapie}} - Handles scikit-learn, tf, pytorch, etc. with wrappers. Computes conformal PIs for Regression, Classification, and Time Series models.\n\nRegression\n\nMethods: naive, split, jackknife, jackknife+, jackknife-minmax, jackknife-after-bootstrap, CV, CV+, CV-minmax, ensemble batch prediction intervals (EnbPI).\n‚ÄúSince the typical coverage levels estimated by jackknife+ follow very closely the target coverage levels, this method should be used when accurate and robust prediction intervals are required.‚Äù\n‚ÄúFor practical applications where N is large and/or the computational time of each leave-one-out simulation is high, it is advised to adopt the CV+ method‚Äù even though the interval width will be slightly larger than jackknife+\n‚ÄúThe jackknife-minmax and CV-minmax methods are more conservative since they result in higher theoretical and practical coverages due to the larger widths of the prediction intervals. It is therefore advised to use them when conservative estimates are needed.‚Äù\n‚ÄúThe conformalized quantile regression method allows for more adaptiveness on the prediction intervals which becomes key when faced with heteroscedastic data.‚Äù\nEnbPI is for time series and residuals must be updated each time new observations are available\n\nClassification\n\nMethods: LAC, Top-K, Adaptive Prediction Sets (APS), Regularized Adaptive Prediction Sets (RAPS), Split and Cross-Conformal methods.\nThe difference between these methods is the way the conformity scores are computed\nLAC method is not adaptive: the coverage guarantee only holds on average (i.e.¬†marginal coverage). Difficult classification cases may have prediction sets that are too small, and easy cases may have sets that are too large. (See below for details on process). Doesn‚Äôt seem like to great a task to manually make it adaptive though (See example below).\nAPS‚Äô conformity score used to determine the threshold is a constrained sum of the predicted probabilities for that observation. Only the predicted probabilites \\(\\ge\\) the predicted probability of the true class are included in the sum. Everything else is the same as the LAC algorithm, although the default behavior is to keep the last class that crosses the threshold through the argument, include_last_label = [True, ‚Äúrandomized‚Äù, False]. The value of the argument can determine whether conditional coverage is (approximately) attained with True being the most liberal setting. Note that only ‚Äúrandomized‚Äù can produce empty predicted class sets. Algorithm tends to produce large predicted class sets when there are many classes in the outcome variable.\nRAPS attenuates the lengthier predicted class sets in APS through regularization. A penalty, \\(\\lambda\\), is added to predicted probabilities with ranks greater that some value, \\(k\\). Everything else is the same as APS.\nNot sure what Split is, but Cross-Conformal is CV applied to LAC and APS.\n\n\n\nNotes from\n\nHow to Handle Uncertainty in Forecasts: A deep dive into conformal prediction\n\nThe conformity score formula used in this article, \\(s_i = |\\;y_i - \\hat p_i(y_i\\;|\\;X_i)\\;|\\) where \\(y_i\\) is the observed class and \\(\\hat p\\) is the predicted probability, has the same results as to the one below, but it‚Äôs not workable in production since there is no observed class.\n\nConformal Prediction for Machine Learning Classification ‚Äî From the Ground Up\n‚ÄúMAPIE‚Äù Explained Exactly How You Wished Someone Explained to You\n\nResources\n\nIntroduction To Conformal Prediction With Python A Short Guide for Quantifying Uncertainty of Machine Learning Models\n\nSee R &gt;&gt; Documents &gt;&gt; Machine Learning\n\n\nNormal PIs require iid data while conformal PIs only require the ‚Äúidentically distributed‚Äù part (not independent) and therefore should provide more robust coverage.\nContinuous outcome (link) using quantile regression\n\n\n\nClassification\n\nLAC (aka Score Method) Process\n\nSplit data into Train, Calibration (aka Validation), and Test\nTrain the model on the training set\nOn the calibration (aka validation) set, compute the conformity scores only for the observed class (i.e.¬†true label) for each observation\n\\[\ns_{i, j}  = 1 - \\hat p_{i,j}(y_i | X_i)\n\\]\n\nVariables\n\n\\(s_{i,j}\\): Conformity Score for the ith observation and class \\(j\\)\n\\(y_i\\): Observed Class\n\\(\\hat p_{i,j}\\): Predicted probability by the model for class \\(j\\)\n\\(X_i\\): Predictors\n\\(i\\): Index of the observed data\n\\(j\\): Class of the outcome variable\n\nRange: [0, 1]\nIn general, Low = good, High = bad\nIn R, the predicted probabilities for statistical models are always for the event (i.e.¬†\\(y_i = 1\\)) in a binary outcome context, so when the observed class = 0, the score will be \\(s_{i,0} = 1-(1- \\hat p_{i, 1}(y_i | X_i)) = \\hat p_{i, 1}(y_i | X_i)\\) which is just the predicted probability.\n\nOrder the conformity scores from highest to lowest\nAdjust the chosen the \\(\\alpha\\) using a finite sample correction, \\(q_{\\text{level}} = 1- \\frac{ceil((n_{\\text{cal}}+1)\\alpha)}{n_{\\text{cal}}}\\) and calculate the quantile.\nCalculate the critical value or threshold for the quantile\n\n\nx-axis corresponds to an ordered set of conformity scores\nIf \\(\\alpha = 0.05\\), find the score value at the the 95th percentile (e.g.¬†quantile(scores, 0.95))\nBlue: conformity scores are not statistically significant. They‚Äôre within our prediction interval.\nRed: Very large conformity scores indicate high divergence from the true label. These conformal scores are statistically significant and thereby outside of our prediction interval.\n\nPredict on the Test set and calculate conformity scores for each class\nFor each test set observation, select classes that have scores below the threshold score as the model prediction.\n\nAn observation could potentially have both classes or no classes selected. ( Not sure if this is true in a binary outcome situation)\n\n\nExample: LAC Method, Multinomial\n\nModel\nclassifier = LogisticRegression(random_state=42)\nclassifier.fit(X_train, y_train)\nScores calculated using only the predicted probability for the true class on the Validation set (aka Calibration set)\n# Get predicted probabilities for calibration set\ny_pred = classifier.predict(X_Cal)\ny_pred_proba = classifier.predict_proba(X_Cal)\nsi_scores = []\n# Loop through all calibration instances\nfor i, true_class in enumerate(y_cal):\n    # Get predicted probability for observed/true class\n    predicted_prob = y_pred_proba[i][true_class]\n    si_scores.append(1 - predicted_prob) \nThe threshold determines what¬†coverage our predicted labels will have\nnumber_of_samples = len(X_Cal)\nalpha = 0.05\nqlevel = (1 - alpha) * ((number_of_samples + 1) / number_of_samples)\nthreshold = np.percentile(si_scores, qlevel*100)\nprint(f'Threshold: {threshold:0.3f}')\n#&gt; Threshold: 0.598\n\nFinite sample correction for the 95th quantile: multiply 0.95 by¬†(n+1)/n\n\nThreshold is then used to get predicted labels of the test set\n# Get standard predictions for comparison\ny_pred = classifier.predict(X_test)\n# Calc scores, then only take scores in the 95% conformal PI\nprediction_sets = (1 - classifier.predict_proba(X_test) &lt;= threshold)\n\n# Get labels for predictions in conformal PI\ndef get_prediction_set_labels(prediction_set, class_labels):\n    # Get set of class labels for each instance in prediction sets\n    prediction_set_labels = [\n        set([class_labels[i] for i, x in enumerate(prediction_set) if x]) for prediction_set in \n        prediction_sets]\n    return prediction_set_labels\n\n# Compare conformal prediction with observed and traditional preds\nresults_sets = pd.DataFrame()\nresults_sets['observed'] = [class_labels[i] for i in y_test]\nresults_sets['conformal'] = get_prediction_set_labels(prediction_sets, class_labels)\nresults_sets['traditional'] = [class_labels[i] for i in y_pred]\nresults_sets.head(10)\n#&gt;    observed  conformal        traditional\n#&gt; 0  blue      {blue}           blue\n#&gt; 1  green     {green}          green\n#&gt; 2  blue      {blue}           blue\n#&gt; 3  green     {green}          green\n#&gt; 4  orange    {orange}         orange\n#&gt; 5  orange    {orange}         orange\n#&gt; 6  orange    {orange}         orange\n#&gt; 7  orange    {blue, orange}   blue\n#&gt; 8  orange    {orange}         orange\n#&gt; 9  orange    {orange}         orange\n\nconformity scores are calculated for each potential class using the predicted probabilities on the test set\nThe predicted class for an observation is determined by whether a class has a score below the threshold.\nTherefore, an observation may have 1 or more predicted classes or 0 predicted classes.\n\nStatistics (See Statistics section for functions)\n\nOverall\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n#&gt; Overall coverage: 0.947\n#&gt; Average set size: 1.035\n\nOverall coverage is very close to the target coverage of 95%, therefore, marginal coverage is achieved which is expected for this method\n\nPer Class\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n#&gt;         Class counts  Coverage   Average set size\n#&gt; blue    241           0.817427   1.087137\n#&gt; orange  848           0.954009   1.037736\n#&gt; green   828           0.977053   1.016908\n\nOverall coverage (i.e.¬†for all labels) will be at or very near 95% but coverage for individual classes may vary.\n\nAn illustration of how this method lacks Conditional Coverage\nSolution: Get thresholds for each class. (See next example)\n\nNote that the blue class had substantially fewer observations that the other 2 classes.\n\n\n\nExample: LAC-adapted - Threshold per Class\n\nDon‚Äôt think {{mapie}} has this option.\nAlso possible do this for subgroups of data, such as ensuring equal coverage for a diagnostic across racial groups, if we found coverage using a shared threshold led to problems.\nCalculate individual class thresholds\n# Set alpha (1 - coverage)\nalpha = 0.05\nthresholds = []\n# Get predicted probabilities for calibration set\ny_cal_prob = classifier.predict_proba(X_Cal)\n# Get 95th percentile score for each class's s-scores\nfor class_label in range(n_classes):\n    mask = y_cal == class_label\n    y_cal_prob_class = y_cal_prob[mask][:, class_label]\n    s_scores = 1 - y_cal_prob_class\n    q = (1 - alpha) * 100\n    class_size = mask.sum()\n    correction = (class_size + 1) / class_size\n    q *= correction\n    threshold = np.percentile(s_scores, q)\n    thresholds.append(threshold)\nApply individual class thresholds to test set scores\n# Get Si scores for test set\npredicted_proba = classifier.predict_proba(X_test)\nsi_scores = 1 - predicted_proba\n\n# For each class, check whether each instance is below the threshold\nprediction_sets = []\nfor i in range(n_classes):\n    prediction_sets.append(si_scores[:, i] &lt;= thresholds[i])\nprediction_sets = np.array(prediction_sets).T\n\n# Get prediction set labels and show first 10\nprediction_set_labels = get_prediction_set_labels(prediction_sets, class_labels)\nStatistics\n\nOverall\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n#&gt; Overall coverage: 0.95\n#&gt; Average set size: 1.093\n\nSimilar to previous example\n\nPer Class\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n#&gt;         Class counts  Coverage   Average set size\n#&gt; blue    241           0.954357   1.228216\n#&gt; orange  848           0.956368   1.139151\n#&gt; green   828           0.942029   1.006039\n\nCoverages now very close to 95% and the average set sizes have increased, especially for Blue.\n\n\n\n\n\n\nContinuous\n\nConformalized Quantile Regression Process\n\nSplit data into Training, Calibration, and Test sets\n\nTraining data: data on which the quantile regression model learns.\nCalibration data: data on which CQR calibrates the intervals.\n\nIn the example, he split the data into 3 equal sets\n\nTest data: data on which we evaluate the goodness of intervals.\n\nFit quantile regression model on training data.\nUse the model obtained at previous step to predict intervals on calibration data.\n\nPIs are predictions at the quantiles:\n\n(alpha/2)*100) (e.g 0.025, alpha = 0. 05)\n(1-(alpha/2))*100) (e.g.¬†0.975)\n\n\nCompute conformity scores on calibration data and intervals obtained at the previous step.\n\nResiduals are calculated for the PI vectors\nScores are calculated by taking the row-wise maximum of both (upper/lower quantile) residual vectors (e.g s_i &lt;- pmax(lower_pi_res, upper_pi_res))\n\nGet 1-alpha quantile from the distribution of conformity scores (e.g threshold &lt;- quantile(s_i, 0.95)\n\nThis score value will be the threshold\n\nUse the model obtained at step 1 to make predictions on test data.\n\nCompute PI vectors (i.e.¬†predictions at the previously stated quantiles) on Test set\ni.e.¬†Same calculation as with the calibration data in step 2 where you use the model to predict at upper and lower PI quantiles.\n\nCompute lower/upper end of the interval by subtracting/adding the threshold from/to the quantile predictions (aka PIs)\n\nLower conformity interval: lower_pi &lt;- test_lower_pred  - threshold\nUpper conformity interval: upper_pi &lt;- test_upper_pred + threshold\n\n\nExample: Quantile Random Forest\nimport numpy as np\nfrom skgarden import RandomForestQuantileRegressor\n\nalpha = .05\n\n# 1. Fit quantile regression model on training data\nmodel = RandomForestQuantileRegressor().fit(X_train, y_train)\n\n# 2. Make prediction on calibration data\ny_cal_interval_pred = np.column_stack([\n¬† ¬† model.predict(X_cal, quantile=(alpha/2)*100),¬†\n¬† ¬† model.predict(X_cal, quantile=(1-alpha/2)*100)])\n\n# 3. Compute conformity scores on calibration data\ny_cal_conformity_scores = np.maximum(\n¬† ¬† y_cal_interval_pred[:,0] - y_cal,¬†\n¬† ¬† y_cal - y_cal_interval_pred[:,1])\n\n# 4. Threshold: Get 1-alpha quantile from the distribution of conformity scores\n#¬† ¬† Note: this is a single number\nquantile_conformity_scores = np.quantile(\n¬† ¬† y_cal_conformity_scores, 1-alpha)\n\n# 5. Make prediction on test data\ny_test_interval_pred = np.column_stack([\n¬† ¬† model.predict(X_test, quantile=(alpha/2)*100),¬†\n¬† ¬† model.predict(X_test, quantile=(1-alpha/2)*100)])\n\n# 6. Compute left (right) end of the interval by\n#¬† ¬† subtracting (adding) the quantile to the predictions\ny_test_interval_pred_cqr = np.column_stack([\n¬† ¬† y_test_interval_pred[:,0] - quantile_conformity_scores,\n¬† ¬† y_test_interval_pred[:,1] + quantile_conformity_scores])\n\n\n\nStatistics\n\nAverage Set Size\n\nThe average number of predicted classes per observation since there can be more than 1 predicted class in the conformal PI\nExample:\n# average set size for each class\ndef get_average_set_size(prediction_sets, y_test):\n    average_set_size = []\n    for i in range(n_classes):\n        average_set_size.append(\n            np.mean(np.sum(prediction_sets[y_test == i], axis=1)))\n    return average_set_size   \n\n# Overall average set size (weighted by class size)\n# Get class counts\ndef get_class_counts(y_test):\n    class_counts = []\n    for i in range(n_classes):\n        class_counts.append(np.sum(y_test == i))\n    return class_counts\n\ndef get_weighted_set_size(set_size, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_set_size = np.sum((set_size * class_counts) / total_counts)\n    weighted_set_size = round(weighted_set_size, 3)\n    return weighted_set_size\n\nCoverage\n\nClassification: Percentage of correct classifications\nExample: Classification\n# coverage for each class\ndef get_coverage_by_class(prediction_sets, y_test):\n    coverage = []\n    for i in range(n_classes):\n        coverage.append(np.mean(prediction_sets[y_test == i, i]))\n    return coverage\n\n# overall coverage (weighted by class size)\n# Get class counts\ndef get_class_counts(y_test):\n    class_counts = []\n    for i in range(n_classes):\n        class_counts.append(np.sum(y_test == i))\n    return class_counts\n\ndef get_weighted_coverage(coverage, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_coverage = np.sum((coverage * class_counts) / total_counts)\n    weighted_coverage = round(weighted_coverage, 3)\n    return weighted_coverage\n\n\n\n\nVisualization\n\nConfusion Matrix\n\n\nBinary target where labels are 0 and 1\nInterpretation\n\nTop-left: predictions where both labels are not statistically significant (i.e.¬†inside the ‚Äúprediction interval‚Äù).\n\nThe model predicts both classes well since both labels have low scores.\nDepending the threshold, maybe the model could be relatively agnostic (e.g.¬†predicted probabilites like 0.50-0.50, 0.60-0.40)\n\nBottom-right: predictions where both labels are statistically significant¬† (i.e.¬†outside the ‚Äúprediction interval‚Äù).\n\nModel totally whiffs. Confident it‚Äôs one label when it‚Äôs actually another.\n\nExample\n\n1 (truth) - low predicted probability = high score -&gt; Red and significant\n0 - high predicted probability = high score -&gt; Red and significant\n\n\n\nTop-right: predictions where all 0 labels are not statistically significant.\n\nModel predicted the 0=class well (i.e.¬†low scores) but the 1-class poorly (i.e.¬†high scores)\n\nBottom-left: predictions where all 1 labels are not statistically significant. Here, the model predicted that 1 is the true class.\n\nVice versa of top-right",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/big-data.html",
    "href": "qmd/big-data.html",
    "title": "Big Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-misc",
    "href": "qmd/big-data.html#sec-bgdat-misc",
    "title": "Big Data",
    "section": "",
    "text": "RcppArmadillo::fastLmPure Not sure what this does but it‚Äôs rcpp so maybe faster than lm for big data.\n.lm.fit is a base R lm function that is 30%-40% faster than lm.",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-hghperf",
    "href": "qmd/big-data.html#sec-bgdat-hghperf",
    "title": "Big Data",
    "section": "High Performance",
    "text": "High Performance\n\n{rpolars}: arrow product; uses SIMD which is a low-level vectorization that can be used to speed up simple operations like addition, subtraction, division, and multiplication\n\nResources\n\nCookbook Polars for R\n\nAlso see collapse &gt;&gt; vs arrow/polars\nExample: Read and Summarize\ndf &lt;- pl$scan_csv(file_name)$\n    group_by(\"state\")$\n    agg(\n        pl$\n          col(\"measurement\")$\n          min()$\n          alias(\"min_m\"),\n        pl$\n          col(\"measurement\")$\n          max()$\n          alias(\"max_m\"),\n        pl$\n          col(\"measurement\")$\n          mean()$\n          alias(\"mean_m\")\n    )$\n    collect()\n\nFastest at this operation according to this benchmark\n\nExample: groupby state + min, max, mean\n# polars sql\nlf &lt;- polars::pl$LazyFrame(D) \npolars::pl$SQLContext(frame = lf)$execute(\n  \"select min(measurement) as min_m, \n          max(measurement) as max_m, \n          avg(measurement) as mean_m \n  from frame \n  group by state\")$collect()\n\n# polars\npolars::pl$\n  DataFrame(D)$\n  group_by(\"state\")$\n  agg(polars::pl$\n        col(\"measurement\")$\n        min()$alias(\"min_m\"),\n      polars::pl$\n        col(\"measurement\")$\n        max()$alias(\"max_m\"),\n      polars::pl$\n        col(\"measurement\")$\n        mean()$alias(\"mean_m\"))\n\n{collapse}: Fast grouped & weighted statistical computations, time series and panel data transformations, list-processing, data manipulation functions, summary statistics and various utilities such as support for variable labels. Class-agnostic framework designed to work with vectors, matrices, data frames, lists and related classes i.e.¬†xts, data.table, tibble, pdata.frame, sf.\n\noptions(collapse_mask = \"all\")\nlibrary(collapse)\n\nCode chunk above can optimize any script. No other changes necessary. Quick demo.\nvs arrow/polars (benchmark)\n\nDepends on the data/groups ratio\n\nIf you have ‚Äúmany groups and little data in each group‚Äù then use collapse\n\nIf your calculations involve ‚Äúmore complex statistics algorithms like the median (involving selection) or mode or distinct value count (involving hashing)(cannot, to my knowledge, benefit from SIMD)‚Äù then use collapse.\n\nExample: groupby state + min, max, mean\nD |&gt;\n  fgroup_by(state) |&gt; \n  fsummarise(min = fmin(measurement), \n             max = fmax(measurement), \n             mean = fmean(measurement)) |&gt;\n  fungroup()\n\n{r2c}: Fast grouped statistical computation; currently limited to a few functions, sometimes faster than {collapse}\n{data.table}: Enhanced data frame class with concise data manipulation framework offering powerful aggregation, extremely flexible split-apply-combine computing, reshaping, joins, rolling statistics, set operations on tables, fast csv read/write, and various utilities such as transposition of data.\n\nExample: groupby state + min, max, mean\nD[ ,.(mean = mean(measurement),\n      min = min(measurement),\n      max = max(measurement)),\n   by=state]\n\n# Supposedly faster\nrbindlist(lapply(unique(D$state), \n                 \\(x) data.table(state = x, \n                                 y[state == x, \n                                   .(mean(measurement), \n                                     min(measurement), \n                                     max(measurement))\n                                   ]\n                                 )))\n\n{rfast}: A collection of fast (utility) functions for data analysis. Column- and row- wise means, medians, variances, minimums, maximums, many t, F and G-square tests, many regressions (normal, logistic, Poisson), are some of the many fast functions\n\nThe vast majority of the functions accept matrices only, not data.frames.\nDo not have matrices or vectors with have missing data (i.e NAs). There are no checks and C++ internally transforms them into zeros (0), so you may get wrong results.\nExample: groupby state + min, max, mean\nlev_int &lt;- as.numeric(D$state)\nminmax &lt;- Rfast::group(D$measurement, lev_int, method = \"min.max\")\ndata.frame(\n    state = levels(D$state),\n    mean = Rfast::group(D$measurement, lev_int, method = \"mean\"),\n    min = minmax[1, ],\n    max = minmax[2, ]\n)\n\n{matrixStats}: Efficient row-and column-wise (weighted) statistics on matrices and vectors, including computations on subsets of rows and columns.\n{kit}: Fast vectorized and nested switches, some parallel (row-wise) statistics, and some utilities such as efficient partial sorting and unique values.\n{fst}: A compressed data file format that is very fast to read and write. Full random access in both rows and columns allows reading subsets from a ‚Äò.fst‚Äô file.",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-lgmem",
    "href": "qmd/big-data.html#sec-bgdat-lgmem",
    "title": "Big Data",
    "section": "Larger than Memory",
    "text": "Larger than Memory\n\nOnly work with a sample of the data\n\nRandom sample in CLI\n\nSee binder for code\nAlso this snippet from Healy for a zipped csv.\n\n\nImproved version\ngzip -cd giantfile.csv.gz | (read HEADER; echo $HEADER; perl -ne 'print if (rand() &lt; 0.001)‚Äô) &gt; sample.csv\n\nRemoves the need to decompress the file twice, adds the header row, and removes the risk of a double header row\n\n\n\nOnly read the first n lines\n\nset n_max arg in readr::read_*\n\n\ndatasette.io - App for exploring and publishing data. It helps people take data of any shape, analyze and explore it, and publish it as an interactive website and accompanying API.\n\nWell documented, many plugins\n\nRill - A tool for effortlessly transforming data sets into powerful, opinionated dashboards using SQL.\n\nDocs, Example Projects\nPowered by Sveltekit & DuckDB = conversation-fast, not wait-ten-seconds-for-result-set fast\nWorks with your local and remote datasets ‚Äì imports and exports Parquet and CSV (s3, gcs, https, local)\nNo more data analysis ‚Äúside-quests‚Äù ‚Äì helps you build intuition about your dataset through automatic profiling\nNo ‚Äúrun query‚Äù button required ‚Äì responds to each keystroke by re-profiling the resulting dataset\nRadically simple interactive dashboards ‚Äì thoughtful, opinionated, interactive dashboard defaults to help you quickly derive insights from your data\nDashboards as code ‚Äì each step from data to dashboard has versioning, Git sharing, and easy project rehydration\n\nOnline duckdb shell for parquet files (gist, https://shell.duckdb.org/)\nselect max(wind)¬†\nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/gridwatch/gridwatch_2023-01-08.parquet';\n-- Takes 6 seconds on the first query, 200ms on subsequent similar queries\n\nselect *¬†\nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/NSPL/NSPL.parquet'¬†\nwhere pcd = 'SW1A1AA';\n-- Takes 13 seconds on the first query, 100ms on subsequent similar queries\nCSV Editors\n\nFor editing or reformatting cells\nPopular spreadsheet programs like googlesheets (100MB) and excel (25MB online) have file size limits and they‚Äôre slow to upload to. The following programs are free(-ish) local alternatives only limited by your RAM.\nSuggest for files over a few hundred MBs that you open as Read-Only\n\nOpening the files as ‚ÄúEditable‚Äù will probably balloon the memory cost to at least 5 times the file size. (e.g.¬†350MB csv \\(\\rightarrow\\) 2GB RAM)\n\nModern CSV - Nice modern interface, read-only mode that can open large csvs (100s of MBs) without making much of a dent in your RAM, fully featured (moreso if you pay a small-ish one time fee)\n\nDocs, Feature free/upgrade list\nStill has some functionality in read-only mode (e.g.¬†search, sort)\n\nOpenRefine - Has read-only, Several add-ons, Completely open source.\n\nDocs, List of Extensions\nNo functionality when read-only (must create a project to do anything) ‚Äî just reading\nStarts with a 1024 MB RAM usage limit which is proably fine for editing around a 100MB csv. Need to set the limit higher in a config file in order to edit larger files.\nOnce you create a project, I think it has some editing features that you‚Äôd have to pay for with Modern CV.\nOpens other file formats besides csv (e.g.¬†xlsx, xml, json, etc)\n\n\ncsvkit - suite of command-line tools for converting to and working with CSV\n\nInstallation docs\n\nOne of the articles your terminal has to be a bash terminal but I dunno\n\nIf so, they recommend cmder or enabling the Linux subsystem with WSL2.\n\n\nNotes from\n\nArticle with additional examples and options\n\nFeatures\n\nPrint CSV files out nicely formatted\nCut out specific columns\nGet statistical information about columns\n\nConvert excel files to CSV files:\nin2csv excel_file.xlsx &gt; new_file.csv\n# +remove .xlsx file\nin2csv excel_file.xlsx &gt; new_file.csv && rm excel_file\nSearch within columns with regular expressions:\ncsvgrep -c county -m \"HOLT\" new_file.csv\n# subset of columns (might be faster) with pretty formatting\ncsvcut -c county,total_cost new_file.csv | csvgrep -c county -m \"HOLT\" | csvlook\n\nSearches for ‚ÄúHOLT‚Äù in the ‚Äúcounty‚Äù column\n\nQuery with SQL\n\nsyntax csvsql --query \"ENTER YOUR SQL QUERY HERE\" FILE_NAME.csv\nExample\n\n\nView top lines: head new_file.csv\nView columns names: csvcut -n new_file.csv\nSelect specific columns: csvcut -c county,total_cost,ship_date new_file.csv\n\nWith pretty output: csvcut -c county,total_cost,ship_date new_file.csv | csvlook\nCan also use column indexes instead of names\n\nJoin 2 files: csvjoin -c cf data1.csv data2.csv &gt; joined.csv\n\n‚Äúcf‚Äù is the common column between the 2 files\n\nEDA-type stats:\ncsvstat new_file.csv\n# subset of columns\ncsvcut -c total_cost,ship_date new_file.csv | csvstat\n\nJSONata - a lightweight, open-source query and transformation language for JSON data, inspired by the ‚Äòlocation path‚Äô semantics of XPath 3.1.\n\nMisc\n\nNotes from: Hrbrmstr‚Äôs article\nJSONata also doesn‚Äôt throw errors for non-existing data in the input document. If during the navigation of the location path, a field is not found, then the expression returns nothing.\n\nThis can be beneficial in certain scenarios where the structure of the input JSON can vary and doesn‚Äôt always contain the same fields.\n\nTreats single values and arrays containing a single value as equivalent\nBoth JSONata and¬†jq¬†can work in the browser (JSONata embedding code, demo), but¬†jq¬†has a slight speed edge thanks to WASM. However, said edge comes at the cost of a slow-first-start\n\nFeatures\n\nDeclarative syntax that is pretty easy to read and write, which allows us to focus on the desired output rather than the procedural steps required to achieve it\nBuilt-in operators and functions for manipulating and combining data, making it easier to perform complex transformations without writing custom code in a traditional programming language like python or javascript\nUser-defined functions that let us extend JSONata‚Äôs capabilities and tailor it to our specific needs\nFlexible output structure that lets us format query results into pretty much any output type\n\n\njq + jsonlite - json files\njsoncrack.com - online editor/tool to visualize nested json (or regular json)\njj - cli tool for nested json. Full support for ndjson as well as setting/updating/deleting values. Plus it lets you perform similar pretty/ugly printing that jq does.\nsqlite3 - CLI utility allows the user to manually enter and execute SQL statements against an SQLite database or against a ZIP archive.\n\nalso directly against csv files (post)\n\ntextql - Execute SQL against structured text like CSV or TSV\n\nRequire Go language installed\nOnly for Macs or running a docker image\n\ncolumnq-cli - sql query json, csv, parquet, arrow, and more\nfread + CLI tools\n\nFor large csvs and fixing large csv with jacked-up formating see article, RBlogger version\n\n{arrow}\n\nconvert file into parquet files\n\npass the file path to open_dataset, use group_by to partition the Dataset into manageable chunks\nuse write_datasetto write each chunk to a separate Parquet file‚Äîall without needing to read the full CSV file into R\n\ndplyr support\n\nmultiplyr\n\nOption for data &gt; 10M rows and you only have access to one machine\nSpreads data over local cores\n\n{sparklyr}\n\nspin up a spark cluster\ndplyr support\nSet-up a cloud bucket and load data into it. Then, read into a local spark cluster. Process data.\n\n{h2o}\n\nh2o.import_file(path=path) holds data in the h2o cluster and not in memory\n\n{disk.frame}\n\nsupports many dplyr verbs\nsupports¬† future package to take advantage of multi-core CPUs but single machine focused\nstate-of-the-art data storage techniques such as fast data compression, and random access to rows and columns provided by the fst package to provide superior data manipulation speeds\n\nMatrix ops\n\nsee bkmks: mathematics &gt;&gt; packages\n\n{ff}\n\nsee bkmks: data &gt;&gt; loading/saving/memory\nThink it converts files to a ff file type, then you load them and use ffapply to perform row and column operations with base R functions and expressions\nmay not handle character and factor types but may work with {bit} pkg to solve this",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-viz",
    "href": "qmd/big-data.html#sec-bgdat-viz",
    "title": "Big Data",
    "section": "Viz",
    "text": "Viz\n\nScatter¬†plots\n\n{scattermore}, {ggpointdensity}\n{ggrastr}\n\nRasterize only specific layers of a ggplot2 plot (for instance, large scatter plots with many points) while keeping all labels and text in vector format. This allows users to keep plots within a reasonable size limit without losing the vector properties of scale-sensitive information.\ngithub; tweet\n\n\nH2O\n\nh2o.aggregator Reduces data size to a representive sample, then you can visualize a clustering-based method for reducing a numerical/categorical dataset into a dataset with fewer rows A count column is added to show how many rows is represented by the exemplar row (I think)\n\nAggregator maintains outliers as outliers but lumps together dense clusters into exemplars with an attached count column showing the member points.\nFor cat vars:\n\nAccumulate the category frequencies.\nFor the top 1,000 or fewer categories (by frequency), generate dummy variables (called one-hot encoding by ML people, called dummy coding by statisticians).\nCalculate the first eigenvector of the covariance matrix of these dummy variables.\nReplace the row values on the categorical column with the value from the eigenvector corresponding to the dummy values.\n\ndocs; article\n\n\n{dbplot}\n\nplots data that are in databases\n\nAlso able to plot data within a spark cluster\n\ndocs\n\nObservableHQ\n\n{{{deepscatter}}}\n\nThread (using Arrow, duckdb)",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/bayes-workflow.html",
    "href": "qmd/bayes-workflow.html",
    "title": "Workflow",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Bayes",
      "Workflow"
    ]
  },
  {
    "objectID": "qmd/bayes-workflow.html#sec-bayes-wkflw-misc",
    "href": "qmd/bayes-workflow.html#sec-bayes-wkflw-misc",
    "title": "Workflow",
    "section": "",
    "text": "Also see Model Building, Concepts &gt;&gt; Misc &gt;&gt; Regression Workflow\nNotes from\n\nBayesian Workflow (Gelman, Vehtari) (arXiv link)\n\nResources\n\nVehtari Video: On Bayesian Workflow (2022) (Based on the paper, but I haven‚Äôt watched it, yet)\nNabiximols treatment efficiency: Vehtari‚Äôs example of applyijng his workflow in the context of comparing continuous and discrete observation models.\n\nCurrent Checklist\n\nCheck convergence diagnostics\nDo posterior predictive checking\nCheck residual plots\nModel comparison (if prediction)\n\nAnalysis Checklist (Thread)\n\nA suitably flexible Bayesian regression adjustment model,\nChosen by cross-validation/LOO,\nIncluding Gaussian processes for the unit-level effects over time (and space/network if relevant),\nImputation of missing data, and\nInformative priors for biases in the data collection process.",
    "crumbs": [
      "Bayes",
      "Workflow"
    ]
  },
  {
    "objectID": "qmd/glossary-ds-terms.html",
    "href": "qmd/glossary-ds-terms.html",
    "title": "Glossary: DS terms",
    "section": "",
    "text": "200 Status - An API serving an ML model returns a HTTP 200 OK success status response code indicates that the request has succeeded.\nAMI - amazon machine image. Thing that has R and the main¬†packages you need to load onto the cloud server\nAnti-Patterns - certain patterns in software development that are considered bad programming practices.\n\nAs opposed to design patterns which are common approaches to common problems which have been formalized and are generally considered a good development practice, anti-patterns are the opposite and are undesirable.\n\nArm -¬†a group of patients receiving a specific treatment (or no treatment). Trials involving several arms, or randomized trials, treat randomly-selected groups of patients with different therapies in order to compare their medical outcomes. Experimental arms, which receive an experimental drug, are compared with control arms.¬†Single-arm or non-randomized trials, in which everyone enrolled in a trial receives the experimental therapy\nArtifacts - objects that are created as a result of a process. e.g.¬†model objects, cleaned data sets, visuals, etc.\nAsynchronous Programming - code runs (or must run) after something else happens and also not sequentially (e.g.¬†when a function calls a callback function in JS).\nAthena - amazon query service that works with S3. Best for analyses using kubernetes. ODBC drivers are best with interactive app\nB2C, B2B - business-to-consumer, business-to-business, describes a business that‚Äôs end-product is being sold to a consumer or a business.\nBalanced Design (aka orthogonal) has an equal number of observations for all possible level combinations. For example in an experiment where gender is an independent variable, an equal number of males receive the treatment as do females receive treatment. If the male/female counts were unequal, then the experiment is unbalanced.\n\nStat tests have greater power for balanced designs\nTest stat less susceptible to to small departures from the assumption of equal variances (homoscedasticity).\n\nBatch - collect a large number of data points, process them periodically and store results somewhere (contrasts with real-time in which a data input leads to an immediate prediction)\nBootstrapping (CS) - usually applies to a situation where a system depends on itself to start, sort of a chicken and egg problem. (e.g.¬†How do you start an OS initialization process if you don‚Äôt have the OS running yet?) Typically a simple file that starts a large process.\nBounce, Email - When an email cannot be delivered to an email server.\n\nHard Bounce - indicates a permanent reason an email cannot be delivered (e.g.¬†Recipient email address doesn‚Äôt exist; Recipient email server has completely blocked delivery)\nSoft Bounce - indicates a temporary delivery issue (for details on the reasons, see link)\n\nBounce Rate - the percentage of visitors to a particular website who navigate away from the site after viewing only one page. Low bounce rate can indicate the landing page needs improvement\nBPI - Business process improvement is a management exercise in which enterprise leaders use various methodologies to analyze their procedures to identify areas where they can improve accuracy, effectiveness and/or efficiency and then redesign those processes to realize the improvements.\nBLUE - best linear unbiased estimator, e.g.¬†regression line\nCAC - customer acquisition cost - measures how much an organization spends to acquire new customers. The total cost of sales and marketing efforts, as well as property or equipment, needed to convince a customer to buy a product or service.\nCapEx - Capital Expenditure - 1 of 2 main forward budgeting mechanisms for a corporation (also see OpEx). Often used to undertake new projects or investments or large-scale asset acquisitions (buildings and vehicles)\nClinical Trial - research studies (e.g.¬†RCT) performed in people that are aimed at evaluating a medical, surgical, or behavioral intervention\nCDI - Customer Data Infrastructure - built to collect behavioral data from primary or first-party data sources, but some solutions also support a handful of secondary data sources (third-party tools)\nCDP - Customer Data Platform - add-ons from CDI vendors; a layer on top of CDI that offers a set of capabilities to analyze data using a visual interface.\nCDN - content delivery network - a system of distributed servers (network) that deliver pages and other web content to a user, based on the geographic locations of the user, the origin of the webpage and the content delivery server.\nCLV/CLTV - Customer Lifetime Value - how much money a customer will bring your brand throughout their entire time as a paying customer.\nCOGS - Cost of goods sold (aka Cost of Sales) - refers to the direct costs of producing the goods sold by a company. This amount includes the cost of the materials and labor directly used to create the good. It excludes indirect expenses, such as distribution costs and sales force costs.\nComplete Factorial Design - a research study involving two or more independent variables in which every possible combination of the levels of each variable is represented. For instance, in a study of two drug treatments, one (A) having two dosages and the other (B) having three dosages, a complete factorial design would pair the dosages administered to different individuals or groups of participants as follows: A1 with B1, A1 with B2, A1 with B3, A2 with B1, A2 with B2, and A2 with B3.\nCPG - Consumer packaged goods are items used daily by average consumers that require routine replacement or replenishment, such as food, beverages, clothes, tobacco, makeup, and household products.\nCPC - Cost Per Click¬†- refers to the cost an advertiser pays each time an online user clicks on his or her digital ad\nCRM - customer relationship management i.e.¬†customer service. Salesforce tracks this data. Example: what features your salesperson promised, and when? How much revenue you have from each customer? Or which salesperson sold the most in the past year?\ncron- standard tool used on Unix and Unix-like systems to schedule the periodic execution in the background of a command or script (like a batch script)\nCrossed Factors - when every category of one factor co-occurs in the design with every category of the other factor. In other words, there is at least one observation in every combination of categories for the two factors. (in contrast to ‚Äúnested factors‚Äù). As a consequence, interaction terms involving these two factors is allowed.\nCrossover Study - A type of clinical trial in which the study participants receive each treatment in a random order. With this type of study, every patient serves as his or her own control. Crossover studies are often used when researchers feel it would be difficult to recruit participants willing to risk going without a promising new treatment.\nCross-Section Data - randomly sampled data from a population. Like a survey. Aka observational data. See experimental data for comparison.\n\nPooled - differs from panel data in that it is observations of different subjects (instead of the same subjects) in different time periods.\nRolling - both the presence of an individual in the sample and the time at which the individual is included in the sample are determined randomly.\n\nCross-Tabs - section of survey analysis where the aggregated results are broken down by demography, party affiliation, etc.\nCTA - marketing term, call-to-action.¬†any device designed to prompt an immediate response or encourage an immediate sale; words or phrases that can be incorporated into sales scripts, advertising messages or web pages that encourage consumers to take prompt action\nCTR - click through rate: the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement. It is commonly used to measure the success of an online advertising campaign for a particular website as well as the effectiveness of email campaigns.\nCRM - Customer Relationship Management - acquiring new customers but especially about retaining existing ones\nDAU - daily active users, ex: daily avg # of registered users of the site over past 30 days\nDBA - Database Administrator is an admin role that understands the particular database technology and how to get the best out of it. This includes improving performance, backups and recovery.\nDDL - Data definition or description language - Subset of SQL. Used to:\n\nKeep a snapshot of the database structure\nSet up a test system where the database acts like the production system but contains no data\nProduce templates for new objects that you can create based on existing ones. For example, generate the DDL for the Customer table, then edit the DDL to create the table Customer_New with the same schema.\n\nDesparate Impact Analysis - Analysis of the result of the application of a standard, requirement, test or other screening tool used for selection that‚Äîthough appearing neutral‚Äîhas an adverse effect on individuals who belong to a legally protected class Differential Dropout**]{style=‚Äòcolor: #009499‚Äô} -¬†Differing dropout rates between treatment arms\nDMA - Designated Market Area; a geographic region where Nielsen, the ratings company, analyzes and quantifies how television is viewed. Residents can receive the same local TV and radio stations\nDNS -¬†¬†Domain Name System**]{style=‚Äòcolor: #009499‚Äô} -¬† translates domain names to IP addresses so browsers can load Internet resources.\nDSL - domain-specific language - a computer language specialized to a particular application domain\nEMR - Amazon version of a spark cluster used for big data processing and analysis.\nEndogenous - A model variable is correlated with other variables excluded from the model (omitted variable bias). Determined by measuring the correlation between the variable and residuals of the model. If a predictor variable hasn‚Äôt been randomly assigned, it‚Äôs likely to be endogenous.\nEquitability - concept that says¬†a dependence measure should give equal importance to linear and nonlinear relationships. Consistent strength measurements across different variable relationships that have similar amounts of noise.\nERP - enterprise resource planning, sort of a catch-all for manufacturing, supply-chain, etc, see the wiki\nETL - extract, transfer, load - usually refers to transferring data from one location to another\nEndpoint (biostats) - Outcome variable measured in a medical study. e.g.¬†Death, stroke, or quality of life are good endpoints. Blood tests and images on scans are not good endpoints.\n\nA composite endpoint is one that consists of two or more events\n\nExample: death due to cardiovascular causes or hospitalization due to heart failure\n\nSo the binary outcome would be a 1 if either of those events took place or a 0 if they did not. Or in a survival model, time until either of those events.\n\n\n\nEOF - End of file - Input from a terminal never really ‚Äúends‚Äù (unless the device is disconnected), but it is useful to enter more than one ‚Äúfile‚Äù into a terminal, so a key sequence is reserved to indicate end of input.\nex ante - based on assumption and prediction and being essentially subjective and estimative\nex post - based on knowledge and retrospection and being essentially objective and factual\nExperimental Data - data from a RCE/RCT. Compare with observational data\nFaaS - Function as a service - type of cloud service for developing, running, and managing apps (e.g.¬†AWS Lambda)\nFactorial Design - Experiment where you‚Äôre interested in the effect of two or more independent variables.\nFraud Rules - fraud scores are calculated based on rules, which add or subtract points. The user action may be a transaction, signup or login. Rules look at data points such as an email address, IP address, or social media presence.\nFraud Score - assigned values to how risky a user action is. Scoring determined by fraud rules.\nFuzzy Design - See Sharp Design\nGHA - Github Actions\nGMV - Gross merchandises value - the total value of merchandise sold over a given period of time through a customer-to-customer (C2C) exchange site\nGRP - Gross Rating Point. A standard measure in advertising, it measures advertising impact. You calculate it as a percent of the target market reached multiplied by the exposure frequency. Thus, if you get advertise to 30% of the target market and give them 4 exposures, you would have 120 GRP.\nHTE - Heterogeneous Treatment Effect - Also called differential treatment effect, includes difference of means, odds ratios, and Hazard ratios for time-to-event outcome vars\n\nAscertaining subpopulations for which a treatment is most beneficial (or harmful) is an important goal of many clinical trials.\nOutcome heterogeneity is due to wide distributions of baseline prognostic factors. When strong risk factors exist, there is hetergeneity in the outcome variable.\n\nSolution: add baseline predictors to your model that account for these strong risk factors.\n\nHeterogeneity of Treatment Effects - The degree to which different treatments have differential causal effects on each unit.\n\nHit Ratio - percent of records that were read in order to complete a query in a database. Cloud db providers often charge by the number of records searched\nHomogeneity of Treatment Effects - See Heterogeneity of Treatment Effects\nHPC - High Performance Computing\nHoneypot - data (for example, in a network site) that appears to be a legitimate part of the site, but is actually isolated and monitored, and that seems to contain information or a resource of value to attackers, who are then blocked.\nIaaS - infrastructure-as-a-service ( Hardware is provided by an external provider and managed for you)\nIAM - identity and access management, keys and passwords etc\nIRB - institutional review board, reviews studies ethical and moral issues\nITT - Intent-to-Treat analysis¬†includes all randomized patients in the groups to which they were randomly assigned, regardless of their adherence with the entry criteria, regardless of the treatment they actually received, and regardless of subsequent withdrawal from treatment or deviation from the protocol. Avoids overoptimistic estimates of the efficacy of an intervention resulting from the removal of non-compliers by accepting that noncompliance and protocol deviations are likely to occur in actual clinical practice. So mimics likely situation in the real world, but not good for estimating the causal effect of a treatment.\nKernels - (article) - system kernels - the interface between the operating system, i.e.¬†the software, and the hardware components in a device. It is used in all devices with an operating system, for example, computers, laptops, smartphones, smartwatches, etc.\n\nWhen we use a program on a computer, such as Excel, we handle it on the so-called Graphical User Interface (GUI). The program converts every button click or other action into machine code and sends it to the operating system kernel. If we want to add a new column in an Excel table, this call goes to the system core. This in turn passes the call on to the computer processing unit (CPU), which executes the action.\nJupyter Kernels - an engine that executes notebook code and is specific to a particular programming language (e.g.¬†python kernel)\nKaggle Kernels - a free platform from Kaggle to run Jupyter notebooks in the browser. Advantage is that you don‚Äôt have to set-up an environment locally.\n\nKPI- key performance indicator\nKYC - Know-Your-Customer is info a company collects to verify your identity to combat fraud. Used by telecoms and financial services\nLazy Evaluation - ‚Äù never pulls data into R unless you explicitly ask for it. It delays doing any work until the last possible moment. It collects together everything you want to do and then sends it to the database in one step.‚Äù\nLikelihood - probability of seeing this data given a specific value for a distribution parameter (eg mean, sd). Goal is to search for parameter values until the likelihood is maximized.\nLOB - Line of Business is a general term which refers to a product or a set of related products that serve a particular customer transaction or business need. (i.e.¬†product categories)\n\nExamples\n\nConsumer Banking: credit cards, line of credit or loan program, mortgages, and corporate, small business and personal bank accounts.\nFinancial services and brokerages: mergers and acquisitions or partnerships, real estate investments, and wealth management\nProperty and casualty insurance companies: property and casualty insurance (i.e., homeowners, car, boat, renters, etc.), life insurance, health insurance, and commercial business insurance.\n\nSub-lines of Business would be sub-categories within each LOB\n\nLongitudinal Data - see panel data\nLTV - see CLV/CLTV\nManual Review - A human is reviews the case to determine whether action is needed. In fraud, an model output may trigger a ‚Äúmanual review‚Äù to determine whether an event was indeed fraudulent.\nMLlib - Apache Spark machine learning library\nMVC - Minimum Viable Corpus - a data size threshold; such that below this threshold, the data simply isn‚Äôt useful/valuable. Used in data products business.\nMVP - minimum viable project, agile term. Version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort\nNamespace - allows you to use two functions with the same name but from different packages, e.g.¬†dplyr::select or in general, package::function. https://stackoverflow.com/questions/3384204/what-are-namespaces/3384384#3384384\nNNH - Numbers Needed to Harm - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a particular adverse outcome. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNNT - Numbers Needed to Treat - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a favorable outcome such as treatment response. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNPS - Net Promoter Score - a measure of customer loyalty. Widely used market research metric that typically takes the form of a single survey question asking respondents to rate the likelihood that they would recommend a company, product, or a service to a friend or colleague.\nNRT - near real-time, aka streaming data\nObservational Data - see cross sectional data\nOEM - original equipment manufacturer\nOKR - Objectives and Key Results is a popular management strategy for goal setting within organizations. A framework for turning strategic intent into measurable outcomes for an organization.\nOnline Machine Learning - A method of machine learning where the model incrementally learns from a stream of data points in real-time. It‚Äôs a dynamic process that adapts its predictive algorithm over time, allowing the model to change as new data arrives.\nOn-Prem - on-premises ‚Äî working with servers in the the building and not in the cloud.\nOOD - out-of-distribution - data which differ from the training data and on which a model might underperform\nOpen Cohort - subjects can leave or be added over time.\nOpEx - Operational Expenditures - 1 of 2 main forward budgeting mechanisms for a corporation (also see CapEx). Relates to day-to-day expenses (such as payroll and software subscriptions). Smaller payouts over time.\nOpportunity Sizing - Quantitative analysis to select a subset of ideas to which to devote resources in product development\nNested Factors - happens when all the levels of one factor only occur in combination with one level of another factor (in contrast to ‚Äúcrossed factors‚Äù). As a consequence, your model can‚Äôt have an interaction term involving these two variables.\nP&L - Profit and Loss Statement Panel data - cross section data with a time element. Repeated measures of the same subject over time. Synonym for Longitudinal Data\nParcel - a land record that defines the boundary of a piece of land. These boundaries are the basic administrative unit of local government in regards to land and property. Managing ownership and tax records are the primary reason local governments generate these files. So these are boundaries differentiating ownership of properties.\nPEP8 - style guide for python\nPI - principal investigator\nPivot Table - Excel name for a group_by %\\&gt;% summarize calculation\n\ne.g.¬†from a table of individual fruit sales: group_by(fruit_type, country) %\\&gt;% summarize(total_amt = sum(amount))\n\nPLG - Product-led growth is an end user-focused growth model that relies on the product itself as the primary driver of customer acquisition, conversion, and expansion. e.g.¬†open source a product, let the customer go through the documentation and use and experiment with the product on their own time. In contrast to sales pitching a product to a customer and letting them use it for a trial basis.\nPM - product manager\nPoC - Proof of Concept\nPOS - point of sale, The point of sale or point of purchase is the time and place where a retail transaction is completed. It can be in a physical store, where POS terminals and systems are used to process card payments or a virtual sales point such as a computer or mobile electronic device.\nRCE - randomized controlled experiment, subjects randomly assigned to two groups, treatment and control. Double blind means the researcher doesn‚Äôt know who is in which group.\nRCT - randomized clinical trial\nRDD - Regression discontinuity design\nRedis - REmote DIctionary Server - is an in-memory, key-value database, commonly referred to as a data structure server. Used when volume of read and write operations exceed the capabilities of traditional databases. With Redis‚Äôs capability to easily persist the data to disk, it is a superior alternative to the traditional memcached solution for caching.\nRefactoring - updating or optimizing code\nRegression Testing - checks if changes made to a system negatively impacted or broke any of the existing features. It is often performed right after each update or commit to the code base to identify new bugs and ensure that your system works properly.\nRFI - Request for Information - Used to collect written information about the capabilities of various suppliers. Normally it follows a format that can be used for comparative purposes. An RFI is primarily used to gather information to help make a decision on what steps to take next. RFIs are therefore seldom the final stage and are instead often used in combination with request for proposal (RFP), request for tender (RFT), and request for quotation (RFQ).\nRFM - recency, frequency, monetary value - method of estimating customer value; common in retail\nRFP - Request for Proposal - A document that an organization, often a government agency or large enterprise, posts to elicit a response ‚Äì a formal bid ‚Äì from potential vendors for a desired solution. The RFP specifies what the customer is looking for and describes each evaluation criterion on which a vendor‚Äôs proposal will be assessed.\n\nROAS - return on ad spend\nRUG - Regional User Group\nS3 - Amazon simple storage service, database\nSaaS - Software-as-a-service is a mechanism through which companies offer the functionality of their apps, which remain on their company servers, to other companies or customers.\nSCO - sales cycle optimization, active process of providing content on your site (and beyond) that speaks to each of the key phases\nSEO - Search engine optimization, generating high page rankings for key search terms\nSDK - software development kit\nSharp Design - Each individual or group receives the same ‚Äúamount‚Äù of treatment (e.g.¬†a state law or medication dosage). Opposite being fuzzy design (?)\nSKU - Stock Keeping Unit**]{style=‚Äòcolor: #009499‚Äô} - Usually a bar code that has all the information to distinguish it from another product. These attributes can include manufacturer, description, material, size, color, packaging, and warranty terms. When a business takes inventory of its stock, it counts the quantity it has of each SKU.\nSLA - service level agreement - a contract between a service provider and its internal or external customers that documents what services the provider will furnish and defines the service standards this provider is obligated to meet. service. Important for holding prediction latency of an app to a certain standard or maintaining data reliability with vendors. (see link for more details on SLA, SLO, and SLI)\nSLI - service level indicators - metrics that measure compliance with an SLO (see link for more details on SLA, SLO, and SLI)\nSLO - service level objectives - objectives your team must meet in order to meet the conditions of the SLA (see link for more details on SLA, SLO, and SLI)\nSMB - (small to medium-sized business) generally defined as companies with fewer than 1000 employees and less than $1 billion in annual revenue.\nSME - Subject Matter Experts\nSPC - Statistical process control is a method of quality control which employs statistical methods to monitor and control a process\nSpill - missed opportunity metric, measures ‚Äúlost trading days‚Äù on which flights or hotels filled too quickly (the result of pricing too low)\nSpoil - missed opportunity metric, measures empty seats or rooms (often the result of pricing too high)\nSSH - secure shell is a cryptographic Network protocol for operating Network Services securely over an unsecured Network. Typical applications include remote command line login in remote command execution\nstdout - standard output, which is the terminal by default\nTDD - Test-driven development is a style of programming where coding, testing, and design are tightly interwoven\nTF-IDF- stands for term frequency-inverse document frequency, and is often used in information retrieval and text mining.\nThroughput - the amount of material or items passing through a system or process.\ntx - treatment, seen as variable with different treatments as values\nURI - Uniform Resource Identifier - a string of characters that unambiguously identifies a particular resource. e.g.¬†s3//bucket/path/to/folder or http://127.0.0.1:5000or c:\\Users\\me\\path\\to\\folder\nUTM - Urchin Traffic Monitor - used to identify marketing channels\n\ne.g.¬†http://yourwebsite.com/your-post-title/?utm_source=google\n\nutm code = string after ‚Äú?‚Äù\n\nThis person clicked a google ad to get to your site\n\nName comes from Urchin Tracker, a web analytics software that served as the base for Google Analytics.\n\nVPS - virtual private server\nWIP - Work-in-Progress\nWithin Person Study - multiple treatments on each person either all in the same period or different treatments in different periods\nYear-Over-Year - used to make comparisons between one time period and another that is one year earlier.\n\nFormula (percentage): (value_this_year / value_previous_year) - 1\nExample: (sales_Jul_2023 / sales_Jul_2022) - 1",
    "crumbs": [
      "Glossary: DS terms"
    ]
  },
  {
    "objectID": "qmd/python-general.html",
    "href": "qmd/python-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-misc",
    "href": "qmd/python-general.html#sec-py-gen-misc",
    "title": "General",
    "section": "",
    "text": "Tools (see article, article for installation and usage)\n\nruff - rust-based, linter and sorts imports\n\nFast, sensible default settings, focuses on more important things out of the box, and has less legacy burden\n\npydocstring - tool for checking compliance with Python docstring conventions\nblack - code formatter\nisort - sorts your imports\npytest, pytest-watch - unit tests\ncommitizen - guides you through a series of steps to create a commit message that conforms to the structure of a Conventional Commit\nnbQA - linting in jupyter notebooks\nmypy - type checker; good support and docs\npylance - checks type hinting in VSCode (see Functions &gt;&gt; Documentation &gt;&gt; Type Hinting)\ndoit - task runner; {targets}-like tool; tutorial\npre-commit - specify which checks you want to run against your code before committing changes to your git repository\nREADME templates - link\n\nPut as much config as possible into pyproject.toml. A lot of configurations tools will happily read from it, and it will give you one source of truth.\nAn underscore _ at the beginning is used to denote private variables in Python.\ndef set_temperature(self, value):\n¬† ¬† ¬† ¬† if value &lt; -273.15:\n¬† ¬† ¬† ¬† ¬† ¬† raise ValueError(\"Temperature below -273.15 is not possible.\")\n¬† ¬† ¬† ¬† self._temperature = value\n\nyou can still access ‚Äú_temperature‚Äù but it‚Äôs just meant for internal use by the class and the underscore indicates this\n\n{{warnings::warnings.filterwarnings(‚Äòignore‚Äô)}}\nsys.getsizeof(obj) to get the size of an object in memory.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#terms",
    "href": "qmd/python-general.html#terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nclasses - code template for creating objects, we can think of it as a blueprint. It describes the possible states and behaviors that every object of a certain type could have.\nobject - data structure storing information about the state and behavior of a certain entity and is an instance of a class\nstub file - a file containing a skeleton of the public interface of that Python module, including classes, variables, functions ‚Äì and most importantly, their types. (Source)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#base",
    "href": "qmd/python-general.html#base",
    "title": "General",
    "section": "Base",
    "text": "Base\n\nInfo method\n\nX.info()\nRemove an object: del\nCheck object type\n\ntype() : outputs the type of an object\nisinstance() : outputs type and inheritance of an object\nSee article for details on differences\n\nImport Libraries\nimport logging\nimport bentoml\nfrom transformers import (\n¬† ¬† SpeechT5Processor,\n¬† ¬† SpeechT5ForTextToSpeech,\n¬† ¬† SpeechT5HifiGan,\n¬† ¬† WhisperForConditionalGeneration,\n¬† ¬† WhisperProcessor,\n)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-fund",
    "href": "qmd/python-general.html#sec-py-gen-fund",
    "title": "General",
    "section": "Fundamentals",
    "text": "Fundamentals\n\nSlicing\n\nFormat my_list[start:stop:step]\n\n** start value is inclusive and the end value is exclusive\n\n1 element and more than 1 element\n\"Python\"[0] # P\n\"Python\"[0:1] # P\n\"Python\"[0:5] # Pytho\nillustrates how when using a range, the last element is exclusive\nNegative indexing my_list[0:-1]\n\nEverything but the last object\n\nSkip every second element\nmy_list = list(\"Python\")\nmy_list[0:len(my_list):2]\n&gt;&gt; ['P', 't', 'o']\nstart at 0, end at len(my_list), step = 2\nShortcuts\nmy_list[0:-1] == my_list[:-1]\nmy_list[0:len(my_list):2] == my_list[::2]\n\"Python\"[::-1] == \"Python\"[-1:-7:-1]\n\nDefaults\n\n0 for the start value\nlen(list) for the stop value\n1 for the step value\n\nDefaults for negative step value\n\n-1 for the start value\n-len(list) - 1 for the stop value\n\n\nAlias vs new object\nb = a # alias\nb = a[:] # new object\n\nWith the alias, changes to a will happen to b as well\n\nCommon use cases\n\n\n\n\n\n\n\nEvery element but the first and the last one\n[1:-1]\n\n\nEvery element in reverse order\n[::-1]\n\n\nEvery element but the first and the last one in reverse order\n[-2:0:-1]\n\n\nEvery second element but the first and the last one in reverse order\n[-2:0:-2]\n\n\n\nUsing slice function\nsequence = list(\"Python\")\nmy_slice = slice(None, None, 2) # equivalent to [::2]\nindices = my_slice.indices(len(sequence))\n&gt;&gt; (0, 6, 2)\n\nShows start = 0, stop = 6, step = 2\n\n\n\n\nF-Strings\n\nCheatsheet\nParameterize with {}\n&gt;&gt; x = 5\n&gt;&gt; f\"One icecream is worth [{x}]{style='color: #990000'} dollars\"\n'One icecream is worth 5 dollars'\n! - functions\n\n!r ‚Äî Shows the string delimiter, calls the repr() method.\n\nrepr‚Äôs goal is to be unambiguous and str‚Äôs is to be readable. For example, if we suspect a float has a small rounding error, repr will show us while str may not\n\n!a ‚Äî Shows the Ascii for the characters.\n!s ‚Äî Converts the value to a string.\n\nGuessing this the str() method (see !r for details)\n\n\nfood2brand = \"Mcdonalds\"\nfood2 = \"French fries\"\nf\"I like eating {food2brand} {food2!r}\"\n\"I like eating Mcdonalds 'French fries'\"\nChange format with ‚Äú:‚Äù\n&gt;&gt; import datetime\n&gt;&gt; date = datetime.datetime.utcnow()\n&gt;&gt; f\"The date is {date:%m-%Y %d}\"\n'The date is 02-2022 15'\nFormatting with ‚Äú&gt;‚Äù and ‚Äú&lt;‚Äù\n\n\n&lt;6 says width is 6 characters and text starts at the left edge\n&gt;10.2f says width is 10 characters, text starts the right hand edge, and number is rounded to 2 decimal places\n\n\n\n\nOperators\n\n(docs)\nExponential: 5**3\nInteger division: 5//3\nModulo: 5%3\nIdentity: is\nx = 5\ny = 3\nprint(\"The result for x is y is\", x is y)\nThe result for x is y is false\n\nThink you can also use == here too\n\nLogical: and and or\nprint(\"The result for 5 &gt; 3 and 6 &gt; 8 is\", 5 &gt; 3 and 6 &gt; 8)\nprint(\"The result for 5 &gt; 3 or 6 &gt; 8 is\", 5 &gt; 3 or 6 &gt; 8)\nThe result for 5 &gt; 3 and 6 &gt; 8 is False\nThe result for 5 &gt; 3 or 6 &gt; 8 is True\nSubset: in and not in\nprint(\"Is the number 3 in the list [1,2,3]?\", 3 in [1,2,3])\nIs the number 3 in the list [1,2,3]? True\n\nprint(\"Is the number 3 not in the list [1,2,3]?\", 3 not in [1,2,3])\nIs the number 3 not in the list [1,2,3]? False\nAssignment",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-dattyp",
    "href": "qmd/python-general.html#sec-py-gen-dattyp",
    "title": "General",
    "section": "Types",
    "text": "Types\n\nScalars\n\nCreate scalars by subsetting a list\ninputs = [1, 0.04, 0.9]\n# 1 numeric¬†\nrmse = inputs[0] # rmse = 1 and is type 'float'\n# multiple numerics\nrmse, mape, rsq = inputs\n\nTuples\n\nLists are mutable and tuples are not\n\ni.e.¬†we can add or remove elements to a list after we create it but we cannot do such thing to a tuple\n\nSyntax: name_of_tuple = (a, b)\n\nLists\n\nCreate list of objects (e.g.¬†floats)\nacc_values = [rmse, mape, rsq]\n\nalt method: asterisk-comma notation\n*acc_names, = \"RMSE\", \"MAPE\", \"R-SQ\"\n\nasterisk is ‚Äúunzipping operator‚Äù\n\n\nMake a copy\nold_list = [2, 3, 4]\nnew_list = list(old_list)\n\nDictionaries\n\n** if creating a simple dict, more performant to use curly braces **\n\nAvoid d = dict(1=1, x='x')\n\nJoin 2 dicts -¬† d.update(d2)\n\nIf d and d2 share keys, d2‚Äôs values for those keys will be used\n\nAccess a value from a key: sample_dict['key_name']\nMake a copy\nold_dict = {stuff: 2, more_stuff: 3}\nnew_dict = dict(old_dict)\nConvert list of tuples to a dict\nacc_dict = dict(acc_list)\n\nzip creates lists of tuples (See Loops &gt;&gt; zip section)\n\nAdd key, value pair to a dict\ntransaction_data['user_address'] = '221b Baker Street, London - UK'\n# or\ntransaction_data.update(user_address='221b Baker Street, London - UK')\nUnpack dict into separate tuples for key:value pairs\nrmse, mape, rsq = acc_dict.items()\nrmse\n('RMSE', 1)\n\n** fastest way to iterate over both keys and values in a dict **\ncan also use zip to unpack pairs into a list (see loops &gt;&gt; zip)\n\nUnpack dict into separate lists for keys and values\nacc_keys = list(acc_dict.keys())¬†\nacc_values = list(acc_dict.values())\n\n** fastest way to iterate over a dict‚Äôs keys or values **\n\nUnpack values from dicts into separate scalars\nrmse, mape, rsq = acc_dict.values()\nrmse\n1\nPull the value for a key (e.g.¬†k) or return the default value - d.get(k, default)\n\nDefault is ‚ÄúNone‚Äù. I think this can be set with d.setdefault(k, default)\n\nCheck for specific key (logical)\n‚Äòsend_currency‚Äô in transaction_data\n‚Äòsend_currency‚Äô in transaction_data.keys()\n‚Äòsend_currency‚Äô not in transaction_data.keys()\n\nLike %in% in R\n\nCheck for specific value (logical)\n‚ÄòGBP‚Äô in transaction_data.values()\nCheck for key, value pair\n(‚Äòsend_currency‚Äô, ‚ÄòGBP‚Äô) in transaction_data.items()\nPretty printing of dictionaries\n¬† ¬† _ = [print(k, \":\", f'{v:.1f}') for k,v in acc_dict.items()]\n¬† ¬† RMSE : 1.00\n¬† ¬† MAPE : 0.04\n¬† ¬† R-sq : 0.90\n\nfor-in loop format (see Loops &gt;&gt; Comprehension)\nprint returns ‚Äúnone‚Äù for each key:value at the bottom of the output for some reason. Assigning the print statement to a variable fixes it.\n\ndefaultdict\n\nCreates a key from a list element and groups the properties into a list of values where the value may also be a dict.\nFrom {{collections}}\nAlso see\n\nPybites video\nJSON &gt;&gt; Python &gt;&gt; Example: Parse Nested JSON into a dataframe\n\n\n\nSets\n\nIf performing set logic, always more performant to use sets instead of dicts or lists\n\n\nIf using numpy/pandas, using the .unique() syntax is more efficient for arrays/series‚Äô with numeric values\nIf using strings, it‚Äôs more efficient to use list(set(my_array))\n\n\nStrings\n\nOperators\nOperator Description\n%d Signed decimal integer\n%u unsigned decimal integer\n%c Character\n%s String\n%f Floating-point real number\nExample\n\nname = \"india\"\nage = 19\nmarks = 20.56\nstring1 = 'Hey %s' % (name)\nprint(string1)\nstring2 = 'my age is %d' % (age)\nprint(string2)\nstring3= 'Hey %s, my age is %d' % (name, age)\nprint(string3)\nstring3= 'Hey %s, my subject mark is %f' % (name, marks)\nprint(string3)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-pip",
    "href": "qmd/python-general.html#sec-py-gen-pip",
    "title": "General",
    "section": "pip",
    "text": "pip\n\nLooks for packages on https://pypi.org, downloads, and installs it\nMisc\n\nIf you installed python using the app-store, replace python with python3.\nDon‚Äôt use sudo to install libraries, since it will install things outside of the virtual environment.\nNor should you use ‚Äú‚Äìuser‚Äù, since it‚Äôs made to install things outside of the virtual environment.\nDon‚Äôt mix pip, venv and Anaconda. Avoid Anaconda if you can. If you have to use Anaconda, don‚Äôt use pip and venv. Limit yourself to Anaconda tools.\nIf you get SSL errors (common if you are in a hotel or a company network) use the ‚Äìtrusted-host pypi.org ‚Äìtrusted-host files.pythonhosted.org options with pip to work around the problem.\n\ne.g.¬†python -m pip install pendulum --trusted-host pypi.org --trusted-host files.pythonhosted.org\n\nIf you are behind a corporate proxy that requires authentication (common if you are in a company network), you can use the ‚Äìproxy option with pip to give the proxy address and your credentials.\n\ne.g.¬†python -m pip install pendulum --proxy http://your_username:yourpassword@proxy_address\nIt also works with the https_proxy environment variables\n\n\nInstall library\n$ python -m pip install &lt;library_name&gt;\n\n# inside ipython or a colab notebook, \"!\" signifies a shell command\n!pip install &lt;library_name&gt;\nInstall library from github\npython -m pip install git+https://github.com/bbalasub1/glmnet_python.git@1.0\n\n‚Äú@1.0‚Äù is the version number\n\nUninstall library\n$ python -m pip uninstall &lt;library_name&gt;\n\nWon‚Äôt uninstall the dependencies of this library.\nIf you wish to also uninstall the unused dependencies as well, take a look at pip-autoremove\n\nRemove all packages in environment\n$ python -m pip uninstall -y -r &lt;(pip freeze)\nRemove all packages in environment but write the names of the packages to a requirements.txt file first\n$ python -m pip freeze &gt; requirements.txt && python3 -m pip uninstall -r         requirements.txt -y\nInstall requirements.txt\n$ python -m pip install -r requirements.txt\nWrite names of all the packages in your environment to a requirement.txt file\n$ python -m pip freeze &gt; requirements.txt\n\nWrites the specific version of the packages that you have installed in your environment (e.g.¬†pandas==1.0.0)\n\nThis may not be what you always want, so you‚Äôll need to manually change to just the library name in that case (e.g.¬†pandas)\n\nOnly aware of the packages installed using the pip install command\n\ni.e.¬†any packages installed using a different approach such as peotry, setuptools, condaetc. won‚Äôt be included in the final requirements.txt file.\n\nDoes not account for dependency versioning conflicts\nSaves all packages in the environment including those that are not relevent to the project\nIf you are not using a virtual environment, pip freeze generates a requirement file containing all the libraries in including those beyond the scope of your project.\n\nList your installed libraries\n$ python -m pip list\nSee if you have a particular library installed\n$ python -m pip list | grep &lt;library_name&gt;\nGet library info (name, version, summary, license, dependencies and other)\n$ python -m pip show &lt;library_name&gt;\nCheck that all installed packages are compatible\n$ python -m pip check\nUpdate package\n$ python -m pip install package_name --upgrade\nSearch for PyPI libraries (pip source for libraries)\n$ python -m pip search &lt;search_term&gt;\n\nreturns all libraries matching search term\n\nDownload a package without installing it\npython -m pip download &lt;library name&gt;\n\nIt will download the package and all its dependencies in the current directory (the files, called, wheels, have a .whl extension).\nYou can then install them offline by doing python -m pip install on the wheels.\n\nBuild Wheel archives for the libraries and dependencies in your environment\n$ python -m pip wheel\n\nI think these are binaries, so they don‚Äôt need compiled if installed in a future environment\nReal Python Tutorial\n\nManage configuration\n$ python -m pip config &lt;action name&gt;\n\nActions: edit, get, list, set or unset\nExample\n$ python -m pip config set global.index-url https://your.global.index\n\nDisplay debug information specific to pip\n$ python -m pip debug",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-anac",
    "href": "qmd/python-general.html#sec-py-gen-anac",
    "title": "General",
    "section": "Anaconda",
    "text": "Anaconda\n\nCheck configuration of current environment\nconda list\n\nShows python version used, package names installed and their versions\n\nInstall packages\nconda install &lt;package1&gt; &lt;package2&gt;\nInstall a package from a specific channel\nconda install &lt;package_name&gt; -c &lt;channel_name&gt; -y # Short form\nconda install &lt;package_name&gt; --channel &lt;channel_name&gt; -y # Long form\nPackage installation channels (some packages not available in default channel)\n\nCheck current channels\nconda config --show channels\n\nThe order in which these channels are displayed shows the channel priority.\n\nWhen a package is installed, anaconda will the check the channel at the top of list first then work it‚Äôs way down\n\n\nAdd a channel\nconda config --add channels conda-forge\n\nAdds ‚Äúconda-forge‚Äù to list of available channels\n\nRemove a channel\nconda config --remove channels conda-forge\n\nRemoves the ‚Äúconda-forge‚Äù channel",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-env",
    "href": "qmd/python-general.html#sec-py-gen-env",
    "title": "General",
    "section": "Environments",
    "text": "Environments\n\nMisc\n\nWhen you‚Äôre in a virtual environment\n\nAnytime you use the ‚Äúpython‚Äù command while your virtual environment is activated, it will be only the one from this env.\nIf you start a Python shell now, it will see only the things in the current directory, and the things installed in the virtual environment.\nIf you run a script, it will see only the things in the current directory, and the things installed in the virtual environment.\nIf you run a command, the command will be taken from the virtual environment.\nAnd they will only use exactly the version of Python of the virtual environment.\n\nStore environment files with project\nBreakage\n\nYou cannot move a virtual environment, it will stop working. Create a ‚Äúrequirements.txt‚Äù file, delete the virtual environment and create a new one.\nDon‚Äôt rename a directory containing a virtual environment. Or if you do, prepare yourself to create a ‚Äúrequirements.txt‚Äù file, delete the virtual environment and create a new one.\nIf you change the Python used in the virtual environment, such as when uninstalling it, it will stop working.\n\nCreate one big virtual environment for all small scripts.\n\nIf you make a lot of venv, you may be tempted to install everything at the system level for convenience. After all, it‚Äôs a bore to create and activate a virtual environment each time you want to write a five liner. A good balance is one single virtual environment you use for all things quick and dirty.\n\nCreate several virtual environments per versions of python if your project needs to support several versions. You may need several requirements.txt files as well, one for each env.\nRecommendations for a stable dependency environment for your project (article)\n\nDon‚Äôt install the latest major version of Python\n\nMaximum: 1 version under the latest version\nMinimum: 4 versions under the latest version (e.g.¬†latest = 3.11, min = 3.7)\n\nUse only the python.org installer on Windows and Mac, or official repositories on Linux.\nNever install or run anything outside of a virtual environment\nLimit yourself to the basics: ‚Äúpip‚Äù and ‚Äúvenv‚Äù\nIf you run a command, use ‚Äú-m‚Äù\n\nIt lets you run any importable Python module, no matter where you are. Because most commands are Python modules, we can use this to say, ‚Äúrun the module X of this particular python‚Äù.\nThere is currently no way for you to run any python command reliably without ‚Äú-m‚Äù.\nExamples:\n# Don't do :\npip install\n# Do:\npython -m pip install\n\n# Don't do :\nblack\n# Do:\npython -m black\n\n# Don't do :\njupyter notebook\n# Do:\npython -m jupyter notebook\n\nWhen creating a virtual environment, be explicit about which Python you use\n\nGet current python versions installed: py --list-paths (windows)\n\n\n\n\n\npyenv\n\nJust a simple Python version manager ‚Äî think {rig}\n{{pyenv}}, {{pyenv-win}}\nSet-up in RStudio (article)\nCompiles Python under the hood when you install it. But compiling can fail in a thousand ways\npyenv install --list: To see what python versions are available to install\npyenv install &lt;version number&gt;: To install a specific version\npyenv versions: To see what python versions are installed on your system\npyenv global &lt;version number&gt;: The set one python version as a global default\npyenv local &lt;version number&gt;: The set a python version to be used within a specific directory/project\\\n\n\n\npdm\n\nDocs\nPackage and dependency manager similar to npm. Doesn‚Äôt require virtual environments.\nFeatures: auto-updating pyproject.toml, isolating dependencies from dependencies-of-dependencies, active development and error handling\n\n\n\nvenv\n\nMisc\n\nShipped with Python\nDon‚Äôt mix pip, venv and Anaconda. Avoid Anaconda if you can. If you have to use Anaconda, don‚Äôt use pip and venv. Limit yourself to Anaconda tools.\n\nCreate\n\nWindows: py -&lt;py version&gt; -m venv &lt;env name&gt;\nMac/Linux: python3.8 -m venv .venv\n\nWhere the python version is 3.8 and the environment name is ‚Äú.venv‚Äù\nMac and Linux hide folders with names that have preceding ‚Äú.‚Äù by default, so make sure you have ‚Äúdisplay hiddent folders‚Äù activated or you won‚Äôt see it.\n\nNaming Environments\n\nName your environment directory ‚Äú.venv‚Äù, because:\n\nSome editors check for this name and automatically load it.\nIt contains ‚Äúvenv‚Äù, so it‚Äôs explicit.\nIt has a dot, so it‚Äôs hidden on Linux.\n\nIf you have more than one environment directory, use a suffix to distinguish them.\n\ne.g.¬†A project that must work with two different versions of Python (3.9 and 3.10), I will have a ‚Äú.venv39‚Äù and a ‚Äú.venv310‚Äù\n\nNaming enviroments for misc uses\n\n‚Äú.venv_test‚Äù: located in a personal directory to install new tools you want to play with. It‚Äôs disposable, and often broken, so you can delete it and recreate it regularly.\n‚Äú.venv_scripts‚Äù: Used for all the small scripts. You don‚Äôt want to create one virtual environment for each script, so centralize everything. It‚Äôs better than installing outside of a virtual environment, but is not a big constraint.\n\n\n\nActivate\n\nWindows: .venv\\Scripts\\activate\n\nWhere .venv is the name of the virtual environment\nMay need .bat as extension to activate\n\nMac/Linux: source .venv/bin/activate\n\nAfter that, you can use python -m pip install to install packages.\nDeactivate: deactivate\n\n\n\nvirtualenv\n\nDocs\nCreate a virtual environment\n python3 -m venv &lt;env_name&gt;\n\n-m venv tells python to run the virtual environment module, venv\nMake sure you‚Äôre in your projects directory\nRemember to add ‚Äú&lt;env_name&gt;/‚Äù to .gitignore\n\nActivate environment\nsource venv/bin/activate # Mac or Linux\nvenv\\Scripts\\activate # Windows\n\n&gt;&gt; (&lt;env_name&gt;) $\n\nPrompt should change if the environment is activated\nAll pip¬† installed packages will now be installed into the ‚Äú&lt;env_name&gt;/lib/python3.9/site-packages‚Äù directory\n\nUse the python contained within your virtual environment\npython main.py\n\nNot sure why you wouldn‚Äôt just activate the environment.\n\nDeactivate environment\ndeactivate\n\nno python¬† or env_name needed?\n\nReproducing environment\n\nDone using requirements.txt (see pip section for details on writing and installing)\n\nI don‚Äôt think the python version is included, so that will need to communicated manually\n\n\n\n\n\nAnaconda\n\nList environments\nconda env list # method 1\nconda info --envs # method 2\n\ndefault environment is called ‚Äúbase‚Äù\nActive environment will be in parentheses\nActive environment will be the one in the list with an asterix\n\nCreate a new conda environment\nconda create -n &lt;env name&gt;\nconda activate &lt;env name&gt;\nCreate a new conda environment with a specific python version\nconda create -n py310 python=3.10\nconda activate py310\nconda install jupyter jupyterlab\njupyter lab\n\nAlso install and launch jupyter lab\n\nCreate an environment from a yaml file\nconda env create -f environment.yml # Short form\nconda env create --file environment.yml # Long form\nRemove an environment\nconda deactivate &lt;env_name&gt; # Need to deactivate the environment first\nconda env remove -n &lt;env_name&gt;\n\nShould also delete environment folders (conda env list shows path to folders)\n\nClone an existing environment\nconda create -n testclone --clone test # Short form\nconda create --name testclone --clone test # Long form\n\n‚Äútestclone‚Äù is a copy of ‚Äútest‚Äù\n\nActivate an environment\nconda activate &lt;env_name&gt;\nActivate environment with reticulate in R\nreticulate::use_python(\"/usr/local/bin/python\")¬† ¬†\nreticulate::use_condaenv(\"&lt;env name&gt;\", \"/home/jtimm/anaconda3/bin/conda\")\nDeactivate an environment\nconda activate # Option 1: activates base\nconda deactivate test # Option 2\nExport the specifications of the current environment into a YAML file into the current directory\nconda env export &gt; environment.yml # Option 1\nconda env export -f environment.yml # Option 2\nExample: Conda workflow\n\nCreate an environment that uses a specific python version\n\nWithout a specified python version, the environment will use the same version as ‚Äúbase‚Äù\n\nconda create -n anothertest python=3.9.7 -y\n\n-n is the name flag and ‚Äúanothertest‚Äù is the name of the environment\nUses Python 3.9.7\nWithout the -y flag, there‚Äôd be a prompt you‚Äôd have to answer ‚Äúyes‚Äù to\n\nActivate the environment\nconda activate anothertest\nInstall packages\n\n\nInstalling packages one at time can lead to dependency conflicts.\nConda‚Äôs official documentation recommends to install all packages at the same time so that the dependency conflicts are resolved\nconda install \"numpy&gt;=1.11\" nltk==3.6.2 jupyter -y # install specific versions\nconda install numpy nltk jupyter -y # install all latest versions\n\nDo work and deactivate environment\nconda deactivate anothertest\n\nExample Raschka workflow\n# create & activate\nconda create¬† --prefix ~/code/myproj python=3.8\nconda activate ~/code/myproj\n# export env\nconda env export &gt; myproj.yml\n# create new env from yaml\nconda env create --file myproj.yml --prefix ~/code/myproj2\n\n\n\nPoetry\n\nDocs (like renv)\nApparently buggy (article)\npip‚Äôs dependency resolver is more flexible and won‚Äôt die on you if the package specifies bad metadata, while poetry‚Äôs strictness may mean you can‚Äôt install some packages at all.\nCreate project\n\npoetry new &lt;project-dir-name&gt;\n\nautomatically creates a directory for your project with a skeleton\n‚Äúpyproject.toml‚Äù maintains dependencies for the project with the following sections:\n\ntool.poetry provides an area to capture information about your project such as the name, version and author(s).\ntool.poetry.dependencies lists all dependencies for your project.\ntool.poetry.dev-dependencies lists dependencies your project needs for development that should not be present in any version deployed to a production environment.\nbuild-system references the fact that Poetry has been used to manage the project.\n\n\nAdd library and create lock file: poetry add &lt;library name&gt;\n\nWhen the first library is added, a ‚Äúpoetry.lock‚Äù file wil be generated\n\nActivate environment: poetry shell\n\nDeactivate environment: exit\n\nRun script: poetry run python my_script.py\nPackage the project: poetry build\n\nCreates tar.gz and wheel files (.whl) in ‚Äúdist‚Äù dir\n\nExample: poetry workflow (+pyenv, virtualenv)\n# Create a virtual environment called \"my-new-project\"\n# using Python 3.8.8\npyenv virtualenv 3.8.8 my-new-project\n# Activate the virtual environment\npyenv activate my-new-project\n\n{{pyenv}} - For managing the exact version of Python and activating the environment\nName your package the same name as the directory which is the same name as the virtual environment.\n\nDashes for the latter two and underscores for the package\n\nIntitialize the project and add packages (similar to renv) bash              poetry init     poetry add numpy\nReinstall dependencies\n# navigate to my project directory and run\npoetry install\nTurn off virtualenv management\n# right after installing poetry, run:\npoetry config virtualenvs.create false\n\nDefault poetry behavior is that it will manage your virtual environments for you. This may not be desirable because:\n\nCan‚Äôt just run a script from the command line. Instead, have to run poetry run my-script\n\nAwkward when you want to dockerize your code\n\nEnforces a virtual environment management framework on everybody in a shared codebase\nYour Makefile now needs to know about poetry",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-deps",
    "href": "qmd/python-general.html#sec-py-gen-deps",
    "title": "General",
    "section": "Dependencies",
    "text": "Dependencies\n\nMisc\n\nAfter mastering pip, {{pip-tools}} is recommended.\n\nGet a complete list of dependencies (e.g.¬†dependencies of dependencies) with {{deptree}}\ndeptree\n# output\nFlask==2.2.2¬† # flask\n¬† Werkzeug==2.2.2¬† # Werkzeug&gt;=2.2.2\n¬† ¬† MarkupSafe==2.1.1¬† # MarkupSafe&gt;=2.1.1\n¬† Jinja2==3.1.2¬† # Jinja2&gt;=3.0\n¬† ¬† MarkupSafe==2.1.1¬† # MarkupSafe&gt;=2.0\n¬† itsdangerous==2.1.2¬† # itsdangerous&gt;=2.0\n¬† click==8.1.3¬† # click&gt;=8.0\n# deptree and pip trees\n\nFlask depends on Werkzeug which depends on MarkupSafe\n\nWerkzeug and MarkupSafe qualify as transitive dependencies for this project\n\nCommented part on the right is the compatible range\n\nrequirements.txt format\n# comment\npandas==1.0.0\npyspark\npip: write names of all the packages in your environment to a requirement.txt file\n$ python3 -m pip freeze &gt; requirements.txt\n\nSee pip section for issues with this method\n\n{{pipx}}\n\nA tool for installing Python CLI utilities that gives them their own hidden virtual environment for their dependencies\nAdds the tool itself to your PATH - so you can install stuff without worrying about it breaking anything else\nInstall\npipx install datasette\n\n{{pipreqs}}\n\nScans all the python files (.py) in your project, then generates the requirements.txt file based on the import statements in each python file of the project\nSet-up: pip install pipreqs\nGenerate requirements.txt file: pipreqs /&lt;your_project_root_path&gt;/\nUpdate requirements.txt:¬† pipreqs --force /&lt;your_project_root_path&gt;/¬†\nIgnore the libraries of some python files from a specific subfolder\npipreqs /&lt;your_project_root_path&gt;/ --ignore  /&lt;your_project_root_path&gt;/folder_to_ignore/\n\n{{pip-compile-multi}}\n\nNotes from:\n\nEnd Python Dependency Hell with pip-compile-multi\n\nCreates and nests multiple requirement files\n\ne.g.¬†Able to keep dev environment from production environment separate\n\nAutoresolution of cross-requirement file conflicts\n\nDependency DAG (how all requirement files are connected) must have exactly one ‚Äúsink‚Äù node\n\nOrganize your most ubiquitous dependencies into a single ‚Äúcore‚Äù set of dependencies that all other nodes require (a source node), and all of your development dependencies in a node that requires all others (directly or indirectly) require (a sink).\n\nSimplifies and allows use of autoresolution functionality\n\nExample: DAG (directionality of the arrows is opposite compared to library docs)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-loadsav",
    "href": "qmd/python-general.html#sec-py-gen-loadsav",
    "title": "General",
    "section": "Loading/Saving",
    "text": "Loading/Saving\n\nMisc\n\n{{pickle}} needs custom class(es) to be defined in another module/file and then imported. Otherwise, PicklingError will be raised.\n\n\n\nFile paths\n\nMisc\n\n{{pathlib}} is recommended\n\n{{os}}\n\nGet current working directory: os.getcwd()\nList all files and directories in working directory: os.listdir()\nList all files and directories from a subdirectory: os.listdir(os.getcwd()+'\\\\01_Main_Directory')\nUsing os.walk(): gathers paths, folders, and files\n\nPaths\n\n\npath = os.getcwd()+'\\\\01_Main_Directory'\nfor folder_path, folders, files in os.walk(path):\n¬† ¬† print(folder_path)\nFolders\n\n\nSimilar code, just replace print(folder_path) with print(folders)\n\nFiles\n\n\n{{glob}}\n\nGet a file path string\nimport glob\npath = os.getcwd()+'\\\\01_Main_Directory'\nfor filepath in glob.glob(path):\n¬† ¬† print(filepath)\n# C:\\Users\\Suraj\\Challenges\\01_Main_Directory\nList all files and subdirectories from a path\npath = os.getcwd()+'\\\\01_Main_Directory\\\\*'\nfor filepath in glob.glob(path):\n¬† ¬† print(filepath)\n\nNote the * wildcard\n\nList all files and subdirectories with a ‚Äú1‚Äù in the name\npath = os.getcwd()+'\\\\01_Main_Directory\\\\*1.*'\nfor filepath in glob.glob(path):\n¬† ¬† print(filepath)\nGet a list of csv file paths from a directory: all_files = glob.glob(\"C:/Users/path/to/dir/*.csv\")\n\nNote that you don‚Äôt need a loop to save to an object\n\nList all files and subdirectories and files in those subdirectories\npath = os.getcwd()+'\\\\01_Main_Directory\\\\**\\\\*.txt'\nfor filepath in glob.glob(path, recursive=True):\n¬† ¬† print(filepath)\n#Output\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_2.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_3.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_4.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_5.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_1\\File_1_in_SubDict_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_1\\File_2_in_SubDict_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_2\\File_1_in_SubDict_2.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_2\\File_2_in_SubDict_2.txt\n\nComponents: ‚Äú**‚Äù and ‚Äúrecursive=True‚Äù\n\n\n{{pathlib}}\n\nProvides a single Path class with a range of methods (instead of separate functions) that can be used to perform various operations on a path.\nCreate a path object for a directory\nfrom pathlib import Path\npath = Path('origin/data/for_arli')\nCheck if a folder or a file is available in a given path\nif path.exists():\n¬† ¬† print(f\"[{path}]{style='color: #990000'} exists.\")\n¬† ¬† if path.is_file():\n¬† ¬† ¬† ¬† print(f\"[{path}]{style='color: #990000'} is a file.\")\n¬† ¬† elif path.is_dir():\n¬† ¬† ¬† ¬† print(f\"[{path}]{style='color: #990000'} is a directory.\")\nelse:\n¬† ¬† raise ValueError(f\"[{path}]{style='color: #990000'} does not exists\")\n\nChecks if the path ‚Äòorigin/data/for_arli‚Äô exists\n\nif it does, it will check whether it is a file or a directory.\nIf the path does not exist, it will print a raise an Error indicating that the path does not exist.\n\n\nList all files/folders in a path\nfor f in path.iterdir():\n¬† ¬† print(f)\n\nUse it in combination with the previous is_dir() and is_file()¬† methods to list either files or directories.\n\nDelete files/folders in a path\nfor f in path.iterdir():\n¬† ¬† f.unlink()\n\npath.rmdir()\n\nunlink deletes each file in the path\nrmdir deletes the directory.\n\ndirectory must be empty\n\n\nCreate a sequence of directories\n# existing directory: D:\\scripts\\myfolder\np = Path(\"D:\\scripts\\myfolder\\logs\\newfolder\")\np.mkdir(parents=True, exist_ok=True)\n\nCreate path object with desired sequence of directories (e.g.¬†logs\\newfolder)\nmkdir with parents=True creates the sequence of directories\n\nW/exist_ok=True no error with occur if the directory already exists\n\n\nRename directory: path.rename('origin/data/new_name')\nConcatenate a path with string\npath = Path(\"/origin/data/for_arli\")\n# Join another path to the original path\nnew_path = path.joinpath(\"la\")\nprint(new_path) # prints 'origin/data/for_arli/bla'\n\nIt also handles the join between two Path objects\n\nDirectory stats\nprint(path.stat()) # print statistics¬†\nprint(path.owner()) # print owner\n\ne.g.¬†creation time, modification time, etc.\n\nWrite to a file\n# Open a file for writing\npath = Path('origin/data/for_arli/example.txt')\nwith path.open(mode='w') as f:\n¬† ¬† # Write to the file\n¬† ¬† f.write('Hello, World!')\n\nYou do not need to create manually example.txt.\n\nRead a file\npath = Path('example.txt')\nwith path.open(mode='r') as f:\n¬† ¬† # Read from the file\n¬† ¬† contents = f.read()\n¬† ¬† print(contents) # Output: Hello World!\n\n\n\n\nModels\n\nSaving and Loading an estimator as a binary using {{joblib}} (aside: pipelines are estimators)\nimport joblib\n#saving the pipeline into a binary file\njoblib.dump(pipe, 'wine_pipeline.bin')\n#loading the saved pipeline from a binary file\npipe = joblib.load('wine_pipeline.bin')\nSaving and loading a trained model as a pickle file\nimport pickle\n# open file connection\npickle_file = open('model.pkl', 'ab')\n# save the model\npickle.dump(model_obj, pickle_file)\n# close file connection¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\npickle_file.close()\n\n# Open conn and save\ntest_dict = {\"Hello\": \"World!\"}\nwith open(\"test.pickle\", \"wb\") as outfile:\n# \"wb\" argument opens the file in binary mode\npickle.dump(test_dict, outfile)\n\n# open file connection\npickle_file = open('model.pkl', 'rb')\n# load saved model\nmodel = pickle.load(pickle_file)\n\n# open conn and load\n# Deserialization\nwith open(\"test.pickle\", \"rb\") as infile:\n¬† ¬† test_dict_reconstructed = pickle.load(infile)\n\nCan serialize almost everything including classes and functions\n\n\n\n\nEnvironment Variables\n\n{{os}}\n\nCheck existence\nenv_var_exists = 'ENV' in os.environ\n# or\nenv_var_exists = os.environ.has_key('ENV')\nList environment variables: print(os.environ)\nLoading\nimport os\n# Errors when not present\nenv_var = os.environ['ENV'] # where ENV is the name of the environment variable\n# Returns None when not present\nenv_var = os.environ.get('ENV', 'DEFAULT_VALUE') # using default value is optional\nSet/Export or overwrite\nos.environ['ENV'] = 'dev'\nLoad or create if not present\ntry:\n¬† ¬† env_var = os.environ['ENV']\nexcept KeyError:\n¬† ¬† os.environ['ENV'] = 'dev'\nDelete\nif 'ENV' in os.environ:\n¬† ¬† del os.environ['ENV']\n\n{{python-decouple}}\n\nAccess environment variables from whatever environment it is running in.\nCreate a .env file in the project root directory: touch .env\nOpen .env in nano text editor: nano .env\n\nNano text editor is pre-installed on macOS and most Linux distros\nCheck if installed/version: nano --version\nBasic usage tutorial\n\nAdd environment variables to file\nUSER=alex\nKEY=hfy92kadHgkk29fahjsu3j922v9sjwaucahf\n\nSave: Ctrl+o\nExit: Ctrl+x\n\n* Add .env to your .gitignore file *\nAccess\nfrom decouple import config\nAPI_USERNAME = config('USER')\nAPI_KEY = config('KEY')\n\n{{python-dotenv}}\n\nReads .env files\nProbably more popular than {{python-decouple}}\nHas a companion R package, {dotenv}, so .env files can be used in projects that use both R and Python.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-funs",
    "href": "qmd/python-general.html#sec-py-gen-funs",
    "title": "General",
    "section": "Functions",
    "text": "Functions\n\nMisc\n\nBenchmarking a function\n\nUsing IPython function\n%time dat['col1001'] = some_function(dat['col1'], dat['col2'], dat['col3'])\n\n%%time prints the wall time for the entire cell whereas %time gives you the time for first line only\n\nUsing a decorator\n\n\nAssigning functions based on arg type\ndef process_data(data):\n¬† ¬† if isinstance(data, dict):\n¬† ¬† ¬† ¬† process_dict(data)¬†\n¬† ¬† else:\n¬† ¬† ¬† ¬† process_list(data)¬†\ndef process_dict(data: dict):\n¬† ¬† print(\"Dict is processed\")\ndef process_list(data: list):\n¬† ¬† print(\"List is processed\")\n\nAssigns data to a particular function depending on whether it‚Äôs a dict or a list\nisinstance checks that the passed argument is of the proper type or a subclass\n\nWrapping functions\nfrom functools import partial\nget_count_df = partial(get_count, df=df)\n\nWraps function to make df the default value for df arg\n\n\n\n\nDocumentation\n\nFunctions should at least include docstrings and type hinting\nDocstrings\n\nTypes: Google-style, Numpydoc, reStructured Text, EpyTex\nInformation to include\n\nFunction description, arg description, return value description, Description of errors, Optional extra notes or examples of usage.\n\nAccess functions docstring:\n\nprint(func_name.__doc__)\nFor large docstrings\nimport inspect\nprint(inspect.getdoc(func_name))\n\nExample: Google-style\ndef send_request(key: str, lat: float = 0, lon: float = 0):\n¬† ¬† \"\"\"Send a request to Climacell Weather API\n¬† ¬† to get weather info based on lat/lon.\n\n¬† ¬† Climacell API provides realtime weather\n¬† ¬† information which can be accessed using\n¬† ¬† their 'Realtime Endpoint'.\n\n¬† ¬† Args:\n¬† ¬† ¬† key (str): an API key with length of 32 chars.\n¬† ¬† ¬† lat (float, optional): value for latitude.\n¬† ¬† ¬† ¬† Default=0\n¬† ¬† ¬† lon (float, optional): value for longitude.\n¬† ¬† ¬† ¬† Default=0\n\n¬† ¬† Returns:\n¬† ¬† ¬† int: status code of the result¬†\n¬† ¬† ¬† dict: Result of the call as a dict\n\n¬† ¬† Notes:\n¬† ¬† ¬† See https://www.climacell.co/weather-api/¬†\n¬† ¬† ¬† for more info on Weather API. You can get\n¬† ¬† ¬† API key from there, too.\n¬† ¬† \"\"\"\n\nFirst sentence should contain the purpose of the function\n\nExample: Numpydoc\ndef send_request(key: str, lat: float = 0, lon: float = 0):\n¬† ¬† \"\"\"\n¬† ¬† Send a request to Climacell Weather API\n¬† ¬† to get weather info based on lat/lon.\n\n¬† ¬† Climacell API provides realtime weather\n¬† ¬† information which can be accessed using\n¬† ¬† their 'Realtime Endpoint'.\n\n¬† ¬† Parameters\n¬† ¬† ----------\n¬† ¬† ¬† key (str): an API key with length of 32 chars.\n¬† ¬† ¬† lat (float, optional): value for latitude.\n¬† ¬† ¬† ¬† Default=0\n¬† ¬† ¬† lon (float, optional): value for longitude.\n¬† ¬† ¬† ¬† Default=0\n\n¬† ¬† Returns\n¬† ¬† -------\n¬† ¬† ¬† int: status code of the result¬†\n¬† ¬† ¬† dict: Result of the call as a dict\n\n¬† ¬† Notes\n¬† ¬† -----\n¬† ¬† ¬† See https://www.climacell.co/weather-api/¬†\n¬† ¬† ¬† for more info on Weather API. You can get\n¬† ¬† ¬† API key from there, too.\n¬† ¬† \"\"\"\n\nType Hinting\n\nThis doesn‚Äôt check the type; it‚Äôs just metadata\n\nsee isinstance (see below), NotImplementedError (see below), or {{typecheck}} and {{mypy} (see bkmks) for type checking that will throw errors\n\nUsing type hints enables you to perform type checking. If you use an IDE like PyCharm or Visual Studio Code, you‚Äôll get visual feedback if you‚Äôre using unexpected types:\nVariables: my_variable_name: tuple[int, ...]\n\nvariable should be a tuple that contains only integers. The ellipsis says the total quantity is unimportant.\n\nFunctions\ndef get_count(threshold: str, column: str, df: pd.DataFrame) -&gt; int:\n¬† ¬† return (df[column] &gt; threshold).sum()\n\n‚Äúthreshold‚Äù, ‚Äúcolumn‚Äù should be strings (str)\n‚Äúdf‚Äù should be a pandas dataframe (pd.DataFrame)\nOutput should be an integer (int)\n\nFunction as an arg: Callable[[Arg1Type, Arg2Type], ReturnType]\n\nExample:\nfrom collections.abc import Callable\ndef foo(bar: Callable[[int, int], int], a: int, b: int) -&gt; int:\n¬† ¬† return bar(a, b)\n\n‚Äúbar‚Äù is a function arg for the function, ‚Äúfoo‚Äù\n‚Äúbar‚Äù is supposed to take: 2 integer args ([int, int]) and return an integer (int)\n\nExample:\ndef calculate(i: int, action: Callable[..., int], *args: int) -&gt; int:\n¬† ¬† return action(i, *args)\n\n‚Äúaction‚Äù takes any number and type of arguments but must return an integer.\nWith *args: int, you also allow a variable number of optional arguments, as long as they‚Äôre integers.\n\nExample: Lambda\nf: Callable[[int, int], int] = lambda x, y: 3*x + y\n\nMay not work\n\n\n\n\n\n\nArgs and Operators\n\nMisc\n\n** Args are not reset to default values after each call **\n\nExample:\n\ndef func(list1=[]):¬† ¬† ¬† # here l1 is a default argument set to []\n¬† ¬† list1.append(\"Temp\")\n¬† ¬† return list1\n\n‚ÄúNone‚Äù + conditional must be used to get the arg to reset back to the default value\n\ndef func(l1=None):¬† ¬† ¬†\n¬† ¬† if l1 is None:¬†\n¬† ¬† ¬† ¬† l1 = []\n¬† ¬† l1.append(\"Temp\")¬†\n¬† ¬† return l1\n\n*\n\nUnpacks Lists\n\nnum_list = [1,2,3,4,5]\nnum_list_2 = [6,7,8,9,10]\n\nprint(*num_list)\n# 1 2 3 4 5\nnew_list = [*num_list, *num_list_2] # merge multiple lists\n# [1,2,3,4,5,6,7,8,9,10]\n*args\n\nFunctions that can accept a varying number of values\n\ndef names_tuple(*args):\n¬† ¬† return args\n\nnames_tuple('Michael', 'John', 'Nancy')\n# ('Michael', 'John', 'Nancy')\nnames_tuple('Jennifer', 'Nancy')\n# ('Jennifer', 'Nancy')\n**\n\nUnpacks Dictionaries\n\nnum_dict = {‚Äòa‚Äô: 1, ‚Äòb‚Äô: 2, ‚Äòc‚Äô: 3}\nnum_dict_2 = {‚Äòd‚Äô: 4, ‚Äòe‚Äô: 5, ‚Äòf‚Äô: 6}\n\nprint(*num_dict) # only keys printed\n# a b c\nnew_dict = {**num_dict, **num_dict_2} # merge dictionaries\n# {‚Äòa‚Äô: 1, ‚Äòb‚Äô: 2, ‚Äòc‚Äô: 3, ‚Äòd‚Äô: 4, ‚Äòe‚Äô: 5, ‚Äòf‚Äô: 6}\n**kwargs\n\nFunctions that can accept a varying number of variable/value pairs (like a ‚Ä¶ in R)\n\ndef names_dict(**kwargs):\n¬† ¬† return kwargs\n\nnames_dict(Jane = 'Doe')\n# {'Jane': 'Doe'}\nnames_dict(Jane = 'Doe', John = 'Smith')\n# {'Jane': 'Doe', 'John': 'Smith'}\nFunction as an arg\ndef classic_boot(df, estimator, seed=1):\n¬† ¬† df_boot = df.sample(n=len(df), replace=True, random_state=seed)\n¬† ¬† estimate = estimator(df_boot)\n¬† ¬† return estimate\n\nBootstrap function with an ‚Äúestimator‚Äù function (e.g.¬†mean) as arg\nUsing a Callable\n\nClass as an arg\nfrom dataclasses import dataclass\n\n@dataclass\nclass Person:\n    name: str\n    age: int\n    address: str\n    phone: str\n    email: str\n\ndef process_data(person: Person):\n    print(f\"Processing data for {person.name}, {person.age}, living at {person.address}. Contact info: {person.phone}, {person.email}\")\n\nperson = Person(\"Alice\", 30, \"123 Main St\", \"555-1234\", \"alice@example.com\")\nprocess_data(person)\n\nMakes function more readable when the function requires a bunch of args\n\nClass as an arg (safer alternative)\nfrom typing import NamedTuple\n\nclass Person(NamedTuple):\n    name: str\n    age: int\n    address: str\n    phone: str\n    email: str\n\ndef process_data(person: Person):\n    print(f\"Processing data for {person.name}, {person.age}, living at {person.address}. Contact info: {person.phone}, {person.email}\")\n\nperson = Person(\"Alice\", 30, \"123 Main St\", \"555-1234\", \"alice@example.com\")\nprocess_data(person)\n\nUsing NamedTuple means that the attributes cannot be overridden\n\ne.g.¬†Executing person.name = \"Bob\" will result in an error because tuples can‚Äôt be modified.\n\n\nMake an arg optional\nlass Address:\n¬† ¬† def __init__(self, street, city, state, zipcode, street2=''):\n¬† ¬† ¬† ¬† self.street = street\n¬† ¬† ¬† ¬† self.street2 = street2\n¬† ¬† ¬† ¬† self.city = city\n¬† ¬† ¬† ¬† self.state = state\n¬† ¬† ¬† ¬† self.zipcode = zipcode\n\n‚Äústreet2‚Äù has default value of an empty string, so it‚Äôs optional\n\n\n\n\nLambda\n\nUseful if you just have 1 expression that you need to execute.\nBest Practices\n\nlambda is an anonymous function, hence it is not a good idea to store it in a variable for future use\nDon‚Äôt use lambdas for single functions (e.g.¬†sqrt). Make sure it‚Äôs an expression.\n\nExample\n# bad\nsqrt_list = list(map(lambda x: math.sqrt(x), mylist))\n# good\nsqrt_list = list(map(math.sqrt, mylist))\n\nAffects performance\n\n\nDon‚Äôt use for complex expressions that require more than 1 line (meh)\n\nPer PEP8 guidelines, Limit all lines to a maximum of 79 characters\nExample\n# bad (118 characters)\ndf[\"FinalStatus\"] = df[\"Status\"].map(lambda x: 'Completed' if x ==\n'Delivered' or x == 'Shipped' else 'Not Completed')\n# instead\ndf[\"FinalStatus\"] = ''\ndf.loc[(df[\"Status\"] == 'Delivered') |\n¬† ¬† ¬† (df[\"Status\"] == 'Shipped'),\n¬† ¬† ¬† 'FinalStatus'] = 'Completed'\ndf.loc[(df[\"Status\"] == 'Not Delivered') |\n¬† ¬† ¬† (df[\"Status\"] == 'Not Shipped'),\n¬† ¬† ¬† 'FinalStatus'] = 'Not Completed'\n\n\nExample: 1 arg\n# py\nlambda x: np.sin(x / period * 2 * np.pi)\n# r\n~sin(.x / period * 2 * pi)\n# r\n\\(x) {sin(x / period * 2 * pi)}\nExample: 2 args\nGreater = lambda x, y : x if(x &gt; y) else y\nGreater(0.002, 0.5897)\nLambda-Filter\n\nFaster than a comprehension\n\nsee Loops &gt;&gt; Comprehensions\n\nFormat: filter(function, data_object)\n\nReturns a filter object, which needs to be converted into data structure such as list or set\n\nExample: Basic\nyourlist = list(np.arange(2,50,3))\nlist(filter(lambda x:x**2&lt;100, yourlist))\n# Output¬†\n[2, 5, 8]\nExample: Filter w/logical\nimport pandas as pd\nimport datetime as dt\n# create a list of 10,000 dates\ndatlist = pd.date_range(dt.datetime.today(), periods=10000).tolist()¬†\n# convert the dates to strings via list comprehension\ndatstrlist = [d.strftime(\"Day %d in %B of year %Y is a %A\") for d in datlist]\ndatstrlist[:4]\n['Day 21 in October of year 2021 is a Thursday', 'Day 22 in October of year 2021 is a Friday', 'Day 23 in October of year 2021 is a Saturday', 'Day 24 in October of year 2021 is a Sunday']\n\nstrLamb = filter(lambda d: ((d.endswith(\"urday\") or d.endswith(\"unday\")) and \"Oc\" in d), datstrlist)\n\nSearches for Saturdays and Sundays in the month of October of all years in list of strings\n\nExample: Nested Lists\ngroup1 = [1,2,3,43,23,42,8,3,7]\ngroup2 = [[3, 34, 23, 32, 42], [6, 11, 9], [1, 3,9,7,2,8]]\n[list(filter(lambda x: x in group1, sublist)) for sublist in group2]\n&gt;&gt; [[3, 23, 42], [], [1, 3, 7, 2, 8]]\n\nProbably useful for json\nfor-loop attached to the end of the list-filter combo\nEach sublist of group 2 is fed into the lambda-filter and compared to the group 1 list\n\n\nIterating over each element of a list\n\nExample: map\nlist(map(lambda x: x**2+x**3, yourlist))\n\nmap returns a map object that needs to be converted\n\nExample: 2 Lists\nmylist = list(np.arange(4,52,3))\nyourlist = list(np.arange(2,50,3))\nlist(map(lambda x,y: x**2+y**2, yourlist, mylist))\n\nLike a pmap\n\n\nNested lambdas\n\nExample: map\narr = [1,2,3,4,5]\nlist(map(lambda x: x*2, filter(lambda x: x%2 == 0, arr)))\n&gt;&gt; [4,8]\n\nWork inside out (locate where the data object, arr, appears)\n‚Äúarr‚Äù is filtered by the first lambda function for even numbers then iterated by map to be squared by the second lambda function\n\n\nIterate over rows of a column in a df\n\nExample: Using formula over rows\ngrade['evaluate']=grade['MathScore'].apply(lambda x: round((x**x)/2,2))\n\n‚Äúgrade‚Äù is the df; ‚ÄúMathScore‚Äù is a numeric column; ‚Äúevaluate‚Äù is the new column in the df\nFormula applied to each value of ‚ÄúMathScore‚Äù to generate each value of evaluate\n\nExample: Conditional over rows\ngrade['group']=grade['MathScore'].apply(lambda x: 'Excellent' if x&gt;=3.0 else 'Average')\n\n‚Äúgrade‚Äù is the df; ‚ÄúMathScore‚Äù is a numeric column; ‚Äúgroup‚Äù is the new column in the df\nConditional applied to each value of ‚ÄúMathScore‚Äù to generate each value of ‚Äúgroup‚Äù\n\nUsing {{swifter}} for parallelization\nimport swifter\ndf['e'] = df.swifter.apply(lambda x: infer(x['a'], x['b'], x['c'], x['d']), axis = 1)\n\nIn a Pivot Table (like a crosstab)\n\nExample\n\ngrades_df\n\n2 names (‚Äúname‚Äù)\n6 scores (‚Äúscore‚Äù)\nOnly 2 letter grades associated with these scores (‚Äúletter grade‚Äù)\n\nTask: drop lowest score for each letter grade, then calculate the average score for each letter grade\n\ngrades_df.pivot_table(index='name',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† columns='letter grade',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† values='score',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† aggfunc = lambda series : (sorted(list(series))[-1] + sorted(list(series))[-2]) / 2)\n\nletter grade¬† ¬† A¬† ¬† B\nname\nArif¬† ¬† ¬† ¬† ¬† 96.5¬† 87.0\nKayla¬† ¬† ¬† ¬† 95.5¬† 84.0\n\nindex: each row will be a ‚Äúname‚Äù\ncolumns: each column will be a ‚Äúletter grade‚Äù\nvalues: value in the cells will be from the ‚Äúscore‚Äù column according to each combination columns in the index and columns args\naggfunc: uses a lambda to compute the aggregated values\n\n‚Äúseries‚Äù is used a the variable¬† in the lambda function\nsorts series (ascending), takes the top two values (using negative list indexing), and averages them\n\n\n\n\n\n\nScope\n\nPopulated objects within functions persist if you instantiate the object in the argument\n\n\n‚Äúall_numbers‚Äù retained it‚Äôs previous value when the 2nd call to the function was made\n\n\n\n\nClosures\n\nInner functions that can access values in the outer function, even after the outer function has finished its execution\nExample\n\n# closure way\ndef balanceOwed(roomN,rate,nights):\n¬† ¬† def increaseByMeals(extra):\n¬† ¬† ¬† ¬† amountOwned=rate*nights+extra\n¬† ¬† ¬† ¬† print(f\"Dear Guest of Room [{roomN}]{style='color: #990000'}, you have\",¬†\n¬† ¬† ¬† ¬† \"a due balance:\", \"${:.2f}\".format(amountOwned))\n¬† ¬† ¬† ¬† return amountOwned\n¬† ¬† return increaseByMeals\n\nba = balanceOwned(201,400,3)\nba(200)\nba(150)\nba(180)\nba(190)\nDear Guest of Room 201, you have a due balance: $1400.00\nDear Guest of Room 201, you have a due balance: $1350.00\nDear Guest of Room 201, you have a due balance: $1380.00\nDear Guest of Room 201, you have a due balance: $1390.00\n\nTedious way: For each value of ‚Äúextra‚Äù (e.g.¬†meals), the function needs to be called even if the other values of the arguments don‚Äôt change.\nClosure way:\n\nincreaseByMeals() is a closure function, because it remembers the values of the outer function balanceOwed(), even after the execution of the latter\nbalanceOwed() is called with its three arguments only once and then after its execution, we call it four times with the meal expenses (‚Äúextra‚Äù).",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-mods",
    "href": "qmd/python-general.html#sec-py-gen-mods",
    "title": "General",
    "section": "Modules",
    "text": "Modules\n\n.py files are called ‚Äúmodules.‚Äù\nA directory with .py files in which one of the files is an ‚Äú__init__.py‚Äù is called a package.\nMisc\n\nResource: Make your Python life easier by learning how imports find things\nsys.path contains the list of paths where Python is looking for things to import. Your virtual environment and the directory containing your entry point are automatically added to sys.path.\n\nsys.path¬†is a list. Which means you can¬†.append(). Any directory you add there will have its content importable. It‚Äôs a useful hack, but use it as a last resort.\n\nWhen using -m flag to run a script, if you pass a package instead of a module, the package must contain a ‚Äú__main__.py‚Äù file for it to work. This __main__.py module will run.\nIf you have scripts in your projects, don‚Äôt run them directly. Run them using ‚Äú-m‚Äù, and you can assume everything starts from the root.\n\nExample:\ntop_dir\n‚îú‚îÄ‚îÄ foo\n‚îÇ   ‚îú‚îÄ‚îÄ bar.py\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îî‚îÄ‚îÄ blabla.py\n‚îî‚îÄ‚îÄ blabla.py\n\nRunning python foo/bar.py, ‚Äútop_dir‚Äù is the current working directory, but ‚Äúfoo‚Äù is added to the sys.path.\nRunning python -m foo.bar, ‚Äútop_dir‚Äù is the current working directory and added to sys.path.\n\nImports can all start from the root of the project and opened file paths as well.\n\n\n\n\nUsage\n\nProject Structure\n‚îú‚îÄ‚îÄ main.py\n‚îú‚îÄ‚îÄ packages\n‚îÇ¬† ‚îî‚îÄ‚îÄ __init__.py\n‚îÇ¬† ‚îî‚îÄ‚îÄ module_1.py\n‚îÇ¬† ‚îî‚îÄ‚îÄ module_2.py\n‚îÇ¬† ‚îî‚îÄ‚îÄ module_3.py\n‚îî‚îÄ‚îÄ ‚îî‚îÄ‚îÄ module_4.py\n\n‚Äú__init__.py‚Äù contains only 1 line which declares all the functions (or classes?) that are in the modules\n__all__ = [\"func1\", \"func2\"]\n\nIf the module files contained classes with multiple functions, I think you‚Äôd just declare the classes and not every function in that class.\n\nIf using classes, each module should only have 1 class.\n\n\nScripts need to include ‚Äú_main_‚Äù in order to used in other scripts\n# test_function.py\ndef function1():¬†\n¬† ¬† print(\"Hello world\")¬†\nfunction1()\n\n# Define the __main__ script\nif __name__ == '__main__':¬† ¬†\n¬† ¬† # execute only if run as a script\n¬† ¬† function1()\n\nSays if this file is being run non-interactively (i.e.¬†as a script), run this chunk\nAdd else: chunk, then that chunk will be run only if the file is imported as a module\nAllows you to allow or prevent parts of code from being run when the modules are imported\nImporting a module without _main_ in a jupyter notebook results in this\n\n\nLoading\n\nDO NOT USE from &lt;library&gt; import *\n\nThis will import anything and everything from that library and causes several problems:\n\nYou don‚Äôt know what is in that package, so you have no idea what you just imported, or even if what you want is in there.\nYou just filled your local namespace with an unknown quantity of mysterious names, and you don‚Äôt know what they will shadow.\nYour editor will have a hard time helping you since it doesn‚Äôt know what you imported.\nYour colleague will hate you because they have no idea what variables come from where.\n\nException: In the shell, it‚Äôs handy. Sometimes, you want to import all things in __init__.py and you have ‚Äú__all__‚Äù defined (see above)\n\nFrom the working directory, it‚Äôs like importing from a library: from file1 import function1\nFrom a subdirectory, from subdirectory.file1 import function1\nFrom a directory outside the project, add the module to sys.path before importing it\nimport sys\nsys.path.append('/User/moduleDirectory')\n\nWhen a module is imported, it first searches for built-in modules, then the paths listed in sys.path\nThis appends the new path to the end of the sys.path\nimport sys\nsys.path.insert(1, '/User/moduleDirectory')\nPuts this path at the front of the sys.path directory list.\nimport sys\nsys.path.remove('/User/NewDirectory')\n\n*delete path from sys.path after you finish*\nPython will also search this path for future projects unless they are removed",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-cond",
    "href": "qmd/python-general.html#sec-py-gen-cond",
    "title": "General",
    "section": "Conditionals",
    "text": "Conditionals\n\nIf-Else\n\nSyntax\nif &lt;expression&gt;:\n¬† ¬† do something\nelse:\n¬† ¬† do something else\nExample\nregenerate = False\nif regenerate:\n    concepts_list = df2Graph(df, model='zephyr:latest')\n    dfg1 = graph2Df(concepts_list)\n    if not os.path.exists(outputdirectory):\n        os.makedirs(outputdirectory)\n\n    dfg1.to_csv(outputdirectory/\"graph.csv\", sep=\"|\", index=False)\n    df.to_csv(outputdirectory/\"chunks.csv\", sep=\"|\", index=False)\nelse:\n    dfg1 = pd.read_csv(outputdirectory/\"graph.csv\", sep=\"|\")\n\nTry-Except\n\nExample\nimport os\ntry:\n¬† ¬† env_var = os.environ['ENV']\nexcept KeyError:\n¬† ¬† # Do something\n\nIf ‚ÄúENV‚Äù is not a present a KeyError is thrown. Then, except section executed.\n\n\nMatch (&gt; Python 3.10) (switch function)\nmatch object:\n¬† ¬† case &lt;pattern_1&gt;:\n¬† ¬† ¬† ¬† &lt;action_1&gt;\n¬† ¬† case &lt;pattern_2&gt;:\n¬† ¬† ¬† ¬† &lt;action_2&gt;\n¬† ¬† case &lt;pattern_3&gt;:\n¬† ¬† ¬† ¬† &lt;action_3&gt;\n¬† ¬† case _:\n¬† ¬† ¬† ¬† &lt;action_wildcard&gt;\n\n‚Äúobject‚Äù is just a variable name; could be anything\n‚Äúcase_‚Äù is the value used when none of the other cases are a match\nExample: function input inside user function\ndef http_error(status):\n¬† ¬† match status:\n¬† ¬† ¬† ¬† case 200:\n¬† ¬† ¬† ¬† ¬† ¬† return 'OK'\n¬† ¬† ¬† ¬† case 400:\n¬† ¬† ¬† ¬† ¬† ¬† return 'Bad request'\n¬† ¬† ¬† ¬† case 401 | 403 | 404:\n¬† ¬† ¬† ¬† ¬† ¬† return 'Not allowed'\n¬† ¬† ¬† ¬† case _:\n¬† ¬† ¬† ¬† ¬† ¬† return 'Something is wrong'\nExample: dict input inside a function\ndef get_service_level(user_data: dict):\n¬† ¬† match user_data:\n¬† ¬† ¬† ¬† case {'subscription': _, 'msg_type': 'info'}:\n¬† ¬† ¬† ¬† ¬† ¬† print('Service level = 0')\n¬† ¬† ¬† ¬† case {'subscription': 'free', 'msg_type': 'error'}:\n¬† ¬† ¬† ¬† ¬† ¬† print('Service level = 1')\n¬† ¬† ¬† ¬† case {'subscription': 'premium', 'msg_type': 'error'}:\n¬† ¬† ¬† ¬† ¬† ¬† print('Service level = 2')\nExample: inside a class\nclass ServiceLevel:\n¬† ¬† def __init__(self, subscription, msg_type):\n¬† ¬† ¬† ¬† self.subscription = subscription\n¬† ¬† ¬† ¬† self.msg_type = msg_type\n\n¬† ¬† def get_service_level(user_data):\n¬† ¬† ¬† ¬† match user_data:\n¬† ¬† ¬† ¬† ¬† ¬† case ServiceLevel(subscription=_, msg_type='info'):\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print('Level = 0')\n¬† ¬† ¬† ¬† ¬† ¬† case ServiceLevel(subscription='free', msg_type='error'):\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print('Level = 1')\n¬† ¬† ¬† ¬† ¬† ¬† case ServiceLevel(subscription='premium', msg_type='error'):\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print('Level = 2')\n¬† ¬† ¬† ¬† ¬† ¬† case _:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print('Provide valid parameters')\n\nNote that inside the function, the change from ‚Äú:‚Äù to ‚Äú=‚Äù¬† and ‚Äú()‚Äù following the class name in the ‚Äúcase‚Äù portion of the match\n\n\nAssert\n\nUsed to confirm a condition\n\nIncorrect: assert condition, message¬†\n\nCorrect method:¬†\nif not condition:¬†\n¬† ¬† raise AssertionError\n\nassert is useful for debugging code because it lets you test if a condition in your code returns True, if not, the program will raise an AssertionError.\n** Do not use in production, because when code is executed with the -O (optimize) flag, the assert statements are removed from the bytecode. **",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-loops",
    "href": "qmd/python-general.html#sec-py-gen-loops",
    "title": "General",
    "section": "Loops",
    "text": "Loops\n\nMisc\n\nList Comprehensions vs Generators in terms of memory usage\n\n{{tqdm}} - progress bar for loops\nfrom tqdm import tqdm\nfor i in tqdm(range(10000))\n¬† ¬† ...\nbreak terminates the loop containing it\n\nIf in a nested loop, it will terminate the inner-most loop containing it\n\ncontinue is used to skip the remaining code inside a loop for the current iteration only; forces the start of the next iteration of the loop\npass does nothing\n\nused when a statement or a condition is required to be present in the program but we do not want any command or code to execute\n\n\n\n\nIterators\n\nRemembers values\nExample\nD = {\"123\":\"Y\",\"111\":\"PT\",\"313\":\"Y\",\"112\":\"Y\",\"201\":\"PT\"}\nff = filter(lambda e:e[1]==\"Y\", D.items())\n\nprint(next(ff))\n&gt;&gt; ('123', 'Y')\nprint(next(ff))\n&gt;&gt; ('313', 'Y')\napply\n\naxis\n\n0 or ‚Äòindex‚Äô: apply function to each column.\n1 or ‚Äòcolumns‚Äô: apply function to each row.\n\nExample: Function applied to rows of a column of a dataframe (i.e.¬†cells)\ndef df2Graph(dataframe: pd.DataFrame, model=None) -&gt; list:\n  # dataframe.reset_index(inplace=True)\n  results = dataframe.apply(\n    lambda row: graphPrompt(row.text, {\"chunk_id\": row.chunk_id}, model), axis=1\n  )\n\ntext and chunk_id are column names of the dataframe\nrow is the row of the dataframe since axis=1, and from that row, the columns text and chunk_id are subsetted in the arguments of user-defined function.\n\nExample: Formula applied to rows of a column of a dataframe (i.e.¬†cells)\ngrade['evaluate']=grade['MathScore'].apply(lambda x: round((x**x)/2,2))\n\ngrade is the df; MathScore is a numeric column; evaluate is the new column in the df\n\n\n\n\n\nGenerators\n\nGenerators are iterators, a kind of iterable you can only iterate over once. (normal iterators like lists, strings, etc. can be repeatedly iterated over)\nGenerators do not store all the values in memory, they generate the values on the fly\n\nyield - Pauses the function saving all its states and later continues from there on successive calls.\n\nAllows you to consume one element at a time and work with it without requiring you to have every element in memory.\nProduces a generator\n\n\nMisc\n\n{{itertools}} islice can slice a generator.\nAlso see APIs &gt;&gt; {{requests}} for an example\n\nExample: Using a comprehension¬†\nmygenerator = (x*x for x in range(3))\nfor i in mygenerator:\n...¬† ¬† print(i)\n\nProduce a list and ( ) produce a generator¬†\n\nExample: Using a function\ndef create_generator():\n¬† ¬† mylist = range(3)\n¬† ¬† for i in mylist:\n¬† ¬† ¬† ¬† yield i*i\n\nfor i in mygenerator:\n¬† ¬† print(i)\n0\n1\n4\n\nThe first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then it‚Äôll return the first value of the loop.\nThen, each subsequent call will run another iteration of the loop you have written in the function and return the next value.\nThis will continue until the generator is considered empty, which happens when the function runs without hitting yield.\n\nThat can be because the loop has come to an end, or because you no longer satisfy an ‚Äúif/else‚Äù\n\n\nExample: Sending values to (yield)\ndef grep(pattern):\nprint \"Looking for %s\" % pattern\nwhile True:\n¬† ¬† line = (yield)\n¬† ¬† if pattern in line:\n¬† ¬† ¬† ¬† print line,\ng = grep(\"python\")¬† # instantiate with \"python\" pattern to search for\n\ng.next() # Prime it\n&gt;&gt; Looking for python\n\ng.send(\"A series of tubes\") # \"python\" not present so returns nothing\ng.send(\"python generators rock!\") # \"python\" present so returns line\n&gt;&gt; python generators rock!\ng.close() # closes coroutine\n\n(yield) receives the input of the .send method and creates a generator object which is assigned to ‚Äúline‚Äù.\nAll coroutines must be ‚Äúprimed‚Äù by first calling .next() (or send(None))\n\nThis advances execution to the location of the first yield expression\n\n\nExample: Sending values to (yield)\ndef writer():\n¬† ¬† \"\"\"A coroutine that writes data *sent* to it to fd, socket, etc.\"\"\"\n¬† ¬† while True:\n¬† ¬† ¬† ¬† w = (yield)\n¬† ¬† ¬† ¬† print('&gt;&gt; ', w)\ndef writer_wrapper(coro):\n¬† ¬† # TBD\n¬† ¬† pass\nw = writer()\nwrap = writer_wrapper(w)\nwrap.send(None)¬† # \"prime\" the coroutine\nfor i in range(4):\n¬† ¬† wrap.send(i)\n&gt;&gt;¬† 0\n&gt;&gt;¬† 1\n&gt;&gt;¬† 2\n&gt;&gt;¬† 3\n\nA more complex framework if you want to break the workflow into multiple functions\n\n\n\nUsing yield from\n\nAllows for two-way usage (reading/sending) of generators\nExample (reading from a generator)\ndef reader():\n¬† ¬† \"\"\"A generator that fakes a read from a file, socket, etc.\"\"\"\n¬† ¬† for i in range(4):\n¬† ¬† ¬† ¬† yield '&lt;&lt; %s' % i\n\n# with yield\ndef reader_wrapper(g):\n¬† ¬† # Manually iterate over data produced by reader\n¬† ¬† for v in g:\n¬† ¬† ¬† ¬† yield v\n# OR with yield from\ndef reader_wrapper(g):\n¬† ¬† yield from g\nwrap = reader_wrapper(reader())\nfor i in wrap:\n¬† ¬† print(i)\n\nBasic; only eliminates 1 line of code\n\nExample (sending to a generator)\n# with (yield)\ndef writer_wrapper(coro):\n¬† ¬† coro.send(None)¬† # prime the coro\n¬† ¬† while True:\n¬† ¬† ¬† ¬† try:\n¬† ¬† ¬† ¬† ¬† ¬† x = (yield)¬† # Capture the value that's sent\n¬† ¬† ¬† ¬† ¬† ¬† coro.send(x)¬† # and pass it to the writer\n¬† ¬† ¬† ¬† except StopIteration:\n¬† ¬† ¬† ¬† ¬† ¬† pass\n# OR with yield from\ndef writer_wrapper(coro):\n¬† ¬† yield from coro\n\nNeed to see example 4 for the writer() code and the use case\nShows the other advantage of using ‚Äúyield from‚Äù: it automatically includes the code to stop prime and stop the loop.\n\nReusable generator\n\n\nreading example using ‚Äúyield from‚Äù\n\nSlicing a generator\nfrom itertools import islice\ndef gen():\n¬† ¬† yield from range(1,11)\ng = gen()\nmyslice = islice(g, 2)\n&gt;&gt; list(myslice)\n[1, 2]\n&gt;&gt; [i for i in g]\n[3,4,5,6,7,8,9,10]\n\n\n\n\nFor\n\n\nSyntax - for &lt;sequence&gt;: &lt;loop body&gt;\nNumeric Range\nfor i = 1 to 10\n¬† ¬† &lt;loop body&gt;\n\n# from 0 to 519\nfor i in range(520)\n¬† ¬† &lt;loop body&gt;\n\nres = 0\nfor idx in np.arange(0, 100000):\n¬† res += df.loc[idx, 'int']\n\nnp.arange() ran 8000 times faster than the same chunk using range()\n\nList\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\neven_numbers = []\nfor item in numbers:\n¬† ¬† if item % 2 == 0:\n¬† ¬† ¬† ¬† even_numbers.append(item)\nprint(even_numbers)\n\n# results: [2, 4, 6, 8]\nList: index and value\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nfor index, element in enumerate(numbers):\n¬† ¬† if element % 2 != 0:\n¬† ¬† ¬† ¬† numbers[index] = element * 2\n¬† ¬† else:\n¬† ¬† ¬† ¬† continue\nprint(numbers)\n# results: [2, 2, 6, 4, 10, 6, 14, 8, 18]\n\nenumerate also gets the index of the respective element at the same time\n\nWith three expressions\nfor (for i = 1; i &lt;= 10; i+=1)\n¬† ¬† &lt;loop body&gt;\nCollection-Based\n\nIf the collection is a dict, then this just iterates over the keys\n\nfor i in &lt;collection&gt;:\n¬† ¬† &lt;loop body&gt;\nIterate over a sliding window\n\nOver dictionary keys and values of a dict\nfor a,b in transaction_data.items():\n¬† ¬† print(a,‚Äô~‚Äô,b)\n\nThe .items method includes both key and value, so it iterates over the pairs.\n\nOver nested dictionaries\nfor k, v in transaction_data_n.items():\n¬† ¬† if type(v) is dict:\n¬† ¬† ¬† ¬† for nk, nv in v.items():¬†\n¬† ¬† ¬† ¬† ¬† ¬† print(nk,‚Äô ‚Üí‚Äô, nv)\n\nIf the item of the dict is itself a dict then another loop iterates through its items.\nnk and nv stand for nested key and nested value\n\nSelecting a specific item in a nested dictionary\nfor k, v in transaction_data_n.items():\n¬† ¬† if type(v) is dict and k == 'transaction_2':\n¬† ¬† ¬† ¬† for sk, sv in v.items():\n¬† ¬† ¬† ¬† ¬† ¬† print(sk,'--&gt;', sv)\n\nOnly transaction_2‚Äô s items are printed\n\nRows of a data.frame\nres = 0\nfor row in df.itertuples():\n¬† res += getattr(row, 'int')\n\nitertuples()¬† is 30x faster than iterrows()\n\n\n\n\nzip\n\nCombine lists into 1 list of tuples\nacc_values = [1, 0.04, 0.9]\nacc_names = [\"RMSE\", \"MAPE\", \"R-sq\"]\nacc_list = list(zip(acc_names, acc_values))\nacc_list\n[('RMSE', 1), ('MAPE', 0.04), ('R-sq', 0.9)]\n\nzip does take lists of different lengths but will create shortest length list with corresponding elements\nCombine lists of unequal lengths but keep the non-paired elements\nfrom itertools import zip_longest\nacc_names3 = [\"RMSE\", \"MAPE\", \"R-sq\", \"MSE\"]\nacc_values3 = [rmse, mape, rsq]¬†\nacc_list3 = list(zip_longest(acc_names3, acc_values3))\n\nUnzip list of tuples into separate lists\nnames, values = zip(*acc_list)\n\nAsterisk is the ‚Äúunzipping operator‚Äù\n\nUnpack dict into a list of separate tuples for key:value pairs\nacc_tuples = list(zip(acc_dict.keys(), acc_dict.values()))\nacc_tuples\n[('RMSE', 1), ('MAPE', 0.04), ('R-sq', 0.9)]\n\n\n\nComprehensions\n\nMisc\n\n‚Äòfor ‚Äî in‚Äô construct within comprehensions is faster than the traditional for-loops\n\nnot faster than (all?) lambda-filters (see functions &gt;&gt; lambda)\n\nReturns lists or dicts (just change the bracket types)\n\nDicts\n\nSyntax: mydict = {key:val for key, val in zip(keys_list, vals_list)}\nCombine key:value lists into a dictionary\nacc_dict = {k:v for k,v in zip(acc_names, acc_values)}\nReturn value and output of expression\nmydict = {v: v**2 for v in numberslist}\nIf numberslist =[1,2,3], then mydict = {1:1, 2:4, 3:9}\n\nLists\n\nSyntax: newlist = [expression for item in iterable if condition == True]\nWith expression\nmylist = [x**2 for x in numberslist]\n\nif numberslist =[1,2,3], then mylist = [1,4,9]\n\nSet values in a list to uppercase\nnewlist = [x.upper() for x in fruits]\nWith conditional expression (if ‚Äî else)\n\nAppend to the comprehension to filter the dictionary or list\nSyntax: mylist = [expressionA if (condition2==True) else expressionB for item in list if (condition1==True)]\nExample: newlist = [x if x != \"banana\" else \"orange\" for x in fruits]\n\nReturn ‚Äúorange‚Äù instead of ‚Äúbanana‚Äù\n\nExample: new_list = [(x**2) if (x&gt;90) else (x**3) for x in old_list if (x%2==0)]\n\nSays\n\nSquare an argument if it exceeds 90, else cube it¬†\nReturn all the exponentiated results only if the argument was an even number\n\n\nExample: c = [d for d in datstrlist if ((d.endswith(\"urday\") or d.endswith(\"unday\")) and \"Oc\" in d)]\n\nString filter than looks for strings with saturdays and sundays in october\n*Slower than a lamda-filter* (See Functions &gt;&gt; lambda)\n\n\n\nNested\n\nSyntax: myset = {{expression(itemA, itemB) for itemA in setA} for itemB in setB}\nExample: {j for i in range(2, int(N**0.5)+1) for j in range(i**2, N, i)}\n\nN = 100000\nCreates a set of all the integers from 2 to 100,000.\nPaces through all the integers i up to the square root of N\nDiscards from the set of 100,000 those numbers j which are equal or larger than the square of i\n\nExample: From link\n# Function to get set labels\ndef get_prediction_set_labels(prediction_set, class_labels):\n    # Get set of class labels for each instance in prediction sets\n    prediction_set_labels = [\n        set([class_labels[i] for i, x in enumerate(prediction_set) if x]) for prediction_set in \n        prediction_sets]\n    return prediction_set_labels\n\nReturns a list where each object in the list is a set object (e.g.¬†{green}, {green, orange})",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-debug",
    "href": "qmd/python-general.html#sec-py-gen-debug",
    "title": "General",
    "section": "Debugging",
    "text": "Debugging\n\nMisc\nTerms\n\nException Errors - Raised when the syntax is correct but the program results in an error.\nSyntax Errors - Occur when the interpreter detects invalid syntax (relatively easier to fix)\n\ne.g.¬†unmatched parenthesis\n\nTraceback - A report that helps us understand the reason for an exception.\n\nContains function calls made in the code along with their line numbers",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-errhand",
    "href": "qmd/python-general.html#sec-py-gen-errhand",
    "title": "General",
    "section": "Error Handling",
    "text": "Error Handling\n\ntry + except\n\nSays try the main code snippet, but if an exception (error) occurs, run the secondary code snippet, the workaround.\n\ndef pct_difference_error_handling(n1, n2):\n¬† '''Function that takes two numbers and return the percentual difference\n¬† between n1 and n2, being n1 the reference number'''\n\n¬† # Try the main code\n¬† try:\n¬† ¬† pct_diff = (n1-n2)/n1\n¬† ¬† return f'The difference between {n1} and {n2} is {n1-n2}, which is {pct_diff*100}% of {n1}'\n\n¬† # If you find an error, use this code instead\n¬† except:\n¬† ¬† pct_diff = (int(n1)-int(n2))/int(n1)\n¬† ¬† return f'The difference between {n1} and {n2} is {int(n1)-int(n2){style='color: #990000'}[}]{style='color: #990000'}, which is {pct_diff*100}% of {n1}'\n\n¬† # Optional\n¬† finally:\n¬† ¬† print(\"Code ended\")\n\nAssumes the error will be the user enters a string instead of a numeric. If errors, converts string to numeric and calcs.\nfinally: - This argument will always run, regardless if the try block raises an error or not. So it could be a completion message or a summary, for example.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/distributions.html",
    "href": "qmd/distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "Terms",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-terms",
    "href": "qmd/distributions.html#sec-distr-terms",
    "title": "Distributions",
    "section": "",
    "text": "Conditional Probability Distributions\n\nNotes from https://www.causact.com/joint-distributions-tell-you-everything.html#joint-distributions-tell-you-everything\nNotation: \\(P(Y | X) = P(Y \\;\\text{and}\\; X) / P(X) = P(Y, X) / P(X)\\)\n\ni.e.¬†ratio of 2 marginal distributions\n\nExample: two tests for cancer are conducted to determine whether a biopsy should be performed\n\nConditional approach: Biopsy everyone at determined to be high risk from test 1; measure the genetic marker (aka test 2) for patients at intermediate risk and biopsy those with a probability of cancer past a certain level based on the marker\n\nWhen we perform regression analysis, we are essentially estimating conditional distributions. The conditional distribution, \\(P(Y|X_1, \\ldots, X_n)\\) represents the distribution of the response variable, \\(Y\\), given the specific values of the predictor variables, \\(X_1, \\ldots, X_n\\).\n\nEmpirical CDF\n\\[\nF_n (x) = \\frac {1}{n} \\sum_{i = 1}^n I(X_i \\leq x)\n\\]\n\nWhere \\(X_1, X_2,\\ldots,X_n\\) are from a population with CDF, \\(F_n (x)\\)\nProcess\n\nTake n samples from an unknown distribution. The more samples you take, the closer the empirical distribution will resemble the true distribution.\nSort these samples, and place them on the x-axis.\nStart plotting a ‚Äòstep-function‚Äô style line ‚Äî each time you encounter a datapoint on the x-axis, increase the step by 1/N.\n\nExample\n\n\nThe CDF of a normal distribution (green) and its empirical CDF (blue)\n\n\nJoint Probability Distribution - Assigns a probability value to all possible combinations of values for a set of random variables.\n\nNotation: \\(P(x_1, x_2, ... ,x_n)\\)\nPlugging in a value for each random variable returns a probability for that combination of values\nExample: Two tests for cancer are conducted to determine whether a biopsy should be performed\n\nJoint approach: biopsy anyone who is either at high risk of cancer (test 1) or who was determined to have a probability of cancer past a certain level, based on the marker from the genetic test (test 2)\nCompare with example in Conditional Probability Distributions\n\nIn the context of regression modeling: the joint distribution refers to the distribution of all the variables involved in the regression analysis. For example, if you have a regression model with a response variable \\(Y\\) and predictor variables \\(X_1, \\ldots, X_n\\), the joint distribution would describe the combined distribution of \\(Y, X_1, \\ldots, X_n\\).\n\nLocation - Distribution parameter determines the shift of the distribution\n\ne.g.¬†mean, mu, of the normal distribution.\n\nMarginal Probability Distribution - Assigns a probability value to all possible combinations of values for a subset of random variables\n\nNotation: \\(P(x_1)\\)\n\n\\(P(x_1,x_2)\\) is sometimes called the Joint Marginal Probability Distribution\n\nThe marginal distribution, \\(P(Y)\\) where \\(Y\\) is a subset of random variables, is calculated from the joint distribution, \\(P(Y = y, Z = z)\\) where \\(Z\\) is the subset of random variables not in \\(Y\\) .\n\n\\(P(Y) = \\sum_{Z=z} P(Y = y, Z = z)\\)\n\nIf \\(Y\\) is just one variable\n\nSays sum all the joint probabilities for all the combinations of values for the variables in \\(Z\\) while holding \\(Y\\) constant\nRepeat for each value of \\(Y\\) to get this summed probability value\nThe marginal distribution is made up of all these values, one for each value of \\(Y\\) (or combination of values if \\(Y\\) is a subset of variables)\n\n\nWhen the joint probability distribution is in tabular form, one just sums up the probabilities in each row where \\(Y = y\\).\nIn the context of regression modeling, the marginal distribution of \\(Y\\) represents the distribution of \\(Y\\) alone, without considering the specific values of the predictor variables.\n\n\nScale - Distribution parameter; the larger the scale parameter, the more spread out the distribution\n\ne.g.¬†s.d., sigma, \\(\\sigma\\) of the normal distribtution\nRate Parameter: the inverse of the scale parameter (see Gamma distribution)\n\nShape - Distribution parameter that affects the shape of a distribution rather than simply shifting it (as a location parameter does) or stretching/shrinking it (as a scale parameter does).\n\ne.g.¬†‚ÄúPeakedness‚Äù refers to how round the main peak is",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-tests",
    "href": "qmd/distributions.html#sec-distr-tests",
    "title": "Distributions",
    "section": "Tests",
    "text": "Tests\n\nWhy normality tests are great‚Ä¶ as a teaching example and should be avoided in research\n\ntl;dr; KS test has very low power as a Normality test as compared to Shapiro-Wilk, and Shapiro-Wilk isn‚Äôt very good for n &lt; 100\nFor detecting moderate skew, you want at least n &gt; 75 to get 80% power for Shapiro-Wilk\nShapiro-Wilk can detect very fat tails at n &lt; 100, but would require larger sample sizes to detect more moderately thick tails.\nKS is worthless in detecting fat tails and near-worthless at detecting skew\nWhen n gets large (e.g.¬†1000s), these types of tests will almost always reject the null even when the practical deviation from normality is not practically significant.\n\nKolmogorov‚ÄìSmirnov test (KS)\n\nUsed to compare distributions\n\nCan be used as a Normality test or any distribution test\nCan compare two samples\n\nMisc\n\nVectors may need to be standardized (e.g.¬†normality test) first unless comparing two samples H0: Both distributions are from the same distribution\n\nPackages\n\n{KSgeneral} has tests to use for contiuous, mixed, and discrete distributions written in C++\n{stats} and {dgof} also have functions, ks.test\n\nBoth handle continuous and discrete distributions\n\nAll functions take a numeric vector and a base R density function (e.g.¬†pnorm, pexp, etc.) as args\n\nKSgeneral docs don‚Äôt say you can supply your own comparison sample (2nd arg) only the density function but with stats and dgof, you can.\nAlthough they have function to compute the CDFs, so if you need speed, it might be possible to use their functions and do it manually\n\n\n2-sample test as the greatest distance between the CDFs (Cumulative Distribution Function) of each sample\n\nSpecifically, this test determines the distribution of your unknown data sample by constructing and comparing the sample‚Äôs empirical CDF¬† (see Terms) with the CDF you hypothesized. If the two CDFs are close, your unknown data sample likely follows the hypothesized distribution.\n\nKS statistic, \\(D_{n,m} = \\max|\\text{CDF}_1 - \\text{CDF}_2|\\) where \\(n\\) as the number of observations on Sample 1 and \\(m\\) as the number of observations in Sample 2\nCompare the KS statistic with the respective KS distribution based on parameter ‚Äúen‚Äù to obtain the p-value of the test\n\n\\(en = (m \\times n) / (m + n)\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-beta",
    "href": "qmd/distributions.html#sec-distr-beta",
    "title": "Distributions",
    "section": "Beta",
    "text": "Beta\n\n\nDefined on the interval [0,1]\nThe key difference between the Binomial and Beta distributions is that for the Beta distribution the probability, x, is a random variable, however for the Binomial distribution the probability, x, is a fixed parameter.\nShape parameters are \\(\\alpha\\) and \\(\\beta\\), usually.\n\n\\(\\alpha\\) and \\(\\beta\\) are two positive parameters that appear as exponents of the random variable\n\npdf\n\\[\nf(x) = \\frac {x^{\\alpha - 1} (1-x)^{\\beta - 1}} {B(\\alpha, \\beta)}\n\\]\n\\(\\mathbb{E}(X) = \\frac {\\alpha} {\\alpha + \\beta}\\)\n\\(\\text{Var}(X) = \\frac {\\alpha \\cdot \\beta} {(\\alpha + \\beta)^2 \\cdot (\\alpha + \\beta + 1)}\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-betbin",
    "href": "qmd/distributions.html#sec-distr-betbin",
    "title": "Distributions",
    "section": "Beta-Binomial",
    "text": "Beta-Binomial\n\n\n\n\n\n\n\n\nWhere k is the number of events in n trials\n\n\n\n\n\n\n\nWhere \\(\\theta\\) is the probability of an event\n\n\n\n\n\n\n\nUsed when the probability of success, p, in a fixed number of Bernoulli trials is unknown or random and can change from trial to trial.\nShape parameters Œ± and Œ≤ define the probability of success (i.e.¬†the success parameter is modeled by the Beta Distribution).\n\nFor large values of Œ± and Œ≤, the distribution approaches a binomial distribution.\nWhen Œ± and Œ≤ both equal 1, the distribution equals a discrete uniform distribution from 0 to n\n\nAccuracy analysis data from psychology follow beta-binomial distributions (Jaeger, 2008; Kruschke, 2014)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-dirichlet",
    "href": "qmd/distributions.html#sec-distr-dirichlet",
    "title": "Distributions",
    "section": "Dirichlet",
    "text": "Dirichlet\n\nA family of continuous multivariate probability distributions parameterized by a vector Œ± of positive reals",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-exp",
    "href": "qmd/distributions.html#sec-distr-exp",
    "title": "Distributions",
    "section": "Exponential",
    "text": "Exponential\n\n\nNotes from\n\nStatistical Rethinking &gt;&gt; Chapter 10\n\nConstrained to be zero or positive\nFundamental distribution of distance and duration, kinds of measurements that represent displacement from some point of reference, either in time or space.\nIf the probability of an event is constant in time or across space, then the distribution of events tends towards exponential.\nIts shape is described by a single parameter, the rate of events \\(\\lambda\\), or the average displacement \\(\\lambda ‚àí1\\) .\nThis distribution is the core of survival and event history analysis",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gamma",
    "href": "qmd/distributions.html#sec-distr-gamma",
    "title": "Distributions",
    "section": "Gamma",
    "text": "Gamma\n\n\nNotes from\n\nStatistical Rethinking &gt;&gt; Chapter 10\n\nConstrained to be zero or positive\nLike Exponential but can have a peak above zero\nIf an event can only happen after two or more exponentially distributed events happen, the resulting waiting times will be gamma distributed.\n\ne.g.¬†age of cancer onset is approximately gamma distributed, since multiple events are necessary for onset.\n\nThe gamma can be viewed as the sum of iid n exponential random variables. Exponential random variables have a rate parameter, so it makes sense for the Gamma to inherit a rate parameterization. The rate parameter also happens to be related to a scale parameter, so it makes sense for the Gamma to have a scale parameterization.\nShape parameter \\(k\\) and a scale parameter \\(\\theta\\)\n\\(\\mathbb{E}[X] = k\\theta = \\frac{\\alpha}{\\beta}\\)\n\nShape parameter \\(\\alpha = k\\) and an\nInverse Scale parameter (aka Rate Parameter) \\(\\beta = \\frac {1}{\\theta}\\)\nTherefore if you want a gamma distributions with a certain ‚Äúmean‚Äù and ‚Äústandard deviation,‚Äù you‚Äôd:\n\nSet your mean to \\(\\mathbb{E}[X]\\), your standard deviation to \\(\\theta\\) (probably but maybe it‚Äôs \\(\\beta\\))\nCalculate \\(\\beta\\)\nCalculate \\(\\alpha\\)\nprior(gamma(alpha, beta))\n\n\nExample: Gamma distribution as the sums of random exponential variables\n\nn &lt;- 12\nbeta &lt;- 1.2\n\nrvs &lt;- replicate(1000, {\n  sum(rexp(n, beta))\n})\n\nhist(rvs, freq = F)\ncurve(dgamma(x, shape = n, rate = beta), col='red', add=T)\n\nGamma distribution density overlayed with a histogram of exponential variable sums\n\nUsed in Survival Regression",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gauss",
    "href": "qmd/distributions.html#sec-distr-gauss",
    "title": "Distributions",
    "section": "Gaussian",
    "text": "Gaussian\n\nSpecial case of Student‚Äôs t-distribution with the \\(\\nu\\) parameter (i.e.¬†degree of freedom) set to infinity.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gumb",
    "href": "qmd/distributions.html#sec-distr-gumb",
    "title": "Distributions",
    "section": "Gumbel",
    "text": "Gumbel\n\n\nKnown as the type-I generalized extreme value distribution\n\nEVT says it is likely to be useful if the distribution of the underlying sample data is of the normal or exponential type.\n\nUsed to model the distribution of the maximum (or the minimum) of a number of samples of various distributions.\n\nTo model minimums, use the negative of the original data.\n\nUse Cases\n\nRepresent the distribution of the maximum level of a river in a particular year if there was a list of maximum values for the past ten years.\nPredicting the chance that an extreme earthquake, flood or other natural disaster will occur.\nDistribution of the residuals in Multinomial Logit and Nested Logit models\n\nParameters\n\nGumbel(\\(\\mu, \\beta\\)) (location, scale)\nMean: \\(\\mu + \\beta\\gamma\\) where \\(\\gamma\\) is Euler‚Äôs constant (\\(\\approx\\) 0.5772)\nMedian: \\(\\mu - \\beta \\ln(\\ln(2))\\)\nMode: \\(\\mu\\)\nVariance: \\(\\frac{\\pi^2}{6}\\beta^2\\)\nStandard Gumbel: When \\(\\mu = 0\\), mean = \\(\\gamma\\), median = \\(-\\ln(\\ln(2)) \\approx 0.3665\\) and the standard deviation = \\(\\pi/\\sqrt{6}\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-multgauss",
    "href": "qmd/distributions.html#sec-distr-multgauss",
    "title": "Distributions",
    "section": "Multivariate Gaussian",
    "text": "Multivariate Gaussian\n\nIf the random variable components in the vector are not normally distributed themselves, the result is not multivariate normally distributed.\nVariance-Covariance matrix must be semi-definite and therefore symmetric\n\nExample of not symmetric for two random variables",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-pareto",
    "href": "qmd/distributions.html#sec-distr-pareto",
    "title": "Distributions",
    "section": "Pareto",
    "text": "Pareto\n\nAlso see Extreme Value Theory &gt;&gt; Distribution Tail Classification\n‚ÄúGaussian distributions tend to prevail when events are completely independent of each other. As soon as you introduce the assumption of interdependence across events, Paretian distributions tend to surface because positive feedback loops tend to amplify small initial events.‚Äù\nPareto has similar relationship with the exponential distribution as lognormal does with normal \\[\nY_{exp} = \\log \\frac {X_{pareto}} {x_m}\n\\]\n\nWhere \\(X_{pareto} = x_m e^{Y_{\\text{exp}}}\\)\n\n\\(x_m\\) is the (positive) minimum of the randomly distributed pareto variable, X that has index Œ±\n\\(Y_{exp}\\) is exponentially distributed with rate \\(\\alpha\\)\n\n\nSome theoretical statistical moments may not exist\n\nIf the theoretical moments do not exist, then calculating the sample moments is useless\nExample: Pareto (\\(\\alpha\\) = 1.5) has a finite mean and an infinite variance\n\nNeed \\(\\alpha &gt; 2\\) for a finite variance\nNeed \\(\\alpha &gt; 1\\) for a finite mean\nIn general you need \\(\\alpha &gt; p\\) for the pth moment to exist\nIf the nth moment is not finite, then the (n+1)th moment is not finite.\n\n\nFat Tails \\[\n\\bar{F} = x^{-\\alpha} L(x)\n\\]\n\n\\(L(x)\\) is just characterized as slowly varying function that gets dominated by the decaying inverse power law element, \\(x-\\alpha\\). as \\(x\\) goes to infinity\n\n\\(\\alpha\\) is a shape parameter, aka ‚Äútail index‚Äù aka ‚ÄúPareto index‚Äù",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-poisson",
    "href": "qmd/distributions.html#sec-distr-poisson",
    "title": "Distributions",
    "section": "Poisson",
    "text": "Poisson\n\nObtained as the limit of the binomial distribution when the number of attempts is high and the success probability low. Or the Poisson distribution can be approximated by a normal distribution when Œª is large\nProbability Mass Function \\[\n\\text{Pr}(Y = y) = f(y; \\lambda) = \\frac {e^{-\\lambda} \\cdot \\lambda^y} {y!}\n\\]\n\n\\(\\mathbb{E}[Y] = \\text{Var}(Y) = \\lambda\\)\n\n{distributions3}\n\nStats\nY &lt;- Poisson(lambda = 1.5) \nprint(Y) \n## [1] \"Poisson distribution (lambda = 1.5)\"\n\nmean(Y) \n## [1] 1.5 \nvariance(Y) \n## [1] 1.5 \npdf(Y, 0:5) \n## [1] 0.22313 0.33470 0.25102 0.12551 0.04707 0.01412 \ncdf(Y, 0:5) \n## [1] 0.2231 0.5578 0.8088 0.9344 0.9814 0.9955 \nquantile(Y, c(0.1, 0.5, 0.9)) \n## [1] 0 1 3 \nset.seed(0) \nrandom(Y, 5) \n## [1] 3 1 1 2 3\n\nVisualize\n\nplot(Poisson(0.5), main = expression(lambda == 0.5), xlim = c(0, 15)) \nplot(Poisson(2),   main = expression(lambda == 2),   xlim = c(0, 15)) \nplot(Poisson(5),   main = expression(lambda == 5),   xlim = c(0, 15)) \nplot(Poisson(10),  main = expression(lambda == 10),  xlim = c(0, 15))",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-studt",
    "href": "qmd/distributions.html#sec-distr-studt",
    "title": "Distributions",
    "section": "Student‚Äôs t-distribution",
    "text": "Student‚Äôs t-distribution\n\nStandard Deviation\n\\[\n\\text{sd} = \\sqrt {\\frac {\\nu} {\\nu - 2}}\n\\]\n\n\\(\\nu\\) = degrees of freedom\n\nWhen ŒΩ is small, the Student‚Äôs t-distribution is more robust to multivariate outliers\nThe smaller the degree of freedom, the more ‚Äúheavy-tailed‚Äù it is\n\n\n-3 on the y-axis says that the probability of being in the tail is 1 in 103\n\nDon‚Äôt pay attention to the x-axis. Just note how much the probability of being in the tail gets larger as the dof get smaller\n\nAs the degrees of freedom goes to 1, the t distribution goes to the Cauchy distribution\nAs the degrees of freedom goes to infinity, it goes to the Normal distribution.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-tri",
    "href": "qmd/distributions.html#sec-distr-tri",
    "title": "Distributions",
    "section": "Triangular",
    "text": "Triangular\n\nTriangle shaped distribution\nUseful when you have a known min and max value\nextraDistr::rtriang(n, a, b, c) %\\&gt;% hist()\n\n# Discrete distribution\nextraDistr::rtriang(n, a, b, c) %\\&gt;% round() \\`\\`\\`\n\nn is the number of random values you wish to draw\na is the min value\nb is the max value\nc is the mode\n\nCan use to adjust the skew of the distribution\n\n\n\n\n\n\nWhere k is the number of events in n trials\nWhere \\(\\theta\\) is the probability of an event",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/mathematics-glossary.html",
    "href": "qmd/mathematics-glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "A priori - a type of knowledge that can be derived by reason alone\n\nA priori analyses are performed as part of the research planning process.\n\nA posteriori - a type of knowledge that expresses an empirical fact unknowable by reason alone.\n\nSame as post-hoc. Post-Hoc analysis is conducted after the experiment.\n\nbias - see unbiased estimator\nceteris paribus - latin for ‚Äúall things being equal‚Äù or ‚Äúother things held constant.‚Äù\nclosed form - a mathematical expression that uses a finite number of standard operations. It may contain constants, variables, certain well-known operations, and functions, but usually no limit, differentiation, or integration.\nconsistency - Requires that the outcome of the procedure with unlimited data should identify the underlying truth. Usage is restricted to cases where essentially the same procedure can be applied to any number of data items. In complicated applications of statistics, there may be several ways in which the number of data items may grow. For example, records for rainfall within an area might increase in three ways: records for additional time periods; records for additional sites with a fixed area; records for extra sites obtained by extending the size of the area. In such cases, the property of consistency may be limited to one or more of the possible ways a sample size can grow\ndegrees of freedom - When discussed about variable-sample size tradeoff, usually means n-p, where is the number of rows and p is the number of variables. The more variables used in the model the fewer degrees of freedom and therefore less power and precision.\nexchangeability - means we can swap around, or reorder, variables in the sequence without changing their joint distribution.\n\nEvery IID (independent, identically distributed) sequence is exchangeable - but not the other way around. Every exchangeable sequence is identically distributed, though\n\nExample: If you draw a sequence of red and blue marbles from a bag without replacement, the sample is exchangeable but not independent. e.g.¬†drawing a red marble affects the probability of drawing a red or blue marble next.\n\n\nefficiency - A test, estimator, etc. is more efficient than another test, estimator, etc. if it requires fewer observation to obtain the same level of performance.\nergodicity - the idea that a point of a moving system, either a dynamical system or a stochastic process, will eventually visit all parts of the space that the system moves in, in a uniform and random sense\nexternal validity - Our estimates are externally valid if inferences and conclusions can be generalized from the population and setting studied to other populations and settings. (also see internal validity)\nidentifiable (aka point-indentifiable) - theoretically possible to learn the true values of this model‚Äôs underlying parameters after obtaining an infinite number of observations from it (see non-identifiability, partially-indentifiable)\nill-conditioned - In SVD decomposition, when there‚Äôs a huge difference between largest and smallest eigenvalue of¬†the original matrix, A, the ratio of which is called condition number.\ninternal validity - our estimates are internally valid if statistical inferences about causal effects are valid for the population being studied. (also see external validity)\nintractable - problems for which there exist no efficient algorithms to solve them. Most intractable problems have an algorithm ‚Äì the same algorithm ‚Äì that provides a solution, and that algorithm is the brute-force search\nlocality - effects have causes and chains of cause and effect must be unbroken in space and time (not the case in ‚Äòentanglement‚Äô)\nmarginalization - The process of eliminating one or more variables from a joint probability distribution or a multivariate statistical model to obtain the distribution or model for a subset of variables. The resulting distribution or model is called a marginal distribution or marginal model. It allows you to focus on the behavior of specific variables while considering the uncertainty associated with others.\n\nFor example, marginalizing over a joint distribution (i.e.¬†many variables) gets you a marginal distribution (i.e.¬†fewer variables). In other words, if you have a joint probability distribution for two variables \\(X\\) and \\(Y\\), the marginal distribution of \\(X\\) is obtained by summing or integrating over all possible values of \\(Y\\). Similarly, the marginal distribution of \\(Y\\) is obtained by summing or integrating over all possible values of \\(X\\).\nNotation: \\(P(X) = \\sum_Y P(X,Y) \\;\\text{or}\\; P(X) = \\int P(X,Y)\\;dY\\)\nOnce you have to the marginal distribution, this allows you compute conditional distributions. For example, after obtaining the marginal distribution, \\(P(X,Y)\\), from the joint distribution, \\(P(X,Y,Z)\\), you can compute the conditional distributions, \\(P(X|Y)\\) and \\(P(Y|X)\\).\nThe uncertainty associated with \\(Z\\) is indirectly considered in the sense that the marginal distribution \\(P(X,Y)\\) accounts for all possible values of \\(Z\\) by integrating over them. However, \\(P(X,Y)\\) itself doesn‚Äôt provide explicit information about the uncertainty associated with \\(Z\\).\n\nnon-identifiability - the structure of the data and model do not make it possible to estimate the parameter‚Äôs value. Multicollinearity is a type of non-identifiability problem. (i.e.¬†two or more parametrizations of the model are observationally equivalent) (see identifiable, partially-indentifiable)\noverdetermined system - In linear regression, when there are more observations than features, n &gt; p\npartial coefficient - The coefficient of a variable in a multivariable regression. In a simple regression, the coefficient of the variable is just called the ‚Äúregression coefficient.‚Äù\npartially-indentifiable (aka set identifiable) - non-identifiable but possible to learn the true values of a certain subset of the model parameters\nrobust - a ‚Äúrobust‚Äù estimator in statistics is one that is insensitive to outliers, whereas a ‚Äúrobust‚Äù estimator in econometrics is insensitive to heteroskedasticity and autocorrelation (hyndman)\nsupport (aka range) - the set of values that the random variable can take.\n\nFor discrete random variables, it is the set of all the realizations that have a strictly positive probability of being observed.\nFor continuous random variables, it is the set of all numbers whose probability density is strictly positive.\nSee link for examples\n\nunderspecification - In general, the solution to a problem is underspecified if there are many distinct solutions that solve the problem equivalently.\nAn unbiased estimator is an accurate statistic that‚Äôs used to approximate a population parameter.\n\n‚ÄúAccurate‚Äù in this sense means that it‚Äôs neither an overestimate nor an underestimate. If an overestimate or underestimate does happen, the mean of the difference is called a ‚Äúbias.‚Äù\n\nWeak Law of Large Numbers (Bernoulli‚Äôs theorem) - states that if you have a sample of independent and identically distributed random variables, as the sample size grows larger, the sample mean will tend toward the population mean",
    "crumbs": [
      "Mathematics",
      "Glossary"
    ]
  }
]