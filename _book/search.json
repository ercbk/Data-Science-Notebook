[
  {
    "objectID": "qmd/surveys-census-data.html",
    "href": "qmd/surveys-census-data.html",
    "title": "Census Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "title": "Census Data",
    "section": "",
    "text": "Notes from\n\nTidycensus Workshop 2024\n\nFIPS GEOID\n\npopular variable calculations from variables in ACS\nCensus Geocoder (link)\n\nEnter an address and codes for various geographies are returned\nBatch geocoding available for up to 10K records\n\nCodes for geographies returned in a .csv file\n\n\nTIGERweb (link)\n\nAllows you to get geography codes by searching for an area on a map\nOnce zoomed-in on your desired area, you turn on geography layers to find the geography code for your area.\n\nUS Census Regions",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "title": "Census Data",
    "section": "Geographies",
    "text": "Geographies\n\n\nMisc\n\n{tidycensus} docs on various geographies, function arguments, and which surveys (ACS, Census) they‚Äôre available in.\nACS Geography Boundaries by Year (link)\n\nTypes\n\nLegal/Administrative\n\nCensus gets boundaries from outside party (state, county, city, etc.)\ne.g.¬†election areas, school districts, counties, county subdivisions\n\nStatistical\n\nCensus creates these boundaries\ne.g.¬†regions, census tracts, ZCTAs, block groups, MSAs, urban areas\n\n\nNested Areas\n\n\nCensus Tracts\n\nAreas within a county\nAround 1200 to 8000 people\nSmall towns, rural areas, neighborhoods\n** Census tracts may cross city boundaries **\n\nBlock Groups\n\nAreas within a census tract\nAround 600 to 3000 people\n\nCensus Blocks\n\nAreas within a block group\nNot for ACS, only for the 10-yr census\n\n\nPlaces\n\nMisc\n\nOne place cannot overlap another place\nExpand and contract as population or commercial activity increases or decreases\nMust represent an organized settlement of people living in close proximity.\n\nIncorporated Places\n\ncities, towns, villages\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\nCensus Designated Places (CDPs)\n\nAreas that can‚Äôt become Incorporated Places because of state or city regulations\nConcentrations of population, housing, commericial structures\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\n\nCounty Subdivisions\n\nMinor Civil Divisions (MCDs)\n\nLegally defined by the state or county, stable entity. May have elected government\ne.g.¬†townships, charter townships, or districts\n\nCensus County Divisions (CCDs)\n\nno population requirment\nSubcounty units with stable boundaries and recognizable names\n\n\nZip Code Tabulation Areas (ZCTAs)\n\n\nMisc\n\n{crosswalkZCTA} - Contains the US Census Bureau‚Äôs 2020 ZCTA to County Relationship File, as well as convenience functions to translate between States, Counties and ZIP Code Tabulation Areas (ZCTAs)\n\nApproximate USPS Code distribution for housing units\n\nThe most frequently occurring zip code within an census block is assigned to a census block\nThen blocks are aggregated into areas (ZCTAs)\n\nZCTAs do NOT nest within any other geographies\n\nI guess the aggregated ZCTA blocks can overlap block groups\n\n2010 ZCTAs exclude large bodies of water and unpopulated areas",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "title": "Census Data",
    "section": "American Community Survey (ACS)",
    "text": "American Community Survey (ACS)\n\nAbout\n\nYearly estimates based on samples of the population over a 5yr period\n\nTherefore a Margin of Error (MoE) is included with the estimates.\n\nDetailed social, economic, housing, and demographic characteristics\ncensus.gov/acs\n\nACS Release Schedule (releases)\n\nSeptember - 1-Year Estimates (from previous year‚Äôs collection)\n\nEstimates for areas with populations of &gt;65K\n\nOctober - 1-Year Supplemental Estimates\n\nEstimates for areas with populations between 20K-64999\n\nDecember - 5-Year Estimates\n\nEstimates for areas including census tract and block groups\n\n\nData Collected\n\nPopulation\n\nSocial\n\nAncestry, Citizenship, Citizen Voting Age¬† Population, Disability, Education Attainment, Fertility, Grandparents, Language, Marital Status, Migration, School Enrollment, Veterans\n\nDemographic\n\nAge, Hispanic Origin, Race, Relationship, Sex\n\nEconomic\n\nClass of worker, Commuting, Employment Status, Food Stamps (SNAP), Health Insurance, Hours/Week, Weeks/Year, Income, Industry & Occupation\n\n\nHousing\n\nComputer & Internet Use, Costs (Mortgage, Taxes, Insurance), Heating Fuel, Home Value, Occupancy, Plumbing/Kitchen Facilities, Structure, Tenure (Own/Rent), Utilities, Vehicles, Year Built/Year Movied In",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "title": "Census Data",
    "section": "Dicennial US Census",
    "text": "Dicennial US Census\n\nMisc\n\nA complete count ‚Äî not based on samples like the ACS\nApplies differential privacy to preserve respondent confidentiality\n\nAdds noise to data. Greater effect at lower levels (i.e.¬†block level)\nThe exception is that is no differetial privacy for household-level data.\n\n\n\n\nPL94-171\n\nPopulation data which the government needs for redistricting\nsumfile = ‚Äúpl‚Äù\nState Populations\npop20 &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"P1_001N\",\n    year = 2020\n  )\n\nFor 2020, default is sumfile = ‚Äúpl‚Äù\n\n\n\n\nDHC\n\nAge, Sex, Race, Ethnicity, and Housing Tenure (most popular dataset)\nsumfile = ‚Äúdhc‚Äù\nCounty\ntx_population &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\nCensus Block (analogous to a city block)\nmatagorda_blocks &lt;- \n  get_decennial(\n    geography = \"block\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    county = \"Matagorda\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\n\n\n\nDemographic Profile\n\nPretabulated percentages from dhc\nsumfile = ‚Äúdp‚Äù\n\nTabulations for 118th Congress and Island Areas (i.e.¬†Congressional Districts)\n\nsumfile = ‚Äúcd118‚Äù\n\n\nC suffix variables are counts while P suffix variables are percentages\n\n0.4 is 0.4% not 40%\n\nExample: Same-sex married and partnered in California by County\nca_samesex &lt;- \n  get_decennial(\n    geography = \"county\",\n    state = \"CA\",\n    variables = c(married = \"DP1_0116P\",\n                  partnered = \"DP1_0118P\"),\n    year = 2020,\n    sumfile = \"dp\",\n    output = \"wide\"\n  )\n\n\n\nDetailed DHC-A\n\nDetailed demographic data; Thousands of racial and ethnic groups; Tabulation by sex and age.\nDifferent groups are in different tables, so specific groups can be hard to locate.\nAdaptive design means the demographic group (i.e.¬†variable) will only be available in certain areas. For privacy, data gets supressed when the area has low population.\n\nThere‚Äôs typically considerable sparsity especially when going down census tract\n\nArgs\n\nsumfile = ‚Äúddhca‚Äù\npop_group - Population group code (See get_pop_groups below)\n\n‚Äúall‚Äù for all groups\npop_group_label = TRUE - Adds group labels\n\n\nget_pop_groups(2020, \"ddhca\") - Gets group codes for ethnic groups\n\nFor various groups there could be at least two variables (e..g Somaili, Somali and any combination)\nFor time series analysis, analagous groups to 2020‚Äôs for 2000 is SF2/SF4 and for 2010 is SF2. (SF stands for Summary File)\n\ncheck_ddhca_groups - Checks which variables are available for a specific group\n\nExample: Somali\ncheck_ddhca_groups(\n  geography = \"county\", \n  pop_group = \"1325\", \n  state = \"MN\", \n  county = \"Hennepin\"\n)\n\nExample: Minnesota group populations\nload_variables(2020, \"ddhca\") %&gt;% \n  View()\nmn_population_groups &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"all\", # for all groups\n    pop_group_label = TRUE\n  )\n\nIncludes aggregate categories like European Alone, Other White Alone, etc., so you can‚Äôt just aggregate the value column to get the total population in Minnesota.\n\nSo, in order to calculate ethnic group ratios of the total state or county, etc. population, you need to get those state/county totals from other tables (e.g.¬†PL94-171)\n\n\nUse dot density and not chloropleths to visualize these sparse datasets\n\nExample: Somali populations by census tract in Minneapolis\n\nhennepin_somali &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    county = \"Hennepin\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"1325\", # somali\n    pop_group_label = TRUE,\n    geometry = TRUE\n  )\n\nsomali_dots &lt;- \n  as_dot_density(\n    hennepin_somali,\n    value = \"value\", # column name which is by default, \"value\"\n    values_per_dot = 25\n  )\n\nmapview(somali_dots, \n        cex = 0.01, \n        layer.name = \"Somali population&lt;br&gt;1 dot = 25 people\",\n        col.regions = \"navy\", \n        color = \"navy\")\n\nvalues_per_dot = 25 says make each dot worth 25 units (e.g.¬†people or housing units)\n\n\n\n\n\nTime Series Analysis\n\n{tidycensus} only has 2010 and 2020 censuses\n\nSee https://nhgis.org for older census data\n\nIssue: county names and boundaries change over time (e.g.¬†Alaska redraws a lot)\n\nCensus gives a different GeoID to counties that get renamed even though they‚Äôre the same county.\nNA values showing up after you calculate how the value changes over time is a good indication of this problem. Check for NAs: filter(county_change, is.na(value10))\n\nExample: Join 2010 and 2020 and Calculate Percent Change\ncounty_pop_10 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P001001\", \n    year = 2010,\n    sumfile = \"sf1\"\n  )\n\ncounty_pop_10_clean &lt;- \n  county_pop_10 %&gt;%\n    select(GEOID, value10 = value) \n\ncounty_pop_20 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    year = 2020,\n    sumfile = \"dhc\"\n  ) %&gt;%\n    select(GEOID, NAME, value20 = value)\n\ncounty_joined &lt;- \n  county_pop_20 %&gt;%\n    left_join(county_pop_10_clean, by = \"GEOID\") \n\ncounty_joined\n\ncounty_change &lt;- \n  county_joined %&gt;%\n    mutate( \n      total_change = value20 - value10, \n      percent_change = 100 * (total_change / value10) \n    ) \nExample: Age distribution over time in Michigan\n\n\nCode available in the github repo or R/Workshops/tidycensus-umich-workshop-2024-main/census-2020/bonus-chart.R\nDistribution shape remains pretty much the same, but decreasing for most age cohorts, i.e.¬†people are leaving the state across most age groups.\n\ne.g.¬†The large hump representing the group of people in there mid-40s in 2000 steadily decreases over time.",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "title": "Census Data",
    "section": "tidycensus",
    "text": "tidycensus\n\nGet an API key\n\nRequest a key, then activate the key from the link in your email.(https://api.census.gov/data/key_signup.html)\nSet as an environment variable: census_api_key(\"&lt;api key&gt;\", install = TRUE)\n\nOr add this line to .Renviron file, CENSUS_API_KEY=‚Äò&lt;api key‚Äô\n\n\nSearch Variables\n\nColumns\n\nName - ID of the variable (Use this in the survey functions)\nLabel - Detailed description of the variable\nContext - Subject of the table that the variable is located in.\n\nPrefixes (Variables can have combinations of prefixes)\n\nP: i.e.¬†Person; Data available at the census block and larger\nCT: Data available at the census track and larger\nH: Data available at the Housing Unit level\n\nI think housing unit is an alternatve unit. So instead of the unit being a person, which I assume is the typical unit, it‚Äôs a housing unit (~family).\nNot affected by Differential Privacy (i.e.¬†no noise added; true value)\nExample: Total Deleware housing units at census block level\ndp_households &lt;- \n      get_decennial(\n            geography = \"block\",\n            variables = \"H1_001N\",\n            state = \"DE\",\n            sumfile = \"dhc\",\n            year = 2020\n      )\n\n\nExample: DHC data in census for 2020\n\nvars &lt;- load_variables(2020, \"dhc\")\n\nView(vars)\n\nView table, click filter, and then search for parameters (e.g.¬†Age, Median, etc.) with the Label, Context boxes, and overall search box\n\n\nsummary_var - Argument for supplying an additional variable that you need to calculate some kind of summary statistic\n\nExample: Race Percentage per Congressional District\n\nrace_vars &lt;- c(\n  Hispanic = \"P5_010N\", # all races identified as hispanic\n  White = \"P5_003N\", # white not hispanic\n  Black = \"P5_004N\", # black not hispanic\n  Native = \"P5_005N\", # native american not hispanic\n  Asian = \"P5_006N\", # asian not hispanic\n  HIPI = \"P5_007N\" # hawaiian, islander not hispanic\n)\n\ncd_race &lt;- \n  get_decennial(\n    geography = \"congressional district\",\n    variables = race_vars,\n    summary_var = \"P5_001N\", # total population for county\n    year = 2020,\n    sumfile = \"cd118\"\n)\n\ncd_race_percent &lt;- \n  cd_race %&gt;%\n    mutate(percent = 100 * (value / summary_value)) %&gt;% \n    select(NAME, variable, percent)\n\ngeometry = TRUE- Joins shapefile with data and returns a SF (Simple Features) dataframe for mapping\n\nMisc\n\nYou can create a discrete color palette with the at argument in the mapview function.\n\nExample\n# check min and max of your data to select range of bins\nmin(iowa_over_65, na.rm = TRUE) # 0\nmax(iowa_over_65, na.rm = TRUE) # 38.4\n\nm1 &lt;- \n  mapview(iowa_over_65, \n          zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1),\n          at = c(0, 10, 20, 30, 40))\n\nThis will result in a discrete palette with bins of 0-10, 10-20, etc. Looks like an overlap, so I‚Äôm sure which bin contains the endpoints.\n\n\n\nExample: Over 65 in Iowa by census tract\n\nlibrary(mapviw); library(viridisLite)\n\niowa_over_65 &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"DP1_0024P\",\n    state = \"IA\",\n    geometry = TRUE,\n    sumfile = \"dp\",\n    year = 2020\n  )\nm1 &lt;- \n  mapview(iowa_over_65, zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1))\nExport as an HTML file\nhtmlwidgets::saveWidget(m1@map, \"iowa_over_65.html\")\n\nCan embed it elsewhere (html report or website) by adding it as an asset",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html",
    "href": "qmd/model-building-concepts.html",
    "title": "Concepts",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html#sec-modbld-misc",
    "href": "qmd/model-building-concepts.html#sec-modbld-misc",
    "title": "Concepts",
    "section": "",
    "text": "Packages\n\n{multiverse} - makes it easy to specify and execute all combinations of reasonable analyses of a dataset\n\n\n\nPaper, Summary of it‚Äôs usage\nLots of vignettes\n\n\nRegression Workflow (Paper)\n\nMake ML model pipelines reusable and reproducible\n\n\nNotes from 7 Tips to Future-Proof Machine Learning Projects\nModularization - Useful for debugging and iteration\n\nDon‚Äôt used declarative programming. Create functions/classes for preprocessing, training, tuning, etc., and keep in separate files. You‚Äôll call these functions in the main script\n\nHelper function\n## file preprocessing.py ##\ndef data_preparation(data):\n    data = data.drop(['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am'], axis=1)\n    numeric_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']\n    data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)\n    return data\nMain script\nfrom preprocessing import data_preparation \ntrain_preprocessed = data_preparation(train_data)\ninference_preprocessed = data_preparation(inference_data)\n\nKeep parameters in a separate config file\n\nConfig file\n## parameters.py ##\nDROP_COLS = ['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am']\nNUM_COLS = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']\nProprocessing script\n## preprocessing.py ##\nfrom parameters import DROP_COLS, NUM_COLS\ndef data_preparation(data):\n    data = data.drop(DROP_COLS, axis=1)\n    data[NUM_COLS] = data[NUM_COLS].fillna(data[NUM_COLS].mean())\n    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)\n    return data\n\n\nVersioning Code, Data, and Models - Useful for investigating drift\n\nSee tools like DVC, MLFlow, Weights and Biases, etc. for model and data versioning\n\nImportant to save data snapshots throughout the project lifecycle, for example: raw data, processed data, train data, validation data, test data and inference data.\n\nGithub and dbt for code versioning\n\nConsistent Structures - Consistency in project structures and naming can reduce human error, improve communication, and just make things easier to find.\n\nNaming examples:\n\n&lt;model-name&gt;-&lt;parameters&gt;-&lt;model-version&gt;\n&lt;model-name&gt;-&lt;data-version&gt;-&lt;use-case&gt;\n\nExample: Reduced project template based on {{cookiecutter}}\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ output      &lt;- The output data from the model. \n‚îÇ   ‚îú‚îÄ‚îÄ processed      &lt;- The final, canonical data sets for modeling.\n‚îÇ   ‚îî‚îÄ‚îÄ raw            &lt;- The original, immutable data dump.\n‚îÇ\n‚îú‚îÄ‚îÄ models             &lt;- Trained and serialized models, model predictions, or model summaries\n‚îÇ\n‚îú‚îÄ‚îÄ notebooks          &lt;- Jupyter notebooks. \n‚îÇ\n‚îú‚îÄ‚îÄ reports            &lt;- Generated analysis as HTML, PDF, LaTeX, etc.\n‚îÇ   ‚îî‚îÄ‚îÄ figures        &lt;- Generated graphics and figures to be used in reporting\n‚îÇ\n‚îú‚îÄ‚îÄ requirements.txt   &lt;- The requirements file for reproducing the analysis environment, e.g.\n‚îÇ                         generated with `pip freeze &gt; requirements.txt`\n‚îÇ\n‚îú‚îÄ‚îÄ code              &lt;- Source code for use in this project.\n    ‚îú‚îÄ‚îÄ __init__.py    &lt;- Makes src a Python module\n    ‚îÇ\n    ‚îú‚îÄ‚îÄ data           &lt;- Scripts to generate and process data\n    ‚îÇ   ‚îú‚îÄ‚îÄ data_preparation.py\n    ‚îÇ   ‚îî‚îÄ‚îÄ data_preprocessing.py\n    ‚îÇ\n    ‚îú‚îÄ‚îÄ models         &lt;- Scripts to train models and then use trained models to make\n    ‚îÇ   ‚îÇ                 predictions\n    ‚îÇ   ‚îú‚îÄ‚îÄ inference_model.py\n    ‚îÇ   ‚îî‚îÄ‚îÄ train_model.py\n    ‚îÇ\n    ‚îî‚îÄ‚îÄ analysis  &lt;- Scripts to create exploratory and results oriented visualizations\n        ‚îî‚îÄ‚îÄ analysis.py\n\n\nModel is performing well on the training set but much worse on the validation/test set\n\n\nAndrew Ng calls the validation set the ‚ÄúDev Set‚Äù üôÑ\nTest: Random sample the training set and use that as your validation set. Score your model on this new validation set\n\n‚ÄúTrain-Dev‚Äù is the sampled validation set\nPossibilities\n\nVariance: The data distribution of the training set is the same as the validation/test sets\n\n\nThe model has been overfit to the training data\n\nData Mismatch: The data distribution of the training set is NOT the same as the validation/test sets\n\n\nUnlucky and the split was bad\n\nSomething maybe is wrong with the splitting function\n\nSplit ratio needs adjusting. Validation set isn‚Äôt getting enough data to be representative.\n\n\n\n\nModel is performing well on the validation/test set but not in the real world\n\nInvestigate the validation/test set and figure out why it‚Äôs not reflecting real world data. Then, apply corrections to the dataset.\n\ne.g.¬†distributions of your validation/tests sets should look like the real world data.\n\nChange the metric\n\nConsider weighting cases that your model is performing extremely poorly on.\n\n\nSplits\n\nHarrell: ‚Äúnot appropriate to split data into training and test sets unless n&gt;20,000 because of the luck (or bad luck) of the split.‚Äù\nIf your dataset is over 1M rows, then having a test set of 200K might be overkill (e.g.¬†ratio of 60/20/20).\n\nMight be better to use a ratio of 98/1/1 for big data projects and 60/20/20 for smaller data projects\n\nlink\n\nShows that simple data splitting does not give valid confidence intervals (even asymptotically) when one refits the model on the whole dataset. Thus, if one wants valid confidence intervals for prediction error, we can only recommend either data splitting without refitting the model (which is viable when one has ample data), or nested CV.",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/cli.html",
    "href": "qmd/cli.html",
    "title": "CLI",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-misc",
    "href": "qmd/cli.html#sec-cli-misc",
    "title": "CLI",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly‚Äôs suggestions are prioritized in real time with a small neural network\n\nPath to a folder that‚Äôs above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder\n\nDebian vs.¬†Ubuntu (from ChatGPT)\n\nStability vs.¬†Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it‚Äôs known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu‚Äôs.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as ‚ÄúDebian spins,‚Äù catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-r",
    "href": "qmd/cli.html#sec-cli-r",
    "title": "CLI",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n¬† ¬† ¬† janitor::clean_names() %&gt;%\n¬† ¬† ¬† select(date, geo, county = name, negative, positive) %&gt;%\n¬† ¬† ¬† filter(geo == \"County\") %&gt;%\n¬† ¬† ¬† mutate(date = lubridate::as_date(date)) %&gt;%\n¬† ¬† ¬† select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it‚Äôll search automatically\n35548",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-awk",
    "href": "qmd/cli.html#sec-cli-awk",
    "title": "CLI",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‚Äò$13 &gt; 98‚Äô adult_t.csv|head\nFilter lines with ‚ÄúDoctorate‚Äù and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-bash",
    "href": "qmd/cli.html#sec-cli-bash",
    "title": "CLI",
    "section": "Bash",
    "text": "Bash\n\nMisc\n\nNotes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn‚Äôt take notes on\n\nBash Scripting on Linux: The Complete Guide - video course\n\nResources\n\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n‚Äú&gt;‚Äù redirects the output from a program to a file.\n\n‚Äú&gt;&gt;‚Äù does the same thing, but it‚Äôs appending to an existing file instead of overwriting it, if it already exists.\n\n\n\n\n\nCommands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you‚Äôre using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you‚Äôre about to move does already exist under the new directory,\n\nThis way you don‚Äôt accidentally overwrite files that you didn‚Äôt mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\n\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\nOutput to file: echo ‚ÄúThis is an example for redirect‚Äù &gt; file1.txt\nAppend line to file: echo ‚ÄúThis is the second line of the file‚Äù &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to ‚Äútmp‚Äù directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name ‚Äúmy_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n‚Äúrwx‚Äù stand for read, write, and execute rights, respectively\nThe 3 ‚Äúrwx‚Äù blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a ‚Äúd‚Äù for directory or ‚Äúl‚Äù for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - ‚Äúsuper user‚Äù - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‚Äòerror‚Äô, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo ‚ÄúNo such directory exist.Check‚Äù\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn‚Äôt exist, then the message ‚ÄúNo such directory exists. check‚Äù message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for ‚Äúerror‚Äù and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for ‚Äúerror‚Äù in each line. Lines with ‚Äúerror‚Äù get written to ‚Äúerror_file.txt‚Äù\n\nFilter lines\ngrep -i ‚ÄúDoctorate‚Äù adult_t.csv |grep -i ‚ÄúHusband‚Äù|grep -i ‚ÄúBlack‚Äù|csvlook\n# -i, --ignore-case-Ignore¬† case¬† distinctions,¬† so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i ‚ÄúDoctorate‚Äù adult_t.csv | wc -l\n\nCounts how many rows have ‚ÄúDoctorate‚Äù\n\n-wc is ‚Äúword count‚Äù\n\n\n\n\n\n\nVariables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via ‚Äú$‚Äù\n# local\nev_car=‚ÄôTesla‚Äô\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=‚ÄôTesla‚Äô\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it‚Äôs okay not to is on the left-hand-side of an [[ ]] condition. But even there I‚Äôd recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or ‚Äìhelp or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this\n\n\n\n\n\nScripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like ‚Äìsilent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script‚Äôs directory close to the start of the script.\n\nAnd it‚Äôs usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck. Heed its warnings.\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don‚Äôt give executable permission to the script file.\nStarts with ‚Äú#!‚Äù the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory ‚Äú/bin‚Äù\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you‚Äôre good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n¬† ¬† #!/bin/bash\n¬† ¬† echo ‚ÄòHello World‚Äô\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n¬† ¬† set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n¬† ¬† echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n¬† ¬† exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n¬† ¬† echo do awesome stuff\n}\nmain \"$@\"\n\n\n\nJob Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.\n\n\n\n\ntmux (‚Äòterminal multiplexer‚Äô)\n\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‚Äòmoose‚Äô\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named ‚Äúmoose‚Äù\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you‚Äôve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‚Äòmoose‚Äô\n\n\nPane Creation and Navigation\n\ncontrol+b then press ‚Äù (i.e.¬†shift+‚Äô): add another terminal pane below\ncontrol+b then press % (i.e.¬†shift+5) : add another terminal pane to the right\ncontrol+b then press ‚Üí : move to the terminal pane on the right (similar for left, up, down)\n\n\n\n\nSSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n‚Äússh-keygen‚Äù is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named ‚Äúid_rsa.pub‚Äù and a private key named ‚Äúid_rsa‚Äù, and place both into your ‚Äú~/.ssh‚Äù directory\nYou‚Äôll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: ‚Äú~/.ssh/config‚Äù\nContents\nHost dev\n¬† HostName remote\n¬† IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist\n\n\n\n\nVim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you‚Äôll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you‚Äôre in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file\n\n\n\n\nPackages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to ‚Äúupgrading‚Äù the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and ‚Äúupdates‚Äù them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade\n\n\n\n\nExpressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-powsh",
    "href": "qmd/cli.html#sec-cli-powsh",
    "title": "CLI",
    "section": "Powershell",
    "text": "Powershell\n\nMisc\n\nDocs, Sample Scripts\nUse Get-Help &lt;cmdlet-name&gt;¬†to access help information for specific cmdlets.\n\n$_ (Docs)\n\nAutomatic Variable; Alias for $PSItem\n\nDocs for automatic variables\n\nUseful for looping through objects in a directory. See ‚ÄúChange Name (or Extensions) of Multiple Files‚Äù for an example of usage\nGet the properties of an object\n\nExample: Using Get-ItemProperty (Docs)\nGet-ItemProperty scrapsheet.txt | Format-List\n\nProperty names will be on the left side of the output\n\nExample: Using Get-Member (Docs)\n Get-ChildItem *.txt | Select-Object -First 1 | Get-Member\n\nSelects the first text file in the directory and gets the Properties and a bunch of other stuff like methods which also can be used with $_\nDescriptions are awful for a lot of the stuff, but for the most part, you can guess what the property is.\n\n\nSome properties that can be used with $_\n\nFile System Objects:\n\n.Name: Returns the name component of a file\n.FullName: Returns the full path of the file.\n.Length: Returns the size of the file in bytes.\n.CreationTime: Returns the date and time the file was created.\n.LastWriteTime: Returns the date and time the file was last modified.\n.IsReadOnly: Returns $true if the file is read-only, $false otherwise.\n\nProcess Objects:\n\n.Id: Returns the unique identifier (PID) of the process.\n.Name: Returns the name of the process.\n.MainWindowTitle: Returns the title of the process‚Äôs main window (if applicable).\n.WorkingDirectory: Returns the working directory of the process.\n.CPU: Returns the CPU usage of the process.\n\nRegistry Objects:\n\n.Name: Returns the name of the registry key.\n.Hive: Returns the hive of the registry key (e.g., HKLM, HKCU).\n.Value: Returns the value of the registry key (if applicable).\n.Path: Returns the full path of the registry key.\n\n\n\nComments: &lt;# comment #&gt;\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you‚Äôre already in the directory that you want the folder in. You can also use a path, e.g.¬†\"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nChange Name (or Extensions) of Multiple Files\nGet-ChildItem *.md | Rename-Item -NewName { $_.Name -replace '.md','.qmd' }\n\nGet-ChildItem: cmdlet gets all the files in the current folder that have a .mc file extension\nResults piped to Rename-Item\n\nNewName: Has a value that‚Äôs a script block that runs before the value is submitted to the NewName parameter.\n\n$_: (automatic variable) It represents each file object as it comes to the command through the pipeline.\n-replace: Operator replaces the file extension of each file with .qmd.\n\nNotice that matching using the -replace operator is not case sensitive.\n\n\n\n\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it‚Äôs a backslash (\\), but in Powershell, it‚Äôs a backtick ( ` )\n*Don‚Äôt forget that there‚Äôs a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it‚Äôs open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-batscri",
    "href": "qmd/cli.html#sec-cli-batscri",
    "title": "CLI",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g.¬†timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-wsl",
    "href": "qmd/cli.html#sec-cli-wsl",
    "title": "CLI",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for ‚Äìdistribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/json.html",
    "href": "qmd/json.html",
    "title": "JSON",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-misc",
    "href": "qmd/json.html#sec-json-misc",
    "title": "JSON",
    "section": "",
    "text": "Packages\n\n{yyjsonr} - A fast JSON parser/serializer, which converts R data to/from JSON and NDJSON. It is around 2x to 10x faster than jsonlite at both reading and writing JSON.\n{RcppSimdJson} - Comparable to {yyjsonr} in performance.\n\nAlso see\n\nBig Data &gt;&gt; Larger than Memory\nSQL &gt;&gt; Processing Expressions &gt;&gt; Nested Data\nDatabases &gt;&gt; DuckDB &gt;&gt; Misc\n\nhrbmstr recommends trying duckdb before using the cli tools in ‚ÄúBig Data‚Äù\n\n\nTools\n\n{listviewer}: Allows you to interactively explore and edit json files through the Viewer in the IDE. Docs show how it can be embedded into a Shiny app as well.\n\nExample\nlibrary(listviewer)\nmoose &lt;- jsonlite::read_json(\"path/to/file.json\")\njsonedit(moose)\nreactjson(moose)\n\nI‚Äôve also used this a .config file which looked like a json file when I opened in a text editor, so this seems to work on anything json-like.\nreactjson has a copy button which is nice so that you can paste your edited version into a file.\njsonedit seems like it has more features, but I didn‚Äôt see a copy button. But there‚Äôs a view in which you can manually select everything a copy it via keyboard shortcut.",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-jsonlite",
    "href": "qmd/json.html#sec-json-jsonlite",
    "title": "JSON",
    "section": "{jsonlite}",
    "text": "{jsonlite}\n\nRead",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-py",
    "href": "qmd/json.html#sec-json-py",
    "title": "JSON",
    "section": "Python",
    "text": "Python\n\nExample: Parse Nested JSON into a dataframe (article)\n\nRaw JSON\n\n\n‚Äúentry‚Äù has the data we want\n‚Äú‚Ä¶‚Äù at the end indicates there are multiple objectss inside the element, ‚Äúentry‚Äù\n\nProbably other root elements other than ‚Äúfeed‚Äù as well\n\n\nRead a json file from a URL using {{requests}} and convert to list\n\nimport requests\n\nurl = \"https://itunes.apple.com/gb/rss/customerreviews/id=1500780518/sortBy=mostRecent/json\"\n\nr = requests.get(url)\n\ndata = r.json()\nentries = data[\"feed\"][\"entry\"]\n\nIt looks like the list conversion also ordered the elements alphabetically\nThe output list is subsetted by the root element ‚Äúfeed‚Äù and the child element ‚Äúentry‚Äù\n\nGet a feel for the final structure you want by hardcoding elements into a df\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    parsed_data[\"author_uri\"].append(entry[\"author\"][\"uri\"][\"label\"])\n    parsed_data[\"author_name\"].append(entry[\"author\"][\"name\"][\"label\"])\n    parsed_data[\"author_label\"].append(entry[\"author\"][\"label\"])\n    parsed_data[\"content_label\"].append(entry[\"content\"][\"label\"])\n    parsed_data[\"content_attributes_type\"].append(entry[\"content\"][\"attributes\"][\"type\"])\n    ... \nGeneralize extracting the properties of each object in ‚Äúentry‚Äù with a nested loop\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    for key, val in entry.items():\n        for subkey, subval in val.items():\n            if not isinstance(subval, dict):\n                parsed_data[f\"{key}_{subkey}\"].append(subval)\n            else:\n                for att_key, att_val in subval.items():\n                    parsed_data[f\"{key}_{subkey}_{att_key}\"].append(att_val)\n\ndefaultdict creates a key from a list element (e.g.¬†‚Äúauthor‚Äù) and groups the properties into a list of values where the value may also be a dict.\n\nSee Python, General &gt;&gt; Types &gt;&gt; Dictionaries\n\nFor each item in ‚Äúentry‚Äù, it looks at the first key-value pair knowing that value is always a dictionary (object in JSON)\nThen handles two different cases\n\nFirst Case: The value dictionary is flat and does not contain another dictionary, only key-value pairs.\n\nCombine the outer key with the inner key to a column name and take the value as column value for each pair.\n\nSecond Case: Dictionary contains a key-value pair where the value is again a dictionary.\n\nAssumes at most two levels of nested dictionaries\nIterates over the key-value pairs of the inner dictionary and again combines the outer key and the most inner key to a column name and take the inner value as column value.\n\n\n\nRecursive function that handles json elements with deeper structures\n\ndef recursive_parser(entry: dict, data_dict: dict, col_name: str = \"\") -&gt; dict:\n    \"\"\"Recursive parser for a list of nested JSON objects\n\n    Args:\n        entry (dict): A dictionary representing a single entry (row) of the final data frame.\n        data_dict (dict): Accumulator holding the current parsed data.\n        col_name (str): Accumulator holding the current column name. Defaults to empty string.\n    \"\"\"\n    for key, val in entry.items():\n        extended_col_name = f\"{col_name}_{key}\" if col_name else key\n        if isinstance(val, dict):\n            recursive_parser(entry[key], data_dict, extended_col_name)\n        else:\n            data_dict[extended_col_name].append(val)\n\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    recursive_parser(entry, parsed_data, \"\")\n\ndf = pd.DataFrame(parsed_data)\n\nNotice the check for a deeper structure with isinstance. If there is one, then the function is called again.\nFunction outputs a dict which is coerced into dataframe\nTo get rid of ‚Äúlabel‚Äù in column names: df.columns = [col if not \"label\" in col else \"_\".join(col.split(\"_\")[:-1]) for col in df.columns]\nobject types can be cast into more efficient types: df[\"im:rating\"] = df[\"im:rating\"].astype(int)",
    "crumbs": [
      "JSON"
    ]
  }
]