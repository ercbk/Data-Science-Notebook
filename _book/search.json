[
  {
    "objectID": "qmd/surveys-census-data.html",
    "href": "qmd/surveys-census-data.html",
    "title": "Census Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "title": "Census Data",
    "section": "",
    "text": "Notes from\n\nTidycensus Workshop 2024\n\nFIPS GEOID\n\npopular variable calculations from variables in ACS\nCensus Geocoder (link)\n\nEnter an address and codes for various geographies are returned\nBatch geocoding available for up to 10K records\n\nCodes for geographies returned in a .csv file\n\n\nTIGERweb (link)\n\nAllows you to get geography codes by searching for an area on a map\nOnce zoomed-in on your desired area, you turn on geography layers to find the geography code for your area.\n\nUS Census Regions",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "title": "Census Data",
    "section": "Geographies",
    "text": "Geographies\n\n\nMisc\n\n{tidycensus} docs on various geographies, function arguments, and which surveys (ACS, Census) they’re available in.\nACS Geography Boundaries by Year (link)\n\nTypes\n\nLegal/Administrative\n\nCensus gets boundaries from outside party (state, county, city, etc.)\ne.g. election areas, school districts, counties, county subdivisions\n\nStatistical\n\nCensus creates these boundaries\ne.g. regions, census tracts, ZCTAs, block groups, MSAs, urban areas\n\n\nNested Areas\n\n\nCensus Tracts\n\nAreas within a county\nAround 1200 to 8000 people\nSmall towns, rural areas, neighborhoods\n** Census tracts may cross city boundaries **\n\nBlock Groups\n\nAreas within a census tract\nAround 600 to 3000 people\n\nCensus Blocks\n\nAreas within a block group\nNot for ACS, only for the 10-yr census\n\n\nPlaces\n\nMisc\n\nOne place cannot overlap another place\nExpand and contract as population or commercial activity increases or decreases\nMust represent an organized settlement of people living in close proximity.\n\nIncorporated Places\n\ncities, towns, villages\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\nCensus Designated Places (CDPs)\n\nAreas that can’t become Incorporated Places because of state or city regulations\nConcentrations of population, housing, commericial structures\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\n\nCounty Subdivisions\n\nMinor Civil Divisions (MCDs)\n\nLegally defined by the state or county, stable entity. May have elected government\ne.g. townships, charter townships, or districts\n\nCensus County Divisions (CCDs)\n\nno population requirment\nSubcounty units with stable boundaries and recognizable names\n\n\nZip Code Tabulation Areas (ZCTAs)\n\n\nMisc\n\n{crosswalkZCTA} - Contains the US Census Bureau’s 2020 ZCTA to County Relationship File, as well as convenience functions to translate between States, Counties and ZIP Code Tabulation Areas (ZCTAs)\n\nApproximate USPS Code distribution for housing units\n\nThe most frequently occurring zip code within an census block is assigned to a census block\nThen blocks are aggregated into areas (ZCTAs)\n\nZCTAs do NOT nest within any other geographies\n\nI guess the aggregated ZCTA blocks can overlap block groups\n\n2010 ZCTAs exclude large bodies of water and unpopulated areas",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "title": "Census Data",
    "section": "American Community Survey (ACS)",
    "text": "American Community Survey (ACS)\n\nAbout\n\nYearly estimates based on samples of the population over a 5yr period\n\nTherefore a Margin of Error (MoE) is included with the estimates.\n\nDetailed social, economic, housing, and demographic characteristics\ncensus.gov/acs\n\nACS Release Schedule (releases)\n\nSeptember - 1-Year Estimates (from previous year’s collection)\n\nEstimates for areas with populations of &gt;65K\n\nOctober - 1-Year Supplemental Estimates\n\nEstimates for areas with populations between 20K-64999\n\nDecember - 5-Year Estimates\n\nEstimates for areas including census tract and block groups\n\n\nData Collected\n\nPopulation\n\nSocial\n\nAncestry, Citizenship, Citizen Voting Age  Population, Disability, Education Attainment, Fertility, Grandparents, Language, Marital Status, Migration, School Enrollment, Veterans\n\nDemographic\n\nAge, Hispanic Origin, Race, Relationship, Sex\n\nEconomic\n\nClass of worker, Commuting, Employment Status, Food Stamps (SNAP), Health Insurance, Hours/Week, Weeks/Year, Income, Industry & Occupation\n\n\nHousing\n\nComputer & Internet Use, Costs (Mortgage, Taxes, Insurance), Heating Fuel, Home Value, Occupancy, Plumbing/Kitchen Facilities, Structure, Tenure (Own/Rent), Utilities, Vehicles, Year Built/Year Movied In",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "title": "Census Data",
    "section": "Dicennial US Census",
    "text": "Dicennial US Census\n\nMisc\n\nA complete count — not based on samples like the ACS\nApplies differential privacy to preserve respondent confidentiality\n\nAdds noise to data. Greater effect at lower levels (i.e. block level)\nThe exception is that is no differetial privacy for household-level data.\n\n\n\n\nPL94-171\n\nPopulation data which the government needs for redistricting\nsumfile = “pl”\nState Populations\npop20 &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"P1_001N\",\n    year = 2020\n  )\n\nFor 2020, default is sumfile = “pl”\n\n\n\n\nDHC\n\nAge, Sex, Race, Ethnicity, and Housing Tenure (most popular dataset)\nsumfile = “dhc”\nCounty\ntx_population &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\nCensus Block (analogous to a city block)\nmatagorda_blocks &lt;- \n  get_decennial(\n    geography = \"block\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    county = \"Matagorda\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\n\n\n\nDemographic Profile\n\nPretabulated percentages from dhc\nsumfile = “dp”\n\nTabulations for 118th Congress and Island Areas (i.e. Congressional Districts)\n\nsumfile = “cd118”\n\n\nC suffix variables are counts while P suffix variables are percentages\n\n0.4 is 0.4% not 40%\n\nExample: Same-sex married and partnered in California by County\nca_samesex &lt;- \n  get_decennial(\n    geography = \"county\",\n    state = \"CA\",\n    variables = c(married = \"DP1_0116P\",\n                  partnered = \"DP1_0118P\"),\n    year = 2020,\n    sumfile = \"dp\",\n    output = \"wide\"\n  )\n\n\n\nDetailed DHC-A\n\nDetailed demographic data; Thousands of racial and ethnic groups; Tabulation by sex and age.\nDifferent groups are in different tables, so specific groups can be hard to locate.\nAdaptive design means the demographic group (i.e. variable) will only be available in certain areas. For privacy, data gets supressed when the area has low population.\n\nThere’s typically considerable sparsity especially when going down census tract\n\nArgs\n\nsumfile = “ddhca”\npop_group - Population group code (See get_pop_groups below)\n\n“all” for all groups\npop_group_label = TRUE - Adds group labels\n\n\nget_pop_groups(2020, \"ddhca\") - Gets group codes for ethnic groups\n\nFor various groups there could be at least two variables (e..g Somaili, Somali and any combination)\nFor time series analysis, analagous groups to 2020’s for 2000 is SF2/SF4 and for 2010 is SF2. (SF stands for Summary File)\n\ncheck_ddhca_groups - Checks which variables are available for a specific group\n\nExample: Somali\ncheck_ddhca_groups(\n  geography = \"county\", \n  pop_group = \"1325\", \n  state = \"MN\", \n  county = \"Hennepin\"\n)\n\nExample: Minnesota group populations\nload_variables(2020, \"ddhca\") %&gt;% \n  View()\nmn_population_groups &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"all\", # for all groups\n    pop_group_label = TRUE\n  )\n\nIncludes aggregate categories like European Alone, Other White Alone, etc., so you can’t just aggregate the value column to get the total population in Minnesota.\n\nSo, in order to calculate ethnic group ratios of the total state or county, etc. population, you need to get those state/county totals from other tables (e.g. PL94-171)\n\n\nUse dot density and not chloropleths to visualize these sparse datasets\n\nExample: Somali populations by census tract in Minneapolis\n\nhennepin_somali &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    county = \"Hennepin\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"1325\", # somali\n    pop_group_label = TRUE,\n    geometry = TRUE\n  )\n\nsomali_dots &lt;- \n  as_dot_density(\n    hennepin_somali,\n    value = \"value\", # column name which is by default, \"value\"\n    values_per_dot = 25\n  )\n\nmapview(somali_dots, \n        cex = 0.01, \n        layer.name = \"Somali population&lt;br&gt;1 dot = 25 people\",\n        col.regions = \"navy\", \n        color = \"navy\")\n\nvalues_per_dot = 25 says make each dot worth 25 units (e.g. people or housing units)\n\n\n\n\n\nTime Series Analysis\n\n{tidycensus} only has 2010 and 2020 censuses\n\nSee https://nhgis.org for older census data\n\nIssue: county names and boundaries change over time (e.g. Alaska redraws a lot)\n\nCensus gives a different GeoID to counties that get renamed even though they’re the same county.\nNA values showing up after you calculate how the value changes over time is a good indication of this problem. Check for NAs: filter(county_change, is.na(value10))\n\nExample: Join 2010 and 2020 and Calculate Percent Change\ncounty_pop_10 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P001001\", \n    year = 2010,\n    sumfile = \"sf1\"\n  )\n\ncounty_pop_10_clean &lt;- \n  county_pop_10 %&gt;%\n    select(GEOID, value10 = value) \n\ncounty_pop_20 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    year = 2020,\n    sumfile = \"dhc\"\n  ) %&gt;%\n    select(GEOID, NAME, value20 = value)\n\ncounty_joined &lt;- \n  county_pop_20 %&gt;%\n    left_join(county_pop_10_clean, by = \"GEOID\") \n\ncounty_joined\n\ncounty_change &lt;- \n  county_joined %&gt;%\n    mutate( \n      total_change = value20 - value10, \n      percent_change = 100 * (total_change / value10) \n    ) \nExample: Age distribution over time in Michigan\n\n\nCode available in the github repo or R/Workshops/tidycensus-umich-workshop-2024-main/census-2020/bonus-chart.R\nDistribution shape remains pretty much the same, but decreasing for most age cohorts, i.e. people are leaving the state across most age groups.\n\ne.g. The large hump representing the group of people in there mid-40s in 2000 steadily decreases over time.",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "title": "Census Data",
    "section": "tidycensus",
    "text": "tidycensus\n\nGet an API key\n\nRequest a key, then activate the key from the link in your email.(https://api.census.gov/data/key_signup.html)\nSet as an environment variable: census_api_key(\"&lt;api key&gt;\", install = TRUE)\n\nOr add this line to .Renviron file, CENSUS_API_KEY=‘&lt;api key’\n\n\nSearch Variables\n\nColumns\n\nName - ID of the variable (Use this in the survey functions)\nLabel - Detailed description of the variable\nContext - Subject of the table that the variable is located in.\n\nPrefixes (Variables can have combinations of prefixes)\n\nP: i.e. Person; Data available at the census block and larger\nCT: Data available at the census track and larger\nH: Data available at the Housing Unit level\n\nI think housing unit is an alternatve unit. So instead of the unit being a person, which I assume is the typical unit, it’s a housing unit (~family).\nNot affected by Differential Privacy (i.e. no noise added; true value)\nExample: Total Deleware housing units at census block level\ndp_households &lt;- \n      get_decennial(\n            geography = \"block\",\n            variables = \"H1_001N\",\n            state = \"DE\",\n            sumfile = \"dhc\",\n            year = 2020\n      )\n\n\nExample: DHC data in census for 2020\n\nvars &lt;- load_variables(2020, \"dhc\")\n\nView(vars)\n\nView table, click filter, and then search for parameters (e.g. Age, Median, etc.) with the Label, Context boxes, and overall search box\n\n\nsummary_var - Argument for supplying an additional variable that you need to calculate some kind of summary statistic\n\nExample: Race Percentage per Congressional District\n\nrace_vars &lt;- c(\n  Hispanic = \"P5_010N\", # all races identified as hispanic\n  White = \"P5_003N\", # white not hispanic\n  Black = \"P5_004N\", # black not hispanic\n  Native = \"P5_005N\", # native american not hispanic\n  Asian = \"P5_006N\", # asian not hispanic\n  HIPI = \"P5_007N\" # hawaiian, islander not hispanic\n)\n\ncd_race &lt;- \n  get_decennial(\n    geography = \"congressional district\",\n    variables = race_vars,\n    summary_var = \"P5_001N\", # total population for county\n    year = 2020,\n    sumfile = \"cd118\"\n)\n\ncd_race_percent &lt;- \n  cd_race %&gt;%\n    mutate(percent = 100 * (value / summary_value)) %&gt;% \n    select(NAME, variable, percent)\n\ngeometry = TRUE- Joins shapefile with data and returns a SF (Simple Features) dataframe for mapping\n\nMisc\n\nYou can create a discrete color palette with the at argument in the mapview function.\n\nExample\n# check min and max of your data to select range of bins\nmin(iowa_over_65, na.rm = TRUE) # 0\nmax(iowa_over_65, na.rm = TRUE) # 38.4\n\nm1 &lt;- \n  mapview(iowa_over_65, \n          zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1),\n          at = c(0, 10, 20, 30, 40))\n\nThis will result in a discrete palette with bins of 0-10, 10-20, etc. Looks like an overlap, so I’m sure which bin contains the endpoints.\n\n\n\nExample: Over 65 in Iowa by census tract\n\nlibrary(mapviw); library(viridisLite)\n\niowa_over_65 &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"DP1_0024P\",\n    state = \"IA\",\n    geometry = TRUE,\n    sumfile = \"dp\",\n    year = 2020\n  )\nm1 &lt;- \n  mapview(iowa_over_65, zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1))\nExport as an HTML file\nhtmlwidgets::saveWidget(m1@map, \"iowa_over_65.html\")\n\nCan embed it elsewhere (html report or website) by adding it as an asset",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html",
    "href": "qmd/model-building-concepts.html",
    "title": "Concepts",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html#sec-modbld-misc",
    "href": "qmd/model-building-concepts.html#sec-modbld-misc",
    "title": "Concepts",
    "section": "",
    "text": "Packages\n\n{multiverse} - makes it easy to specify and execute all combinations of reasonable analyses of a dataset\n\n\n\nPaper, Summary of it’s usage\nLots of vignettes\n\n\nRegression Workflow (Paper)\n\nMake ML model pipelines reusable and reproducible\n\n\nNotes from 7 Tips to Future-Proof Machine Learning Projects\nModularization - Useful for debugging and iteration\n\nDon’t used declarative programming. Create functions/classes for preprocessing, training, tuning, etc., and keep in separate files. You’ll call these functions in the main script\n\nHelper function\n## file preprocessing.py ##\ndef data_preparation(data):\n    data = data.drop(['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am'], axis=1)\n    numeric_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']\n    data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)\n    return data\nMain script\nfrom preprocessing import data_preparation \ntrain_preprocessed = data_preparation(train_data)\ninference_preprocessed = data_preparation(inference_data)\n\nKeep parameters in a separate config file\n\nConfig file\n## parameters.py ##\nDROP_COLS = ['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am']\nNUM_COLS = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']\nProprocessing script\n## preprocessing.py ##\nfrom parameters import DROP_COLS, NUM_COLS\ndef data_preparation(data):\n    data = data.drop(DROP_COLS, axis=1)\n    data[NUM_COLS] = data[NUM_COLS].fillna(data[NUM_COLS].mean())\n    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)\n    return data\n\n\nVersioning Code, Data, and Models - Useful for investigating drift\n\nSee tools like DVC, MLFlow, Weights and Biases, etc. for model and data versioning\n\nImportant to save data snapshots throughout the project lifecycle, for example: raw data, processed data, train data, validation data, test data and inference data.\n\nGithub and dbt for code versioning\n\nConsistent Structures - Consistency in project structures and naming can reduce human error, improve communication, and just make things easier to find.\n\nNaming examples:\n\n&lt;model-name&gt;-&lt;parameters&gt;-&lt;model-version&gt;\n&lt;model-name&gt;-&lt;data-version&gt;-&lt;use-case&gt;\n\nExample: Reduced project template based on {{cookiecutter}}\n├── data\n│   ├── output      &lt;- The output data from the model. \n│   ├── processed      &lt;- The final, canonical data sets for modeling.\n│   └── raw            &lt;- The original, immutable data dump.\n│\n├── models             &lt;- Trained and serialized models, model predictions, or model summaries\n│\n├── notebooks          &lt;- Jupyter notebooks. \n│\n├── reports            &lt;- Generated analysis as HTML, PDF, LaTeX, etc.\n│   └── figures        &lt;- Generated graphics and figures to be used in reporting\n│\n├── requirements.txt   &lt;- The requirements file for reproducing the analysis environment, e.g.\n│                         generated with `pip freeze &gt; requirements.txt`\n│\n├── code              &lt;- Source code for use in this project.\n    ├── __init__.py    &lt;- Makes src a Python module\n    │\n    ├── data           &lt;- Scripts to generate and process data\n    │   ├── data_preparation.py\n    │   └── data_preprocessing.py\n    │\n    ├── models         &lt;- Scripts to train models and then use trained models to make\n    │   │                 predictions\n    │   ├── inference_model.py\n    │   └── train_model.py\n    │\n    └── analysis  &lt;- Scripts to create exploratory and results oriented visualizations\n        └── analysis.py\n\n\nModel is performing well on the training set but much worse on the validation/test set\n\n\nAndrew Ng calls the validation set the “Dev Set” 🙄\nTest: Random sample the training set and use that as your validation set. Score your model on this new validation set\n\n“Train-Dev” is the sampled validation set\nPossibilities\n\nVariance: The data distribution of the training set is the same as the validation/test sets\n\n\nThe model has been overfit to the training data\n\nData Mismatch: The data distribution of the training set is NOT the same as the validation/test sets\n\n\nUnlucky and the split was bad\n\nSomething maybe is wrong with the splitting function\n\nSplit ratio needs adjusting. Validation set isn’t getting enough data to be representative.\n\n\n\n\nModel is performing well on the validation/test set but not in the real world\n\nInvestigate the validation/test set and figure out why it’s not reflecting real world data. Then, apply corrections to the dataset.\n\ne.g. distributions of your validation/tests sets should look like the real world data.\n\nChange the metric\n\nConsider weighting cases that your model is performing extremely poorly on.\n\n\nSplits\n\nHarrell: “not appropriate to split data into training and test sets unless n&gt;20,000 because of the luck (or bad luck) of the split.”\nIf your dataset is over 1M rows, then having a test set of 200K might be overkill (e.g. ratio of 60/20/20).\n\nMight be better to use a ratio of 98/1/1 for big data projects and 60/20/20 for smaller data projects\n\nlink\n\nShows that simple data splitting does not give valid confidence intervals (even asymptotically) when one refits the model on the whole dataset. Thus, if one wants valid confidence intervals for prediction error, we can only recommend either data splitting without refitting the model (which is viable when one has ample data), or nested CV.",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/cli.html",
    "href": "qmd/cli.html",
    "title": "CLI",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-misc",
    "href": "qmd/cli.html#sec-cli-misc",
    "title": "CLI",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly’s suggestions are prioritized in real time with a small neural network\n\nPath to a folder that’s above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder\n\nDebian vs. Ubuntu (from ChatGPT)\n\nStability vs. Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it’s known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu’s.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as “Debian spins,” catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-r",
    "href": "qmd/cli.html#sec-cli-r",
    "title": "CLI",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n      janitor::clean_names() %&gt;%\n      select(date, geo, county = name, negative, positive) %&gt;%\n      filter(geo == \"County\") %&gt;%\n      mutate(date = lubridate::as_date(date)) %&gt;%\n      select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it’ll search automatically\n35548",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-awk",
    "href": "qmd/cli.html#sec-cli-awk",
    "title": "CLI",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‘$13 &gt; 98’ adult_t.csv|head\nFilter lines with “Doctorate” and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-bash",
    "href": "qmd/cli.html#sec-cli-bash",
    "title": "CLI",
    "section": "Bash",
    "text": "Bash\n\nMisc\n\nNotes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn’t take notes on\n\nBash Scripting on Linux: The Complete Guide - video course\n\nResources\n\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n“&gt;” redirects the output from a program to a file.\n\n“&gt;&gt;” does the same thing, but it’s appending to an existing file instead of overwriting it, if it already exists.\n\n\n\n\n\nCommands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you’re using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you’re about to move does already exist under the new directory,\n\nThis way you don’t accidentally overwrite files that you didn’t mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\n\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\nOutput to file: echo “This is an example for redirect” &gt; file1.txt\nAppend line to file: echo “This is the second line of the file” &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to “tmp” directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name “my_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n“rwx” stand for read, write, and execute rights, respectively\nThe 3 “rwx” blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a “d” for directory or “l” for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - “super user” - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‘error’, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo “No such directory exist.Check”\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn’t exist, then the message “No such directory exists. check” message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for “error” and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for “error” in each line. Lines with “error” get written to “error_file.txt”\n\nFilter lines\ngrep -i “Doctorate” adult_t.csv |grep -i “Husband”|grep -i “Black”|csvlook\n# -i, --ignore-case-Ignore  case  distinctions,  so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i “Doctorate” adult_t.csv | wc -l\n\nCounts how many rows have “Doctorate”\n\n-wc is “word count”\n\n\n\n\n\n\nVariables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via “$”\n# local\nev_car=’Tesla’\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=’Tesla’\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it’s okay not to is on the left-hand-side of an [[ ]] condition. But even there I’d recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or –help or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this\n\n\n\n\n\nScripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like –silent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script’s directory close to the start of the script.\n\nAnd it’s usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck. Heed its warnings.\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don’t give executable permission to the script file.\nStarts with “#!” the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory “/bin”\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you’re good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n    #!/bin/bash\n    echo ‘Hello World’\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n    set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n    echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n    exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n    echo do awesome stuff\n}\nmain \"$@\"\n\n\n\nJob Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.\n\n\n\n\ntmux (‘terminal multiplexer’)\n\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‘moose’\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named “moose”\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you’ve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‘moose’\n\n\nPane Creation and Navigation\n\ncontrol+b then press ” (i.e. shift+’): add another terminal pane below\ncontrol+b then press % (i.e. shift+5) : add another terminal pane to the right\ncontrol+b then press → : move to the terminal pane on the right (similar for left, up, down)\n\n\n\n\nSSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n“ssh-keygen” is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named “id_rsa.pub” and a private key named “id_rsa”, and place both into your “~/.ssh” directory\nYou’ll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: “~/.ssh/config”\nContents\nHost dev\n  HostName remote\n  IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist\n\n\n\n\nVim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you’ll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you’re in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file\n\n\n\n\nPackages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to “upgrading” the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and “updates” them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade\n\n\n\n\nExpressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-powsh",
    "href": "qmd/cli.html#sec-cli-powsh",
    "title": "CLI",
    "section": "Powershell",
    "text": "Powershell\n\nMisc\n\nDocs, Sample Scripts\nUse Get-Help &lt;cmdlet-name&gt; to access help information for specific cmdlets.\n\n$_ (Docs)\n\nAutomatic Variable; Alias for $PSItem\n\nDocs for automatic variables\n\nUseful for looping through objects in a directory. See “Change Name (or Extensions) of Multiple Files” for an example of usage\nGet the properties of an object\n\nExample: Using Get-ItemProperty (Docs)\nGet-ItemProperty scrapsheet.txt | Format-List\n\nProperty names will be on the left side of the output\n\nExample: Using Get-Member (Docs)\n Get-ChildItem *.txt | Select-Object -First 1 | Get-Member\n\nSelects the first text file in the directory and gets the Properties and a bunch of other stuff like methods which also can be used with $_\nDescriptions are awful for a lot of the stuff, but for the most part, you can guess what the property is.\n\n\nSome properties that can be used with $_\n\nFile System Objects:\n\n.Name: Returns the name component of a file\n.FullName: Returns the full path of the file.\n.Length: Returns the size of the file in bytes.\n.CreationTime: Returns the date and time the file was created.\n.LastWriteTime: Returns the date and time the file was last modified.\n.IsReadOnly: Returns $true if the file is read-only, $false otherwise.\n\nProcess Objects:\n\n.Id: Returns the unique identifier (PID) of the process.\n.Name: Returns the name of the process.\n.MainWindowTitle: Returns the title of the process’s main window (if applicable).\n.WorkingDirectory: Returns the working directory of the process.\n.CPU: Returns the CPU usage of the process.\n\nRegistry Objects:\n\n.Name: Returns the name of the registry key.\n.Hive: Returns the hive of the registry key (e.g., HKLM, HKCU).\n.Value: Returns the value of the registry key (if applicable).\n.Path: Returns the full path of the registry key.\n\n\n\nComments: &lt;# comment #&gt;\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you’re already in the directory that you want the folder in. You can also use a path, e.g. \"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nChange Name (or Extensions) of Multiple Files\nGet-ChildItem *.md | Rename-Item -NewName { $_.Name -replace '.md','.qmd' }\n\nGet-ChildItem: cmdlet gets all the files in the current folder that have a .mc file extension\nResults piped to Rename-Item\n\nNewName: Has a value that’s a script block that runs before the value is submitted to the NewName parameter.\n\n$_: (automatic variable) It represents each file object as it comes to the command through the pipeline.\n-replace: Operator replaces the file extension of each file with .qmd.\n\nNotice that matching using the -replace operator is not case sensitive.\n\n\n\n\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it’s a backslash (\\), but in Powershell, it’s a backtick ( ` )\n*Don’t forget that there’s a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it’s open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-batscri",
    "href": "qmd/cli.html#sec-cli-batscri",
    "title": "CLI",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g. timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-wsl",
    "href": "qmd/cli.html#sec-cli-wsl",
    "title": "CLI",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for –distribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/json.html",
    "href": "qmd/json.html",
    "title": "JSON",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-misc",
    "href": "qmd/json.html#sec-json-misc",
    "title": "JSON",
    "section": "",
    "text": "Packages\n\n{yyjsonr} - A fast JSON parser/serializer, which converts R data to/from JSON and NDJSON. It is around 2x to 10x faster than jsonlite at both reading and writing JSON.\n{RcppSimdJson} - Comparable to {yyjsonr} in performance.\n\nAlso see\n\nBig Data &gt;&gt; Larger than Memory\nSQL &gt;&gt; Processing Expressions &gt;&gt; Nested Data\nDatabases &gt;&gt; DuckDB &gt;&gt; Misc\n\nhrbmstr recommends trying duckdb before using the cli tools in “Big Data”\n\n\nTools\n\n{listviewer}: Allows you to interactively explore and edit json files through the Viewer in the IDE. Docs show how it can be embedded into a Shiny app as well.\n\nExample\nlibrary(listviewer)\nmoose &lt;- jsonlite::read_json(\"path/to/file.json\")\njsonedit(moose)\nreactjson(moose)\n\nI’ve also used this a .config file which looked like a json file when I opened in a text editor, so this seems to work on anything json-like.\nreactjson has a copy button which is nice so that you can paste your edited version into a file.\njsonedit seems like it has more features, but I didn’t see a copy button. But there’s a view in which you can manually select everything a copy it via keyboard shortcut.",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-jsonlite",
    "href": "qmd/json.html#sec-json-jsonlite",
    "title": "JSON",
    "section": "{jsonlite}",
    "text": "{jsonlite}\n\nRead",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-py",
    "href": "qmd/json.html#sec-json-py",
    "title": "JSON",
    "section": "Python",
    "text": "Python\n\nExample: Parse Nested JSON into a dataframe (article)\n\nRaw JSON\n\n\n“entry” has the data we want\n“…” at the end indicates there are multiple objectss inside the element, “entry”\n\nProbably other root elements other than “feed” as well\n\n\nRead a json file from a URL using {{requests}} and convert to list\n\nimport requests\n\nurl = \"https://itunes.apple.com/gb/rss/customerreviews/id=1500780518/sortBy=mostRecent/json\"\n\nr = requests.get(url)\n\ndata = r.json()\nentries = data[\"feed\"][\"entry\"]\n\nIt looks like the list conversion also ordered the elements alphabetically\nThe output list is subsetted by the root element “feed” and the child element “entry”\n\nGet a feel for the final structure you want by hardcoding elements into a df\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    parsed_data[\"author_uri\"].append(entry[\"author\"][\"uri\"][\"label\"])\n    parsed_data[\"author_name\"].append(entry[\"author\"][\"name\"][\"label\"])\n    parsed_data[\"author_label\"].append(entry[\"author\"][\"label\"])\n    parsed_data[\"content_label\"].append(entry[\"content\"][\"label\"])\n    parsed_data[\"content_attributes_type\"].append(entry[\"content\"][\"attributes\"][\"type\"])\n    ... \nGeneralize extracting the properties of each object in “entry” with a nested loop\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    for key, val in entry.items():\n        for subkey, subval in val.items():\n            if not isinstance(subval, dict):\n                parsed_data[f\"{key}_{subkey}\"].append(subval)\n            else:\n                for att_key, att_val in subval.items():\n                    parsed_data[f\"{key}_{subkey}_{att_key}\"].append(att_val)\n\ndefaultdict creates a key from a list element (e.g. “author”) and groups the properties into a list of values where the value may also be a dict.\n\nSee Python, General &gt;&gt; Types &gt;&gt; Dictionaries\n\nFor each item in “entry”, it looks at the first key-value pair knowing that value is always a dictionary (object in JSON)\nThen handles two different cases\n\nFirst Case: The value dictionary is flat and does not contain another dictionary, only key-value pairs.\n\nCombine the outer key with the inner key to a column name and take the value as column value for each pair.\n\nSecond Case: Dictionary contains a key-value pair where the value is again a dictionary.\n\nAssumes at most two levels of nested dictionaries\nIterates over the key-value pairs of the inner dictionary and again combines the outer key and the most inner key to a column name and take the inner value as column value.\n\n\n\nRecursive function that handles json elements with deeper structures\n\ndef recursive_parser(entry: dict, data_dict: dict, col_name: str = \"\") -&gt; dict:\n    \"\"\"Recursive parser for a list of nested JSON objects\n\n    Args:\n        entry (dict): A dictionary representing a single entry (row) of the final data frame.\n        data_dict (dict): Accumulator holding the current parsed data.\n        col_name (str): Accumulator holding the current column name. Defaults to empty string.\n    \"\"\"\n    for key, val in entry.items():\n        extended_col_name = f\"{col_name}_{key}\" if col_name else key\n        if isinstance(val, dict):\n            recursive_parser(entry[key], data_dict, extended_col_name)\n        else:\n            data_dict[extended_col_name].append(val)\n\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    recursive_parser(entry, parsed_data, \"\")\n\ndf = pd.DataFrame(parsed_data)\n\nNotice the check for a deeper structure with isinstance. If there is one, then the function is called again.\nFunction outputs a dict which is coerced into dataframe\nTo get rid of “label” in column names: df.columns = [col if not \"label\" in col else \"_\".join(col.split(\"_\")[:-1]) for col in df.columns]\nobject types can be cast into more efficient types: df[\"im:rating\"] = df[\"im:rating\"].astype(int)",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html",
    "href": "qmd/cli-linux.html",
    "title": "Linux",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-misc",
    "href": "qmd/cli-linux.html#sec-cli-lin-misc",
    "title": "Linux",
    "section": "",
    "text": "Notes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn’t take notes on\n\nBash Scripting on Linux: The Complete Guide - video course\n\nResources\n\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n“&gt;” redirects the output from a program to a file.\n\n“&gt;&gt;” does the same thing, but it’s appending to an existing file instead of overwriting it, if it already exists.\n\n\nDebian vs. Ubuntu (from ChatGPT)\n\nStability vs. Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it’s known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu’s.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as “Debian spins,” catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-com",
    "href": "qmd/cli-linux.html#sec-cli-lin-com",
    "title": "Linux",
    "section": "Commands",
    "text": "Commands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you’re using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you’re about to move does already exist under the new directory,\n\nThis way you don’t accidentally overwrite files that you didn’t mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\n\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\nOutput to file: echo “This is an example for redirect” &gt; file1.txt\nAppend line to file: echo “This is the second line of the file” &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to “tmp” directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name “my_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n“rwx” stand for read, write, and execute rights, respectively\nThe 3 “rwx” blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a “d” for directory or “l” for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - “super user” - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‘error’, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo “No such directory exist.Check”\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn’t exist, then the message “No such directory exists. check” message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for “error” and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for “error” in each line. Lines with “error” get written to “error_file.txt”\n\nFilter lines\ngrep -i “Doctorate” adult_t.csv |grep -i “Husband”|grep -i “Black”|csvlook\n# -i, --ignore-case-Ignore  case  distinctions,  so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i “Doctorate” adult_t.csv | wc -l\n\nCounts how many rows have “Doctorate”\n\n-wc is “word count”",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-var",
    "href": "qmd/cli-linux.html#sec-cli-lin-var",
    "title": "Linux",
    "section": "Variables",
    "text": "Variables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via “$”\n# local\nev_car=’Tesla’\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=’Tesla’\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it’s okay not to is on the left-hand-side of an [[ ]] condition. But even there I’d recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or –help or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-script",
    "href": "qmd/cli-linux.html#sec-cli-lin-script",
    "title": "Linux",
    "section": "Scripting",
    "text": "Scripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like –silent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script’s directory close to the start of the script.\n\nAnd it’s usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck. Heed its warnings.\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don’t give executable permission to the script file.\nStarts with “#!” the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory “/bin”\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you’re good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n    #!/bin/bash\n    echo ‘Hello World’\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n    set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n    echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n    exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n    echo do awesome stuff\n}\nmain \"$@\"",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-jobm",
    "href": "qmd/cli-linux.html#sec-cli-lin-jobm",
    "title": "Linux",
    "section": "Job Management",
    "text": "Job Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-tmux",
    "href": "qmd/cli-linux.html#sec-cli-lin-tmux",
    "title": "Linux",
    "section": "tmux (terminal multiplexer)",
    "text": "tmux (terminal multiplexer)\n\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‘moose’\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named “moose”\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you’ve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‘moose’\n\n\nPane Creation and Navigation\n\ncontrol+b then press ” (i.e. shift+’): add another terminal pane below\ncontrol+b then press % (i.e. shift+5) : add another terminal pane to the right\ncontrol+b then press → : move to the terminal pane on the right (similar for left, up, down)",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-ssh",
    "href": "qmd/cli-linux.html#sec-cli-lin-ssh",
    "title": "Linux",
    "section": "SSH",
    "text": "SSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n“ssh-keygen” is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named “id_rsa.pub” and a private key named “id_rsa”, and place both into your “~/.ssh” directory\nYou’ll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: “~/.ssh/config”\nContents\nHost dev\n  HostName remote\n  IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-vim",
    "href": "qmd/cli-linux.html#sec-cli-lin-vim",
    "title": "Linux",
    "section": "Vim",
    "text": "Vim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you’ll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you’re in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-pkg",
    "href": "qmd/cli-linux.html#sec-cli-lin-pkg",
    "title": "Linux",
    "section": "Packages",
    "text": "Packages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to “upgrading” the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and “updates” them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-expr",
    "href": "qmd/cli-linux.html#sec-cli-lin-expr",
    "title": "Linux",
    "section": "Expressions",
    "text": "Expressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-general.html",
    "href": "qmd/cli-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-misc",
    "href": "qmd/cli-general.html#sec-cli-gen-misc",
    "title": "General",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly’s suggestions are prioritized in real time with a small neural network\n\nPath to a folder that’s above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-r",
    "href": "qmd/cli-general.html#sec-cli-gen-r",
    "title": "General",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n      janitor::clean_names() %&gt;%\n      select(date, geo, county = name, negative, positive) %&gt;%\n      filter(geo == \"County\") %&gt;%\n      mutate(date = lubridate::as_date(date)) %&gt;%\n      select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it’ll search automatically\n35548",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-awk",
    "href": "qmd/cli-general.html#sec-cli-gen-awk",
    "title": "General",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‘$13 &gt; 98’ adult_t.csv|head\nFilter lines with “Doctorate” and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html",
    "href": "qmd/cli-windows.html",
    "title": "Windows",
    "section": "",
    "text": "PowerShell",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-powsh",
    "href": "qmd/cli-windows.html#sec-cli-win-powsh",
    "title": "Windows",
    "section": "",
    "text": "Misc\n\nDocs, Sample Scripts\nUse Get-Help &lt;cmdlet-name&gt; to access help information for specific cmdlets.\nCheck version: $PSVersionTable\n\nFor a breakdown of the version number (e.g. build, revison, etc.): $PSVersionTable.PSVersion\n\nUpdate to latest stable version: github\nComments: &lt;# comment #&gt;\nClear terminal: clear or cls or Clear-Host\nBefore you’ll be able to run a script, you need to open PowerShell as administrator and execute this command: Set-ExecutionPolicy RemoteSigned\n\n\n\nLoops\n\nIterables\n\nArrays : $folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n$_ (Docs)\n\nAutomatic Variable; Alias for $PSItem\n\nDocs for automatic variables\n\nUseful for looping through objects in a directory. See “Change Name (or Extensions) of Multiple Files” for an example of usage\nGet the properties of an object\n\nExample: Using Get-ItemProperty (Docs)\nGet-ItemProperty scrapsheet.txt | Format-List\n\nProperty names will be on the left side of the output\n\nExample: Using Get-Member (Docs)\n Get-ChildItem *.txt | Select-Object -First 1 | Get-Member\n\nSelects the first text file in the directory and gets the Properties and a bunch of other stuff like methods which also can be used with $_\nDescriptions are awful for a lot of the stuff, but for the most part, you can guess what the property is.\n\n\nSome properties that can be used with $_\n\nFile System Objects:\n\n.Name: Returns the name component of a file\n.FullName: Returns the full path of the file.\n.Length: Returns the size of the file in bytes.\n.CreationTime: Returns the date and time the file was created.\n.LastWriteTime: Returns the date and time the file was last modified.\n.IsReadOnly: Returns $true if the file is read-only, $false otherwise.\n\nProcess Objects:\n\n.Id: Returns the unique identifier (PID) of the process.\n.Name: Returns the name of the process.\n.MainWindowTitle: Returns the title of the process’s main window (if applicable).\n.WorkingDirectory: Returns the working directory of the process.\n.CPU: Returns the CPU usage of the process.\n\nRegistry Objects:\n\n.Name: Returns the name of the registry key.\n.Hive: Returns the hive of the registry key (e.g., HKLM, HKCU).\n.Value: Returns the value of the registry key (if applicable).\n.Path: Returns the full path of the registry key.\n\n\n\n\n\n\nForeach\n\nUses a typical for-loop structure\nIterate over an array\n# Create an array of folders\n$folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n\n# Perform iteration to create the same file in each folder\nforeach ($i in $folders) {\n    Add-Content -Path \"$i\\SampleFile.txt\" -Value \"This is the content of the file\"\n}\n\n$i is the for-loop variable and $folders is the iterable\nAdd-Content creates a text file in each of the folders in the array.\n\nIterate over the output of Get-ChildItem\n# Define the directory containing the files\n$directory = \"C:\\Path\\To\\Files\"\n\n# Define the FFmpeg argument string\n$ffmpegArg = \"-i {0}.mp4 -c:v libx265 -crf 28 -preset medium -vf scale=-1:720 -c:a copy Documents\\temp-storage\\{1}.mp4\"\n\n# Get all files in the directory\n$files = Get-ChildItem -Path $directory -Filter \"*.mp4\"\n\n# Loop through each file and apply the FFmpeg command\nforeach ($file in $files) {\n  # Construct the full argument string with the current file path\n  $fullArg = $ffmpegArg -f [string]::Format($file.FullName, $file.Name)\n\n  # Execute the FFmpeg command\n  Start-Process -FilePath \"ffmpeg.exe\" -ArgumentList $fullArg -Wait -NoNewWindow\n}\n\nWrite-Host \"Finished processing files!\"\n\nGet-ChildItem retrieves files from the specified directory ($directory).\n\n-Filter filters files that match the pattern (e.g., *.mp4).\n\nThe foreach loop iterates through each file ($file) in the $files collection.\n-f flag stands for format. Says to replace {0} and {1} in the $ffmpegCommand template with these properties.\n\n[string]::Format casts the file properties, FullName and Name, into strings.\n\nStart-Process launches ffmpeg.exe with the constructed $fullCommand arguments.\n\n-Wait ensures the command finishes before continuing.\n-NoNewWindow hides the ffmpeg console window.\n\n\n\n\n\nForEach-Object\n\nSimilar to {purrr::map}\nIterable is piped into ForEach-Object\n$folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n$folders | ForEach-Object (Add-Content -Path \"$_\\SampleFile.txt\" -Value \"This is the content of the file\")\n\nDoes the same thing as the first example in the Foreach section\nAdd-Content creates a text file in each of the folders in the array.\n$_ is the for-loop variable — called an “automatic variable.” See Iterables section.\n\n\n\n\nForEach Method\n\nSimilar to using Pyhon’s apply on an iterable.\nMethod applied an array\n$folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n$folders.ForEach({\n    Add-Content -Path \"$_\\SampleFile.txt\" -Value \"This is the content of the file\"\n})\n\nDoes the same thing as the first example in the Foreach section\nAdd-Content creates a text file in each of the folders in the array.\n$_ is the for-loop variable — called an “automatic variable.” See Iterables section.\n\n\n\n\n\nCommands\n\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you’re already in the directory that you want the folder in. You can also use a path, e.g. \"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nChange Name (or Extensions) of Multiple Files\nGet-ChildItem *.md | Rename-Item -NewName { $_.Name -replace '.md','.qmd' }\n\nGet-ChildItem: cmdlet gets all the files in the current folder that have a .mc file extension\nResults piped to Rename-Item\n\nNewName: Has a value that’s a script block that runs before the value is submitted to the NewName parameter.\n\n$_: (automatic variable) It represents each file object as it comes to the command through the pipeline.\n-replace: Operator replaces the file extension of each file with .qmd.\n\nNotice that matching using the -replace operator is not case sensitive.\n\n\n\n\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it’s a backslash (\\), but in Powershell, it’s a backtick ( ` )\n*Don’t forget that there’s a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it’s open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"\n\n\n\n\nSnippets\n\nRead in name of servers and ping each of them\n\n$servers = Get-Content .\\servers.txt\n\nforeach ($server in $servers) {\n    try {\n        $null = Test-Connection -ComputerName $server -Count 1 -ErrorAction STOP\n        Write-Output \"$server - OK\"\n    }\n    catch {\n        Write-Output \"$server - $($_.Exception.Message)\"\n    }\n}\n\nGet-Content reads the server names from each line in the the server.txt file\nforeach iterates through the server names\ntry tests the connection and catch outputs an error message if a server fails.\n\nThe error message line has an interesting syntax that I don’t quite understand. I think $server is the input for $_.\n\nI guess Test-Connection output has to be stored into a variable even though that variable isn’t used for anything",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-batscri",
    "href": "qmd/cli-windows.html#sec-cli-win-batscri",
    "title": "Windows",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g. timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-wsl",
    "href": "qmd/cli-windows.html#sec-cli-win-wsl",
    "title": "Windows",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for –distribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html",
    "href": "qmd/confidence-and-prediction-intervals.html",
    "title": "Confidence & Prediction Intervals",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-misc",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-misc",
    "title": "Confidence & Prediction Intervals",
    "section": "",
    "text": "Also see Mathematices, Statistics &gt;&gt; Descriptive Statistics &gt;&gt; Understanding CI, sd, and sem Bars\nSE used for CIs of the difference in proportion\n\\[\n\\text{SE} = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\n\\]",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-terms",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-terms",
    "title": "Confidence & Prediction Intervals",
    "section": "Terms",
    "text": "Terms\n\nConfidence Intervals: A range of values within which we are reasonably confident the true parameter (e.g mean) of a population lies, based on a sample statistic (e.g. t-stat).\n\nFrequentist Interpretation: The confidence interval is constructed by a procedure, which, if you were to repeat the experiment and collecting samples many many times, in 95% of the experiments, the corresponding confidence intervals would cover the true value of the population mean. (link)\n\\[\n[100\\cdot(1-\\alpha)]\\;\\%\\: \\text{CI for}\\: \\hat\\beta_i = \\hat\\beta_i \\pm \\left[t_{(1-\\alpha/2)(n-k)} \\cdot \\text{SE}(\\hat\\beta_i)\\right]\n\\]\n\n\\(t\\) is the t-stat for\n\n\\(n-k\\) = sample size - number of predictors\n\\(1-\\alpha\\) for 2-sided; \\(1 - (\\alpha/2)\\) for 1 sided (I think)\n\n\\(\\text{SE}(\\beta_i)\\) is the sqrt of the corresponding value on the diagonal of the variance-covariance matrix for the coefficients.\n\nBayesian Interpretation: the true value is in that interval with 95% probability\n\nCoverage or Empirical Coverage: The level of coverage actually observed when evaluated on a dataset, typically a holdout dataset not used in training the model. Rarely will your model produce the Expected Coverage exactly\n\nAdaptive Coverage: Setting your Expected Coverage so that your Empirical Coverage = Target Coverage. A conformal prediction algorithm is adaptive if it not only achieves marginal coverage, but also (approximately) conditional coverage\n\nExample: 90% target coverage\n\nIf our model is slightly overfit, you might see that a 90% expected coverage leads to an 85% empirical coverage on a holdout dataset. To align your target and empirical coverage at 90%, may require setting expected coverage at something like 93%\n\n\nExpected Coverage: The level of confidence in the model for the prediction intervals.\nConditional Coverage: The coverage for each individual class of the outcome variable or subset of data specified by a grouping variable.\nMarginal Coverage: The overall average coverage across all classes of the outcome variable. All conformal methods achieve at or near the Expected Coverage averaged across classes but not necessarily for each individual class.\nTarget Coverage: The level of coverage you want to attain on a holdout dataset\n\ni.e. The proportion of observations you want to fall within your prediction intervals\n\n\nJeffrey’s Interval: Bayesian CIs for Binomial proportions (i.e. probability of an event)\n# probability of event\n# n_rain in the number of events (rainy days)\n# n is the number of trials (total days)\nmutate(pct_rain = n_rain / n, \n       # jeffreys interval\n       # bayesian CI for binomial proportions\n       low = qbeta(.025, n_rain + .5, n - n_rain + .5), \n       high = qbeta(.975, n_rain + .5, n - n_rain + .5))\nPrediction Interval: Used to estimate the range within which a future observation is likely to fall\n\nStandard Procedure for computing PIs for predictions (See link for examples and further details)\n\\[\n\\hat Y_0 \\pm t^{n-p}_{\\alpha/2} \\;\\hat\\sigma \\sqrt{1 + \\vec x_0'(X'X)^{-1}\\vec x_0}\n\\]\n\n\\(Y_0\\) is a single prediction\n\\(t\\) is the t-stat for\n\n\\(n-p\\) = sample size - number of predictors\n\\(1 - \\alpha\\) for 2-sided; \\(1 - (\\alpha/2)\\) for 1 sided (I think)\n\n\\(\\hat\\sigma\\) is the variance given by residual standard error, summary(Model1)$sigma\n\\[\nS^2 = \\frac{1}{n-p}\\;||\\;Y-X\\hat \\beta\\;||^2\n\\]\n\n\\(S = \\hat \\sigma\\)\nI think this is also the \\(\\operatorname{MSE}/\\operatorname{dof}\\) that you sometimes see in other formulas\n\n\\(x_0\\) is new data for the predictor variable values for the prediction (also would need to include a 1 for the intercept)\n\\((X'X)^{-1}\\) is the variance covariance matrix, vcov(model)",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-diag",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-diag",
    "title": "Confidence & Prediction Intervals",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nMean Interval Score (MIS)\n\n(Proper) Score of both coverage and interval width\n\nI don’t think there’s a closed range, so it’s meant for model comparison\nLower is better\n\ngreybox::MIS and (scaled) greybox::sMIS\n\nOnline docs don’t have these functions, but docs in RStudio do\n\nAlso scoringutils::interval_score\n\nDocs have formula\n\nThe actual paper is dense Need to take the mean of MIS\n\n\n\nCoverage\n\nExample: Coverage %\ncoverage &lt;- function(df, ...){\n  df %&gt;%\n    mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price pred_upper, 1, 0)) %&gt;% \n    group_by(...) %&gt;% \n    summarise(n = n(),\n              n_covered = sum(\n                covered\n              ),\n              stderror = sd(covered) / sqrt(n),\n              coverage_prop = n_covered / n)\n}\nrf_preds_test %&gt;% \n  coverage() %&gt;% \n  mutate(across(c(coverage_prop, stderror), ~.x * 100)) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt_number(\"stderror\", decimals = 2) %&gt;% \n  gt::fmt_number(\"coverage_prop\", decimals = 1)\n\nFrom Quantile Regression Forests for Prediction Intervals\nSale_Price is the outcome variable\nrf_preds_test is the resulting object from predict with a tidymodels model as input\n\nExample: Test consistency of coverage across quintiles\npreds_intervals %&gt;%  # preds w/ PIs\n  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;%  # quintiles\n  mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price &lt;= .pred_upper, 1, 0)) %&gt;% \n  with(chisq.test(price_grouped, covered))\n\np value &lt; 0.05 says coverage significantly differs by quintile\nSale_Price is the outcome variable\n\n\nInterval Width\n\nNarrower bands should mean a more precise model\nExample: Average interval width across quintiles\nlm_interval_widths &lt;- preds_intervals %&gt;% \n  mutate(interval_width = .pred_upper - .pred_lower,\n        interval_pred_ratio = interval_width / .pred) %&gt;% \n  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;% # quintiles\n  group_by(price_grouped) %&gt;% \n  summarize(n = n(),\n            mean_interval_width_percentage = mean(interval_pred_ratio),\n            stdev = sd(interval_pred_ratio),\n            stderror = stdev / sqrt(n)) %&gt;% \n  mutate(x_tmp = str_sub(price_grouped, 2, -2)) %&gt;% \n  separate(x_tmp, c(\"min\", \"max\"), sep = \",\") %&gt;% \n  mutate(across(c(min, max), as.double)) %&gt;% \n  select(-price_grouped)\n\nlm_interval_widths %&gt;% \n  mutate(across(c(mean_interval_width_percentage, stdev, stderror), ~.x*100)) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt_number(c(\"stdev\", \"stderror\"), decimals = 2) %&gt;% \n  gt::fmt_number(\"mean_interval_width_percentage\", decimals = 1)\n\nInterval width has actually been transformed into a percentage as related to the prediction (removes the scale of the outcome variable)",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-boot",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-boot",
    "title": "Confidence & Prediction Intervals",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nMisc\n\nDo NOT bootstrap the standard deviation\n\narticle\nbootstrap is “based on a weak convergence of moments”\nif you use an estimate based standard deviation of the bootstrap, you are being overly conservative (i.e. overestimate the sd)\n\nbootstrapping uses the original, initial sample as the population from which to resample, whereas Monte Carlo simulation is based on setting up a data generation process (with known values of the parameters of a known distribution). Where Monte Carlo is used to test drive estimators, bootstrap methods can be used to estimate the variability of a statistic and the shape of its sampling distribution\nPackages\n\n{ebtools::get_boot_ci}\n\n\nSteps\n\nResample with replacement\nCalculate statistic of resample\nStore statistic\nRepeat 10K or so times\nCalculate mean, sd, and quantiles for CIs across all collected statistics\n\nCIs\n\nPlenty of articles for means and models, see bkmks\nrsample::reg_intervals is a convenience function for lm, glm, survival models\n\nPIs\n\nBootstrapping PIs is a bit complicated\n\nSee Shalloway’s article (code included)\nonly use out-of-sample estimates to produce the interval\nestimate the uncertainty of the sample using the residuals from a separate set of models built with cross-validation",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-conf",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-conf",
    "title": "Confidence & Prediction Intervals",
    "section": "Conformal Prediction Intervals",
    "text": "Conformal Prediction Intervals\n\nMisc\n\nPackages\n\n{{mapie}} - Handles scikit-learn, tf, pytorch, etc. with wrappers. Computes conformal PIs for Regression, Classification, and Time Series models.\n\nRegression\n\nMethods: naive, split, jackknife, jackknife+, jackknife-minmax, jackknife-after-bootstrap, CV, CV+, CV-minmax, ensemble batch prediction intervals (EnbPI).\n“Since the typical coverage levels estimated by jackknife+ follow very closely the target coverage levels, this method should be used when accurate and robust prediction intervals are required.”\n“For practical applications where N is large and/or the computational time of each leave-one-out simulation is high, it is advised to adopt the CV+ method” even though the interval width will be slightly larger than jackknife+\n“The jackknife-minmax and CV-minmax methods are more conservative since they result in higher theoretical and practical coverages due to the larger widths of the prediction intervals. It is therefore advised to use them when conservative estimates are needed.”\n“The conformalized quantile regression method allows for more adaptiveness on the prediction intervals which becomes key when faced with heteroscedastic data.”\nEnbPI is for time series and residuals must be updated each time new observations are available\n\nClassification\n\nMethods: LAC, Top-K, Adaptive Prediction Sets (APS), Regularized Adaptive Prediction Sets (RAPS), Split and Cross-Conformal methods.\nThe difference between these methods is the way the conformity scores are computed\nLAC method is not adaptive: the coverage guarantee only holds on average (i.e. marginal coverage). Difficult classification cases may have prediction sets that are too small, and easy cases may have sets that are too large. (See below for details on process). Doesn’t seem like to great a task to manually make it adaptive though (See example below).\nAPS’ conformity score used to determine the threshold is a constrained sum of the predicted probabilities for that observation. Only the predicted probabilites \\(\\ge\\) the predicted probability of the true class are included in the sum. Everything else is the same as the LAC algorithm, although the default behavior is to keep the last class that crosses the threshold through the argument, include_last_label = [True, “randomized”, False]. The value of the argument can determine whether conditional coverage is (approximately) attained with True being the most liberal setting. Note that only “randomized” can produce empty predicted class sets. Algorithm tends to produce large predicted class sets when there are many classes in the outcome variable.\nRAPS attenuates the lengthier predicted class sets in APS through regularization. A penalty, \\(\\lambda\\), is added to predicted probabilities with ranks greater that some value, \\(k\\). Everything else is the same as APS.\nNot sure what Split is, but Cross-Conformal is CV applied to LAC and APS.\n\n\n\nNotes from\n\nHow to Handle Uncertainty in Forecasts: A deep dive into conformal prediction\n\nThe conformity score formula used in this article, \\(s_i = |\\;y_i - \\hat p_i(y_i\\;|\\;X_i)\\;|\\) where \\(y_i\\) is the observed class and \\(\\hat p\\) is the predicted probability, has the same results as to the one below, but it’s not workable in production since there is no observed class.\n\nConformal Prediction for Machine Learning Classification — From the Ground Up\n“MAPIE” Explained Exactly How You Wished Someone Explained to You\n\nResources\n\nIntroduction To Conformal Prediction With Python A Short Guide for Quantifying Uncertainty of Machine Learning Models\n\nSee R &gt;&gt; Documents &gt;&gt; Machine Learning\n\n\nNormal PIs require iid data while conformal PIs only require the “identically distributed” part (not independent) and therefore should provide more robust coverage.\nContinuous outcome (link) using quantile regression\n\n\n\nClassification\n\nLAC (aka Score Method) Process\n\nSplit data into Train, Calibration (aka Validation), and Test\nTrain the model on the training set\nOn the calibration (aka validation) set, compute the conformity scores only for the observed class (i.e. true label) for each observation\n\\[\ns_{i, j}  = 1 - \\hat p_{i,j}(y_i | X_i)\n\\]\n\nVariables\n\n\\(s_{i,j}\\): Conformity Score for the ith observation and class \\(j\\)\n\\(y_i\\): Observed Class\n\\(\\hat p_{i,j}\\): Predicted probability by the model for class \\(j\\)\n\\(X_i\\): Predictors\n\\(i\\): Index of the observed data\n\\(j\\): Class of the outcome variable\n\nRange: [0, 1]\nIn general, Low = good, High = bad\nIn R, the predicted probabilities for statistical models are always for the event (i.e. \\(y_i = 1\\)) in a binary outcome context, so when the observed class = 0, the score will be \\(s_{i,0} = 1-(1- \\hat p_{i, 1}(y_i | X_i)) = \\hat p_{i, 1}(y_i | X_i)\\) which is just the predicted probability.\n\nOrder the conformity scores from highest to lowest\nAdjust the chosen the \\(\\alpha\\) using a finite sample correction, \\(q_{\\text{level}} = 1- \\frac{ceil((n_{\\text{cal}}+1)\\alpha)}{n_{\\text{cal}}}\\) and calculate the quantile.\nCalculate the critical value or threshold for the quantile\n\n\nx-axis corresponds to an ordered set of conformity scores\nIf \\(\\alpha = 0.05\\), find the score value at the the 95th percentile (e.g. quantile(scores, 0.95))\nBlue: conformity scores are not statistically significant. They’re within our prediction interval.\nRed: Very large conformity scores indicate high divergence from the true label. These conformal scores are statistically significant and thereby outside of our prediction interval.\n\nPredict on the Test set and calculate conformity scores for each class\nFor each test set observation, select classes that have scores below the threshold score as the model prediction.\n\nAn observation could potentially have both classes or no classes selected. ( Not sure if this is true in a binary outcome situation)\n\n\nExample: LAC Method, Multinomial\n\nModel\nclassifier = LogisticRegression(random_state=42)\nclassifier.fit(X_train, y_train)\nScores calculated using only the predicted probability for the true class on the Validation set (aka Calibration set)\n# Get predicted probabilities for calibration set\ny_pred = classifier.predict(X_Cal)\ny_pred_proba = classifier.predict_proba(X_Cal)\nsi_scores = []\n# Loop through all calibration instances\nfor i, true_class in enumerate(y_cal):\n    # Get predicted probability for observed/true class\n    predicted_prob = y_pred_proba[i][true_class]\n    si_scores.append(1 - predicted_prob) \nThe threshold determines what coverage our predicted labels will have\nnumber_of_samples = len(X_Cal)\nalpha = 0.05\nqlevel = (1 - alpha) * ((number_of_samples + 1) / number_of_samples)\nthreshold = np.percentile(si_scores, qlevel*100)\nprint(f'Threshold: {threshold:0.3f}')\n#&gt; Threshold: 0.598\n\nFinite sample correction for the 95th quantile: multiply 0.95 by (n+1)/n\n\nThreshold is then used to get predicted labels of the test set\n# Get standard predictions for comparison\ny_pred = classifier.predict(X_test)\n# Calc scores, then only take scores in the 95% conformal PI\nprediction_sets = (1 - classifier.predict_proba(X_test) &lt;= threshold)\n\n# Get labels for predictions in conformal PI\ndef get_prediction_set_labels(prediction_set, class_labels):\n    # Get set of class labels for each instance in prediction sets\n    prediction_set_labels = [\n        set([class_labels[i] for i, x in enumerate(prediction_set) if x]) for prediction_set in \n        prediction_sets]\n    return prediction_set_labels\n\n# Compare conformal prediction with observed and traditional preds\nresults_sets = pd.DataFrame()\nresults_sets['observed'] = [class_labels[i] for i in y_test]\nresults_sets['conformal'] = get_prediction_set_labels(prediction_sets, class_labels)\nresults_sets['traditional'] = [class_labels[i] for i in y_pred]\nresults_sets.head(10)\n#&gt;    observed  conformal        traditional\n#&gt; 0  blue      {blue}           blue\n#&gt; 1  green     {green}          green\n#&gt; 2  blue      {blue}           blue\n#&gt; 3  green     {green}          green\n#&gt; 4  orange    {orange}         orange\n#&gt; 5  orange    {orange}         orange\n#&gt; 6  orange    {orange}         orange\n#&gt; 7  orange    {blue, orange}   blue\n#&gt; 8  orange    {orange}         orange\n#&gt; 9  orange    {orange}         orange\n\nconformity scores are calculated for each potential class using the predicted probabilities on the test set\nThe predicted class for an observation is determined by whether a class has a score below the threshold.\nTherefore, an observation may have 1 or more predicted classes or 0 predicted classes.\n\nStatistics (See Statistics section for functions)\n\nOverall\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n#&gt; Overall coverage: 0.947\n#&gt; Average set size: 1.035\n\nOverall coverage is very close to the target coverage of 95%, therefore, marginal coverage is achieved which is expected for this method\n\nPer Class\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n#&gt;         Class counts  Coverage   Average set size\n#&gt; blue    241           0.817427   1.087137\n#&gt; orange  848           0.954009   1.037736\n#&gt; green   828           0.977053   1.016908\n\nOverall coverage (i.e. for all labels) will be at or very near 95% but coverage for individual classes may vary.\n\nAn illustration of how this method lacks Conditional Coverage\nSolution: Get thresholds for each class. (See next example)\n\nNote that the blue class had substantially fewer observations that the other 2 classes.\n\n\n\nExample: LAC-adapted - Threshold per Class\n\nDon’t think {{mapie}} has this option.\nAlso possible do this for subgroups of data, such as ensuring equal coverage for a diagnostic across racial groups, if we found coverage using a shared threshold led to problems.\nCalculate individual class thresholds\n# Set alpha (1 - coverage)\nalpha = 0.05\nthresholds = []\n# Get predicted probabilities for calibration set\ny_cal_prob = classifier.predict_proba(X_Cal)\n# Get 95th percentile score for each class's s-scores\nfor class_label in range(n_classes):\n    mask = y_cal == class_label\n    y_cal_prob_class = y_cal_prob[mask][:, class_label]\n    s_scores = 1 - y_cal_prob_class\n    q = (1 - alpha) * 100\n    class_size = mask.sum()\n    correction = (class_size + 1) / class_size\n    q *= correction\n    threshold = np.percentile(s_scores, q)\n    thresholds.append(threshold)\nApply individual class thresholds to test set scores\n# Get Si scores for test set\npredicted_proba = classifier.predict_proba(X_test)\nsi_scores = 1 - predicted_proba\n\n# For each class, check whether each instance is below the threshold\nprediction_sets = []\nfor i in range(n_classes):\n    prediction_sets.append(si_scores[:, i] &lt;= thresholds[i])\nprediction_sets = np.array(prediction_sets).T\n\n# Get prediction set labels and show first 10\nprediction_set_labels = get_prediction_set_labels(prediction_sets, class_labels)\nStatistics\n\nOverall\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n#&gt; Overall coverage: 0.95\n#&gt; Average set size: 1.093\n\nSimilar to previous example\n\nPer Class\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n#&gt;         Class counts  Coverage   Average set size\n#&gt; blue    241           0.954357   1.228216\n#&gt; orange  848           0.956368   1.139151\n#&gt; green   828           0.942029   1.006039\n\nCoverages now very close to 95% and the average set sizes have increased, especially for Blue.\n\n\n\n\n\n\nContinuous\n\nConformalized Quantile Regression Process\n\nSplit data into Training, Calibration, and Test sets\n\nTraining data: data on which the quantile regression model learns.\nCalibration data: data on which CQR calibrates the intervals.\n\nIn the example, he split the data into 3 equal sets\n\nTest data: data on which we evaluate the goodness of intervals.\n\nFit quantile regression model on training data.\nUse the model obtained at previous step to predict intervals on calibration data.\n\nPIs are predictions at the quantiles:\n\n(alpha/2)*100) (e.g 0.025, alpha = 0. 05)\n(1-(alpha/2))*100) (e.g. 0.975)\n\n\nCompute conformity scores on calibration data and intervals obtained at the previous step.\n\nResiduals are calculated for the PI vectors\nScores are calculated by taking the row-wise maximum of both (upper/lower quantile) residual vectors (e.g s_i &lt;- pmax(lower_pi_res, upper_pi_res))\n\nGet 1-alpha quantile from the distribution of conformity scores (e.g threshold &lt;- quantile(s_i, 0.95)\n\nThis score value will be the threshold\n\nUse the model obtained at step 1 to make predictions on test data.\n\nCompute PI vectors (i.e. predictions at the previously stated quantiles) on Test set\ni.e. Same calculation as with the calibration data in step 2 where you use the model to predict at upper and lower PI quantiles.\n\nCompute lower/upper end of the interval by subtracting/adding the threshold from/to the quantile predictions (aka PIs)\n\nLower conformity interval: lower_pi &lt;- test_lower_pred  - threshold\nUpper conformity interval: upper_pi &lt;- test_upper_pred + threshold\n\n\nExample: Quantile Random Forest\nimport numpy as np\nfrom skgarden import RandomForestQuantileRegressor\n\nalpha = .05\n\n# 1. Fit quantile regression model on training data\nmodel = RandomForestQuantileRegressor().fit(X_train, y_train)\n\n# 2. Make prediction on calibration data\ny_cal_interval_pred = np.column_stack([\n    model.predict(X_cal, quantile=(alpha/2)*100), \n    model.predict(X_cal, quantile=(1-alpha/2)*100)])\n\n# 3. Compute conformity scores on calibration data\ny_cal_conformity_scores = np.maximum(\n    y_cal_interval_pred[:,0] - y_cal, \n    y_cal - y_cal_interval_pred[:,1])\n\n# 4. Threshold: Get 1-alpha quantile from the distribution of conformity scores\n#    Note: this is a single number\nquantile_conformity_scores = np.quantile(\n    y_cal_conformity_scores, 1-alpha)\n\n# 5. Make prediction on test data\ny_test_interval_pred = np.column_stack([\n    model.predict(X_test, quantile=(alpha/2)*100), \n    model.predict(X_test, quantile=(1-alpha/2)*100)])\n\n# 6. Compute left (right) end of the interval by\n#    subtracting (adding) the quantile to the predictions\ny_test_interval_pred_cqr = np.column_stack([\n    y_test_interval_pred[:,0] - quantile_conformity_scores,\n    y_test_interval_pred[:,1] + quantile_conformity_scores])\n\n\n\nStatistics\n\nAverage Set Size\n\nThe average number of predicted classes per observation since there can be more than 1 predicted class in the conformal PI\nExample:\n# average set size for each class\ndef get_average_set_size(prediction_sets, y_test):\n    average_set_size = []\n    for i in range(n_classes):\n        average_set_size.append(\n            np.mean(np.sum(prediction_sets[y_test == i], axis=1)))\n    return average_set_size   \n\n# Overall average set size (weighted by class size)\n# Get class counts\ndef get_class_counts(y_test):\n    class_counts = []\n    for i in range(n_classes):\n        class_counts.append(np.sum(y_test == i))\n    return class_counts\n\ndef get_weighted_set_size(set_size, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_set_size = np.sum((set_size * class_counts) / total_counts)\n    weighted_set_size = round(weighted_set_size, 3)\n    return weighted_set_size\n\nCoverage\n\nClassification: Percentage of correct classifications\nExample: Classification\n# coverage for each class\ndef get_coverage_by_class(prediction_sets, y_test):\n    coverage = []\n    for i in range(n_classes):\n        coverage.append(np.mean(prediction_sets[y_test == i, i]))\n    return coverage\n\n# overall coverage (weighted by class size)\n# Get class counts\ndef get_class_counts(y_test):\n    class_counts = []\n    for i in range(n_classes):\n        class_counts.append(np.sum(y_test == i))\n    return class_counts\n\ndef get_weighted_coverage(coverage, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_coverage = np.sum((coverage * class_counts) / total_counts)\n    weighted_coverage = round(weighted_coverage, 3)\n    return weighted_coverage\n\n\n\n\nVisualization\n\nConfusion Matrix\n\n\nBinary target where labels are 0 and 1\nInterpretation\n\nTop-left: predictions where both labels are not statistically significant (i.e. inside the “prediction interval”).\n\nThe model predicts both classes well since both labels have low scores.\nDepending the threshold, maybe the model could be relatively agnostic (e.g. predicted probabilites like 0.50-0.50, 0.60-0.40)\n\nBottom-right: predictions where both labels are statistically significant  (i.e. outside the “prediction interval”).\n\nModel totally whiffs. Confident it’s one label when it’s actually another.\n\nExample\n\n1 (truth) - low predicted probability = high score -&gt; Red and significant\n0 - high predicted probability = high score -&gt; Red and significant\n\n\n\nTop-right: predictions where all 0 labels are not statistically significant.\n\nModel predicted the 0=class well (i.e. low scores) but the 1-class poorly (i.e. high scores)\n\nBottom-left: predictions where all 1 labels are not statistically significant. Here, the model predicted that 1 is the true class.\n\nVice versa of top-right",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/big-data.html",
    "href": "qmd/big-data.html",
    "title": "Big Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-misc",
    "href": "qmd/big-data.html#sec-bgdat-misc",
    "title": "Big Data",
    "section": "",
    "text": "RcppArmadillo::fastLmPure Not sure what this does but it’s rcpp so maybe faster than lm for big data.\n.lm.fit is a base R lm function that is 30%-40% faster than lm.",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-hghperf",
    "href": "qmd/big-data.html#sec-bgdat-hghperf",
    "title": "Big Data",
    "section": "High Performance",
    "text": "High Performance\n\n{rpolars}: arrow product; uses SIMD which is a low-level vectorization that can be used to speed up simple operations like addition, subtraction, division, and multiplication\n\nResources\n\nCookbook Polars for R\n\nAlso see collapse &gt;&gt; vs arrow/polars\nExample: Read and Summarize\ndf &lt;- pl$scan_csv(file_name)$\n    group_by(\"state\")$\n    agg(\n        pl$\n          col(\"measurement\")$\n          min()$\n          alias(\"min_m\"),\n        pl$\n          col(\"measurement\")$\n          max()$\n          alias(\"max_m\"),\n        pl$\n          col(\"measurement\")$\n          mean()$\n          alias(\"mean_m\")\n    )$\n    collect()\n\nFastest at this operation according to this benchmark\n\nExample: groupby state + min, max, mean\n# polars sql\nlf &lt;- polars::pl$LazyFrame(D) \npolars::pl$SQLContext(frame = lf)$execute(\n  \"select min(measurement) as min_m, \n          max(measurement) as max_m, \n          avg(measurement) as mean_m \n  from frame \n  group by state\")$collect()\n\n# polars\npolars::pl$\n  DataFrame(D)$\n  group_by(\"state\")$\n  agg(polars::pl$\n        col(\"measurement\")$\n        min()$alias(\"min_m\"),\n      polars::pl$\n        col(\"measurement\")$\n        max()$alias(\"max_m\"),\n      polars::pl$\n        col(\"measurement\")$\n        mean()$alias(\"mean_m\"))\n\n{collapse}: Fast grouped & weighted statistical computations, time series and panel data transformations, list-processing, data manipulation functions, summary statistics and various utilities such as support for variable labels. Class-agnostic framework designed to work with vectors, matrices, data frames, lists and related classes i.e. xts, data.table, tibble, pdata.frame, sf.\n\noptions(collapse_mask = \"all\")\nlibrary(collapse)\n\nCode chunk above can optimize any script. No other changes necessary. Quick demo.\nvs arrow/polars (benchmark)\n\nDepends on the data/groups ratio\n\nIf you have “many groups and little data in each group” then use collapse\n\nIf your calculations involve “more complex statistics algorithms like the median (involving selection) or mode or distinct value count (involving hashing)(cannot, to my knowledge, benefit from SIMD)” then use collapse.\n\nExample: groupby state + min, max, mean\nD |&gt;\n  fgroup_by(state) |&gt; \n  fsummarise(min = fmin(measurement), \n             max = fmax(measurement), \n             mean = fmean(measurement)) |&gt;\n  fungroup()\n\n{r2c}: Fast grouped statistical computation; currently limited to a few functions, sometimes faster than {collapse}\n{data.table}: Enhanced data frame class with concise data manipulation framework offering powerful aggregation, extremely flexible split-apply-combine computing, reshaping, joins, rolling statistics, set operations on tables, fast csv read/write, and various utilities such as transposition of data.\n\nExample: groupby state + min, max, mean\nD[ ,.(mean = mean(measurement),\n      min = min(measurement),\n      max = max(measurement)),\n   by=state]\n\n# Supposedly faster\nrbindlist(lapply(unique(D$state), \n                 \\(x) data.table(state = x, \n                                 y[state == x, \n                                   .(mean(measurement), \n                                     min(measurement), \n                                     max(measurement))\n                                   ]\n                                 )))\n\n{rfast}: A collection of fast (utility) functions for data analysis. Column- and row- wise means, medians, variances, minimums, maximums, many t, F and G-square tests, many regressions (normal, logistic, Poisson), are some of the many fast functions\n\nThe vast majority of the functions accept matrices only, not data.frames.\nDo not have matrices or vectors with have missing data (i.e NAs). There are no checks and C++ internally transforms them into zeros (0), so you may get wrong results.\nExample: groupby state + min, max, mean\nlev_int &lt;- as.numeric(D$state)\nminmax &lt;- Rfast::group(D$measurement, lev_int, method = \"min.max\")\ndata.frame(\n    state = levels(D$state),\n    mean = Rfast::group(D$measurement, lev_int, method = \"mean\"),\n    min = minmax[1, ],\n    max = minmax[2, ]\n)\n\n{matrixStats}: Efficient row-and column-wise (weighted) statistics on matrices and vectors, including computations on subsets of rows and columns.\n{kit}: Fast vectorized and nested switches, some parallel (row-wise) statistics, and some utilities such as efficient partial sorting and unique values.\n{fst}: A compressed data file format that is very fast to read and write. Full random access in both rows and columns allows reading subsets from a ‘.fst’ file.",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-lgmem",
    "href": "qmd/big-data.html#sec-bgdat-lgmem",
    "title": "Big Data",
    "section": "Larger than Memory",
    "text": "Larger than Memory\n\nOnly work with a sample of the data\n\nRandom sample in CLI\n\nSee binder for code\nAlso this snippet from Healy for a zipped csv.\n\n\nImproved version\ngzip -cd giantfile.csv.gz | (read HEADER; echo $HEADER; perl -ne 'print if (rand() &lt; 0.001)’) &gt; sample.csv\n\nRemoves the need to decompress the file twice, adds the header row, and removes the risk of a double header row\n\n\n\nOnly read the first n lines\n\nset n_max arg in readr::read_*\n\n\ndatasette.io - App for exploring and publishing data. It helps people take data of any shape, analyze and explore it, and publish it as an interactive website and accompanying API.\n\nWell documented, many plugins\n\nRill - A tool for effortlessly transforming data sets into powerful, opinionated dashboards using SQL.\n\nDocs, Example Projects\nPowered by Sveltekit & DuckDB = conversation-fast, not wait-ten-seconds-for-result-set fast\nWorks with your local and remote datasets – imports and exports Parquet and CSV (s3, gcs, https, local)\nNo more data analysis “side-quests” – helps you build intuition about your dataset through automatic profiling\nNo “run query” button required – responds to each keystroke by re-profiling the resulting dataset\nRadically simple interactive dashboards – thoughtful, opinionated, interactive dashboard defaults to help you quickly derive insights from your data\nDashboards as code – each step from data to dashboard has versioning, Git sharing, and easy project rehydration\n\nOnline duckdb shell for parquet files (gist, https://shell.duckdb.org/)\nselect max(wind) \nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/gridwatch/gridwatch_2023-01-08.parquet';\n-- Takes 6 seconds on the first query, 200ms on subsequent similar queries\n\nselect * \nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/NSPL/NSPL.parquet' \nwhere pcd = 'SW1A1AA';\n-- Takes 13 seconds on the first query, 100ms on subsequent similar queries\nCSV Editors\n\nFor editing or reformatting cells\nPopular spreadsheet programs like googlesheets (100MB) and excel (25MB online) have file size limits and they’re slow to upload to. The following programs are free(-ish) local alternatives only limited by your RAM.\nSuggest for files over a few hundred MBs that you open as Read-Only\n\nOpening the files as “Editable” will probably balloon the memory cost to at least 5 times the file size. (e.g. 350MB csv \\(\\rightarrow\\) 2GB RAM)\n\nModern CSV - Nice modern interface, read-only mode that can open large csvs (100s of MBs) without making much of a dent in your RAM, fully featured (moreso if you pay a small-ish one time fee)\n\nDocs, Feature free/upgrade list\nStill has some functionality in read-only mode (e.g. search, sort)\n\nOpenRefine - Has read-only, Several add-ons, Completely open source.\n\nDocs, List of Extensions\nNo functionality when read-only (must create a project to do anything) — just reading\nStarts with a 1024 MB RAM usage limit which is proably fine for editing around a 100MB csv. Need to set the limit higher in a config file in order to edit larger files.\nOnce you create a project, I think it has some editing features that you’d have to pay for with Modern CV.\nOpens other file formats besides csv (e.g. xlsx, xml, json, etc)\n\n\ncsvkit - suite of command-line tools for converting to and working with CSV\n\nInstallation docs\n\nOne of the articles your terminal has to be a bash terminal but I dunno\n\nIf so, they recommend cmder or enabling the Linux subsystem with WSL2.\n\n\nNotes from\n\nArticle with additional examples and options\n\nFeatures\n\nPrint CSV files out nicely formatted\nCut out specific columns\nGet statistical information about columns\n\nConvert excel files to CSV files:\nin2csv excel_file.xlsx &gt; new_file.csv\n# +remove .xlsx file\nin2csv excel_file.xlsx &gt; new_file.csv && rm excel_file\nSearch within columns with regular expressions:\ncsvgrep -c county -m \"HOLT\" new_file.csv\n# subset of columns (might be faster) with pretty formatting\ncsvcut -c county,total_cost new_file.csv | csvgrep -c county -m \"HOLT\" | csvlook\n\nSearches for “HOLT” in the “county” column\n\nQuery with SQL\n\nsyntax csvsql --query \"ENTER YOUR SQL QUERY HERE\" FILE_NAME.csv\nExample\n\n\nView top lines: head new_file.csv\nView columns names: csvcut -n new_file.csv\nSelect specific columns: csvcut -c county,total_cost,ship_date new_file.csv\n\nWith pretty output: csvcut -c county,total_cost,ship_date new_file.csv | csvlook\nCan also use column indexes instead of names\n\nJoin 2 files: csvjoin -c cf data1.csv data2.csv &gt; joined.csv\n\n“cf” is the common column between the 2 files\n\nEDA-type stats:\ncsvstat new_file.csv\n# subset of columns\ncsvcut -c total_cost,ship_date new_file.csv | csvstat\n\nJSONata - a lightweight, open-source query and transformation language for JSON data, inspired by the ‘location path’ semantics of XPath 3.1.\n\nMisc\n\nNotes from: Hrbrmstr’s article\nJSONata also doesn’t throw errors for non-existing data in the input document. If during the navigation of the location path, a field is not found, then the expression returns nothing.\n\nThis can be beneficial in certain scenarios where the structure of the input JSON can vary and doesn’t always contain the same fields.\n\nTreats single values and arrays containing a single value as equivalent\nBoth JSONata and jq can work in the browser (JSONata embedding code, demo), but jq has a slight speed edge thanks to WASM. However, said edge comes at the cost of a slow-first-start\n\nFeatures\n\nDeclarative syntax that is pretty easy to read and write, which allows us to focus on the desired output rather than the procedural steps required to achieve it\nBuilt-in operators and functions for manipulating and combining data, making it easier to perform complex transformations without writing custom code in a traditional programming language like python or javascript\nUser-defined functions that let us extend JSONata’s capabilities and tailor it to our specific needs\nFlexible output structure that lets us format query results into pretty much any output type\n\n\njq + jsonlite - json files\njsoncrack.com - online editor/tool to visualize nested json (or regular json)\njj - cli tool for nested json. Full support for ndjson as well as setting/updating/deleting values. Plus it lets you perform similar pretty/ugly printing that jq does.\nsqlite3 - CLI utility allows the user to manually enter and execute SQL statements against an SQLite database or against a ZIP archive.\n\nalso directly against csv files (post)\n\ntextql - Execute SQL against structured text like CSV or TSV\n\nRequire Go language installed\nOnly for Macs or running a docker image\n\ncolumnq-cli - sql query json, csv, parquet, arrow, and more\nfread + CLI tools\n\nFor large csvs and fixing large csv with jacked-up formating see article, RBlogger version\n\n{arrow}\n\nconvert file into parquet files\n\npass the file path to open_dataset, use group_by to partition the Dataset into manageable chunks\nuse write_datasetto write each chunk to a separate Parquet file—all without needing to read the full CSV file into R\n\ndplyr support\n\nmultiplyr\n\nOption for data &gt; 10M rows and you only have access to one machine\nSpreads data over local cores\n\n{sparklyr}\n\nspin up a spark cluster\ndplyr support\nSet-up a cloud bucket and load data into it. Then, read into a local spark cluster. Process data.\n\n{h2o}\n\nh2o.import_file(path=path) holds data in the h2o cluster and not in memory\n\n{disk.frame}\n\nsupports many dplyr verbs\nsupports  future package to take advantage of multi-core CPUs but single machine focused\nstate-of-the-art data storage techniques such as fast data compression, and random access to rows and columns provided by the fst package to provide superior data manipulation speeds\n\nMatrix ops\n\nsee bkmks: mathematics &gt;&gt; packages\n\n{ff}\n\nsee bkmks: data &gt;&gt; loading/saving/memory\nThink it converts files to a ff file type, then you load them and use ffapply to perform row and column operations with base R functions and expressions\nmay not handle character and factor types but may work with {bit} pkg to solve this",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-viz",
    "href": "qmd/big-data.html#sec-bgdat-viz",
    "title": "Big Data",
    "section": "Viz",
    "text": "Viz\n\nScatter plots\n\n{scattermore}, {ggpointdensity}\n{ggrastr}\n\nRasterize only specific layers of a ggplot2 plot (for instance, large scatter plots with many points) while keeping all labels and text in vector format. This allows users to keep plots within a reasonable size limit without losing the vector properties of scale-sensitive information.\ngithub; tweet\n\n\nH2O\n\nh2o.aggregator Reduces data size to a representive sample, then you can visualize a clustering-based method for reducing a numerical/categorical dataset into a dataset with fewer rows A count column is added to show how many rows is represented by the exemplar row (I think)\n\nAggregator maintains outliers as outliers but lumps together dense clusters into exemplars with an attached count column showing the member points.\nFor cat vars:\n\nAccumulate the category frequencies.\nFor the top 1,000 or fewer categories (by frequency), generate dummy variables (called one-hot encoding by ML people, called dummy coding by statisticians).\nCalculate the first eigenvector of the covariance matrix of these dummy variables.\nReplace the row values on the categorical column with the value from the eigenvector corresponding to the dummy values.\n\ndocs; article\n\n\n{dbplot}\n\nplots data that are in databases\n\nAlso able to plot data within a spark cluster\n\ndocs\n\nObservableHQ\n\n{{{deepscatter}}}\n\nThread (using Arrow, duckdb)",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/bayes-workflow.html",
    "href": "qmd/bayes-workflow.html",
    "title": "Workflow",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Bayes",
      "Workflow"
    ]
  },
  {
    "objectID": "qmd/bayes-workflow.html#sec-bayes-wkflw-misc",
    "href": "qmd/bayes-workflow.html#sec-bayes-wkflw-misc",
    "title": "Workflow",
    "section": "",
    "text": "Also see Model Building, Concepts &gt;&gt; Misc &gt;&gt; Regression Workflow\nNotes from\n\nBayesian Workflow (Gelman, Vehtari) (arXiv link)\n\nResources\n\nVehtari Video: On Bayesian Workflow (2022) (Based on the paper, but I haven’t watched it, yet)\nNabiximols treatment efficiency: Vehtari’s example of applyijng his workflow in the context of comparing continuous and discrete observation models.\n\nCurrent Checklist\n\nCheck convergence diagnostics\nDo posterior predictive checking\nCheck residual plots\nModel comparison (if prediction)\n\nAnalysis Checklist (Thread)\n\nA suitably flexible Bayesian regression adjustment model,\nChosen by cross-validation/LOO,\nIncluding Gaussian processes for the unit-level effects over time (and space/network if relevant),\nImputation of missing data, and\nInformative priors for biases in the data collection process.",
    "crumbs": [
      "Bayes",
      "Workflow"
    ]
  },
  {
    "objectID": "qmd/glossary-ds-terms.html",
    "href": "qmd/glossary-ds-terms.html",
    "title": "Glossary: DS terms",
    "section": "",
    "text": "200 Status - An API serving an ML model returns a HTTP 200 OK success status response code indicates that the request has succeeded.\nAMI - amazon machine image. Thing that has R and the main packages you need to load onto the cloud server\nAnti-Patterns - certain patterns in software development that are considered bad programming practices.\n\nAs opposed to design patterns which are common approaches to common problems which have been formalized and are generally considered a good development practice, anti-patterns are the opposite and are undesirable.\n\nArm - a group of patients receiving a specific treatment (or no treatment). Trials involving several arms, or randomized trials, treat randomly-selected groups of patients with different therapies in order to compare their medical outcomes. Experimental arms, which receive an experimental drug, are compared with control arms. Single-arm or non-randomized trials, in which everyone enrolled in a trial receives the experimental therapy\nArtifacts - objects that are created as a result of a process. e.g. model objects, cleaned data sets, visuals, etc.\nAsynchronous Programming - code runs (or must run) after something else happens and also not sequentially (e.g. when a function calls a callback function in JS).\nAthena - amazon query service that works with S3. Best for analyses using kubernetes. ODBC drivers are best with interactive app\nB2C, B2B - business-to-consumer, business-to-business, describes a business that’s end-product is being sold to a consumer or a business.\nBalanced Design (aka orthogonal) has an equal number of observations for all possible level combinations. For example in an experiment where gender is an independent variable, an equal number of males receive the treatment as do females receive treatment. If the male/female counts were unequal, then the experiment is unbalanced.\n\nStat tests have greater power for balanced designs\nTest stat less susceptible to to small departures from the assumption of equal variances (homoscedasticity).\n\nBatch - collect a large number of data points, process them periodically and store results somewhere (contrasts with real-time in which a data input leads to an immediate prediction)\nBootstrapping (CS) - usually applies to a situation where a system depends on itself to start, sort of a chicken and egg problem. (e.g. How do you start an OS initialization process if you don’t have the OS running yet?) Typically a simple file that starts a large process.\nBounce, Email - When an email cannot be delivered to an email server.\n\nHard Bounce - indicates a permanent reason an email cannot be delivered (e.g. Recipient email address doesn’t exist; Recipient email server has completely blocked delivery)\nSoft Bounce - indicates a temporary delivery issue (for details on the reasons, see link)\n\nBounce Rate - the percentage of visitors to a particular website who navigate away from the site after viewing only one page. Low bounce rate can indicate the landing page needs improvement\nBPI - Business process improvement is a management exercise in which enterprise leaders use various methodologies to analyze their procedures to identify areas where they can improve accuracy, effectiveness and/or efficiency and then redesign those processes to realize the improvements.\nBLUE - best linear unbiased estimator, e.g. regression line\nCAC - customer acquisition cost - measures how much an organization spends to acquire new customers. The total cost of sales and marketing efforts, as well as property or equipment, needed to convince a customer to buy a product or service.\nCapEx - Capital Expenditure - 1 of 2 main forward budgeting mechanisms for a corporation (also see OpEx). Often used to undertake new projects or investments or large-scale asset acquisitions (buildings and vehicles)\nClinical Trial - research studies (e.g. RCT) performed in people that are aimed at evaluating a medical, surgical, or behavioral intervention\nCDI - Customer Data Infrastructure - built to collect behavioral data from primary or first-party data sources, but some solutions also support a handful of secondary data sources (third-party tools)\nCDP - Customer Data Platform - add-ons from CDI vendors; a layer on top of CDI that offers a set of capabilities to analyze data using a visual interface.\nCDN - content delivery network - a system of distributed servers (network) that deliver pages and other web content to a user, based on the geographic locations of the user, the origin of the webpage and the content delivery server.\nCLV/CLTV - Customer Lifetime Value - how much money a customer will bring your brand throughout their entire time as a paying customer.\nCOGS - Cost of goods sold (aka Cost of Sales) - refers to the direct costs of producing the goods sold by a company. This amount includes the cost of the materials and labor directly used to create the good. It excludes indirect expenses, such as distribution costs and sales force costs.\nComplete Factorial Design - a research study involving two or more independent variables in which every possible combination of the levels of each variable is represented. For instance, in a study of two drug treatments, one (A) having two dosages and the other (B) having three dosages, a complete factorial design would pair the dosages administered to different individuals or groups of participants as follows: A1 with B1, A1 with B2, A1 with B3, A2 with B1, A2 with B2, and A2 with B3.\nCPG - Consumer packaged goods are items used daily by average consumers that require routine replacement or replenishment, such as food, beverages, clothes, tobacco, makeup, and household products.\nCPC - Cost Per Click - refers to the cost an advertiser pays each time an online user clicks on his or her digital ad\nCRM - customer relationship management i.e. customer service. Salesforce tracks this data. Example: what features your salesperson promised, and when? How much revenue you have from each customer? Or which salesperson sold the most in the past year?\ncron- standard tool used on Unix and Unix-like systems to schedule the periodic execution in the background of a command or script (like a batch script)\nCrossed Factors - when every category of one factor co-occurs in the design with every category of the other factor. In other words, there is at least one observation in every combination of categories for the two factors. (in contrast to “nested factors”). As a consequence, interaction terms involving these two factors is allowed.\nCrossover Study - A type of clinical trial in which the study participants receive each treatment in a random order. With this type of study, every patient serves as his or her own control. Crossover studies are often used when researchers feel it would be difficult to recruit participants willing to risk going without a promising new treatment.\nCross-Section Data - randomly sampled data from a population. Like a survey. Aka observational data. See experimental data for comparison.\n\nPooled - differs from panel data in that it is observations of different subjects (instead of the same subjects) in different time periods.\nRolling - both the presence of an individual in the sample and the time at which the individual is included in the sample are determined randomly.\n\nCross-Tabs - section of survey analysis where the aggregated results are broken down by demography, party affiliation, etc.\nCTA - marketing term, call-to-action. any device designed to prompt an immediate response or encourage an immediate sale; words or phrases that can be incorporated into sales scripts, advertising messages or web pages that encourage consumers to take prompt action\nCTR - click through rate: the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement. It is commonly used to measure the success of an online advertising campaign for a particular website as well as the effectiveness of email campaigns.\nCRM - Customer Relationship Management - acquiring new customers but especially about retaining existing ones\nDAU - daily active users, ex: daily avg # of registered users of the site over past 30 days\nDBA - Database Administrator is an admin role that understands the particular database technology and how to get the best out of it. This includes improving performance, backups and recovery.\nDDL - Data definition or description language - Subset of SQL. Used to:\n\nKeep a snapshot of the database structure\nSet up a test system where the database acts like the production system but contains no data\nProduce templates for new objects that you can create based on existing ones. For example, generate the DDL for the Customer table, then edit the DDL to create the table Customer_New with the same schema.\n\nDesparate Impact Analysis - Analysis of the result of the application of a standard, requirement, test or other screening tool used for selection that—though appearing neutral—has an adverse effect on individuals who belong to a legally protected class Differential Dropout**]{style=‘color: #009499’} - Differing dropout rates between treatment arms\nDMA - Designated Market Area; a geographic region where Nielsen, the ratings company, analyzes and quantifies how television is viewed. Residents can receive the same local TV and radio stations\nDNS -  Domain Name System**]{style=‘color: #009499’} -  translates domain names to IP addresses so browsers can load Internet resources.\nDSL - domain-specific language - a computer language specialized to a particular application domain\nEMR - Amazon version of a spark cluster used for big data processing and analysis.\nEndogenous - A model variable is correlated with other variables excluded from the model (omitted variable bias). Determined by measuring the correlation between the variable and residuals of the model. If a predictor variable hasn’t been randomly assigned, it’s likely to be endogenous.\nEquitability - concept that says a dependence measure should give equal importance to linear and nonlinear relationships. Consistent strength measurements across different variable relationships that have similar amounts of noise.\nERP - enterprise resource planning, sort of a catch-all for manufacturing, supply-chain, etc, see the wiki\nETL - extract, transfer, load - usually refers to transferring data from one location to another\nEndpoint (biostats) - Outcome variable measured in a medical study. e.g. Death, stroke, or quality of life are good endpoints. Blood tests and images on scans are not good endpoints.\n\nA composite endpoint is one that consists of two or more events\n\nExample: death due to cardiovascular causes or hospitalization due to heart failure\n\nSo the binary outcome would be a 1 if either of those events took place or a 0 if they did not. Or in a survival model, time until either of those events.\n\n\n\nEOF - End of file - Input from a terminal never really “ends” (unless the device is disconnected), but it is useful to enter more than one “file” into a terminal, so a key sequence is reserved to indicate end of input.\nex ante - based on assumption and prediction and being essentially subjective and estimative\nex post - based on knowledge and retrospection and being essentially objective and factual\nExperimental Data - data from a RCE/RCT. Compare with observational data\nFaaS - Function as a service - type of cloud service for developing, running, and managing apps (e.g. AWS Lambda)\nFactorial Design - Experiment where you’re interested in the effect of two or more independent variables.\nFraud Rules - fraud scores are calculated based on rules, which add or subtract points. The user action may be a transaction, signup or login. Rules look at data points such as an email address, IP address, or social media presence.\nFraud Score - assigned values to how risky a user action is. Scoring determined by fraud rules.\nFuzzy Design - See Sharp Design\nGHA - Github Actions\nGMV - Gross merchandises value - the total value of merchandise sold over a given period of time through a customer-to-customer (C2C) exchange site\nGRP - Gross Rating Point. A standard measure in advertising, it measures advertising impact. You calculate it as a percent of the target market reached multiplied by the exposure frequency. Thus, if you get advertise to 30% of the target market and give them 4 exposures, you would have 120 GRP.\nHTE - Heterogeneous Treatment Effect - Also called differential treatment effect, includes difference of means, odds ratios, and Hazard ratios for time-to-event outcome vars\n\nAscertaining subpopulations for which a treatment is most beneficial (or harmful) is an important goal of many clinical trials.\nOutcome heterogeneity is due to wide distributions of baseline prognostic factors. When strong risk factors exist, there is hetergeneity in the outcome variable.\n\nSolution: add baseline predictors to your model that account for these strong risk factors.\n\nHeterogeneity of Treatment Effects - The degree to which different treatments have differential causal effects on each unit.\n\nHit Ratio - percent of records that were read in order to complete a query in a database. Cloud db providers often charge by the number of records searched\nHomogeneity of Treatment Effects - See Heterogeneity of Treatment Effects\nHPC - High Performance Computing\nHoneypot - data (for example, in a network site) that appears to be a legitimate part of the site, but is actually isolated and monitored, and that seems to contain information or a resource of value to attackers, who are then blocked.\nIaaS - infrastructure-as-a-service ( Hardware is provided by an external provider and managed for you)\nIAM - identity and access management, keys and passwords etc\nIRB - institutional review board, reviews studies ethical and moral issues\nITT - Intent-to-Treat analysis includes all randomized patients in the groups to which they were randomly assigned, regardless of their adherence with the entry criteria, regardless of the treatment they actually received, and regardless of subsequent withdrawal from treatment or deviation from the protocol. Avoids overoptimistic estimates of the efficacy of an intervention resulting from the removal of non-compliers by accepting that noncompliance and protocol deviations are likely to occur in actual clinical practice. So mimics likely situation in the real world, but not good for estimating the causal effect of a treatment.\nKernels - (article) - system kernels - the interface between the operating system, i.e. the software, and the hardware components in a device. It is used in all devices with an operating system, for example, computers, laptops, smartphones, smartwatches, etc.\n\nWhen we use a program on a computer, such as Excel, we handle it on the so-called Graphical User Interface (GUI). The program converts every button click or other action into machine code and sends it to the operating system kernel. If we want to add a new column in an Excel table, this call goes to the system core. This in turn passes the call on to the computer processing unit (CPU), which executes the action.\nJupyter Kernels - an engine that executes notebook code and is specific to a particular programming language (e.g. python kernel)\nKaggle Kernels - a free platform from Kaggle to run Jupyter notebooks in the browser. Advantage is that you don’t have to set-up an environment locally.\n\nKPI- key performance indicator\nKYC - Know-Your-Customer is info a company collects to verify your identity to combat fraud. Used by telecoms and financial services\nLazy Evaluation - ” never pulls data into R unless you explicitly ask for it. It delays doing any work until the last possible moment. It collects together everything you want to do and then sends it to the database in one step.”\nLikelihood - probability of seeing this data given a specific value for a distribution parameter (eg mean, sd). Goal is to search for parameter values until the likelihood is maximized.\nLOB - Line of Business is a general term which refers to a product or a set of related products that serve a particular customer transaction or business need. (i.e. product categories)\n\nExamples\n\nConsumer Banking: credit cards, line of credit or loan program, mortgages, and corporate, small business and personal bank accounts.\nFinancial services and brokerages: mergers and acquisitions or partnerships, real estate investments, and wealth management\nProperty and casualty insurance companies: property and casualty insurance (i.e., homeowners, car, boat, renters, etc.), life insurance, health insurance, and commercial business insurance.\n\nSub-lines of Business would be sub-categories within each LOB\n\nLongitudinal Data - see panel data\nLTV - see CLV/CLTV\nManual Review - A human is reviews the case to determine whether action is needed. In fraud, an model output may trigger a “manual review” to determine whether an event was indeed fraudulent.\nMLlib - Apache Spark machine learning library\nMVC - Minimum Viable Corpus - a data size threshold; such that below this threshold, the data simply isn’t useful/valuable. Used in data products business.\nMVP - minimum viable project, agile term. Version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort\nNamespace - allows you to use two functions with the same name but from different packages, e.g. dplyr::select or in general, package::function. https://stackoverflow.com/questions/3384204/what-are-namespaces/3384384#3384384\nNNH - Numbers Needed to Harm - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a particular adverse outcome. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNNT - Numbers Needed to Treat - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a favorable outcome such as treatment response. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNPS - Net Promoter Score - a measure of customer loyalty. Widely used market research metric that typically takes the form of a single survey question asking respondents to rate the likelihood that they would recommend a company, product, or a service to a friend or colleague.\nNRT - near real-time, aka streaming data\nObservational Data - see cross sectional data\nOEM - original equipment manufacturer\nOKR - Objectives and Key Results is a popular management strategy for goal setting within organizations. A framework for turning strategic intent into measurable outcomes for an organization.\nOnline Machine Learning - A method of machine learning where the model incrementally learns from a stream of data points in real-time. It’s a dynamic process that adapts its predictive algorithm over time, allowing the model to change as new data arrives.\nOn-Prem - on-premises — working with servers in the the building and not in the cloud.\nOOD - out-of-distribution - data which differ from the training data and on which a model might underperform\nOpen Cohort - subjects can leave or be added over time.\nOpEx - Operational Expenditures - 1 of 2 main forward budgeting mechanisms for a corporation (also see CapEx). Relates to day-to-day expenses (such as payroll and software subscriptions). Smaller payouts over time.\nOpportunity Sizing - Quantitative analysis to select a subset of ideas to which to devote resources in product development\nNested Factors - happens when all the levels of one factor only occur in combination with one level of another factor (in contrast to “crossed factors”). As a consequence, your model can’t have an interaction term involving these two variables.\nP&L - Profit and Loss Statement Panel data - cross section data with a time element. Repeated measures of the same subject over time. Synonym for Longitudinal Data\nParcel - a land record that defines the boundary of a piece of land. These boundaries are the basic administrative unit of local government in regards to land and property. Managing ownership and tax records are the primary reason local governments generate these files. So these are boundaries differentiating ownership of properties.\nPEP8 - style guide for python\nPI - principal investigator\nPivot Table - Excel name for a group_by %\\&gt;% summarize calculation\n\ne.g. from a table of individual fruit sales: group_by(fruit_type, country) %\\&gt;% summarize(total_amt = sum(amount))\n\nPLG - Product-led growth is an end user-focused growth model that relies on the product itself as the primary driver of customer acquisition, conversion, and expansion. e.g. open source a product, let the customer go through the documentation and use and experiment with the product on their own time. In contrast to sales pitching a product to a customer and letting them use it for a trial basis.\nPM - product manager\nPoC - Proof of Concept\nPOS - point of sale, The point of sale or point of purchase is the time and place where a retail transaction is completed. It can be in a physical store, where POS terminals and systems are used to process card payments or a virtual sales point such as a computer or mobile electronic device.\nRCE - randomized controlled experiment, subjects randomly assigned to two groups, treatment and control. Double blind means the researcher doesn’t know who is in which group.\nRCT - randomized clinical trial\nRDD - Regression discontinuity design\nRedis - REmote DIctionary Server - is an in-memory, key-value database, commonly referred to as a data structure server. Used when volume of read and write operations exceed the capabilities of traditional databases. With Redis’s capability to easily persist the data to disk, it is a superior alternative to the traditional memcached solution for caching.\nRefactoring - updating or optimizing code\nRegression Testing - checks if changes made to a system negatively impacted or broke any of the existing features. It is often performed right after each update or commit to the code base to identify new bugs and ensure that your system works properly.\nRFI - Request for Information - Used to collect written information about the capabilities of various suppliers. Normally it follows a format that can be used for comparative purposes. An RFI is primarily used to gather information to help make a decision on what steps to take next. RFIs are therefore seldom the final stage and are instead often used in combination with request for proposal (RFP), request for tender (RFT), and request for quotation (RFQ).\nRFM - recency, frequency, monetary value - method of estimating customer value; common in retail\nRFP - Request for Proposal - A document that an organization, often a government agency or large enterprise, posts to elicit a response – a formal bid – from potential vendors for a desired solution. The RFP specifies what the customer is looking for and describes each evaluation criterion on which a vendor’s proposal will be assessed.\n\nROAS - return on ad spend\nRUG - Regional User Group\nS3 - Amazon simple storage service, database\nSaaS - Software-as-a-service is a mechanism through which companies offer the functionality of their apps, which remain on their company servers, to other companies or customers.\nSCO - sales cycle optimization, active process of providing content on your site (and beyond) that speaks to each of the key phases\nSEO - Search engine optimization, generating high page rankings for key search terms\nSDK - software development kit\nSharp Design - Each individual or group receives the same “amount” of treatment (e.g. a state law or medication dosage). Opposite being fuzzy design (?)\nSKU - Stock Keeping Unit**]{style=‘color: #009499’} - Usually a bar code that has all the information to distinguish it from another product. These attributes can include manufacturer, description, material, size, color, packaging, and warranty terms. When a business takes inventory of its stock, it counts the quantity it has of each SKU.\nSLA - service level agreement - a contract between a service provider and its internal or external customers that documents what services the provider will furnish and defines the service standards this provider is obligated to meet. service. Important for holding prediction latency of an app to a certain standard or maintaining data reliability with vendors. (see link for more details on SLA, SLO, and SLI)\nSLI - service level indicators - metrics that measure compliance with an SLO (see link for more details on SLA, SLO, and SLI)\nSLO - service level objectives - objectives your team must meet in order to meet the conditions of the SLA (see link for more details on SLA, SLO, and SLI)\nSMB - (small to medium-sized business) generally defined as companies with fewer than 1000 employees and less than $1 billion in annual revenue.\nSME - Subject Matter Experts\nSPC - Statistical process control is a method of quality control which employs statistical methods to monitor and control a process\nSpill - missed opportunity metric, measures “lost trading days” on which flights or hotels filled too quickly (the result of pricing too low)\nSpoil - missed opportunity metric, measures empty seats or rooms (often the result of pricing too high)\nSSH - secure shell is a cryptographic Network protocol for operating Network Services securely over an unsecured Network. Typical applications include remote command line login in remote command execution\nstdout - standard output, which is the terminal by default\nTDD - Test-driven development is a style of programming where coding, testing, and design are tightly interwoven\nTF-IDF- stands for term frequency-inverse document frequency, and is often used in information retrieval and text mining.\nThroughput - the amount of material or items passing through a system or process.\ntx - treatment, seen as variable with different treatments as values\nURI - Uniform Resource Identifier - a string of characters that unambiguously identifies a particular resource. e.g. s3//bucket/path/to/folder or http://127.0.0.1:5000or c:\\Users\\me\\path\\to\\folder\nUTM - Urchin Traffic Monitor - used to identify marketing channels\n\ne.g. http://yourwebsite.com/your-post-title/?utm_source=google\n\nutm code = string after “?”\n\nThis person clicked a google ad to get to your site\n\nName comes from Urchin Tracker, a web analytics software that served as the base for Google Analytics.\n\nVPS - virtual private server\nWIP - Work-in-Progress\nWithin Person Study - multiple treatments on each person either all in the same period or different treatments in different periods\nYear-Over-Year - used to make comparisons between one time period and another that is one year earlier.\n\nFormula (percentage): (value_this_year / value_previous_year) - 1\nExample: (sales_Jul_2023 / sales_Jul_2022) - 1",
    "crumbs": [
      "Glossary: DS terms"
    ]
  },
  {
    "objectID": "qmd/python-general.html",
    "href": "qmd/python-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-misc",
    "href": "qmd/python-general.html#sec-py-gen-misc",
    "title": "General",
    "section": "",
    "text": "Tools (see article, article for installation and usage)\n\nruff - rust-based, linter and sorts imports\n\nFast, sensible default settings, focuses on more important things out of the box, and has less legacy burden\n\npydocstring - tool for checking compliance with Python docstring conventions\nblack - code formatter\nisort - sorts your imports\npytest, pytest-watch - unit tests\ncommitizen - guides you through a series of steps to create a commit message that conforms to the structure of a Conventional Commit\nnbQA - linting in jupyter notebooks\nmypy - type checker; good support and docs\npylance - checks type hinting in VSCode (see Functions &gt;&gt; Documentation &gt;&gt; Type Hinting)\ndoit - task runner; {targets}-like tool; tutorial\npre-commit - specify which checks you want to run against your code before committing changes to your git repository\nREADME templates - link\n\nPut as much config as possible into pyproject.toml. A lot of configurations tools will happily read from it, and it will give you one source of truth.\nAn underscore _ at the beginning is used to denote private variables in Python.\ndef set_temperature(self, value):\n        if value &lt; -273.15:\n            raise ValueError(\"Temperature below -273.15 is not possible.\")\n        self._temperature = value\n\nyou can still access “_temperature” but it’s just meant for internal use by the class and the underscore indicates this\n\n{{warnings::warnings.filterwarnings(‘ignore’)}}\nsys.getsizeof(obj) to get the size of an object in memory.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#terms",
    "href": "qmd/python-general.html#terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nclasses - code template for creating objects, we can think of it as a blueprint. It describes the possible states and behaviors that every object of a certain type could have.\nobject - data structure storing information about the state and behavior of a certain entity and is an instance of a class\nstub file - a file containing a skeleton of the public interface of that Python module, including classes, variables, functions – and most importantly, their types. (Source)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#base",
    "href": "qmd/python-general.html#base",
    "title": "General",
    "section": "Base",
    "text": "Base\n\nInfo method\n\nX.info()\nRemove an object: del\nCheck object type\n\ntype() : outputs the type of an object\nisinstance() : outputs type and inheritance of an object\nSee article for details on differences\n\nImport Libraries\nimport logging\nimport bentoml\nfrom transformers import (\n    SpeechT5Processor,\n    SpeechT5ForTextToSpeech,\n    SpeechT5HifiGan,\n    WhisperForConditionalGeneration,\n    WhisperProcessor,\n)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-fund",
    "href": "qmd/python-general.html#sec-py-gen-fund",
    "title": "General",
    "section": "Fundamentals",
    "text": "Fundamentals\n\nSlicing\n\nFormat my_list[start:stop:step]\n\n** start value is inclusive and the end value is exclusive\n\n1 element and more than 1 element\n\"Python\"[0] # P\n\"Python\"[0:1] # P\n\"Python\"[0:5] # Pytho\nillustrates how when using a range, the last element is exclusive\nNegative indexing my_list[0:-1]\n\nEverything but the last object\n\nSkip every second element\nmy_list = list(\"Python\")\nmy_list[0:len(my_list):2]\n&gt;&gt; ['P', 't', 'o']\nstart at 0, end at len(my_list), step = 2\nShortcuts\nmy_list[0:-1] == my_list[:-1]\nmy_list[0:len(my_list):2] == my_list[::2]\n\"Python\"[::-1] == \"Python\"[-1:-7:-1]\n\nDefaults\n\n0 for the start value\nlen(list) for the stop value\n1 for the step value\n\nDefaults for negative step value\n\n-1 for the start value\n-len(list) - 1 for the stop value\n\n\nAlias vs new object\nb = a # alias\nb = a[:] # new object\n\nWith the alias, changes to a will happen to b as well\n\nCommon use cases\n\n\n\n\n\n\n\nEvery element but the first and the last one\n[1:-1]\n\n\nEvery element in reverse order\n[::-1]\n\n\nEvery element but the first and the last one in reverse order\n[-2:0:-1]\n\n\nEvery second element but the first and the last one in reverse order\n[-2:0:-2]\n\n\n\nUsing slice function\nsequence = list(\"Python\")\nmy_slice = slice(None, None, 2) # equivalent to [::2]\nindices = my_slice.indices(len(sequence))\n&gt;&gt; (0, 6, 2)\n\nShows start = 0, stop = 6, step = 2\n\n\n\n\nF-Strings\n\nCheatsheet\nParameterize with {}\n&gt;&gt; x = 5\n&gt;&gt; f\"One icecream is worth [{x}]{style='color: #990000'} dollars\"\n'One icecream is worth 5 dollars'\n! - functions\n\n!r — Shows the string delimiter, calls the repr() method.\n\nrepr’s goal is to be unambiguous and str’s is to be readable. For example, if we suspect a float has a small rounding error, repr will show us while str may not\n\n!a — Shows the Ascii for the characters.\n!s — Converts the value to a string.\n\nGuessing this the str() method (see !r for details)\n\n\nfood2brand = \"Mcdonalds\"\nfood2 = \"French fries\"\nf\"I like eating {food2brand} {food2!r}\"\n\"I like eating Mcdonalds 'French fries'\"\nChange format with “:”\n&gt;&gt; import datetime\n&gt;&gt; date = datetime.datetime.utcnow()\n&gt;&gt; f\"The date is {date:%m-%Y %d}\"\n'The date is 02-2022 15'\nFormatting with “&gt;” and “&lt;”\n\n\n&lt;6 says width is 6 characters and text starts at the left edge\n&gt;10.2f says width is 10 characters, text starts the right hand edge, and number is rounded to 2 decimal places\n\n\n\n\nOperators\n\n(docs)\nExponential: 5**3\nInteger division: 5//3\nModulo: 5%3\nIdentity: is\nx = 5\ny = 3\nprint(\"The result for x is y is\", x is y)\nThe result for x is y is false\n\nThink you can also use == here too\n\nLogical: and and or\nprint(\"The result for 5 &gt; 3 and 6 &gt; 8 is\", 5 &gt; 3 and 6 &gt; 8)\nprint(\"The result for 5 &gt; 3 or 6 &gt; 8 is\", 5 &gt; 3 or 6 &gt; 8)\nThe result for 5 &gt; 3 and 6 &gt; 8 is False\nThe result for 5 &gt; 3 or 6 &gt; 8 is True\nSubset: in and not in\nprint(\"Is the number 3 in the list [1,2,3]?\", 3 in [1,2,3])\nIs the number 3 in the list [1,2,3]? True\n\nprint(\"Is the number 3 not in the list [1,2,3]?\", 3 not in [1,2,3])\nIs the number 3 not in the list [1,2,3]? False\nAssignment",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-dattyp",
    "href": "qmd/python-general.html#sec-py-gen-dattyp",
    "title": "General",
    "section": "Types",
    "text": "Types\n\nScalars\n\nCreate scalars by subsetting a list\ninputs = [1, 0.04, 0.9]\n# 1 numeric \nrmse = inputs[0] # rmse = 1 and is type 'float'\n# multiple numerics\nrmse, mape, rsq = inputs\n\nTuples\n\nLists are mutable and tuples are not\n\ni.e. we can add or remove elements to a list after we create it but we cannot do such thing to a tuple\n\nSyntax: name_of_tuple = (a, b)\n\nLists\n\nCreate list of objects (e.g. floats)\nacc_values = [rmse, mape, rsq]\n\nalt method: asterisk-comma notation\n*acc_names, = \"RMSE\", \"MAPE\", \"R-SQ\"\n\nasterisk is “unzipping operator”\n\n\nMake a copy\nold_list = [2, 3, 4]\nnew_list = list(old_list)\n\nDictionaries\n\n** if creating a simple dict, more performant to use curly braces **\n\nAvoid d = dict(1=1, x='x')\n\nJoin 2 dicts -  d.update(d2)\n\nIf d and d2 share keys, d2’s values for those keys will be used\n\nAccess a value from a key: sample_dict['key_name']\nMake a copy\nold_dict = {stuff: 2, more_stuff: 3}\nnew_dict = dict(old_dict)\nConvert list of tuples to a dict\nacc_dict = dict(acc_list)\n\nzip creates lists of tuples (See Loops &gt;&gt; zip section)\n\nAdd key, value pair to a dict\ntransaction_data['user_address'] = '221b Baker Street, London - UK'\n# or\ntransaction_data.update(user_address='221b Baker Street, London - UK')\nUnpack dict into separate tuples for key:value pairs\nrmse, mape, rsq = acc_dict.items()\nrmse\n('RMSE', 1)\n\n** fastest way to iterate over both keys and values in a dict **\ncan also use zip to unpack pairs into a list (see loops &gt;&gt; zip)\n\nUnpack dict into separate lists for keys and values\nacc_keys = list(acc_dict.keys()) \nacc_values = list(acc_dict.values())\n\n** fastest way to iterate over a dict’s keys or values **\n\nUnpack values from dicts into separate scalars\nrmse, mape, rsq = acc_dict.values()\nrmse\n1\nPull the value for a key (e.g. k) or return the default value - d.get(k, default)\n\nDefault is “None”. I think this can be set with d.setdefault(k, default)\n\nCheck for specific key (logical)\n‘send_currency’ in transaction_data\n‘send_currency’ in transaction_data.keys()\n‘send_currency’ not in transaction_data.keys()\n\nLike %in% in R\n\nCheck for specific value (logical)\n‘GBP’ in transaction_data.values()\nCheck for key, value pair\n(‘send_currency’, ‘GBP’) in transaction_data.items()\nPretty printing of dictionaries\n    _ = [print(k, \":\", f'{v:.1f}') for k,v in acc_dict.items()]\n    RMSE : 1.00\n    MAPE : 0.04\n    R-sq : 0.90\n\nfor-in loop format (see Loops &gt;&gt; Comprehension)\nprint returns “none” for each key:value at the bottom of the output for some reason. Assigning the print statement to a variable fixes it.\n\ndefaultdict\n\nCreates a key from a list element and groups the properties into a list of values where the value may also be a dict.\nFrom {{collections}}\nAlso see\n\nPybites video\nJSON &gt;&gt; Python &gt;&gt; Example: Parse Nested JSON into a dataframe\n\n\n\nSets\n\nIf performing set logic, always more performant to use sets instead of dicts or lists\n\n\nIf using numpy/pandas, using the .unique() syntax is more efficient for arrays/series’ with numeric values\nIf using strings, it’s more efficient to use list(set(my_array))\n\n\nStrings\n\nOperators\nOperator Description\n%d Signed decimal integer\n%u unsigned decimal integer\n%c Character\n%s String\n%f Floating-point real number\nExample\n\nname = \"india\"\nage = 19\nmarks = 20.56\nstring1 = 'Hey %s' % (name)\nprint(string1)\nstring2 = 'my age is %d' % (age)\nprint(string2)\nstring3= 'Hey %s, my age is %d' % (name, age)\nprint(string3)\nstring3= 'Hey %s, my subject mark is %f' % (name, marks)\nprint(string3)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-pip",
    "href": "qmd/python-general.html#sec-py-gen-pip",
    "title": "General",
    "section": "pip",
    "text": "pip\n\nLooks for packages on https://pypi.org, downloads, and installs it\nMisc\n\nIf you installed python using the app-store, replace python with python3.\nDon’t use sudo to install libraries, since it will install things outside of the virtual environment.\nNor should you use “–user”, since it’s made to install things outside of the virtual environment.\nDon’t mix pip, venv and Anaconda. Avoid Anaconda if you can. If you have to use Anaconda, don’t use pip and venv. Limit yourself to Anaconda tools.\nIf you get SSL errors (common if you are in a hotel or a company network) use the –trusted-host pypi.org –trusted-host files.pythonhosted.org options with pip to work around the problem.\n\ne.g. python -m pip install pendulum --trusted-host pypi.org --trusted-host files.pythonhosted.org\n\nIf you are behind a corporate proxy that requires authentication (common if you are in a company network), you can use the –proxy option with pip to give the proxy address and your credentials.\n\ne.g. python -m pip install pendulum --proxy http://your_username:yourpassword@proxy_address\nIt also works with the https_proxy environment variables\n\n\nInstall library\n$ python -m pip install &lt;library_name&gt;\n\n# inside ipython or a colab notebook, \"!\" signifies a shell command\n!pip install &lt;library_name&gt;\nInstall library from github\npython -m pip install git+https://github.com/bbalasub1/glmnet_python.git@1.0\n\n“@1.0” is the version number\n\nUninstall library\n$ python -m pip uninstall &lt;library_name&gt;\n\nWon’t uninstall the dependencies of this library.\nIf you wish to also uninstall the unused dependencies as well, take a look at pip-autoremove\n\nRemove all packages in environment\n$ python -m pip uninstall -y -r &lt;(pip freeze)\nRemove all packages in environment but write the names of the packages to a requirements.txt file first\n$ python -m pip freeze &gt; requirements.txt && python3 -m pip uninstall -r         requirements.txt -y\nInstall requirements.txt\n$ python -m pip install -r requirements.txt\nWrite names of all the packages in your environment to a requirement.txt file\n$ python -m pip freeze &gt; requirements.txt\n\nWrites the specific version of the packages that you have installed in your environment (e.g. pandas==1.0.0)\n\nThis may not be what you always want, so you’ll need to manually change to just the library name in that case (e.g. pandas)\n\nOnly aware of the packages installed using the pip install command\n\ni.e. any packages installed using a different approach such as peotry, setuptools, condaetc. won’t be included in the final requirements.txt file.\n\nDoes not account for dependency versioning conflicts\nSaves all packages in the environment including those that are not relevent to the project\nIf you are not using a virtual environment, pip freeze generates a requirement file containing all the libraries in including those beyond the scope of your project.\n\nList your installed libraries\n$ python -m pip list\nSee if you have a particular library installed\n$ python -m pip list | grep &lt;library_name&gt;\nGet library info (name, version, summary, license, dependencies and other)\n$ python -m pip show &lt;library_name&gt;\nCheck that all installed packages are compatible\n$ python -m pip check\nUpdate package\n$ python -m pip install package_name --upgrade\nSearch for PyPI libraries (pip source for libraries)\n$ python -m pip search &lt;search_term&gt;\n\nreturns all libraries matching search term\n\nDownload a package without installing it\npython -m pip download &lt;library name&gt;\n\nIt will download the package and all its dependencies in the current directory (the files, called, wheels, have a .whl extension).\nYou can then install them offline by doing python -m pip install on the wheels.\n\nBuild Wheel archives for the libraries and dependencies in your environment\n$ python -m pip wheel\n\nI think these are binaries, so they don’t need compiled if installed in a future environment\nReal Python Tutorial\n\nManage configuration\n$ python -m pip config &lt;action name&gt;\n\nActions: edit, get, list, set or unset\nExample\n$ python -m pip config set global.index-url https://your.global.index\n\nDisplay debug information specific to pip\n$ python -m pip debug",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-anac",
    "href": "qmd/python-general.html#sec-py-gen-anac",
    "title": "General",
    "section": "Anaconda",
    "text": "Anaconda\n\nCheck configuration of current environment\nconda list\n\nShows python version used, package names installed and their versions\n\nInstall packages\nconda install &lt;package1&gt; &lt;package2&gt;\nInstall a package from a specific channel\nconda install &lt;package_name&gt; -c &lt;channel_name&gt; -y # Short form\nconda install &lt;package_name&gt; --channel &lt;channel_name&gt; -y # Long form\nPackage installation channels (some packages not available in default channel)\n\nCheck current channels\nconda config --show channels\n\nThe order in which these channels are displayed shows the channel priority.\n\nWhen a package is installed, anaconda will the check the channel at the top of list first then work it’s way down\n\n\nAdd a channel\nconda config --add channels conda-forge\n\nAdds “conda-forge” to list of available channels\n\nRemove a channel\nconda config --remove channels conda-forge\n\nRemoves the “conda-forge” channel",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-env",
    "href": "qmd/python-general.html#sec-py-gen-env",
    "title": "General",
    "section": "Environments",
    "text": "Environments\n\nMisc\n\nWhen you’re in a virtual environment\n\nAnytime you use the “python” command while your virtual environment is activated, it will be only the one from this env.\nIf you start a Python shell now, it will see only the things in the current directory, and the things installed in the virtual environment.\nIf you run a script, it will see only the things in the current directory, and the things installed in the virtual environment.\nIf you run a command, the command will be taken from the virtual environment.\nAnd they will only use exactly the version of Python of the virtual environment.\n\nStore environment files with project\nBreakage\n\nYou cannot move a virtual environment, it will stop working. Create a “requirements.txt” file, delete the virtual environment and create a new one.\nDon’t rename a directory containing a virtual environment. Or if you do, prepare yourself to create a “requirements.txt” file, delete the virtual environment and create a new one.\nIf you change the Python used in the virtual environment, such as when uninstalling it, it will stop working.\n\nCreate one big virtual environment for all small scripts.\n\nIf you make a lot of venv, you may be tempted to install everything at the system level for convenience. After all, it’s a bore to create and activate a virtual environment each time you want to write a five liner. A good balance is one single virtual environment you use for all things quick and dirty.\n\nCreate several virtual environments per versions of python if your project needs to support several versions. You may need several requirements.txt files as well, one for each env.\nRecommendations for a stable dependency environment for your project (article)\n\nDon’t install the latest major version of Python\n\nMaximum: 1 version under the latest version\nMinimum: 4 versions under the latest version (e.g. latest = 3.11, min = 3.7)\n\nUse only the python.org installer on Windows and Mac, or official repositories on Linux.\nNever install or run anything outside of a virtual environment\nLimit yourself to the basics: “pip” and “venv”\nIf you run a command, use “-m”\n\nIt lets you run any importable Python module, no matter where you are. Because most commands are Python modules, we can use this to say, “run the module X of this particular python”.\nThere is currently no way for you to run any python command reliably without “-m”.\nExamples:\n# Don't do :\npip install\n# Do:\npython -m pip install\n\n# Don't do :\nblack\n# Do:\npython -m black\n\n# Don't do :\njupyter notebook\n# Do:\npython -m jupyter notebook\n\nWhen creating a virtual environment, be explicit about which Python you use\n\nGet current python versions installed: py --list-paths (windows)\n\n\n\n\n\npyenv\n\nJust a simple Python version manager — think {rig}\n{{pyenv}}, {{pyenv-win}}\nSet-up in RStudio (article)\nCompiles Python under the hood when you install it. But compiling can fail in a thousand ways\npyenv install --list: To see what python versions are available to install\npyenv install &lt;version number&gt;: To install a specific version\npyenv versions: To see what python versions are installed on your system\npyenv global &lt;version number&gt;: The set one python version as a global default\npyenv local &lt;version number&gt;: The set a python version to be used within a specific directory/project\\\n\n\n\npdm\n\nDocs\nPackage and dependency manager similar to npm. Doesn’t require virtual environments.\nFeatures: auto-updating pyproject.toml, isolating dependencies from dependencies-of-dependencies, active development and error handling\n\n\n\nvenv\n\nMisc\n\nShipped with Python\nDon’t mix pip, venv and Anaconda. Avoid Anaconda if you can. If you have to use Anaconda, don’t use pip and venv. Limit yourself to Anaconda tools.\n\nCreate\n\nWindows: py -&lt;py version&gt; -m venv &lt;env name&gt;\nMac/Linux: python3.8 -m venv .venv\n\nWhere the python version is 3.8 and the environment name is “.venv”\nMac and Linux hide folders with names that have preceding “.” by default, so make sure you have “display hiddent folders” activated or you won’t see it.\n\nNaming Environments\n\nName your environment directory “.venv”, because:\n\nSome editors check for this name and automatically load it.\nIt contains “venv”, so it’s explicit.\nIt has a dot, so it’s hidden on Linux.\n\nIf you have more than one environment directory, use a suffix to distinguish them.\n\ne.g. A project that must work with two different versions of Python (3.9 and 3.10), I will have a “.venv39” and a “.venv310”\n\nNaming enviroments for misc uses\n\n“.venv_test”: located in a personal directory to install new tools you want to play with. It’s disposable, and often broken, so you can delete it and recreate it regularly.\n“.venv_scripts”: Used for all the small scripts. You don’t want to create one virtual environment for each script, so centralize everything. It’s better than installing outside of a virtual environment, but is not a big constraint.\n\n\n\nActivate\n\nWindows: .venv\\Scripts\\activate\n\nWhere .venv is the name of the virtual environment\nMay need .bat as extension to activate\n\nMac/Linux: source .venv/bin/activate\n\nAfter that, you can use python -m pip install to install packages.\nDeactivate: deactivate\n\n\n\nvirtualenv\n\nDocs\nCreate a virtual environment\n python3 -m venv &lt;env_name&gt;\n\n-m venv tells python to run the virtual environment module, venv\nMake sure you’re in your projects directory\nRemember to add “&lt;env_name&gt;/” to .gitignore\n\nActivate environment\nsource venv/bin/activate # Mac or Linux\nvenv\\Scripts\\activate # Windows\n\n&gt;&gt; (&lt;env_name&gt;) $\n\nPrompt should change if the environment is activated\nAll pip  installed packages will now be installed into the “&lt;env_name&gt;/lib/python3.9/site-packages” directory\n\nUse the python contained within your virtual environment\npython main.py\n\nNot sure why you wouldn’t just activate the environment.\n\nDeactivate environment\ndeactivate\n\nno python  or env_name needed?\n\nReproducing environment\n\nDone using requirements.txt (see pip section for details on writing and installing)\n\nI don’t think the python version is included, so that will need to communicated manually\n\n\n\n\n\nAnaconda\n\nList environments\nconda env list # method 1\nconda info --envs # method 2\n\ndefault environment is called “base”\nActive environment will be in parentheses\nActive environment will be the one in the list with an asterix\n\nCreate a new conda environment\nconda create -n &lt;env name&gt;\nconda activate &lt;env name&gt;\nCreate a new conda environment with a specific python version\nconda create -n py310 python=3.10\nconda activate py310\nconda install jupyter jupyterlab\njupyter lab\n\nAlso install and launch jupyter lab\n\nCreate an environment from a yaml file\nconda env create -f environment.yml # Short form\nconda env create --file environment.yml # Long form\nRemove an environment\nconda deactivate &lt;env_name&gt; # Need to deactivate the environment first\nconda env remove -n &lt;env_name&gt;\n\nShould also delete environment folders (conda env list shows path to folders)\n\nClone an existing environment\nconda create -n testclone --clone test # Short form\nconda create --name testclone --clone test # Long form\n\n“testclone” is a copy of “test”\n\nActivate an environment\nconda activate &lt;env_name&gt;\nActivate environment with reticulate in R\nreticulate::use_python(\"/usr/local/bin/python\")   \nreticulate::use_condaenv(\"&lt;env name&gt;\", \"/home/jtimm/anaconda3/bin/conda\")\nDeactivate an environment\nconda activate # Option 1: activates base\nconda deactivate test # Option 2\nExport the specifications of the current environment into a YAML file into the current directory\nconda env export &gt; environment.yml # Option 1\nconda env export -f environment.yml # Option 2\nExample: Conda workflow\n\nCreate an environment that uses a specific python version\n\nWithout a specified python version, the environment will use the same version as “base”\n\nconda create -n anothertest python=3.9.7 -y\n\n-n is the name flag and “anothertest” is the name of the environment\nUses Python 3.9.7\nWithout the -y flag, there’d be a prompt you’d have to answer “yes” to\n\nActivate the environment\nconda activate anothertest\nInstall packages\n\n\nInstalling packages one at time can lead to dependency conflicts.\nConda’s official documentation recommends to install all packages at the same time so that the dependency conflicts are resolved\nconda install \"numpy&gt;=1.11\" nltk==3.6.2 jupyter -y # install specific versions\nconda install numpy nltk jupyter -y # install all latest versions\n\nDo work and deactivate environment\nconda deactivate anothertest\n\nExample Raschka workflow\n# create & activate\nconda create  --prefix ~/code/myproj python=3.8\nconda activate ~/code/myproj\n# export env\nconda env export &gt; myproj.yml\n# create new env from yaml\nconda env create --file myproj.yml --prefix ~/code/myproj2\n\n\n\nPoetry\n\nDocs (like renv)\nApparently buggy (article)\npip’s dependency resolver is more flexible and won’t die on you if the package specifies bad metadata, while poetry’s strictness may mean you can’t install some packages at all.\nCreate project\n\npoetry new &lt;project-dir-name&gt;\n\nautomatically creates a directory for your project with a skeleton\n“pyproject.toml” maintains dependencies for the project with the following sections:\n\ntool.poetry provides an area to capture information about your project such as the name, version and author(s).\ntool.poetry.dependencies lists all dependencies for your project.\ntool.poetry.dev-dependencies lists dependencies your project needs for development that should not be present in any version deployed to a production environment.\nbuild-system references the fact that Poetry has been used to manage the project.\n\n\nAdd library and create lock file: poetry add &lt;library name&gt;\n\nWhen the first library is added, a “poetry.lock” file wil be generated\n\nActivate environment: poetry shell\n\nDeactivate environment: exit\n\nRun script: poetry run python my_script.py\nPackage the project: poetry build\n\nCreates tar.gz and wheel files (.whl) in “dist” dir\n\nExample: poetry workflow (+pyenv, virtualenv)\n# Create a virtual environment called \"my-new-project\"\n# using Python 3.8.8\npyenv virtualenv 3.8.8 my-new-project\n# Activate the virtual environment\npyenv activate my-new-project\n\n{{pyenv}} - For managing the exact version of Python and activating the environment\nName your package the same name as the directory which is the same name as the virtual environment.\n\nDashes for the latter two and underscores for the package\n\nIntitialize the project and add packages (similar to renv) bash              poetry init     poetry add numpy\nReinstall dependencies\n# navigate to my project directory and run\npoetry install\nTurn off virtualenv management\n# right after installing poetry, run:\npoetry config virtualenvs.create false\n\nDefault poetry behavior is that it will manage your virtual environments for you. This may not be desirable because:\n\nCan’t just run a script from the command line. Instead, have to run poetry run my-script\n\nAwkward when you want to dockerize your code\n\nEnforces a virtual environment management framework on everybody in a shared codebase\nYour Makefile now needs to know about poetry",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-deps",
    "href": "qmd/python-general.html#sec-py-gen-deps",
    "title": "General",
    "section": "Dependencies",
    "text": "Dependencies\n\nMisc\n\nAfter mastering pip, {{pip-tools}} is recommended.\n\nGet a complete list of dependencies (e.g. dependencies of dependencies) with {{deptree}}\ndeptree\n# output\nFlask==2.2.2  # flask\n  Werkzeug==2.2.2  # Werkzeug&gt;=2.2.2\n    MarkupSafe==2.1.1  # MarkupSafe&gt;=2.1.1\n  Jinja2==3.1.2  # Jinja2&gt;=3.0\n    MarkupSafe==2.1.1  # MarkupSafe&gt;=2.0\n  itsdangerous==2.1.2  # itsdangerous&gt;=2.0\n  click==8.1.3  # click&gt;=8.0\n# deptree and pip trees\n\nFlask depends on Werkzeug which depends on MarkupSafe\n\nWerkzeug and MarkupSafe qualify as transitive dependencies for this project\n\nCommented part on the right is the compatible range\n\nrequirements.txt format\n# comment\npandas==1.0.0\npyspark\npip: write names of all the packages in your environment to a requirement.txt file\n$ python3 -m pip freeze &gt; requirements.txt\n\nSee pip section for issues with this method\n\n{{pipx}}\n\nA tool for installing Python CLI utilities that gives them their own hidden virtual environment for their dependencies\nAdds the tool itself to your PATH - so you can install stuff without worrying about it breaking anything else\nInstall\npipx install datasette\n\n{{pipreqs}}\n\nScans all the python files (.py) in your project, then generates the requirements.txt file based on the import statements in each python file of the project\nSet-up: pip install pipreqs\nGenerate requirements.txt file: pipreqs /&lt;your_project_root_path&gt;/\nUpdate requirements.txt:  pipreqs --force /&lt;your_project_root_path&gt;/ \nIgnore the libraries of some python files from a specific subfolder\npipreqs /&lt;your_project_root_path&gt;/ --ignore  /&lt;your_project_root_path&gt;/folder_to_ignore/\n\n{{pip-compile-multi}}\n\nNotes from:\n\nEnd Python Dependency Hell with pip-compile-multi\n\nCreates and nests multiple requirement files\n\ne.g. Able to keep dev environment from production environment separate\n\nAutoresolution of cross-requirement file conflicts\n\nDependency DAG (how all requirement files are connected) must have exactly one “sink” node\n\nOrganize your most ubiquitous dependencies into a single “core” set of dependencies that all other nodes require (a source node), and all of your development dependencies in a node that requires all others (directly or indirectly) require (a sink).\n\nSimplifies and allows use of autoresolution functionality\n\nExample: DAG (directionality of the arrows is opposite compared to library docs)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-loadsav",
    "href": "qmd/python-general.html#sec-py-gen-loadsav",
    "title": "General",
    "section": "Loading/Saving",
    "text": "Loading/Saving\n\nMisc\n\n{{pickle}} needs custom class(es) to be defined in another module/file and then imported. Otherwise, PicklingError will be raised.\n\n\n\nFile paths\n\nMisc\n\n{{pathlib}} is recommended\n\n{{os}}\n\nGet current working directory: os.getcwd()\nList all files and directories in working directory: os.listdir()\nList all files and directories from a subdirectory: os.listdir(os.getcwd()+'\\\\01_Main_Directory')\nUsing os.walk(): gathers paths, folders, and files\n\nPaths\n\n\npath = os.getcwd()+'\\\\01_Main_Directory'\nfor folder_path, folders, files in os.walk(path):\n    print(folder_path)\nFolders\n\n\nSimilar code, just replace print(folder_path) with print(folders)\n\nFiles\n\n\n{{glob}}\n\nGet a file path string\nimport glob\npath = os.getcwd()+'\\\\01_Main_Directory'\nfor filepath in glob.glob(path):\n    print(filepath)\n# C:\\Users\\Suraj\\Challenges\\01_Main_Directory\nList all files and subdirectories from a path\npath = os.getcwd()+'\\\\01_Main_Directory\\\\*'\nfor filepath in glob.glob(path):\n    print(filepath)\n\nNote the * wildcard\n\nList all files and subdirectories with a “1” in the name\npath = os.getcwd()+'\\\\01_Main_Directory\\\\*1.*'\nfor filepath in glob.glob(path):\n    print(filepath)\nGet a list of csv file paths from a directory: all_files = glob.glob(\"C:/Users/path/to/dir/*.csv\")\n\nNote that you don’t need a loop to save to an object\n\nList all files and subdirectories and files in those subdirectories\npath = os.getcwd()+'\\\\01_Main_Directory\\\\**\\\\*.txt'\nfor filepath in glob.glob(path, recursive=True):\n    print(filepath)\n#Output\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_2.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_3.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_4.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_5.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_1\\File_1_in_SubDict_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_1\\File_2_in_SubDict_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_2\\File_1_in_SubDict_2.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_2\\File_2_in_SubDict_2.txt\n\nComponents: “**” and “recursive=True”\n\n\n{{pathlib}}\n\nProvides a single Path class with a range of methods (instead of separate functions) that can be used to perform various operations on a path.\nCreate a path object for a directory\nfrom pathlib import Path\npath = Path('origin/data/for_arli')\nCheck if a folder or a file is available in a given path\nif path.exists():\n    print(f\"[{path}]{style='color: #990000'} exists.\")\n    if path.is_file():\n        print(f\"[{path}]{style='color: #990000'} is a file.\")\n    elif path.is_dir():\n        print(f\"[{path}]{style='color: #990000'} is a directory.\")\nelse:\n    raise ValueError(f\"[{path}]{style='color: #990000'} does not exists\")\n\nChecks if the path ‘origin/data/for_arli’ exists\n\nif it does, it will check whether it is a file or a directory.\nIf the path does not exist, it will print a raise an Error indicating that the path does not exist.\n\n\nList all files/folders in a path\nfor f in path.iterdir():\n    print(f)\n\nUse it in combination with the previous is_dir() and is_file()  methods to list either files or directories.\n\nDelete files/folders in a path\nfor f in path.iterdir():\n    f.unlink()\n\npath.rmdir()\n\nunlink deletes each file in the path\nrmdir deletes the directory.\n\ndirectory must be empty\n\n\nCreate a sequence of directories\n# existing directory: D:\\scripts\\myfolder\np = Path(\"D:\\scripts\\myfolder\\logs\\newfolder\")\np.mkdir(parents=True, exist_ok=True)\n\nCreate path object with desired sequence of directories (e.g. logs\\newfolder)\nmkdir with parents=True creates the sequence of directories\n\nW/exist_ok=True no error with occur if the directory already exists\n\n\nRename directory: path.rename('origin/data/new_name')\nConcatenate a path with string\npath = Path(\"/origin/data/for_arli\")\n# Join another path to the original path\nnew_path = path.joinpath(\"la\")\nprint(new_path) # prints 'origin/data/for_arli/bla'\n\nIt also handles the join between two Path objects\n\nDirectory stats\nprint(path.stat()) # print statistics \nprint(path.owner()) # print owner\n\ne.g. creation time, modification time, etc.\n\nWrite to a file\n# Open a file for writing\npath = Path('origin/data/for_arli/example.txt')\nwith path.open(mode='w') as f:\n    # Write to the file\n    f.write('Hello, World!')\n\nYou do not need to create manually example.txt.\n\nRead a file\npath = Path('example.txt')\nwith path.open(mode='r') as f:\n    # Read from the file\n    contents = f.read()\n    print(contents) # Output: Hello World!\n\n\n\n\nModels\n\nSaving and Loading an estimator as a binary using {{joblib}} (aside: pipelines are estimators)\nimport joblib\n#saving the pipeline into a binary file\njoblib.dump(pipe, 'wine_pipeline.bin')\n#loading the saved pipeline from a binary file\npipe = joblib.load('wine_pipeline.bin')\nSaving and loading a trained model as a pickle file\nimport pickle\n# open file connection\npickle_file = open('model.pkl', 'ab')\n# save the model\npickle.dump(model_obj, pickle_file)\n# close file connection                     \npickle_file.close()\n\n# Open conn and save\ntest_dict = {\"Hello\": \"World!\"}\nwith open(\"test.pickle\", \"wb\") as outfile:\n# \"wb\" argument opens the file in binary mode\npickle.dump(test_dict, outfile)\n\n# open file connection\npickle_file = open('model.pkl', 'rb')\n# load saved model\nmodel = pickle.load(pickle_file)\n\n# open conn and load\n# Deserialization\nwith open(\"test.pickle\", \"rb\") as infile:\n    test_dict_reconstructed = pickle.load(infile)\n\nCan serialize almost everything including classes and functions\n\n\n\n\nEnvironment Variables\n\n{{os}}\n\nCheck existence\nenv_var_exists = 'ENV' in os.environ\n# or\nenv_var_exists = os.environ.has_key('ENV')\nList environment variables: print(os.environ)\nLoading\nimport os\n# Errors when not present\nenv_var = os.environ['ENV'] # where ENV is the name of the environment variable\n# Returns None when not present\nenv_var = os.environ.get('ENV', 'DEFAULT_VALUE') # using default value is optional\nSet/Export or overwrite\nos.environ['ENV'] = 'dev'\nLoad or create if not present\ntry:\n    env_var = os.environ['ENV']\nexcept KeyError:\n    os.environ['ENV'] = 'dev'\nDelete\nif 'ENV' in os.environ:\n    del os.environ['ENV']\n\n{{python-decouple}}\n\nAccess environment variables from whatever environment it is running in.\nCreate a .env file in the project root directory: touch .env\nOpen .env in nano text editor: nano .env\n\nNano text editor is pre-installed on macOS and most Linux distros\nCheck if installed/version: nano --version\nBasic usage tutorial\n\nAdd environment variables to file\nUSER=alex\nKEY=hfy92kadHgkk29fahjsu3j922v9sjwaucahf\n\nSave: Ctrl+o\nExit: Ctrl+x\n\n* Add .env to your .gitignore file *\nAccess\nfrom decouple import config\nAPI_USERNAME = config('USER')\nAPI_KEY = config('KEY')\n\n{{python-dotenv}}\n\nReads .env files\nProbably more popular than {{python-decouple}}\nHas a companion R package, {dotenv}, so .env files can be used in projects that use both R and Python.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-funs",
    "href": "qmd/python-general.html#sec-py-gen-funs",
    "title": "General",
    "section": "Functions",
    "text": "Functions\n\nMisc\n\nBenchmarking a function\n\nUsing IPython function\n%time dat['col1001'] = some_function(dat['col1'], dat['col2'], dat['col3'])\n\n%%time prints the wall time for the entire cell whereas %time gives you the time for first line only\n\nUsing a decorator\n\n\nAssigning functions based on arg type\ndef process_data(data):\n    if isinstance(data, dict):\n        process_dict(data) \n    else:\n        process_list(data) \ndef process_dict(data: dict):\n    print(\"Dict is processed\")\ndef process_list(data: list):\n    print(\"List is processed\")\n\nAssigns data to a particular function depending on whether it’s a dict or a list\nisinstance checks that the passed argument is of the proper type or a subclass\n\nWrapping functions\nfrom functools import partial\nget_count_df = partial(get_count, df=df)\n\nWraps function to make df the default value for df arg\n\n\n\n\nDocumentation\n\nFunctions should at least include docstrings and type hinting\nDocstrings\n\nTypes: Google-style, Numpydoc, reStructured Text, EpyTex\nInformation to include\n\nFunction description, arg description, return value description, Description of errors, Optional extra notes or examples of usage.\n\nAccess functions docstring:\n\nprint(func_name.__doc__)\nFor large docstrings\nimport inspect\nprint(inspect.getdoc(func_name))\n\nExample: Google-style\ndef send_request(key: str, lat: float = 0, lon: float = 0):\n    \"\"\"Send a request to Climacell Weather API\n    to get weather info based on lat/lon.\n\n    Climacell API provides realtime weather\n    information which can be accessed using\n    their 'Realtime Endpoint'.\n\n    Args:\n      key (str): an API key with length of 32 chars.\n      lat (float, optional): value for latitude.\n        Default=0\n      lon (float, optional): value for longitude.\n        Default=0\n\n    Returns:\n      int: status code of the result \n      dict: Result of the call as a dict\n\n    Notes:\n      See https://www.climacell.co/weather-api/ \n      for more info on Weather API. You can get\n      API key from there, too.\n    \"\"\"\n\nFirst sentence should contain the purpose of the function\n\nExample: Numpydoc\ndef send_request(key: str, lat: float = 0, lon: float = 0):\n    \"\"\"\n    Send a request to Climacell Weather API\n    to get weather info based on lat/lon.\n\n    Climacell API provides realtime weather\n    information which can be accessed using\n    their 'Realtime Endpoint'.\n\n    Parameters\n    ----------\n      key (str): an API key with length of 32 chars.\n      lat (float, optional): value for latitude.\n        Default=0\n      lon (float, optional): value for longitude.\n        Default=0\n\n    Returns\n    -------\n      int: status code of the result \n      dict: Result of the call as a dict\n\n    Notes\n    -----\n      See https://www.climacell.co/weather-api/ \n      for more info on Weather API. You can get\n      API key from there, too.\n    \"\"\"\n\nType Hinting\n\nThis doesn’t check the type; it’s just metadata\n\nsee isinstance (see below), NotImplementedError (see below), or {{typecheck}} and {{mypy} (see bkmks) for type checking that will throw errors\n\nUsing type hints enables you to perform type checking. If you use an IDE like PyCharm or Visual Studio Code, you’ll get visual feedback if you’re using unexpected types:\nVariables: my_variable_name: tuple[int, ...]\n\nvariable should be a tuple that contains only integers. The ellipsis says the total quantity is unimportant.\n\nFunctions\ndef get_count(threshold: str, column: str, df: pd.DataFrame) -&gt; int:\n    return (df[column] &gt; threshold).sum()\n\n“threshold”, “column” should be strings (str)\n“df” should be a pandas dataframe (pd.DataFrame)\nOutput should be an integer (int)\n\nFunction as an arg: Callable[[Arg1Type, Arg2Type], ReturnType]\n\nExample:\nfrom collections.abc import Callable\ndef foo(bar: Callable[[int, int], int], a: int, b: int) -&gt; int:\n    return bar(a, b)\n\n“bar” is a function arg for the function, “foo”\n“bar” is supposed to take: 2 integer args ([int, int]) and return an integer (int)\n\nExample:\ndef calculate(i: int, action: Callable[..., int], *args: int) -&gt; int:\n    return action(i, *args)\n\n“action” takes any number and type of arguments but must return an integer.\nWith *args: int, you also allow a variable number of optional arguments, as long as they’re integers.\n\nExample: Lambda\nf: Callable[[int, int], int] = lambda x, y: 3*x + y\n\nMay not work\n\n\n\n\n\n\nArgs and Operators\n\nMisc\n\n** Args are not reset to default values after each call **\n\nExample:\n\ndef func(list1=[]):      # here l1 is a default argument set to []\n    list1.append(\"Temp\")\n    return list1\n\n“None” + conditional must be used to get the arg to reset back to the default value\n\ndef func(l1=None):     \n    if l1 is None: \n        l1 = []\n    l1.append(\"Temp\") \n    return l1\n\n*\n\nUnpacks Lists\n\nnum_list = [1,2,3,4,5]\nnum_list_2 = [6,7,8,9,10]\n\nprint(*num_list)\n# 1 2 3 4 5\nnew_list = [*num_list, *num_list_2] # merge multiple lists\n# [1,2,3,4,5,6,7,8,9,10]\n*args\n\nFunctions that can accept a varying number of values\n\ndef names_tuple(*args):\n    return args\n\nnames_tuple('Michael', 'John', 'Nancy')\n# ('Michael', 'John', 'Nancy')\nnames_tuple('Jennifer', 'Nancy')\n# ('Jennifer', 'Nancy')\n**\n\nUnpacks Dictionaries\n\nnum_dict = {‘a’: 1, ‘b’: 2, ‘c’: 3}\nnum_dict_2 = {‘d’: 4, ‘e’: 5, ‘f’: 6}\n\nprint(*num_dict) # only keys printed\n# a b c\nnew_dict = {**num_dict, **num_dict_2} # merge dictionaries\n# {‘a’: 1, ‘b’: 2, ‘c’: 3, ‘d’: 4, ‘e’: 5, ‘f’: 6}\n**kwargs\n\nFunctions that can accept a varying number of variable/value pairs (like a … in R)\n\ndef names_dict(**kwargs):\n    return kwargs\n\nnames_dict(Jane = 'Doe')\n# {'Jane': 'Doe'}\nnames_dict(Jane = 'Doe', John = 'Smith')\n# {'Jane': 'Doe', 'John': 'Smith'}\nFunction as an arg\ndef classic_boot(df, estimator, seed=1):\n    df_boot = df.sample(n=len(df), replace=True, random_state=seed)\n    estimate = estimator(df_boot)\n    return estimate\n\nBootstrap function with an “estimator” function (e.g. mean) as arg\nUsing a Callable\n\nClass as an arg\nfrom dataclasses import dataclass\n\n@dataclass\nclass Person:\n    name: str\n    age: int\n    address: str\n    phone: str\n    email: str\n\ndef process_data(person: Person):\n    print(f\"Processing data for {person.name}, {person.age}, living at {person.address}. Contact info: {person.phone}, {person.email}\")\n\nperson = Person(\"Alice\", 30, \"123 Main St\", \"555-1234\", \"alice@example.com\")\nprocess_data(person)\n\nMakes function more readable when the function requires a bunch of args\n\nClass as an arg (safer alternative)\nfrom typing import NamedTuple\n\nclass Person(NamedTuple):\n    name: str\n    age: int\n    address: str\n    phone: str\n    email: str\n\ndef process_data(person: Person):\n    print(f\"Processing data for {person.name}, {person.age}, living at {person.address}. Contact info: {person.phone}, {person.email}\")\n\nperson = Person(\"Alice\", 30, \"123 Main St\", \"555-1234\", \"alice@example.com\")\nprocess_data(person)\n\nUsing NamedTuple means that the attributes cannot be overridden\n\ne.g. Executing person.name = \"Bob\" will result in an error because tuples can’t be modified.\n\n\nMake an arg optional\nlass Address:\n    def __init__(self, street, city, state, zipcode, street2=''):\n        self.street = street\n        self.street2 = street2\n        self.city = city\n        self.state = state\n        self.zipcode = zipcode\n\n“street2” has default value of an empty string, so it’s optional\n\n\n\n\nLambda\n\nUseful if you just have 1 expression that you need to execute.\nBest Practices\n\nlambda is an anonymous function, hence it is not a good idea to store it in a variable for future use\nDon’t use lambdas for single functions (e.g. sqrt). Make sure it’s an expression.\n\nExample\n# bad\nsqrt_list = list(map(lambda x: math.sqrt(x), mylist))\n# good\nsqrt_list = list(map(math.sqrt, mylist))\n\nAffects performance\n\n\nDon’t use for complex expressions that require more than 1 line (meh)\n\nPer PEP8 guidelines, Limit all lines to a maximum of 79 characters\nExample\n# bad (118 characters)\ndf[\"FinalStatus\"] = df[\"Status\"].map(lambda x: 'Completed' if x ==\n'Delivered' or x == 'Shipped' else 'Not Completed')\n# instead\ndf[\"FinalStatus\"] = ''\ndf.loc[(df[\"Status\"] == 'Delivered') |\n      (df[\"Status\"] == 'Shipped'),\n      'FinalStatus'] = 'Completed'\ndf.loc[(df[\"Status\"] == 'Not Delivered') |\n      (df[\"Status\"] == 'Not Shipped'),\n      'FinalStatus'] = 'Not Completed'\n\n\nExample: 1 arg\n# py\nlambda x: np.sin(x / period * 2 * np.pi)\n# r\n~sin(.x / period * 2 * pi)\n# r\n\\(x) {sin(x / period * 2 * pi)}\nExample: 2 args\nGreater = lambda x, y : x if(x &gt; y) else y\nGreater(0.002, 0.5897)\nLambda-Filter\n\nFaster than a comprehension\n\nsee Loops &gt;&gt; Comprehensions\n\nFormat: filter(function, data_object)\n\nReturns a filter object, which needs to be converted into data structure such as list or set\n\nExample: Basic\nyourlist = list(np.arange(2,50,3))\nlist(filter(lambda x:x**2&lt;100, yourlist))\n# Output \n[2, 5, 8]\nExample: Filter w/logical\nimport pandas as pd\nimport datetime as dt\n# create a list of 10,000 dates\ndatlist = pd.date_range(dt.datetime.today(), periods=10000).tolist() \n# convert the dates to strings via list comprehension\ndatstrlist = [d.strftime(\"Day %d in %B of year %Y is a %A\") for d in datlist]\ndatstrlist[:4]\n['Day 21 in October of year 2021 is a Thursday', 'Day 22 in October of year 2021 is a Friday', 'Day 23 in October of year 2021 is a Saturday', 'Day 24 in October of year 2021 is a Sunday']\n\nstrLamb = filter(lambda d: ((d.endswith(\"urday\") or d.endswith(\"unday\")) and \"Oc\" in d), datstrlist)\n\nSearches for Saturdays and Sundays in the month of October of all years in list of strings\n\nExample: Nested Lists\ngroup1 = [1,2,3,43,23,42,8,3,7]\ngroup2 = [[3, 34, 23, 32, 42], [6, 11, 9], [1, 3,9,7,2,8]]\n[list(filter(lambda x: x in group1, sublist)) for sublist in group2]\n&gt;&gt; [[3, 23, 42], [], [1, 3, 7, 2, 8]]\n\nProbably useful for json\nfor-loop attached to the end of the list-filter combo\nEach sublist of group 2 is fed into the lambda-filter and compared to the group 1 list\n\n\nIterating over each element of a list\n\nExample: map\nlist(map(lambda x: x**2+x**3, yourlist))\n\nmap returns a map object that needs to be converted\n\nExample: 2 Lists\nmylist = list(np.arange(4,52,3))\nyourlist = list(np.arange(2,50,3))\nlist(map(lambda x,y: x**2+y**2, yourlist, mylist))\n\nLike a pmap\n\n\nNested lambdas\n\nExample: map\narr = [1,2,3,4,5]\nlist(map(lambda x: x*2, filter(lambda x: x%2 == 0, arr)))\n&gt;&gt; [4,8]\n\nWork inside out (locate where the data object, arr, appears)\n“arr” is filtered by the first lambda function for even numbers then iterated by map to be squared by the second lambda function\n\n\nIterate over rows of a column in a df\n\nExample: Using formula over rows\ngrade['evaluate']=grade['MathScore'].apply(lambda x: round((x**x)/2,2))\n\n“grade” is the df; “MathScore” is a numeric column; “evaluate” is the new column in the df\nFormula applied to each value of “MathScore” to generate each value of evaluate\n\nExample: Conditional over rows\ngrade['group']=grade['MathScore'].apply(lambda x: 'Excellent' if x&gt;=3.0 else 'Average')\n\n“grade” is the df; “MathScore” is a numeric column; “group” is the new column in the df\nConditional applied to each value of “MathScore” to generate each value of “group”\n\nUsing {{swifter}} for parallelization\nimport swifter\ndf['e'] = df.swifter.apply(lambda x: infer(x['a'], x['b'], x['c'], x['d']), axis = 1)\n\nIn a Pivot Table (like a crosstab)\n\nExample\n\ngrades_df\n\n2 names (“name”)\n6 scores (“score”)\nOnly 2 letter grades associated with these scores (“letter grade”)\n\nTask: drop lowest score for each letter grade, then calculate the average score for each letter grade\n\ngrades_df.pivot_table(index='name',\n                      columns='letter grade',\n                      values='score',\n                      aggfunc = lambda series : (sorted(list(series))[-1] + sorted(list(series))[-2]) / 2)\n\nletter grade    A    B\nname\nArif          96.5  87.0\nKayla        95.5  84.0\n\nindex: each row will be a “name”\ncolumns: each column will be a “letter grade”\nvalues: value in the cells will be from the “score” column according to each combination columns in the index and columns args\naggfunc: uses a lambda to compute the aggregated values\n\n“series” is used a the variable  in the lambda function\nsorts series (ascending), takes the top two values (using negative list indexing), and averages them\n\n\n\n\n\n\nScope\n\nPopulated objects within functions persist if you instantiate the object in the argument\n\n\n“all_numbers” retained it’s previous value when the 2nd call to the function was made\n\n\n\n\nClosures\n\nInner functions that can access values in the outer function, even after the outer function has finished its execution\nExample\n\n# closure way\ndef balanceOwed(roomN,rate,nights):\n    def increaseByMeals(extra):\n        amountOwned=rate*nights+extra\n        print(f\"Dear Guest of Room [{roomN}]{style='color: #990000'}, you have\", \n        \"a due balance:\", \"${:.2f}\".format(amountOwned))\n        return amountOwned\n    return increaseByMeals\n\nba = balanceOwned(201,400,3)\nba(200)\nba(150)\nba(180)\nba(190)\nDear Guest of Room 201, you have a due balance: $1400.00\nDear Guest of Room 201, you have a due balance: $1350.00\nDear Guest of Room 201, you have a due balance: $1380.00\nDear Guest of Room 201, you have a due balance: $1390.00\n\nTedious way: For each value of “extra” (e.g. meals), the function needs to be called even if the other values of the arguments don’t change.\nClosure way:\n\nincreaseByMeals() is a closure function, because it remembers the values of the outer function balanceOwed(), even after the execution of the latter\nbalanceOwed() is called with its three arguments only once and then after its execution, we call it four times with the meal expenses (“extra”).",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-mods",
    "href": "qmd/python-general.html#sec-py-gen-mods",
    "title": "General",
    "section": "Modules",
    "text": "Modules\n\n.py files are called “modules.”\nA directory with .py files in which one of the files is an “__init__.py” is called a package.\nMisc\n\nResource: Make your Python life easier by learning how imports find things\nsys.path contains the list of paths where Python is looking for things to import. Your virtual environment and the directory containing your entry point are automatically added to sys.path.\n\nsys.path is a list. Which means you can .append(). Any directory you add there will have its content importable. It’s a useful hack, but use it as a last resort.\n\nWhen using -m flag to run a script, if you pass a package instead of a module, the package must contain a “__main__.py” file for it to work. This __main__.py module will run.\nIf you have scripts in your projects, don’t run them directly. Run them using “-m”, and you can assume everything starts from the root.\n\nExample:\ntop_dir\n├── foo\n│   ├── bar.py\n│   ├── __init__.py\n│   └── blabla.py\n└── blabla.py\n\nRunning python foo/bar.py, “top_dir” is the current working directory, but “foo” is added to the sys.path.\nRunning python -m foo.bar, “top_dir” is the current working directory and added to sys.path.\n\nImports can all start from the root of the project and opened file paths as well.\n\n\n\n\nUsage\n\nProject Structure\n├── main.py\n├── packages\n│  └── __init__.py\n│  └── module_1.py\n│  └── module_2.py\n│  └── module_3.py\n└── └── module_4.py\n\n“__init__.py” contains only 1 line which declares all the functions (or classes?) that are in the modules\n__all__ = [\"func1\", \"func2\"]\n\nIf the module files contained classes with multiple functions, I think you’d just declare the classes and not every function in that class.\n\nIf using classes, each module should only have 1 class.\n\n\nScripts need to include “_main_” in order to used in other scripts\n# test_function.py\ndef function1(): \n    print(\"Hello world\") \nfunction1()\n\n# Define the __main__ script\nif __name__ == '__main__':   \n    # execute only if run as a script\n    function1()\n\nSays if this file is being run non-interactively (i.e. as a script), run this chunk\nAdd else: chunk, then that chunk will be run only if the file is imported as a module\nAllows you to allow or prevent parts of code from being run when the modules are imported\nImporting a module without _main_ in a jupyter notebook results in this\n\n\nLoading\n\nDO NOT USE from &lt;library&gt; import *\n\nThis will import anything and everything from that library and causes several problems:\n\nYou don’t know what is in that package, so you have no idea what you just imported, or even if what you want is in there.\nYou just filled your local namespace with an unknown quantity of mysterious names, and you don’t know what they will shadow.\nYour editor will have a hard time helping you since it doesn’t know what you imported.\nYour colleague will hate you because they have no idea what variables come from where.\n\nException: In the shell, it’s handy. Sometimes, you want to import all things in __init__.py and you have “__all__” defined (see above)\n\nFrom the working directory, it’s like importing from a library: from file1 import function1\nFrom a subdirectory, from subdirectory.file1 import function1\nFrom a directory outside the project, add the module to sys.path before importing it\nimport sys\nsys.path.append('/User/moduleDirectory')\n\nWhen a module is imported, it first searches for built-in modules, then the paths listed in sys.path\nThis appends the new path to the end of the sys.path\nimport sys\nsys.path.insert(1, '/User/moduleDirectory')\nPuts this path at the front of the sys.path directory list.\nimport sys\nsys.path.remove('/User/NewDirectory')\n\n*delete path from sys.path after you finish*\nPython will also search this path for future projects unless they are removed",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-cond",
    "href": "qmd/python-general.html#sec-py-gen-cond",
    "title": "General",
    "section": "Conditionals",
    "text": "Conditionals\n\nIf-Else\n\nSyntax\nif &lt;expression&gt;:\n    do something\nelse:\n    do something else\nExample\nregenerate = False\nif regenerate:\n    concepts_list = df2Graph(df, model='zephyr:latest')\n    dfg1 = graph2Df(concepts_list)\n    if not os.path.exists(outputdirectory):\n        os.makedirs(outputdirectory)\n\n    dfg1.to_csv(outputdirectory/\"graph.csv\", sep=\"|\", index=False)\n    df.to_csv(outputdirectory/\"chunks.csv\", sep=\"|\", index=False)\nelse:\n    dfg1 = pd.read_csv(outputdirectory/\"graph.csv\", sep=\"|\")\n\nTry-Except\n\nExample\nimport os\ntry:\n    env_var = os.environ['ENV']\nexcept KeyError:\n    # Do something\n\nIf “ENV” is not a present a KeyError is thrown. Then, except section executed.\n\n\nMatch (&gt; Python 3.10) (switch function)\nmatch object:\n    case &lt;pattern_1&gt;:\n        &lt;action_1&gt;\n    case &lt;pattern_2&gt;:\n        &lt;action_2&gt;\n    case &lt;pattern_3&gt;:\n        &lt;action_3&gt;\n    case _:\n        &lt;action_wildcard&gt;\n\n“object” is just a variable name; could be anything\n“case_” is the value used when none of the other cases are a match\nExample: function input inside user function\ndef http_error(status):\n    match status:\n        case 200:\n            return 'OK'\n        case 400:\n            return 'Bad request'\n        case 401 | 403 | 404:\n            return 'Not allowed'\n        case _:\n            return 'Something is wrong'\nExample: dict input inside a function\ndef get_service_level(user_data: dict):\n    match user_data:\n        case {'subscription': _, 'msg_type': 'info'}:\n            print('Service level = 0')\n        case {'subscription': 'free', 'msg_type': 'error'}:\n            print('Service level = 1')\n        case {'subscription': 'premium', 'msg_type': 'error'}:\n            print('Service level = 2')\nExample: inside a class\nclass ServiceLevel:\n    def __init__(self, subscription, msg_type):\n        self.subscription = subscription\n        self.msg_type = msg_type\n\n    def get_service_level(user_data):\n        match user_data:\n            case ServiceLevel(subscription=_, msg_type='info'):\n                print('Level = 0')\n            case ServiceLevel(subscription='free', msg_type='error'):\n                print('Level = 1')\n            case ServiceLevel(subscription='premium', msg_type='error'):\n                print('Level = 2')\n            case _:\n                print('Provide valid parameters')\n\nNote that inside the function, the change from “:” to “=”  and “()” following the class name in the “case” portion of the match\n\n\nAssert\n\nUsed to confirm a condition\n\nIncorrect: assert condition, message \n\nCorrect method: \nif not condition: \n    raise AssertionError\n\nassert is useful for debugging code because it lets you test if a condition in your code returns True, if not, the program will raise an AssertionError.\n** Do not use in production, because when code is executed with the -O (optimize) flag, the assert statements are removed from the bytecode. **",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-loops",
    "href": "qmd/python-general.html#sec-py-gen-loops",
    "title": "General",
    "section": "Loops",
    "text": "Loops\n\nMisc\n\nList Comprehensions vs Generators in terms of memory usage\n\n{{tqdm}} - progress bar for loops\nfrom tqdm import tqdm\nfor i in tqdm(range(10000))\n    ...\nbreak terminates the loop containing it\n\nIf in a nested loop, it will terminate the inner-most loop containing it\n\ncontinue is used to skip the remaining code inside a loop for the current iteration only; forces the start of the next iteration of the loop\npass does nothing\n\nused when a statement or a condition is required to be present in the program but we do not want any command or code to execute\n\n\n\n\nIterators\n\nRemembers values\nExample\nD = {\"123\":\"Y\",\"111\":\"PT\",\"313\":\"Y\",\"112\":\"Y\",\"201\":\"PT\"}\nff = filter(lambda e:e[1]==\"Y\", D.items())\n\nprint(next(ff))\n&gt;&gt; ('123', 'Y')\nprint(next(ff))\n&gt;&gt; ('313', 'Y')\napply\n\naxis\n\n0 or ‘index’: apply function to each column.\n1 or ‘columns’: apply function to each row.\n\nExample: Function applied to rows of a column of a dataframe (i.e. cells)\ndef df2Graph(dataframe: pd.DataFrame, model=None) -&gt; list:\n  # dataframe.reset_index(inplace=True)\n  results = dataframe.apply(\n    lambda row: graphPrompt(row.text, {\"chunk_id\": row.chunk_id}, model), axis=1\n  )\n\ntext and chunk_id are column names of the dataframe\nrow is the row of the dataframe since axis=1, and from that row, the columns text and chunk_id are subsetted in the arguments of user-defined function.\n\nExample: Formula applied to rows of a column of a dataframe (i.e. cells)\ngrade['evaluate']=grade['MathScore'].apply(lambda x: round((x**x)/2,2))\n\ngrade is the df; MathScore is a numeric column; evaluate is the new column in the df\n\n\n\n\n\nGenerators\n\nGenerators are iterators, a kind of iterable you can only iterate over once. (normal iterators like lists, strings, etc. can be repeatedly iterated over)\nGenerators do not store all the values in memory, they generate the values on the fly\n\nyield - Pauses the function saving all its states and later continues from there on successive calls.\n\nAllows you to consume one element at a time and work with it without requiring you to have every element in memory.\nProduces a generator\n\n\nMisc\n\n{{itertools}} islice can slice a generator.\nAlso see APIs &gt;&gt; {{requests}} for an example\n\nExample: Using a comprehension \nmygenerator = (x*x for x in range(3))\nfor i in mygenerator:\n...    print(i)\n\nProduce a list and ( ) produce a generator \n\nExample: Using a function\ndef create_generator():\n    mylist = range(3)\n    for i in mylist:\n        yield i*i\n\nfor i in mygenerator:\n    print(i)\n0\n1\n4\n\nThe first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then it’ll return the first value of the loop.\nThen, each subsequent call will run another iteration of the loop you have written in the function and return the next value.\nThis will continue until the generator is considered empty, which happens when the function runs without hitting yield.\n\nThat can be because the loop has come to an end, or because you no longer satisfy an “if/else”\n\n\nExample: Sending values to (yield)\ndef grep(pattern):\nprint \"Looking for %s\" % pattern\nwhile True:\n    line = (yield)\n    if pattern in line:\n        print line,\ng = grep(\"python\")  # instantiate with \"python\" pattern to search for\n\ng.next() # Prime it\n&gt;&gt; Looking for python\n\ng.send(\"A series of tubes\") # \"python\" not present so returns nothing\ng.send(\"python generators rock!\") # \"python\" present so returns line\n&gt;&gt; python generators rock!\ng.close() # closes coroutine\n\n(yield) receives the input of the .send method and creates a generator object which is assigned to “line”.\nAll coroutines must be “primed” by first calling .next() (or send(None))\n\nThis advances execution to the location of the first yield expression\n\n\nExample: Sending values to (yield)\ndef writer():\n    \"\"\"A coroutine that writes data *sent* to it to fd, socket, etc.\"\"\"\n    while True:\n        w = (yield)\n        print('&gt;&gt; ', w)\ndef writer_wrapper(coro):\n    # TBD\n    pass\nw = writer()\nwrap = writer_wrapper(w)\nwrap.send(None)  # \"prime\" the coroutine\nfor i in range(4):\n    wrap.send(i)\n&gt;&gt;  0\n&gt;&gt;  1\n&gt;&gt;  2\n&gt;&gt;  3\n\nA more complex framework if you want to break the workflow into multiple functions\n\n\n\nUsing yield from\n\nAllows for two-way usage (reading/sending) of generators\nExample (reading from a generator)\ndef reader():\n    \"\"\"A generator that fakes a read from a file, socket, etc.\"\"\"\n    for i in range(4):\n        yield '&lt;&lt; %s' % i\n\n# with yield\ndef reader_wrapper(g):\n    # Manually iterate over data produced by reader\n    for v in g:\n        yield v\n# OR with yield from\ndef reader_wrapper(g):\n    yield from g\nwrap = reader_wrapper(reader())\nfor i in wrap:\n    print(i)\n\nBasic; only eliminates 1 line of code\n\nExample (sending to a generator)\n# with (yield)\ndef writer_wrapper(coro):\n    coro.send(None)  # prime the coro\n    while True:\n        try:\n            x = (yield)  # Capture the value that's sent\n            coro.send(x)  # and pass it to the writer\n        except StopIteration:\n            pass\n# OR with yield from\ndef writer_wrapper(coro):\n    yield from coro\n\nNeed to see example 4 for the writer() code and the use case\nShows the other advantage of using “yield from”: it automatically includes the code to stop prime and stop the loop.\n\nReusable generator\n\n\nreading example using “yield from”\n\nSlicing a generator\nfrom itertools import islice\ndef gen():\n    yield from range(1,11)\ng = gen()\nmyslice = islice(g, 2)\n&gt;&gt; list(myslice)\n[1, 2]\n&gt;&gt; [i for i in g]\n[3,4,5,6,7,8,9,10]\n\n\n\n\nFor\n\n\nSyntax - for &lt;sequence&gt;: &lt;loop body&gt;\nNumeric Range\nfor i = 1 to 10\n    &lt;loop body&gt;\n\n# from 0 to 519\nfor i in range(520)\n    &lt;loop body&gt;\n\nres = 0\nfor idx in np.arange(0, 100000):\n  res += df.loc[idx, 'int']\n\nnp.arange() ran 8000 times faster than the same chunk using range()\n\nList\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\neven_numbers = []\nfor item in numbers:\n    if item % 2 == 0:\n        even_numbers.append(item)\nprint(even_numbers)\n\n# results: [2, 4, 6, 8]\nList: index and value\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nfor index, element in enumerate(numbers):\n    if element % 2 != 0:\n        numbers[index] = element * 2\n    else:\n        continue\nprint(numbers)\n# results: [2, 2, 6, 4, 10, 6, 14, 8, 18]\n\nenumerate also gets the index of the respective element at the same time\n\nWith three expressions\nfor (for i = 1; i &lt;= 10; i+=1)\n    &lt;loop body&gt;\nCollection-Based\n\nIf the collection is a dict, then this just iterates over the keys\n\nfor i in &lt;collection&gt;:\n    &lt;loop body&gt;\nIterate over a sliding window\n\nOver dictionary keys and values of a dict\nfor a,b in transaction_data.items():\n    print(a,’~’,b)\n\nThe .items method includes both key and value, so it iterates over the pairs.\n\nOver nested dictionaries\nfor k, v in transaction_data_n.items():\n    if type(v) is dict:\n        for nk, nv in v.items(): \n            print(nk,’ →’, nv)\n\nIf the item of the dict is itself a dict then another loop iterates through its items.\nnk and nv stand for nested key and nested value\n\nSelecting a specific item in a nested dictionary\nfor k, v in transaction_data_n.items():\n    if type(v) is dict and k == 'transaction_2':\n        for sk, sv in v.items():\n            print(sk,'--&gt;', sv)\n\nOnly transaction_2’ s items are printed\n\nRows of a data.frame\nres = 0\nfor row in df.itertuples():\n  res += getattr(row, 'int')\n\nitertuples()  is 30x faster than iterrows()\n\n\n\n\nzip\n\nCombine lists into 1 list of tuples\nacc_values = [1, 0.04, 0.9]\nacc_names = [\"RMSE\", \"MAPE\", \"R-sq\"]\nacc_list = list(zip(acc_names, acc_values))\nacc_list\n[('RMSE', 1), ('MAPE', 0.04), ('R-sq', 0.9)]\n\nzip does take lists of different lengths but will create shortest length list with corresponding elements\nCombine lists of unequal lengths but keep the non-paired elements\nfrom itertools import zip_longest\nacc_names3 = [\"RMSE\", \"MAPE\", \"R-sq\", \"MSE\"]\nacc_values3 = [rmse, mape, rsq] \nacc_list3 = list(zip_longest(acc_names3, acc_values3))\n\nUnzip list of tuples into separate lists\nnames, values = zip(*acc_list)\n\nAsterisk is the “unzipping operator”\n\nUnpack dict into a list of separate tuples for key:value pairs\nacc_tuples = list(zip(acc_dict.keys(), acc_dict.values()))\nacc_tuples\n[('RMSE', 1), ('MAPE', 0.04), ('R-sq', 0.9)]\n\n\n\nComprehensions\n\nMisc\n\n‘for — in’ construct within comprehensions is faster than the traditional for-loops\n\nnot faster than (all?) lambda-filters (see functions &gt;&gt; lambda)\n\nReturns lists or dicts (just change the bracket types)\n\nDicts\n\nSyntax: mydict = {key:val for key, val in zip(keys_list, vals_list)}\nCombine key:value lists into a dictionary\nacc_dict = {k:v for k,v in zip(acc_names, acc_values)}\nReturn value and output of expression\nmydict = {v: v**2 for v in numberslist}\nIf numberslist =[1,2,3], then mydict = {1:1, 2:4, 3:9}\n\nLists\n\nSyntax: newlist = [expression for item in iterable if condition == True]\nWith expression\nmylist = [x**2 for x in numberslist]\n\nif numberslist =[1,2,3], then mylist = [1,4,9]\n\nSet values in a list to uppercase\nnewlist = [x.upper() for x in fruits]\nWith conditional expression (if — else)\n\nAppend to the comprehension to filter the dictionary or list\nSyntax: mylist = [expressionA if (condition2==True) else expressionB for item in list if (condition1==True)]\nExample: newlist = [x if x != \"banana\" else \"orange\" for x in fruits]\n\nReturn “orange” instead of “banana”\n\nExample: new_list = [(x**2) if (x&gt;90) else (x**3) for x in old_list if (x%2==0)]\n\nSays\n\nSquare an argument if it exceeds 90, else cube it \nReturn all the exponentiated results only if the argument was an even number\n\n\nExample: c = [d for d in datstrlist if ((d.endswith(\"urday\") or d.endswith(\"unday\")) and \"Oc\" in d)]\n\nString filter than looks for strings with saturdays and sundays in october\n*Slower than a lamda-filter* (See Functions &gt;&gt; lambda)\n\n\n\nNested\n\nSyntax: myset = {{expression(itemA, itemB) for itemA in setA} for itemB in setB}\nExample: {j for i in range(2, int(N**0.5)+1) for j in range(i**2, N, i)}\n\nN = 100000\nCreates a set of all the integers from 2 to 100,000.\nPaces through all the integers i up to the square root of N\nDiscards from the set of 100,000 those numbers j which are equal or larger than the square of i\n\nExample: From link\n# Function to get set labels\ndef get_prediction_set_labels(prediction_set, class_labels):\n    # Get set of class labels for each instance in prediction sets\n    prediction_set_labels = [\n        set([class_labels[i] for i, x in enumerate(prediction_set) if x]) for prediction_set in \n        prediction_sets]\n    return prediction_set_labels\n\nReturns a list where each object in the list is a set object (e.g. {green}, {green, orange})",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-debug",
    "href": "qmd/python-general.html#sec-py-gen-debug",
    "title": "General",
    "section": "Debugging",
    "text": "Debugging\n\nMisc\nTerms\n\nException Errors - Raised when the syntax is correct but the program results in an error.\nSyntax Errors - Occur when the interpreter detects invalid syntax (relatively easier to fix)\n\ne.g. unmatched parenthesis\n\nTraceback - A report that helps us understand the reason for an exception.\n\nContains function calls made in the code along with their line numbers",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-errhand",
    "href": "qmd/python-general.html#sec-py-gen-errhand",
    "title": "General",
    "section": "Error Handling",
    "text": "Error Handling\n\ntry + except\n\nSays try the main code snippet, but if an exception (error) occurs, run the secondary code snippet, the workaround.\n\ndef pct_difference_error_handling(n1, n2):\n  '''Function that takes two numbers and return the percentual difference\n  between n1 and n2, being n1 the reference number'''\n\n  # Try the main code\n  try:\n    pct_diff = (n1-n2)/n1\n    return f'The difference between {n1} and {n2} is {n1-n2}, which is {pct_diff*100}% of {n1}'\n\n  # If you find an error, use this code instead\n  except:\n    pct_diff = (int(n1)-int(n2))/int(n1)\n    return f'The difference between {n1} and {n2} is {int(n1)-int(n2){style='color: #990000'}[}]{style='color: #990000'}, which is {pct_diff*100}% of {n1}'\n\n  # Optional\n  finally:\n    print(\"Code ended\")\n\nAssumes the error will be the user enters a string instead of a numeric. If errors, converts string to numeric and calcs.\nfinally: - This argument will always run, regardless if the try block raises an error or not. So it could be a completion message or a summary, for example.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/distributions.html",
    "href": "qmd/distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "Terms",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-terms",
    "href": "qmd/distributions.html#sec-distr-terms",
    "title": "Distributions",
    "section": "",
    "text": "Conditional Probability Distributions\n\nNotes from https://www.causact.com/joint-distributions-tell-you-everything.html#joint-distributions-tell-you-everything\nNotation: \\(P(Y | X) = P(Y \\;\\text{and}\\; X) / P(X) = P(Y, X) / P(X)\\)\n\ni.e. ratio of 2 marginal distributions\n\nExample: two tests for cancer are conducted to determine whether a biopsy should be performed\n\nConditional approach: Biopsy everyone at determined to be high risk from test 1; measure the genetic marker (aka test 2) for patients at intermediate risk and biopsy those with a probability of cancer past a certain level based on the marker\n\nWhen we perform regression analysis, we are essentially estimating conditional distributions. The conditional distribution, \\(P(Y|X_1, \\ldots, X_n)\\) represents the distribution of the response variable, \\(Y\\), given the specific values of the predictor variables, \\(X_1, \\ldots, X_n\\).\n\nEmpirical CDF\n\\[\nF_n (x) = \\frac {1}{n} \\sum_{i = 1}^n I(X_i \\leq x)\n\\]\n\nWhere \\(X_1, X_2,\\ldots,X_n\\) are from a population with CDF, \\(F_n (x)\\)\nProcess\n\nTake n samples from an unknown distribution. The more samples you take, the closer the empirical distribution will resemble the true distribution.\nSort these samples, and place them on the x-axis.\nStart plotting a ‘step-function’ style line — each time you encounter a datapoint on the x-axis, increase the step by 1/N.\n\nExample\n\n\nThe CDF of a normal distribution (green) and its empirical CDF (blue)\n\n\nJoint Probability Distribution - Assigns a probability value to all possible combinations of values for a set of random variables.\n\nNotation: \\(P(x_1, x_2, ... ,x_n)\\)\nPlugging in a value for each random variable returns a probability for that combination of values\nExample: Two tests for cancer are conducted to determine whether a biopsy should be performed\n\nJoint approach: biopsy anyone who is either at high risk of cancer (test 1) or who was determined to have a probability of cancer past a certain level, based on the marker from the genetic test (test 2)\nCompare with example in Conditional Probability Distributions\n\nIn the context of regression modeling: the joint distribution refers to the distribution of all the variables involved in the regression analysis. For example, if you have a regression model with a response variable \\(Y\\) and predictor variables \\(X_1, \\ldots, X_n\\), the joint distribution would describe the combined distribution of \\(Y, X_1, \\ldots, X_n\\).\n\nLocation - Distribution parameter determines the shift of the distribution\n\ne.g. mean, mu, of the normal distribution.\n\nMarginal Probability Distribution - Assigns a probability value to all possible combinations of values for a subset of random variables\n\nNotation: \\(P(x_1)\\)\n\n\\(P(x_1,x_2)\\) is sometimes called the Joint Marginal Probability Distribution\n\nThe marginal distribution, \\(P(Y)\\) where \\(Y\\) is a subset of random variables, is calculated from the joint distribution, \\(P(Y = y, Z = z)\\) where \\(Z\\) is the subset of random variables not in \\(Y\\) .\n\n\\(P(Y) = \\sum_{Z=z} P(Y = y, Z = z)\\)\n\nIf \\(Y\\) is just one variable\n\nSays sum all the joint probabilities for all the combinations of values for the variables in \\(Z\\) while holding \\(Y\\) constant\nRepeat for each value of \\(Y\\) to get this summed probability value\nThe marginal distribution is made up of all these values, one for each value of \\(Y\\) (or combination of values if \\(Y\\) is a subset of variables)\n\n\nWhen the joint probability distribution is in tabular form, one just sums up the probabilities in each row where \\(Y = y\\).\nIn the context of regression modeling, the marginal distribution of \\(Y\\) represents the distribution of \\(Y\\) alone, without considering the specific values of the predictor variables.\n\n\nScale - Distribution parameter; the larger the scale parameter, the more spread out the distribution\n\ne.g. s.d., sigma, \\(\\sigma\\) of the normal distribtution\nRate Parameter: the inverse of the scale parameter (see Gamma distribution)\n\nShape - Distribution parameter that affects the shape of a distribution rather than simply shifting it (as a location parameter does) or stretching/shrinking it (as a scale parameter does).\n\ne.g. “Peakedness” refers to how round the main peak is",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-tests",
    "href": "qmd/distributions.html#sec-distr-tests",
    "title": "Distributions",
    "section": "Tests",
    "text": "Tests\n\nWhy normality tests are great… as a teaching example and should be avoided in research\n\ntl;dr; KS test has very low power as a Normality test as compared to Shapiro-Wilk, and Shapiro-Wilk isn’t very good for n &lt; 100\nFor detecting moderate skew, you want at least n &gt; 75 to get 80% power for Shapiro-Wilk\nShapiro-Wilk can detect very fat tails at n &lt; 100, but would require larger sample sizes to detect more moderately thick tails.\nKS is worthless in detecting fat tails and near-worthless at detecting skew\nWhen n gets large (e.g. 1000s), these types of tests will almost always reject the null even when the practical deviation from normality is not practically significant.\n\nKolmogorov–Smirnov test (KS)\n\nUsed to compare distributions\n\nCan be used as a Normality test or any distribution test\nCan compare two samples\n\nMisc\n\nVectors may need to be standardized (e.g. normality test) first unless comparing two samples H0: Both distributions are from the same distribution\n\nPackages\n\n{KSgeneral} has tests to use for contiuous, mixed, and discrete distributions written in C++\n{stats} and {dgof} also have functions, ks.test\n\nBoth handle continuous and discrete distributions\n\nAll functions take a numeric vector and a base R density function (e.g. pnorm, pexp, etc.) as args\n\nKSgeneral docs don’t say you can supply your own comparison sample (2nd arg) only the density function but with stats and dgof, you can.\nAlthough they have function to compute the CDFs, so if you need speed, it might be possible to use their functions and do it manually\n\n\n2-sample test as the greatest distance between the CDFs (Cumulative Distribution Function) of each sample\n\nSpecifically, this test determines the distribution of your unknown data sample by constructing and comparing the sample’s empirical CDF  (see Terms) with the CDF you hypothesized. If the two CDFs are close, your unknown data sample likely follows the hypothesized distribution.\n\nKS statistic, \\(D_{n,m} = \\max|\\text{CDF}_1 - \\text{CDF}_2|\\) where \\(n\\) as the number of observations on Sample 1 and \\(m\\) as the number of observations in Sample 2\nCompare the KS statistic with the respective KS distribution based on parameter “en” to obtain the p-value of the test\n\n\\(en = (m \\times n) / (m + n)\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-beta",
    "href": "qmd/distributions.html#sec-distr-beta",
    "title": "Distributions",
    "section": "Beta",
    "text": "Beta\n\n\nDefined on the interval [0,1]\nThe key difference between the Binomial and Beta distributions is that for the Beta distribution the probability, x, is a random variable, however for the Binomial distribution the probability, x, is a fixed parameter.\nShape parameters are \\(\\alpha\\) and \\(\\beta\\), usually.\n\n\\(\\alpha\\) and \\(\\beta\\) are two positive parameters that appear as exponents of the random variable\n\npdf\n\\[\nf(x) = \\frac {x^{\\alpha - 1} (1-x)^{\\beta - 1}} {B(\\alpha, \\beta)}\n\\]\n\\(\\mathbb{E}(X) = \\frac {\\alpha} {\\alpha + \\beta}\\)\n\\(\\text{Var}(X) = \\frac {\\alpha \\cdot \\beta} {(\\alpha + \\beta)^2 \\cdot (\\alpha + \\beta + 1)}\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-betbin",
    "href": "qmd/distributions.html#sec-distr-betbin",
    "title": "Distributions",
    "section": "Beta-Binomial",
    "text": "Beta-Binomial\n\n\n\n\n\n\n\n\nWhere k is the number of events in n trials\n\n\n\n\n\n\n\nWhere \\(\\theta\\) is the probability of an event\n\n\n\n\n\n\n\nUsed when the probability of success, p, in a fixed number of Bernoulli trials is unknown or random and can change from trial to trial.\nShape parameters α and β define the probability of success (i.e. the success parameter is modeled by the Beta Distribution).\n\nFor large values of α and β, the distribution approaches a binomial distribution.\nWhen α and β both equal 1, the distribution equals a discrete uniform distribution from 0 to n\n\nAccuracy analysis data from psychology follow beta-binomial distributions (Jaeger, 2008; Kruschke, 2014)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-dirichlet",
    "href": "qmd/distributions.html#sec-distr-dirichlet",
    "title": "Distributions",
    "section": "Dirichlet",
    "text": "Dirichlet\n\nA family of continuous multivariate probability distributions parameterized by a vector α of positive reals",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-exp",
    "href": "qmd/distributions.html#sec-distr-exp",
    "title": "Distributions",
    "section": "Exponential",
    "text": "Exponential\n\n\nNotes from\n\nStatistical Rethinking &gt;&gt; Chapter 10\n\nConstrained to be zero or positive\nFundamental distribution of distance and duration, kinds of measurements that represent displacement from some point of reference, either in time or space.\nIf the probability of an event is constant in time or across space, then the distribution of events tends towards exponential.\nIts shape is described by a single parameter, the rate of events \\(\\lambda\\), or the average displacement \\(\\lambda −1\\) .\nThis distribution is the core of survival and event history analysis",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gamma",
    "href": "qmd/distributions.html#sec-distr-gamma",
    "title": "Distributions",
    "section": "Gamma",
    "text": "Gamma\n\n\nNotes from\n\nStatistical Rethinking &gt;&gt; Chapter 10\n\nConstrained to be zero or positive\nLike Exponential but can have a peak above zero\nIf an event can only happen after two or more exponentially distributed events happen, the resulting waiting times will be gamma distributed.\n\ne.g. age of cancer onset is approximately gamma distributed, since multiple events are necessary for onset.\n\nThe gamma can be viewed as the sum of iid n exponential random variables. Exponential random variables have a rate parameter, so it makes sense for the Gamma to inherit a rate parameterization. The rate parameter also happens to be related to a scale parameter, so it makes sense for the Gamma to have a scale parameterization.\nShape parameter \\(k\\) and a scale parameter \\(\\theta\\)\n\\(\\mathbb{E}[X] = k\\theta = \\frac{\\alpha}{\\beta}\\)\n\nShape parameter \\(\\alpha = k\\) and an\nInverse Scale parameter (aka Rate Parameter) \\(\\beta = \\frac {1}{\\theta}\\)\nTherefore if you want a gamma distributions with a certain “mean” and “standard deviation,” you’d:\n\nSet your mean to \\(\\mathbb{E}[X]\\), your standard deviation to \\(\\theta\\) (probably but maybe it’s \\(\\beta\\))\nCalculate \\(\\beta\\)\nCalculate \\(\\alpha\\)\nprior(gamma(alpha, beta))\n\n\nExample: Gamma distribution as the sums of random exponential variables\n\nn &lt;- 12\nbeta &lt;- 1.2\n\nrvs &lt;- replicate(1000, {\n  sum(rexp(n, beta))\n})\n\nhist(rvs, freq = F)\ncurve(dgamma(x, shape = n, rate = beta), col='red', add=T)\n\nGamma distribution density overlayed with a histogram of exponential variable sums\n\nUsed in Survival Regression",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gauss",
    "href": "qmd/distributions.html#sec-distr-gauss",
    "title": "Distributions",
    "section": "Gaussian",
    "text": "Gaussian\n\nSpecial case of Student’s t-distribution with the \\(\\nu\\) parameter (i.e. degree of freedom) set to infinity.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gumb",
    "href": "qmd/distributions.html#sec-distr-gumb",
    "title": "Distributions",
    "section": "Gumbel",
    "text": "Gumbel\n\n\nKnown as the type-I generalized extreme value distribution\n\nEVT says it is likely to be useful if the distribution of the underlying sample data is of the normal or exponential type.\n\nUsed to model the distribution of the maximum (or the minimum) of a number of samples of various distributions.\n\nTo model minimums, use the negative of the original data.\n\nUse Cases\n\nRepresent the distribution of the maximum level of a river in a particular year if there was a list of maximum values for the past ten years.\nPredicting the chance that an extreme earthquake, flood or other natural disaster will occur.\nDistribution of the residuals in Multinomial Logit and Nested Logit models\n\nParameters\n\nGumbel(\\(\\mu, \\beta\\)) (location, scale)\nMean: \\(\\mu + \\beta\\gamma\\) where \\(\\gamma\\) is Euler’s constant (\\(\\approx\\) 0.5772)\nMedian: \\(\\mu - \\beta \\ln(\\ln(2))\\)\nMode: \\(\\mu\\)\nVariance: \\(\\frac{\\pi^2}{6}\\beta^2\\)\nStandard Gumbel: When \\(\\mu = 0\\), mean = \\(\\gamma\\), median = \\(-\\ln(\\ln(2)) \\approx 0.3665\\) and the standard deviation = \\(\\pi/\\sqrt{6}\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-multgauss",
    "href": "qmd/distributions.html#sec-distr-multgauss",
    "title": "Distributions",
    "section": "Multivariate Gaussian",
    "text": "Multivariate Gaussian\n\nIf the random variable components in the vector are not normally distributed themselves, the result is not multivariate normally distributed.\nVariance-Covariance matrix must be semi-definite and therefore symmetric\n\nExample of not symmetric for two random variables",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-pareto",
    "href": "qmd/distributions.html#sec-distr-pareto",
    "title": "Distributions",
    "section": "Pareto",
    "text": "Pareto\n\nAlso see Extreme Value Theory &gt;&gt; Distribution Tail Classification\n“Gaussian distributions tend to prevail when events are completely independent of each other. As soon as you introduce the assumption of interdependence across events, Paretian distributions tend to surface because positive feedback loops tend to amplify small initial events.”\nPareto has similar relationship with the exponential distribution as lognormal does with normal \\[\nY_{exp} = \\log \\frac {X_{pareto}} {x_m}\n\\]\n\nWhere \\(X_{pareto} = x_m e^{Y_{\\text{exp}}}\\)\n\n\\(x_m\\) is the (positive) minimum of the randomly distributed pareto variable, X that has index α\n\\(Y_{exp}\\) is exponentially distributed with rate \\(\\alpha\\)\n\n\nSome theoretical statistical moments may not exist\n\nIf the theoretical moments do not exist, then calculating the sample moments is useless\nExample: Pareto (\\(\\alpha\\) = 1.5) has a finite mean and an infinite variance\n\nNeed \\(\\alpha &gt; 2\\) for a finite variance\nNeed \\(\\alpha &gt; 1\\) for a finite mean\nIn general you need \\(\\alpha &gt; p\\) for the pth moment to exist\nIf the nth moment is not finite, then the (n+1)th moment is not finite.\n\n\nFat Tails \\[\n\\bar{F} = x^{-\\alpha} L(x)\n\\]\n\n\\(L(x)\\) is just characterized as slowly varying function that gets dominated by the decaying inverse power law element, \\(x-\\alpha\\). as \\(x\\) goes to infinity\n\n\\(\\alpha\\) is a shape parameter, aka “tail index” aka “Pareto index”",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-poisson",
    "href": "qmd/distributions.html#sec-distr-poisson",
    "title": "Distributions",
    "section": "Poisson",
    "text": "Poisson\n\nObtained as the limit of the binomial distribution when the number of attempts is high and the success probability low. Or the Poisson distribution can be approximated by a normal distribution when λ is large\nProbability Mass Function \\[\n\\text{Pr}(Y = y) = f(y; \\lambda) = \\frac {e^{-\\lambda} \\cdot \\lambda^y} {y!}\n\\]\n\n\\(\\mathbb{E}[Y] = \\text{Var}(Y) = \\lambda\\)\n\n{distributions3}\n\nStats\nY &lt;- Poisson(lambda = 1.5) \nprint(Y) \n## [1] \"Poisson distribution (lambda = 1.5)\"\n\nmean(Y) \n## [1] 1.5 \nvariance(Y) \n## [1] 1.5 \npdf(Y, 0:5) \n## [1] 0.22313 0.33470 0.25102 0.12551 0.04707 0.01412 \ncdf(Y, 0:5) \n## [1] 0.2231 0.5578 0.8088 0.9344 0.9814 0.9955 \nquantile(Y, c(0.1, 0.5, 0.9)) \n## [1] 0 1 3 \nset.seed(0) \nrandom(Y, 5) \n## [1] 3 1 1 2 3\n\nVisualize\n\nplot(Poisson(0.5), main = expression(lambda == 0.5), xlim = c(0, 15)) \nplot(Poisson(2),   main = expression(lambda == 2),   xlim = c(0, 15)) \nplot(Poisson(5),   main = expression(lambda == 5),   xlim = c(0, 15)) \nplot(Poisson(10),  main = expression(lambda == 10),  xlim = c(0, 15))",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-studt",
    "href": "qmd/distributions.html#sec-distr-studt",
    "title": "Distributions",
    "section": "Student’s t-distribution",
    "text": "Student’s t-distribution\n\nStandard Deviation\n\\[\n\\text{sd} = \\sqrt {\\frac {\\nu} {\\nu - 2}}\n\\]\n\n\\(\\nu\\) = degrees of freedom\n\nWhen ν is small, the Student’s t-distribution is more robust to multivariate outliers\nThe smaller the degree of freedom, the more “heavy-tailed” it is\n\n\n-3 on the y-axis says that the probability of being in the tail is 1 in 103\n\nDon’t pay attention to the x-axis. Just note how much the probability of being in the tail gets larger as the dof get smaller\n\nAs the degrees of freedom goes to 1, the t distribution goes to the Cauchy distribution\nAs the degrees of freedom goes to infinity, it goes to the Normal distribution.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-tri",
    "href": "qmd/distributions.html#sec-distr-tri",
    "title": "Distributions",
    "section": "Triangular",
    "text": "Triangular\n\nTriangle shaped distribution\nUseful when you have a known min and max value\nextraDistr::rtriang(n, a, b, c) %\\&gt;% hist()\n\n# Discrete distribution\nextraDistr::rtriang(n, a, b, c) %\\&gt;% round() \\`\\`\\`\n\nn is the number of random values you wish to draw\na is the min value\nb is the max value\nc is the mode\n\nCan use to adjust the skew of the distribution\n\n\n\n\n\n\nWhere k is the number of events in n trials\nWhere \\(\\theta\\) is the probability of an event",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/mathematics-glossary.html",
    "href": "qmd/mathematics-glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "A priori - a type of knowledge that can be derived by reason alone\n\nA priori analyses are performed as part of the research planning process.\n\nA posteriori - a type of knowledge that expresses an empirical fact unknowable by reason alone.\n\nSame as post-hoc. Post-Hoc analysis is conducted after the experiment.\n\nbias - see unbiased estimator\nceteris paribus - latin for “all things being equal” or “other things held constant.”\nclosed form - a mathematical expression that uses a finite number of standard operations. It may contain constants, variables, certain well-known operations, and functions, but usually no limit, differentiation, or integration.\nconsistency - Requires that the outcome of the procedure with unlimited data should identify the underlying truth. Usage is restricted to cases where essentially the same procedure can be applied to any number of data items. In complicated applications of statistics, there may be several ways in which the number of data items may grow. For example, records for rainfall within an area might increase in three ways: records for additional time periods; records for additional sites with a fixed area; records for extra sites obtained by extending the size of the area. In such cases, the property of consistency may be limited to one or more of the possible ways a sample size can grow\ndegrees of freedom - When discussed about variable-sample size tradeoff, usually means n-p, where is the number of rows and p is the number of variables. The more variables used in the model the fewer degrees of freedom and therefore less power and precision.\nexchangeability - means we can swap around, or reorder, variables in the sequence without changing their joint distribution.\n\nEvery IID (independent, identically distributed) sequence is exchangeable - but not the other way around. Every exchangeable sequence is identically distributed, though\n\nExample: If you draw a sequence of red and blue marbles from a bag without replacement, the sample is exchangeable but not independent. e.g. drawing a red marble affects the probability of drawing a red or blue marble next.\n\n\nefficiency - A test, estimator, etc. is more efficient than another test, estimator, etc. if it requires fewer observation to obtain the same level of performance.\nergodicity - the idea that a point of a moving system, either a dynamical system or a stochastic process, will eventually visit all parts of the space that the system moves in, in a uniform and random sense\nexternal validity - Our estimates are externally valid if inferences and conclusions can be generalized from the population and setting studied to other populations and settings. (also see internal validity)\nidentifiable (aka point-indentifiable) - theoretically possible to learn the true values of this model’s underlying parameters after obtaining an infinite number of observations from it (see non-identifiability, partially-indentifiable)\nill-conditioned - In SVD decomposition, when there’s a huge difference between largest and smallest eigenvalue of the original matrix, A, the ratio of which is called condition number.\ninternal validity - our estimates are internally valid if statistical inferences about causal effects are valid for the population being studied. (also see external validity)\nintractable - problems for which there exist no efficient algorithms to solve them. Most intractable problems have an algorithm – the same algorithm – that provides a solution, and that algorithm is the brute-force search\nlocality - effects have causes and chains of cause and effect must be unbroken in space and time (not the case in ‘entanglement’)\nmarginalization - The process of eliminating one or more variables from a joint probability distribution or a multivariate statistical model to obtain the distribution or model for a subset of variables. The resulting distribution or model is called a marginal distribution or marginal model. It allows you to focus on the behavior of specific variables while considering the uncertainty associated with others.\n\nFor example, marginalizing over a joint distribution (i.e. many variables) gets you a marginal distribution (i.e. fewer variables). In other words, if you have a joint probability distribution for two variables \\(X\\) and \\(Y\\), the marginal distribution of \\(X\\) is obtained by summing or integrating over all possible values of \\(Y\\). Similarly, the marginal distribution of \\(Y\\) is obtained by summing or integrating over all possible values of \\(X\\).\nNotation: \\(P(X) = \\sum_Y P(X,Y) \\;\\text{or}\\; P(X) = \\int P(X,Y)\\;dY\\)\nOnce you have to the marginal distribution, this allows you compute conditional distributions. For example, after obtaining the marginal distribution, \\(P(X,Y)\\), from the joint distribution, \\(P(X,Y,Z)\\), you can compute the conditional distributions, \\(P(X|Y)\\) and \\(P(Y|X)\\).\nThe uncertainty associated with \\(Z\\) is indirectly considered in the sense that the marginal distribution \\(P(X,Y)\\) accounts for all possible values of \\(Z\\) by integrating over them. However, \\(P(X,Y)\\) itself doesn’t provide explicit information about the uncertainty associated with \\(Z\\).\n\nnon-identifiability - the structure of the data and model do not make it possible to estimate the parameter’s value. Multicollinearity is a type of non-identifiability problem. (i.e. two or more parametrizations of the model are observationally equivalent) (see identifiable, partially-indentifiable)\noverdetermined system - In linear regression, when there are more observations than features, n &gt; p\npartial coefficient - The coefficient of a variable in a multivariable regression. In a simple regression, the coefficient of the variable is just called the “regression coefficient.”\npartially-indentifiable (aka set identifiable) - non-identifiable but possible to learn the true values of a certain subset of the model parameters\nrobust - a “robust” estimator in statistics is one that is insensitive to outliers, whereas a “robust” estimator in econometrics is insensitive to heteroskedasticity and autocorrelation (hyndman)\nsupport (aka range) - the set of values that the random variable can take.\n\nFor discrete random variables, it is the set of all the realizations that have a strictly positive probability of being observed.\nFor continuous random variables, it is the set of all numbers whose probability density is strictly positive.\nSee link for examples\n\nunderspecification - In general, the solution to a problem is underspecified if there are many distinct solutions that solve the problem equivalently.\nAn unbiased estimator is an accurate statistic that’s used to approximate a population parameter.\n\n“Accurate” in this sense means that it’s neither an overestimate nor an underestimate. If an overestimate or underestimate does happen, the mean of the difference is called a “bias.”\n\nWeak Law of Large Numbers (Bernoulli’s theorem) - states that if you have a sample of independent and identically distributed random variables, as the sample size grows larger, the sample mean will tend toward the population mean",
    "crumbs": [
      "Mathematics",
      "Glossary"
    ]
  },
  {
    "objectID": "qmd/git-general.html",
    "href": "qmd/git-general.html",
    "title": "25  General",
    "section": "",
    "text": "25.1 Misc",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#misc",
    "href": "qmd/git-general.html#misc",
    "title": "25  General",
    "section": "",
    "text": "View HTML file in browser\n\nSyntax: “https://raw.githack.com/&lt;acct name&gt;/&lt;repo name&gt;/&lt;branch name&gt;/&lt;directory name&gt;/&lt;file name&gt;.html”\n\nInstalling from a git repo (From link)\n\nMake a fork of the repo and then clone it to your local machine.\nTo update, after setting an upstream remote (git remote add upstream git://github.com/benfulcher/hctsa.git) you can use git pull upstream main.\nTo update the submodule in the repo, git submodule update --init\n\nStart R project and Git repo in whichever order (I think)\n\nCreate R project in RStudio\n\nChoose “New Directory” for all the templated projects (e.g. quarto book, shiny, etc.). None of the other choices have them.\n\nIf you’ve already created a directory, it will NOT overwrite this directory or add to it. So you’ll either have alter the name of your old directory or choose a new name.\n\n\nCreate repo on Github\n\nAdd license and readme\n\nDo work\nTools &gt;&gt; Version Control &gt;&gt; Project Set-up &gt;&gt; Version Control System &gt;&gt; Select Git\nOpen terminal and go to working directory of project\ngit checkout -B main\ngit pull origin main --allow-unrelated-histories\ngit add .\ngit commit -m \"initial commit\"\ngit push --set-upstream origin main \n\nTurn off “LF will be replaced by CRLF the next time Git touches it”\n\nMessage spams terminal when committing changes from a window machines. Has to do with line endings in windows vs unix.\nTurn off: git config core.autocrlf true\nSee SO post for more details\n\nURL format to download files from repositories\n\nhttps://raw.githubusercontent.com/user/repository/branch/filename\n\n# Or evidently this way works too\n# adds ?raw=true to the end of the url\nfeat_all_url &lt;- url(\"https://github.com/notast/hierarchical-forecasting/blob/main/3feat_all.RData?raw=true\")\nload(feat_all_url)\nclose(feat_all_url)\nGet filelist from repo and download to a directory\n\n** Directory urls change as commits are made **\n\nlibrary(httr)\n\n# example: get url for the data dir of covidcast repo\nreq &lt;- httr::GET(\"https://api.github.com/repos/ercbk/Indiana-COVIDcast-Dashboard/git/trees/master?recursive=1\") %&gt;% \n  httr::content()\n# alphabetical order\ntrees &lt;- req$tree %&gt;% \n  map(., ~pluck(.x, 1)) %&gt;% \n  as.character()\n# returns 20 which is first instance, so 19 should the \"data\" folder\ndetect_index(trees, ~str_detect(., \"data/\"))\n# url for data dir\nreq$tree[[19]]$url\n\n# example\n# Get all the file paths from a repo\nreq &lt;- GET(\"https://api.github.com/repos/etiennebacher/tidytuesday/git/trees/master?recursive=1\")\n# any request errors get printed\nstop_for_status(req)\nfile_paths &lt;- unlist(lapply(content(req)$tree, \"[\", \"path\"), use.names = F)\n# file_path wanted &lt;- filter file path to file you want\n# gets the very last part of the path\nfile_wanted &lt;- basename(file_path_wanted)\norigin &lt;- paste0(\"https://raw.githubusercontent.com/etiennebacher/tidytuesday/master/\", file_wanted)\ndestination &lt;- \"output-path-with-filename-ext\"\n# if file doesn't already exist, download it from repo into destination\nif (!file.exists(destination)) {\n      # if root dir doesn't exist create it\n      if (!file.exists(\"_gallery/img\")) {\n        dir.create(\"_gallery/img\")\n      }\n      download.file(origin, destination)\nThe insides of .git",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#config-options",
    "href": "qmd/git-general.html#config-options",
    "title": "25  General",
    "section": "25.2 Config Options",
    "text": "25.2 Config Options\n\nNotes from: Popular git config options - More options listed that are not presented here.\nSetting Options\n\nAdd via CLI: git config --global &lt;name&gt; &lt;value&gt;\n\nExample: git config --global diff.algorithm histogram\n\nDelete by going into ~/.gitconfig and delete the parameter and value\n\nmerge.conflictstyle diff3 - Provides extra information on merge conflicts\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\ndef parse(input):\n    return input.split(\"\\n\")\n||||||| b9447fc\ndef parse(input):\n    return input.split(\"\\n\\n\")\n=======\ndef parse(text):\n    return text.split(\"\\n\\n\")\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; somebranch\n\nBelow &lt;&lt;&lt;&lt;&lt;&lt; HEAD: This is your local code that you’re trying to push\nBetween |||||||| b9447fc and =======: This is the original version of the code\nAbove &lt;&lt;&lt;&lt;&lt;&lt; somebranch: This is code from the branch that got merged before yours (I think)\nTherefore, the correct merge conflict resolution is return text.split(\"\\n\"), since that combines the changes from both sides.\n\nmerge.conflictstyle zdiff3 - A newer version of merge.conflictstyle diff3\nA\nB\nC\nD\nE\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; ours\nF\nG\n||||||| base\n# Add More Letters\n=======\nX\nY\nZ\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; theirs\n\nAbove &lt;&lt;&lt;&lt;&lt;&lt; ours: This is the original code plus the code that belongs to the branch that got merged that is not in conflict with your code\nBelow &lt;&lt;&lt;&lt;&lt;&lt; ours: This is the code that is in conflict with the branch (e.g. main) your merging into.\nBelow |||||||| base: This is the code that has been removed from the original code for both mergers\nAbove &lt;&lt;&lt;&lt;&lt;&lt; theirs: This is code for another branch that was merged before yours that is in conflict with your code.\n\npush.default current - Says that when using git push to always push the local branch to a remote branch with the same name.\n\npush.default simple is the default in Git. Means git push only works if your branch is already tracking a remote branch.\nI guess it’s possible to push a local branch to a remote branch of a different name.\n\ninit.defaultBranch main - Create a main branch instead of a master branch when creating a new repo. I normally do this on Github.\ncommit.verbose true - This adds the whole commit diff in the text editor where you’re writing your commit message, to help you remember what you were doing.\nrerere.enabled true - This enables rerere (”reuse recovered resolution”), which remembers how you resolved merge conflicts during a git rebase and automatically resolves conflicts for you when it can.\ncore.pager delta - The “pager” is what git uses to display the output of git diff, git log, git show, etc.\n\nValues:\n\ndelta: A fancy diff viewing tool with syntax highlighting\nless -x5,9 - Sets tabstops, which I guess helps if you have a lot of files with tabs in them?\nless -F -X - Not sure about this one, -F seems to disable the pager if everything fits on one screen if but her git seems to do that already anyway\ncat - To disable paging altogether\n\nDelta also suggests that you set up interactive.diffFilter delta –color-only to syntax highlight code when you run git add -p.\n\ndiff.algorithm histogram - Improves the Patience algorithm for presenting diffs. See link in article for more details.\n\nDefault (I think the default algorithm is Myers.)\n-.header {\n+.footer {\n     margin: 0;\n }\n\n-.footer {\n+.header {\n     margin: 0;\n+    color: green;\n }\n\nfooter didn’t actually have margin: 0 and color: green in the original code like this diff makes it seem. In reality, the two rules have switched order with header gaining the additional property, color: green.\n\nHistogram\n-.header {\n-    margin: 0;\n-}\n-\n .footer {\n     margin: 0;\n }\n\n+.header {\n+    margin: 0;\n+    color: green;\n+}\n\nThis shows header’s old rule without color: green at the top and being removed. footer is accurately depicted as unchanged. Then, it shows header with the addtional property, color: green, added below footer.\n\n\nincludeIf - Allows you to use different options depending which directory your project is in.\n\nExample: Use this config file only if you’re in the “work” directory\n[includeIf \"gitdir:~/code/&lt;work&gt;/\"]\n    path = \"~/code/&lt;work&gt;/.gitconfig\"\n\nGood if, for example, you want to have a work email set for work repos and personal email for set for personal repos\n\n\ninsteadOf - Useful to correct little mistakes often you make\n\nSee article for other usecases\nExample: If you accidently clone using http when you want to use SSH\n[url \"git@github.com:\"]\n    insteadOf = \"https://github.com/\"\n\nNow when you accidently clone a repo using the http address, it’ll change it to the ssh address in .git/config. Now you’ll be using ssh to push changes which is more secure.\n\n\nSubmodules\nstatus.submoduleSummary true\ndiff.submodule log\nsubmodule.recurse true\n\nSee thread for details\nThe top two “make git status and git diff display some more useful information on how things differ in submodules.”\nThe bottom one aids in the updating of submodules when switching branches\n\ndiff.colorMoved default - Uses different colours to highlight lines in diffs that have been “moved”\n\ndiff.colorMovedWS allow-indentation-change - With diff.colorMoved set, also ignores indentation changes\n\ngpg.format ssh - Allows you to sign commits with SSH keys\nmerge.tool meld (or nvim, or nvimdiff) - Enables use git mergetool to help resolve merge conflicts",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#optimizations",
    "href": "qmd/git-general.html#optimizations",
    "title": "25  General",
    "section": "25.3 Optimizations",
    "text": "25.3 Optimizations\n\nFor large repos, simple actions, like running git status or adding new commits can take many seconds. Cloning repos can take many hours.\nBenefits\n\nIt improves the overall performance of your development workflow, allowing you to work more efficiently. This is especially important when working with large organizations and open source projects, where multiple developers are constantly committing changes to the same repository. A faster repository means less time waiting for Git commands such as git clone or git push to finish. It helps to optimize the storage space, as large files are replaced by pointers which take up less space. This can help avoid storage issues, especially when working with remote servers.\n\nMisc\n\nSee How to Improve Performance in Git: The Complete Guide\n\nExplainer, config settings, advanced gc, checkout, and clone commands\n\n\nUse .gitignore\n\nGenerated files, like cache or build files\n\nThey will be modified at each different generation — and there’s no need to keep track of those changes.\n\nThird-party libraries\n\nInstead, aim for a list of the required dependencies (and the correct version) so that everyone can download and install them whenever the repo is cloned.\n\nFor example, with a package.json file for JavaScript projects you can (and should) exclude the /node_modules folder.\n.DS_Store files (which are automatically created by macOS) are another good candidate\n\n\n\nGit LFS\n\nDesigned specifically to handle large file versioning. LFS saves your local repositories from becoming unnecessarily big, preventing you from downloading unnessary data.\n\nGit LFS intercepts any large files and sends them to a separate server, leaving a smaller pointer file in the repository that links to the actual asset on the Git LFS server.\n\nThis is an extension to the standard Git feature set, so you will need to make sure that your code hosting provider supports it (all the popular ones do).\nAlso need to download and install the CLI extension on your machine before installing it in your repository.\nSet-Up\n$ git lfs install\n$ git lfs track \"*.wav\"\n$ git lfs track \"images/*.psd\"\n$ git lfs track \"videos\"\n$ git add .gitattributes\n\nTells Git LFS which file extensions it should manage.\n.gitattributes notes the file names and patterns in this text file and, just like any other change, it should be staged and committed to the repository.\nCan now add files and commit as normal\nList all file extensions being tracked: git lfs track\nList all files being managed: git lfs ls-files\n\n\nDon’t download the version history if you don’t need to\n\ngit clone –depth 1 gitj@github.com:name/repo.git",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#troubleshooting",
    "href": "qmd/git-general.html#troubleshooting",
    "title": "25  General",
    "section": "25.4 Troubleshooting",
    "text": "25.4 Troubleshooting\n\nDiverged Branches\n\n\nKeeps asking for username/password when pushing\n\nSolution: You (or if you used usethis::use_github/git) probably set-up a https connection when you need a ssh connection.\n\nsee https://docs.github.com/en/get-started/getting-started-with-git/managing-remote-repositories#changing-a-remote-repositorys-url to change from https to ssh.\n\n\nUndo a commit, but save changes made (e.g. you forgot to pull before you pushed)\n\nSteps\n\ngit log - Shows commit history. Copy the hash for your last commit\ngit diff &lt;last commit hash&gt; &gt; patch - save the diff of the latest commit to a file\ngit reset --hard HEAD^ to revert to the previous commit\n\n**After this, your changes will be lost locally **\n\ngit log - confirm that you are now at the previous commit\ngit pull - correct the mistake you made in first place\npatch -p1 &lt; patch - apply the changes you originally made\ngit diff - to confirm that the changes have been reapplied\nNow, you do the regular commit, push routine\n\n\nUndo uncommitted changes: git stash followed by git stash drop\n\n“but only use if you commit often” - guessing this is not good if your commit is somehow large and/or involves multiple files\n\nSearch commits by string: git log --grep &lt;string&gt;\nPinpoint bugs in your commit history\n\nInstead of sequentially searching each previous commit to look for the bad commit, git bisect helps you perform a bisect search for the commit which saves time.\nScenario: A bug is introduced in a codebase, but it is not discovered until later. The feature used to work, but now, it does not. The feature was definitely known to work 3 weeks ago.\nManual Workflow\n\nMake sure you’re in the current commit that’s bad and start git bisect\ngit bisect start\n1git bisect bad\n2git log --before=\"3 weeks\"\n3git checkout 3348b0\n\n1\n\nThis labels the current commit as bad (i.e. bug is present)\n\n2\n\nThis lists every commit for last 3 weeks\n\n3\n\nSwitch to the commit that’s the version of the project that was 3 weeks ago when supposedly the feature was working. The first commit listed (i.e. top) will be the commit closest to 3 weeks ago — with older commits below it. You only need to use the first 6 or so digits of the commit hash.\n\n\nRecompile code and test commit for bug\ndevtools::load_all()\n\nload_all will recompile your package using this current version’s code\nAfter recompiling code, use your reproducible examplet to see if the bug is present in this version\nIf the bug is stil present, then go to the next older commit and repeat process. Keep loading older commits until you find one that doesn’t have the bug.\n\nIf ths is the case and assuming you don’t have to go back too much further to find a “good” commit, then you can stop here since you’ll have found the bad commit that introduced the bug.\nIf you don’t find a good commit around this time period, then quit the current git bisect session using git bisect reset and choose whichever commit you stop at as the new starting point for a new git bisect session and repeat this whole workflow.\n\n\nGo to terminal and mark this commit as good\ngit bisect good\n\nGit will automatically switch you to commit that’s the midway point between the “start” commit and the commit you labeled as “good.”\nIt tells you how many commits that are currently between you and the “start” commit which is the same amount as between this midway commit and the commit you labelled as “good.\nIt also tells you how many more bisections (“steps”) you’ll have to go through to find the commit resposible for the bug.\n\nRepeat Step 2 and test verstion for the bug. Then label commit as good or bad\ngit bisect bad\n\nAfterwards, git will automatically checkout to the commit that is either midway between this commit and “start” or the end commit based on whether you label this current commit as good or bad.\n\nContinue labelling commits until git’s message is “&lt;some commit hash&gt; is the first bad commit.”\n\nGit will also show you the commit message and a list of files that were changed.\n\nUse git show &lt;commit hash&gt; to see the diff\n(Optional) Use git bisect log &gt; file-name to save the session to a file.\nUse git bisect reset to exit and return you to where you were at the start of this workflow (HEAD)\n\nAutomatic Workflow\n\nWrite script that includes you reproducible exaample and have it return an error code of 0 if it does not contain the bug or return an non-zero error if it does contain the bug.\n\nExample\ndevtools::load_all()\n\nif (nr != nrow(df)) {\n  stop(\"error\")\n}\n\nload_allwill recompile your package using this current version’s code\nReturns non-zero error code if condition is not triggered (i.e. False) and a 0 error code when the condition is triggered (i.e. True).\nCould also use stopifnot here.\n\n\n(Optional) If you already know the commit hash of commit from 3 weeks ago and that does not have the bug, you can bypass the step 3.\n# Make sure you're in the current commit that's got the bug\ngit bisect start\n1git bisect bad\n2git bisect good 3348b0\n\n1\n\nThis labels the current commit as bad (i.e. bug is present)\n\n2\n\nThis labels the commit from 3 weeks ago that you know doesn’t have the bug\n\n\nDo steps 1, 2, and 3 of the Manual Workflow\nRun auto-bisect\ngit bisect run Rscript test.R\n\ntest.R is the script from step 1 that determines whether the version (i.e. commit hash) of your code has the bug.\nThis run through all the steps of the Manual Workflow and determine with the version of the code is “good” or “bad” by whether the script returns an error code of zero or non-zero.\n\nRead final message to get the commit hash with bug in it.\n\nMessage will be “&lt;some commit hash&gt; is the first bad commit.”\nGit will also show you the commit message and a list of files that were changed.\n\nSee steps 6, 7, and 8 of the Manual Workflow",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#pulling",
    "href": "qmd/git-general.html#pulling",
    "title": "25  General",
    "section": "25.5 Pulling",
    "text": "25.5 Pulling\n\nSave your changes, pull in an update, apply your changes\ngit stash\ngit pull\ngit stash pop\n\ngit stash pop throws away the (topmost, by default) stash after applying it, whereas\ngit stash apply leaves it in the stash list for possible later reuse (or you can then git stash drop it).\n\nRe potential merge conflicts\n\n“For instance, say your stashed changes conflict with other changes that you’ve made since you first created the stash. Both pop and apply will helpfully trigger merge conflict resolution mode, allowing you to nicely resolve such conflicts… and neither will get rid of the stash, even though perhaps you’re expecting pop too. Since a lot of people expect stashes to just be a simple stack, this often leads to them popping the same stash accidentally later because they thought it was gone.”\n\nPulling is fetching + merging\n\nFetching just gets the info about the commits made to the remote repo\ngit fetch origin\nSome technical discussion for always using git pull –ff\n\nhttps://blog.sffc.xyz/post/185195398930/why-you-should-use-git-pull-ff-only-git-is-a\nhttps://megakemp.com/2019/03/20/the-case-for-pull-rebase/\nit’s still confusing but pull rebase sounds fine to me\n–global tag says do it for all my repos\nnot sure what the true and only are for\n\ngit pull –help will open doc in browser\n\n\nPulling by rebase\n\nLocal: using this method as default\ngit config pull.rebase true\ngit pull\nRemote\ngit pull --rebase\n\nPulling by fast-forward\n\nLocal: using this method as default\ngit config --global pull.ff only\ngit pull\nRemote\ngit pull --ff",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#branching",
    "href": "qmd/git-general.html#branching",
    "title": "25  General",
    "section": "25.6 Branching",
    "text": "25.6 Branching\n\nMisc\n\nCreate a new branch for each ticket you are working on or each data model. It can get sloppy when you put all your code changes on one branch.\nHEAD\n\nDetached HEAD\n\n\nCreate a branch (e.g. “testing”)\ngit branch testing\nWork in a branch\ngit checkout testing\nThe files in your working directory change to the version saved in that branch\nIt adds, removes, and modifies files automatically to make sure your working copy is what the branch looked like on your last commit to it.\nCreate and work in a branch\n# new way\ngit switch -c testing\nor\ngit checkout -b testing\nor\ngit branch testing\ngit checkout testing\ncreates the branch and switches you to working in that branch\nIf you did a bunch of changes in a codebase, only to realize that you’re working on `master`,  switch will bring those local changes with you to the new branch. So I guess they won’t affect master then.\n\nUnless If you already committed to main, then those changes are both in your new branch and in main. So you would still have to clean up the main branch.\n\nDeleting a branch\n\nlocal branch\ngit branch -d testing\n\nremote branch\ngit push &lt;remoteName&gt; --delete &lt;branchName&gt;\nSee existing branches\ngit branch\nSee what has been commited the remote repo branches\ngit fetch origin\ngit branch -vv\n“origin” is the name of the remote\nresult\ntesting    7e424c3 [origin/testing: ahead 2, behind 1] change abc \nmaster      1ae2a45 [origin/master] Deploy index fix\n* issue    f8674d9 [origin/issue: behind 1] should do it         \ncart        5ea463a Try something new\nformat: branch, last commit sha-1, local branch status vs remote branch status, commit message\nthe star indicates the HEAD pointer’s location (where you’re at, i.e. checkout)\ntesting branch\n\n“ahead 2” means  I committed twice to the local testing branch and this work has not been pushed to the remote testing branch repo yet.\n“behind 1” means someone has pushed a commit to the remote testing branch repo and we haven’t merged this work to our local testing branch\n\nGet the last 10 branches that you’ve committed to locally:\ngit branch --sort=-committerdate | head -n 10\nRename branch\n# change locally\ngit branch --move &lt;bad-branch-name&gt; &lt;corrected-branch-name&gt;\n# change remotely in repo\ngit push --set-upstream origin &lt;corrected-branch-name&gt;\n# confirm change\ngit branch --all\nHEAD determines to which branch new commits are added\n\nExample\n\n“testing” branch is created (not shown in above picture)\n\nHEAD points at “master” branch\n“master” branch and the new “testing” branch both point at commit, f30ab.\nf30ab commit points to previous commit 34ac2\n\nuser executes checkout to “testing” branch (not shown in picture)\n\nHEAD now points to testing branch\n\nuser commits 87ab2 (shown in pic)\n\n87ab2 is committed to the “testing” branch\n“testing” branch is now ahead of the “master” branch by 1 commit\n\n\nExample\n\nEverything above happens but now another user commits the master branch.\n\nBoth branches are in conflict. The testing branch is ahead and behind by 1 commit\n\n\n\nMerging\n\n\nNotes\n\nNEVER merge your branch locally on your machine with the master branch, ALWAYS merge online via pull request\n\nSteps\n\nPush final changes and use of a pull request\nSwitch to master branch locally and pull the merged changes\n\n\n\nUpdate branch with work that’s been done in master branch\n\nAfter updating your local branch, push to remote repo (no commit necessary)\n# while in branch\ngit merge master\n\n\nFast-Forward\n\nExample\n\nBefore the merge\n\nthe testing branch is 1 commit ahead of the master branch and the master branch doesnt have a new commit\n\nAfter the merge\n\nmaster is moved forward to the testing branch commit\n\n\nCode (merging work in branch with the master branch for production)\n# currently in test branch\ngit checkout master\ngit merge testing\n\nLines in file are marked\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.html\n# &lt;div id=\"footer\"&gt;contact : email.support@github.com&lt;/div&gt;\n# =======\n# &lt;div id=\"footer\"&gt;\n# please contact us at support@github.com\n# &lt;/div&gt;\n# &gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html\nAbove ======= is the master branch version of the code and below is the iss53 branch version\nMake necessary changes and save the file\ngit add . or git add &lt;resolved file&gt;\n\nTells git that conflict is resolved\n\nCheck status to confirm everything has been resolved\ngit status\n\n    On branch master\n    All conflicts fixed but you are still merging.\n      (use \"git commit\" to conclude merge)\n    Changes to be committed:\n      modified:  index.html\ngit commit\n\nno message required (there’s a default message) but you can add one if you want\n\nExample\n\niss53 branch ahead of master by 2 commits (c3, c5) and behind 1 commit (c2)\nSame code as Fast-Forward merge but git handles the merge a bit differently\ngit checkout master \ngit merge iss53\n\n\n\nC6 (right pic) is called a “merge commit.” Its created by git and points to two commits instead of one.\nNo need to merge with master (i.e. update local iss53 branch with c4 changes in master) before committing final changes\n\nIf there are changes in the same lines of code C4 and C5, then there will be a conflict (See below, Conflicts &gt;&gt; Example)\n\n\nConflicts\n\nExample\n\nChanged files in C4 (see above example) are in the same lines of the same files that you made changes to in C5\n\nRemember: you’re now in the master branch since you did checkout master as part of the merge code\nSteps\n\nCheck status to which files are causing the conflict (e.g. index.html)\ngit status\n  Unmerged paths:\n  (use \"git add &lt;file&gt;...\" to mark resolution) \n    both modified:      index.html\n\n\n\n\nMoving between branches\n\nfrom master to testing\ngit checkout testing\n\nlocal files are deleted and replaced with branch versions\n\nalternative: worktree\n\nExample\n\nWhat happens when you move from branch-a to branch-b\nBRANCH-A        BRANCH-B\nalpha.txt      alpha.txt\nbravo.txt\ncharlie.txt    charlie.txt\n                delta.txt\n\nbravo text is deleted from your local disc and delta.txt is added\nIf any changes to alpha.txt or charlie.txt have been made and no commit has been made, the checkout will be aborted\n\nSo either revert the changes or commit the changes\n\nUntracked files or newly created files\n\nIf you have branch-A checked out and you create a new file called echo.txt, Git will not touch this file when you checkout branch-B. This way, you can decide that you want to commit echo.txt against branch-B without having to go through the hassle of (1) move the file outside the repo, (2) checkout the correct branch, and (3) move the file back into the repo.",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#collaboration",
    "href": "qmd/git-general.html#collaboration",
    "title": "25  General",
    "section": "25.7 Collaboration",
    "text": "25.7 Collaboration\n\nAdd collaborators to your repository\nOne person invites the others and provides them with read/write access (github docs)\n\nSteps\n\nGo to the settings for your repository\nmanage access &gt;&gt; “invite a collaborator”\n\nSearch for each collaborator by full name, acct name, or email\nClick “Add &lt;name&gt; to &lt;repo&gt;”\n\nEach collaborator will need to accept the invitation\n\nSent by email",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/js.html",
    "href": "qmd/js.html",
    "title": "JS",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-misc",
    "href": "qmd/js.html#sec-js-misc",
    "title": "JS",
    "section": "",
    "text": "Resources\n\nLearn Just Enough JavaScript\n\nBasics: variables, objects, arrays, functions, conditionals, loops\n\nHow to run R code in the browser with webR\n\nNice breakdown of generic JS code to run scripts on a webpage\n\nJavaScript for Data Science\n\nhrbmstr: “javascript has the advantage over R/Python for both visualization speed — thanks to GPU integration — and interface creation — thanks to the ubiquity of HTML5 — means that people will increasingly bring their own data to websites for initial exploration first”\nconsole.log is the print method",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-basics",
    "href": "qmd/js.html#sec-js-basics",
    "title": "JS",
    "section": "Basics",
    "text": "Basics\n\nOperators\n\n// : comments\n... : If you want to copy all the values in your array, and add some new ones, you can use the {…} notation.\n${&lt;code&gt;} : Anything within the${} get ran as code\n\nExample:\n`${b.letter}: ${ (b.frequency*100).toFixed(2) }%`\n\nBackticks indicate it’s like a glue string or f string (i.e. uses code)\nb.letter and b.frequency are properties in an array\nto.Fixed is a method that rounds the value to to 2 decimal places\nThis was an example of a tooltip, so output would look like “F: 12.23%”\n\n\n\nVariables\nmyNumber = 10 * 1000\nvariableSetToCodeBlock = {\n  const today = new Date();\n  return today.getFullYear()\n}\nObject: myObject = ({name: \"Paul\", age: 25})\n\nContained within curly braces, { }\nSubset property, name:\n\nmyObject.name which returns value, Paul\nmyObject[\"name\"] which is useful if you have spaces, etc. in your property names\n\nTypes\n\nMap: Object holds key-value pairs and remembers the original insertion order of the keys\n\ne.g. See Stats &gt;&gt; By Group\nD3 Groups, Rollup, Index Docs\n\n\n\nArrays\n\nList of objects\n\nContained within brackets, [ ]\nEach row is an object and each column is a property of that object and that property has a value associated with it\n\nBasic examples\nmyArray = [1, 2, 3, 4]\nmyArray = [[1, 2], [3, 4]] // arrays within arrays\nmyArray = [1, 'cat', {name: 'kitty'}] // objects within arrays\nDF-like array\nmyData = [\n  {name: 'Paul', city: 'Denver'},\n  {name: 'Robert', city: 'Denver'},\n  {name: 'Ian', city: 'Boston'},\n  {name: 'Cobus', city: 'Boston'},\n  {name: 'Ayodele', city: 'New York'},\n  {name: 'Mike', city: 'New York'},\n]\n\nEquivalent Functions: Traditional vs Arrow\n// traditional\nfunction myFunctionWithParameters(firstName, lastName) {\n  return `My first name is ${firstName}, and my last name is ${lastName}.`\n}\n// arrow\nmyModernFunctionWithParameters = (firstName, lastName) =&gt; {\n  return `My first name is ${firstName}, and my last name is ${lastName}.`\n}\n\nArrow: Arguments are in the parentheses and the function is inside the curly braces\nString with variables needs to be surrounded by backticks\n\nFunctions Inside Methods: Traditional vs Arrow\n// traditional\n[1, 2, 3, 4, 5].filter(function(d) { return d &lt; 3 })\n// arrow\n[1, 2, 3, 4, 5].filter(d =&gt; d &lt; 3)\n\nThe argument is d but without parentheses and the function is d &lt; 3 without the curly braces\nThe function inputs each row/value of the array, so d is a row/value of the array. Then, the function does something to that row.\n\nConditionals\n\n== vs ===\n1 == '1' // true\n1 === '1' // false\n\n== is a logical test to see if two values are the same\n\n=== is a logical test to see if two values are the same and also checks if the value types are the same\n\nIf/Then\nif(1 &gt; 2) {                              // If this statement is true\n    return 'Math is broken'              // return this\n} else {                                // if the first statement was not true\n    return 'Math still works!'          // return this\n}\n\n// using ternary operator \"?\"\n\nUsing ternary operator “?”\n\nSyntax: condition ? exprIfTrue : exprIfFalse\nExample: d =&gt; d.frequency &gt;= minFreq ? \"steelblue\" : \"lightgray\"\n\nSays if the frequency property is &gt;= the variable, minFreq, value, then use steelblue otherwise use lightgray\n\n\n\n\nFor-Loop\nlet largestNumber = 0; // Declare a variable for the largest number\n\nfor(let i = 0; i &lt; myValues.length - 1; i++) {    // Loop through all the values in my array\n    if(myValues[i] &gt; largestNumber) {              // Check if the value in the array is larger that the largestNumber\n      largestNumber = myValues[i]                  // If so, assign the value as the new largest number\n    }\n}\n\nreturn largestNumber\n\nThe first statement sets a variable (let i = 0)\nThe second statement provides a condition for when the loop will run (whenever i &lt; myValues.length - 1)\nThe third statement says what to do each time the code block is executed (i++, which means to add 1 to i)\n\nWhile-Loop\nlet largestNumber = 0;                        // Create a variable for the largest number\nlet i = 0;\nwhile(i &lt; myValues.length - 1) {\n    if(myValues[i] &gt; largestNumber) {        // Check if the value in the array is larger that the largestNumber\n      largestNumber = myValues[i]            // If so, assign the value as the new largest number\n    }\n    i++;\n}\nreturn largestNumber",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-cleaning",
    "href": "qmd/js.html#sec-js-cleaning",
    "title": "JS",
    "section": "Cleaning",
    "text": "Cleaning\n\nMisc\n\nNotes from: Horst article\n\nFilter objects: myData.filter(d =&gt; d.city == 'Denver')\nSelect properties: myNewArray = salesData.map(d =&gt; ({ date: d.date, product: d.product, totalRevenue: d.totalRevenue }))\n\nIn some contexts, this, d =&gt; d[\"mileage (mpg)\"] , is also used to select columns\n\nArrange objects: salesData.sort((a, b) =&gt; a.totalRevenue - b.totalRevenue)\n\nReorders salesData by totalRevenue (low to high)\n\nMutate properties: salesData.map(d =&gt; ({...d, discountedPrice: 0.9 * d.unitPrice }))\n\nAdds a new column to salesData with a discountedPrice, which takes 10% off each unitPrice.\n\nGroup_By: d3.rollup(salesData, v =&gt; d3.sum(v, d =&gt; d.totalRevenue), d =&gt; d.region)\n\nReturn the sum of totalRevenue for each region in salesData.\nrollup might actually be a summarize and the group_by is handled in the syntax\n\nRename: salesData.map(d =&gt; ({...d, saleDate: d.date }))\n\nAdds a new column called saleDate by storing a version of the date with new name saleDate and keeping all other columns.\n\nSubset value: salesData.map(d =&gt; d.description)[3]\n\nAccess the fourth value from the description property in salesData\n\nUnite:\nsalesData.map(d =&gt; ({...d, fullDescription: `${d.product} ${d.description}`}))\n\nUnite the product and description columns into a single column called fullDescription, using a comma as a separator.\n\nLeft Join: *using {{{arquero}}} tables* salesData.join_left(productDetails, ['product', 'product_id'])\n\nJoin information from a productDetails table to salesData. Join on product in salesData and product_id in productDetails.",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-stats",
    "href": "qmd/js.html#sec-js-stats",
    "title": "JS",
    "section": "Stats",
    "text": "Stats\n\nMisc\n\nNotes from: Horst article\n\nIn examples, waterUsage is the array; waterGallons is the property.\n\n\nMean: d3.mean(waterUsage.map(d =&gt; d.waterGallons))\n\nReturns a Value\n\nStd.Dev: d3.deviation(waterUsage.map(d =&gt; d.waterGallons))\nMedian: d3.median(waterUsage.map(d =&gt; d.waterGallons))\nMin/Max: d3.min(waterUsage.map(d =&gt; d.waterGallons))\nTotal Observations (i.e. nrow ): waterUsage.length\nBy Group:\n\npropertyId is the discrete, grouping variable\nMean: waterMeans = d3.rollup(waterUsage, v =&gt; d3.mean(v, d =&gt; d.waterGallons), d =&gt; d.propertyId)\n// Returns a map object\nwaterMeans\n{\n  \"A001\" =&gt; 39.53389830508475\n  \"B002\" =&gt; 53.57627118644068\n  \"C003\" =&gt; 27.45762711864407\n  \"D004\" =&gt; 80.1864406779661\n}\n\n// View in a JS Table\n// ** Must be in a separate cell **\nInputs.table(waterMeans.map(([propertyId, meanWaterGallons]) =&gt; ({propertyId, meanWaterGallons})))\nCount: d3.rollup(waterUsage, v =&gt; d3.count(v, d =&gt; d.waterGallons), d =&gt; d.propertyId)\n\nConditional Counts: waterUsage.filter(d =&gt; d.waterGallons &gt; 90 && d.propertyId == \"B002\").length\n\nApplies two conditionals and counts the observations\n\nRanks\nwaterUsage.map((d, i) =&gt; ({...d, rank: d3.rank(waterUsage.map(d =&gt; d.waterGallons), d3.descending)[i] + 1}))\n\n1 is added so that ranks start at 1 instead of 0\n\nPercentiles: d3.quantile(waterUsage.map(d =&gt; d.waterGallons), 0.9) (e.g. 90th)",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-obs",
    "href": "qmd/js.html#sec-js-obs",
    "title": "JS",
    "section": "Observable",
    "text": "Observable\n\nA collaborative, online notebook platform that comes with libraries loaded to make it fairly straightforward to dive into ad hoc data analysis or produce complete reports.\nIn Observable, if you’re running a JavaScript cell that contains more than just a simple variable assignment (like myVariable = 'Hello World' ), you need to run a code block (i.e. bracket lines of code in curly braces, {}).\nYou can open your notebook in Safe Mode and edit your work without running it.\n\nGood for debugging (e.g. infinite while-loops)",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-def",
    "href": "qmd/js.html#sec-js-def",
    "title": "JS",
    "section": "Definitions",
    "text": "Definitions\n\nJSON vs R List\n{                              list(\n    boolean: true,                boolean = TRUE,\n    string: \"hello\",              string = \"hello\",\n    vector: [1,2,3]                vector = c(1,2,3)\n}                              )\n\n// Access                      # Access\njson.vector                    list$vector\nDependencies\nHTML                                                  R (shiny)\n&lt;head&gt;                                                tags$head(\n    &lt;!-- JavaScript --&gt;                                  tags$script(src = \"path/to/file.js\")\n    &lt;script src=\"path/to/file.js\"&gt;&lt;/script&gt;              tags$link(\n    &lt;!-- CSS --&gt;                                          rel = \"stylesheet\",\n    &lt;link rel=\"stylesheet\" href=\"path/to/file.css&gt;        href = \"path/to/file.css\n&lt;/head&gt;                                                  ))\nd is each row and =&gt; is function\n(d) =&gt; d.year === 2020\n\nSays for each row in your data, the year column must equal 2020\n\nCallback Function - A function that is passed to another function as a parameter. In other words, a function “calls back” to previously defined function.\nfunction print(callback) { \n    callback();\n}\n\ncallback is the callback function and is a parameter of the print function\nCallbacks make sure that a function is not going to run before a task is completed but will run right after the task has completed.\nExample:\n// \"Click here\" button in a web app\n&lt;button id=\"callback-btn\"&gt;Click here&lt;/button&gt;\ndocument.queryselector(\"#callback-btn\")\n    .addEventListener(\"click\", function() {   \n      console.log(\"User has clicked on the button!\");\n});\n\nFirst, button selected by its id, and then we add an event listener with the addEventListener method. It takes 2 parameters. The first one is its type, click, and the second parameter is a callback function, which logs the message when the button is clicked.\n\n\nAnonymous Function - Same as a callback but unnamed. It’s a  function that is defined within another function.\nsetTimeout(function() { \n    console.log(\"This message is shown after 3 seconds\");\n}, 3000);\n\n// if the function were named\nconst message = function() { \n    console.log(\"This message is shown after 3 seconds\");\n}\n\n// as an arrow function\nsetTimeout(() =&gt; { \n    console.log(\"This message is shown after 3 seconds\");\n}, 3000);\n\nThe function used as a parameter has no name. console.log is the contents of the function.",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-nfcd",
    "href": "qmd/js.html#sec-js-nfcd",
    "title": "JS",
    "section": "Notes From Covidcast Dashboard",
    "text": "Notes From Covidcast Dashboard\n\nNotes from\n\nCovidcast Dashboard: reactable + sparkline tooltip (link)\n\ndiv = vertical label or container , span = horizontal\nFormat: type, styling, value\n2 divs would result in a 2 element vertical label while 2 spans would be a 2 element horizontal label\nExample: A div container holding 2 spans which creates a “date value” horizontal label\n\"function (_ref) {\nvar datum = _ref.datum;\nreturn React.createElement(\n  'div',\n  null,\n  datum.date && React.createElement(\n      'span',\n      {style: {\n          backgroundColor: 'black', color: 'white',\n          padding: '3px', margin: '0px 4px 0px 0px', textAlign: 'center'\n        }},\n      datum.date[0].split('-').slice(1).join('/')\n  ),\n  React.createElement(\n      'span',\n      {style: {\n        fontWeight: 'bold', fontSize: '1.1em',\n        padding: '2px'\n      }},\n      datum.y ? datum.y.toLocaleString(undefined, {maximumFractionDigits: 0}) : '--'\n  )\n  );\n}\"\n\nCSS: margin, padding\n\nFormat is top, right, bottom, left (ordered like a clock)\nRequires units like “px”\nNo commas separate the values\n{margin: '0px 4px', padding: '0px 0px 0px 4px'}\n\nMaybe for 0s it doesn’t matter\nSee bkmk in css/definitions for explanations behind specifications with less than 4 numbers\n\ne.g. 2 is ‘top/bottom left/right’\n\n\n\nString manipulation\ndatum.endDate[0].split('-').slice(1).join('/')\n\nTreats variable as a string object\nLooks in data arg, finds endDate variable\nIts a list variable so requires the [0] (0 part an index?)\nDate format is ymd, so splits value by “-” separator, removes 1st value (year), joins the rest of the values (month, day) with “/”\n\nIf slice(2), removes first 2 values (left to right)\n\n\nConditional\nlabelPosition = htmlwidgets::JS(\"(d, i) =&gt; (i === 0 || i === 1 ? 'right' : 'left')\")\n\nSays that if index of data value, d, is 0 or 1 then label should be positioned on the right of the point, else place the label on the left of the point",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/simulation-data.html",
    "href": "qmd/simulation-data.html",
    "title": "Simulation, Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Simulation, Data"
    ]
  },
  {
    "objectID": "qmd/simulation-data.html#sec-sim-data-misc",
    "href": "qmd/simulation-data.html#sec-sim-data-misc",
    "title": "Simulation, Data",
    "section": "",
    "text": "Todo - for the “trtAssign” mess with ratio and the number of ratios\nAlso see\n\nBkmks Data &gt;&gt; Data Simulation\nPandas-Time Series-Simulation\n\nA Gaussian and Standard GARCH time-series that’s frequently encountered in econometrics\n\n\nPackages\n\n{structmcmc} - A set of tools for performing structural inference for Bayesian Networks using MCMC\n\nPapers\n\nGeneration and analysis of synthetic data via Bayesian networks: a robust approach for uncertainty quantification via Bayesian paradigm",
    "crumbs": [
      "Simulation, Data"
    ]
  },
  {
    "objectID": "qmd/simulation-data.html#sec-sim-data-simstudy",
    "href": "qmd/simulation-data.html#sec-sim-data-simstudy",
    "title": "Simulation, Data",
    "section": "{simstudy}",
    "text": "{simstudy}\n\nMisc\n\nDocs\n\n\n\nReference\n\nAvailable distributions (link)\n\nProbability Distributions\nnonrandom: For constants; can be a numeric or a string with a formula that defines a dependency on another variable\nclusterSize: For variable cluster sizes but a constant total sample size\n\nformula: The (fixed) total sample size\nvariance: A (non-negative) dispersion measure that represents the variability of size across clusters\n\nIf the dispersion is set to 0, then cluster sizes are constant\n\n\ntrtAssign: For treatment assignment\n\nformula: Ratio which is separated by semicolons and number of treatments\n\ne.g. 2 values = 2 groups and “1;2” says group 2 has twice as many units and group 1\n\nvariance: Stratification; ratio in formula is used as the stratification ratio (e.g. unbalanced treatment groups → unbalanced stratification)\nExample\ndef &lt;- \n  defData(def, \n          varname = \"rx\", \n          dist = \"trtAssign\",\n          formula = \"1;1;2\", \n          variance = \"male;over65\")\n\ncount(studytbl, rx)\n#&gt; # A tibble: 3 × 2\n#&gt;     rx    n\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1    1    84\n#&gt; 2    2    82\n#&gt; 3    3  164\n\ncount(studytbl, male, rx)\n#&gt; # A tibble: 6 × 3\n#&gt;   male    rx    n\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1    0    1    40\n#&gt; 2    0    2    39\n#&gt; 3    0    3    78\n#&gt; 4    1    1    44\n#&gt; 5    1    2    43\n#&gt; 6    1    3    86\n\ncount(studytbl, over65, rx)\n#&gt; # A tibble: 6 × 3\n#&gt;   over65    rx    n\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1      0    1    66\n#&gt; 2      0    2    65\n#&gt; 3      0    3  130\n#&gt; 4      1    1    18\n#&gt; 5      1    2    17\n#&gt; 6      1    3    34\n\n\nFunctions\n\ndefData(dtDefs = NULL, varname, formula, variance = 0, dist = \"normal\", link = \"identity\", id = \"id\") - Initially creates a data.table or adds a column to a data.table with instructions about creating a variable\n\nformula: Numeric constant or string formula for the mean, probability of event (binary), probability of success (binomial), etc.\n\ndefDataAdd(dtDefs = NULL, varname, formula, variance = 0, dist = \"normal\", link = \"identity\") - Creates a variable definition like defData but is used to augment a already generated dataset. Used as input to addColumns which will generate the variable data from the instructions in this object and add it as a column to the already generated dataset.\ngenCluster(dtClust, cLevelVar, numIndsVar, level1ID, allLevel2 = TRUE) - After generating cluster-level data, this function takes the number of clusters and the sizes of each cluster from that data, and does something like expand.grid to generate an individual-level dataset. Also, adds an id variable.\n\ndtClust: Cluster-Level Data\ncLevelVar: Cluster variable from the cluster-level data\nnumIndsvar: Variable with the number of units per cluster from the cluster-level data\nlevel1ID: Name you want for your individual-level ID variable\n\n\n\n\n\nVariable Dependence\n\nBinary depends on a Binary\n\nDefinitions\ndef &lt;- defData(varname = \"male\", dist = \"binary\",\n               formula = .5 , id=\"cid\")\ndef &lt;- defData(def, varname = \"over65\", dist = \"binary\",\n               formula = \"-1.7 + .8*male\", link=\"logit\")\nWhat’s happening\nmale &lt;- c(1,1,0,1,0,0,0,1,0,1)\nlogits &lt;- -1.7 + 0.8 * male\nprobabilities &lt;- boot::inv.logit(logits)\nover65 &lt;- rbinom(n = 10, size = 1, prob = probabilities)\n\nThe formula in the logits line defines the relationship between being male and being over 65yrs old.\nMales in this sample will have a higher probability (0.2890505) of being over 65yrs old than females (0.1544653)\nTo sample from a Bernoulli distribution, set size = 1\nover65 is an indicator where each value is determined by a separate probability parameter for a Bernoulli distribution\n\n\n\n\n\nClustered with Cluster-Level Random Effect\n\nExample: Fixed Cluster sizes; Balanced\n\nCluster Definitions\nd0 &lt;- defData(varname = \"n\", formula = 20, dist = \"nonrandom\")\nd0 &lt;- defData(d0, varname = \"a\", formula = 0, variance = 0.33)\nd0 &lt;- defData(d0, varname = \"rx\", formula = \"1;1\", dist = \"trtAssign\")\nd1 &lt;- defDataAdd(varname = \"y\", formula = \"18 + 1.6 * rx + a\",\n                 variance = 16, dist = \"normal\")\n\nn: sample size for the cluster\n\ndist = “nonrandom” and formula = 20 says use a constant for the cluster sizer\n\nrx: treatment indicator\n\ndist = “trtAssign” and formula = “1;1” says 2 treatment groups and they’re balanced\n\ny: the individual-level outcome is a function of the treatment assignment and the cluster effect, as well as random individual-level variation\na: random individual-level variation (i.e. random effect)\n\nRandom Effects are sampled from \\(\\mathcal{N}(0, \\sigma)\\) where the variance is typically estimated in a Mixed Effects model.\n\n\nGenerate Cluster-Level Data\nset.seed(2761)\ndc &lt;- genData(10, d0, \"site\")\ndc\n##    site  n      a rx\n##  1:    1 20 -0.3548  1\n##  2:    2 20 -1.1232  1\n##  3:    3 20 -0.5963  0\n##  4:    4 20 -0.0503  1\n##  5:    5 20  0.0894  0\n##  6:    6 20  0.5294  1\n##  7:    7 20  1.2302  0\n##  8:    8 20  0.9663  1\n##  9:    9 20  0.0993  0\n## 10:  10 20  0.6508  0\n\nGenerates 10 clusters labelled as site according to the instructions in d0\n\nGenerate Individual Level Data\ndd &lt;- genCluster(dc, \"site\", \"n\", \"id\")\ndd &lt;- addColumns(d1, dd)\ndd\n##      site  n      a rx  id    y\n##  1:    1 20 -0.355  1  1 17.7\n##  2:    1 20 -0.355  1  2 16.2\n##  3:    1 20 -0.355  1  3 19.2\n##  4:    1 20 -0.355  1  4 20.6\n##  5:    1 20 -0.355  1  5 14.7\n##  ---                           \n## 196:  10 20  0.651  0 196 25.3\n## 197:  10 20  0.651  0 197 22.1\n## 198:  10 20  0.651  0 198 13.2\n## 199:  10 20  0.651  0 199 15.6\n## 200:  10 20  0.651  0 200 13.8\n\ngenCluster performs an expand.grid to generate an individual-level dataset along with adding an ID variable\naddColumns uses individual-level data and outcome variable definition to generate the outcome variable and add it to the dataset.\n\n\nExample: Varying Cluster Sizes and therefore Varying Sample Size\nd0 &lt;- defData(varname = \"n\", formula = 20, dist = \"poisson\")\ngenData(10, d0, \"site\")\n##    site  n\n##  1:    1 13\n##  2:    2 18\n##  3:    3 21\n##  4:    4 26\n##  5:    5 25\n##  6:    6 27\n##  7:    7 23\n##  8:    8 30\n##  9:    9 23\n## 10:  10 20\n\nFormula sets the poisson distribution parameter, \\(\\lambda = 20\\). So sizes are sampled from poisson distribution with that mean/variance\nTo increase the variability between clusters, use the negative binomial distribution\nMost likely leads to an unbalanced design\n\nExample: Varying Cluster Sizes but Constant Sample Size\n# moderately varying cluster sizes\nd0 &lt;- defData(varname = \"n\", formula = 200, variance = 0.2, dist = \"clusterSize\")\ngenData(10, d0, \"site\")\n\n##    site  n\n##  1:    1 20\n##  2:    2 28\n##  3:    3 25\n##  4:    4 24\n##  5:    5 28\n##  6:    6 22\n##  7:    7  7\n##  8:    8 13\n##  9:    9 22\n## 10:  10 11\n\n# Very highly varying cluster sizes\nd0 &lt;- defData(varname = \"n\", formula = 200, variance = 5, dist = \"clusterSize\")\ngenData(10, d0, \"site\")\n##    site  n\n##  1:    1  10\n##  2:    2  2\n##  3:    3  17\n##  4:    4  2\n##  5:    5  49\n##  6:    6 110\n##  7:    7  1\n##  8:    8  4\n##  9:    9  1\n## 10:  10  4\n\nTotal sample size is fixed at 200 (formula), but individual cluster sizes are allowed to vary.\nvariance: A dispersion parameter that controls the amount of varying of the cluster sizes",
    "crumbs": [
      "Simulation, Data"
    ]
  },
  {
    "objectID": "qmd/association-general.html",
    "href": "qmd/association-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-misc",
    "href": "qmd/association-general.html#sec-assoc-gen-misc",
    "title": "General",
    "section": "",
    "text": "Also see\n\nEDA &gt;&gt; Correlation\nNotebook &gt;&gt; Statistical Inference &gt;&gt; Correlation\n\nE(υ|x)=0 is equivalent to Cov(x,υ)=0 or Cor(x,υ)=0\nA negative correlation between variables is also called anticorrelation or inverse correlation\nIndependence - Two random variables are independent if the product of their individual probability density functions equals the joint probability density function",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-partcor",
    "href": "qmd/association-general.html#sec-assoc-gen-partcor",
    "title": "General",
    "section": "Partial Correlation",
    "text": "Partial Correlation\n\nStatistical Formula\n\\[\n\\frac{\\mbox{Cov}(X, Y) - \\mbox{Cov}(X, Z) \\cdot \\mbox{Cov}(Y, Z)}{\\sqrt{\\mbox{Var}(X) - \\mbox{Cov}(X, Z)^2}\\cdot \\sqrt{\\mbox{Var}(Y) - \\mbox{Cov}(Y, Z)^2}}\n\\]\nMeasures the association (or correlation) between two variables when the effects of one or more other variables are removed from such a relationship.\n\nIn the above equation, I think it’s the partial correlation between x and y given z.\n\nMisc\n\nResources\n\nDealing with correlation in designed field experiments: part I\n\nExcellent tutorial on partial, joint correlations in block design\n\nppcor pkg: An R Package for a Fast Calculation to Semi-partial Correlation Coefficients\n\nExplainer for semi-partial, partial correlation\n\nAlso see notebook for a method using regression models\n\n\nExample: psych::partial.r(y ~ x - z, data)\nExample: {correlation}\nhead(correlation::correlation(mtcars, partial = TRUE))\n\n#&gt; # Correlation Matrix (pearson-method)\n\n#&gt; Parameter1 | Parameter2 |     r |         95% CI | t(30) |      p\n#&gt; -----------------------------------------------------------------\n#&gt; mpg        |        cyl | -0.02 | [-0.37,  0.33] | -0.13 | &gt; .999\n#&gt; mpg        |       disp |  0.16 | [-0.20,  0.48] |  0.89 | &gt; .999\n#&gt; mpg        |         hp | -0.21 | [-0.52,  0.15] | -1.18 | &gt; .999\n#&gt; mpg        |       drat |  0.10 | [-0.25,  0.44] |  0.58 | &gt; .999\n#&gt; mpg        |         wt | -0.39 | [-0.65, -0.05] | -2.34 | &gt; .999\n#&gt; mpg        |       qsec |  0.24 | [-0.12,  0.54] |  1.34 | &gt; .999\n#&gt; \n#&gt; p-value adjustment method: Holm (1979)\n#&gt; Observations: 32\n\nVisualization\n\npacman::p_load(see, ggraph)\ncorrelation::correlation(mtcars, partial = TRUE) |&gt; \n  plot()\n\nGraphical LASSO\n\nComputing covariance matrices are computationally expensive while computing its inverse can be less so. This algorithm calculates the inverse covariance matrix (ICT), aka Precision Matrix, and it’s based on an interplay between probability theory and graph theory, in which the properties of an underlying graph specify the conditional independence properties of a set of random variables.\n\nSee Statistical Learning With Sparsity (Hastie, Tibshirani, Wainright)\n\nMathematical introduction to graphical models and Graphical LASSO, pg 241 (252 in pdf), See R &gt;&gt; Documents &gt;&gt; Regression\n\n\nAssumes that the observations have a multivariate Gaussian distribution\nMisc\n\nPackages\n\n{glasso} - The original package by the authors of the algorithm. Estimation of a sparse inverse covariance matrix using a lasso (L1) penalty. Facilities are provided for estimates along a path of values for the regularization parameter. Can be slow or nonconvergent for large dimension datasets.\n{huge} - Provides functions for estimating high dimensional undirected graphs from data. Also provides functions for fitting high dimensional semiparametric Gaussian copula models (Vignette)\n{cglasso} - Conditional Graphical Lasso Inference with Censored and Missing Values (Vignette)\n\n\nPreprocessing: All variables should be standardized.\nThe terms in the ICT are not equivalent but are proportional to the partial correlation between the two corresponding variables\n\nTransform the ICT, \\(\\Omega\\) into a partial correlation matrix, \\(R\\)\n\\[\nR_{j,k} = \\frac{-\\Omega_{i,j}}{\\sqrt{\\Omega_{j,j}\\Omega_{k,k}}}\n\\]\nparr.corr &lt;- matrix(nrow=nrow(P), ncol=ncol(P))\nfor(k in 1:nrow(parr.corr)) {\n  for(j in 1:ncol(parr.corr)) {\n    parr.corr[j, k] &lt;- -P[j,k]/sqrt(P[j,j]*P[k,k])\n  }\n}\ncolnames(parr.corr) &lt;- colnames(P)\nrownames(parr.corr) &lt;- colnames(P)\ndiag(parr.corr) &lt;- 0\nSetting the terms on the diagonal to zero prevents variables from having connections with themselves in a network graph if you want to visualize the relationships\n\nWhere the nodes are variables and edges are the partial correlations.\n\n\nHyperparameter, \\(\\rho\\) , adjusts the sparsity of the matrix output\n\nHigher: Isolates the strongest relationships in your data (more sparse)\nLower: Preserving more tenuous connections, perhaps identifying variables with connections to multiple groups (less sparse)\n\nCheck symmetry. Assymmetry in the ICT can arise due to numerical computation and rounding errors, which can cause problems later depending on what you want to do with the matrix.\nExample: Stock Analysis using {glasso} (link)\nrho &lt;- 0.75\ninvcov &lt;- glasso(S, rho=rho)  \n\n# inverse covariance matrix\nP &lt;- invcov$wi\ncolnames(P) &lt;- colnames(S)\nrownames(P) &lt;- rownames(S)\n\n# check symmetry\nif(!isSymmetric(P)) {\n  P[lower.tri(P)] = t(P)[lower.tri(P)]  \n}\n\nGoal: Remove stocks relationship with market Beta and other confounding stocks to get the true relationsip between stock pairs.\nPost also has a network visualization. Data was put through PCA, then DBSCAN to get clusters. The cluster assignments were used to color the clusters in the network graph.\nPost also examines output from a lower \\(\\rho\\) and has an interesting analysis of the non-connected variables (i.e. no partial correlation).",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-cont",
    "href": "qmd/association-general.html#sec-assoc-gen-cont",
    "title": "General",
    "section": "Continuous",
    "text": "Continuous\n\nSpearman’s Rank\n\\[\n\\rho = 1 - \\frac{6\\sum_i d_i^2}{n(n^2-1)}\n\\]\n\n\\(d_i\\): The difference in ranks for the ith observation\nMeasures how well the relationship between the two variables can be described by a monotonic function\nRank correlation measures the similarity of the order of two sets of data, relative to each other (recall that PCC did not directly measure the relative rank).\n\nValues range from -1 to 1 where 0 is no association and 1 is perfect association\nNegative values don’t mean anything in ranked correlation, so just remove the negative\n\nLinear relationship is a specific type of monotonic relationship where the rate of increase remains constant — in other words, unlike a linear relationship, the amount of change (increase or decrease) in a monotonic relationship can vary.\nSee bkmks for CIs\nPackages\n\n{stats::cor.test(method = “spearman”)}\n{DescTools::SpearmanRho}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\n\nKendall’s Tau\n\nNon-parametric rank correlation\n\nNon-parametric because it only measures the rank correlation based on the relative ordering of the data (and not the specific values of the data).\n\nShould be pretty close to Sspearman’s Rank but a potentially faster calculation\nFlavors: a, b (makes adjustment for ties), c (for different sample sizes for each variable)\n\nUse Tau-b if the underlying scale of both variables has the same number of possible values (before ranking) and Tau-c if they differ.\ne.g. One variable might be scored on a 5-point scale (very good, good, average, bad, very bad), whereas the other might be based on a finer 10-point scale. In this case, Tau-c would be recommended.\n\nPackages\n\n{stats::cor.test(method = “kendall”)} - Doesn’t state specifically but I think it calculates a and b depending on whether ties are present or not\n{DescTools} - has all 3 flavors\n\n\nHoeffding’s D\n\nRank-based approach that measures the difference between the joint ranks of (X,Y) and the product of marginal ranks.(?) A non-parametric test of independence. the product of their marginal ranks.\nUnlike the Pearson or Spearman measures, it can pick up on nonlinear relationships.\nRange: [-.5,1]\nGuidelines: Larger values indicate a stronger relationship between the variables.\nPackages\n\n{Hmisc::hoeffd}\n{DescTools::HoeffD}\n\n\nBayesian\n\nSteps: {brms}\n\nList the variables you’d like correlations for within mvbind().\nPlace the mvbind() function within the left side of the model formula.\nOn the right side of the model formula, indicate you only want intercepts (i.e., ~ 1).\nWrap that whole formula within bf().\nThen use the + operator to append set_rescor(TRUE), which will ensure brms fits a model with residual correlations.\nUse non-default priors and the resp argument to specify which prior is associated with which criterion variable\n\nGaussian\n\nExample: multiple variables\nf9 &lt;- \n   brm(data = d,\n    family = gaussian,\n    bf(mvbind(x_s, y_s, z_s) ~ 0,\n       sigma ~ 0) +\n    set_rescor(TRUE),\n    prior(lkj(2), class = rescor),\n    chains = 4, cores = 4,\n    seed = 1)\n\n## Residual Correlations: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## rescor(xs,ys)    0.90      0.02    0.87    0.93 1.00    3719    3031\n## rescor(xs,zs)    0.57      0.07    0.42    0.69 1.00    3047    2773\n## rescor(ys,zs)    0.29      0.09    0.11    0.46 1.00    2839    2615\nStandardized data is used here but isn’t required\n\nWill need to set priors though (see article for further details)\n\nSince the data is standardized, the sd can be fixed at 1\n\nbrms models log of sd by default, hence sigma ~ 0 since log 1 = 0\n\nCorrelations are the estimates for rescor(xs,ys), rescor(xs,zs) rescor(ys,zs)\n\nStudent t-distribution\n\nIf the data has any outliers, pearson’s coefficient is substantially biased.\nExample: correlation between x and y\n\\\nf2 &lt;- \n    brm(data = x.noisy, \n    family = student,\n    bf(mvbind(x, y) ~ 1) + set_rescor(TRUE),\n    prior = c(prior(gamma(2, .1), class = nu),\n              prior(normal(0, 100), class = Intercept, resp = x),\n              prior(normal(0, 100), class = Intercept, resp = y),\n              prior(normal(0, 100), class = sigma, resp = x),\n              prior(normal(0, 100), class = sigma, resp = y),\n              prior(lkj(1), class = rescor)),\n    iter = 2000, warmup = 500, chains = 4, cores = 4, \n    seed = 210191)\n\n## Population-Level Effects: \n##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## x_Intercept    -2.07      3.59    -9.49    4.72 1.00    2412    2651\n## y_Intercept    1.93      7.20  -11.31    16.81 1.00    2454    2815\n## \n## Family Specific Parameters: \n##        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma_x    18.35      2.99    13.12    24.76 1.00    2313    2816\n## sigma_y    36.52      5.90    26.13    49.49 1.00    2216    3225\n## nu          2.65      0.99    1.36    4.99 1.00    3500    2710\n## nu_x        1.00      0.00    1.00    1.00 1.00    6000    6000\n## nu_y        1.00      0.00    1.00    1.00 1.00    6000    6000\n## \n## Residual Correlations: \n##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## rescor(x,y)    -0.93      0.03    -0.97    -0.85 1.00    2974    3366\n\nN = 40 simulated from a multivariate normal with 3 outliers\nCorrelation is the rescor(x,y) estimate -0.93; true value is -0.96\n\nUsing a pearson coefficient, cor = -0.6365649\nUsing brms::brm with family = gaussian, rescor(x,y) estimate -0.61",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-disc",
    "href": "qmd/association-general.html#sec-assoc-gen-disc",
    "title": "General",
    "section": "Discrete",
    "text": "Discrete\n\nMisc\n\nAlso see\n\nMultiple Correspondence Analysis (MCA) (see bkmks &gt;&gt; Features &gt;&gt; Reduction)\nDiscrete Analysis Notebook\n\nPackages\n\n{PAsso} - Assesses the Partial Association Between Ordinal Variables\n\nAllows users to perform a wide spectrum of assessments, including quantification, visualization, and hypothesis testing.\nVignette\n\n\nBinary vs Binary Similarity measures (paper)\n\nNote that a pearson correlation between binaries can be useful (see EDA &gt;&gt; Misc &gt;&gt; {correlationfunnel})\nTypes:\n\nJaccard-Needham\nDice\nYule\nRussell-Rao\nSokal-Michener\nRogers-Tanimoto\nKulzinsky\n\nPackages\n\n{{scipy}} - Also has other similarity measures\n\n\n\nPhi Coefficient - Used for binary variables when the categories are truly binary and not crudely measuring some underlying continuous variable (i.e. dichotomization of a continuous variable)\n\n“A Pearson correlation coefficient estimated for two binary variables will return the phi coefficient” (Phi coefficient wiki)\n(Contingency Table) Two binary variables are considered positively associated if most of the data falls along the diagonal cells. In contrast, two binary variables are considered negatively associated if most of the data falls off the diagonal\nAlso see StackExchange discussion on the difference between Phi Coefficient and Tetrachoric correlation\n{DescTools::Phi}\n\nCramer’s V - Association between two nominal variables\n\nSee Discrete Analysis notebook\n{DescTools::CramerV}\n\nPolychoric - Suppose each of the ordinal variables was obtained by categorizing a normally distributed underlying variable, and those two unobserved variables follow a bivariate normal distribution. Then the (maximum likelihood) estimate of that correlation is the polychoric correlation.\n\n{polycor}\n{psych::polychoric}\n\nFor correct=FALSE, the results agree perfectly with {polycor}\nFor very small data sets, the correction for continuity for the polychoric correlations can lead to difficulties, particularly if using the global=FALSE option, or if doing just one correlation at a time. Setting a smaller correction value (i.e., correct =.1) seems to help.\n\n{DescTools::CorPolychor}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\nTetrachoric - Used for binary variables when those variables are a sort of crude measure of an underlying continuous variable\n\nAlso see StackExchange discussion on the difference between Phi Coefficient and Tetrachoric correlation\nExample of appropriate use case: Suppose there are two judges who judge cakes, say, on some continuous scale, then based on a fixed, perhaps unknown, cutoff, pronounce the cakes as “bad” or “good”. Suppose the latent continuous metric of the two judges has correlation coefficient ρ.\n“the contingency tables are ‘balanced’ row-wise and col-wise, you get good correlation between the two metrics, but the tetrachoric tends to be a bit larger than the phi coefficient. When the cutoffs are somewhat imbalanced, you get slightly worse correlation between the metrics, and the phi appears to ‘shink’ towards zero.”\nThe estimation procedure is two stage ML.\n\nCell frequencies for each pair of items are found. Cells with zero counts are replaced with .5 as a correction for continuity (correct=TRUE).\nThe marginal frequencies are converted to normal theory thresholds and the resulting table for each item pair is converted to the (inferred) latent Pearson correlation that would produce the observed cell frequencies with the observed marginals\n\n{psych::tetrachoric}\n\nThe correlation matrix gets printed, but the correlations can also be extracted with $rho\nCan be sped up considerably by using multiple cores and using the parallel package. The number of cores to use when doing polychoric or tetrachoric may be specified using the options command. (e.g options(\"mc.cores\"=4);)\nsmooth = TRUE - For sets of data with missing data, the matrix will sometimes not be positive definite. Uses a procedure to transform the negative eigenvalues.\nFor relatively small samples with dichotomous data if some cells are empty, or if the resampled matrices are not positive semi-definite, warnings are issued. this leads to serious problems if using multi.cores. The solution seems to be to not use multi.cores (e.g., options(mc.cores =1)\n\nGoodman and Kruskal’s Gamma\n\nA measure of rank correlation, i.e., the similarity of the orderings of the data when ranked by each of the quantities. It measures the strength of association of the cross tabulated data when both variables are measured at the ordinal level.\nFor 2-way contingincy tables (i.e. 2x2 tables)\nIt makes no adjustment for either table size or ties.\nValues range from −1 (100% negative association, or perfect inversion) to +1 (100% positive association, or perfect agreement). A value of zero indicates the absence of association.\n{DescTools::GoodmanKruskalGamma}",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-mix",
    "href": "qmd/association-general.html#sec-assoc-gen-mix",
    "title": "General",
    "section": "Mixed",
    "text": "Mixed\n\nMisc\n\nAlso see\n\nPaper: JEL Ratio Test is non-parametric test that uses the categorical Gini covariance.\n\n{psych::mixedCor} - Finds Pearson correlations for the continous variables, polychorics for the polytomous items, tetrachorics for the dichotomous items, and the polyserial or biserial correlations for the various mixed variables (no polydi?)\n\nBiserial - correlation between a continuous variable and binary variable, which is assumed to have resulted from a dichotomized normal variable\n\n{psych::biserial}\n\nPolydi - correlation between multinomial variable and binary variable\n\n{psych::polydi}\n\nPolyserial - polychoric correlation between a continuous variable and ordinal variable\n\nBased on the assumption that the joint distribution of the quantitative variable and a latent continuous variable underlying the ordinal variable is bivariate normal\n{polycor}\n{psych::polyserial}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\nX2Y\n\nHandles types: continuous-continuous, continuous-categorical, categorical-continuous and categorical-categorical\nCalculates the % difference in prediction error after fitting a decision tree between two variables of interest and the mean (numeric) or most frequent (categorical)\nFunction is available through a script (Code &gt;&gt; statistical-testing &gt;&gt; correlation)\n\narticle with documentation and usage, https://rviews.rstudio.com/2021/04/15/an-alternative-to-the-correlation-coefficient-that-works-for-numeric-and-categorical-variables/\n\nAll x2y values where the y variable is continuous will be measuring a % reduction in MAE. All x2y values where the y variable is categorical will be measuring a % reduction in Misclassification Error. Is a 30% reduction in MAE equal to a 30% reduction in Misclassification Error? It is problem dependent, there’s no universal right answer.\n\nOn the other hand, since (1) all x2y values are on the same 0-100% scale (2) are conceptually measuring the same thing, i.e., reduction in prediction error and (3) our objective is to quickly scan and identify strongly-related pairs (rather than conduct an in-depth investigation), the x2y approach may be adequate.\n\nNot symmetric, but can average both scores to get a pseudo-symmetric value\nBootstrap CIs available\n\nCopulas\n\nlatentcor PKG: semi-parametric latent Gaussian copula models",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-nonlin",
    "href": "qmd/association-general.html#sec-assoc-gen-nonlin",
    "title": "General",
    "section": "Non-linear",
    "text": "Non-linear\n\nMisc\n\nAlso see General Additive Models &gt;&gt; Diagnostics for a method of determining a nonlinear relationship for either continuous or categorical outcomes.\n\nξ (xi) coefficient\n\nPaper: A New Coefficient of Correlation\nArticle: Exploring the XI Correlation Coefficient\nExcels at oscillatory and highly non-monotonic dependencies\nXICOR::xicor - calculates ξ and performs a significance test (H0: independent)\n\nXICOR::calculateXI just calculates the ξ coefficient\n\nProperties (value ranges; interpretation)\n\nIf y is a function of x, then ξ goes to 1 asymptotically as n (the number of data points, or the length of the vectors x and y) goes to Infinity.\nIf y and x are independent, then ξ goes to 0 asymptotically as n goes to Infinity.\n\nValues can be negative, but this negativity does not have any innate significance other than being close to zero\nn &gt; 20 necessary\n\nn larger than about 250 probably sufficient to get a good estimate\n\nFairly efficient (O(nlogn), compared to some more powerful methods, which are O(n2))\nIt measures dependency in one direction only (is y dependent on x not vice versa)\nDoesn’t tell you if the relationship is direct or inverse",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/base-r.html",
    "href": "qmd/base-r.html",
    "title": "Base R",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-misc",
    "href": "qmd/base-r.html#sec-baser-misc",
    "title": "Base R",
    "section": "",
    "text": "Magrittr + base\nmtcars %&gt;% {plot(.$hp, .$mpg)}\nmtcars %$% plot(hp, mpg)\n\nBy wrapping the RHS in curly braces, we can override the rule where the LHS is passed to the first argument ## Options {#sec-baser-opts .unnumbered}\n\nRemove scientific notation\noptions(scipen = 999)\nWide and long printing tibbles\n# in .Rprofile\nmakeActiveBinding(\".wide\", function() { print(.Last.value, width = Inf) }, .GlobalEnv)\n\nAfter printing a tibble, if you want to see it in wide, then just type .wide + ENTER.\nCan have similar bindings for `.long` and `.full`.\n\nHeredocs - Powerful feature in various programming languages that allow you to define a block of text within the code, preserving line breaks, indentation, and other whitespace.\ntext &lt;- r\"(\nThis is a\nmultiline string\nin R)\"\n\ncat(text)",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-usrfuns",
    "href": "qmd/base-r.html#sec-baser-usrfuns",
    "title": "Base R",
    "section": "User Defined Functions",
    "text": "User Defined Functions\n\nAnonymous (aka lambda) functions: \\(x) {} (&gt; R 4.1)\nfunction(x) {\n  x[which.max(x$mpg), ]\n}\n# equivalent to the above\n\\(x) {\n  x[which.max(x$mpg), ]\n}\nDots (…)\n\nMisc\n\n{ellipsis}: Functions for testing functions with dots so they fail loudly\n{rlang} dynamic dots: article\n\nSplice arguments saved in a list with the splice operator, !!! .\nInject names with glue syntax on the left-hand side of := .\n\n\nUser Defined Functions\nmoose &lt;- function(...) {\n    dots &lt;- list(...)\n    dots_names &lt;- names(dots)\n    if (is.null(dots_names) || \"\" %in% dots_names {\n        stop(\"All arguments must be named\")\n    }\n}\nNested Functions\nf02 &lt;- function(...){\n  vv &lt;- list(...)\n  print(vv)\n}\nf01 &lt;- function(...){\n  f02(b = 2,...)\n}\n\nf01(a=1,c=3)\n#&gt; $b\n#&gt; [1] 2\n#&gt; \n#&gt; $a\n#&gt; [1] 1\n#&gt; \n#&gt; $c\n#&gt; [1] 3\nSubset dots values\nadd2 &lt;- function(...) {\n    ..1 + ..2\n}\nadd2(3, 0.14)\n# 3.14\nSubset dots dynamically: ...elt(n)\n\nSet a value to n and get back the value of that argument\n\nNumber of arguments in … : ...length()",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-funs",
    "href": "qmd/base-r.html#sec-baser-funs",
    "title": "Base R",
    "section": "Functions",
    "text": "Functions\n\ndo.call - allows you to call other functions by constructing the function call as a list\n\nArgs\n\nwhat – Either a function or a non-empty character string naming the function to be called\nargs – A list of arguments to the function call. The names attribute of args gives the argument names\nquote – A logical value indicating whether to quote the arguments\nenvir – An environment within which to evaluate the call. This will be most useful if what is a character string and the arguments are symbols or quoted expressions\n\nExample: Apply function to list of vectors\nvectors &lt;- list(c(1, 2, 3), c(4, 5, 6), c(7, 8, 9))\ncombined_matrix &lt;- do.call(rbind, vectors)\n\ncombined_matrix\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    5    6\n## [3,]    7    8    9\nExample: Apply multiple functions\ndata_frames &lt;- list(\n  data.frame(a = 1:3), \n  data.frame(a = 4:6), \n  data.frame(a = 7:9)\n  )\nmean_results &lt;- do.call(\n  rbind, \n  lapply(data_frames, function(df) mean(df$a))\n  )\n\nmean_results\n##      [,1]\n## [1,]    2\n## [2,]    5\n## [3,]    8\n\nFirst the mean is calculated for column a of each df using lapply\n\nlapply is supplying the data for do.call in the required format, which is a list or character vector.\n\nSecond the results are combined into a matrix with rbind\n\n\nsink - used to divert R output to an external connection.\n\nUse Cases: exporting data to a file, logging R output, or debugging R code.\nArgs\n\nfile: The name of the file to which R output will be diverted. If file is NULL, then R output will be diverted to the console.\nappend: A logical value indicating whether R output should be appended to the file (TRUE) or overwritten (FALSE). The default value is FALSE.\ntype: A character string. Either the output stream or the messages stream. The name will be partially match so can be abbreviated.\nsplit: logical: if TRUE, output will be sent to the new sink and the current output stream, like the Unix program tee.\n\nExample: Logging output of code to file\nsink(\"r_output.log\")      # Redirect output to this file\n# Your R code goes here\nsink()                    # Turn off redirection\n\noutput file could also have an extension like “.txt”\n\nExample: Debugging\nsink(\"my_function.log\")   # Redirect output to this file\nmy_function()\nsink()                    # Turn off redirection\nExample: Appending output to a file\nsink(\"output.txt\", append = TRUE)  # Append output to the existing file\ncat(\"Additional text\\n\")  # Append custom text\nplain text\nsink()  # Turn off redirection\n\npmin and pmax\n\nFind the element-wise maximum and minimum values across vectors in R\nExample\nvec1 &lt;- c(3, 9, 2, 6)\nvec2 &lt;- c(7, 1, 8, 4)\npmax(vec1, vec2)\n#&gt; [1] 7 9 8 6\npmin(vec1, vec2)\n#&gt; [1] 3 1 2 4\nExample: With NAs\ndata1 &lt;- c(7, 3, NA, 12)\ndata2 &lt;- c(9, NA, 5, 8)\npmax(data1, data2, na.rm = TRUE)\n#&gt; [1] 9 3 5 12\n\nswitch\n\nExample:\nswitch(parallel,\n         windows = \"snow\" -&gt; para_proc,\n         other = \"multicore\" -&gt; para_proc,\n         no = \"no\" -&gt; para_proc,\n         stop(sprintf(\"%s is not one of the 3 possible parallel argument values. See documentation.\", parallel)))\n\nparallel is the function argument. If it doesn’t match one of the 3 values, then an error is thrown.\nIf the argument value is matched, then the quoted value is stored in para_proc\n\n\ndynGet\n\nLooks for objects in the environment of a function.\nWhen an object from the outer function is an input for a function nested around 3 layers deep or more, it may not be found by that most inner function. dynGet allows that function to find the object in the outer frame\nArguments\n\nminframe: Integer specifying the minimal frame number to look into (i.e. how far back to look for the object)\ninherits: Should the enclosing frames of the environment be searched?\n\nExample:\n1function(args) {\n  if (method == \"kj\") {\n      ncv_list &lt;- purrr::map2(grid$dat, \n                              grid$repeats, \n                              function(dat, reps) {\n         rsample::nested_cv(dat,\n                            outside = vfold_cv(v = 10, \n                                               repeats = dynGet(\"reps\")),\n                            inside = bootstraps(times = 25))\n      })\n  }\n}\n\n2function(data) {\n    if (chk::vld_used(...)) {\n        dots &lt;- list(...)\n        init_boot_args &lt;-\n          list(data = dynGet(\"data\"),\n               stat_fun = cles_boot, # internal function\n               group_variables = group_variables,\n               paired = paired)\n        get_boot_args &lt;-\n          append(init_boot_args,\n                 dots)\n    }\n    cles_booted &lt;-\n      do.call(\n        get_boot_ci,\n        get_boot_args\n      )\n}\n\n1\n\nExample from Nested Cross-Validation Comparison\n\n2\n\nExample from {ebtools::cles}\n\n\n\nmatch.arg\n\nPartially matches a function’s argument values to list of choices. If the value doesn’t match the choices, then an error is thrown\nExample:\nkeep_input &lt;- \"input_le\"\nkeep_input_val &lt;- \n  match.arg(keep_input,\n            choices = c(\"input_lags\",\n                        \"input_leads\",\n                        \"both\"),\n            several.ok = FALSE)\nkeep_input_val\n#&gt; [1] \"input_leads\"\n\nseveral.ok = FALSE says only 1 match is allowed otherwise an error is thrown.\nThe error message is pretty informative btw.\n\n\nmatch.fun\n\nExample\nf &lt;- function(a,b) {\n  a + b\n}\ng &lt;- function(a,b,c) {\n  (a + b) * c\n}\nh &lt;- function(d,e) {\n  d - e\n}\nyolo &lt;- function(FUN, ...) {\n  FUN &lt;- match.fun(FUN)\n  params &lt;- list(...)\n  FUN_formals &lt;- formals(FUN)\n  idx &lt;- names(params) %in% names(FUN)\n  do.call(FUN, params[idx])\n}\nyolo(h, d = 2, e = 3)\n#&gt; -1",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-pipe",
    "href": "qmd/base-r.html#sec-baser-pipe",
    "title": "Base R",
    "section": "Pipe",
    "text": "Pipe\n\n\nBenefits of base pipe\n\nMagrittr pipe is bloated with special features which may make it slower than the base pipe\nIf not using tidyverse, it’s one less dependency (maybe one day it will be deprecated in tidyverse)\n\nBase pipe with base and anonymous functions\n# verbosely\nmtcars |&gt; (function(.) plot(.$hp, .$mpg))()\n# using the anonymous function shortcut, emulating the dot syntax\nmtcars |&gt; (\\(.) plot(.$hp, .$mpg))()\n# or if you prefer x to .\nmtcars |&gt; (\\(x) plot(x$hp, x$mpg))()\n# or if you prefer to be explicit with argument names\nmtcars |&gt; (\\(data) plot(data$hp, data$mpg))()\nUsing “_” placeholder:\n\nmtcars |&gt; lm(mpg ~ disp, data = _)\nmtcars |&gt; lm(mpg ~ disp, data = _) |&gt; _$coef\n\nBase pipe .[ ]  hack\nwiki |&gt;\n  read_html() |&gt;\n  html_nodes(\"table\") |&gt;\n  (\\(.) .[[2]])() |&gt;\n  html_table(fill = TRUE) |&gt;\n  clean_names()\n# instead of\ndjia &lt;- wiki %&gt;%\n  read_html() %&gt;%\n  html_nodes(\"table\") %&gt;%\n  .[[2]] %&gt;%\n  html_table(fill = TRUE) %&gt;%\n  clean_names()\nMagrittr, base pipe differences\n\nmagrittr: %&gt;% allows you change the placement with a . placeholder.\n\nbase: R 4.2.0 added a _ placeholder to the base pipe, with one additional restriction: the argument has to be named\n\nmagrittr: With %&gt;% you can use . on the left-hand side of operators like “\\(\", \\[\\[, \\[ and use in multiple arguments (e.g. df %&gt;% {split(.\\)x, .$y))\n\nbase: can hack this by using anonymous function\n\nsee Base pipe with base and anonymous functions above\nsee Base pipe .[ ]  hack above\n\n\nmagrittr: %&gt;% allows you to drop the parentheses when calling a function with no other arguments (e.g. dat %&gt;% distinct)\n\nbase: |&gt; always requires the parentheses. (e.g. dat |&gt; distinct())\n\nmagrittr: %&gt;% allows you to start a pipe with . to create a function rather than immediately executing the pipe\n\nPurrr with base pipe\ndata_list |&gt;\n  map(\\(x) clean_names(x))\n# instead of\ndata_list %&gt;%\n  map( ~.x %&gt;% clean_names)\n# with split\nstar |&gt;\n  split(~variable) |&gt;\n  map_df(\\(.) hedg_g(., reading ~ value), .id = \"variable\")\n# instead of\nstar %&gt;%\n  split(.$variable) %&gt;%\n  map_df(. %&gt;% hedg_g(., reading ~ value), .id = \"variable\")",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-str",
    "href": "qmd/base-r.html#sec-baser-str",
    "title": "Base R",
    "section": "Strings",
    "text": "Strings\n\nsprintf\nx &lt;- 123.456               # Create example data\n\nsprintf(\"%f\", x)           # sprintf with default specification\n#&gt; [1] \"123.456000\"\n\nsprintf(\"%.10f\", x)        # sprintf with ten decimal places\n#&gt; [1] \"123.4560000000\"\n\nsprintf(\"%.2f\", x)         # sprintf with two rounded decimal places\n#&gt; [1] \"123.46\"\n\nsprintf(\"%1.0f\", x)        # sprintf without decimal places\n#&gt; [1] \"123\"\n\nsprintf(\"%10.0f\", x)       # sprintf with space before number\n#&gt; [1] \"       123\"\n\nsprintf(\"%10.1f\", x)       # Space before number & decimal places\n#&gt; [1] \"     123.5\"\n\nsprintf(\"%-15f\", x)        # Space on right side\n#&gt; [1] \"123.456000     \"\n\nsprintf(\"%+f\", x)          # Print plus sign before number\n#&gt; [1] \"+123.456000\"\n\nsprintf(\"%e\", x)           # Exponential notation\n#&gt; [1] \"1.234560e+02\"\n\nsprintf(\"%E\", x)           # Exponential with upper case E\n#&gt; [1] \"1.234560E+02\"\n\nsprintf(\"%g\", x)           # sprintf without decimal zeros\n#&gt; [1] \"123.456\"\n\nsprintf(\"%g\", 1e10 * x)    # Scientific notation\n#&gt; [1] \"1.23456e+12\"\n\nsprintf(\"%.13g\", 1e10 * x) # Fixed decimal zeros\n#&gt; [1] \"1234560000000\"\n\npaste0(sprintf(\"%f\", x),   # Print %-sign at the end of number\n       \"%\")\n#&gt; [1] \"123.456000%\"\n\nsprintf(\"Let's create %1.0f more complex example %1.0f you.\", 1, 4)\n#&gt; [1] \"Let's create 1 more complex example 4 you.\"\nstr2lang - Allows you to turn plain text into code.\ngrowth_rate &lt;- \"circumference / age\"\nclass(str2lang(growth_rate))\n#&gt; [1] \"call\"\n\nExample: Basic\neval(str2lang(\"2 + 2\"))\n#&gt; [1] 4\n\neval(str2lang(\"x &lt;- 3\"))\nx\n#&gt; [1] 3\nExample: Run formula against a df\ngrowth_rate &lt;- \"circumference / age\"\nwith(Orange, eval(str2lang(growth_rate)))\n\n#&gt;   [1] 0.25423729 0.11983471 0.13102410 0.11454183 0.09748172 0.10349854\n#&gt;   [7] 0.09165613 0.27966102 0.14256198 0.16716867 0.15537849 0.13972380\n#&gt;  [13] 0.14795918 0.12831858 0.25423729 0.10537190 0.11295181 0.10756972\n#&gt;  [19] 0.09341998 0.10131195 0.08849558 0.27118644 0.12809917 0.16867470\n#&gt;  [25] 0.16633466 0.14541024 0.15233236 0.13527181 0.25423729 0.10123967\n#&gt;  [31] 0.12198795 0.12450199 0.11535337 0.12682216 0.11188369",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-cond",
    "href": "qmd/base-r.html#sec-baser-cond",
    "title": "Base R",
    "section": "Conditionals",
    "text": "Conditionals\n\n&& and || are intended for use solely with scalars, they return a single logical value.\n\nSince they always return a scalar logical, you should use && and || in your if/while conditional expressions (when needed). If an & or | is used, you may end up with a non-scalar vector inside if (…) {} and R will throw an error.\n\n& and | work with multivalued vectors, they return a vector whose length matches their input arguments.\nAlternative way of negating a condition or set of conditions: if (!(condition))\n\nMakes it less readable IMO, but maybe for a complicated set of conditions if makes more sense in your head to do it this way\nExample\nif (!(nr == nrow(iris) || (nr == nrow(iris) - 2))) {print(\"moose\")}\n\nUsing else if\nif (condition1) {\n  expr1\n} else if (condition2) {\n  expr2\n} else {\n  expr3\n}\nstopifnot\npred_fn &lt;- function(steps_forward, newdata) {\n  stopifnot(steps_forward &gt;= 1)\n  stopifnot(nrow(newdata) == 1)\n  model_f = model_map[[steps_forward]]\n  # apply the model to the last \"before the test period\" row to get\n  # the k-steps_forward prediction\n  as.numeric(predict(model_f, newdata = newdata))\n}\n%||%\n\nCollapse operator which acts like:\n`%||%` &lt;- function(x, y) {\n   if (is_null(x)) y else x\n}\n\nSays if the first (left-hand) input x is NULL, return y. If x is not NULL, return the input\n\nUse Cases\n\nDetermine whether a function argument is NULL\ngithub_remote &lt;- \n  function(repo, username = NULL, ...) {\n    meta &lt;- parse_git_repo(repo)\n    meta$username &lt;- username %||%\n      getOption(\"github.user\") %||%\n      stop(\"Unknown username\")\n  }\nWithin the print argument collapse\nlibrary(rlang)\n\nadd_commas &lt;- function(x) {\n  if (length(x) &lt;= 1) {\n    collapse_arg &lt;- NULL\n  } else {\n    collapse_arg &lt;- \", \"\n  }\n  print(paste0(x, collapse = collapse_arg %||% \"\"))\n}\n\nadd_commas(c(\"apples\"))\n[1] \"apples\"\nadd_commas(c(\"apples\", \"bananas\"))\n[1] \"apples, bananas\"",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-sort",
    "href": "qmd/base-r.html#sec-baser-sort",
    "title": "Base R",
    "section": "Ordering Columns and Sorting Rows",
    "text": "Ordering Columns and Sorting Rows\n\nAscending: df[with(df, order(value)), ]\n\n“value” is the column used to sort the df by\n\nDescending: df[with(df, order(-value)), ]\nBy Multiple Columns\n\nDescending then Ascending: df[with(df, order(-value, id)), ]\n\nChange position of columns\n# Reorder column by index manually\ndf2 &lt;- df[, c(5, 1, 4, 2, 3)]\ndf3 &lt;- df[, c(1, 5, 2:4)]\n# Reorder column by name manually\nnew_order = c(\"emp_id\",\"name\",\"superior_emp_id\",\"dept_id\",\"dept_branch_id\")\ndf2 &lt;- df[, new_order]",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#set-operations",
    "href": "qmd/base-r.html#set-operations",
    "title": "Base R",
    "section": "Set Operations",
    "text": "Set Operations\n\nUnique values in A that are not in B\na &lt;- c(\"thing\", \"object\")\nb &lt;- c(\"thing\", \"gift\")\n\nunique(a[!(a %in% b)])\n#&gt; [1] \"object\"\n\nsetdiff(a, b)\n\nsetdiff is slower\n\nUnique values of the two vectors combined\nunique(c(a, b))\n#&gt; [1] \"thing\"  \"object\" \"gift\"\n\nunion(a, b)\n\nunion is just a wrapper for unique",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-subset",
    "href": "qmd/base-r.html#sec-baser-subset",
    "title": "Base R",
    "section": "Subsetting",
    "text": "Subsetting\n\nLists and Vectors\n\nRemoving Rows\n# Remove specific value from vector\nx[!x == 'A']\n\n# Remove multiple values by list\nx[!x %in% c('A', 'D', 'E')]\n\n# Using setdiff\nsetdiff(x, c('A','D','E'))\n\n# Remove elements by index\nx[-c(1,2,5)]\n\n# Using which\nx[-which(x %in% c('D','E') )]\n\n# Remove elements by name\nx &lt;- c(C1='A',C2='B',C3='C',C4='E',C5='G')\nx[!names(x) %in% c('C1','C2')]\n\nDataframes\n\nRemove specific Rows\ndf &lt;- df[-c(25, 3, 62), ]\nRemove column by name\ndf &lt;- df[, which(names(df) == \"col_name\")]\ndf &lt;- subset(df, select = -c(col_name))\ndf &lt;- df[, !names(df) %in% c(\"col1\", \"col2\"), drop = FALSE]\nFilter and Select\ndf &lt;- subset(df, subset = col1 &gt; 56, select = c(col2, col3))",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#joins",
    "href": "qmd/base-r.html#joins",
    "title": "Base R",
    "section": "Joins",
    "text": "Joins\n\nInner join: inner &lt;- merge(flights, weather, by = mergeCols)\nLeft (outer) join: left  &lt;- merge(flights, weather, by = mergeCols, all.x = TRUE)\nRight (outer) join: right &lt;- merge(flights, weather, by = mergeCols, all.y = TRUE)\nFull (outer) join: full &lt;- merge(flights, weather, by = mergeCols, all = TRUE)\nCross Join (Cartesian product): cross &lt;- merge(flights, weather, by = NULL)\nNatural join: natural &lt;- merge(flights, weather)",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-err",
    "href": "qmd/base-r.html#sec-baser-err",
    "title": "Base R",
    "section": "Error Handling",
    "text": "Error Handling\n\nstop\n\nExample:\nswitch(parallel,\n       windows = \"snow\" -&gt; para_proc,\n       other = \"multicore\" -&gt; para_proc,\n       no = \"no\" -&gt; para_proc,\n       stop(sprintf(\"%s is not one of the 3 possible parallel argument values. See documentation.\", parallel)))\n\nparallel is the function argument. If it doesn’t match one of the 3 values, then an error is thrown.\n\n\ntry\n\nIf something errors, then do something else\nExample\n\ncurrent &lt;- try(remDr$findElement(using = \"xpath\",\n                                '//*[contains(concat( \" \", @class, \" \" ),\n                                concat( \" \", \"product-price-value\", \" \" ))]'),\n                                silent = T)\n#If error : current price is NA\nif(class(current) =='try-error'){\n    currentp[i] &lt;- NA\n} else {\n    # do stuff\n}\ntryCatch\n\nRun the main code, but if it “catches” an error, then the secondary code (the workaround) will run.\n\npct_difference_error_handling &lt;- function(n1, n2) {\n# Try the main code\n  tryCatch(pct_diff &lt;- (n1-n2)/n1,\n        # If you find an error, use this code instead\n          error = return(\n            cat( 'The difference between', as.integer(n1), 'and', as.integer(n2), 'is',\n                  (as.integer(n1)-as.integer(n2)), 'which is',\n                  100*(as.integer(n1)-as.integer(n2))/as.integer(n1),\n                  '% of', n1 )#cat\n            ),\n          # finally = print('Code ended') # optional\n          )#trycatch\n  # If no error happens, return this statement\n  return ( cat('The difference between', n1, 'and', n2, 'is', n1-n2,\n              ', which is', pct_diff*100, '% of', n1) )\n}\n\nAssumes the error will be the user enters a string instead of a numeric. If errors, converts string to numeric and calcs.\n“finally” - This argument will always run, regardless if the try block raises an error or not. So it could be a completion message or a summary, for example.",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-mods",
    "href": "qmd/base-r.html#sec-baser-mods",
    "title": "Base R",
    "section": "Models",
    "text": "Models\n\nreformulate - Create formula sytax programmatically\n# Creating a formula using reformulate()\nformula &lt;- reformulate(c(\"hp\", \"cyl\"), response = \"mpg\")\n\n# Fitting a linear regression model\nmodel &lt;- lm(formula, data = mtcars)\n\nformula\n##&gt; mpg ~ hp + cyl\n\nCan also use as.formula\n\nDF2formula - Turns the column names from a data frame into a formula. The first column will become the outcome variable, and the rest will be used as predictors\nDF2formula(Orange)\n#&gt; Tree ~ age + circumference\nformula - Provides a way of extracting formulae which have been included in other objects\nrec_obj |&gt; prep() |&gt; formula()\n\nWhere “rec_obj” is a tidymodels recipe object\n\nData from Model Object\n\nmodel$model return the data dataframe\ndeparse(model$call$data) gives you that name of your data object as a string.\n\nmodel$call$data gives you the data as an unevaluated symbol;\n\neval(model$call$data) gives you back the original data object, if it is available in the current environment.",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/privacy.html",
    "href": "qmd/privacy.html",
    "title": "Privacy",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/privacy.html#sec-priv-misc",
    "href": "qmd/privacy.html#sec-priv-misc",
    "title": "Privacy",
    "section": "",
    "text": "Also see Simulation, Data",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/privacy.html#sec-priv-tag",
    "href": "qmd/privacy.html#sec-priv-tag",
    "title": "Privacy",
    "section": "Tags",
    "text": "Tags\n\nTag sensitive information in dataframes\nnames(df)\n[1] \"date\" \"first_name\" \"card_number\" \"payment\"\n# assign pii tags\nattr(df, \"pii\") &lt;- c(\"name\", \"ccn\", \"transaction\")\n\nPersonally Identifiable Information (PII)\n\nTag dataframes with the names of regulations that are applicable\nattr(df, \"regs\") &lt;- c(\"CCPA\", \"GDPR\", \"GLBA\")\n\nCCPA is the privacy regulation for California\nGDPR is the privacy regulation for the European Union\nGLBA is the financial regulation for the United States\n\nNeeded because df has credit card and financial information\n\nSaving objects as .rds files preserves tags",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/privacy.html#sec-priv-hash",
    "href": "qmd/privacy.html#sec-priv-hash",
    "title": "Privacy",
    "section": "Hashing",
    "text": "Hashing\n\n{digest}\n\nHash Function\n\nApply Hash Function to PII Fields",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#debugging",
    "href": "qmd/cli-linux.html#debugging",
    "title": "Linux",
    "section": "Debugging",
    "text": "Debugging\n\n\nAlso see set -o xtrace in Scripting &gt;&gt; Commands that should start your script",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html",
    "href": "qmd/regression-survival.html",
    "title": "43  Survival",
    "section": "",
    "text": "43.1 Misc",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-misc",
    "href": "qmd/regression-survival.html#sec-reg-surv-misc",
    "title": "43  Survival",
    "section": "",
    "text": "Model for estimating the time until a particular event occurs\n\ne.g. death of a patient being treated for a disease, failure of an engine part in a vehicle\n\nPrediction models for survival outcomes are important for clinicians who wish to estimate a patient’s risk (i.e. probability) of experiencing a future outcome. The term ‘survival’ outcome is used to indicate any prognostic or time-to-event outcome, such as death, progression, or recurrence of disease. Such risk estimates for future events can support shared decision making for interventions in high-risk patients, help manage the expectations of patients, or stratify patients by disease severity for inclusion in trials.1 For example, a prediction model for persistent pain after breast cancer surgery might be used to identify high risk patients for intervention studies\nOutcome variable: Time until event occurs\nPackages\n\nCRAN Task View\n{survival}\n{censored} - {tidymodels} for censored and survival modelling\n{quantreg} - Quantile Survival Regression\n{msm} - Multi-State Models\n\nVignette\nSee Multistate Models for Medical Applications\n\nTutorial using a heart transplant dataset\n\nStandard survival models only directly model two states: alive and dead. Multi-state models enable directly modeling disease progression where patients are observed to be in various states of health or disease at random intervals, but for which, except for death, the times of entering or leaving states are unknown.\nMulti-state models easily accommodate interval censored intermediate states while making the usual assumption that death times are known but may be right censored.\n\n{grf} - Generalized Random Forest; Causal forest with time-to-event data\n{partykit} - Conditional inference trees; Model-based recursive partitioning trees; can be used with {survival} to create random survival forests\n\n{bonsai}: tidymodels, partykit conditional trees, forests; successor to treesnip — Model Wrappers for Tree-Based Models\n\n{aorsf} - Optimized software to fit, interpret, and make predictions with oblique random survival forests (ORSFs)\n{{sklearn}} - Random Survival Forests, Survival Support Vector Machine\n{rmstbart} - Prognostic model that directly targets the RMST (see Terms) as a function of baseline covariates. The interpretation of each estimated RMST is transparent and does not require a proportional hazards assumption plus additional modeling of a baseline hazard function.\n\nFrom paper: Generalized Bayesian Additive Regression Trees for Restricted Mean Survival Time Inference. (Code)\n\n\nNotes from\n\nWhat is Cox’s proportional hazards model?\n\nWhy not use a standard regression model?\n\nUnits that “survive” until the end of the study will have a censored survival time.\n\ni.e. We won’t have an observed survival time for these units because they survive for an unknown time after the study is completed.\nWe don’t want to discard these units though, as they still have useful information.\n\n\nSample Size\nModels\n\nKaplan Meier model (i.e. K-M survival curve)\n\nOften used as a baseline in survival analysis\nCan not be used to compare risk between groups and compute metrics like the hazard ratio\n\nExponential model, the Weibull model, Cox Proportional-Hazards, Log-logistic and the Accelerated Failure Time (AFT)\nMulti-State Models\nHazard rates and Cumulative Hazard rates are typical quantities of interest\n\nLog-Rank Test (aka Mantel-Cox test) - tests if two groups survival curves are different\n\nNon-Parametric; a special case with one binary X\nThe intuition behind the test is that if the two groups have different hazard rates, the two survival curves (so their slopes) will differ.\nCompares the observed number of events in each group to what would be expected if the survival curves were identical (i.e., if the null hypothesis were true).\nExample\nlibrary(survival)\ndat &lt;- data.frame(\n  group = c(rep(1, 6), rep(2, 6)),\n  time = c(4.1, 7.8, 10, 10, 12.3, 17.2, 9.7, 10, 11.1, 13.1, 19.7, 24.1),\n  event = c(1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0)\n)\ndat\n##    group time event\n## 1      1  4.1    1\n## 2      1  7.8    0\n## 3      1 10.0    1\n## 4      1 10.0    1\n## 5      1 12.3    0\n## 6      1 17.2    1\n## 7      2  9.7    1\n## 8      2 10.0    1\n## 9      2 11.1    0\n## 10    2 13.1    0\n## 11    2 19.7    1\n## 12    2 24.1    0\nsurvdiff(Surv(time, event) ~ group,\n  data = dat\n)\n##        N Observed Expected (O-E)^2/E (O-E)^2/V\n## group=1 6        4    2.57    0.800      1.62\n## group=2 6        3    4.43    0.463      1.62\n## \n##  Chisq= 1.6  on 1 degrees of freedom, p= 0.2\n\n# plot curves with pval from test\nfit &lt;- survfit(Surv(time, event) ~ group, data = dat)\nggsurvplot(fit,\n  pval = TRUE,\n  pval.method = TRUE\n)\n\npval &gt; 0.05, so there isn’t enough evidence to that they’re different.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-terms",
    "href": "qmd/regression-survival.html#sec-reg-surv-terms",
    "title": "43  Survival",
    "section": "43.2 Terms",
    "text": "43.2 Terms\n\nCensoring Time (C): Time at which censoring occurs\n\nFor each unit, we observe Survival Time (T) or C.\n\nY = min(T, C)\n\nRight Censoring: occurs when the event has happened after the enrollment (but the time is unknown).\n\nThe patient does not experience the event for the whole duration of the study.\nThe patient withdraws from the study.\nThe patient is lost to follow-up.\n\nLeft Censoring: occurs when the event has happened before the enrollment (but the time is unknown).\n\nCumulative hazard function (aka Cumulative Hazard Rates)\n\nShows the total accumulated risk of an event occurring at time t\nThe area under the hazard function\n\nHazard Rate (aka Risk Score), \\(h(t \\;|\\; X)\\)\n\nThe hazard rate is the probability that a unit with predictors, X, will experience an event at time, t, given that the unit has survived just before time, t.\nThe formula for the Hazard Rate is the Hazard function.\n\nHazard Ratio (aka Relative Risk of an event): Risk of an event given category / risk of an event given by reference category\n\nThe ratio of two instantaneous event rates\nCoefficient of the Cox Proportional Hazards model (e.g. paper)\n\n\neβ &gt; 1 (or β &gt; 0) for an increased risk of event (e.g. death).\neβ &lt; 1 (or β &lt; 0) for a reduced risk of event.\nHR of 2 is equivalent to raising the entire survival curve for a control subject to the second power to get the survival curve for an exposed subject\n\nExample: if a control subject has 5y survival probability of 0.7 and the exposed:control HR is 2, the exposed subject has a 5y survival probability of 0.49\nIf the HR is 1/2, the exposed subject has a survival curve that is the square root of the control, so S(5) would be √0.7 = 0.837\n\n\n\nRestricted Mean Survival Time (RMST) - The expected survival duration up to a pre-specified truncation time, \\(\\tau\\). It has direct clinical interpretation, which holds regardless of the survival model used in an analysis. Changes in RMST are often cited as an alternative to hazard ratios in the context of treatment comparisons and in survival regression modeling.\n\nUnlike proportional hazards, the interpretation of change in RMST holds regardless of whether or not a particular survival regression model holds.\n\nStatus indicator, \\(\\delta\\)\n\nδ = 1, if T ≤ C (e.g. unit fails before study ends)\n\nTrue survival time is observed\n\nδ = 0, if T &gt; C (e.g. unit survives until end of study or has dropped out)\n\nCensoring time is observed\n\n\nSurvival function (aka Survival Rate), \\(S(T \\lt t)\\):\n\nOutputs the probability of a subject surviving (i.e., not experiencing the event) beyond time t\nMonotonically decreasing (i.e. level or decreasing)\nBaseline survival curve illustrates the survival function when all the covariates are set to their median value\n\nSurvival Time (T) (aka Death, Failure Time, Event Time): Time at which the event occurs",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-km",
    "href": "qmd/regression-survival.html#sec-reg-surv-km",
    "title": "43  Survival",
    "section": "43.3 Kaplan-Meir",
    "text": "43.3 Kaplan-Meir\n\nMisc\n\nUseful for validation of Proportional Hazards assumption. When lines cross the assumption, hazards are found to be non-proportional.\nHarrell RMS (Ch. 20.3):\n\nFor external validation: at least 200 events\nNeed 184 subjects with an event, or censored late, to estimate to within a margin of error of 0.1 everywhere, at the 0.95 confidence level\n\n\nOrder event times (T) of units from smallest to largest, t1 &lt; …. &lt; tk\nCalculate probability that a unit survives past event time, ti, given that they survived up until event time, ti (i.e. past ti-1) (conditional probability)\n\ne.g. for t1, it’s (n1 - d1) / n1\n\nn1 is the number of units that have survived at t1\nd1 is the number of units that have experienced the event (e.g. died) at t1\nSimilar for other t values\n\nMedian survival time is where the survival probability equals 0.5\n\nSurvival function[](./_resources/Regression,_Survival.resources/1-bp-FZyIs1WlqYgQMWn9Obw.gif]]\n\nThe survival function computes the products of these probabilities resulting in the K-M survival curve\nThe product of these conditional probabilities reflects the fact that to survive past event time, t, a unit must have survived all previous event times and the current event time.\n\nExample: 50 patients\n\n\nDotted lines represent 95% CI\nRed dots indicate time when patients died (aka event times)\nMedian survival time is ~ 13yrs",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-exp",
    "href": "qmd/regression-survival.html#sec-reg-surv-exp",
    "title": "43  Survival",
    "section": "43.4 Exponential",
    "text": "43.4 Exponential\n\nAssumes that the hazard rate is constant\n\ni.e. risk of the event of interest occurring remains the same throughout the period of observation\n\nSurvival function\n\nHazard function\n\n\nh(t) is the constand hazard rate\n\nEstimated parameter: λ",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-weibull",
    "href": "qmd/regression-survival.html#sec-reg-surv-weibull",
    "title": "43  Survival",
    "section": "43.5 Weibull",
    "text": "43.5 Weibull\n\nAssumes the change in hazard rate is linear.\nSurvival function\n\nHazard function\n\nEstimated parameters: λ and ρ\n\nλ parameter indicates how long it takes for 63.2% of the subjects to experience the event.\nρ parameter indicates whether the hazard rate is increasing, decreasing, or constant.\n\nIf ρ is greater than 1, the hazard rate is constantly increasing.\nIf ρ is less than 1, the hazard rate is constantly decreasing.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-coxph",
    "href": "qmd/regression-survival.html#sec-reg-surv-coxph",
    "title": "43  Survival",
    "section": "43.6 Cox’s Proportional Hazards",
    "text": "43.6 Cox’s Proportional Hazards\n\nMultivariable regression model\nAllows the hazard rate to fluctuate\nHarrell: “under PH [assumption] and absence of covariate interactions, HR is a good overall effect estimate for binary [treatment]”\nMisc\n\nPackages\n\n{glmnet} - Regularized Cox Regression\n{coxphf} - Cox Regression with Firth’s Penalized Likelihood\n\nSee Regression, Regularized &gt;&gt; Firth’s Estimator\n\n\nSample Size\n\nHarrell RMS (Ch. 20.3):\n\nTo achieve a Multiplicative Margin of Error (MMOE ) of 1.2 (?) in estimating eβ^ with equal numbers of events in the two groups (balanced, binary treatment variable) and α = 0.05 → requires a total of 462 events\n\n\n\nAssumes\n\nHazard ratios (ratio of hazard rates or exp(β) between groups/units remain constant\n\ni.e. no matter how the hazard rates of the subjects change during the period of observation, the hazard rate of one group relative to the other will always stay the same\n\nHazard Ratios are independent of time\nExample: Immunotherapy typically violates PH assumptions (post)\n\nThe survival probabilities between the treatment (blue) and the chemo (red) cross at around the 4.2 months\n\nThe distance between the lines should remain somewhat constant throughout the trial in order to adhere to the PH assumptions (1st assumption)\nAlso think the lines should be somewhat straight. (2nd assumption)\n\nPatients in immunotherapy drug trials often experience a period of toxicity, but if they survive this period, they have a much better outcome down the road.\n\nTests\n\nGrambsch and Therneau (G&T)\nSee Harrell RMS (Ch. 20.6.2)\n\nIf assumptions are violated,\n\nGelman says to try and “expand the model, at the very least by adding an interaction.” (post)\nSee Harrell RMS (Ch. 20.7)\nUse a different model\n\nAccelerated Failure Time (AFT) model (See ML &gt;&gt; Gradient Boosting Survival Trees)\nAdjusted Cox PH model with Time-Varying Coefficients\n\nMust choose a functional form describing how the effect of the treatment changes over time\n\nRecommended to use AIC criteria to guide one’s choice among a large number of candidates\n\n\n\n\n\nModels event time (T) outcome variable and outputs parameter estimates for treatment (X) effects\n\nProvides a way to have time-dependent (i.e. repeated measures) explanatory variables (e.g. age, health status, biomarkers)\nCan handle other types of censoring such as left or interval censoring.\nHas extensions such as lasso to handle high dimensional data\nDL and ML models also have versions of this method\n\nHazard function\n\nh(t |X) = h0(t)exβ\nThe hazard rate for a unit with predictors, X, is the product of a baseline hazard, h0(t) (corresponding to X = 0) and a factor that depends on X and the regression parameters, β\nOptimized to yield partial maximum likelihood estimates, β^.\n\nDoesn’t require the specification of h0(t) which makes the method flexible and robust\n\n\nHazard Ratio (aka relative risk) for a binary covariate (e.g. treatment)  = eᶜᵒᵉᶠ\nInterpretation:\n\nHazard Rate (risk of the event): Moving from the reference category to the other category changes the hazard rate by a factor of eᶜᵒᵉᶠ\n\ne.g. eᶜᵒᵉᶠ = 0.68 means the change in category results in a (1 - 0.68) = 0.32 = 32% decrease in the hazard rate on average.\n\nRelative Risk (of an event): Risk of an event given category / risk of an event given by reference category\nExample: Treatment = Smoking\n\nRisk Score (aka hazard rate) given by smoking: (xᵢ=1): h₀(t)exp(β⋅xᵢ) = h₀(t)exp(β*1) = h₀(t)exp(β)\nRisk Score (aka base hazard rate) given by not smoking: (xᵢ=0): h₀(t)exp(β⋅xᵢ) = h₀(t)exp(β*0) = h₀(t)\nRelative risk (aka hazard ratio) = risk given by smoking / risk given by not smoking: h₀(t)exp(β) / h₀(t) = exp(β)",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-ml",
    "href": "qmd/regression-survival.html#sec-reg-surv-ml",
    "title": "43  Survival",
    "section": "43.7 ML",
    "text": "43.7 ML\n\n43.7.1 Misc\n\nSplit data so partitions have the same censoring distribution.\n\nThe censoring distribution might be obtained from a Kaplan-Meier estimator applied to the data.\n\nDynamic AUC is a recommended metric\n\n\n\n43.7.2 Random Survival Forests\n\nThe main difference from a standard RF lies in the metric used to assess the quality of a split: log-rank (see Misc) which is typically used when comparing survival curves among two or more groups.\nPackages\n\n{{sklearn}}\n{aorsf} - Optimized software to fit, interpret, and make predictions with oblique random survival forests (ORSFs)\n\nInstead of using one variable to split the data, use a weighted combination of variables, i.e. \\(\\text{instead of}\\;\\; x_1 &lt; \\text{cutpoint (left), use}\\;\\; c_1x_1 + c_2x_2 &lt; \\text{cutpoint (right)}\\)\n\nPredictions of Standard RF vs Oblique RF\n\n\n\n\n\n\n\n\nStandard Random Forest\n\n\n\n\n\n\n\nOblique Random Forest\n\n\n\n\n\n\n\nIn the standard rf, the decision boundaries are essentially perpendicular while the oblique rf boundaries are more angular. This should make the oblique model more flexible.\n\nKaplan-Meir Curves are fit in the leaves of the trees\n\n\nTime is on the x-axis and probability of survival on the y-axis\n\n\n\nExample: {aorsf}\n\nFrom Machine Learning for Risk Prediction using Oblique Random Survival Forests (Video; Slides & Code)\nVia package\n# equivalent syntaxes\nfit_orsf &lt;- orsf(data = pbc_orsf, \n                 formula = Surv(time + status) ~ . - id)\nfit_orsf &lt;- orsf(data = pbc_orsf, \n                 formula = time + status ~ . - id)\n\nTop model is fit with the typical survival::coxph syntax\ntime: time to event\nstatus: dummy variable indicating whether event occurred\nid: unit or patient id which is excluded\n\nVia {tidymodels}\nlibrary(parsnip)\nlibrary(censored) # must be version 0.2.0 or higher\nrf_spec &lt;- \n  rand_forest(trees = 200) %&gt;%\n  set_engine(\"aorsf\") %&gt;% \n  set_mode(\"censored regression\") \nfit_tidy &lt;- \n  rf_spec %&gt;% \n  parsnip::fit(data = pbc_orsf, \n               formula = Surv(time, status) ~ . - id)\nEstimated Expected Risk via Partial Dependence (PD)\n\nPD and importance rank for variables\norsf_summarize_uni(fit_orsf, n_variables = 1)\n## \n## -- bili (VI Rank: 1) ----------------------------\n## \n##         |---------------- risk ----------------|\n##   Value      Mean    Median     25th %    75th %\n##  &lt;char&gt;     &lt;num&gt;     &lt;num&gt;      &lt;num&gt;     &lt;num&gt;\n##    0.80 0.2343668 0.1116206 0.04509389 0.3729834\n##     1.4 0.2547884 0.1363122 0.05985486 0.4103148\n##     3.5 0.3698634 0.2862611 0.16196924 0.5533383\n## \n##  Predicted risk at time t = 1788 for top 1 predictors\n\nComputes expected risk (predicted probability) at different quantiles as a predictor variable varies.\n\nValue is 3 values of the predictor which are the 25th, 50th, and 75th quantile.\n\nn_variables says how many “important” variables to look at\n\ne.g. n_variables = 2 would look at the top 2 variables in terms of variable importance.\nVI Rank: 1 indicates the bili is ranked first in variable importance\n\nbili: serum bilirubin (mg/dl); continuous predictor variable\nIt choses time = 1788 because that’s the median\nAlso see Diagnostics, Model Agnostic &gt;&gt; DALEX &gt;&gt; Dataset Level &gt;&gt; Partial Dependence Profiles\n\nPD at specified predictor values and time values\npd_by_gender &lt;- orsf_pd_oob(fit_orsf, \n                  pred_spec = list(sex = c(\"m\", \"f\")),\n                  pred_horizon = 365 * 1:5)\npd_by_gender %&gt;% \n  dplyr::select(pred_horizon, sex, mean) %&gt;% \n  tidyr::pivot_wider(names_from = sex, values_from = mean) %&gt;% \n  dplyr::mutate(ratio = m / f)\n\n## # A tibble: 5 x 4\n##   pred_horizon      m      f ratio\n##          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1          365 0.0768 0.0728  1.06\n## 2          730 0.125  0.111   1.13\n## 3         1095 0.230  0.195   1.18\n## 4         1460 0.298  0.251   1.19\n## 5         1825 0.355  0.296   1.20\n\norsf_pd_oob - Computes expected risk using out-of-bag only\n\nBoth values (m,f) for sex are specified\npred_horizon specifies the time values\n\nHere, time is in days, so these values specify expected risk (predicted probabilities) at years 1 through 5.\n\n\nratio is the risk ratio of males compared to females.\nOthers\n\norsf_pd_inb - Computes expected risk using all training data\norsf_pd_new - Computes expected risk using new data\n\n\n\n\n\n\n\n43.7.3 Gradient Boosting Survival Trees\n\nLoss Functions\n\nPartial likelihood loss of Cox’s proportional hazards model\nSquared regression loss\nInverse probability of censoring weighted least squares error.\n\nAllows the model to accelerate or decelerate the time to an event by a constant factor. It is known as the Accelerated Failure Time (AFT). It contrasts with the Cox proportional hazards model where only the features influence the hazard function.\n\n\nPackages\n\n{{sklearn}}\n\n\n\n\n43.7.4 Survival Support Vector Machine\n\nPredictions cannot be easily related to the standard quantities of survival analysis, that is, the survival function and the cumulative hazard function.\nPackages\n\n{{sklearn}}",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-diag",
    "href": "qmd/regression-survival.html#sec-reg-surv-diag",
    "title": "43  Survival",
    "section": "43.8 Diagnostics",
    "text": "43.8 Diagnostics\n\nMisc\n\nNotes from How to Evaluate Survival Analysis Models\nPackages\n\n{survex} - Explainable Machine Learning in Survival Analysis\n\nFrom Dalex group\n\n\n\nConcordance Index (C-Index, Harrell’s C)\n\nConsider a pair of patients (i, j). Intuitively, a higher risk should result in a shorter time to the adverse event. Therefore, if a model predicts a higher risk score for the first patient (ηᵢ &gt; ηⱼ), we also expect a shorter survival time in comparison with the other patient (Tᵢ &lt; Tⱼ).\nEach pair (i, j) that fulfills this expectation (ηᵢ &gt; ηⱼ : Tᵢ &lt; Tj or ηᵢ &lt; ηⱼ : Tᵢ &gt; Tⱼ) as concordant pair, and discordant otherwise.\n\nA high number of concordant pairs is an evidence of the quality of the model, as the predicted higher risks correspond to an effectively shorter survival time compared to lower risks\n\nFormula\n\n\nIf both patients i and j are censored, we have no information on Tᵢ and Tⱼ, hence the pair is discarded.\nIf only one patient is censored, we keep the pair only if the other patient experienced the event prior to the censoring time. Otherwise, we have no information on which patient might have experienced the event first, and the pair is discarded\n\nProgrammatic Formula\n\n\nWhere the variable Δⱼ indicates whether Tⱼ has been fully observed (Δⱼ = 1) or not (Δⱼ = 0). Therefore, the multiplication by Δⱼ allows to discard noncomparable pairs because the smaller survival time is censored (Δⱼ = 0).\n\nGuidelines\n\nC = 1: perfect concordance between risks and event times.\nC = 0: perfect anti-concordance between risks and event times.\nC = 0.5: random assignment. The model predicts the relationship between risk and survival time as well as a coin toss.\nDesirable values range between 0.5 and 1.\n\nThe closer to 1, the more the model differentiates between early events (higher risk) and later occurrences (lower risk).\n\n\nIssues\n\nThe C-index maintains an implicit dependency on time.\nThe C-index becomes more biased (upwards) the more the amount of censoring (see Uno’s C below)\n\n\nUno’s C\n\n** Preferable to Harrell’s C in the presence of a higher amount of censoring. **\nVariation of Harrell’s C that includes the inverse probability of censoring weighting\n\nWeights based on the estimated censoring cumulative distribution\nUses the Kaplan-Meier estimator for the censoring distribution\n\nSo the disribution of censored units should be independent of the covariate variables\n\nIn the paper, Uno showed through simulation this measure is still pretty robust even when the censoring is dependent on the covariates\n\n\n\n\nDynamic AUC\n\nAUC where the False Positive Rates (FPR) and True Positive Rates (TPR) are time-dependent\n\nSince a unit is a True Negative until the event then becomes a True Positive\nRecommended for tasks when you want to measure performance over a specific period of time (e.g. predicting churn in the first year of subscription).\n\nFormula\n\n\nf^s are predicted risk scores\nώ is the inverse probability of censoring weight (see Uno’s C)\n\ndisregard the freaking dash above the omega, microsoft deleted the regular one for some reason\n\nI(yi,j &gt; t) indicates whether the unit pair’s, i and j, event time is greater or less the time, t. (I think)\n\n\n\n\n\n\nStandard Random Forest\nOblique Random Forest",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  }
]