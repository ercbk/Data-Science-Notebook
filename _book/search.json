[
  {
    "objectID": "qmd/causal-inference.html",
    "href": "qmd/causal-inference.html",
    "title": "Causal Inference",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-misc",
    "href": "qmd/causal-inference.html#sec-causinf-misc",
    "title": "Causal Inference",
    "section": "",
    "text": "Notes from\n\nhttps://fabiandablander.com/r/Causal-Inference.html\n\nStatistical models measure associations (e.g. linear, non-linear) which is mutual information among the variables\n\ne.g. wind and leaves moving in a tree (doesn’t answer whether the leaves moving creates the wind or the wind creates leaving moving)\n\nCausal inference predicts the conseqences after an intervention (i.e. action)\n\nYou must know the direction of causation in order to predict the conseqences of an intervention (unlike measuring associations)\nAnswers the question, “What happens if I do this?”\n\nCausal inference is able to reconstruct unobserved counterfactual outcomes.\n\nAnswers the question, “What happens if I had done something else?”\n\nCausal assumptions are necessary in order to make causal inferences\n\nmultiple regression does not distinguish causes from confounds\np-values are not causal statements\n\nDesigned to control type I error rate\n\nAIC, etc are purely predictive\n\nCausal Experiment Assumptions\n\nsee tlverse workshop notes and ebook for listing of assumptions and definitions,  https://tlverse.org/acic2019-workshop/intro.html#identifiability\n\nThe tlverse Project seeks to use ML models to calculate causal effects. Uses Super Learner ensembling and Targeted Maximum Likelihood Estimation (TMLE) which they call Targeted Learning.\n\nIgnorability - By randomly assigning treatment, researchers can ensure that the potential outcomes are independent of treatment assignment, so that the average difference in outcomes between the two groups can only be attributable to treatment\n\nEngineering outcome variables using potential adjustment variables does not automatically adjust for those variables in your model\n\nNotes from There Are No Magic Outcome Variables\nExample\n\n\nP is population density\nX is the variable of interest\nGDP and P have been used to create GDP/P\nP influences X and provides a backdoor path to GDP/P, so P must be adjusted for\nEven if P doesn’t influence X, the point is that constructing GDP/P using P doens’t automatically adjust for P\n\n\nRandomized experiments remove all paths from the treatment variable, X\n\n\nAdjusting for Z, B, and C can add precision to measurement of the treatment effect since they are causal to Y, but they aren’t necessary to get an unbiased estimate of the treatment effect.\n\nTable 2 fallacy (Notes from McElreath video, 2022 SR Lecture 6)\n\n\nThe 2nd table presented in a paper is usually a summary of all the effects of a regression. The fallacy is that the coefficient of each variable is treated as causal.\nExample: The effect of HIV on Stroke\n\nThe model is lm(Stroke ~ HIV + Smoke + Age)\n\nOnly the coefficient of the HIV variable should be treated as causal and none of the other adjustment variables (Smoke, Age)\n\nThe effects for Smoke and Age are only partial.\nThere are likely unobserved confounding variables, U, on the effect of Smoking on Stroke (e.g. other lifestyle variables).\n\nSmoke is confounded so it’s causal estimate is biased\nAge is also confounded since Smoke is now a collider and has been conditioned upon. This opens the non-causal path, Age-Smoke-U-Stroke.\n\nAge-Smoke is frontdoor, but the backdoor path, Smoke-U, also becomes a backdoor path for Age once Smoke is conditioned upon. (aka sub-backdoor path)\nSo any open path that contains a backdoor path must also be closed\n\n\n\nSolutions\n\nDon’t include effect estimates of adjustment variables\nExplicitly interpret each effect estimate according to the causal model\n\nSee 2022 SR at the end of Lecture 6 where McElreath breaks down the interpretation of each adjustment variable estimated effect.\n\n\n\nPartial Identification (Handling Unobserved Confounds)\n\nMisc\n\nAlso see\n\nPaper: Hidden yet quantifiable: A lower bound for confounding strength using randomized trials (code)\n\nUsing RCT results and Observational data, this paper proposes a statistical test and a method for determining the lower bound confounder strength.\nIn the context of pharmacuticals, RCT results are evidently often released after FDA approval, but this method can be used in any field where there’s a combination of RCT and observational studies..\n\n\n\nSometimes the confounding paths of a DAG model can be not be resolved.\n\nFor confounders that influence the treatment and outcome, see:\n\nStructural Causal Models &gt;&gt; Bayesian examples\nIf there’s a mediator, see Other Articles &gt;&gt; Frontdoor Adjustment\n\nMeasure proxies for the unobserved confound if it’s not practical/ethical to measure\n\ni.e. If the confound is ability, then test scores, letters of recommendation, etc. could be proxies.\n\nExample: 2022 SR Lecture 10 video, code\n\n\nA: Admitted to Grad School, G: Gender, D: Dept, u: Ability, T1,2,3: 3 Test Scores\n\nAbility is latent variable/unobserved confounder\nTest Scores are proxies for Ability\n\nBoth models are fit simultaneously\nCouldn’t find a way to use {brms} to code this and Kurz didn’t included it in his brms SR book.\n\n\n\nA biased estimate is better than no estimate. It can provide an upper bound\nFind a natural experiment or design one\nSensitivity Analysis\n\nAfter the analyis, you should be able to make the statement, “In order for the confound to be responsible for the entire causal effect, it was have to be .”\n\n\nPackages\n\n{tipr} - tools for tipping point sensitivity analyses\n\nSteps for using sensitivity analysis\n\nPerform a sensitivity analysis to determine plausibly how much of the causal effect is due to confounding paths\n\nAssume the confound exists, model it’s consequences for different strengths/kinds of influence\nExample: 2022 SR Lecture 10 video, code \n\nA: Admitted to Grad School, G: Gender, D: Dept, u: Unobserved Confounder\nBoth models are fit simultaneously\nValues for β and γ are specified and u is estimated as a parameter\nI think Gender (G) is an interaction in both models which I didn’t think was possible given there are no arrows of influence from gender to u.\n\nSince gender is a moderator it wouldn’t necessarily have to be an influence arrow, it would only need to be an arrow from G to the effect of u on D (see Moderator Analysis), so maybe this is kosher\nCould also be that I’m misunderstanding McElreath’s code he uses to specify his models with {Rethinking}.\n\nCouldn’t find a way to use {brms} to code this and Kurz didn’t included it in his brms SR book.\n\n\nUse previous studies that have effect strengths of those potential confounding variables\nCompare the strengths from the previous studies to the strength determined from the sensitivity analysis. The difference is a good guess for the strength of the causal effect of your treatment variable.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-causdes",
    "href": "qmd/causal-inference.html#sec-causinf-causdes",
    "title": "Causal Inference",
    "section": "Causal Design",
    "text": "Causal Design\n\nNotes from McElreath video\nMisc\n\nWhen trying to determine the relationship (e.g. linear, nonlinear) between variables and remove inconsequential variables, the Double Debiased ML procedure might be useful.\n\nDouble Debiased Machine Learning - basic concepts, links to papers, videos\nEconML (Microsoft) and causalml (Uber) has included the method in their libraries\n\n\nWhen trying to infer causal relationships, we should not blindly enter all variables into a regression in order to “control” for them, but think carefully about what the underlying causal DAG could look like. Otherwise, we might induce spurious associations (e.g. confounding such as collider bias).\nOverview\n\nMake a causal model (i.e. DAG)\n\nNeed background information in order to make the causal assumptions represented in the DAG\nDAGs only show whether or not a variable influences another, not how the influence occurs (e.g. DAGs can’t show interactions between variables or whether the association is non-linear)\n\nUse it to design data collection and statistical procedures\n\nSteps:\n\nDetermine two variables of interest (exposure, outcome) that you want to determine if a causal relationship exists and what effect the exposure has.\nUse domain knowledge or prior scholarship to determine the relevant variable and the likely associations between all variables in data\nCreate the DAG\n\nIdentify the direct causal path between exposure and outcome\nIdentify other explanatory variables and label their directions of influence with each other, the exposure, and the outcome variable\nConsider which variables (especially the exposure and the outcome) have unobserved variables influencing them.\n\nAnalyze the DAG\n\nIdentify colliders and use d-separation to determine conditional independencies\nIdentify additional paths (backdoor paths, sub-backdoor paths) between exposure and outcome\nUse the backdoor criterion to determine the set of variables that need to be adjusted for in order to block all backdoor paths with only the direct causal path remaining open.\nAdd additional adjustment variables that are causal to the outcome variable (but don’t confound the treatment effect) in order to add precision to the estimate of the treatment effect\n\nCreate simulated data that fits the DAG (i.e. a generative model)\nPerform statistical analysis (i.e. SCMs) on the simulated data  to make sure you can measure the causal effect.\nDesign experiment and collect the data\nRun the statistical analysis on the collected data and calculate the average causal effect (ACE) under the assumptions that your DAG and model specifications are correct.\nBased on your results, revise the DAG and SCM as necessary and repeat as necessary\n\nBad Adjustment Variables (Code and more details included in 2022 SR, Lecture 6)\n\nFor all examples, Z is the adjustment variable that’s being considered; X is the treatment and Y is the outcome\n\nIn each scenario, including Z produces a biased estimate of X, so the correct model is Y ~ X.\n\nM-bias\n\n\nZ doesn’t have a direct causal influence on the either X or Y, but when it’s conditioned upon it becomes a collider due to unobserved confounds that have a direct causal influence on X and Y.\nCommon issue in Political Science and network analysis\nExample\n\nY: Health of Person 2\nX: Health of Person 1\nZ: Friendship status\n\nPre-treatment variable (tend to be open to collider paths) since they could be friends before the exposure\n\nU: Hobbies of Person 1\nV: Hobbies of Person 2\n\n\nPost-Treatment Bias\n\n\nZ is a mediator and conditioning upon Z blocks the path from X to Y, but opens the backdoor path through the unobserved confound, U.\nCommon in medical studies  \nExample\n\nY: Lifespan\nX: Win Lottery\nZ: Happiness\nU: Contextual Confounds\n\n\nSelection Bias\n\n\nSame as collider bias\n\nThis version adds an unobserved confounder\n\nExample\n\nY: Income\nX: Education\nZ: Values\nU: Family\n\n\nCase-Control Bias\n\n\nZ is a descendent. Since Z has information about Y, conditioning on it will narrow the variation of Y and distort the measured effect of X.\nAlso see Association &gt;&gt; Single Path DAGs &gt;&gt; Descendent\nExample\n\nY: Occupation\nX: Education\nZ: Income\n\n\nPrecision Parasite\n\n\n2 versions: with and without U\n\nWithout U, conditioning on Z removes variation from X and lessens (but doesn’t bias) the precision of the estimated effect of X on Y (i.e. inflated std.error)\nWith U, the effect of X is biased and that bias is amplified when Z is included.\n\n\nPeer Bias\n\n\nClassic DAG of the Berkley Admission-Race-Department study\nAlso see Structural Causal Models &gt;&gt; Example (Bayesian Peer Bias)\nX is race, E is department, Q is unobserved (e.g. student quality), Y is Admission\nDepartment cannot be conditioned upon because it’s a collider with Q and would bias the estimate of X through a sub-backdoor path, X-E-Q-Y\nOnly the total effect of X on Y can be estimated (Y ~ X) since E cannot be conditioned upon but that’s not interesting and maybe not precise",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-terms",
    "href": "qmd/causal-inference.html#sec-causinf-terms",
    "title": "Causal Inference",
    "section": "Terms",
    "text": "Terms\n\nAverage Causal Efffect (ACE) - average population effect that’s calculated from an intervention (see Counterfactual definition for info on Individual Causal Effects)\n\nIf X is binary, then   is the average causal effect (see Simpson’s Paradox example)\n\nCalculated from a contingency table\n\nAlso, \n\nThis looks like the interpretation of the slope in a regression model.\n\n\nBackdoor Criterion - A valid causal estimate is available if it is possible to condition on variables such that all backdoor paths are closed\n\nGiven two nodes, X and Y, an adjustment set, L, fulfills the backdoor criterion if \n\nno member in L is a descendant of X and\nmembers in L block all backdoor paths (“shutting the backdoor”) between X and Y.\n\nAdjusting for L thus yields the causal effect of X→Y.\nAfter executing an intervention, the conditional distribution in the observational DAG (seeing) will correspond to the interventional distribution (doing) when blocking the spurious path. (see Simpson’s Paradox example)\n\nBackdoor Path - A non-causal path that enters a causal variable in a DAG rather than exits it.\n\ne.g. the path that connects a collider to a causal variable points from the collider to the causal variable\nSub-backdoor Path - this path begins with a frontdoor path but through conditioning on a variable, it opens a connecting backdoor path which biases the treatment effect\n\nsee Misc &gt;&gt; Table 1 Fallacy and Causal Design &gt;&gt; Bad Adjustment Variables &gt;&gt; Peer Bias\n\n\nThe causal effect is the distribution of Y when we change x, averaged over the distributions of the adjustment variables (Z)\nCausal Hierarchy (lowest to highest)\n\nAssociation\n\nassociated action: Seeing - observational; observing the value of Y when X = x\n\n , observational distribution; What values Y would likely take on if X happened to equal x.\n\n\nIntervention\n\nassociated action (do-Calculus): Doing -  experimental; observing the value of Y after setting X = x\n\n , interventional distribution; What values Y would likely take on if X would be set to x.\nUsing the do operator allows us to make inferences about the population but not individuals.\ndo(X) means to cut all of the backdoor paths into X, as if we did a manipulative experiment. The do-operator changes the graph, closing the backdoors.\nThe do-operator defines a causal relationship, because Pr(Y|do(X)) tells us the expected result of manipulating X on Y, given a causal graph.\n\nWe might say that some variable X is a cause of Y when Pr(Y|do(X)) &gt; Pr(Y|do(not-X)).\n\n(makes more sense to me with a binary outcome, Pr(Y = 1|do(X), but maybe Y as a continuous variable can be defined a subset. …I dunno)\n\n\nThe ordinary conditional probability comparison, Pr(Y|X) &gt; Pr(Y|not-X), is not the same. It does not close the backdoor.\nNote that what the do-operator gives you is not just the direct causal effect. It is the total causal effect through all forward paths.\n\nTo get a direct causal effect, you might have to close more backdoors.\n\nThe do-operator can also be used to derive causal inference strategies even when some backdoors cannot be closed.\n\n\nCounterfactual\n\nassociated action: Imagining - what would be the outcome if the alternative would’ve happened.\nIndividual Causal Effects can be calculated but it requires stronger assumptions and deeper understanding of the causal mechanisms\n\nNeed to research this part further.\nIf the underlying SCM is linear then the ICE = ACE.\n\n\n\nA collider along a path blocks that path. However, conditioning on a collider (or any of its descendants) unblocks that path\n\nWhen a collider is conditioned upon, the change in the association between the two nodes it separates is called collider bias.\n\ne.g. if Z is a collider between X and Y, conditioning upon Z will induce an association between X and Y.\n\n\nA conditioning set, \\(L\\), is the set of nodes we condition on (it can be empty).\nConfounding is the situation where a (possibly unobserved) common cause obscures the causal relationship between two or more variables.\n\nThere is more than one causal path between two nodes.\nA causal effect of X on Y is confounded if  \nCollider bias is a type of confounding. When a collider is controlled for, a second (or more) path opens, and the effect is confounded\n\nX and Y are d-separated by [L if conditioning on all members in [L blocks all paths between the nodes, X and Y.\n\nTool for checking the conditional independencies which are visualized in DAGs.\n\nA descendant is a node connected to a parent node by that parent node’s outgoing arrow.\nFrontdoor Adjustment - In a causal chain with three nodes X→Z→Y, we can estimate the effect of X on Y indirectly by combining two distinct quantities: (Useful for when unobserved confounders prevent direct causal estimation)\n\nThe estimate of the effect of X on Z, P(Z|do(X))\nThe estimate of the effect of Z on Y, P(Y|do(Z), X)\n\nFrontdoor Path - a path that exits a causal variable in a DAG rather than enters it.\n\ne.g. the path that connects a causal variable, X, to an outcome variable, Y, has an arrow that points from X to Y.\n\nMarkov Equivalence - A set of DAGs, each with the same conditional independencies\nMediation Analysis - seeks to identify and explain the mechanism or process that underlies an observed relationship between an independent variable and a dependent variable via the inclusion of a third hypothetical variable, known as a mediator variable (z-variable in the DAGs of “pipes” below)\n\nIncluding a mediator and the independent variable in a regression will result in the independent variable not being signficant and the mediator being significant.\n\nModeration Analysis - Like mediation analysis, it allows you to test for the influence of a third variable, Z, on the relationship between variables X and Y, but rather than testing a causal link between these other variables, moderation tests for when or under what conditions an effect occurs.\nA node is a parent of another node if it has an outgoing arrow to that node\nA path from X to Y is a sequence of nodes and edges such that the start and end nodes are X and Y, respectively.\nResidual Confounding occurs when a confounding variable is measured imperfectly or with some error and the adjustment using this imperfect measure does not completely remove the effect of the confounding variable.\n\nExample: Women who smoke during pregnancy have a decreased risk of having a Down syndrome birth.\n\nThis is puzzling, as smoking is not often thought of as a good thing to do. Should we ask women to start smoking during pregnancy?\nIt turns out that there is a relationship between age and smoking during pregnancy, with younger women being more likely to indulge in this bad habit. Younger women are also less likely to give birth to a child with Down syndrome. When you adjust the model relating smoking and Down syndrome for the important covariate of age, then the effect of smoking disappears. But when you make the adjustment using a binary variable (age&lt;35 years, age &gt;=35 years), the protective effect of smoking appears to remain.\n\n\nStructural Causal Models (SCMs) - relate causal and probabilistic statements; each equation is a causal statement\n\n\n\n“:=” is the assignment operator\nX is a direct cause of Y which it influences through the function f( )\n\nwhere f is a statistical model\n\nThe noise variables, ϵX and ϵY, are assumed to be independent.\n\nThere are Stochastic and Deterministic SCMs. Deterministic SCMs presented in article.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-assoc",
    "href": "qmd/causal-inference.html#sec-causinf-assoc",
    "title": "Causal Inference",
    "section": "Association",
    "text": "Association\n\n\n\nFar left: lm(Y ~ X); X and Y show a linear correlation when Z is NOT conditioned upon\nLeft: lm(Y ~ X + Z); X and Y show NO linear correlation when Z is conditioned upon\nRight: lm(Y ~ X); X and Y show NO linear correlation when Z is NOT conditioned upon\nFar Right:  lm(Y ~ X + Z); X and Y show a linear correlation when Z is conditioned upon",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-singpath",
    "href": "qmd/causal-inference.html#sec-causinf-singpath",
    "title": "Causal Inference",
    "section": "Single path DAGs",
    "text": "Single path DAGs\n\n\nFor each of these DAGs, Z would be the only member of the conditioning set.\nThe first 3 DAGs represent the scatter plots above\n\nZ only blocks the path between X and Y when it’s conditioned upon.\n\nX and Y are associated (e.g. linear correlation, mutual information, etc.) when Z is ignored\nConditioning on Z results in X and Y no longer being associated (i.e. conditional independence)\n\nThe first and second DAGs are elemental confounds or relations called “Pipes.”\n\nThe left one\n\nIn general, DO NOT add these variables to your model\n\nThese paths are causal so they shouldn’t be blocked\nIf your goal isn’t causal inference, then adding these variables might provide predictive information\ne.g. If there was a causal arrow from X to Y, the far left DAG would NOT have a backdoor path and therefore Z would not  be conditioned upon to block the path, X-Z-Y\n\nThe path from X to Z is a frontdoor path since the arrow exits X.\n\n\nSometimes you DO condition on these variables\n\nDuring mediation analysis, you condition on these variables as part of the process to determine how much of the effect goes through Z.\nThe mediation path can have an important interpretation depending on your research question\n\ne.g. indirect descrimination\n\nSee Statistical Rethinking &gt;&gt; Chapter 11 &gt;&gt; Conclusion of Berkeley Admissions example\n\nalso Lecture 9 2022 video\n\n\n\n\n\nThe right one is a backdoor path and should be conditioned on.\nEverything you can learn about Y from X (or vice versa) happens through Z, therefore learning about X separately provides no additional information\nZ is traditionally labelled a mediator\n\nThe third DAG is an elemental confound  or relation called a “Fork.”\n\nIn general, add these variables to your model\nThese are backdoor paths and are NOT causal\nX and Y have a common cause in Z and some of the mutual information about Z they each contain, overlaps, and creates an association (when Z isn’t conditioned upon).\n\n\nThe fourth DAG is an elemental confound or relation called a “Collider.”\n\n\nIn general, do NOT add these variables to your model\nZ blocks the path between X and Y unless conditioned upon.\nAn association between X and Y is induced  by conditioning on Z, lm(Y ~ X + Z)\n\nX and Y are independent causes of Z. Z contains information about both X and Y, but X doesn’t contain any information about Y and vice versa.\nA small X and a sufficiently large Y (and vice versa) can produce a Z = 1. So X and Y have compensatory relationship in causing Z.\n\ni.e. For a given value of Z, learning something about X tells us what Y might have been.\n\n\n\nThe last elemental confound or relation is called a “Descendent.”\n\n\nConditioning on a descendent variable, D, is like conditioning on the variable, Z itself, but weaker. A descendent is a variable influenced by another variable.\nControlling for D will also control, to a lesser extent, for Z. The reason is that D has some information about Z. This will (partially) open the path from X to Y, because Z is a collider. The same holds for non-colliders. If you condition on a descendent of Z in the pipe, it’ll still be like (weakly) closing the pipe.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-dualpath",
    "href": "qmd/causal-inference.html#sec-causinf-dualpath",
    "title": "Causal Inference",
    "section": "Dual path DAGs",
    "text": "Dual path DAGs\n\n\nCausal paths do not flow against arrows but associations can.\nTwo examples of DAGs representing confounding\n\nThese are the 2 middle DAGs above with an additional path from X to Y\nIf Z is NOT conditioned on (i.e. top path is not blocked), then the causal effect of X on Y would be confounded.\n\n\n\n\nThe paths from X to Y:\n\nThe path through Z matches the first DAG.\n\nTherefore X and Y are conditionally independent given Z.\n\nThe path through W matches the fourth DAG\n\nTherefore X and Y are conditionally dependent given W.\n\n\nThe path through W (collider) is blocked unless W is conditioned upon\nThe path through Z is open unless Z is conditioned upon\nIf Z and W are conditioned upon, then the path between X and Y is open through W and an association is present.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-interv",
    "href": "qmd/causal-inference.html#sec-causinf-interv",
    "title": "Causal Inference",
    "section": "Intervention",
    "text": "Intervention\n\n\nSince actual interventions are usually unfeasible, we want to be able to determine causality with observational data. This requires two assumptions:\n\nThe intervention occurs locally. Which means that only the variable we target is the one that receives the intervention.\nThe mechanism by which variables interact do not change through interventions; that is, the mechanism by which a cause brings about its effects does not change whether this occurs naturally or by intervention\n\nThe Doing row of DAGs (aka manipulated DAGs) represents setting X = x\n\nFor DAGs 1 and 4, Y is still affected\n\nMoving from seeing to doing didn’t change anything\n\n\nFor DAGs 2 and 3, Y is now UNaffected\n\nUsing the assumptions and some mathematical manipulation (See article for details):\n\n\n\nThus, the interventional distribution we care about is equal to the (observational) conditional distribution of Y given X when we adjust for Z\n\n\n\n\nThe rule: After an intervention, incoming arrows are cut from the node where the intervention took place.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-confound",
    "href": "qmd/causal-inference.html#sec-causinf-confound",
    "title": "Causal Inference",
    "section": "Confounding",
    "text": "Confounding\n\n\nThe backdoor criterion tells us which variable we need to adjust for in order to for our model to yield a causal relationship between two variables (i.e. graphically, nodes)\n\nBlocks all spurious, that is, non-causal paths between X and Y.\nLeaves all directed paths from X to Y unblocked\nCreates no spurious paths\n\nExample\n\nCausal effect of Z on U is confounded by X because in addition to the legitimate causal path Z→Y→W→U, there is also an unblocked path Z←X→W→U which confounds the causal effect\n\nSince X’s arrow enters the causal variable of interest, Z, it’s arrow is a backdoor path and needs to be blocked/closed\nThere are some descendant nodes that make the confounding a little difficult to parse out, but this graph is essentially\n\n\nwhich is the same as the second example DAG for confounding in the Association section\n\n\nThe backdoor criterion would have us condition on X, which blocks the spurious path and renders the causal effect of Z on U unconfounded.\n\nThe reduced, confounding DAG above is the same as the third DAG (without the path from Z to U) in the Association section. Conditioning on Z in that example blocked the path between X and Y, so it makes sense that conditioning on X in the reduced DAG would block the Z to X to U path. And therefore, the Z←X→W→U would also be blocked in the complete DAG.\n\nNote that conditioning on W would also block this spurious path; however, it would also block the causal path, Z→Y→W→U.\n\n\nIf we breakdown the complete DAG into the modular components involving W, we can see these are the same as the first example DAG in the Association section.\nW is also collider for X and Y, but I don’t think that has any bearing when discussing the causal effect of Z on U.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-appsimp",
    "href": "qmd/causal-inference.html#sec-causinf-appsimp",
    "title": "Causal Inference",
    "section": "Application: Simpson’s Paradox Example",
    "text": "Application: Simpson’s Paradox Example\n\nSex as the adjustment variable           \n\nPatients CHOOSE whether or not to take a drug to cure some disease.\nMen choosing to take the drug recover at a higher percentage that those that didn’t\nWomen choosing to take the drug recover at a higher percentage that those that didn’t\nBut overall, those that chose to take the drug recovered at a lower percentage than those that didn’t.\nSo should a doctor prescribe the drug or not?\nSuppose we know that women are more likely to take the drug, that being a woman has an effect on recovery more generally, and that the drug has an effect on recovery. \nCreate DAGs\n\n\nS=1 as being female,\nD=1 as having chosen to take the drug\nR=1 as having recovered\nThe right DAG indicates either forcing everyone to either take the drug or not take the drug\nNotice that   therefore our calculated effect will be confounded.\n\nBackdoor criterion says the manipulated DAG (right) will correspond to the observational DAG (left) if we condition on Sex.\n\n\nUse intervention formula from Intervention section\n\n\nAverage Causal Effect = 0.832 - 0.782 = 0.050. So the drug has a positive effect on average.\n\n\nBlood Pressure as the adjustment variable \n\nBlood Pressure instead of sex is used as the adjustment. Blood Pressure is a post-treatment variable.\nRelatively same observations as before. High or Low Blood Pressure with the drug produces better results than those that chose not to take the drug. Yet overall, those that chose the drug recovered at a lower percentage.\n\nSince Blood Pressure (B) is post-treatment, it has no effect on whether the patient takes the drug or not (D).\nTaking or not taking the drug (D) has an indirect effect on recovery (R) through Blood Pressure (B) along with a direct effect.   so our calculated effect will be unconfounded.\n\nSo with BP as the adjustment variable, the drug now has a small, negative effect (harmful), 0.78 - 0.83 = -0.05\n\nThe unconfounded, average causal effect for the population is negative, therefore the doctor should NOT prescribe the drug.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-scms",
    "href": "qmd/causal-inference.html#sec-causinf-scms",
    "title": "Causal Inference",
    "section": "Structural Causal Models (SCMs)",
    "text": "Structural Causal Models (SCMs)\n\nYou add additional assumptions to your DAG to derive a causal estimator\n“Full Luxury” Bayesian approach\n\n“Full Luxury” is just a term coined by McElreath; it’s just a bayesian model but bayesian models can fully model a DAG where standard regression approachs can fail (see examples)\nNo other approach will find something that the bayesian approach doesn’t\n\nMain disadvantage is that it can be computationally intensive (same with all baysian models)\n\nProvides ways to add “causes” for missingness and measurement error\n\nExample (2 Moms)\n\nNotes from McElreath video\nHypothesis: a mother’s family size is causal to her daughter’s family size\n\nTruth: no relationship\n\nVariables:\n\nM - Mother’s family size (i.e. number of children the birth)\nD - Daughter’s family size\nB1 - Mother’s birth order; binary, first born or not\nB2 - Daughters’ birth order; binary, first born or not\nU - unobserved confounds  (shown as curved dotted line)\n\n\n\nUnobserved confounds (economic status, education, cultural background, etc.) are causal to both Mother and Daughter (curved dotted line) which makes regression, D ~ M, impossible\n\nSee Baysian Two Moms example below for results of a typical regression\nStill possible to calculate the effect of M on D with SCMs\n\n\nAssumptions: Relationships are linear (i.e. linear system)\nCausal Effects\n\n\nWe want m which is the causal effect of M on D\nAssumes causal effect of birth order is the same on mother and daughter\nAside: There is no arrow/coefficient from M to B2 because it’s not germane to the calculation of m\n\nCalculate linear effect (i.e. regression coefficient) without a regression model using a linear system of equations\n\nNote: a regression coefficent, β = cov(X,Y) / var(X)\nWe can’t calculate the covariance of M and D directly because it depends on unobserved confounders but we can calculate the covariance between B1 and D and use that to get m.\nThe covariance for each path is the product of the path coefficients and the variance of the originating causal variable.\nPath B1 → M: cov(B1, M) = b*var(B1)\nPath B1 → D: cov(B1, D) = b*m*var(B1)\n2 equations and 2 unknowns, m and b\nSolve for b in the first equation, substitute b into the second equation, and solve for m\n\nm = cov(B1, D) / cov(B1, M)\n\nStill need an uncertainty of this value (e.g. bootstrap)\n\n\nExample (Bayesian 2 Moms)\n\nSee previous example for link, hypothesis, and definition of the variables\n\nFunctions (right side)\n\nEach variable’s function’s inputs are variables that are causal influences (i.e. have arrows pointing at the particular variable\n\ne.g. M has two arrows pointing at it in the DAG: B1 and u\n\n\nCode\n\nThe assumption is that this is a lineary system, so M and D have Normal distributions for their functions with means as linear regression equations\nB1 and B2 are binary so they get bernoulli distributions\nU gets a standard normal prior\n\nAside: evidently this is a typical prior for latent variables in psychology\n\np, intercepts, sd, k get typical priors for bayesian regressions\n\nResults\n\n\nTruth: no effect\n1st 3 lm models shows how the unobserved confound biases the estimate when using a typical regression model to estimate the causal effect\n\nIncluding B2 adds precision to the biased estimate since it is causal to the outcome D while adding B1 increases the bias\n\nBayesian model isn’t fooled because U is specified as an input to the functions for M and D\n\nInterpretation: There is no reliable estimate of an effect. The most likely effect is a moderately positive one but it could also be negative.\nAdding more simulated data to this example will move the point estimate towards zero\n\n\n\nExample (Bayesian Peer Bias)\n\nAlso see Causal Design &gt;&gt; Bad Adjustment Variables &gt;&gt; Peer Bias\nHypothesis: racial discrimination in acceptance of applicatioon to Berkeley grad schools\n\nTruth: moderate negative effect, -0.8\n\nVariables:\n\nX is race, E is department, Q is an unobserved confound (latent variable: student quality), Y is binary; Admission/No Admission\nR1 and R2 are proxy variables for Q (e.g. test scores, lab work, extracurriculars, etc.)\n\nAssumptions: System is linear\nDAG and Code\n\n\nXX is the race variable with X as the coefficient in the code\n\nThis code uses his {rethinking} package so some of this syntax is unfamiliar\n\nR1 and R2 are shown in the DAG to be influenced by student quality, Q\nEvery prior is normal except for Q’s coefficient\n\nResults\n\n\nTruth: -0.8\n1st 3 glm models shows how the unobserved confound, Q, biases the estimate when using a typical logistic regression model to estimate the causal effect\nBayesian model isn’t fooled because Q is specified as an input to the function for Y\n\nInterpretation: There is a reliably negative effect (no 0 in the CI). The most likely effect is a moderately negative one.\nNot quite equal to the truth but reliably negative and the point estimate is closer than the glms\n\n\n\nExample\n\nAssumptions: Relationships between variables are linear and error terms are independent\nEquations\n\n,  \n\n\nDAG 1 (left) shows the association DAG which represents the SCM\nmanipulated DAG 1 (middle) shows intervention where z is set to a constant\n\nincoming causal arrows get cutoff the intervening variable\n\nmanipulated DAG 1 (right) shows intervention where x is set to a constant\n\nSimulation of the SCM (n = 1000) (code in article)\n\n\nZ is more predictive of Y than X\n\nSimulate interventions (code in article)\n\n\nLeft - histogram of SCM for Y without an intervention\nMiddle - Intervention on Z\n\nconfirms the DAG which shows no effect on Y and Z is not causal\n\nRight - intervention on X\n\nconfirms the DAG which shows an intervention on X produces an effect on Y and X is causal\n\nAverage Causal Effect (ACE) can be determined by subtracting the expected values of interventions where  X = x +1 and  X = x",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-ctrfact",
    "href": "qmd/causal-inference.html#sec-causinf-ctrfact",
    "title": "Causal Inference",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nExample(code in article): Test whether Grandma’s home remedy can speed recovery time for the common cold\n\nSCM\n\n\nT is 1/0, i.e. whether patient receives Grandma’s treatment, with p = 0.5; \nR is recovery time\nμ is the intercept\nβ is the average causal effect, since\n\n\nwhere \n\n\nFrom fitting the model, we find μ = 7, β = -2, Τ = 0, ε1 = 0.78\n\nTherefore, the Individual Causal Effect for patient 1\n\n\nJust plug and chug where we substitute T = 1 into the SCM and we already have the T = 0 part from the model\n\n\nIn this case, the SCM is linear, so the ICE = ACE.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-medanal",
    "href": "qmd/causal-inference.html#sec-causinf-medanal",
    "title": "Causal Inference",
    "section": "Mediation Analysis",
    "text": "Mediation Analysis\n\n\nFigure\n\nc’ is the direct effect of X on the outcome after the indirect path has been removed (i.e. conditioned upon, outcome ~ X + mediator)\nc is the to total effect (outcome ~ X)\nc - c’ equals the indirect effect\nSee definitions below\n\nAllows you to test for the influence of a third variable, the mediator, on the relationship between (i.e. effect of) the treatment variable, X, and the Outcome variable, Y.\nMisc\n\nNotes from: Mediation Models\n\nOverview of packages (Aug 2020)\n\n{brms} very flexible in terms of models. You’ll just have to calculate the effects by hand unless some outside package (e.g. sjstats) takes a brms model and does it for you.\n\nSee below for formulas. {mediation} papers should have other formulas for other types of models (e.g. poisson, binomial)\n\n{mediation} handles a lot for you. Method isn’t bayesian but is very similar to it in a frequentist-bootstrappy-simulation way.\n\nPackage has been substantially updated since that article was written.\n\n\nAlso see\n\nOther Articles &gt;&gt; Frontdoor Adjustment\nStatistical Rethinking &gt;&gt; Chapter 11 &gt;&gt; Conclusion of Berkeley Admissions example\n\nalso Lecture 9 2022 video\n\nebook (w/brms) Introduction to Mediation, Moderation, and Conditional Process Analysis\n\nIncluding a mediator and the independent variable in a regression will result in the independent variable not being signficant and the mediator being significant.\n\nExample: Causal effect of education on income\n\nSay occupation is your mediator. Education has a big impact on your occupation, which in turn has a big impact on your income. You don’t want to control for a mediator if you are interested in the full effect of X on Y! Because a huge part of how X impacts on Y is precisely through the mediation of C, in our case choice of and access to occupation, given a certain level of education. If you ‘control’ for occupation you will be greatly underestimating the importance of education.\n\n\nWhen would you want to only measure the Direct Effect?\n\nExample: Determining the amount of remuneration for discrimination\n\nFrom Simulating confounders, colliders and mediators\nVariables\n\nOutcome: Pay Gap\nTreatment: Gender\n\nIn this case, this variable is actually “gender discrimination in the current workplace in making a pay decision” (for which we use actual, observed Gender as a proxy)\n\nMediators: Occupation and Experience\n\nWhen determining whether a type of descrimination exists, you don’t want to condtion on the mediators, because the effect of gender will be underestimated. So, you’d want the total effect. But here, discrimation is already determined and Gender is now a proxy variable. Under Gender’s new definition, Occupation and Experience might influence the amount of “gender discrimiation,” so they can’t be definitively labelled mediators any more.\nSo if you want to estimate that final “equal pay for equal work” step of the chain then yes it is legitimate to control for occupation and experience.\n\n\nShould always compare a mediation model to a model without mediation\n\nAn unnecessary mediation model will almost certainly be weaker and probably more confusing than the model you would otherwise have.\n\nAverage Causal Mediation Effect (ACME) (aka Indirect Effect)- the expected difference in the potential outcome when the mediator took the value that it would have under the treatment condition as opposed to the control condition, while the treatment status itself is held constant.\n\nIf this isn’t significant, there isn’t a mediation effect\nIt is possible that the ACME takes different values depending on the baseline treatment status. Shown by analyzing the interaction between the treatment variable and the mediator\nδ(t) = E[Y (t, M(t1)) − Y (t, M(t0))]\n\nwhere\n\nt, t1, t0 are particular values of the treatment T such that t1 ≠ t0,\nM(t) is the potential mediator\nY (t, m) is the potential outcome variable\n\n\n\nAverage Direct Effect (ADE) - the expected difference in the potential outcome when the treatment is changed but the mediator is held constant at the value that it would have if the treatment equals t.\n\nζ(t) = E[Y (t1, M(t)) − Y (t0, M(t))]\n\nThe Total Effect of the treatment on the outcome is ACME + ADE.\n\nConditions where you likely do NOT need mediation analysis :\n\nIf you cannot think of your model in temporal or physical terms, such that X necessarily leads to the mediator, which then necessarily leads to the outcome.\nIf you could see the arrows going either direction.\nIf when describing your model, everyone thinks you’re talking about an interaction (a.k.a. moderation).\nIf there is NO strong correlation between key variables (variables of interest) and mediator, and if there is NO strong correlation between mediator and the outcome.\n\nSobel test - tests whether the suspected mediator’s influence on the independent variable is significant.\n\nPerforming the test in R via bda::mediation.test - article\n\nMethods\n\nBaron & Kenny’s (1986) 4-step indirect effect method has low power\nProduct-of-Paths (or difference in coefficients)\n\nc - c’ = a*b (see figure at start of this section) where c - c’ is the indirect effect (aka ACME)\n\nif either a or b are nearly zero, then the indirect effect can only be nearly zero\nFormula only appropriate for the analysis of causal mediation effects when both the mediator and outcome models are linear regressions where treatment (IV) and moderator enter the models additively (e.g. without interaction)\n\nEffect formulas for models with an interaction between treatment and moderator (Paper)\n\nmediator: M = α2 + β2Ti + ξT2Xi + εi2(T~i`)\noutcome: Y = α~3 + β3Ti + γMi + κTiMi + ξT3Xi + εi3(Ti, Mi)\nACME = β2(γ + κt) where t = 0,1\nADE = β3 + κ{α2 + β2t + ξT2Ε(Xi)}\nATE = β2γ + β3 +κ{α2 + β2 + ξT2Ε(Xi)}\n\nAlternatively, fit Y = α1 + β1Ti + ξT1Xi + ηTTiXi + εi1\n\nThen ATE = β1 + ηTE(Xi)\n\n\nNotes\n\nVariables\n\nT is treatment, M is mediator, X is a set of adjustment variables\n\nThe exponentiated T in ξT is to let you know it can be a set of coefficients for a set of adjustment variables (I guess)\n\n\nCouldn’t figure out why curly braces are being used\nACME with have two estimates (t=0, t=1)\nATE (average total effect)\nΕ(Xi) is the sample average of each adjustment variable and it’s multiplied by its associated ξ2 coefficient\nSee paper for other types of models\n\n\n{lavaan}, {brms}\n\nTingley, Yamamoto, Hirose, Keele, & Imai, 2014\n\nQuasi-bayesian approach (paper ,esp Appendix D, for details)\n\nFits the mediation and outcome models (see 1st example)\nTakes the coefficients and vcov matrices from both models\n\nUses the coefs (means) and vcovs (variances) as inputs to a mvnorm function to simulate distributions for the coefficients.\nI do not understand what these are used for… would have to look at the code.\n\nSamples predictions of each model K times for treatment = 1, then for treatment = 0\nCalcs difference between predictions for each set of samples, then averages to get the ACME\n\nAssumes Sequential Ignorability\n\nRequires treatment randomization or an equivalent assignment mechanism\nmediator is also ignorable given the observed treatment and pre-treatment confounders. This additional assumption is quite strong because it excludes the existence of (measured or unmeasured) post-treatment confounders as well as that of unmeasured pretreatment confounders. This assumption, therefore, rules out the possibility of multiple mediators that are causally related to each other (see Section 6 for the method that is designed to deal with such a scenario).\nCan’t be tested but a sensitivity analysis can be conducted using mediation::medsens (see vignette)\n\n{mediation} (vignette)\n\nMultiple types of models for both mediator and outcome\n\nincluding multilevel model functions from {lme4} supported\n\nMethods for:\n\n‘moderated’ mediation\n\nthe magnitude of the ACME depends on (or is moderated by) a pre-treatment covariate. Such a pre-treatment covariate is called a moderator. (see Moderator Analysis)\nACME can depend on treatment status (i.e. interaction between treatment and mediator), but this situation is talking about a separate variable moderating the effect of the treatment on the mediator.\n\nmultiple mediators (which violates sequential ingnorability but can be handled)\nvarious experimental designs (e.g. parallel, crossover)\ntreatment non-compliance\n\nUses MASS (so may have conflicts with dplyr)\nNo latent variable capabilities\n\n\nEtsy article calculates generalized average causal mediation effect (GACME) and generalized average direct effect (GADE) and uses a known mediator to measure the direct causal effect even when the DAG has multiple unknown mediators (paper, video, R code linked in article)\n\nExample: Tingley, 2014 Method\n\nEquations\n\n\n\nPredictions for “job_seek” in the mediator model (top) are used as predictor values in the outcome model (bottom).\n\nData: data(jobs, package = 'mediation')\n\ndepress2: outcome, numeric: Measure of depressive symptoms post-treatment. The outcome variable.\ntreat: treatment, binary: whether participant was randomly selected for the JOBS II training program.\n\n1 = assignment to participation.\n\njob_seek: mediator, ordinal: measures the level of job-search self-efficacy with values from 1 to 5.\necon_hard: adjustment, ordinal: Level of economic hardship pre-treatment with values from 1 to 5.\nsex: adjustment, binary: 1 = female\nage: adjustment, numeric: Age in years\n\n{mediation}\nmodel_mediator &lt;- lm(job_seek ~ treat + econ_hard + sex + age, data = jobs)\nmodel_outcome  &lt;- lm(depress2 ~ treat + econ_hard + sex + age + job_seek, data = jobs)\n\n# Estimation via quasi-Bayesian approximation \nmediation_result &lt;- mediate(\n  model_mediator, \n  model_outcome, \n  sims = 500,\n  treat = \"treat\",\n  mediator = \"job_seek\"\n)\n\nSummary - summary(mediation_result)\n\n\nerror bar plot also available via plot(mediation_result)\nSays ACME isn’t significant, therefore no mediation effect detected.\n“Prop Mediated” is supposed to be the ratio of the indirect effect to the total.\n\nHowever this is not a proportion, and can even be negative, and so “it is mostly a meaningless number.”\n\n\n\n\nExample: product-of-paths (or difference in coefficients)\n\n{lavaan}\nsem_model = '\n  job_seek ~ a*treat + econ_hard + sex + age\n  depress2 ~ c*treat + econ_hard + sex + age + b*job_seek\n  # direct effect\n  direct := c\n  # indirect effect\n  indirect := a*b\n  # total effect\n  total := c + (a*b)\n'\nmodel_sem = sem(sem_model, data=jobs, se='boot', bootstrap=500)\nsummary(model_sem, rsq=T)  # compare with ACME in mediation\nDefined Parameters:\n                  Estimate  Std.Err  z-value  P(&gt;|z|)\n    direct          -0.040    0.045  -0.904    0.366\n    indirect        -0.016    0.012  -1.324    0.185\n    total            -0.056    0.046  -1.224    0.221\n\nAlso outputs the typical summary regression estimates, std.errors, pvals, R2 etc.\nBootstraps std.errors\nSame results for “indirect” here as with {mediation} ACME estimate\nR2s are poor for both regression models which could be why no mediation effect is detected.\n\n{brms}\nmodel_mediator &lt;- bf(job_seek ~ treat + econ_hard + sex + age)\nmodel_outcome  &lt;- bf(depress2 ~ treat + job_seek + econ_hard + sex + age)\nmed_result = brm(\n  model_mediator + model_outcome + set_rescor(FALSE), \n  data = jobs\n)\nsummary(med_result) # regression results\n# using brms we can calculate the indirect effect as follows\nhypothesis(med_result, 'jobseek_treat*depress2_job_seek = 0')\n\nExact same brms syntax (except priors are specified) as in Statistical Rethinking &gt;&gt; Chapter 5 &gt;&gt; Counterfactual Plots\nExample has a mediator DAG as well.\nhypothesis tests H0: a*b == 0\n\npval &lt; 0.05 says there is a mediation effect.\n\n\n{sjstats}\n\nsjstats::mediation(med_result) %&gt;% kable_df()\n\nmediator (b): the effect of “job_seek” on “depress2”\nindirect (c-c’): ACME\ndirect (c’): ADE\nproportion mediated: See {mediation} example",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-modanal",
    "href": "qmd/causal-inference.html#sec-causinf-modanal",
    "title": "Causal Inference",
    "section": "Moderation Analysis",
    "text": "Moderation Analysis\n\n\nMisc\n\nAlso see Introduction to Mediation, Moderation, and Conditional Process Analysis\n\nLike mediation analysis, it allows you to test for the influence of a third variable, Z (moderator), on the relationship between variables X and Y, but rather than testing a causal link between these other variables, moderation tests for when or under what conditions an effect occurs.\n\nModerators are conceptually different from mediators (“when” (moderator) vs “how/why” (mediator)).\n\nThere can be moderated mediation effect though. (see Mediation Analysis &gt;&gt; Methods &gt;&gt; {mediation})\n\nModerators can stengthen, weaken, or reverse the nature of a relationship.\nSome variables may be a moderator or a mediator depending on your question.\n\nAssumption: assumes that there is little to no measurement error in the moderator variable and that the DV did not CAUSE the moderator.\n\nIf moderator error is likely to be high, researchers should collect multiple indicators of the construct and use SEM to estimate latent variables.\nThe safest ways to make sure your moderator is not caused by your DV are to experimentally manipulate the variable or collect the measurement of your moderator before you introduce your IV.\n\nModeration can be tested by interacting variables of interest (moderator x IV) and plotting the simple slopes of the interaction, if present.\n\nSee Regression, Interactions for simple slopes/effects analysis\nMean center both your moderator and your IV to reduce multicolinearity and make interpretation easier. (“c” in variable names indicates variable was centered)\n\nExample: academic self-efficacy (moderator)(confidence in own’s ability to do well in school) moderates the relationship between task importance (independent variable (IV)) and the amount of test anxiety (outcome) a student feels (Nie, Lau, & Liau, 2011).\n\nStudents with high self-efficacy experience less anxiety on important tests (task importance) than students with low self-efficacy while all students feel relatively low anxiety for less important tests.\nSelf-efficacy (Z) is considered a moderator in this case because it interacts with task importance (X), creating a different effect on test anxiety (Y) at different levels of task importance.\n\nExample: What is the relationship between the number of hours of sleep (X, independent variable (IV)) a graduate student receives and the attention that they pay to this tutorial (Y, outcome) and is this relationship influenced by their consumption of coffee (Z, moderator)\nmod &lt;- lm(Y ~ Xc + Zc + Xc*Zc)\nsummary(mod)\n## Coefficients:\n##            Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept) 48.54443    1.17286  41.390  &lt; 2e-16 ***\n## Xc          5.20812    0.34870  14.936  &lt; 2e-16 ***\n## Zc          1.10443    0.15537  7.108 2.08e-10 ***\n## Xc:Zc        0.23384    0.04134  5.656 1.59e-07 ***\n\nSince we have significant interactions in this model, there is no need to interpret the separate main effects of either our IV or our moderator\nPlot the simple slopes (1 SD above and 1 SD below the mean) of the moderating effect\n\n\nFor details on this plot and analysis, see Regression, Interactions &gt;&gt; OLS &gt;&gt; numeric:numeric &gt;&gt; Calculate simple slopes for the IV at 3 representative values for the moderator variable\nInterpretation\n\nThose who drank less coffee (moderator, black line) paid more attention (outcome) with the more sleep (IV) that they got last night but paid less attention overall than average (the red line).\nThose who drank more coffee (moderator, green line) paid more attention (outcome) when they slept more (IV) as well and paid more attention than average.\nThe difference in the slopes for those who drank more or less coffee (moderator) shows that coffee consumption moderates the relationship between hours of sleep and attention paid",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-sr",
    "href": "qmd/causal-inference.html#sec-causinf-sr",
    "title": "Causal Inference",
    "section": "Statistical Rethinking",
    "text": "Statistical Rethinking\n\nMisc\n\nArrows indicate directions of influence\nArrows in DAGs “create” correlations\n\ni.e. if arrow, then correlation\nThe direction it points determines whether its association is causal or not.\n\nUnlike a statistical model, a DAG, if it is correct, will tell you the consequences of intervening to change a variable.\n** The data alone can never tell us when a DAG is right. But the data can tell us when a DAG is wrong. **\nMany dynamical systems cannot be usefully represented by DAGs, because they have complex behavior that is sensitive to initial conditions. But these models can still be analyzed and causal interventions designed from them.\nA DAG path means any series of variables you could walk through to get from one variable to another, ignoring the directions of the arrows.\nThe variable, U, in DAGs represents one or more unobserved variables\n\nUsually has circle around the U or is just represented by a dashed line\n\n“Conditioned upon,” “adjusted for,” or “controlled for” is all the same thing\n“a” or “α” is used in bayesian formulas to represent the intercept\nNotation\n\nX is not independent of Y, i.e \nconditional independence: Y is not associated with some variable X, after conditioning on some other variable Z, i.e. \n\nthey are statements of which variables should be associated with one another (or not) in the data.\nthey are statements of which variables become dis-associated when we condition on some other set of variables.\nThere is no other path of influence from X to Y except through Z\n\n\n(Total ) Causal Effect and Direct Causal Effect\n\n\nWeight (W) is the outcome, Height (H) and Sex (S) are explanatory\n(Total) Causal Effect is simply, W ~ S\nDirect Causal Effect shuts the backdoor paths, W ~ S + H\n\nSometimes we want the total causal effect and not the direct causal effect. (e.g. if H is a post-treatment variable, see SR, Ch.6)\n\n\n\n\n\nTestable Implications\n\nDiffering associations between plausible DAGs that are testable through statistical models\nAny DAG may imply that some variables are independent of others under certain conditions.\nNO conditional independencies → NO testable implications\nwww.dagitty.net - Enter DAG and it will give you the Adjustment Set and Testable Implications\nExample\n\nQuestion: What is the causal relationship between Divorce Rate (D), Marriage Rate (M), and Median Age at Marriage (A)\nData:\n\n2 regressions are fit\n\nD ~ α + βM\n\nShows that M is positively correlated with D\n\nD ~ α + βA\n\nShows that A is negatively correlated with D\n\n\n\nPlausible DAGs (note: marriage cannot influence your age… technically)\n\n\n\nA directly influences D\nM directly influences D\nA directly influences M\nReasoning: First, Age can have a direct effect, perhaps because younger people change faster than older people and are therefore more likely to grow incompatible with a partner. Second, it can have an indirect effect by influencing the marriage rate. If people get married earlier, then the marriage rate may rise, because there are more young people. Consider for example if an evil dictator forced everyone to marry at age 65. Since a smaller fraction of the population lives to 65 than to 25, forcing delayed marriage will also reduce the marriage rate. If marriage rate itself has any direct effect on divorce, maybe by making marriage more or less normative, then some of that direct effect could be the indirect effect of age at marriage.\n\n\n\nSimilar to 1 except M does not directly influence D\nReasoning This DAG is plausible even though there’s a correlation between M and D (regression 1). It could be that M derives it’s correlation with D through it’s association with A.\n\nThe direction of influence doesn’t prevent a correlation between M and D\n\n\n\nTestable implications\n\nDAG 1\n\nThe DAG shows all three are associated to each other, i.e. \nIt would be natural to think about measuring correlation and if a pair shows no correlation you could discard the DAG, but it is NOT a good test since there are many ways two variables can show correlation yet not be directly associated. (see reasoning under DAG 2 above and under DAG2 below)\nDAG1 has NO conditional independencies and therefore, NO testable implications\n\nDAG 2\n\nThis DAG also shows all three variables are associated with each other.\nD and M are associated with one another, because A influences them both. They share a cause, and this leads them to be correlated with one another through that cause. But suppose we condition on A. All of the information in M that is relevant to predicting D is in A. So once we’ve conditioned on A, M tells us nothing more about D\nThe testable implication is that D is independent of M, conditional on A, i.e. \n\n(Conditioning on A does not make D independent of M, because M really influences D all by itself in this model.)\n\ni.e A and M are marginally dependent\n\n\n\nOnly difference between both DAGs is the conditional independence in DAG2.\n\nTest\n\nRun a multiple regression D ~ α + βMM + βAA\nIf the effect measured from regression 1 disappears in the multiple regression, then we can discard DAG 1. If the effect remains, then we discard DAG 2.\n\n\nDAGs that are consistent with the data associations (M & N are associated but the causal relationship isn’t known)\n\nwhere U is an unknown variable. Unobserved variables are circled.\n\nAll three DAGs have no conditional independencies and therefore not testable implications\n\nA set of DAGs, each with the same conditional independencies known as a Markov Equivalence\n\nData cannot eliminate any of these DAGS. Domain knowledge must be used to reduce the number of Markov Equivalent DAGs.\n\n\n\n\n“Shutting the backdoor” to potential confounding paths\n\nSection 6.4\nwww.dagitty.net - Enter DAG and it will give you the Adjustment Set and Testable Implications\nRecipe\n\nList all of the paths connecting X (the potential cause of interest) and Y (the outcome).\nClassify each path by whether it is open or closed. A path is open unless it contains a collider.\nClassify each path by whether it is a backdoor path. A backdoor path has an arrow entering X.\nIf there are any backdoor paths that are also open, decide which variable(s) to condition on to close it.\n\nIf you have a choice between two variables where conditioning on either will close a backdoor path and one of them is causal to the outcome variable, then condition on the variable that is causal to the outcome variable. It will add precision to the estimate of the treatment effect.\nAny frontdoor paths that lead to backdoor paths must also be closed (see Misc &gt;&gt; Table 2 fallacy)\n\n\nExamples:\n\n\n\nProblem: We want to measure the causal effect of X –&gt; Y\nPotential confounding paths: XUAC, XUBC\n\nXUAC doesn’t have a collider so a variable needs conditioned on (aka adjusted for)\n\nU is unobserved, so either A or C. C directly influences Y, so it’s more efficient and will “aid in precision.”\n\nXUBC has a collider, B. So, no need to condition on any variable\n\nSolution: Y ~ a + X + C\n\nlibrary(dagitty)\ndag_6.1 &lt;- dagitty( \"dag { \n    U [unobserved]\n    X -&gt; Y\n    X &lt;- U &lt;- A -&gt; C -&gt; Y\n    U -&gt; B &lt;- C\n}\")\nadjustmentSets( dag_6.1 , exposure=\"X\" , outcome=\"Y\" )\n#&gt; { C }\n#&gt; { A }\n\n\nProblem: We want to measure the causal effect of the number of Waffle Houses, W, on Divorce, D.\nPotential confounding paths: WSM, WSA, WSMA (Also WSAM but McElreath on says there are 3. Maybe a combo of same letters is equivalent?)\n\nWSM doesn’t have a collider and therefore either S or M needs conditioned on\nWSA doesn’t have a collider and therefor either S or A needs conditioned on\nWSMA has a collider, M. So that path is blocked\nM is a choice for WSM but it’s a collider so it’s out. S is in both WSM and WSA, so conditioning on it kills two birds.\n\nSolution: D ~ a + W + S\n\nlibrary(dagitty)\ndag_6.2 &lt;- dagitty( \"dag {\n    A -&gt; D\n    A -&gt; M -&gt; D\n    A &lt;- S -&gt; M\n    S -&gt; W -&gt; D\n}\")\nadjustmentSets( dag_6.2 , exposure=\"W\" , outcome=\"D\" )\n#&gt; { A, M }\n#&gt; { S }\n\nEvidently conditioning on A and M is also a solution\n\nConditioning on M does close WSM but would then open WSMA. So, by then conditioning on A which is on a fork (or pipe depending on the path) it closes WSMA.\n\nIn his brms ebook, Kurz fits these regressions and a couple others for comparison. There wasn’t a consensus point estimate for W in the regressions that adjust for S and A + M.\n\nMcElreath mentions, “This DAG is obviously not satisfactory–it assumes there are no unobserved confounds, which is very unlikely for this sort of data.”\nThe inconsistent point estimates are probably do to an omitted variable(s) that is confounding the regression.\n\nConditional independencies:\nimpliedConditionalIndependencies( dag_6.2 )\n#&gt; A _||_ W | S\n#&gt; D _||_ S | A, M, W\n#&gt; M _||_ W | S",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-othart",
    "href": "qmd/causal-inference.html#sec-causinf-othart",
    "title": "Causal Inference",
    "section": "Other Articles",
    "text": "Other Articles\n\nFrontdoor Adjustment\n\nFrom http://arelbundock.com/posts/frontdoor/\nUseful when an unobserved confounder creates a backdoor path that prevents direct causal estimation\nIn a causal chain with three nodes X→Z→Y, we can estimate the effect of X on Y indirectly by combining two distinct quantities:\n\nThe estimate of the effect of X on Z, P(Z|do(X))\nThe estimate of the effect of Z on Y, P(Y|do(Z), X)\n\nAssumptions\n\nFull mediation: there is no direct path from X to Y, except through Z.\nUn-confoundedness 1: There is no open backdoor from X to Z.\nUn-confoundedness 2: All backdoors from Z to Y are blocked by X\n\nExample: 1\n\nOur goal is to estimate P(Y|do(X)). Unfortunately, this relationship between X and Y is confounded by the unobserved variable U, via this backdoor path: X←U→Y. Therefore, we cannot estimate the causal quantity of interest directly.\n\n\ncause X, a mediator Z, an outcome Y, and an unobserved confounder U\n\nlibrary(data.table)\nset.seed(731460) \nN = 1e5\nU = rbinom(N, 1, prob = .2)\nX = rbinom(N, 1, prob = .1 + U * .6)\nZ = rbinom(N, 1, prob = .3 + X * .5)\nY = rbinom(N, 1, prob = .1 + U * .3 + Z * .5)\ndat = data.table(X, Z, Y)\n\n# truth\ncoef(lm(Y ~ X + U))[\"X\"]\n## 0.2549541\nEstimate the effect of X on Z, P(Z|do(X))\nstep1 = lm(Z ~ X, dat)\nEstimate the effect of Z on Y, P(Y|do(Z), X)\nstep2 = lm(Y ~ Z + X, dat)\nCombine both estimates by multiplication\ncoef(step1)[\"X\"] * coef(step2)[\"Z\"]\n## 0.2496002\n\nExample 2\n\nSame as first example but using {dosearch} package\nlibrary('dosearch')\n   data1 &lt;- \"P(X, Y, Z)\"\nquery1 &lt;- \"P(Y | do(X))\"\ngraph1 &lt;- \"U -&gt; X\n          U -&gt; Y\n          X -&gt; Z\n          Z -&gt; Y \"\n   # compute\n   frontdoor &lt;- dosearch(data1, query1, graph1)\n   frontdoor\n\nOutput:\n\nEstimate the causal effect\ndat[, `P(X)`    := fifelse(X == 1, mean(X), 1 - mean(X)) ][\n    , `P(Z|X)`  := mean(Z), by = X                      ][\n    , `P(Y|Z,X)` := mean(Y), by = .(Z, X)                ][\n    , `P(Z|X)`  := mean(Z), by = X                      ][\n    , Y := NULL                                          ]\ndat = unique(dat)\ndat[, `P(Y|do(Z))` := sum(`P(Y|Z,X)` * `P(X)`), by = Z]\n`P(Y|do(X=0))` = with(dat[X == 0], \n  `P(Z|X)`          [Z == 1] * \n  `P(Y|do(Z))`      [Z == 1] +\n  (1 - `P(Z|X)`)    [Z == 0] * \n  `P(Y|do(Z))`      [Z == 0]\n)\n`P(Y|do(X=1))` = with(dat[X == 1], {\n  `P(Z|X)`          [Z == 1] * \n  `P(Y|do(Z))`      [Z == 1] +\n  (1 - `P(Z|X)`)    [Z == 0] * \n  `P(Y|do(Z))`      [Z == 0]\n})\n`P(Y|do(X=1))` - `P(Y|do(X=0))`\n## 0.249766\nComparison\n\nTruth: 0.2549541\nlm: 0.2496002\ndosearch: 0.249766",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/business-plots.html",
    "href": "qmd/business-plots.html",
    "title": "Business Plots",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Business Plots"
    ]
  },
  {
    "objectID": "qmd/business-plots.html#sec-bizplots-misc",
    "href": "qmd/business-plots.html#sec-bizplots-misc",
    "title": "Business Plots",
    "section": "",
    "text": "{modelplotr}\n\nGithub, Vignette\nNice implementations but package is not maintained\nNotes for marketing and financial graphs taken from articles and vignettes introducing that package.",
    "crumbs": [
      "Business Plots"
    ]
  },
  {
    "objectID": "qmd/business-plots.html#sec-bizplots-market",
    "href": "qmd/business-plots.html#sec-bizplots-market",
    "title": "Business Plots",
    "section": "Marketing Plots",
    "text": "Marketing Plots\n\nTL;DR - Most useful/popular are the Cumulative Gains and Cumulative Response graphs.\nThe example objective is to select the customers of a bank that are most likely to respond to an offer to purchase a “term deposit”. The outcome is binary: “term deposit” or “no”\nInformation from models used in these plots\n\nPredicted probability for the target class\nX-Axis: Equally sized groups based on this predicted probability\n\ne.g. Splitting observations into deciles. Top 10% in predicted probability for target class would be in the first decile.\n\nNumber of observed target class observations in these groups\n\nThe test dataset is used for the plots to get a realistic idea of what a marketing campaign in the would would produce.\n\nResponse Plot has some GOF capability so I could maybe see using the validation set with that plot to compare models with.\n\n\n\nCumulative Gains\n\n\nAKA Gains Plot\nAnswers the question: “When we apply the model and select the best X quantiles, what % of the actual target class observations can we expect to target?”\n\ny-axis = % of positive events (1s in binary classification) out of the entire dataset\n\nHow to apply:\n\nChoose a probability threshold (i.e. the corresponding quantile on the x-axis). The graph shows the percentage of observations on the y-axis that are within that threshold\nChoose the percentage of customers that you can afford to target with your campaign. The corresponding quantile on the x-axis shows the quantile and therefore the associated probability of positive result.\n\n“When we select 20% with the highest probability according to gradient boosted trees, this selection holds 87% of all term deposit cases in test data.”\n\nSays using the top 20% will include 87% of all the 1s (in binary classification) in the entire dataset.\n\n\nIf the gains is 87%, then there are potentially 13% of the total 1s that won’t be included in the campaign if we only target the top 20% percent.\n\nwizard model (perfect model) line - line takes steepest route to 100% on y-axis as possible, depending on the percentage of your outcome variable is the target level.\n\nFor the graph above, it looks like around 12% of the outcome variable values are the positive event case since the line reaches the 100% on the y-axis a little past the 1st decile. So the perfect model predicts all those values as being the positive class.\n\n\n\n\nCumulative Lift\n\n\nAKA Index or Lift Plot\nEspecially useful for companies with little to no experience with data models\nAnswers the question: “When we apply the model and select the best X quantiles, how many times better is that than using no model at all?”\n“no model at all” (i.e. coin flip) is a random model (also seen in the gains plot) is represented by a horizontal line at y = 1 or 100% depending on how the y-axis is specified. It is the ratio of the % of actual target category observations in each quantile to the overall % of actual target category observations after randomization of the rows of the data set.\nThe amount of lift can’t be generalized to all models and all data sets. So there aren’t guidelines as to what is a “good” lift score and what isn’t. If 50% of your data belongs to the target (positive) class of interest, a perfect model would ‘only’ do twice as good (lift: 2) as a random selection. If 10% of the data belong to the positive class, then lift = 10 or 1000% is the best possible lift score.\nHow to apply:\n\nChoose a quantile (x-axis) and the corresponding y value can be used to explain to stakeholders how many times or what percent better this model is at selecting the top prospects than random selection.\n\n“A term deposit campaign targeted at a selection of 20% of all customers based on our gradient boosted trees model can be expected to have a 4 times higher response (434%) compared to a random sample of customers.”\n\n\n\n\n\nResponse Plot\n\n\nPlots the percentage of *target class* observations per quantile\n\nnote: the cumulative gains y-axis is total observations where this plot’s y-axis is just positive class (1s in a binary classification model)\n\nAnswers the question: “When we apply the model and select quantile X, what is the expected % of target class observations in that quantile?” but also gives information about the model fit.\nHow to apply:\n\nThis plot is more important in what it tells about the model fit than what it says about how many observations are in a particular quantile\n\nA good fitting model will have a sharp sloping line with the highest response % in the lower quantiles. This says that the model is giving high probability scores to the vast majority of the positive class observations\nFor model comparison: the earlier the line crosses the horizontal (random model) line should indicate a steeper slope and therefore a better fit.\n\n“When we select decile 1 (10th percentile) according to model gradient boosted trees in dataset test data the % of term deposit cases in the selection is 51%.”\nThe horizontal line represents a random model (i.e. the % of target class cases in the total set)\n\nFrom the quantile where the line intersects the horizontal dashed-line and onwards, the % of target class cases is lower than a random selection of cases would hold.\n\n\n\n\n\nCumulative Response\n\n\nAnswers the question: “When we apply the model and select up until quantile X, what is the expected % of target class observations in the selection?\n\nOften used to decide - together with business colleagues - up until what decile to select for a marketing campaign\n\nHow to apply:\n\n“When we select quantiles 1 until 30 according to model gradient boosted trees in dataset test data, the % of term deposit cases in the selection is 36%.”\n\nIn other words, targeting these customers should produce a response rate (percent of customers purchasing a term deposit) of 35% on average as compared to randomly selecting the same number of customers which is 12% (term deposits/total obs for the test set).\nThe y-axis is the percentage of 1s (in binary classification) in that subset (quantiles from 1 to 30). Different from cumulative gains where the y-axis is the percentage of 1s in the entire dataset.\n\nIs that response big enough to have a successfull campaign, given costs and other expectations? Will the absolute number of sold term deposits meet the targets? Or do we lose too much of all potential term deposit buyers by only selecting the top 30%? To answer that question, we can go back to the cumulative gains plot.\nThe dashed horizontal is the same as in the Response Plot",
    "crumbs": [
      "Business Plots"
    ]
  },
  {
    "objectID": "qmd/business-plots.html#sec-bizplots-fin",
    "href": "qmd/business-plots.html#sec-bizplots-fin",
    "title": "Business Plots",
    "section": "Financial Plots",
    "text": "Financial Plots\n\nExample objective is to select the customers of a bank that are most likely to respond to an offer to purchase a “term deposit”. The outcome is binary: “term deposit” or “no”\n\nfixed costs = $75,000 (a tv commercial and some glossy print material)\nvariable costs per unit = $50 (customers are given an incentive to buy)\nprofit per unit = $250\n\nInformation from models used in these plots\n\nSame stuff as Marketing Plots\nFixed Costs (e.g. sales force expenses, advertising campaigns, sales promotion, and distribution costs)\nVariable Costs per unit (e.g.sales commission, bonuses, and performance allowances)\nProfit per Sale\n\nThe test dataset is used for the plots to get a realistic idea of what a marketing campaign in the would would cost and return. A validation set could be used on the Revenue and Costs Plot and models could be compared based risk of nonprofitability.\n\n\nProfit Plot\n\n\nAnswers the question: “When we apply the model and select up until quantile X, what is the expected profit of the campaign?”\nHow to apply:\n\nThe most profitable quantile is the one directly under the apex of the curve.\nThe most profitable quantile is highlighted by default, but this can be specified if so desired\nannotation means?\n\n\n\n\nCosts and Revenues Plot\n\n\nAnswers the question: “When we apply the model and select up until decile X, what are the expected revenues and investments of the campaign?”\nThe costs are the cumulative costs of selecting up until a given decile and consist of both fixed costs and variable costs.\nThe revenues take into account the expected response % - as plotted in the cumulative response plot - as well as the expected revenue per response.\nSolid curve is the revenue and the dashed diagonal line is the total costs\nHow to apply:\n\nThe campaign is profitable in the plot area where revenues exceed costs.\nGives an idea of the range of spending that can be considered while the campaign remains profitable. Ranges could be associated with risk. The smaller the range, the greater the risk given the uncertainty of the models. Various campaign ranges could be compared based on this risk.\nSee profits plot for optimal quantile.\n\n\n\n\nROI Plot\n\n\nAnswers the question: “When we apply the model and select up until decile X, what is the expected % return on investment of the campaign?”\nThe quantile at which the campaign profit is maximized is not necessarily the same as the quantile where the campaign ROI is maximized\n\nIt can be the case that a bigger selection (higher decile) results in a higher profit, however this selection needs a larger investment (cost), impacting the ROI negatively.\nSo maximum ROI can be considered the most effficient use of resources, but it takes money to make (the most) money.\n\nBasic formula for ROI = Net Profit / Total Investment * 100\nHow to apply:\n\nThe quantile directly underneath the apex of the curve is where the ROI is maximized.",
    "crumbs": [
      "Business Plots"
    ]
  },
  {
    "objectID": "qmd/web-design.html",
    "href": "qmd/web-design.html",
    "title": "Web Design",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Web",
      "Web Design"
    ]
  },
  {
    "objectID": "qmd/web-design.html#sec-webdes-misc",
    "href": "qmd/web-design.html#sec-webdes-misc",
    "title": "Web Design",
    "section": "",
    "text": "Resources\n\nWebsite Inspiration Catelog\n\nFirst-load under five seconds budget (guideline for non-highend mobile devices)\n\nJavaScript-heavy content: ~1.3MB, with 650KB of HTML, CSS, images, and fonts and 650KB of JavaScript\nMarkup-centric stacks: 2.5MB, with 2.4MB of HTML, CSS, images, and fonts and 100KB of JavaScript.\nSee hrbrmstr daily drop for more details on how developer tools can be used to analyze the size of webpages.\n\nUsing “Brand” to help choose font, palette, and imagery\n\nNotes from Erik Kennedy Video\nBrand is just adjectives to describe your business, organization, etc.\n\ne.g. Trustworthy, Geeky, Casual, Precise, Fun, Technical, etc.\n\nCommon Brands\n\n\n“Neat, modern”, “Luxury, formal”, etc. are more of what I’d consider brand adjectives\n“Clean & Simple”,“Fancy”, “Techie”, etc. are how I’d describe the sites that epitomize those brands, but they could also be brand descriptors\n\nBlending Brands\n\n\nShows names of company websites that most represent the brand/website types\ne.g. The Apple website is a blend of Techie and Fancy.\n\n\nWebsites should be under 14kb (article)\n\nMost web servers TCP slow start algorithm starts by sending 10 TCP packets which works out to 14kb\n\n404 pages\n\nGuidelines\n\nbe brief: the message on the 404 page should be straightforward and easy to understand, informing the user that the page they were trying to access is not available.\nbe contrite: the tone of the 404 page should be friendly and apologetic, acknowledging the user’s inconvenience and expressing empathy.\nbe helpful: provide links to other areas of the website or a search box that can help users find what they’re looking for quickly and easily.\nbe informative: include contact information, such as a feedback form, social media account, or email address to give users an alternative way to reach out to you for assistance.\nbe you: incorporate your brand’s visual identity, including logos and colors, to help reinforce brand recognition and create a cohesive user experience.\n\nExamples\n\n404 Page SVG Animations That Maximize Visitor Retention\n21 Stunning 404 Pages to Convert Lost Visitors 2023\nguinslym/awesome-404: A curated list of awesome 404 web pages greynoise’s ‘404’ equivalent and hrbmstr’s.",
    "crumbs": [
      "Web",
      "Web Design"
    ]
  },
  {
    "objectID": "qmd/git-general.html",
    "href": "qmd/git-general.html",
    "title": "25  General",
    "section": "",
    "text": "25.1 Misc",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#misc",
    "href": "qmd/git-general.html#misc",
    "title": "25  General",
    "section": "",
    "text": "View HTML file in browser\n\nSyntax: “https://raw.githack.com/&lt;acct name&gt;/&lt;repo name&gt;/&lt;branch name&gt;/&lt;directory name&gt;/&lt;file name&gt;.html”\n\nInstalling from a git repo (From link)\n\nMake a fork of the repo and then clone it to your local machine.\nTo update, after setting an upstream remote (git remote add upstream git://github.com/benfulcher/hctsa.git) you can use git pull upstream main.\nTo update the submodule in the repo, git submodule update --init\n\nStart R project and Git repo in whichever order (I think)\n\nCreate R project in RStudio\n\nChoose “New Directory” for all the templated projects (e.g. quarto book, shiny, etc.). None of the other choices have them.\n\nIf you’ve already created a directory, it will NOT overwrite this directory or add to it. So you’ll either have alter the name of your old directory or choose a new name.\n\n\nCreate repo on Github\n\nAdd license and readme\n\nDo work\nTools &gt;&gt; Version Control &gt;&gt; Project Set-up &gt;&gt; Version Control System &gt;&gt; Select Git\nOpen terminal and go to working directory of project\ngit checkout -B main\ngit pull origin main --allow-unrelated-histories\ngit add .\ngit commit -m \"initial commit\"\ngit push --set-upstream origin main \n\nTurn off “LF will be replaced by CRLF the next time Git touches it”\n\nMessage spams terminal when committing changes from a window machines. Has to do with line endings in windows vs unix.\nTurn off: git config core.autocrlf true\nSee SO post for more details\n\nURL format to download files from repositories\n\nhttps://raw.githubusercontent.com/user/repository/branch/filename\n\n# Or evidently this way works too\n# adds ?raw=true to the end of the url\nfeat_all_url &lt;- url(\"https://github.com/notast/hierarchical-forecasting/blob/main/3feat_all.RData?raw=true\")\nload(feat_all_url)\nclose(feat_all_url)\nGet filelist from repo and download to a directory\n\n** Directory urls change as commits are made **\n\nlibrary(httr)\n\n# example: get url for the data dir of covidcast repo\nreq &lt;- httr::GET(\"https://api.github.com/repos/ercbk/Indiana-COVIDcast-Dashboard/git/trees/master?recursive=1\") %&gt;% \n  httr::content()\n# alphabetical order\ntrees &lt;- req$tree %&gt;% \n  map(., ~pluck(.x, 1)) %&gt;% \n  as.character()\n# returns 20 which is first instance, so 19 should the \"data\" folder\ndetect_index(trees, ~str_detect(., \"data/\"))\n# url for data dir\nreq$tree[[19]]$url\n\n# example\n# Get all the file paths from a repo\nreq &lt;- GET(\"https://api.github.com/repos/etiennebacher/tidytuesday/git/trees/master?recursive=1\")\n# any request errors get printed\nstop_for_status(req)\nfile_paths &lt;- unlist(lapply(content(req)$tree, \"[\", \"path\"), use.names = F)\n# file_path wanted &lt;- filter file path to file you want\n# gets the very last part of the path\nfile_wanted &lt;- basename(file_path_wanted)\norigin &lt;- paste0(\"https://raw.githubusercontent.com/etiennebacher/tidytuesday/master/\", file_wanted)\ndestination &lt;- \"output-path-with-filename-ext\"\n# if file doesn't already exist, download it from repo into destination\nif (!file.exists(destination)) {\n      # if root dir doesn't exist create it\n      if (!file.exists(\"_gallery/img\")) {\n        dir.create(\"_gallery/img\")\n      }\n      download.file(origin, destination)\nThe insides of .git",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#optimizations",
    "href": "qmd/git-general.html#optimizations",
    "title": "25  General",
    "section": "25.3 Optimizations",
    "text": "25.3 Optimizations\n\nFor large repos, simple actions, like running git status or adding new commits can take many seconds. Cloning repos can take many hours.\nBenefits\n\nIt improves the overall performance of your development workflow, allowing you to work more efficiently. This is especially important when working with large organizations and open source projects, where multiple developers are constantly committing changes to the same repository. A faster repository means less time waiting for Git commands such as git clone or git push to finish. It helps to optimize the storage space, as large files are replaced by pointers which take up less space. This can help avoid storage issues, especially when working with remote servers.\n\nMisc\n\nSee How to Improve Performance in Git: The Complete Guide\n\nExplainer, config settings, advanced gc, checkout, and clone commands\n\n\nUse .gitignore\n\nGenerated files, like cache or build files\n\nThey will be modified at each different generation — and there’s no need to keep track of those changes.\n\nThird-party libraries\n\nInstead, aim for a list of the required dependencies (and the correct version) so that everyone can download and install them whenever the repo is cloned.\n\nFor example, with a package.json file for JavaScript projects you can (and should) exclude the /node_modules folder.\n.DS_Store files (which are automatically created by macOS) are another good candidate\n\n\n\nGit LFS\n\nDesigned specifically to handle large file versioning. LFS saves your local repositories from becoming unnecessarily big, preventing you from downloading unnessary data.\n\nGit LFS intercepts any large files and sends them to a separate server, leaving a smaller pointer file in the repository that links to the actual asset on the Git LFS server.\n\nThis is an extension to the standard Git feature set, so you will need to make sure that your code hosting provider supports it (all the popular ones do).\nAlso need to download and install the CLI extension on your machine before installing it in your repository.\nSet-Up\n$ git lfs install\n$ git lfs track \"*.wav\"\n$ git lfs track \"images/*.psd\"\n$ git lfs track \"videos\"\n$ git add .gitattributes\n\nTells Git LFS which file extensions it should manage.\n.gitattributes notes the file names and patterns in this text file and, just like any other change, it should be staged and committed to the repository.\nCan now add files and commit as normal\nList all file extensions being tracked: git lfs track\nList all files being managed: git lfs ls-files\n\n\nDon’t download the version history if you don’t need to\n\ngit clone –depth 1 gitj@github.com:name/repo.git",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#troubleshooting",
    "href": "qmd/git-general.html#troubleshooting",
    "title": "25  General",
    "section": "25.4 Troubleshooting",
    "text": "25.4 Troubleshooting\n\nDiverged Branches\n\n\nKeeps asking for username/password when pushing\n\nSolution: You (or if you used usethis::use_github/git) probably set-up a https connection when you need a ssh connection.\n\nsee https://docs.github.com/en/get-started/getting-started-with-git/managing-remote-repositories#changing-a-remote-repositorys-url to change from https to ssh.\n\n\nUndo a commit, but save changes made (e.g. you forgot to pull before you pushed)\n\nSteps\n\ngit log - Shows commit history. Copy the hash for your last commit\ngit diff &lt;last commit hash&gt; &gt; patch - save the diff of the latest commit to a file\ngit reset --hard HEAD^ to revert to the previous commit\n\n**After this, your changes will be lost locally **\n\ngit log - confirm that you are now at the previous commit\ngit pull - correct the mistake you made in first place\npatch -p1 &lt; patch - apply the changes you originally made\ngit diff - to confirm that the changes have been reapplied\nNow, you do the regular commit, push routine\n\n\nUndo uncommitted changes: git stash followed by git stash drop\n\n“but only use if you commit often” - guessing this is not good if your commit is somehow large and/or involves multiple files\n\nSearch commits by string: git log --grep &lt;string&gt;",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#pulling",
    "href": "qmd/git-general.html#pulling",
    "title": "25  General",
    "section": "25.5 Pulling",
    "text": "25.5 Pulling\n\nSave your changes, pull in an update, apply your changes\ngit stash\ngit pull\ngit stash pop\n\ngit stash pop throws away the (topmost, by default) stash after applying it, whereas\ngit stash apply leaves it in the stash list for possible later reuse (or you can then git stash drop it).\n\nRe potential merge conflicts\n\n“For instance, say your stashed changes conflict with other changes that you’ve made since you first created the stash. Both pop and apply will helpfully trigger merge conflict resolution mode, allowing you to nicely resolve such conflicts… and neither will get rid of the stash, even though perhaps you’re expecting pop too. Since a lot of people expect stashes to just be a simple stack, this often leads to them popping the same stash accidentally later because they thought it was gone.”\n\nPulling is fetching + merging\n\nFetching just gets the info about the commits made to the remote repo\ngit fetch origin\nSome technical discussion for always using git pull –ff\n\nhttps://blog.sffc.xyz/post/185195398930/why-you-should-use-git-pull-ff-only-git-is-a\nhttps://megakemp.com/2019/03/20/the-case-for-pull-rebase/\nit’s still confusing but pull rebase sounds fine to me\n–global tag says do it for all my repos\nnot sure what the true and only are for\n\ngit pull –help will open doc in browser\n\n\nPulling by rebase\n\nLocal: using this method as default\ngit config pull.rebase true\ngit pull\nRemote\ngit pull --rebase\n\nPulling by fast-forward\n\nLocal: using this method as default\ngit config --global pull.ff only\ngit pull\nRemote\ngit pull --ff",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#branching",
    "href": "qmd/git-general.html#branching",
    "title": "25  General",
    "section": "25.6 Branching",
    "text": "25.6 Branching\n\nMisc\n\nCreate a new branch for each ticket you are working on or each data model. It can get sloppy when you put all your code changes on one branch.\nHEAD\n\nDetached HEAD\n\n\nCreate a branch (e.g. “testing”)\ngit branch testing\nWork in a branch\ngit checkout testing\nThe files in your working directory change to the version saved in that branch\nIt adds, removes, and modifies files automatically to make sure your working copy is what the branch looked like on your last commit to it.\nCreate and work in a branch\n# new way\ngit switch -c testing\nor\ngit checkout -b testing\nor\ngit branch testing\ngit checkout testing\ncreates the branch and switches you to working in that branch\nIf you did a bunch of changes in a codebase, only to realize that you’re working on `master`,  switch will bring those local changes with you to the new branch. So I guess they won’t affect master then.\n\nUnless If you already committed to main, then those changes are both in your new branch and in main. So you would still have to clean up the main branch.\n\nDeleting a branch\n\nlocal branch\ngit branch -d testing\n\nremote branch\ngit push &lt;remoteName&gt; --delete &lt;branchName&gt;\nSee existing branches\ngit branch\nSee what has been commited the remote repo branches\ngit fetch origin\ngit branch -vv\n“origin” is the name of the remote\nresult\ntesting    7e424c3 [origin/testing: ahead 2, behind 1] change abc \nmaster      1ae2a45 [origin/master] Deploy index fix\n* issue    f8674d9 [origin/issue: behind 1] should do it         \ncart        5ea463a Try something new\nformat: branch, last commit sha-1, local branch status vs remote branch status, commit message\nthe star indicates the HEAD pointer’s location (where you’re at, i.e. checkout)\ntesting branch\n\n“ahead 2” means  I committed twice to the local testing branch and this work has not been pushed to the remote testing branch repo yet.\n“behind 1” means someone has pushed a commit to the remote testing branch repo and we haven’t merged this work to our local testing branch\n\nGet the last 10 branches that you’ve committed to locally:\ngit branch --sort=-committerdate | head -n 10\nRename branch\n# change locally\ngit branch --move &lt;bad-branch-name&gt; &lt;corrected-branch-name&gt;\n# change remotely in repo\ngit push --set-upstream origin &lt;corrected-branch-name&gt;\n# confirm change\ngit branch --all\nHEAD determines to which branch new commits are added\n\nExample\n\n“testing” branch is created (not shown in above picture)\n\nHEAD points at “master” branch\n“master” branch and the new “testing” branch both point at commit, f30ab.\nf30ab commit points to previous commit 34ac2\n\nuser executes checkout to “testing” branch (not shown in picture)\n\nHEAD now points to testing branch\n\nuser commits 87ab2 (shown in pic)\n\n87ab2 is committed to the “testing” branch\n“testing” branch is now ahead of the “master” branch by 1 commit\n\n\nExample\n\nEverything above happens but now another user commits the master branch.\n\nBoth branches are in conflict. The testing branch is ahead and behind by 1 commit\n\n\n\nMerging\n\n\nNotes\n\nNEVER merge your branch locally on your machine with the master branch, ALWAYS merge online via pull request\n\nSteps\n\nPush final changes and use of a pull request\nSwitch to master branch locally and pull the merged changes\n\n\n\nUpdate branch with work that’s been done in master branch\n\nAfter updating your local branch, push to remote repo (no commit necessary)\n# while in branch\ngit merge master\n\n\nFast-Forward\n\nExample\n\nBefore the merge\n\nthe testing branch is 1 commit ahead of the master branch and the master branch doesnt have a new commit\n\nAfter the merge\n\nmaster is moved forward to the testing branch commit\n\n\nCode (merging work in branch with the master branch for production)\n# currently in test branch\ngit checkout master\ngit merge testing\n\nLines in file are marked\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.html\n# &lt;div id=\"footer\"&gt;contact : email.support@github.com&lt;/div&gt;\n# =======\n# &lt;div id=\"footer\"&gt;\n# please contact us at support@github.com\n# &lt;/div&gt;\n# &gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html\nAbove ======= is the master branch version of the code and below is the iss53 branch version\nMake necessary changes and save the file\ngit add . or git add &lt;resolved file&gt;\n\nTells git that conflict is resolved\n\nCheck status to confirm everything has been resolved\ngit status\n\n    On branch master\n    All conflicts fixed but you are still merging.\n      (use \"git commit\" to conclude merge)\n    Changes to be committed:\n      modified:  index.html\ngit commit\n\nno message required (there’s a default message) but you can add one if you want\n\nExample\n\niss53 branch ahead of master by 2 commits (c3, c5) and behind 1 commit (c2)\nSame code as Fast-Forward merge but git handles the merge a bit differently\ngit checkout master \ngit merge iss53\n\n\n\nC6 (right pic) is called a “merge commit.” Its created by git and points to two commits instead of one.\nNo need to merge with master (i.e. update local iss53 branch with c4 changes in master) before committing final changes\n\nIf there are changes in the same lines of code C4 and C5, then there will be a conflict (See below, Conflicts &gt;&gt; Example)\n\n\nConflicts\n\nExample\n\nChanged files in C4 (see above example) are in the same lines of the same files that you made changes to in C5\n\nRemember: you’re now in the master branch since you did checkout master as part of the merge code\nSteps\n\nCheck status to which files are causing the conflict (e.g. index.html)\ngit status\n  Unmerged paths:\n  (use \"git add &lt;file&gt;...\" to mark resolution) \n    both modified:      index.html\n\n\n\n\nMoving between branches\n\nfrom master to testing\ngit checkout testing\n\nlocal files are deleted and replaced with branch versions\n\nalternative: worktree\n\nExample\n\nWhat happens when you move from branch-a to branch-b\nBRANCH-A        BRANCH-B\nalpha.txt      alpha.txt\nbravo.txt\ncharlie.txt    charlie.txt\n                delta.txt\n\nbravo text is deleted from your local disc and delta.txt is added\nIf any changes to alpha.txt or charlie.txt have been made and no commit has been made, the checkout will be aborted\n\nSo either revert the changes or commit the changes\n\nUntracked files or newly created files\n\nIf you have branch-A checked out and you create a new file called echo.txt, Git will not touch this file when you checkout branch-B. This way, you can decide that you want to commit echo.txt against branch-B without having to go through the hassle of (1) move the file outside the repo, (2) checkout the correct branch, and (3) move the file back into the repo.",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#collaboration",
    "href": "qmd/git-general.html#collaboration",
    "title": "25  General",
    "section": "25.7 Collaboration",
    "text": "25.7 Collaboration\n\nAdd collaborators to your repository\nOne person invites the others and provides them with read/write access (github docs)\n\nSteps\n\nGo to the settings for your repository\nmanage access &gt;&gt; “invite a collaborator”\n\nSearch for each collaborator by full name, acct name, or email\nClick “Add &lt;name&gt; to &lt;repo&gt;”\n\nEach collaborator will need to accept the invitation\n\nSent by email",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html",
    "href": "qmd/db-normalization.html",
    "title": "Normalization",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html#sec-db-norm-misc",
    "href": "qmd/db-normalization.html#sec-db-norm-misc",
    "title": "Normalization",
    "section": "",
    "text": "Organizing according to data attributes to reduce or eliminate data redundancy (i.e. having the same data in multiple places).\n\nIt gives you a set of rules to be able to start categorizing your data and forming a layout\n\nBy establishing structure in a database, you are able to help establish a couple of important things: data integrity and scalability.\n\nIntegrity ensures that data is entered correctly and accurately.\nScalability ensures you have organized the data in a way that it is more computationally efficient when you start to run SQL queries.\n\nNotes from When Spreadsheets Aren’t Good Enough: A Lesson in Relational Databases\n\nGives an example of normalizing a dataset through a MySQL analysis\n\nResources\n\nUsing DuckDB-WASM for in-browser Data Engineering has a complete example that builds a snowflake schema with SQL code.\n\nPackages\n\n{{autonormalize}} - analyzes transaction df and creates relational tables - python library for automated dataset normalization",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html#sec-db-norm-terms",
    "href": "qmd/db-normalization.html#sec-db-norm-terms",
    "title": "Normalization",
    "section": "Terms",
    "text": "Terms\n\nDimension Tables - Contains data about how the data in Fact Table is being analyzed. They facilitate the fact table in gathering different dimensions on the measures which are to be taken.\n\nProvides descriptive attributes for the fact table data. Think of them as adding context and details to the “what” and “how much”. Each dimension table focuses on a specific category of information.\nExample: An online retail store wants to analyze sales data.\n\nCustomer Dimension - Provides information about individual customers, like their name and location. The customer id field is the primary key that links it back to the fact table.\nProduct Dimension - Describes each product, including its category, brand, and other relevant details. The product id field is the primary key that links it back to the fact table.\n\n\nFact Tables - Contain data corresponding to any business process. Every row represents any event that can be associated with any process. It stores quantitative information for analysis.\n\nFocuses on quantitative data representing business events or transactions. Think of it as storing the “what” and “how much” of your data.\nExample: An online retail store wants to analyze sales data.\n\nFields: quantity, price, and total say what products were ordered, by whom, when, and how much they cost.\nAn order id would be the primary key. and the table would also contain the foreign keys for all the dimension tables (e.g product id and customer id) associated with it.",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html#sec-db-norm-form",
    "href": "qmd/db-normalization.html#sec-db-norm-form",
    "title": "Normalization",
    "section": "Forms",
    "text": "Forms\n\nDatabases are often considered as “normalized” if they meet the third normal form\nSee A Complete Guide to Database Normalization in SQL for details on the other 4 forms.\n\nAlso gives an example of normalizing a dataset through a posgresSQL analysis\n\nFirst normal form (1NF)\n\nEvery value in each column of a table must be reduced to its most simple value, also known as atomic.\n\nAn atomic value is one where there are no sets of values within a column. (i.e. 1 value per cell)\n\nThere are no repeating columns or rows within the database.\nEach table should have a primary key which can be defined as a non-null, unique value that identifies each row insertion. Second normal form (2NF)\nConforms to first normal form rules.\nAdjust columns so that each table only contains data relating to the primary key.\nForeign keys are used to establish relationships between tables. Third normal form (3NF)\nConforms to both first and second normal form rules.\nNecessary to shift or remove columns (attributes) that are transitively dependent, which means they rely on other columns that aren’t foreign or primary keys.",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html#sec-db-norm-schema",
    "href": "qmd/db-normalization.html#sec-db-norm-schema",
    "title": "Normalization",
    "section": "Schema",
    "text": "Schema\n\nMisc\n\nFactors that influence normalizing dimension tables\n\nData redundancy concerns: If minimizing redundancy is crucial, normalization might be preferred.\nQuery performance priorities: If query performance is paramount, denormalization often offers advantages.\nData consistency requirements: High consistency needs might favor normalization.\nMaintenance complexity: Denormalized dimensions can be simpler to maintain in some cases.\n\nDon’t use external IDs as primary keys\n\nSince you don’t control those IDs, they can change the format and break your queries.\n\n\nStar\n\n\nExample: A Star schema of sales data with dimensions such as customer, product & time.\nIn a star schema, as the structure of a star, there is one fact table in the middle and a number of associated dimension tables.\nThe fact table consists of primary information. It surrounds the smaller dimension lookup tables which will have details for different fact tables. The primary key which is present in each dimension is related to a foreign key which is present in the fact table.\nThe fact tables are in 3NF form and the dimension tables are in denormalized form. Every dimension in star schema should be represented by the only one-dimensional table.\n\nSnowflake\n\n\nSnowflake schema acts like an extended version of a star schema. There are additional subdimensions added to dimensions.\nUnlike the Star schema, dimensions are normalized.\nCan be slower than star schemas due to complex joins across multiple tables, but achieves better storage efficiency compared to star schemas due to reduced data redundancy.\nThere are hierarchical relationships and child tables involved that can have multiple parent tables.\nThe advantage of snowflake schema is that it uses small disk space. The implementation of dimensions is easy when they are added to this schema.\n\nFact Constellation or Galaxy\n\n\nA fact constellation can consist of multiple fact tables. These are more than two tables that share the same dimension tables — like connected Star schema.\nThe shared dimensions in this schema are known as conformed dimensions. Denormalization in shared dimension tables might increase storage size compared to fully normalized schemas.\nDimensions can be normalized but is rare in this schema due the level of complexity already present.\nUseful when aggregation of fact tables is necessary. Fact constellations are considered to be more complex than star or snowflake schemas. Therefore, more flexible but harder to implement and maintain. Joins across multiple fact and dimension tables can lead to complex queries with potential performance impacts.",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/db-normalization.html#sec-db-norm-dsgn",
    "href": "qmd/db-normalization.html#sec-db-norm-dsgn",
    "title": "Normalization",
    "section": "Design",
    "text": "Design\n\nMisc\n\nOracle Data Model Documentation\n\nConsiderations\n\n7 Vs\n\nVolume: How big is the incoming data stream and how much storage is needed?\nVelocity: Refers to speed in which the data is generated and how quickly it needs to be accessed.\nVariety: What format the data needs to be stored? Structured such as tables or Unstructured such as text, images, etc.\nValue: What value is derived from storing all the data?\nVeracity:How trustworthy the data source, type and its processing are?\nViscosity: How the data flows through the stream and what is the resistance and the processability?\nVirality: Ability of the data to be distributed over the networks and its dispersion rate across the users_\n\nData Quality (See Database, Engineering &gt;&gt; Data Quality) completeness, uniqueness, timeliness, validity, accuracy, and consistency\n\nComponents\n\n\nMetamodeling:\n\nDefines how the conceptual, logical, and physical models are consistently linked together.\nProvides a standardized way of defining and describing models and their components (i.e. grammar, vocabulary), which helps ensure consistency and clarity in the development and use of these models.\nData ownership should be assigned based on a mapping of data domains to the business architecture domains (i.e. market tables to the marketing department?)\n\nConceptual Modeling - Involves creating business-oriented views of data that capture the major entities, relationships, and attributes involved in particular domains such as Customers, Employees, and Products.\nLogical Modeling - Involves refining the conceptual model by adding more detail, such as specifying data types, keys, and relationships between entities, and by breaking conceptual domains out into logical attributes, such as Customer Name, Employee Name, and Product SKU.\nPhysical Data Modeling - Involves translating the logical data model into specific database schemas that can be implemented on a particular technology platform\n\nProcess (article, article, article)\n\nUnderstand the Core Business Requirements\n\nCreate a catalogue of reporting stories for each stakeholder to an idea of the reports that each will want generated\n\nThese will inform you of the data requirements\ne.g. “As a marketing manager, I need to know the number of products the customer bought last year in order to target them with an upsell offer.”\n\nFrom the story above, I can determine that we will need to aggregate the number of products per customer based on sales from the previous year.\n\n\n\nSelect the tools and technologies:\n\nUsed to build and manage the data warehouse. This may include selecting a database management system (DBMS), data integration and extraction tools, and analysis and visualization tools.\nWarehouses - See Brands\nSee Production, Tools &gt;&gt;\n\nOrchestration\nELT/ETL Operations\n\n\nChoose a data model\n\nIdentify Business Processes\n\nFocus on business process and not business departments as many departments share the same business process\nIf we focus on department, we might end up with multiple copies of models and have different sources of truth.\n\nChoose a data model from the Business Process\n\nStart with the most impactful model with the lowest risk\n\nConsult with the stakeholders\n\nShould be used frequently and be critical to the business and also it must be built accurately\n\nDecide on the data granularity\n\nMost atomic level is the safest choice since all the types of queries is typically unknown\nNeed to consider the size and complexity of the data at the various granularities, as well as the resources available/costs for storing and processing it.\nExamples\n\nCustomer Level - easy to answer questions about individual customers, such as their purchase history or demographic information.\nTransaction Level - easy to answer questions about individual transactions, such as the products purchased and the total amount spent.\nDaily or Monthly?\n\n\n\nCreate Conceptual Data Models (Tables)\n\nThese represent abstract relationships that are part of your business process.\nExplains at the highest level what respective domains or concepts are, and how they are related.\nThe elements within the reporting stories should be consistent with these models\n\nExample: Retail Sales\n\nTime, Location, Product, and Customer.\n\nTime might be used to track sales data over different time periods (e.g. daily, monthly, yearly).\nLocation might be used to track sales data by store or region.\nProduct might be used to track sales data by product category or specific product.\nCustomer might be used to track sales data by customer demographics or customer loyalty status.\n\n\n\nExample:\n\n\nTransactions form a key concept, where each transaction can be linked to the Products that were sold, the Customer that bought them, the method of Payment, and the Store the purchase was made in — each of which constitute their own concept.\nConnectors show that each individual transaction can have at most one customer, store, or employee associated with it, but these in turn can be associated with many transactions (multi-prong connector into Transactions)\n\nExample:\n\n\nEach Customer (1 prong connector) can have 0 or more Orders (multi-prong connector)\nEach Order can have 1 or more Products\nEach Product can have 0 or more Orders\n\n\nCreate Logical Data Models\n\nBreakdown each entity of the conceptual model into attributes\n\nExample:\n\nExample:\n\n\n\nCreate Physical Data Models\n\nDetails are added on where exactly (e.g., in what table), and in what format, these data attributes exist.\n\ne.g. finalizing table names, column names, data types, indexes, constraints, and other database objects\n\nTranslate the logical data model into specific database schemas that can be implemented on a particular technology platform\n\ne.g. dimensional modelling in a star schema or normalisation in a 3rd normal form in a snowflake model.\n\nExample:\n\nExample: Dimension model in a star schema\n\n\nfact_ (quantitative) and dim_ (qualitative)\n\n\nMake Design and Environment decisions\n\nDecide on:\n\nPhysical data models\nHistory requirements\nEnvironment provisions & set up\n\n\nBuild a prototype (aka wireframe) of the end product\n\nThe business end-user may have a vision, they couldn’t coherently articulate at the requirement phase.\nThe prototype need not use real-world data or be in the reporting tool.\n\n** Profile known sources data **\n\nLearn about the data quality issues, and try and remediate those issues before designing your data pipelines.\n\nIf an issue cannot be resolved, you will have to handle it in your data pipeline\n\n\nBuild, Test, and Iterate\n\nCreate ETL jobs or data pipelines\n\nIteratively need to unit test the individual components of the pipeline.\n\nThe data will need to be moved from the source system into our physical warehouse\nProfile data\n\nData types, and if conversion is required\nThe amount of history that needs to be pulled\n\nValidate the model’s output numbers with the business end-user\n\nProgress towards Data Maturity (see Job, Organizational and Team Development &gt;&gt; Data Maturity)",
    "crumbs": [
      "Databases",
      "Normalization"
    ]
  },
  {
    "objectID": "qmd/cli.html",
    "href": "qmd/cli.html",
    "title": "CLI",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-misc",
    "href": "qmd/cli.html#sec-cli-misc",
    "title": "CLI",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly’s suggestions are prioritized in real time with a small neural network\n\nPath to a folder that’s above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder\n\nDebian vs. Ubuntu (from ChatGPT)\n\nStability vs. Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it’s known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu’s.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as “Debian spins,” catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-r",
    "href": "qmd/cli.html#sec-cli-r",
    "title": "CLI",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n      janitor::clean_names() %&gt;%\n      select(date, geo, county = name, negative, positive) %&gt;%\n      filter(geo == \"County\") %&gt;%\n      mutate(date = lubridate::as_date(date)) %&gt;%\n      select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it’ll search automatically\n35548",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-awk",
    "href": "qmd/cli.html#sec-cli-awk",
    "title": "CLI",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‘$13 &gt; 98’ adult_t.csv|head\nFilter lines with “Doctorate” and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-bash",
    "href": "qmd/cli.html#sec-cli-bash",
    "title": "CLI",
    "section": "Bash",
    "text": "Bash\n\nMisc\n\nNotes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn’t take notes on\n\nBash Scripting on Linux: The Complete Guide - video course\n\nResources\n\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n“&gt;” redirects the output from a program to a file.\n\n“&gt;&gt;” does the same thing, but it’s appending to an existing file instead of overwriting it, if it already exists.\n\n\n\n\n\nCommands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you’re using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you’re about to move does already exist under the new directory,\n\nThis way you don’t accidentally overwrite files that you didn’t mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\n\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\nOutput to file: echo “This is an example for redirect” &gt; file1.txt\nAppend line to file: echo “This is the second line of the file” &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to “tmp” directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name “my_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n“rwx” stand for read, write, and execute rights, respectively\nThe 3 “rwx” blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a “d” for directory or “l” for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - “super user” - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‘error’, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo “No such directory exist.Check”\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn’t exist, then the message “No such directory exists. check” message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for “error” and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for “error” in each line. Lines with “error” get written to “error_file.txt”\n\nFilter lines\ngrep -i “Doctorate” adult_t.csv |grep -i “Husband”|grep -i “Black”|csvlook\n# -i, --ignore-case-Ignore  case  distinctions,  so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i “Doctorate” adult_t.csv | wc -l\n\nCounts how many rows have “Doctorate”\n\n-wc is “word count”\n\n\n\n\n\n\nVariables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via “$”\n# local\nev_car=’Tesla’\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=’Tesla’\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it’s okay not to is on the left-hand-side of an [[ ]] condition. But even there I’d recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or –help or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this\n\n\n\n\n\nScripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like –silent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script’s directory close to the start of the script.\n\nAnd it’s usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck. Heed its warnings.\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don’t give executable permission to the script file.\nStarts with “#!” the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory “/bin”\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you’re good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n    #!/bin/bash\n    echo ‘Hello World’\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n    set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n    echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n    exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n    echo do awesome stuff\n}\nmain \"$@\"\n\n\n\nJob Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.\n\n\n\n\ntmux (‘terminal multiplexer’)\n\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‘moose’\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named “moose”\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you’ve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‘moose’\n\n\nPane Creation and Navigation\n\ncontrol+b then press ” (i.e. shift+’): add another terminal pane below\ncontrol+b then press % (i.e. shift+5) : add another terminal pane to the right\ncontrol+b then press → : move to the terminal pane on the right (similar for left, up, down)\n\n\n\n\nSSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n“ssh-keygen” is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named “id_rsa.pub” and a private key named “id_rsa”, and place both into your “~/.ssh” directory\nYou’ll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: “~/.ssh/config”\nContents\nHost dev\n  HostName remote\n  IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist\n\n\n\n\nVim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you’ll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you’re in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file\n\n\n\n\nPackages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to “upgrading” the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and “updates” them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade\n\n\n\n\nExpressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-powsh",
    "href": "qmd/cli.html#sec-cli-powsh",
    "title": "CLI",
    "section": "Powershell",
    "text": "Powershell\n\nComments: &lt;# comment #&gt;\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you’re already in the directory that you want the folder in. You can also use a path, e.g. \"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it’s a backslash (\\), but in Powershell, it’s a backtick ( ` )\n*Don’t forget that there’s a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it’s open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-batscri",
    "href": "qmd/cli.html#sec-cli-batscri",
    "title": "CLI",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g. timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-wsl",
    "href": "qmd/cli.html#sec-cli-wsl",
    "title": "CLI",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for –distribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html",
    "href": "qmd/cloud-services.html",
    "title": "4  Cloud Services",
    "section": "",
    "text": "4.1 Misc",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#misc",
    "href": "qmd/cloud-services.html#misc",
    "title": "4  Cloud Services",
    "section": "",
    "text": "See Cloud Costs Every Programmer Should Know for various service estimates in order to perform back-of-the-napkin calculations of project costs\nFor “Stead-State Workloads” requiring HPC, cloud compute doesn’t make economic sense\n\nSteady-State workloads are projects that are run near constantly\n\nSee Thread for discussion on scenarios, issues, and risks of your data center (DC) in the Cloud vs on-prem.\nExamples:\n\nAcademia: Where academics are in a queue to run experiments on the a HPC cluster\nWeather Forecasting: Forecasts are required nearly in real time, so these models run constantly\nFinancial transaction processing at a bank: The bank’s systems handle a constant stream of transactions\nInventory management system for a manufacturing plant: The system constantly receives updates on raw materials, production output, and finished goods.\nOthers: Week or two long analysis runs at hedge funds, genomic analysis jobs, a swath of AI training / fine tuning, Oil and Gas where they are plowing through seismic data constantly\n\n\n\nRStudio Server on your docker image allows you to access an ide connected to the server through a browser. Useful so you can make sure the correct packages are installed.\nServerless computing is a method of providing backend services on an as-used basis.\n\nA serverless provider allows users to write and deploy code without the hassle of worrying about the underlying infrastructure\nCharged based on their computation and do not have to reserve and pay for a fixed amount of bandwidth or number of servers, as the service is auto-scaling\ne.g. AWS Lambda (i.e. resources only get spun-up when an event is triggered)\n\nNVIDIA GPU Guide (thread)\n\nRTX 20-series or 30-series GPUs are forbidden from inclusion in data centers\nGeneral Recommendations (Oct 2022)\n\nA100 for model training\nT4 for inference workloads\n\nK80\n\nReleased in 2015, the K80 contained a lot of VRAM for the time (24 GB)\nCame before tensor cores and is relatively weak by today’s standards\nOnly okay for learning purposes\n\nP4\n\nReleased in 2016\nValue came from its low power consumption\nMay find it priced higher than its upgraded version (the T4), so recommended to avoid it\n\nT4\n\nReleased in 2018\nSignificant upgrade for inference workloads compared to the P4\nExtremely low power consumption, tensor cores, and plenty (16GB) of VRAM\nCheap, so if you have an inference workload, recommended to strongly consider a T4\n\nP100\n\nBig improvement for model training workloads over the K80 when released\nLess RAM (16GB) than K80\nWay more compute  than K80\n\nCan see memory savings from using mixed-precision training\n\nNo tensor cores\n\nV100\n\nHuge upgrade over the P100\nSame VRAM as P100 many but more CUDA cores\nIntroduces Tensor Cores\nMore cost-efficient than the P100\n\nA100\n\nnewest data center GPU\nupgraded tensor cores\nmost benchmarks show 3x+ faster training compared to the V100\n80GB VRAM\nPrice tag might be big, but it’s usually worth it over the V100",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#price-management",
    "href": "qmd/cloud-services.html#price-management",
    "title": "4  Cloud Services",
    "section": "4.2 Price Management",
    "text": "4.2 Price Management\n\nspot instances for cheaper machines\nautoscaling (kubernetes?) to handle peak usage times (spin-up more machines) while saving during slow times (spin down excess machines)\nUse opensource project management tools (dvc, airflow, etc)\nGoogle\n\nThe Google Kubernetes Engine (GKE) control plane is free, whereas Amazon’s (EKS) costs $0.20 an hour.\n\nAWS\n\nWith a well-defined framework of tag keys and values applied across different AWS resources, billing breakdowns by tag prove extremely useful for greater insight on the source of AWS charges — especially if resources are tagged by department, or team, or different layers of organizational granularity.\nReserved Instances - commit to specific configurations for one or three years at reduced cost\nSpot Instances - pay significantly lower costs but potential for applications to be interrupted\nSavings Plans\n\nEC2 Instance Savings Plans to reduce compute charges for specific instance types and AWS regions\n\nSavings of up to 72%\n\nCompute Savings Plans to reduce compute costs irrespective of type and region.\n\nSavings up to 66% and extends to ECS Fargate and Lambda functions.\n\n\nImage Management\n\nData Lifecycle Manager - automates the creation, retention, and deletion of images\n\nWill not manage images and snapshots created by other means, and it also excludes instance store-backed images.\nEC2 Recycle Bin - serves as a safety net to avoid the accidental deletion of resources — retaining images and snapshots for a configurable time where we may restore them before they are deleted permanently.\n\n\nLambda\n\nCloudwatch - Lambda automatically creates log groups for its functions, unless a group already exists matching the name /aws/lambda/[{functionName}]{style='color: #990000'}. These default groups do not configure a log retention period, leaving logs to accumulate indefinitely and increasing CloudWatch costs.\n\nExplicitly configure groups with matching names and a retention policy to maintain a manageable volume of logs.\n\nMemory Optimization - AWS Lambda Power Tuning can help to identify optimizations, albeit with notable initial costs given the underlying use of AWS Step Functions.\n\nLambda charges based on compute time in GB-seconds, where the duration in seconds is measured from when function code executes until it either returns or otherwise terminates, rounded up to the nearest millisecond. To reduce these times, we desire optimal memory configuration.\n\n\nS3 Lifecycle Configuration\n\nCharged for how much data stored, but also which S3 storage classes are utilized.\n\nStandard (default) class is the most expensive, permitting regular access to objects with high availability and short access times.\nInfrequent Access (IA) classes offer reduced cost for data which requires limited access (usually once per month)\nArchival options via Glacier deliver further cost reductions.\n\nConfiguring the lifecycle allows you to automatically transfer data to different storage classes and thereafter permanently delete it, X and Y days respectively after data creation",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#kaggle",
    "href": "qmd/cloud-services.html#kaggle",
    "title": "4  Cloud Services",
    "section": "4.3 Kaggle",
    "text": "4.3 Kaggle\n\nFree\n\n4-core CPU instances w/30 GB RAM\n2-core CPU, 2xT4 GPU w/13GB RAM\n\nT means tensor cores\n1 hour spent using 2xT4’s takes the same amount of your quota as a P100 (old free gpu offering)\n\nMeans 30-40 hours of free, multi-GPU compute per week",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#saturn-cloud",
    "href": "qmd/cloud-services.html#saturn-cloud",
    "title": "4  Cloud Services",
    "section": "4.4 Saturn Cloud",
    "text": "4.4 Saturn Cloud\n\nSaturn Cloud Recipes\n\nJSON files that specify your environment\nGood for keeping track of server dependencies (e.g. linux libraries)\n\nDunno about R packages",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#google-cloud-platform-gcp",
    "href": "qmd/cloud-services.html#google-cloud-platform-gcp",
    "title": "4  Cloud Services",
    "section": "4.5 Google Cloud Platform (GCP)",
    "text": "4.5 Google Cloud Platform (GCP)\n\nBigQuery sandbox is Google’s GCP free tier cloud SQL database. It’s free but your data only lasts 60 days at a time.\nGCP allows users to run deep learning workloads on TPUs\nSince data expires after 60 days, back-up the model coefficients and performance score tables to Google Sheets. Article suggested this is possible through WebUI.\nAs of Nov.19, regression, logistic regression, and k-nn are the only models available to be run with the sql query editor\nhttps://cloud.google.com/free/\n\n$300 credit for 12 months\nAlways free:\n\n2M requests for containers\n1 GB storage\n\nScalable NoSQL document database.\n50,000 reads, 20,000 writes, 20,000 deletes per day\n\nFunctions\n\n1 f1-micro instance per month (Available only in region: us-west1, Iowa: us-central1, South Carolina: us-east1)\n30 GB-months HDD\n5 GB-months snapshot in select regions\n1 GB network egress from North America to all region destinations per month (excluding China and Australia)\n\nKubernetes\n\nOne-click container orchestration via Kubernetes clusters, managed by Google.\nNo cluster management fee for clusters of all sizes\nEach user node is charged at standard Compute Engine pricing\n\nApp Engine\n\n28 instance hours per day\n5 GB Cloud Storage\nShared memcache\n1,000 search operations per day, 10 MB search indexing\n100 emails per day\n\nBigQuery\n\nFully managed, petabyte scale, analytics data warehouse.\n1 TB of querying per month\n10 GB of storage\n\nOther Stuff\n\nYour free trial credit applies to all GCP resources, with the following exceptions:\n\n* You can’t have more than 8 cores (or virtual CPUs) running at the same time.\n* You can’t add GPUs to your VM instances.\n* You can’t request a quota increase. For an overview of Compute Engine quotas, see Resource quotas.\n* You can’t create VM instances that are based on Windows Server images.\n\nYou must upgrade to a paid account to use GCP after the free trial ends. To take advantage of the features of a paid account (using GPUs, for example), you can upgrade before the trial ends. When you upgrade, the following conditions apply:\n\n* Any remaining, unexpired free trial credit remains in your account.\n* Your credit card on file is charged for resources you use in excess of what’s covered by any remaining credit.\nYou can upgrade your account at any time after starting the free trial. The following conditions apply depending on when you upgrade:\n* If you upgrade before the trial is over, your remaining credit is added to your paid account. You can continue to use the resources you created during the free trial without interruption.\n* If you upgrade within 30 days of the end of the trial, you can restore the resources you created during the trial.\n* If you upgrade more than 30 days after the end of the trial, your free trial resources are lost.\n\nSpot Instances (Preemptible VM)\n\nusage capped at 24 hrs\npricing is fixed and not market-driven\n\nGoogle price calculator: https://cloud.google.com/products/calculator/#id=3115f19f-4ff0-4c57-9028-69cb994fe7ca\nExample\n\ncreating a cluster with:\n\n1 x Dataproc cluster node with 30 GB of RAM\n3 x Dataproc worker nodes with 15 GB of RAM\nUsing less than 5 GB of disk space in a bucket\nAnd running the cluster for only 4 hrs\nWould cost only around $5 at the end of the month\n\n\nFree Tier\n\nincludes a 12-month free trial with $300 credit to use with any GCP services and an Always Free benefit, which provides limited access to many common GCP resources\nUse to test out, but KEEP EVERYTHING SMALL (data, hardware, etc). Need to upgrade it to see the true benefit. Free tier resources look like my desktop computer. Whatever cash is leftover should transfer to account.\nhttps://cloud.google.com/free/docs/gcp-free-tier#how-to-upgrade\nupgrade it from the free trial to a paid account through the GCP Console clicking the Upgrade button at the top of the page\n\n\n\nSteps for new project\n\nGo to interface https://console.cloud.google.com/\ncreate a project. “select a project” on top bar –&gt; “new project” on top right –&gt; choose name (optionally a folder/organization if you have one) –&gt; create\n(article wasn’t very reliable and went on talk about a python implementation so I stopped here\n\nTips\n\nApp Engine\n\nDon’t use App Engine Standard environments — big brother G wants you to use rather Flex environments, otherwise, they’ll punish you.\nReview cost analysis regularly to make sure there are no surprising costs.\nMake sure you clean up redundant App Engine application versions to prevent G from robbing you.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#microsoft-azure",
    "href": "qmd/cloud-services.html#microsoft-azure",
    "title": "4  Cloud Services",
    "section": "4.6 Microsoft Azure",
    "text": "4.6 Microsoft Azure\n\nhttps://azure.microsoft.com/en-us/free/?WT.mc_id=Revolutions-blog-davidsmi\nhttps://visualstudio.microsoft.com/dev-essentials/\n\nstarts azure trial but gives you free sql server developer edition\n\nWon’t be charged until you choose to upgrade.\n12 months access to $ services for free\n$200 credit for any service for 30 days\n\nAt the end of the 30 days, I think the remainder goes into your account after you change to a pay-to-play account\n\nAccess to the services that are always free\n\nAzure Kubernetes Service (AKS)\nFunctions\n\n1,000,000 requests per month\na solution for easily running small pieces of code in the cloud. You can write just the code you need for the problem at hand, without worrying about a whole application or the infrastructure to run it.\nExample use case: for handling WebAPI requests and sending the different data and results to where it needs to go.\n\nApp Service\n\n10 web, mobile, or API apps\n\nActive Directory B2C (identity)\n\n50,000 authentications per month\n\nMachine Learning Server\n\nDevelop and run R and Python models on your platform of choice.\n\nSQL Server 2017 Developer Edition\n\nBuild, test, and demostrate applications in a non-production environment.\n\nOther stuff\n\nBlob storage\n\nobject storage solution for the cloud\noptimized for storing massive amounts of unstructured data\n\nSpot Instances (Low Priority VM)\n\nnot time limit on instance usage\nno warning on termination by Azure\n\nTips\n\nIf you can’t create a service, because Azure servers are under maintenance for more than a couple of minutes — check out your permissions and registrations under the “Resource providers” panel.\nIf you see any strange errors on the Azure Portal — just change the filters’ values.\nIf you use Azure Machine Learning, and your scoring function cannot locate your source code — deliver the code as a Model and add it explicitly to the sys.path in the init function.\nIf you use Azure Machine Learning, don’t use Batch Endpoints — it looks like they are not ready yet — just use the regular Published Pipelines. In fact, “Batch endpoint” is just a wrapper around a published pipeline.\nDon’t include flask in your Azure conda environment specification.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#aws",
    "href": "qmd/cloud-services.html#aws",
    "title": "4  Cloud Services",
    "section": "4.7 AWS",
    "text": "4.7 AWS\n\nInstance types\n\nc-type instances are compute heavy\nr-type instances are RAM heavy\nm-type instances are balanced\n“Each thread is represented as a virtual CPU (vCPU) on the instance. An instance has a default number of CPU cores, which varies according to instance type. For example, an m5.xlarge instance type has two CPU cores and two threads per core by default—four vCPUs in total.”\nspot prices from 03/24/2020, all calculations over the previous month\ngen purpose\n\nm6g.8xlarge\n\ngen purpose, 32 vcpu, 128 gb\nnewer graviton, didn’t see any specs, but supposed to be much better than the xenon 1st gen\n\nm5.8xlarge\n\ngen purpose, 32 vcpu, 128 gb\nolder 3.1 ghz, xenon\non-demand $1.54/hr\n\nm5a.8xlarge\n\ngen purpose 32 vcpu, 128 gb\n2.4 ghz, slower processor speed than m5\n\nm5n.8xlarge\n\ngen purpose 32 vcpu, 128 gb\n3.1 ghz, xenon specialized for neural networks, ML tasks\nn.virg, 71% savings, &lt;5% interruption\nohio, 83% savings, &lt;5% interruption\non-demand $1.90/hr\npotential spot price = $0.32\n\nm5dn.8xlarge\n\nsame but with 2 ssd hard drives\n\nm4.10xlarge\n\ngen purpose 40 vcpu, 160 gb\n2.4 ghz\nsmaller write-up, get the sense these are older processors/instances\n\n\ncompute optimized\n\nRequires HVM AMIs that include drivers for ENA (network adaptor) and NVMe (ssd hard drives)\n\nseems standard on a lot of instances (gen purpose and here), shouldn’ t be an issue\n\nc5.9xlarge\n\n36 vcpu, 72 gb\n3.4 ghz\non-demand $1.53/hr\n\nc5d.9xlarge\n\nsame but with ssd\n\nc5n.9xlarge\n\n36 vcpu, 96 gb\n3.0 ghz, built for task needing high throughput for networking\non-demand, $1.94/hr\n\nc4.8xlarge\n\n36 vcpu, 60 gb\n2.9 ghz\n67% savings, &lt;5% interruption\non-demand $1.59/hr\npotential spot price = $0.52\n\n\nmemory optimized\n\nr5.8xlarge\n\n32 vcpu, 256 gb\n3.1 ghz\nn.virg, 72% savings, 5-10% interruption\nn.cal, 76% savings, &lt;5% interruption\non-demand $2.02/hr\npotential spot price = $0.48\n\nr5a.8xlarge\n\n32 vpu, 256 gb\n2.5 ghz\n\nr5n.8xlarge\n\n32 vcpu, 256 gb\n3.1 ghz, neural network optimized\nus.west. oregon 76% savings, 5-10% interruption\non-demand $2.38/hr\npotential spot price = $0.57\n\nr4.8xlarge\n\n32 vcpu, 244 gb\n2.3 ghz\n\nz1d.6xlarge\n\n24 vcpu, 192 gb\n4.0 ghz\non-demand $2.23\n\n\naccelerated computing\n\ninf1.6xlarge\n\n24 vcpu, 48 gb\nbuilt for ML\non-demand $1.91/hr\n\n\n\nFree Tier (12 months after sign-up)\n\naws.amazon.com – pricing (top) – free tier (mid) – create a free account (mid)\nEC2\n\n750 hrs/mo of t2-micro instance usage\n\nfor Linux, Windows, RHEL, SLES AMIs\n\n\nElastic Block Storage (EBS)\n\n30 GB\ncan be connected to an ec2\n\nElastic Container Registry\n\n500 MB per month\n\nfor storing and retrieving Docker images\nexample in course was a basic nginx image and it was 50MB\n\n\nS3\n\n5 GB of standard storage (high availability/ high durability)\n20,000 Get Requests, 2000 Put Requests per month\n\nElastic Load Balancing\n\n750 hrs per month shared between classic and application load balancers\n\nno idea what the differences are between classic and application\n\n\n\nPricing\n\nPrice per GPU as of 29-06-2023\n\n\nExamples\n\nr3.4xlarge 16 CPUs, 122 GB RAM, 1 x 320 SSD, Spot Price: $0.1517/h\n\nTrained H2O GBM, RF, XGBoost, DeepLearning. Cluster ran for 2 hr 40 min. Total Cost = around $0.42\nhttps://www.daeconomist.com/post/2019-01-15-partii/\n\n\nStorage\n\nS3\n\ncharged by amount stored\n\n$0.023/GB for standard (for first 50 TB)\n0.004/GB for glacier and 0.00099/GB deep glacier\n\ntakes longer to retrieve and not always available\n\n\nfree inbound transfer\nfree transfer between aws services (e.g. S3 to EC2) within the same region\n\nAurora\n\nstorage + inbound/outbound: $0.20 per million requests\n\n\nConsolidated Biling\n\na separate account. All company individual accounts (marketing, sales, etc.) bills are pooled into this account\nhas no access to services\nhas no permissions to access services in other accounts\npooled bill counted towards potential discount billing\n\nCalculators\n\nTotal Cost of Ownership (TCO) calculator\n\ncompares cost of running a project on-premises to aws cloud\n\naws pricing calculator\n\ncalculates price of running a cloud application\ncalculator.aws.com\nestimates cost per service, per service group, and total infrastructure\nhelps find right ec2 instance and region\n\n\nBilling and Cost Management console\n\ncost explorer\n\nview and analysis costs and usage\n\n\n\n\nSpot Instances\n\nSummary\n\nGo to spot advisor and find instances that fit budget and compute requirements\nPrepare strategy for interruption\nOther services\n\nAs of Jan 01, 2019, cloudyr’s aws.ec2 PKG didn’t support all spot instances.\nno time limit on instance usage\nAWS gives a 2 min warning when it decides it needs your spot instance\npricing is market driven depending on capacity levels at the time\nAvailable actions when Amazon “interrupts” your instance:\n\nHibernation:\n\n“like closing your laptop display”\nsaves data and memory and reboots once instance is available again\nRight before interruption, a daemon on the instance freezes the memory and stores it in Elastic Block Store (EBS) root volume\nYour EC2 will retain this root volume and any other EBS data volumes\nOnce market price falls below bid price, instance resumes with memory restored from disk to RAM\nYou aren’t charged while instance is in hibernation, but EBS volumes do cost $.\nAvailable for instance types: C3, C4, M4, R3, and R4 with &lt; 100 GB RAM on Amazon’s Linux, Ubuntu, and Windows\nAll this is done by something called the EC2 Hibernation Agent which sound like its just the name of the program on the servers\n\nStop\n\n“like shutting down your computer to be turned on later”\nlose whatever is in RAM but retain EBS data volumes ($)\nrestores once bid price &lt; market price\n\nTerminate\n\n***default option***\neverything deleted\n\n\nSpot Advisor\n\n**always use this before spinning up spot instances **\nhttps://aws.amazon.com/ec2/spot/instance-advisor\nInput\n\nvCPUs\nMemory size\nPlatform (linux?)\navailability zone (region?)\namount required (number of instances?)\n\noutput\n\ninstance type\nvCPUs\nMemory (GB)\nSavings over On-Demand (%)\nFrequency of termination (%)\n\nliklihood your instance will get terminated\n\n\n\nRunInstance API\n\nFor requesting a spot instance through CLI I think\nLooks like you send something that looks like a python dict with max price, type, region, etc. to this API\n\nSpot Blocks\n\nallows you to set a finite duration that your instance will run for\n\n1 to 6 hrs\nno interruption during that time\n\ntypically 30 to 45% cheaper than on-demand and maybe an additional 5% cheaper during non-peak hours for the region\nrecommended for batch runs\n\nStrategy\n\nUse regions with largest pools of spot instances\n\nLargest pools\n\nus.east.1 (north.virginia)\neu.west.1(ireland)\n\nThese regions have most types/most instances available\nTypically can go uninterrupted for weeks\nless price fluctuation = more certainty\n\n\nSmallest pools\n\neu.central.1 (frankfort)\nap.south.1 (mumbai)\nap.southeast.1 (singapore)\n\ntypically get interrupted within days\n\n\n\nRun groups of instances that come from multiple spot pools\n\nTo used different compute types, jobs/tasks need to be in containers\nspot pools are instances with same region, type, OS, etc.\napplications running on instances from a least 5 different pools can cut interruptions by up to 80%\n\n\nManaging/preparing for interruptions\n\nOnly use for jobs that are short lived\n\ndevelopment and staging environments, short data processing, proof-of-concept, etc.\n\nBuild internal management system that automatically handles interruptions\n\nlook at spot pool historical prices for past 90 days\n\nlooking for least volatile pools\nolder generation (e.g. c-family, m-family) tend to be most stable\n\n\nUse 3rd party platform that manages spot instances and interruptions\n\nSpotinst - uses ML to choose and manage instances that optimizes price and provide continuous activity for apps that are without a single point of failure.\n\nUses on-demand as a fall-back.\nSLA guarantees 99.9% availability.\nSnapshots volumes to migrate data to new instances in case of interruption.\nworks with other services and platforms (kubernetes, codedeploy, etc.)\n\nSpot Fleet - aws service, automanages groups of spot instances according to either of the following strategies:\n\nstrategy options\n\nlowest price - lowest price instances\ndiversified - spread instances across pools\n\nAfter receiving 2 min warning,\n\ntake snapshots of AMI and any attached EBS volumes and use them to launch a new instance.\n\nsnapshot of AMI\n\non EC2 dashboard – left panel – instances – instances\n\nright-click instance – image – create AMI\n\nimage is in left -panel – Images – AMIs\n\n\n\nActually both snapshots might be able to taken in left panel – spot requests\n\nsee AWS note – EC2 for further details\n\n\n\n\n\n\nneed to drain and detach instance from elastic load balancer if one is used\nIf using auto-scaling, need to create an on-demand group and a spot instance group\n\n\nKubernetes\n\nAfter receiving 2 minute interruption warning from AWS:\n\nDetach instance from elastic load balancer (ELB) is one is being used\nMark instance as unschedulable (?)\n\nprevents new pods (group of containers on an instance that performs a job) from being scheduled on that node\nunderlying compute capacity and scheduling of resources of the pods needs to be monitored. Compute capacity and pod resource requirements need to match.\n\n\n\n\n\nComparison\n\nMisc\n\nNotes from\n\nThe Top Clouds Evaluated Such That You Don’t Need to Repeat Our Mistakes\nAWS vs GCP reliability is wildly different\n\nNo services for blockchain development, quantum computing, and graph databases in GCP (May 2022)\nhttps://cloud-gpus.com/ - tool for comparing gpu compute prices across vendors\n\nData centers\n\nCloser the resources are to your business, the less latency\n(May 2022) GCP has caught up and surpassed AWS in the number of data centers and regions that are available\n\nCompute\n\nCheapest vCPU\n\nGCP “e2-micro-preemptible” with 2 vCPU and 1 GB memory.\n\n48% lower than “t4g.nano” from AWS\n5 times lower than “A0” from Azure.\n\nAWS is in-between GCP and Azure in terms of price (i.e. Azure most expensive for cheap vCPUs)\n\nMore performant GCP instances usually cost approximately the same as their analogs from other cloud providers\n\nAzure servers cost the same or slightly less than AWS\n\nGCP: dedicated PostgreSQL server\n\nCheapest instances are 25% lower than the competitors\n\nGPU on-demand availability\n\nConclusion: Assuming you need on-demand boxes to succeed right when you need them, the consensus seems to clearly point to AWS. If you can stand to wait or be redundant to spawn failures, maybe Google’s hardware acceleration customizability can win the day.\nStats\n\nAWS consistently spawned a new GPU in under 15 seconds (average of 11.4s).\nGCP on the other hand took closer to 45 seconds (average of 42.6s).\nAWS encountered one valid launch error in these two weeks whereas GCP had 84\n\nCaveats\n\nGCP allows you to attach a GPU to an arbitrary VM as a hardware accelerator - you can separately configure quantity of the CPUs as needed.\nAWS only provisions defined VMs that have GPUs attached\n\n\n\nRecommendations\n\nAzure\n\nYou use the Microsoft Office stack (Word, Teams, OneDrive, SharePoint, etc.) and/or C# programming language.\nYou head neither for the cheapest servers nor for the most expensive ones — you need something in the middle.\nYou need a memory-optimized solution rather than a general-purpose or a compute-optimized one.\nYou read about the current bugs and inconsistencies in Azure, and it does not scare you.\n\nAWS\n\nYou are rich.\nYou have AWS experts in your team.\nYou build an enterprise-level long-term project.\nOR you just want to rent a cheap virtual machine, and you don’t care about all the other facilities.\n\nGCP\n\nYou are a start-up company.\nYou can’t invest much time in learning AWS and dealing with Azure bugs.\nYou don’t need much flexibility and configuration facilities from the cloud.\nYou are ready to accept the approaches dictated by the platform.\nYou need either a general-purpose or a compute-optimized solution, but not a memory-optimized one.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html",
    "href": "qmd/db-engineering.html",
    "title": "Engineering",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-misc",
    "href": "qmd/db-engineering.html#sec-db-eng-misc",
    "title": "Engineering",
    "section": "",
    "text": "If you’re developing an application, a good rule of thumb is to write your frequently run queries in such a way that they return a response within 500 ms\nColumn storage files (parquet) are more lightweight, as adequate compression can be made for each column. Row storage doesn’t work in that way, since a single row can have multiple data types.\n\n\n(See below) Apache Avro is smaller file size than most row format file types (e.g. csv)\n\n{pins}\n\nConvenient storage method\nUse when:\n\nObject is less than a 1 Gb\n\nUsed {butcher} for large model objects\n\nSome model objects store training data\n\n\n\nBenefits\n\nJust need the pins board name and name of pinned object\n\nThink the set-up is supposed to be easy\n\nEasy to share; don’t need to understand databases",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-terms",
    "href": "qmd/db-engineering.html#sec-db-eng-terms",
    "title": "Engineering",
    "section": "Terms",
    "text": "Terms\n\nACID - A database transaction, by definition, must be atomic, consistent, isolated and durable. These are popularly known as ACID properties.  These properties can ensure the concurrent execution of multiple transactions without conflict. Guarantees data validity despite errors and ensure that data does not become corrupt because of a failure of some sort.\n\nCrucial to business use cases that require a high level of data integrity such as transactions happening in banking.\n\nBatch processing - performing an action on data, such as ingesting it or transforming it, at a given time interval.\nBTEQ - Batch Teradata Query (like SQL) is simply a utility and query tool for Teradata which is a relational database system Creating a BTEQ script to load data from a flat-file.\nConcurrency - multiple computations are happening at the same time\nData Dump - A file or a table containing a significant amount of data to be analysed or transferred. A table containing the “data dump” of all customer addresses.\nData Mart - A subset of a data warehouse, created for a very specific business use case. Finance data mart storing all the relevant financial information required by the Accounting team to process their month-end cycles.\nData Integration - Usually, the hardest part of the project, where multiple sources of data are integrated into a singular application/data warehouse. Integrating finance and customer relationship systems integrating into an MS SQL server database.\nData Lake - A repository for all kinds of structured and unstructured data. Mainly based on Hadoop storage technology. Called a lake as it is flexible enough to store anything from raw data to unstructured email files. Hadoop Data Lake. Storing logs of all customers called into the inbound call centre including call duration.\nData Mesh - Decentralized design where data is owned and managed by teams across the organisation that understands it the most, known as domain-driven ownership. tl;dr - Each department controls they’re own data from ingestion to “data products.” This data product is then made a available to the other departments for them to use in their projects. Each department has their own engineers, scientists, and analysts.\n\nEach business unit or domain aims to infuse product thinking to create quality and reusable data products — a self-contained and accessible data set treated as a product by the data’s producers — which can then published and shared across the mesh to consumers in other domains and business units — called nodes on the mesh.\nEnables teams to work independently with greater autonomy and agility, while still ensuring that data is consistent, reliable and well-governed.\nYou don’t have to figure out who’s in charge of what data, who gets to access it, who needs to protect it and what controls and monitoring is in place to ensure things don’t go wrong.\nExample: Banking\n\nCredit risk domain’s own data engineers can independently create and manage their data pipelines, without relying on a centralised ingestion team far removed from the business and lacking in credit expertise. This credit team will take pride in building and refining high-quality, strategic, and reusable data products that can be shared to different nodes (business domains) across the mesh.\n\n\nData Models - A way of organising the data in a way that it can be understood in a real-world scenario. Taking a huge amount of data and logically grouping it into customer, product and location data.\nData Quality - A discipline of measuring the quality of the data to improve and cleanse it. Checking Customer data for completeness, accuracy and validity.\nData Replication - There are multiple ways to do this, but mainly it is a practice of replicating data to multiple servers to protect an organisation against data loss. Replicating the customer information across two databases, to make sure their core details are not lost.\nDenormalization - database optimization technique in which we add redundant data to one or more tables. Designers use it to tune the performance of systems to support time-critical operations. Done in order to avoid costly joins. Me: Seems like it’s kind of like a View except a View might have calculated columns in it.\nDimensions - A data warehousing term for qualitative information. Name of the customer or their country of residence.\nDistributed SQL -  a single logical database deployed across multiple physical nodes in a single data center or across many data centers if need be; all of which allow it to deliver elastic scale and resilience. Billions of transactions can be handled in a globally distributed database.\nEDW - The same as a data warehouse except it includes all the data within an organisation. This means that the entire enterprise can rely on this warehouse for their business decisions. Organising sales, customer, marketing and finance data in an enterprise data warehouse to be able to create several key management reports.\nEmbedded aka In-Process\n\nEmbedded database as in a database system particularly designed for the “embedded” space (mobile devices and so on.) This means they perform reasonably in tight environments (memory/CPU wise.)\nEmbedded database as in databases that do not need a server, and are embedded in an application (like SQLite.) This means everything is managed by the application.\n\nFacts - A data warehousing term for quantitative information. The number of orders placed by a customer.\nFlat File - Commonly used to transfer data due to their basic nature; flat files are a single table storing data in a plain text format. All customer order numbers stored in a comma-separated value (.csv) file\nHTAP - Hybrid Transactional Analytical Processing - System that attempts be good at both OLAP and OLTP\nMaster Data - This is data that is the best representation of a particular entity in the business. This gives you a 360 view of that data entity by generally consolidating multiple data sources. Best customer data representation from multiple sources of information.\nMulti-Master - allows data to be stored by a group of computers, and updated by any member of the group. All members are responsive to client data queries. The multi-master replication system is responsible for propagating the data modifications made by each member to the rest of the group and resolving any conflicts that might arise between concurrent changes made by different members.\n\nAdvantages\n\nAvailability: If one master fails, other masters continue to update the database.\nDistributed Access: Masters can be located in several physical sites, i.e. distributed across the network.\n\nDisadvantages\n\nConsistency: Most multi-master replication systems are only loosely consistent, i.e. lazy and asynchronous, violating ACID properties. (mysql’s multi-master is acid compliant)\nPerformance: Eager replication systems are complex and increase communication latency.\nIntegrity: Issues such as conflict resolution can become intractable as the number of nodes involved rises and latency increases.\n\nCan be contrasted with primary-replica replication, in which a single member of the group is designated as the “master” for a given piece of data and is the only node allowed to modify that data item. Other members wishing to modify the data item must first contact the master node. Allowing only a single master makes it easier to achieve consistency among the members of the group, but is less flexible than multi-master replication.\n\nNiFi - It is an open-source extract, transform and load tool (refer to ETL), this allows filter, integrating and joining data. Moving postcode data from a .csv file to HDFS using NiFi.\nNormalization - A method of organizing the data in a granular enough format that it can be utilised for different purposes over time. Organizing according to data attributes reduces or eliminates data redundancy (i.e. having the same data in multiple places). Usually, this is done by normalizing the data into different forms such as 1NF (normal form) or 3NF (3rd normal form) which is the most common. (See DB, Relational &gt;&gt; Normalization)\n\nTaking customer order data and creating granular information model; order in one table, item ordered in another table, customer contact in another table, payment of the order in another table. This allows for the data to be re-used for different purposes over time.\n\nNULL indexes - These are the indexes that contain a high ratio of NULL values\nObject-Relational Mapping (ORM) - Allows you to define your data models in Python classes, which are then used to create and interact with the database. See {{SQLAlchemy}}\nODS - Operational data store generally stores limited and current information to help simple queries. Unable to handle historical or complex data queries. An ODS for daily stock fluctuations in a warehouse help the warehouse manager decide what to prioritise in the next order delivery.\nOLAP - Online Analytical Processing - large chunks of tables are read to create summaries of the stored data\n\nUse chunked-columnar data representation\n\nOLTP - Online Transactional Processing - rows in tables are created, updated and removed concurrently\n\ntraditionally use a row-based data representation\npostgres excels at this type of processing\n\nRDBMS - Relational database management system. All of the above examples are RDBMS, meaning they store data in a structured format using rows and columns.\n\nA Microsoft SQL server database.\n\nReal-Time Processing (aka Event Streaming) - each new piece of data that is picked up triggers an event, which is streamed through the data pipeline continuously\nReverse ETL - Instead of ETL where data is transformed before it’s stored or ELT where data is stored and transformed while in storage, Reverse ETL performs transformations in the pipeline between Storage and the Data Product.\n\nSCD Type 1–6 - A method to deal with changes in the data over time in a data warehouse. Type 1 is when history is overwritten whereas Type 2 (most common) is when history is maintained each time a change occurs.\n\nWhen a customer changes their address; SCD Type 1 would overwrite the old address with the new one, whereas Type 2 would store both addresses to maintain history.\n\nSchemas - A term for a collection of database objects. These are generally used to logically separate data within the database and apply access controls.\n\nStoring HR data in HR schema allows logical segregation from other data in the organisation.\n\nSharding - Horizontal Partitioning — divides the data horizontally and usually on different database instances, which reduces performance pressure on a single server.\n\nStaging - The name of a storage area that is temporary in nature; to allow for processing of ETL jobs (refer to ETL). Typically data is loaded from a source database into the staging area database where it is transformed. Once transformed, it’s loaded into the production database where analytics can be performed on it.\n\nA staging area in an ETL routine to allow for data to be cleaned before loading into the final tables.\n\nTransactional Data - This is data that describes an actual event.\n\nOrder placed, a delivery arranged, or a delivery accepted.\n\nUnstructured Data - Data that cannot be nicely organised in a tabular format, like images, PDF files etc.\n\nAn image stored on a data lake cannot be retrieved using common data query languages.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-datqual",
    "href": "qmd/db-engineering.html#sec-db-eng-datqual",
    "title": "Engineering",
    "section": "Data Quality",
    "text": "Data Quality\n\nAlso see Production, Data Validation\nAccuracy - addresses the correctness of data, ensuring it represents real-world situations without errors. For instance, an accurate customer database should contain correct and up-to-date addresses for all customers.\nCompleteness - extent your datasets have all the required information on every record\n\nMonitor: missingness\n\nConsistency - extent that no contradictions in the data received from different sources. Data should be consistent in terms of format, units, and values. For example, a multinational company should report revenue data in a single currency to maintain consistency across its offices in various countries.\nTimeliness - Data should be available at the time it’s required in the system\nValidity - ensuring that data adheres to the established rules, formats, and standards.\n\nMonitor: variable types/classes, numeric variable: ranges, number of decimal places, categorical variable: valid categories, spelling\n\nUniqueness - no replication of the same information twice or more. They appear in two forms; duplicate records and information duplication in multiple places.\n\nMonitor: duplicate rows, duplicate columns in multiple tables",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-costopt",
    "href": "qmd/db-engineering.html#sec-db-eng-costopt",
    "title": "Engineering",
    "section": "Cost Optimization",
    "text": "Cost Optimization\n\nAlso see\n\npage 53 in notebook\nGoogle, BigQuery &gt;&gt; Optimization\n\nAvoid disk operations, make sure that you look out for hints & information in the EXPLAIN PLAN of your query. (e.g. using SORT without an index)\n\nWhen you see filesort, understand that it will try to fit the whole table in the memory in many chunks.\n\nIf the table is too large to fit in memory, it will create a temporary table on disk.\n\nLook out for a using filesort with or without a combination of using temporary.\n\nSplit tables with many columns Might be efficient to split the less-frequently used data into separate tables with a few columns each, and relate them back to the main table by duplicating the numeric ID column from the main table.\n\nEach small table can have a primary key for fast lookups of its data, and you can query just the set of columns that you need using a join operation.\n\nPrimary keys should be global integers.\n\nIntegers consume less memory than strings, and they are faster to compare and hash\n\nJoins\n\nWith correlated keys\n\nThe query planner won’t recognize the correlated keys and do nested loop join when a hash join is more efficient\nI don’t fully understand what correlated keys on a join are, but see SQL &gt;&gt; Terms &gt;&gt; Correlated/Uncorrelated queries\n\nIn the example below, a group of merge_commit_ids will only be from 1 repository id, so the two keys are associated in a sort of traditional statistical sense.\n\nSolutions\n\nUse LEFT_JOIN instead of INNER_JOIN\nUse extended statistics\nCREATE STATISTICS ids_correlation ON repository_id, merge_commit_id FROM pull_requests;\n\n“repository_id” and “merge_commit_id” are the correlated keys\nI’m not sure if “ids_correlation” is a function or just a user-defined name\nPostgreSQL ≥13 will recognize correlation and the query planner will make the correct calculation and perform a hash join\n\n\n\n\nPre-join data before loading it into storage\n\nIf a group of tables is frequently joined and frequently queried, then pre-joining will reduce query costs\ncan be done using an operational transform system such as Spark, Flow, or Flink (dbt can parallelize runs and work w/Spark)\n\nIndexes{#sec-db-eng-costopt-index}\n\nIndexes help in filtering data faster as the data is stored in a predefined order based on some key columns.\n\nIf the query uses those key columns, the index will be used, and the filter will be faster.\n\nSuitable for any combination of columns that are used in filter, group, order, or join\nMySQL Docs\nDon’t use indexes with LIKE\nCluster a table according to an index\n\nAlso see Google, BigQuery &gt;&gt; Optimization &gt;&gt; Partition and Cluster\nRearranges the rows of a table on the disk\nDoesn’t stay “clustered” if table is updated\n\nSee pg_repack for a solution\n\nExample\n-- create index\nCREATE INDEX pull_requests_repository_id ON pull_requests (repository_id, number)\n-- cluster table\nCLUSTER pull_requests USING pull_requests_repository_id\n\n\nUseful for queries such as\nSELECT *\nFROM pull_requests\nWHERE repository_id IN (...) AND number &gt; 1000\nBest Pactices\n\nAvoid too many indexes\n\nA copy of the indexed column + the primary key is created on disk\nIndexes add to the cost of inserts, updates, and deletes because each index must be updated\nBefore creating an index, see if you can repurpose an existing index to cater to an additional query\nCreate the least possible number of indexes to cover most of your queries (i.e. Covering Indexes).\n\nMakes effective use of the index-only scan feature\nAdd INCLUDE to the create index expression\nExample\n-- query\nSELECT y FROM tab WHERE x = 'key';\n-- covering index, x\nCREATE INDEX tab_x_y ON tab(x) INCLUDE (y);\n-- if the index, x, is unique\nCREATE UNIQUE INDEX tab_x_y ON tab(x) INCLUDE (y);\n\ny is called a non-payload column\n\nDon’t add too many non-payload columns to an index. Each one duplicates data from the index’s table and bloat the size of the index.\n\n\nExample: Query with function\n-- query\nSELECT f(x) FROM tab WHERE f(x) &lt; 1;\n-- covering index, x\nCREATE INDEX tab_f_x ON tab (f(x)) INCLUDE (x);\n\nWhere f() can be MEAN, MEDIAN, etc.\n\n\n\nFix unusable indexes\n\nIssues related to data types, collation (i.e. how it’s sorted), character set (how the db encodes characters), etc\nSometimes you can make the indexes work by explicitly forcing the optimizer to use them. (?)\n\nRepurpose or delete stale indexes\n\nIndexes are designed to serve an existing or a future load of queries on the database\nWhen queries change, some indexes originally designed to serve those queries might be completely irrelevant now\nAutomate stale index removal. Dbs keep statistics. Write a script to either notify you or just delete the index if it’s older and not been used past a certain threshold\n\nUse the most cost efficient index type\n\nExample: If your use case only needs a regular expression search, you’re better off having a simple index than a Full Text index.\n\nFull Text indexes occupy much more space and take much more time to update\n\n\nDon’t index huge tables (&gt; 100M rows), partition instead\n\nThen prune the partitions (partition pruning) you don’t need and create indexes for the partitioned tables you do keep.\n\n\nPartitioning\n\nAlso see Google, BigQuery &gt;&gt; Optimization &gt;&gt; Partition and Cluster\nSplits your table into smaller sub-tables under the hood\n\nNot viewable unless you check the table directory to see the multiple files that have been created\n\nThe same goes for indexes on that table.\n\n\nUse on tables with at least 100 million rows (BigQuery recommends &gt; 1 GB) Partitioning helps reduce table size and, in turn, reduces index size, which further speeds up the Data Warehouse (DWH) operations. But, partitioning also introduces complexity in the queries and increases the overhead of managing more data tables, especially backups. So try a few of the other performance techniques before getting to Sharding.\nPartition columns should always be picked based on how you expect to use the data, and not depending on which column would evenly split the data based on size.\n\nExample: partition on county because your analysis or transformations will largely be done by county even though since some counties may be much larger than others and will cause the partitions to be substantially imbalanced.\n\n\nUse ELT (e.g. load data from on-prem server to cloud, then transform) instead of ETL (transform data while on-prem, then load to cloud) for data pipelines\n\nMost of the time you have a lot of joins involved in the transformation step\n\nSQL joins are one of the most resource-intensive commands to run. Joins increase the query’s runtime exponentially as the number of joins increases.\nExample\n\nRunning 100+ pipelines with some pipelines having over 20 joins in a single query.\nEverything facilitated by airflow (see bkmk for code)\nETL: postgres on-prem server, sql queries with joins, tasks ran 12+ hours, then the transformed data is loaded to google storage\n\n13+ hrs for full pipeline completion\n\nELT: running the queries with the joins, etc. with bigquery sql on the data after it’s been loaded into google storage.\n\n6+ hrs for full pipeline completion\n\n\n\n\nUse Materialized Views\n\nA smaller data object that contains the subset of data resulting from a specific query\nWhereas a query happens after data is loaded, a materialized view is a precomputation\nThe computation is done once, and changes to the data are incorporated as they occur, making subsequent updates to the view much cheaper and more efficient than querying the entire database from scratch\n\nFetching a large table will be slower if you try to use multiple cores.\n\nYou have to divide up the table and recombine it. Plus setting up parallel network processes takes time.\nThe time used to fetch some data from the internet depends massively on the internet bandwidth available on your router/network.\n\nUse Random Access via http range header + sparse-hilbert index to optimize db for query searches\nCITEXT extension makes it so you don’t have use lower or upper which are huge hits on performance (at least they are in WHERE expressions) GIN custom indexes for LIKE and ILIKE\nCREATE EXTENSION IF NOT EXISTS btree_gin;\nCREATE EXTENSION IF NOT EXISTS pg_trgm;\nCREATE INDEX index_users_on_email_gin ON users USING gin (email gin_trgm_ops);\n\nCREATE EXTENSION adds btree and pg_trgm extensions\nindex_users_on_email_gin is the name of the index\nusers is the table\nUSING gin (email gin_trgm_ops)\n\ngin specifies that it’s a gin index\nemail is the field\ngin_trgm_ops is from the pg_trgm extension. It splits the index into trigrams which is necessary for the gin index to work with LIKE or ILIKE\n\nSlower to update than the standard ones. So you should avoid adding them to a frequently updated table.\n\nGiST indexes are very good for dynamic data and fast if the number of unique words (lexemes) is under 100,000, while GIN indexes will handle 100,000+ lexemes better but are slower to update.\n\n\nNULLS LASTputs the NULLS in a field in any sorting operations at the end\n\nThe default behavior of ORDER BY will put the NULLS first, so if you use LIMIT , you might get back a bunch of NULLS.\nUsing NULLS LAST fixes this behavior but its slow even on an indexed column\n\nExample: ORDER BY email DESC NULLS LAST LIMIT 10\n\nInstead use two queries\nSELECT *\nFROM users\nORDER BY email DESC\nWHERE email IS NOT NULL LIMIT 10;\n\nSELECT *\nFROM users\nWHERE email IS NULL LIMIT 10;\n\nThe first one would fetch the sorted non-null values. If the result does not satisfy the LIMIT, another query fetches remaining rows with NULL values.\n\n\nRebuild Null Indexes\nDROP INDEX CONCURRENTLY users_reset_token_ix;\nCREATE INDEX CONCURRENTLY users_reset_token_ix ON users(reset_token)\nWHERE reset_token IS NOT NULL;\n\nDrops and rebuilds an index to only include NOT NULL rows\nusers_reset_token_ix is the name of the index\nusers is the table\nI assume “reset_token has to be the field\n\nWrap multiple db update queries into a single transaction\n\nImproves the write performance unless the database update is VERY large.\nA large-scale update performed by a background worker process could potentially timeout web server processes and cause a user-facing app outage\nFor large db updates, add batching\n\nExample: db update has a 100K rows, so update 10K at a time.\nUPDATE messages SET status = 'archived'\n  WHERE id IN\n  (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 0);\nUPDATE messages SET status = 'archived'\n  WHERE id IN\n  (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 10000);\nUPDATE messages SET status = 'archived'\n  WHERE id IN\n  (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 20000);\n\nmessages is the table name\nI guess OFFSET is what’s key here.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-ets",
    "href": "qmd/db-engineering.html#sec-db-eng-ets",
    "title": "Engineering",
    "section": "Event Tracking Systems",
    "text": "Event Tracking Systems\n\nEvents are queued, then batch inserted into your db.\n\nStreaming events does not scale very well and is not fault tolerant.\n\nCommercial Services\n\nSegment\n\nMost popular option\nVery expensive\nSusceptible to ad blockers\nOnly syncs data once per hour or two\nMissing a few key fields in the schema it generates (specifically, session and page ids).\n\nFreshpaint is a newer commercial alternative that aims to solve some of these issues.\n\nOpen Source (each with a managed offering if you don’t feel like hosting it yourself)\n\nSnowplow is the oldest and most popular, but it can take a while to setup and configure.\nRudderstack is a full-featured Segment alternative.\nJitsu is a pared down event tracking library that is laser focused on just getting events into your warehouse as quickly as possible.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-stream",
    "href": "qmd/db-engineering.html#sec-db-eng-stream",
    "title": "Engineering",
    "section": "Streaming",
    "text": "Streaming\n\nStreaming or near real-time (i.e. micro-batch) data\nQuestions\n\nWhat would be the data flow rate in that pipeline?\nDo you require real-time analytics, or is near-real-time sufficient? \n\nData Characteristics\n\nIt is ingested near-real-time.\nUsed for real-time reporting and/or calculating near-real-time aggregates. Aggregation queries on it are temporal in nature so any aggregations defined on the data will be changed over time as the data comes.\nIt is append-only data but can have high ingestion rates so needs support for fast writes.\nHistorical trends can be analyzed to forecast future metrics.\n\nRelational databases can’t handle high ingestion rates and near-real-time aggregates without extensions.\nSteaming is the most expensive way to process the data in the majority of cases. Typically batch ingesting into warehouses is free, but streaming may not be.\nUse Cases: anomaly detection and fraud prevention, real-time personalized marketing and internet of things.\nTools:\n\nApache Kafka - Flexible, connects to app servers, other microservices, databases, sensor networks, financial networks, etc. and can feed the data to same types of systems including analytical tools.\n\nUtilizes a publish-subscribe model where producers (i.e. sources) publish data to topics and consumers (e.g. DBs, BI tools, Processing tools) subscribe to specific topics to receive relevant data.\nHighly scalable due to its distributed architecture, allowing data handling across multiple nodes.\nConfluent’s Kafka Connect - Open source and Commerical Connectors\n\nApache Flume - Similar to Kafka but easier to manage, more lightweight, and built to output to storage (but not as flexible as Kafka)\n\nLess scalable as data ingestion is handled by individual agents, limiting horizontal scaling.\nIts lightweight agents and simple configuration make it ideal for log collection\nCan also handle Batch workloads\nAble to perform basic preprocessing, e.g. filtering specific log types or converting timestamps to a standard format\n\nAmazon Kinesis - A managed, commercial alternative to Kafka. Charges based on data throughput and storage. Additional features include data firehose for delivery to data stores and Kinesis analytics for real-time analysis.\nApache Flink - Processes streaming data with lower latency than Spark Streaming, especially at high throughputs. Less likely to duplicate data. Uses SQL. Steeper learning curve given its more advanced features.\nApache Spark Streaming - See Apache, Spark &gt;&gt; Streaming\nGoogle Pub/Sub - Uses Apache Beam programming API to construct processing pipelines\n\nGoogle Dataflow can create processing pipelines using streaming data from Pub/Sub. Developers write their pipelines using Beam’s API, and then Beam translates them into specific instructions for Flink or Spark to execute.\n\n{{temporian}} can interact with Beam to perform various time series preprocessing\n\nIf you have existing workflows around Hadoop or Spark or expertise in those frameworks, then Google Dataproc allows you to reuse that code. It also allows you to used other libraries that aren’t available in Dataflow. Supports various languages like Java, Python, and Scala.\nFor short-lived batch jobs, Dataproc might be more cost-effective. Although, Dataflow’s serverless nature avoids idle resource charges while Dataproc clusters incur costs even when idle.\n\n\nArchitectures\n\nNotes from\n\nData Pipeline Design Patterns\n\nETL\n\n\nKinesis collects data from a server (e.g. app) and continuously feeds it to a lambda function for transformation. Transformed data is deposited into a S3 bucket, queried using Athena, and visualized using Quicksight.\n\nHybrid (Streaming and Batch)\n\n\nKinesis streams data to S3 and when a threshold is reached, a lambda trigger activates a transformation/batch load to the BQ warehouse\n\n\nTimeScale DB\n\nOpen source extension for postgresql\nSupport all things postgresql like relational queries, full SQL support(not SQL-like) as well as the support of real-time queries\nSupports an ingestion of 1.5M+ metrics per second per server\nNear-real-time aggregation of tables\nProvides integration with Kafka, kinesis, etc for data ingestion.\nCan be integrated with any real-time visualization tool such as Graphana\n\nPipeline DB\n\nOpen source extension for postgresql\nSimilar features as TimeScale DB\nEfficiency comes from it not storing raw data\n\nUsually, it’s recommended to store raw data",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-otools",
    "href": "qmd/db-engineering.html#sec-db-eng-otools",
    "title": "Engineering",
    "section": "Other Tools",
    "text": "Other Tools\n\nDataFold monitors your warehouse and alerts you if there are any anomalies (e.g. if checkout conversion rate drops suddenly right after a deploy).\nHightouch lets you sync data from your warehouse to your marketing and sales platforms.\nWhale is an open source tool to document and catalog your data. \nRetool lets you integrate warehouse data into your internal admin tools.\nGrowth Book that plugs into your data warehouse and handles all of the complicated querying and statistics required for robust A/B test analysis.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-relational.html",
    "href": "qmd/db-relational.html",
    "title": "Relational",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Relational"
    ]
  },
  {
    "objectID": "qmd/db-relational.html#sec-db-rel-misc",
    "href": "qmd/db-relational.html#sec-db-rel-misc",
    "title": "Relational",
    "section": "",
    "text": "Packages\n\n{dplyr}\n\ncompute stores results in a remote temporary table\ncollect retrieves data into a local tibble.\ncollapse doesn’t force computation, but instead forces generation of the SQL query.\n\nsometimes needed to work around bugs in dplyr’s SQL generation.\n\n\n{dm}\n\nCan join multiple tables from a db, but keeps the meta info such as table names, primary and foreign keys, size of original tables etc.\n\n\nRelational databases do not keep all data together but split it into multiple smaller tables. That separation into sub-tables has several advantages:\n\nAll information is stored only once, avoiding repetition and conserving memory\nAll information is updated only once and in one place, improving consistency and avoiding errors that may result from updating the same value in multiple locations\nAll information is organized by topic and segmented into smaller tables that are easier to handle\n\nOptimized for a mix of read and write queries that insert/select a small number of rows at a time and can handle up to 1TB of data reasonably well.\nThe main difference between a “relational database” and a “data warehouse” is that the former is created and optimized to “record” data, whilst the latter is created and built to “react to analytics”.\nTypes\n\nEmbedded aka In-Process (see Databases, Engineering &gt;&gt; Terms): DuckDB (analytics) and SQLite (transactional)\nServer-based: postgres, mysql, SQL Server\n\nMix of transactional and analytical\nDistributed SQL (database replicants across regions or hybrid (on-prem + cloud)\n\nmysql, postgres available for both in AWS Aurora (See below)\npostgres available using yugabytedb\nSQL Server on Azure SQL Database\nCloud Spanner on GCP\n\n\n\nApache Avro\n\nRow storage file format unlike parquet\nA single Avro file contains a JSON-like schema for data types and the data itself in binary format\n4x slower reading than csv but 1.5x faster writing than csv\n1.7x smaller file size than csv\n\nWrapper for db connections (e.g. con_depA &lt;- connect_databaseA(username = ..., password = ...) )\n# ... other stuff including code for \"connect_odbc\" function\n\n# connection attempt loop\nwhile(try &lt; retries) {\n    con &lt;- connect_odbc(source_db = \"&lt;database name&gt;\"\n                        username = username,\n                        password = password)\n    if(class(con) == \"NetexxaSQL\") {\n        try &lt;- retries + 1\n    } else if (!\"NetezzaSQL\" %in% class(con) & try &lt; retries {\n        warning(\"&lt;database name&gt; connection failed. Retrying...\")\n        try &lt;- try + 1\n        Sys.sleep(retry_wait)\n    } else {\n        try &lt;- try + 1\n        warning(\"&lt;database name&gt; connection failed\")\n    }\n}\n\nGuessing “NetezzaSQL” is some kind of error code for a failed connection to the db\n\nBenchmarks\n\nExample\n\nData\n\n~54,000,000 rows and 6 columns\n10 .rds files with gz compression is 220MB total,\n\nIf they were .csv, 1.5 GB\n\nSQLite file is 3 GB\nDuckDB file is 2.5 GB\nArrow creates a structure of directories, 477 MB total\n\nOperation: read, filter, group_by, summarize\nResults\n##  format          median_time mem_alloc\n##  &lt;chr&gt;              &lt;bch:tm&gt; &lt;bch:byt&gt;\n## 1 R (RDS)              1.34m    4.08GB\n## 2 SQL (SQLite)          5.48s    6.17MB\n## 3 SQL (DuckDB)          1.76s  104.66KB\n## 4 Arrow (Parquet)      1.36s  453.89MB\n\nTradional relational db solutions balloon up the file size\n\nSQLite 2x, DuckDB 1.66x (using csv size)",
    "crumbs": [
      "Databases",
      "Relational"
    ]
  },
  {
    "objectID": "qmd/db-relational.html#sec-db-rel-brands",
    "href": "qmd/db-relational.html#sec-db-rel-brands",
    "title": "Relational",
    "section": "Brands",
    "text": "Brands\n\nSQLite vs MySQL as transactional dbs (article)\n\nSQLite:\n\nEmbedded, size ~600KB\nLimited data types\nBeing self-contained, other clients on a network would not have access to the database (no multi-users) unlike with MySQL\nNo built-in authentication that is supported\nMultiple processes are able to access the database at the same time, but making changes at the same time is not something supported\nUse Cases\n\nData being confined in the files of the device is not a problem\nNetwork access to the db is not needed\nApplications that will minimally access the database and not require heavy calculations\n\n\nMySQL:\n\nopposites of the sqlite stuff\nSize ~600MB\nsupports replication and scalability\nSecurity is a large; built-in features to keep unwanted people from easily accessing data\nUse cases\n\ntransactions are more frequent like on web or desktop applications\nif network capabilities are a must\nmulti-user access and therefore security and authentication\nlarge amounts of data\n\n\n\nMySQL\n\nInstallation docs\nBasic intro\nSee SQL notebook\n\nSQLite\n\n{RSQLite}\n\nCloud SQL - Google service to provide hosting services for relational dbs (see Google, BigQuery &gt;&gt; Misc). Can use postgres, mysql, etc. on their machines.\n\nCloud SQL Insights - good query optimization tool\n\nAWS RDS for db instances (see Database, postgres &gt;&gt; AWS RDS)\n\nAvailable: Amazon Aurora, MySQL, MariaDB, postgres, Oracle, Microsoft SQL Server\nRDS (Relational Database Service)\n\nBenefits over hosting db on EC2: AWS handles scaling, availability, backups, and software and operating system updates\n\n\nAWS Aurora - MySQL- and PostgreSQL-compatible enterprise-class database\n\nStarting at &lt;$1/day.\nSupports up to 64TB of auto-scaling storage capacity, 6-way replication across three availability zones, and 15 low-latency read replicas.\nCreate MySQL and Postgres instances using AWS Cloudformation",
    "crumbs": [
      "Databases",
      "Relational"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html",
    "href": "qmd/db-warehouses.html",
    "title": "Warehouses",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-misc",
    "href": "qmd/db-warehouses.html#sec-db-ware-misc",
    "title": "Warehouses",
    "section": "",
    "text": "Resources\n\nEasy way to create Live and Staging environments for your Data\n\nTutorial on setting up Staging and Production environments for a BQ warehouse and a Github repo where the Dataform code lives. Pull Requests move data from the staging area to the production db.\n\n\nThe main difference between a “relational database” and a “data warehouse” is that the former is created and optimized to “record” data, whilst the latter is created and built to “react to analytics.”\nOptimized for read-heavy workloads that scan a small number of columns across a very large number of rows and can easily scale to petabytes of data\nCons\n\nCan become expensive when an organization needs to scale them\nDo not perform well when handling unstructured or complex data formats.\n\nPros\n\nIntegrating multiple data sources in a single database for single queries\nMaintaining data history, improving data quality, and keeping data consistency\nProviding a central view for multiple source system across the enterprise\nRestructuring data for fast performance on complex queries\n\nSet up alerts and monitoring\n\nETL Monitoring: See if anything failed to load. It can be a failed file with the wrong format or a streaming data record that doesn’t comply with table schema.\nData Quality Checks: Any missing datasets from external providers data malformed, i.e. NULL, formats, etc.\nCost Monitoring: Identify any long-running queries and costs\nUsage Monitoring: Alert if when there’s activity on your account.",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#olap-vs-oltp",
    "href": "qmd/db-warehouses.html#olap-vs-oltp",
    "title": "Warehouses",
    "section": "OLAP vs OLTP",
    "text": "OLAP vs OLTP\n\n\nOLAP (Online Analytical Processing)(aka the Cube)(Data Warehouses)\n\ndb designed to optimize performance in analysis-intensive applications\nAggregates transactions to be less frequent but more complex\nExamples: Snowflake, Bigquery\n\nOLTP (Online Transaction Processing) db designed for frequent, small transactions\n\nExecutes a number of transactions occurring concurrently (i.e. at the same time)\nUse cases: online banking, shopping, order entry, or sending text messages\n\nData model: OLTP systems typically use a normalized data model, which means that data is stored in multiple tables and relationships are defined between the tables. This allows for efficient data manipulation and ensures data integrity. OLAP systems, on the other hand, often use a denormalized data model, where data is stored in a single table or a small number of tables. This allows for faster querying, but can make data manipulation more difficult.\nData volume: OLTP systems typically deal with smaller amounts of data, while OLAP systems are designed to handle large volumes of data.\nQuery complexity: OLTP systems are designed to handle simple, short queries that involve a small number of records. OLAP systems, on the other hand, are optimized for more complex queries that may involve aggregating and analyzing large amounts of data.\nData updates: OLTP systems are designed to support frequent data updates and insertions, while OLAP systems are optimized for read-only access to data.\nConcurrency: OLTP systems are designed to support high levels of concurrency and handle a large number of transactions simultaneously. OLAP systems, on the other hand, are optimized for batch processing and may not perform as well with high levels of concurrency.",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-brands",
    "href": "qmd/db-warehouses.html#sec-db-ware-brands",
    "title": "Warehouses",
    "section": "Brands",
    "text": "Brands\n\nAmazon Redshift\n\nRedshift is best when you have data engineers who want control over infrastructure costs and tuning.\n\nGoogle BigQuery\n\nBest when you have very spiky workloads (i.e. not steady-state).\nNot charged any computational expense for loading the data into its storage.\nProactive Storage Cost Optimization.\n\nIf there is data in BigQuery that hasn’t been altered in over 90 days, they move it into a long-term storage tier that is half the price of regular storage.\n\nCaches query results for 24 hours after running a query.\n\nBigQuery stores the results of a query as a temp table, and if the underlying data has not changed, you will not be charged for running the same query twice in that timeframe.\n\n\nSnowflake\n\nA cloud data warehouse for analytics. It’s columnar, which means that data is stored (under the hood) in entire columns instead of rows; this makes large analytical queries faster, so it’s a common choice for how to build analytical DBs.\nBest when you have a more continuous usage pattern\nSupport for semi-structured data, data sharing, and data lake integration\nResource: Snowflake Data Warehouse Tutorials\nSnowpark - Allows you to run Python code inside the database. Brings code to the warehouse instead of importing the data to your code. Has a DataFrame API so you can run cleaning code that is more readable when coded with python than SQL or ML models on the data inside of the warehouse.\nList all schemas\nselect\n    \"database_name\" as DATABASE_NAME\n    ,\"name\" as SCHEMA_NAME\nfrom table(result_scan(last_query_id()))\nwhere SCHEMA_NAME not in ('INFORMATION_SCHEMA') -- optional filter(s)\n;\n\nAzure Synapse Analytics\n\nFully managed, cloud-based data warehousing service offered by Microsoft Azure. It offers integration with Azure Machine Learning and support for real-time analytics.\n\nData Bricks\n\nCompany behind spark technology and have built a cloud-based data warehousing service.\n\nTeradata\nSAP HANA\nClickHouse\n\nOpensource, built by Yandex (Russian search engine)\n\nApache Hadoop running Apache Hive\n\nHive: an open-source data warehouse solution for Hadoop infrastructure. It is used to process structured data of large datasets and provides a way to run HiveQL queries.\n\nResource: Apache Hive Tutorial with Examples",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-strat",
    "href": "qmd/db-warehouses.html#sec-db-ware-strat",
    "title": "Warehouses",
    "section": "Strategies",
    "text": "Strategies\n\nInmon\n\n\nPrioritizes accuracy and consistency of data above all else.\nQuerying is pretty fast (data marts)\nTends to be a lot of upfront work, however subsequent modifications and additions are quite efficient.\nRecommended if:\n\nData accuracy is the most important characteristic of your warehouse\nYou have time/resources to do a lot of upfront work\n\n\nKimball\n\n\nLess structured approach, which speeds up the initial development cycle.\nFuture iterations require the same amount of work, which can be costly if you’re constantly updating the warehouse Fast querying but very few quality checks\nRecommended if:\n\nIf you’re business requirements are well-defined and stable\nYou are querying lots of data often\n\n\nData Vault\n\n\nTrys to fix disadvantages of Kimball and Inmon strategies by waiting to the last minute to develop any kind of structure\nWorkflow: Sources –&gt; unstructured storage (data lake) –&gt; Staging which supports operations such as batch and streaming processes –&gt; data vault which stores all raw data virtually untouched (non-relational db?)\nAdvantages: efficient, fast to implement, and highly dynamic\nDisadvantages: querying can be quite slow\n\nUh doesn’t seem to be much cleaning either\n\nRecommended if:\n\nYour business goals change often\nYou need cheap server and storage costs",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-dsgn",
    "href": "qmd/db-warehouses.html#sec-db-ware-dsgn",
    "title": "Warehouses",
    "section": "Design",
    "text": "Design\n\nMisc\n\nNotes from\n\nData Warehouse Design Patterns\n\nBest practice to keep only a portion of data in the RAW database and use it to update our “BASE” or “PROD” database tables.\n\nDatabases Inside the Warehouse:\n\nRaw or Source: Raw data is inported into this db; source of truth\nBase or Prod: Where data is imported from Source db and has had basic field transformations; storage\nAnalytics: Where data is ready to be queried; ad-hoc analytics and materialized queries and views\n\nEnvironments\n\nEach db will have development and production branches\n\nDevelopment: staging, mocking and developing data transformations\nProduction: Data is validated and transformations applied on a schedule\n\n\nExample: Snowflake Warehouse",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-trig",
    "href": "qmd/db-warehouses.html#sec-db-ware-trig",
    "title": "Warehouses",
    "section": "Database Triggers",
    "text": "Database Triggers\n\n\nA database trigger is a function that gets triggered every time a record is created or updated (or even deleted) in the source table (in this case, a transactional table)\nDatabase triggers provide an effective, solution to extracting data from the transactional system and seamlessly integrating it into the data warehouse while also not adversely impacting that system.\nUse case — You see a couple of data points in your transactional system’s tables that you would require for your reporting metrics but these data points are not being provided by your transactional system’s API endpoints. So, there is no way you can write a script in Python or Java to grab these data points using the API. You cannot use direct querying on your transactional system as it can negatively impact its performance.\nMisc\n\nNotes from Harnessing Triggers in the Absence of API Endpoints\n\nProvides a detailed step-by-step\n\nIf your transactional system does not have a lot of traffic (or) is not directly used by end-user applications, then it can be set up as a synchronous process. In that case, the lambda or the Azure functions would need to have the trigger event as the transactional database’s staging table. The appropriate database connection information would also need to be provided.\n\nDatabase Triggers\n\nDDL Triggers - Set up whenever you want to get notified of structural changes in your database\n\nUseful when you wish to get alerted every time a new schema is defined; or when a new table is created or dropped. Hence, the name DDL (Data Definition Language) triggers.\n\nDML Triggers - Fired when new records are inserted, deleted, or updated\n\ni.e. You’re notified anytime a data manipulation change happens in a system.\n\n\nSyntax: &lt;Timing&gt; &lt;Event&gt;\n\nTrigger Event - The action that should activate the trigger.\nTrigger Timing - Whether you need the trigger to perform an activity before the event occurs or after the event occurs.\n\nSpecialized triggers provided by cloud services\n\nAWS\n\nLambda Triggers: These triggers help initiate a lambda function when a specified event happens. Events can be internal to AWS, or external in nature. Internal events can be related to AWS services such as Amazon S3, Amazon DynamoDB streams, or Amazon Kinesis. External events can come in from the database trigger of a transactional system outside of AWS or an IoT event.\nCloudwatch Events: If you have used standalone relational databases such as Microsoft SQL Server and SQL Server Management Studio (SSMS), you may have used SQL Server Agent to notify users of a job failure. Cloudwatch is specific to AWS and is used not only to notify users of a job failure but also to trigger Lambda functions and to respond to events. The important difference between a CloudWatch Event and a Lambda Trigger is that while Lambda triggers refer to the capability of AWS Lambda to respond to events, CloudWatch Events is a broader event management service that can handle events from sources beyond Lambda. On a side note, while SQL Server Agent requires an email server to be configured, Cloudwatch has no such requirement.\n\nAzure\n\nBlob Trigger: Azure blobs are similar to S3 buckets offered by AWS. Similar to how Amazon S3 notifications can be used to get alerts about changes in S3 buckets; blob triggers can be used to get notified of changes in Azure blob containers.\nAzure Function Trigger: These are the Azure equivalent of AWS Lambda Function Triggers. These triggers can be used to initiate an Azure function in response to an event within Azure or an external event, such as an external transactional database trigger, an HTTP request, or an IoT event hub stream. Azure functions can also be initiated based on a pre-defined schedule using a Timer Trigger.\n\n\nExample: Transfer data from a transactional database to a warehouse (See article for further details)\n\nIdentify table in transactional db with data you want\nCreate a staging table that’s exactly like the transaction table\n\nEnsure that you don’t have any additional constraints copied over from the source transactional table. This is to ensure as minimal impact as possible on the transactional system.\nFor a bulk data transfer of historical transaction data:\n\nCREATE TABLE AS SELECT (SELECT * INTO in SQL Server) while creating the staging table. This will create the staging table pre-populated with all the data currently available in the transaction table.\nDo an empty UPDATE on all the records in the transaction table\n\ne.g. UPDATE TABLE Pricing_info SET OperationDate=OperationDate\nThis is not a recommended approach as it could bog down the transactional system due to the number of updates and undo statements generated. Moreover, the transaction table will also be locked during the entire update operation and will be unavailable for other processes thus impacting the transactional system. This method is okay to use if your transaction table is extremely small in size.\n\n\nIn addition to that, also have a column to indicate the operation performed such as Insert, Update, Delete).\n\nSet up a DML trigger directly on the transaction table\n\nAll DML events namely Insert, Delete, and Update in the transaction table should have a separate trigger assigned to them.\n\nThe below example shows the trigger for Insert. The rest of the triggers are created similarily — just by substituting 2 INSERTs (trigger event, select statement) for DELETE or UPDATE (See article for code) and using a different name in CREATE\n\nInsert trigger in (SQL Server)\n-- Create the trigger\nCREATE TRIGGER TransactionTrigger_pricing_Insert\nON Pricing_info\n--Trigger Event\nAFTER INSERT\nAS\nBEGIN\n    -- Insert new records into the staging table\n    INSERT INTO StagingTable_pricing (ID, Column1, Column2, OperationType)\n    SELECT ID, Column1, Column2, 'INSERT'\n    FROM inserted\nEND;\n\n“Pricing_info” is the name of transactional table with the data you want\n“StagingTable_pricing” is the name of the staging table\nAFTER INSERT where AFTER is the trigger timing and INSERT is the trigger event\nIn the SELECT statement, “INSERT” is the value for that extra column in the staging table that tells us which type of operation this was.\n\n\nSet-up the specialized trigger in the warehouse\n\nAWS \n\nA database DML trigger in the transactional system’s database. Whenever a new record comes into the transactional database table, the trigger would insert the new data into a staging table within the transactional database.\n\nIf you based it on a schedule (using AWS Cloudwatch events), the Lambda trigger would trigger a lambda function to grab the data from the staging table to a table in the datawarehouse (Redshift)\n\n\nAzure \n\nWhen the timer trigger activates, it would run the Azure Function which would then pick up the new/updated/deleted records from the staging table.",
    "crumbs": [
      "Databases",
      "Warehouses"
    ]
  },
  {
    "objectID": "qmd/misc.html",
    "href": "qmd/misc.html",
    "title": "Misc",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-misc",
    "href": "qmd/misc.html#sec-misc-misc",
    "title": "Misc",
    "section": "",
    "text": "Windows\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + Tab\nChange application\n\n\nCtrl + ~\nChange window within an application\n\n\n\nBrowser\n\n\n\n\n\n\n\nAction\nShortcut\n\n\n\n\nTo Address Bar\nCtrl + L\n\n\nOpen a new window\nCtrl + n\n\n\nOpen a new window in Incognito mode\nCtrl + Shift + n\n\n\nOpen a new tab, and jump to it\nCtrl + t\n\n\nReopen previously closed tabs in the order they were closed\nCtrl + Shift + t\n\n\nJump to the next open tab\nCtrl + Tab or Ctrl + PgDn\n\n\nJump to the previous open tab\nCtrl + Shift + Tab or Ctrl + PgUp\n\n\nJump to a specific tab\nCtrl + 1 through Ctrl + 8\n\n\nJump to the rightmost tab\nCtrl + 9\n\n\nOpen your home page in the current tab\nAlt + Home\n\n\nOpen the previous page from your browsing history in the current tab\nAlt + Left arrow\n\n\nOpen the next page from your browsing history in the current tab\nAlt + Right arrow\n\n\nClose the current tab\nCtrl + w or Ctrl + F4\n\n\nClose the current window\nCtrl + Shift + w or Alt + F4\n\n\nMinimize the current window\nAlt + Space then n\n\n\nMaximize the current window\nAlt + Space then x\n\n\nQuit Google Chrome\nAlt + f then x\n\n\nMove tabs right or left\nCtrl + Shift + PgUp or Ctrl + Shift + PgDn\n\n\n\nR-devel (&gt;= 4.4.0) gained a command-line option to adjust the limit connections (previous limit was 128 parallel workers)\n$ R\n&gt; parallelly::availableConnections()\n[1] 128\n\n$ R --max-connections=512\n&gt; parallelly::availableConnections()\n[1] 512",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-rstud",
    "href": "qmd/misc.html#sec-misc-rstud",
    "title": "Misc",
    "section": "RStudio",
    "text": "RStudio\n\nJob: Run script in the background\nlibrary(rstudioapi)\njobRunScript(\"wfsets_desperation_tune.R\", name = \"tune\", exportEnv = \"R_GlobalEnv\")\n\nNeed to look up args\nI think exportEnv takes the variables in your current environment and runs the script with them as inputs\n\nShortcuts\n\n\n\n\n\n\n\nShortcut\nDescription\n\n\n\n\nAlt + Shift + k\nKeyboard Shortcuts\n\n\nCtrl + Shift + p\nCommand Palette\n\n\nCtrl + Shift + f\nFind in Files\n\n\nCtrl + Alt + up/down\nMultiple Cursors\n\n\nCtrl + Shift + z\nReverse Undo\n\n\nCtrl + Shift + a\nFormat highlighted code (style/linter the code)\n\n\nCtrl + d\nDelete current line\n\n\nAlt + up/down\nYank line up or down\n\n\nCtrl + Alt + up/down\nCopy the above line (or selected lines) down or up\n\n\nCtrl + .\nGo to file/function name\n\n\nAlt + Shift + m\nFocus on Terminal\n\n\n\n\nCustomizing Shortcuts in RStudio\n{shrtcts} - Make anything a shortcut in RStudio",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-hack",
    "href": "qmd/misc.html#sec-misc-hack",
    "title": "Misc",
    "section": "Hackathon Criteria",
    "text": "Hackathon Criteria",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-update",
    "href": "qmd/misc.html#sec-misc-update",
    "title": "Misc",
    "section": "Update R",
    "text": "Update R\n\nMisc\n\n{rig} - r version management system\nupdate.packages(checkBuilt = TRUE, ask = FALSE) is supposed to search for packages in other R versions and update them in the new R version, but I haven’t tried it, yet.\nErrors when compiling from source may require installing libraries and they’ll supply code to install via “pacman”\n\nOpen Start &gt;&gt; scroll down to RTools40 &gt;&gt; RTools Bash\nPaste pacman code and hit enter to install\n\nProblem packages in the past\n\n{brms} dependency, {igraph}, didn’t have a binary on CRAN and wouldn’t compile from source even with correct libraries installed.\n\nSol’n: install.packages(\"igraph\", repos = 'https://igraph.r-universe.dev')\n\ninstalls dev version from r-universe\n\n\nSome {easystats} packages had gave {pak} some problems. No difficulties using install.packages with default repo or if they had a r-universe repo though.\n\n\nSteps\n\nCopy user installed packages in current R version\n\nIn R:\nsquirrel &lt;- names(installed.packages(priority = \"NA\")[, 1]) # user installed packages\nreadr::write_rds(squirrel, \"packages.rds\")\n\nThen, close RStudio\n\n\nRTools: Check to see if you have the latest because you’ll need it to compile some of newest versions of packages.\n\nYour rtools folder has the version in it’s folder name.\nrtools website has the latest version and an .exe to download\n\nCheck/Update rig version\n\nIn powershell: rig --version\nCheck current rig release: link\nDownload and install if your version isn’t current\n\nInstall new version of R\n\nClose R if not already closed\nrig add release installs the latest version of R.\nrig default &lt;new_r_version&gt; sets that version as the default\n\nAdd R and RTools to path\n\nRight-click Windows &gt;&gt; System &gt;&gt; (right panel) Advanced System Settings &gt;&gt; Environment Variables &gt;&gt; Under User Variables, highlight Path, click Edit &gt;&gt; Click Add\n\nR: Add path to directory with all the RScript, R exe, etc. e.g. “C:\\Program Files\\R\\R-4.2.3\\bin\\x64”\nRTools: e.g. “C:\\rtools43\\usr\\bin”\n\n\nOpen R and confirm new version\n\nIf RStudio\n\nThe setting of the new version to the “default” version of R in rig should result in RStudio loading the new version.\nIf not, Tools &gt;&gt; Global Options &gt;&gt; General\n\nUnder “R version”, click “change” button; choose new R version\nQuit session and restart RStudio\n\n\n\nInstall “high maintenance” packages\n\nI’ve had issues with {pak} installing packages that need to be compiled. Maybe be worth trying {pak} first to see if they’ve fixed it.\n{cmdstanr} doesn’t live on CRAN, so you have to use: install.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\n\nCheck for latest cmdstan version\n\nAfter loading the package, library(cmdstanr) , it should run a check on your cmdstan version and tell you if there’s a newer version.\nTo update, first check toolchain: check_cmdstan_toolchain()\n\nMight tell you to update RTools or that you need some C++ library added\n\nFix C++ toolchain with check_cmdstan_toolchain(fix = TRUE)\nUpdate cmdstan: install_cmdstan()\nMay need to install {rstudioapi} and run rstudioapi::restartSession() (programmatically) or just ctrl + shift + f10 so that this package can be used as a dependency for other packages that need to be installed.\n\n\n{rstanarm}: install.packages(\"rstanarm\")\n\nInstall other packages\nmoose &lt;- readRDS(\"packages.rds\")\nmoose &lt;- moose[!moose %in% c(\"cmdstanr\", \"rstanarm\", \"ebtools\", \"translations\", \"&lt;RStudio add-ins&gt;\")]\n\n# Next time, add a try/catch? or maybe purrr::safely, so that it continues through errors. Also, need to log pkgs that do error.\nfor (i in seq_len(length(moose))) {\n  print(moose[i])\n  pak::pkg_install(moose[i])\n}\n\nfs::file_delete(\"packages.rds\")\n\n{ebtools} is my personal helper package.\n{translations} is a system package that shouldn’t have been included when I saved the packages from previous version, but was when I recently updated. Might not be necessary to include it in the excuded packages in the future.\n\nCheck for updates of RStudio (link)\n\nCurrent version under Help &gt;&gt; About Rstudio\nPossible to check for updates under Help &gt;&gt; Check for Updates, but that’s failed me before.",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html",
    "href": "qmd/spreadsheets.html",
    "title": "Spreadsheets",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-misc",
    "href": "qmd/spreadsheets.html#sec-spdsht-misc",
    "title": "Spreadsheets",
    "section": "",
    "text": "Some Excel files are binaries and in order to use download.file, you must set mode = “wb”\ndownload.file(url, \n              destfile = glue(\"{rprojroot::find_rstudio_root_file()}/data/cases-age.xlsx\"), \n              mode = \"wb\")\nIndustry studies show that 90 percent of spreadsheets containing more than 150 rows have at least one major mistake.",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-cats",
    "href": "qmd/spreadsheets.html#sec-spdsht-cats",
    "title": "Spreadsheets",
    "section": "Catastrophes",
    "text": "Catastrophes\n\nReleasing confidential information\n\nIrish police accidently handed out officers private information when sharing sheets with statistics due to a freedom of information request. (link)\n\nErrors when combining sheets\n\nWales dismissed anaesthesiologists after mistakenly deeming them “unappointable.” Spreadsheets from different areas lacked standardization in formatting, naming conventions, and overall structure. To make matters worse, data was manually copied and pasted between various spreadsheets, a time-consuming and error-prone process. (link)\nWhen consolidating assets from different spreadsheets, the spreadsheet data was not “cleaned” and formatted properly. The Icelandic bank’s shares were subsequently undervalued by as much as £16 million. (link)\n\nData entry errors\n\nCryto.com accidentally transferred $10.5 million instead of $100 into the account of an Australian customer due to an incorrect number being entered on a spreadsheet. (link)\nNorway’s $1.5tn sovereign wealth fund lost $92M, on an error relating to how it calculated its mandated benchmark. A person used the wrong date, December 1st instead of November 1st. (link)",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-bprac",
    "href": "qmd/spreadsheets.html#sec-spdsht-bprac",
    "title": "Spreadsheets",
    "section": "Best Practices",
    "text": "Best Practices\n\nNotes from Data organization in spreadsheets\n\nBe consistent\nWrite dates like YYYY-MM-DD\nDon’t leave any cells empty\nPut just one thing in a cell\nOrganize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row)\nCreate a data dictionary\nDon’t include calculations in the raw data files\nDon’t use font color or highlighting as data\nChoose good names for things\nMake backups\nUse data validation to avoid data entry errors\nSave the data in plain text files.",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-transspr",
    "href": "qmd/spreadsheets.html#sec-spdsht-transspr",
    "title": "Spreadsheets",
    "section": "Transitioning from Spreadsheet to DB",
    "text": "Transitioning from Spreadsheet to DB\n\nMisc\n\nWhen you start to have multiple datasets or when you want to make use of several columns in one table and other columns in another table you should consider going the local database route.\nUse db “normalization” to figure out a schema\nAlso see\n\nDatabases, Engineering &gt;&gt; Schema\nDatabases, Warehouses &gt;&gt; Design a Warehouse\n\n\nDB advantages over spreadsheets:\n\nEfficient analysis: Relational databases allow information to be retrieved quicker to then be analyzed with SQL (Structured Query Language), to then run queries.\n\nOnce spreadsheets get large, they can lag or freeze when opening, editing, or performing simple analyses in them.\n\nCentralized data management: Since relational databases often require a certain type or format of data to be input into each column of a table, it’s less likely that you’ll end up with duplicate or inconsistent data.\nScalability: If your business is experiencing high growth, this means that the database will expand, and a relational database can accommodate an increased volume of data.\n\nStart documenting the spreadsheets\n\nfile names, file paths\nUnderstand where values are coming from\n\nsource (e.g. department, store, sensor), owner\n\nHow rows of data are being generated\n\nwho/what is inputting the data\n\nHow does each spreadsheet/notebooks/set of spreadsheets fit in the company’s business model\n\nHow are they being used and by whom\n\nMap the spreadsheets relationships to one another\n\nSee Databases, Warehouses &gt;&gt; Design a Warehouse",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/sql.html",
    "href": "qmd/sql.html",
    "title": "SQL",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-misc",
    "href": "qmd/sql.html#sec-sql-misc",
    "title": "SQL",
    "section": "",
    "text": "Resources\n\nPublicly Available SQL Databases: Need to email administrator to gain access\nSQL for Data Scientists in 100 Queries\n\nSQLite, administrative commands, query commands, JSON ops, python\n\n\ndplyr::show_query can convert a dplyr expression to SQL for db object (e.g. dbplyr,  duckdb, arrow)\nQueries in examples\n\nWindow Functions\n\nAverage Salary by Job Title\nAverage Unit Price for each CustomerId\nRank customers by amount spent\nCreate a new column that ranks Unit Price in descending order for each CustomerId\nCreate a new column that provides the previous order date’s Quantity for each ProductId\nCreate a new column that provides the very first Quantity ever ordered for each ProductId\nCalculate a cumulative moving average UnitPrice for each CustomerId\nRank customers for each department by amount spent\nFind the model and year of the car that been on the lot the longest\nCreate a subset (CTE)\n\nCalculate a running monthly total (aka cumsum)\n\nAlso running average\n\nCalculate a running monthly total for each account id\nCalculate a 3 months rolling running total using a window that includes the current month.\nCalculate a 7 months rolling running total using a window where the current month is always the middle month\nCalculate the number of consecutive days spent in each country\n\n\nCTE\n\nAverage monthly cost per campaign for the company’s marketing efforts\nCount the number of interactions of new users\nThe average top Math test score for students in California\n\nBusiness Queries\n\n7-day Simple Moving Average (SMA)\nRank product categories by shipping cost for each shipping address\nDaily counts of open jobs (where “open” is an untracked daily status)\nGet the latest order from each customer\nOverall median price\nMedian price for each product\nOverall median price and quantity\n\nProcessing Expressions\n\nProvide subtotals for a hierarchical group of fields (e.g. family, category, subcategory)\n\nSee NULLs &gt;&gt; COALESCE\n\n\n\nOrder of Operations\n\n\nHigher ranked functions can be inserted inside lower ranked functions\n\ne.g a window function can be inside a SELECT function but not inside a WHERE clause\nThere are exceptions and hacks around this in some cases\n\n\nTypes of Commands\n\nData Query Language (DQL) - used to find and view data without making any permanent changes to the database.\nData Manipulation Language (DML) - used to make permanent changes to the data, such as updating values or deleting them.\nData Definition Language (DDL) - used to make permanent changes to the table, such as creating or deleting a table.\nData Control Language (DCL) - used for administrative commands, such as adding or removing users of different tables and databases.\nTransact Control Language (TCL) - advanced SQL that deals with transaction level statements.\n\nMicrosoft SQL Server format for referencing a table:\n\n[database].[schema].[tablename]\nAlternative\nUSE my_data_base\nGO\n\nCheck if a table is updatable\nSELECT table_name, is_updatable\nFROM information_schema.views\n\nUseful if some of the tables you are working with are missing values that you need to add\nIf not updatable, then you’ll need to contact the database administrator to request permission to update that specific table\nShow all tables\nSHOW FULL TABLES -- mysql",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-setup",
    "href": "qmd/sql.html#sec-sql-setup",
    "title": "SQL",
    "section": "Set-up",
    "text": "Set-up\n\npostgres\n\nDownload postgres\npgAdmin is an IDE commonly used with postgres\n\nOpen pgAdmin and click on “Add new server.”\n\nSets up connection to existing server so make sure postgres is installed beforehand\n\nCreate Tables\n\nhome &gt;&gt; Databases (1) &gt;&gt; postgres &gt;&gt; Query Tool\n\nIf needed, give permission to pgAdmin to access data from a folder\n\nMight be necessary to upload csv files\n\nImport csv file\n\nright-click the table name &gt;&gt; Import/Export\nOptions tab\n\nSelect import, add file path to File Name, choose csv for format, select Yes for Header, add , for Delimiter\n\nColumns tab\n\nuncheck columns not in the csv (probably the primary key)\n\nWonder if NULLs will be automatically inserted for columns in the table that aren’t in the file.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-terms",
    "href": "qmd/sql.html#sec-sql-terms",
    "title": "SQL",
    "section": "Terms",
    "text": "Terms\n\nBatch - a set of sql statements e.g. statements between BEGIN and END\nCompiled object - you can create a function written in C/C++ and load into the database (at least in postgres) to achieve high performance.\nCorrelated Columns - tells how good the match between logical and physical ordering is.\nCorrelated/Uncorrelated Subqueries\n\ncorrelated- a type of query, where inner query depends upon the outcome of the outer query in order to perform its execution\n\nA correlated subquery can be thought of as a filter on the table that it refers to, as if the subquery were evaluated on each row of the table in the outer query\n\n\nuncorrelated - a type of sub-query where inner query doesn’t depend upon the outer query for its execution.\n\nIt is an independent query, the results of which are returned to and used by the outer query once (not per row).\n\n-- Uncorrelated subquery:\n-- inner query, c1, only depends on table2\nselect c1, c2\n  from table1 where c1 = (select max(x) from table2);\n\n-- Correlated subquery:\n-- inner query, c1, depends on table1 and table2\nselect c1, c2\n  from table1 where c1 = (select x from table2 where y = table1.c2);\n\nFunctions execute at a different level of priority and are handled differently than Views. You will likely see better performance.\nIndex - a quick lookup table (e.g. field or set of fields) for finding records users need to search frequently. An index is small, fast, and optimized for quick lookups. It is very useful for connecting the relational tables and searching large tables. (also see DB, Engineering &gt;&gt; Cost Optimizations)\nMigrations (schema) - version control system for your database schema. Management of incremental, reversible changes and version control to relational database schemas. A schema migration is performed on a database whenever it is necessary to update or revert that database’s schema to some newer or older version.\nPhysical Ordering - A PostgreSQL table consists of one or more files of 8KB blocks (or “pages”). The order in which the rows are stored in the file is the physical ordering.\nPredicate - defines a logical condition being applied to rows in a table. (e.g. IN, EXISTS, BETWEEEN, LIKE, ALL, ANY)\nScalar/Non-Scalar Subqueries\n\nA scalar subquery returns a single value (one column of one row). If no rows qualify to be returned, the subquery returns NULL.\nA non-scalar subquery returns 0, 1, or multiple rows, each of which may contain 1 or multiple columns. For each column, if there is no value to return, the subquery returns NULL. If no rows qualify to be returned, the subquery returns 0 rows (not NULLs).\n\nSelectivity - the fraction of rows in a table or partition that is chosen by the predicate\n\nRefers to the quality of a filter in its ability to reduce the number of rows that will need to be examined and ultimately returned\n\nWith a high selectivity, using the primary key or indexes to get right to the rows of interest\nWith a low selectivity, a full table scan would likely be needed to get the rows of interest.\n\nHigher selectivity means: more unique data; fewer duplicates; fewer number of rows for each key value\nUsed to estimate the cost of a particular access method; it is also used to determine the optimal join order. A poor choice of join order by the optimizer could result in a very expensive execution plan.\n\nSoft-deleted - An operation in which a flag is used to mark data as unusable, without erasing the data itself from the database\nSurrogate key - very similar to a primary key in that it is a unique value for an object in a table. However, rather than being derived from actual data present in the table, it is a field generated by the object itself. It has no business value like a primary key does, but is rather only used for data analysis purposes. Can be generated using different columns that already exist in your table or more often from two or more tables. dbt function definition\n\nExamples: PostgreSQL serial column, Oracle sequence column, or MySQL auto_increment column, Snowflake _file + _line columns\nExample: Each employee id is concatenated with a department id (e.g. marketing or finance)\n\n\nTransaction - a set of queries tied together such that if one query fails, the entire set of queries are rolled back to a pre-query state if the situation dictates.\n\nA database transaction, by definition, must be atomic, consistent, isolated and durable. These are popularly known as ACID properties.  These properties can ensure the concurrent execution of multiple transactions without conflict.\n\nViews - database objects that represent saved SELECT queries in “virtual” tables.\n\nContains a query plan.  For each query executed, the planner has to evaluate what’s being asked and calculate an optimal path. Views already have this plan calculated so it allows subsequent queries to be returned with almost no friction in processing aside from data retrieval.\nSome views are updateable but under certain conditions (1-1 mapping of rows in view to underlying table, no group_by, etc.)",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-basics",
    "href": "qmd/sql.html#sec-sql-basics",
    "title": "SQL",
    "section": "Basics",
    "text": "Basics\n\nMisc\n\nDROP TABLE &lt;table name&gt; - Deletes table from database. Used to remove obsolete or redundant tables from the database.\n\n\n\nCreate Tables\n\nIf you don’t include the schema as part of the table name (e.g. schema_name.table_name), pgadmin automatically places it into the “public” schema directory\nField Syntax: name, data type, constraints\nExample: Create table as select (CTAS)\nCREATE TABLE new_table AS \nSELECT * \nFROM old_table \nWHERE condition;\nExample: Table 1 (w/primary key)\nDROP TABLE IF EXISTS classrooms CASCADE;\nCREATE TABLE classrooms (\n    id INT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,\n    teacher VARCHAR(100)\n    );\n -- OR\nCREATE TABLE classrooms ( \n    id INT GENERATED ALWAYS AS IDENTITY, \n    teacher VARCHAR(100)\n    PRIMARY KEY(id) \n    );   \n\n“classrooms” is the name of the table; “id” and “teacher” are the fields\nCASCADE - postgres won’t delete the table if other tables point to it, so cascade will override measure.\nGENERATED ALWAYS AS IDENTITY - makes it so you don’t have to keep track of which “id” values have been used when adding rows. You can ommit the value for “id” and just add the values for the other fields\nSee tutorial for options, usage, removing, adding, etc. this constraint\nINSERT INTO classrooms\n    (teacher)\nVALUES\n    ('Mary'),\n    ('Jonah');\n\nAlso see Add Data &gt;&gt; Example: chatGPT\n\n\nExample: Table 2 (w/foreign key)\nDROP TABLE IF EXISTS students CASCADE;\nCREATE TABLE students (\n    id INT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,\n    name VARCHAR(100),\n    classroom_id INT,\n    CONSTRAINT fk_classrooms\n        FOREIGN KEY(classroom_id)\n        REFERENCES classrooms(id)\n);\n\n“students” is the name of the table; “id”, “name”, and “classroom_id” are the fields\nCreate a foreign key that points to the “classrooms” table\n\nExpression\n\nfk_classrooms is the name of the CONSTRAINT\n“classroom_id” is the field that will be the FOREIGN KEY\nREFERENCES points the foreign key to the classrooms table’s primary key, “id”\n\nforeign keys can point to any table\n\n\nPostgres won’t allow you to insert a row into students with a “classroom_id” that doesn’t exist in the “id” field of classrooms but will allow you to use a NULL placeholder\n-- Explicitly specify NULL\nINSERT INTO students\n    (name, classroom_id)\nVALUES\n    ('Dina', NULL); \n\n-- Implicitly specify NULL\nINSERT INTO students\n    (name)\nVALUES\n    ('Evan');\n\nAlso see Add Data &gt;&gt; Example: chatGPT\n\n\n\nExample\nCREATE TABLE members (\n    id serial primary key,\n    second_name character varying(200) NOT NULL,\n    date_joined date NOT NULL DEFAULT current_date,\n    member_id integer references members(id),\n    booking_start_time timestamp without timezone NOT NULL\n\nThe “serial” data type does the same thing as GENERATED ALWAYS AS IDENTITY (see first example), but is NOT compliant with the SQL standard. Use GENERATED ALWAYS AS IDENTITY\n“references” seems to be another old way to create foreign keys (see 2nd example for proper way)\n“character varying” - variable-length with limit (e.g limit of 200 characters)\n\ncharacter(n), char(n) are for fixed character lengths; text is for unlimited character lengths\n\n“current_date” is a function that will insert the current date as a value\n“timestamp without timezone” is literally that\n\nalso available: time with/without timezone, date, interval (see Docs for details)\n\n\nExample: MySQL\nCREATE DATABASE products;\n\nCREATE TABLE `products`.`prices` (\n  `pid` int(11) NOT NULL AUTO_INCREMENT,\n  `category` varchar(100) NOT NULL,\n  `price` float NOT NULL,\n  PRIMARY KEY (`pid`)\n);\n\nINSERT INTO products.prices\n    (pid, category, price)\nVALUES\n    (1, 'A', 2),\n    (2, 'A', 1),\n    (3, 'A', 5),\n    (4, 'A', 4),\n    (5, 'A', 3),\n    (6, 'B', 6),\n    (7, 'B', 4),\n    (8, 'B', 3),\n    (9, 'B', 5),\n    (10, 'B', 2),\n    (11, 'B', 1)\n;\n\n\n\nAdd Data\n\nExample: Copy/Paste table values into chatGPT to get the query\n\nExample: Add data via .csv\nCOPY assignments(category, name, due_date, weight)\nFROM 'C:/Users/mgsosna/Desktop/db_data/assignments.csv'\nDELIMITER ','\nCSV HEADER;\n\n“assignments” is the table; “category”, “name”, “due_date”, “weight” are fields that you want to import from the csv file\n** The order of the columns must be the same as the ones in the CSV file **\nHEADER keyword to indicate that the CSV file contains a header\nMight need to have superuser access in order to execute the COPY statement successfully\n\n\n\n\nUpdate Table\n\nUpdate target table by transaction id (BQ)(link)\ninsert target_table (transaction_id)\n  select transaction_id \n  from source_table \n  where transaction_id &gt; (select max(transaction_id) from target_table)\n;\n\nMight not be possible with denormalized star-schema datasets in modern data warehouses.\n\nUpdate Target Table by transaction date (BQ) (link)\nmerge last_online t\nusing (\n  select\n      user_id,\n      last_online\n  from\n    (\n        select\n            user_id,\n            max(timestamp) as last_online\n\n        from \n            connection_data\n        where\n            date(_partitiontime) &gt;= date_sub(current_date(), \n                                             interval 1 day)\n        group by\n            user_id\n\n    ) y\n\n) s\non t.user_id = s.user_id\nwhen matched then\n  update set last_online = s.last_online, \n             user_id = s.user_id\nwhen not matched then\n  insert (last_online, user_id) \n    values (last_online, user_id)\n;\nselect * from last_online\n;\n\nMERGE performs UPDATE, DELETE, and INSERT\n\nUPDATE or DELETE clause can be used when two or more data match.\nINSERT clause can be used when two or more data are different and do not match.\nThe UPDATE or DELETE clause can also be used when the given data does not match the source.\n\n_partitiontime is a field BQ creates to record the row’s ingestion time (See Google, Big Query &gt;&gt; Optimization &gt;&gt; Partitions\n\nUpdate Target Table with Source Data (link)\nMERGE INTO target_table tgt\nUSING source_table src \n ON tgt.customer_id = src.customer_id\nWHEN MATCHED THEN\n UPDATE SET\n   tgt.is_active = src.is_active,\n   tgt.updated_date = '2024-04-01'::DATE\nWHEN NOT MATCHED THEN\n INSERT\n   (customer_id, is_active, updated_date)\n VALUES\n (src.customer_id, src.is_active, '2024-04-01'::DATE)\n; \n\nThe statement uses the MERGE keyword to conditionally update or insert rows into a target table based on a source table.\nIt matches rows between the tables using the ON clause and the customer_id column.\nThe WHEN MATCHED THEN clause specifies the update actions for matching rows.\nThe WHEN NOT MATCHED THEN clause specifies the insert actions for rows that don’t have a match in the target table.\nThe ::DATE cast ensures that the updated_date value is treated as a date.\n\n\n\n\nSubqueries\n\n**Using CTEs instead of subqueries make code more readable**\n\nSubqueries make it difficult to understand their context in the larger query\nThe only way to debug a subquery is by turning it into a CTE or pulling it out of the query entirely.\nCTEs and subqueries have a similar runtime, but subqueries make your code more complex for no reason.\n\nNotes from How to Use SubQueries in SQL\n\nAlso shows the alt method of creating a temporary table to compute the queries\n\nUse cases\n\nFiltering rows from a table with the context of another.\nPerforming double-layer aggregations such as average of averages or an average of sums.\nAccessing aggregations with a subquery.\n\nTables used in examples\n\nStore A (store_a)\n\n\nStore B is similar\n\n\nExample: Filtering rows\nselect * \nfrom sandbox.store_b\nwhere product_id IN (\n    select product_id\n    from sandbox.store_b\n    group by product_id \n    having count(product_id) &gt;= 3\n);\n\nfilters the rows with products that have been bought at least three times in store_b\n\nExample: Multi-Layer Aggregation\nselect avg(average_price.total_value) as average_transaction from (\n  select transaction_id, sum(price_paid) as total_value\n  from sandbox.store_a\n  group by transaction_id\n  ) as average_price\n;\n\ncomputes the average of all transactions\ncan’t apply an average directly, as our table is oriented to product_ids and not to transaction_ids\n\nExample: Filtering the table based on an Aggregation\nselect @avg_transaction:= avg(agg_table.total_value)\nfrom (\n  select transaction_id, sum(price_paid) as total_value\n  from sandbox.store_a\n  group by transaction_id\n) as agg_table;\n\nselect * \nfrom sandbox.store_a\nwhere transaction_id in (\n  select transaction_id\n  from sandbox.store_a\n  group by transaction_id\n  having sum(price_paid) &gt; @avg_transaction\n)\n\nfilters transactions that have a value higher than the average (where the output must retain the original product-oriented row)\n\n\n\n\nJoins\n\n\n\nCross Join -  acts like an expand_grid; where each value in the join key column gets all combinations of rows in both tables (also see above pic)\n\n\nEfficient join\nWhen you add the where clause, the cross join acts similarly to an inner join, except you aren’t joining it on any specified column\nExample:\nSELECT\n  schedule.event,\n  calendar.number_of_days\nFROM schedule\nCROSS JOIN calendar\nWHERE schedule.total_number &lt; calendar.number_of_days\n\nOnly join the row in the “schedule” table with the rows in the “calendar” table that meet the specified condition\n\n\nNatural Join - don’t need to specify join columns; need to have two columns in each table with the same name\n\nUse cases\n\nThere are a lot of common columns with the same name across multiple tables\n\nThey will all be used as joining keys.\n\nYou don’t want to type out all of the common columns in select just to avoid outputting the same columns multiple times.\n\n\nselect *\nfrom table_a\nnatural join table_b\n;\n\n-- natural + outer\nselect *\nfrom table_a\nnatural outer join table_b\n;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-r",
    "href": "qmd/sql.html#sec-sql-r",
    "title": "SQL",
    "section": "R",
    "text": "R\n\nGet query from dplyr code\ntbl_to_sql &lt;- function(tbl) {\n  dplyr::show_query(tbl) |&gt; \n    capture.output() |&gt; \n    purrr::discard_at(1) |&gt; \n    paste(collapse = \" \")\n}\n\nTransforms query into a string\nAlso see Generating SQL with {dbplyr} and sqlfluff\n\nConnect to or Create a SQLite database\ncon &lt;- DBI::dbConnect(drv = RSQLite::SQLite(),\n                      here::here(\"db_name.db\"),\n                      timeout = 10)\nConnect to Microsoft SQL Server\ncon &lt;- DBI::dbConnect(odbc::odbc(), \n                      Driver = \"SQL Server\", \n                      Server = \"SERVER\", \n                      Database = \"DB_NAME\", \n                      Trusted_Connection = \"True\", \n                      Port = 1433)\nClose connection: dbDisconnect(con)\nCreate a table from a data source: df &lt;- dbplyr::tbl(con, \"&lt;table name&gt;\")\n\nAllows you to use dplyr verbs with a remote database table then collect\n\nCancel a running query (postgres)\n# Store PID\npid &lt;- DBI::dbGetInfo(conn)$pid\n\n# Cancel query and get control of IDE back\n# SQL command\nSELECT pg_cancel_backend(&lt;PID&gt;)\n\nUseful if query is running too long and you want control of your IDE back\n\nCreate single tables from a list of tibbles to a database\npurrr::map2(table_names, list_of_tbls, ~ dbWriteTable(con, .x, .y))\nLoad all tables from a database into a list\ntables &lt;- dbListTables(con) \nall_data &lt;- map(tables, dbReadTable, conn = con)\nCan use map_dfr if all the tables have the same columns\nDynamic queries with {glue}\n\nExample: MS SQL Server\nvars &lt;- c(\"columns\", \"you\", \"want\", \"to\", \"select\")\ndate_var &lt;- 'date_col'\nstart_date &lt;- as.Date('2022-01-01')\ntoday &lt;- Sys.Date()\ntablename &lt;- \"yourtablename\"\nschema_name &lt;- \"yourschema\"\nquery &lt;- glue_sql(.con = con, \"SELECT TOP(10) {`vars`*} FROM {`schema_name`}.{`tablename`} \")\nDBI::dbGetQuery(con, query)\n\nvars format collapses the vars vector, separated by commas, so that it resembles a SELECT statement\n\n\nPRQL\n\nDocs\n{prqlr}\nA dplyr + SQL hybrid language\nUsing with DuckDB\nlibrary(prqlr); library(duckdb)\ncon &lt;- dbConnect(duckdb(), dbdir = \":memory\")\ndbWriteTable(con, \"mtcars\", mtcars)\n\"from mtcars | filter cyl &gt; 6 | select {cyl, mpg}\" |&gt; \n  prql_compile() |&gt; \n  dbGetQuery(conn = con)",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-bestp",
    "href": "qmd/sql.html#sec-sql-bestp",
    "title": "SQL",
    "section": "Best Practices",
    "text": "Best Practices\n\nMisc\n\nResources\n\nSQL Style Guide\n\nUse aliases only when table names are long enough so that using them improves readability (but choose meaningful aliases)\nDo not use SELECT *. Explicitly list columns instead\nUse comments to document business logic\nA comment at the top should provide a high-level description\nUse an auto-formatter\nGeneral Optimizations\n\nRemoving duplicates or filtering out null values at the beginning of your model will speed up queries\nReplace complex code with window functions\n\nExample: Replace GROUP_BY + TOP with a partition + FIRST_VALUE()\nFIRST_VALUE(test_score) OVER(PARTITION BY student_name ORDER BY test_score DESC)\nExample: AVG(test_score) OVER(PARTITION BY student_name)\n\n\n\nCTEs\n\nBreak down logic in CTEs using WITH … AS\nThe SELECT statement inside each CTE must do a single thing (join, filter or aggregate)\nThe CTE name should provide a high-level explanation\nThe last statement should be a SELECT statement querying the last CTE\n\nJoins\n\nUse WHERE for filtering, not for joining\nFavor LEFT JOIN over INNER JOIN; in most cases, it’s essential to know the distribution of NULLs\nAvoid using”Self-Joins.” Use window functions instead (see Google, BigQuery &gt;&gt; Optimization for details on self-joins)\nWhen doing equijoins (i.e., joins where all conditions have the something=another form), use the USING keyword\nBreak-up joins using OR into UNION because SQL uses nested operations for JOIN + OR queries which slow things.\n\nBad\n\nGood\n\nUNION simply joins the outputs of two separate SELECT statements and retains only one occurrence of duplicated rows if there are any.\n\n\nStyle Guide",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-index",
    "href": "qmd/sql.html#sec-sql-index",
    "title": "SQL",
    "section": "Indexes",
    "text": "Indexes\n\nMisc\n\nAn index may consist of up to 16 columns\nThe first column of the index must always be present in the query’s filter, order , join or group operations to be used\n\nCreate Index on an existing table (postgres)\nCREATE INDEX\n    score_index ON grades(score, student);\n\n“score_index” is the name of the index\n“grades” is the name of the table\n“score” and “student” are fields to be used as the indexes\n\nCreate Index that only uses a specific character length\n/* mysql */\nCREATE TABLE test (blob_col BLOB, INDEX(blob_col(10)));\n\nindex only uses the first 10 characters of the column value of a BLOB column type\n\nCreate index with multiple columns\n/* mysql */\nCREATE TABLE test (\n    id        INT NOT NULL,\n    last_name  CHAR(30) NOT NULL,\n    first_name CHAR(30) NOT NULL,\n    PRIMARY KEY (id),\n    INDEX name (last_name,first_name)\n)\nUsage of multiple column index (** order of columns is important **)\nSELECT * FROM test WHERE last_name='Jones';\nSELECT * FROM test\n  WHERE last_name='Jones' AND first_name='John';\nSELECT * FROM test\n  WHERE last_name='Jones'\n  AND (first_name='John' OR first_name='Jon');\nSELECT * FROM test\n  WHERE last_name='Jones'\n  AND first_name &gt;='M' AND first_name &lt; 'N';\n\nIndex is used when both columns are used as part of filtering criteria or when only the left-most column is used\nif you have a three-column index on (col1, col2, col3), you have indexed search capabilities on (col1), (col1, col2), and (col1, col2, col3).\n\nInvalid usage of multiple column index\nSELECT * FROM test WHERE first_name='John';\nSELECT * FROM test\n  WHERE last_name='Jones' OR first_name='John';\n\nThe “name” index won’t be used in these queries since\n\nfirst_name is NOT the left-most column specified in the index\nOR is used instead of AND\n\n\nCreate index with DESC, ASC\n/* mysql */\nCREATE TABLE t (\n  c1 INT, c2 INT,\n  INDEX idx1 (c1 ASC, c2 ASC),\n  INDEX idx2 (c1 ASC, c2 DESC),\n  INDEX idx3 (c1 DESC, c2 ASC),\n  INDEX idx4 (c1 DESC, c2 DESC)\n);\n\nUsed by ORDER BY\n\nSee Docs to see what operations and index types support Descending Indexes\n\nNote: idx_a on column_p, column_q desc is not the same as an * Index idx_a on column_q desc, column p or, * Index idx_b on column_p desc, column q\n\nUsage of Descending Indexes\nORDER BY c1 ASC, c2 ASC    -- optimizer can use idx1\nORDER BY c1 DESC, c2 DESC  -- optimizer can use idx4\nORDER BY c1 ASC, c2 DESC  -- optimizer can use idx2\nORDER BY c1 DESC, c2 ASC  -- optimizer can use idx3\n\nSee previous example for definition of idx* names\nSee Docs to see what operations and index types support Descending Indexes",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-part",
    "href": "qmd/sql.html#sec-sql-part",
    "title": "SQL",
    "section": "Partitioning",
    "text": "Partitioning\n\nMisc\n\nAlso see\n\nMySQL Docs\nGoogle, BigQuery &gt;&gt; Optimization &gt;&gt; Partitioning and Clustering\nDB, Engineering &gt;&gt; Cost Optimization &gt;&gt; Partitioning\n\nall of your queries to the partitioned table must contain the partition_key in the WHERE clause",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-views",
    "href": "qmd/sql.html#sec-sql-views",
    "title": "SQL",
    "section": "Views",
    "text": "Views\n\nA smaller data object that contains the subset of data resulting from a specific query\nWhereas a query happens after data is loaded, a materialized view is a precomputation\nThe computation is done once, and changes to the data are incorporated as they occur, making subsequent updates to the view much cheaper and more efficient than querying the entire database from scratch.\nCreate a View\n\nExample: Create view as select (CVAS)\nCREATE VIEW high_earner AS \nSELECT p.id AS person_id, j.salary\nFROM People p\nJOIN Job j \nON p.job = j.title\nWHERE j.salary &gt;= 200000;\n\nQuery a view (same as a table): SELECT * FROM high_earner\nUpdate view\nCREATE OR REPLACE VIEW high_earner AS \nSELECT p.id AS person_id, j.salary\nFROM People p\nJOIN Job j \nON p.job = j.title\nWHERE j.salary &gt;= 150000;\n\nExpects the query output to retain the same number of columns, column names, and column data types. Thus, any modification that results in a change in the data structure will raise an error.\n\nList views\n\nSELECT * \nFROM information_schema.views\nWHERE table_schema NOT IN ('pg_catalog', 'information_schema');\n\n“table_name” has the names of the views\n“view_definition” shows the query stored in the view\nWHERE command is included to omit built-in views from PostgreSQL.\n\nDelete view: DROP VIEW high_earner;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-vars",
    "href": "qmd/sql.html#sec-sql-vars",
    "title": "SQL",
    "section": "Variables",
    "text": "Variables\n\nMisc\n\nAlso see Business Queries &gt;&gt; Medians\n\nUser-defined\n\nDECLARE and SET\n-- Declare your variables\nDECLARE @start date\nDECLARE @stop date\n-- SET the relevant values for each variable\nSET @start = '2021-06-01'\nSET @stop = GETDATE()\n\nDECLARE sets the variable type (e.g. date)\nSET assigns a value\n\nOr just use DECLARE\nDECLARE @Iteration Integer = 0;\nExamples\n\nExample: Exclude 3 months of data from the query\nSELECT t1.[DATETIME], COUNT(*) AS vol\nFROM Medium.dbo.Earthquakes t1\nWHERE t1.[DATETIME] BETWEEN @start AND DATEADD(MONTH, -3, @stop)\nGROUP BY t1.[DATETIME]\nORDER BY t1.[DATETIME] DESC;\n\nSee above for the definitions of @start and @stop\n\nExample: Apply a counter\n-- Declare the variable (a SQL Command, the var name, the datatype)\nDECLARE @counter INT;\n-- Set the counter to 20\nSET @counter = 20;\n-- Print the initial value\nSELECT @counter AS _COUNT;\n-- Select and increment the counter by one\nSELECT @counter = @counter + 1;\n-- Print variable\nSELECT @counter AS _COUNT;\n-- Select and increment the counter by one\nSELECT @counter += 1;\n-- Print the variable\nSELECT @counter AS _COUNT;\n\n\nSystem\n\nROWCOUNT - returns the number of rows affected by the last previous statement\n\nExample\nBEGIN\n    SELECT\n        product_id,\n        product_name\n    FROM\n        production.products\n    WHERE\n        list_price &gt; 100000;\n    IF @@ROWCOUNT = 0\n        PRINT 'No product with price greater than 100000 found';\nEND",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-funs",
    "href": "qmd/sql.html#sec-sql-funs",
    "title": "SQL",
    "section": "Functions",
    "text": "Functions\n\n“||” Concantenate strings. e.g ‘Post’ || ‘greSQL’ –&gt; PostgreSQL\nBEGIN…END - defines a compound statement or statement block. A compound statement consists of a set of SQL statements that execute together. A statement block is also known as a batch\n\nA compound statement can have a local declaration for a variable, a cursor, a temporary table, or an exception\n\nLocal declarations can be referenced by any statement in that compound statement, or in any compound statement nested within it.\nLocal declarations are invisible to other procedures that are called from within a compound statement\n\n\nCOMMIT - a transaction control language that is used to permanently save the changes done in the transaction in tables/databases. The database cannot regain its previous state after its execution of commit.\nDATEADD - adds units of time to a variable or value\n\ne.g. DATEADD(month, -3, '2021-06-01')\n\nsubtracts 3 months from 2021-06-01\n\n\nDATE_TRUNC - pulls a component of a date object.\n\ne.g. date_trunc('month', date_var) as month\n\nDENSE_RANK- similar to the RANK , but it does not skip any numbers even if there is a tie between the rows.\n\nValues are ranked by the column specified in ORDER BY expression of the window function\n\nEXPLAIN - a means of running your query as a what-if to see what the planner thinks about it. It will show the process the system goes through to get to the data and return it.\nEXPLAIN\nSELECT\n    s.id AS student_id,\n    g.score\nFROM\n    students AS s\nLEFT JOIN\n    grades AS g\n    ON s.id = g.student_id\nWHERE\n    g.score &gt; 90\nORDER BY\n    g.score DESC;\n/*\nQUERY PLAN\n----------\nSort (cost=80.34..81.88 rows=617 width=8)\n[...] Sort Key: g.score DESC\n[...] -&gt; Hash Join (cost=16.98..51.74 rows=617 width=8)\n[...] Hash Cond: (g.student_id = s.id)\n[...] -&gt; Seq Scan on grades g (cost=0.00..33.13 rows=617 width=8)\n[...] Filter: (score &gt; 90)\n[...] -&gt; Hash (cost=13.10..13.10 rows=310 width=4)\n[...] -&gt; Seq Scan on students s (cost=0.00..13.20 rows=320 width=4)\n*/\n\nSequentially scanning (“Seq Scan”) the grades and students tables because the tables aren’t indexed\n\nAny Seq Scan, parallel or not, is sub-optimal\n\nEXPLAIN (BUFFERS) also shows how may data pages the database had to fetch using slow disk read operations (“read”), and how many of them were cached in memory (“shared hit”)\n\nEXPLAIN ANALYZE - tells the planner to not only hypothesize on what it would do, but actually run the query and show the results.\n\nshows where indexes are being hit — or not hit as it may be. You can step through and re-optimize your basic and complex queries.\n\nGETDATE() - Gets the current date\nGO - Not a sql function. Used by some interpreters as a reset.\n\ni.e. any variables set before the GO statement will now not be recognized by the interpreter.\nHelps to separate code into different sections\n\nISDATE - boolean - checks that a variable is a date type\nQUALIFY - clause filters the results of window functions.\n\nuseful when answering questions like fetching the most XXX value of each category\nQUALIFY does with window functions as what HAVING does with GROUP BY. As a result, in the order of execution, QUALIFY is evaluated after window functions.\nSee Business Queries &gt;&gt; Get the latest order from each customer\n\nUNNEST - BigQuery - takes an ARRAY and returns a table with a row for each element in the ARRAY (docs)\n\nGoogle Analytics, Analysis &gt;&gt; Example 17",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-udfs",
    "href": "qmd/sql.html#sec-sql-udfs",
    "title": "SQL",
    "section": "User Defined Functions (UDF)",
    "text": "User Defined Functions (UDF)\n\nMisc\n\nAvailable in SQL Server (Docs1, Docs2), Postgres (Docs), BigQuery (docs), etc.\nKeep a dictionary with the UDFs you’ve created and make sure to share it with any collaborators.\nCan be persistent or temporary\n\nPersistent UDFs can be used across multiple queries, while temporary UDFs only exist in the scope of a single query\n\n\nCreate\n\nExample: temporary udf (BQ)\nCREATE TEMP FUNCTION AddFourAndDivide(x INT64, y INT64)\nRETURNS FLOAT64\nAS (\n  (x + 4) / y\n);\nSELECT\n  val, AddFourAndDivide(val, 2)\nFROM\n  UNNEST([2,3,5,8]) AS val;\nExample: persistent udf (BQ)\nCREATE FUNCTION mydataset.AddFourAndDivide(x INT64, y INT64)\nRETURNS FLOAT64\nAS (\n  (x + 4) / y\n);\n\nSELECT\n  val, mydataset.AddFourAndDivide(val, 2)\nFROM\n  UNNEST([2,3,5,8,12]) AS val;\n\nDelete persistent udf: DROP FUNCTION &lt;udf_name&gt;\nWith Scalar subquery (BQ)\nCREATE TEMP FUNCTION countUserByAge(userAge INT64)\nAS (\n  (SELECT COUNT(1) FROM users WHERE age = userAge)\n);\nSELECT\n  countUserByAge(10) AS count_user_age_10,\n  countUserByAge(20) AS count_user_age_20,\n  countUserByAge(30) AS count_user_age_30;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-loops",
    "href": "qmd/sql.html#sec-sql-loops",
    "title": "SQL",
    "section": "Loops",
    "text": "Loops\n\nMisc\n\nposgres docs for loops\n\nWHILE\n\nExample: Incrementally add to a counter variable\n-- Declare the initial value\nDECLARE @counter INT;\nSET @counter = 20;\n-- Print initial value\nSELECT @counter AS _COUNT;\n-- Create a loop\nBEGIN;\n-- Loop code starting point\nWHILE @counter &lt; 30\nSELECT @counter = @counter + 1;\n-- Loop finish\nEND;\n-- Check the value of the variable\nSELECT @counter AS _COUNT;\n\nCursors (Docs)\n\nRather than executing a whole query at once, it is possible to set up a cursor that encapsulates the query, and then read the query result a few rows at a time.\n\nOne reason for doing this is to avoid memory overrun when the result contains a large number of rows. (However, PL/pgSQL users do not normally need to worry about that, since FOR loops automatically use a cursor internally to avoid memory problems.)\nA more interesting usage is to return a reference to a cursor that a function has created, allowing the caller to read the rows. This provides an efficient way to return large row sets from functions.\n\nExample (article (do not pay attention dynamic sql. it’s for embedding sql in C programs))\nDECLARE\n    cur_orders CURSOR FOR \n        SELECT order_id, product_id, quantity\n        FROM order_details\n        WHERE product_id = 456;\n    product_inventory INTEGER;\nBEGIN\n    OPEN cur_orders;\n    LOOP\n        FETCH cur_orders INTO order_id, product_id, quantity;\n        EXIT WHEN NOT FOUND;\n        SELECT inventory INTO product_inventory FROM products WHERE product_id = 456;\n        product_inventory := product_inventory - quantity;\n        UPDATE products SET inventory = product_inventory WHERE product_id = 456;\n    END LOOP;\n    CLOSE cur_orders;\n    -- do something after updating the inventory, such as logging the changes\nEND;\n\nA table called “products” that contains information about all products, including the product ID, product name, and current inventory. You can use a cursor to iterate through all orders that contain a specific product and update its inventory.\nA cursor called “cur_orders” that selects all order details that contain a specific product ID. We then define a variable called “product_inventory” to store the current inventory of the product.\nInside the loop, we fetch each order ID, product ID, and quantity from the cursor, subtract the quantity from the current inventory and update the products table with the new inventory value.\nFinally, we close the cursor and do something after updating the inventory, such as logging the changes.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-winfun",
    "href": "qmd/sql.html#sec-sql-winfun",
    "title": "SQL",
    "section": "Window Functions",
    "text": "Window Functions\n\nUnlike GROUP BY, keeps original columns after an aggregation\n\n\nAllows you to work with both aggregate and non-aggregate values all at once\n\nBetter performance than using GROUP BY + JOIN to get the same result\n\n\nDespite the order of operations, if you really need to have a window function inside a WHERE clause or GROUP BY clause, you may get around this limitation by using a subquery or a WITH query\n\nExample: Remove duplicate rows\nWITH temporary_employees as\n(SELECT \n  employee_id,\n  employee_name,\n  department,\n  ROW_NUMBER() OVER(PARTITION BY employee_name,\n                                department,\n                                employee_id) as row_count\nFROM Dummy_employees)\n\nSELECT *\nFROM temporary_employees\nWHERE row_count = 1\n\n3 Types of Window Functions\n\n\nLEAD() will give you the row AFTER the row you are finding a value for.\nLAG() will give you the row BEFORE the row you are finding a value for.\nFIRST_VALUE() returns the first value in an ordered, partitioned data output.\n\nGeneral Syntax\n\n\nwindow_function is the name of the window function we want to use (e.g. see above)\n\nexpression is the name of the column that we want the window function operated on.\n\nMay not be necessary depending on what window_function is used\n\nOVER is just to signify that this is a window function\n\nPARTITION BY divides the rows into partitions so we can specify which rows to use to compute the window function\n\npartition_list is the name of the column(s) we want to partition by (i.e. group_by)\n\nORDER BY is used so that we can order the rows within each partition. This is optional and does not have to be specified\n\norder_list is the name of the column(s) we want to order by\n\nROWS (optional; typically not used) used to subset the rows within each partition.\n\nframe_clause defines how much to offset from our current row\nSyntax: ROWS BETWEEN &lt;starting_row&gt; AND &lt;ending_row&gt;\n\nOptions for starting and ending row\n\nUNBOUNDED PRECEDING — all rows before the current row in the partition, i.e. the first row of the partition\n[some #] PRECEDING — # of rows before the current row\nCURRENT ROW — the current row\n[some #] FOLLOWING — # of rows after the current row\nUNBOUNDED FOLLOWING — all rows after the current row in the partition, i.e. the last row of the partition\n\nExamples\n\nROWS BETWEEN 3 PRECEDING AND CURRENT ROW — this means look back the previous 3 rows up to the current row.\nROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING — this means look from the first row of the partition to 1 row after the current row\nROWS BETWEEN 5 PRECEDING AND 1 PRECEDING — this means look back the previous 5 rows up to 1 row before the current row\nROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING — this means look from the first row of the partition to the last row of the partition\n\n\n\n\nExample: Average Salary by Job Title\n\n\nTables for Examples\n\nExample: Average Unit Price for each CustomerId\n\nSELECT CustomerId, \n      UnitPrice, \n      AVG(UnitPrice) OVER (PARTITION BY CustomerId) AS “AvgUnitPrice”\nFROM [Order] \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Average Unit Price for each group of CustomerId AND EmployeeId\n\nSELECT CustomerId, \n      EmployeeId, \n      AVG(UnitPrice) OVER (PARTITION BY CustomerId, EmployeeId) AS “AvgUnitPrice”\nFROM [Order] \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Create a new column that ranks Unit Price in descending order for each CustomerId\n\nSELECT CustomerId, \n      OrderDate, \n      UnitPrice, \n      ROW_NUMBER() OVER (PARTITION BY CustomerId ORDER BY UnitPrice DESC) AS “UnitRank”\nFROM [Order] \nINNER JOIN OrderDetail \nON [Order].Id = OrderDetail.OrderId\n\nSubstituting RANK in place of ROW_NUMBER should produce the same results\nNote that ranks are skipped (e.g. rank 3 for ALFKI) when there are rows with the same rank\n\nIf you don’t want ranks skipped, use DENSE_RANK for the window function\n\n\nExample: Create a new column that provides the previous order date’s Quantity for each ProductId\n\nSELECT ProductId, \n      OrderDate, \n      Quantity, \n      LAG(Quantity) OVER (PARTITION BY ProductId ORDER BY OrderDate) AS \"LAG\"\nFROM [Order] \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\n\nUse LEAD for the following quantity\n\nExample: Create a new column that provides the very first Quantity ever ordered for each ProductId\n\nSELECT ProductId, \n      OrderDate, \n      Quantity, \n      FIRST_VALUE(Quantity) OVER (PARTITION BY ProductId ORDER BY OrderDate) AS \"FirstValue\"\nFROM [Order] \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Calculate a cumulative moving average UnitPrice for each CustomerId\n\nSELECT CustomerId, \n      UnitPrice, \n      AVG(UnitPrice) OVER (PARTITION BY CustomerId \n      ORDER BY CustomerId \n      ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS “CumAvg”\nFROM [Order]\nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Rank customers for each department by amount spent\nSELECT\n    customer_name,\n    customer_id,\n    amount_spent,\n    department_id,\n    RANK(amount_spent) OVER(ORDER BY amount_spent DESC PARTITION BY department_id) AS spend_rank\nFROM employees\nExample: Find the model and year of car that been on lot the longest\nSELECT \nFIRST_VALUE(name) OVER(PARTITION BY model, year ORDER BY date_at_lot ASC) AS oldest_car_name\nmodel,\nyear\nFROM cars\nRunning Totals/Averages (Cumulative Sums)\n\nUses SUM as the window function\n\nJust replace SUM with AVG to get running averages\n\nExample\n\n\nGenerate a new dataset grouped by month, instead of timestamp. (CTE)\n\nOnly include three fields: account_id, occurred_month and total_amount_usd\nOnly computed for the following accounts: 1041 , 1051, 1061, 10141.\n\nCompute a running total ordered by occurred_month, without collapsing the rows in the result set.\n\nDisplay 2 columns: occurred_month and cum_amnt_usd_by_month\n\n\nBecause no partition was specified, the running total is applied on the full dataset and ordered by (ascending) occurred_month\n\nExample running total by grouping variable\n\nUsing previous CTE\n\nCompute a running total by account_id, ordered by occurred_month, and account_id (i.e. a separate running total for each account_id.)\n\nDisplay 3 columns: account_id, occurred_month, and cum_mon_amnt_usd_by_account\n\n\n\nSame as previous example except a partition column (account_id) is added\n\nExample Running total over various window lengths\n\nUsing previous CTE\n\nCompute a 3 months rolling running total using a window that includes the current month.\nCompute a 7 months rolling running total using a window where the current month is always the middle month.\n\n\nFirst case uses 2 PRECEDING rows and the CURRENT_ROW\nSecond case uses 3 PRECEDING rows and 3 FOLLOWING rows and the CURRENT_ROW\n\nExample: Calculate the number consecutive days spent in each country (sqlite)\nwith ordered as (\n  select \n    created,\n    country,\n    lag(country) over (order by created desc)\n      as previous_country\n  from \n    raw\n),\ngrouped as (\n  select \n    country, \n    created, \n    count(*) filter (\n      where previous_country is null\n      or previous_country != country\n    ) over (\n      order by created desc\n      rows between unbounded preceding\n      and current row\n    ) as grp\n  from \n    ordered\n)\nselect\n  country,\n  date(min(created)) as start,\n  date(max(created)) as end,\n  cast(\n    julianday(date(max(created))) -\n    julianday(date(min(created))) as integer\n  ) as days\nfrom \n  grouped\ngroup by\n  country, grp\norder by\n  start desc;\n\nPost\n\nGoes over the code and thought process step-by-step with shows original data and results during intermediate steps\n\nThread\n\nEvidently only sqlite and postgres support filter. Someone in the thread suggest an alternate method.\n\nOutput:\ncountry         start         end           days\nUnited Kingdom  2023-06-08  2023-06-08  0\nUnited States   2019-09-02  2023-05-11  1347\nFrance          2019-08-25  2019-08-31  6\nMadagascar      2019-07-31  2019-08-07  7\nFrance          2019-07-25  2019-07-25  0\nUnited States   2019-05-04  2019-06-30  57\nUnited Kingdom  2018-08-29  2018-09-10  12\nUnited States   2018-08-05  2018-08-10  5",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-ctes",
    "href": "qmd/sql.html#sec-sql-ctes",
    "title": "SQL",
    "section": "Common Table Expressions (CTE)",
    "text": "Common Table Expressions (CTE)\n\nThe result set of a query which exists temporarily and for use only within the context of a larger query. Much like a derived table, the result of a CTE is not stored and exists only for the duration of the query.\nAlso see\n\nWindow Functions &gt;&gt; Running Totals &gt;&gt; Examples\nGoogle, Google Analytics, Analysis &gt;&gt; Examples 12-15, 18, 19\n\nUse Cases\n\nNeeding to reference a derived table multiple times in a single query\nAn alternative to creating a view in the database\nPerforming the same calculation multiple times over across multiple query components\n\nImproves readability and usually no performance difference\n\nPrior to PostgreSQL 12, https://hakibenita.com/be-careful-with-cte-in-postgre-sql , something with the caching mechanism created a bottleneck. Currently, version 13 is the latest, so hopefully not a common problem anymore.\n\nSteps\n\nInitiate a CTE using “WITH”\nProvide a name for the result soon-to-be defined query\nAfter assigning a name, follow with “AS”\nSpecify column names (optional step)\nDefine the query to produce the desired result set\nIf multiple CTEs are required, initiate each subsequent expression with a comma and repeat steps 2-4.\nReference the above-defined CTE(s) in a subsequent query\n\nSyntax\nWITH\nexpression_name_1 AS\n(CTE query definition 1)\n[, expression_name_X AS\n  (CTE query definition X)\n, etc ]\nSELECT expression_A, expression_B, ...\nFROM expression_name_1\nExample\n\nComparison with a “derived” query\n“What is the average monthly cost per campaign for the company’s marketing efforts?”\nUsing CTE workflow\n-- define CTE:\nWITH Cost_by_Month AS\n(SELECT campaign_id AS campaign,\n      TO_CHAR(created_date, 'YYYY-MM') AS month,\n      SUM(cost) AS monthly_cost\nFROM marketing\nWHERE created_date BETWEEN NOW() - INTERVAL '3 MONTH' AND NOW()\nGROUP BY 1, 2\nORDER BY 1, 2)\n\n-- use CTE in subsequent query:\nSELECT campaign, avg(monthly_cost) as \"Avg Monthly Cost\"\nFROM Cost_by_Month\nGROUP BY campaign\nORDER BY campaign\nUsed derived query\n-- Derived\nSELECT campaign, avg(monthly_cost) as \"Avg Monthly Cost\"\nFROM\n    -- this is where the derived query is used\n    (SELECT campaign_id AS campaign,\n      TO_CHAR(created_date, 'YYYY-MM') AS month,\n      SUM(cost) AS monthly_cost\n    FROM marketing\n    WHERE created_date BETWEEN NOW() - INTERVAL '3 MONTH' AND NOW()\n    GROUP BY 1, 2\n    ORDER BY 1, 2) as Cost_By_Month\nGROUP BY campaign\nORDER BY campaign\n\nExample\n\nCount the number of interactions of new users\nSteps\n\nGet new users\nCount interactions\nGet interactions of new users\n\n\nWITH new_users AS (\n    SELECT id\n    FROM users\n    WHERE created &gt;= '2021-01-01'\n),\ncount_interactions AS (\n    SELECT id,\n        COUNT(*) n_interactions\n    FROM interactions\n    GROUP BY id\n),\ninteractions_by_new_users AS (\n    SELECT id,\n        n_interactions\n    FROM new_users\n        LEFT JOIN count_interactions USING (id)\n)\n\nSELECT *\nFROM interactions_by_new_users\nExample\n\nFind the average top Math test score for students in California\nSteps\n\nGet a subset of students (California)\nGet a subset of test scores (Math)\nJoin them together to get all Math test scores from California students\nGet the top score per student\nTake the overall average\n\nDerived Query (i.e. w/o CTE)\nSELECT AVG(score)\nFROM \n  (SELECT students.id, MAX(test_results.score) as score\n  FROM students \n  JOIN schools ON (\n    students.school_id = schools.id AND schools.state = 'CA'\n  )\n  JOIN test_results ON (\n    students.id = test_results.student_id\n    AND test_results.subject = 'math'\n  )\n  GROUP BY students.id) as tmp\nUsing CTE\nWITH\n  student_subset as (\n    SELECT students.id \n    FROM students \n    JOIN schools ON (\n      students.school_id = schools.id AND schools.state = 'CA'\n    )\n  ),\n  score_subset as (\n    SELECT student_id, score \n    FROM test_results \n    WHERE subject = 'math'\n  ),\n  student_scores as (\n    SELECT student_subset.id, score_subset.score\n    FROM student_subset \n    JOIN score_subset ON (\n        student_subset.id = score_subset.student_id\n    )\n  ),\n  top_score_per_student as (\n    SELECT id, MAX(score) as score \n    FROM student_scores \n    GROUP BY id\n  )\n\nSELECT AVG(score) \nFROM top_score_per_student",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-str",
    "href": "qmd/sql.html#sec-sql-str",
    "title": "SQL",
    "section": "Strings",
    "text": "Strings\n\nConcatenate\n\nAlso see Processing Expressions &gt;&gt; NULLs\n“||”\nSELECT 'PostgreSQL' || ' ' || 'Databases' AS result;\n\n    result\n--------------\nPostgreSQL Databases\nCONCAT\nSELECT CONCAT('PostgreSQL', ' ', 'Databases') AS result;\n\n    result\n--------------\nPostgreSQL Databases\nWith NULL values\nSELECT CONCAT('Harry', NULL, 'Peter');\n\n--------------\nHarryPeter\n\n“||” won’t work with NULLs\n\nColumns\nSELECT first_name, last_name, \nCONCAT(first_name,' ' , last_name) \"Full Name\" \nFROM candidates;\n\nNew column, “Full Name”, is created with concatenated columns\n\n\nSplitting (BQ)\nSELECT\n*,\nCASE WHEN ARRAY_LENGTH(SPLIT(page_location, '/')) &gt;= 5 \n          AND\n          CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+')\n          AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN \n                                      ('accessories','apparel','brands','campus+collection','drinkware',\n                                        'electronics','google+redesign',\n                                        'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                        'office','shop+by+brand','small+goods','stationery','wearables'\n                                        )\n                OR\n                LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN \n                                      ('accessories','apparel','brands','campus+collection','drinkware',\n                                        'electronics','google+redesign',\n                                        'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                        'office','shop+by+brand','small+goods','stationery','wearables'\n                                        )\n          )\n          THEN 'PDP'\n          WHEN NOT(CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+'))\n          AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                        'electronics','google+redesign',\n                                        'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                        'office','shop+by+brand','small+goods','stationery','wearables'\n                                        )\n                OR \n                LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                          'electronics','google+redesign',\n                                          'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                          'office','shop+by+brand','small+goods','stationery','wearables'\n                                          )\n          )\n          THEN 'PLP'\n      ELSE page_title\n      END AS page_title_adjusted \nFROM \n  unnested_events\n\nFrom article, gist\nQuery is creating a new categorical column, “page_title_adjusted,” that is “PDP” when a substring in “page_location” is one of a set of words, and “PLP” when it’s not, and the value of page_title otherwise.\nSPLIT splits the string by separator, ‘/’\nCONTAINS_SUBSTR is looking for substring with a “+”\n[SAFE_OFFSET(3)] pulls the 4th substring (think this indexes by 0?)\nAfter it’s been reversed via ARRAY_REVERSE (?)\nELSE says use the value for page_title when length of the substrings after splitting page_location is 5 or less\n“unnested_events” is a CTE",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-arr",
    "href": "qmd/sql.html#sec-sql-arr",
    "title": "SQL",
    "section": "Arrays",
    "text": "Arrays\n\nMisc\n\nPostGres\n\nIndexing Arrays starts at 1, not at 0\n\n\nCreate Array (BQ)\nSELECT ARRAY\n  (SELECT 1 UNION ALL\n  SELECT 2 UNION ALL\n  SELECT 3) AS new_array;\n+-----------+\n| new_array |\n+-----------+\n| [1, 2, 3] |\n+-----------+\n\nSELECT\n  ARRAY\n    (SELECT AS STRUCT 1, 2, 3\n    UNION ALL SELECT AS STRUCT 4, 5, 6) AS new_array;\n+------------------------+\n| new_array              |\n+------------------------+\n| [{1, 2, 3}, {4, 5, 6}] |\n+------------------------+\n\nSELECT ARRAY\n  (SELECT AS STRUCT [1, 2, 3] UNION ALL\n  SELECT AS STRUCT [4, 5, 6]) AS new_array;\n+----------------------------+\n| new_array                  |\n+----------------------------+\n| [{[1, 2, 3]}, {[4, 5, 6]}] |\n+----------------------------+\nCreate a table with Arrays (Postgres)\n\nCREATE TEMP TABLE shopping_cart (\n  cart_id serial PRIMARY KEY,\n  products text ARRAY\n  );\nINSERT INTO\n  shopping_cart(products)\nVALUES\n  (ARRAY['product_a', 'product_b']),\n  (ARRAY['product_c', 'product_d']),\n  (ARRAY['product_a', 'product_b', 'product_c']),\n  (ARRAY['product_a', 'product_b', 'product_d']),\n  (ARRAY['product_b', 'product_d']);\n\n-- alt syntax w/o ARRAY\nINSERT INTO\n  shopping_cart(products)\nVALUES\n  ('{\"product_a\", \"product_d\"}');\n\nAlso see Basics &gt;&gt; Add Data &gt;&gt; Example: chatGPT\n\nSubset an array (postgres)\n\nSELECT\n  cart_id,\n  products[1] AS first_product -- indexing starts at 1\nFROM\n  shopping_cart;\nSlice an array (postgres)\n\nSELECT\n  cart_id,\n  products [1:2] AS first_two_products\nFROM\n  shopping_cart\nWHERE\n  CARDINALITY(products) &gt; 2;\nUnnest an array (postgres)\n\nSELECT\n  cart_id,\n  UNNEST(products) AS products\nFROM\n  shopping_cart\nWHERE\n  cart_id IN (3, 4);\n\nUseful if you want to perform a join\n\nFilter according to items in arrays (postgres)\nSELECT\n  cart_id,\n  products\nFROM\n  shopping_cart\nWHERE\n  'product_c' = ANY (products);\n\nOnly rows with arrays that have “product_c” will be returned\n\nChange array values using UPDATE, SET\n-- update arrays \nUPDATE\n  shopping_cart\nSET\n  products = ARRAY['product_a','product_b','product_e']\nWHERE\n  cart_id = 1;\n\nUPDATE \n  shopping_cart\nSET\n  products[1] = 'product_f'\nWHERE\n  cart_id = 2;\nSELECT\n  *\nFROM\n  shopping_cart\nORDER BY cart_id;\n\nFirst update: all arrays where cart_id == 1 are set to [‘product_a’,‘product_b’,‘product_e’]\nSecond update: all array first values where cart_id == 2 are set to ‘product_f’\n\nInsert array values\n\nARRAY_APPEND - puts value at the end of the array\nUPDATE\n  shopping_cart\nSET\n  products = ARRAY_APPEND(products, 'product_x')\nWHERE\n  cart_id = 1;\n\narrays in product column where cart_id == 1 get “product_x” appended to the end of their arrays\n\nARRAY_PREPEND - puts value at the beginning of the array\nUPDATE \n  shopping_cart\nSET\n  products = ARRAY_PREPEND('product_x', products)\nWHERE\n  cart_id = 2;\n\narrays in product column where cart_id == 2 get “product_x” prepended to the beginning of their arrays\n\n\nARRAY_REMOVE - remove array item\nUPDATE\n  shopping_cart\nSET\n  products = array_remove(products, 'product_e')\nWHERE cart_id = 1;\n\narrays in product column where cart_id == 1 get “product_e” removed from their arrays\n\nARRAY_CONCAT(BQ), ARRAY_CAT(postgres) - Concantenate\nSELECT ARRAY_CONCAT([1, 2], [3, 4], [5, 6]) as count_to_six;\n+--------------------------------------------------+\n| count_to_six                                    |\n+--------------------------------------------------+\n| [1, 2, 3, 4, 5, 6]                              |\n+--------------------------------------------------+\n\n-- postgres\nSELECT\n  cart_id,\n  ARRAY_CAT(products, ARRAY['promo_product_1', 'promo_product_2'])\nFROM shopping_cart\nORDER BY cart_id;\nARRAY_TO_STRING - Coerce to string (BQ)\nWITH items AS\n  (SELECT ['coffee', 'tea', 'milk' ] as list\n  UNION ALL\n  SELECT ['cake', 'pie', NULL] as list)\nSELECT ARRAY_TO_STRING(list, '--') AS text\nFROM items;\n+--------------------------------+\n| text                          |\n+--------------------------------+\n| coffee--tea--milk              |\n| cake--pie                      |\n+--------------------------------+\n\nWITH items AS\n  (SELECT ['coffee', 'tea', 'milk' ] as list\n  UNION ALL\n  SELECT ['cake', 'pie', NULL] as list)\nSELECT ARRAY_TO_STRING(list, '--', 'MISSING') AS text\nFROM items;\n+--------------------------------+\n| text                          |\n+--------------------------------+\n| coffee--tea--milk              |\n| cake--pie--MISSING            |\n+--------------------------------+\nARRAY_AGG - gather values of a group by variable into an array (doc)\n\nMakes the output more readable\nExample: Get categories for each brand\n-- without array_agg\nselect\n    brand,\n    category\nfrom order_item\ngroup by brand, category\norder by brand, category\n;\nResults:\n| brand  | category  | \n| ------ | ---------- | \n| Arket  | jacket    |\n| COS    | shirts    |\n| COS    | trousers  | \n| COS    | vest      |\n| Levi's | jacket    |\n| Levi's | jeans      |\n\n-- with array_agg\nselect\n  brand,\n  array_agg(distinct category) as all_categories\nfrom order_item\ngroup by brand\norder by brand\n;\nResults:\n| brand  | all_categories              | \n| ------ | ---------------------------- | \n| Arket  | ['jacket']                  |\n| COS    | ['shirts','trousers','vest'] |\n| Levi's | ['jacket','jeans']          |\n| Uniqlo | ['shirts','t-shirts','vest'] |\n\nARRAY_SIZE - function takes an array or a variant as input and returns the number of items within the array/variant (doc)\n\nExample: How many categories does each brand have?\nselect\n  brand,\n  array_agg(distinct category) as all_categories,\n  array_size(all_categories) as no_of_cat\nfrom order_item\ngroup by brand\norder by brand\n;\nResults:\n| brand  | all_categories              | no_of_cat |\n| ------ | ---------------------------  | --------- |\n| Arket  | ['jacket']                  | 1        |\n| COS    | ['shirts','trousers','vest'] | 3        |\n| Levi's | ['jacket','jeans']          | 2        |\n| Uniqlo | ['shirts','t-shirts','vest'] | 3        |\n\n-- postgres using CARDINALITY to get array_size\nSELECT\n  cart_id,\n  CARDINALITY(products) AS num_products\nFROM\n  shopping_cart;\n\nARRAY_CONTAINS checks if a variant is included in an array and returns a boolean value. (doc)\n\nVariant is just a specific category\nNeed to cast the item you’d like to check as a variant first\nSyntax: ARRAY_CONTAINS(variant, array)\nExample: What brands have jackets?\nselect\n  brand,\n  array_agg(distinct category) as all_categories,\n  array_size(all_categories) as no_of_cat,\n  array_contains('jacket'::variant,all_categories) as has_jacket\nfrom order_item\ngroup by brand\norder by brand\n;\nResults:\n| brand  | all_categories              | no_of_cat | has_jacket |\n| ------ | ---------------------------  | --------- | ---------- |\n| Arket  | ['jacket']                  | 1        | true      |\n| COS    | ['shirts','trousers','vest'] | 3        | false      |\n| Levi's | ['jacket','jeans']          | 2        | true      |\n| Uniqlo | ['shirts','t-shirts','vest'] | 3        | false      |\n\n-- postgres contains_operator, @&gt;\nSELECT\n  cart_id,\n  products\nFROM\n  shopping_cart\nWHERE\n  products  @&gt; ARRAY['product_a', 'product_b'];\n\n“@&gt;” example returns all rows with arrays containing product_a and product_b",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-bizq",
    "href": "qmd/sql.html#sec-sql-bizq",
    "title": "SQL",
    "section": "Business Queries",
    "text": "Business Queries\n\nSimple Moving Average (SMA)\n\nExample: 7-day SMA including today\nSELECT\n  Date, Conversions,\n  AVG(Conversions) OVER (ORDER BY Date ROWS BETWEEN 6 PRECEDING AND\n  CURRENT ROW) as SMA\nFROM daily_sales\n\nExample: 3-day SMA not including today\nselect\n  date,\n  sales,\n  avg(sales) over (order by date\n        rows between 3 preceding and current row - 1) as moving_avg\nfrom table_daily_sales\nExample: Rank product categories by shipping cost for each shipping address\n\nSELECT Product_Category,\n  Shipping_Address,\n  Shipping_Cost,\n  ROW_NUMBER() OVER\n              (PARTITION BY Product_Category,\n                            Shipping_Address\n              ORDER BY Shipping_Cost DESC) as RowNumber,\n  RANK() OVER \n        (PARTITION BY Product_Category,\n                      Shipping_Address\n        ORDER BY Shipping_Cost DESC) as RankValues,\n  DENSE_RANK() OVER \n              (PARTITION BY Product_Category,\n                            Shipping_Address \n              ORDER BY Shipping_Cost DESC) as DenseRankValues\nFROM Dummy_Sales_Data_v1\nWHERE Product_Category IS NOT NULL\nAND Shipping_Address IN ('Germany','India')\nAND Status IN ('Delivered')\n\nRANK() retrieves ranked rows based on the condition of ORDER BY clause. As you can see there is a tie between 1st two rows i.e. first two rows have same value in Shipping_Cost column (which is mentioned in ORDER BY clause).\nDENSE_RANK is similar to the RANK , but it does not skip any numbers even if there is a tie between the rows. This you can see in Blue box in the above picture.\nRank resets to 1 when “Shipping_Address” changes location\n\nExample: Total order quantity for each month\nSELECT strftime('%m', OrderDate) as Month,\n      SUM(Quantity) as Total_Quantity\nfrom Dummy_Sales_Data_v1\nGROUP BY strftime('%m', OrderDate)\n\nstrftime extracts the month (%m) from the datetime column, “OrderDate”\n\nExample: Daily counts of open jobs\n\nThe issue is that there aren’t rows for transactions that remain in a type of holding status\n\ne.g. Job Postings website has date columns for the date the job posting was created, the date the job posting went live on the website, and the date the job posting was taken down (action based timestamps), but no dates for the status between “went live” and “taken down”.\n\n\n-- create a calendar column\nSELECT parse_datetime('2020–01–01 08:00:00', 'yyyy-MM-dd H:m:s') + (interval '1' day * d) as cal_date from \nFROM ( SELECT\nROW_NUMBER() OVER () -1 as d\nFROM\n(SELECT 0 as n UNION SELECT 1) p0,\n(SELECT 0 as n UNION SELECT 1) p1,\n(SELECT 0 as n UNION SELECT 1) p2,\n(SELECT 0 as n UNION SELECT 1) p3,\n(SELECT 0 as n UNION SELECT 1) p4,\n(SELECT 0 as n UNION SELECT 1) p5,\n(SELECT 0 as n UNION SELECT 1) p6,\n(SELECT 0 as n UNION SELECT 1) p7,\n(SELECT 0 as n UNION SELECT 1) p8,\n(SELECT 0 as n UNION SELECT 1) p9,\n(SELECT 0 as n UNION SELECT 1) p10\n)\n\n-- left-join your table to the calendar column\nSelect\n    c.cal_date,\n    count(distinct opp_id) as \"historical_prospects\"\nFrom calendar c\nLeft Join\n    opportunities o\n    on\n        o.stage_entered ≤ c.cal_date \n        and (o.stage_exited is null or o.stage_exited &gt; c.cal_date)\n\nCalendar column should probably be a CTE\nNotes from Using SQL to calculate trends based on historical status\nSome flavours of SQL have a generate_series function, which will create this calendar column for you\nFor one particular month, then create an indicator column with “if posting_publish_date ≤ 2022–01–01 and (posting_closed_date is null or posting_closed_date &gt; 2022–01–31) then True” and then filter for True and count.\n\nExample: Get the latest order from each customer\n-- Using QUALIFY\nselect\n    date,\n    customer_id,\n    order_id,\n    price\nfrom customer_order_table\nqualify row_number() over (partition by customer_id order by date desc) = 1\n;\n\n-- CTE w/window function\nwith order_order as\n(\nselect\n    date,\n    customer_id,\n    order_id,\n    price,\n    row_number() over (partition by customer_id order by date desc)   \n    as order_of_orders\nfrom customer_order_table \n)\n\nselect\n    *\nfrom order_order\nwhere order_of_orders = 1\n;\nResults:\n| date      | customer_id | order_id | price |\n|------------|-------------|----------|-------|\n| 2022-01-03 | 002        | 212      | 350  |\n| 2022-01-06 | 005        | 982      | 300  |\n| 2022-01-07 | 001        | 109      | 120  |\nMedians\n\nNotes from How to Calculate Medians with Grouping in MySQL\n\nVariables:\n\npid: unique id variable\ncategory: A or B\nprice: random value between 1 and 6\n\n\nExample: Overall median price\nSELECT AVG(sub.price) AS median\nFROM ( \n    SELECT @row_index := @row_index + 1 AS row_index, p.price\n    FROM products.prices p, (SELECT @row_index := -1) r\n    WHERE p.category = 'A'\n    ORDER BY p.price \n) AS sub\nWHERE sub.row_index IN (FLOOR(@row_index / 2), CEIL(@row_index / 2))\n;\n\nmedian|\n------+\n   3.0|\n\n@row_index is a SQL variable that is initiated in the FROM statement and updated for each row in the SELECT statement.\nThe column whose median will be calculated (the price column in this example) should be sorted. It doesn’t matter if it’s sorted in ascending or descending order.\nAccording to the definition of median, the median is the value of the middle element (total count is odd) or the average value of the two middle elements (total count is even). In this example, category A has 5 rows and thus the median is the value of the third row after sorting. The values of both FLOOR(@row_index / 2) and CEIL(@row_index / 2) are 2 which is the third row. On the other hand, for category B which has 6 rows, the median is the average value of the third and fourth rows.\n\nExample: Median price for each product\nSELECT\n    sub2.category,\n    CASE WHEN MOD(sub2.total, 2) = 1 THEN sub2.mid_prices\n         WHEN MOD(sub2.total, 2) = 0 THEN (SUBSTRING_INDEX(sub2.mid_prices, ',', 1) + SUBSTRING_INDEX(sub2.mid_prices, ',', -1)) / 2\n    END AS median    \nFROM \n    (\n        SELECT \n            sub1.category,\n            sub1.total,\n            CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', CEIL(sub1.total/2)), ',', '-1')\n                 WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', sub1.total/2 + 1), ',', '-2')\n            END AS mid_prices\n        FROM \n            (\n                SELECT\n                    p.category,\n                    GROUP_CONCAT(p.price ORDER BY p.price) AS prices,\n                    COUNT(*) AS total\n                FROM products.prices p\n                GROUP BY p.category\n            ) sub1\n    ) sub2\n;\n\ncategory|median|\n--------+------+\nA       |3     |\nB       |3.5   |\n\nBreaking down the subqueries\n\nSort prices per category\nSELECT\n    category,\n    GROUP_CONCAT(price ORDER BY p.price) AS prices,\n    COUNT(*) AS total\nFROM products.prices p\nGROUP BY p.category\n;\n\ncategory|prices     |total|\n--------+-----------+-----+\nA       |1,2,3,4,5  |    5|\nB       |1,2,3,4,5,6|    6|\n\nIf your table has a lot of data, GROUP_CONCAT would not contain all the data. In this case, you increase the limit for GROUP_CONCAT by: SET GROUP_CONCAT_MAX_LEN = 100000;\n\nGet middle prices according to whether the total count is an odd or even number\nSELECT \n    sub1.category,\n    sub1.total,\n    CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', CEIL(sub1.total/2)), ',', '-1')\n         WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', sub1.total/2 + 1), ',', '-2')\n    END AS mid_prices\nFROM \n    (\n        SELECT\n            p.category,\n            GROUP_CONCAT(p.price ORDER BY p.price) AS prices,\n            COUNT(*) AS total\n        FROM products.prices p\n        GROUP BY p.category\n    ) sub1\n;\n\ncategory|total|mid_prices|\n--------+-----+----------+\nA       |    5|3         |\nB       |    6|3,4       |\n\nWe use the MOD function (modulo) to check if the total count is an odd or even number.\nThe SUBSTRING_INDEX function is used twice to extract the middle elements.\n\n\n\nExample: Overall median of price and quantity\nSELECT\n    CASE WHEN MOD(sub2.total, 2) = 1 THEN sub2.mid_prices\n         WHEN MOD(sub2.total, 2) = 0 THEN (SUBSTRING_INDEX(sub2.mid_prices, ',', 1) + SUBSTRING_INDEX(sub2.mid_prices, ',', -1)) / 2\n    END AS median_of_price,\n    CASE WHEN MOD(sub2.total, 2) = 1 THEN sub2.mid_quantities\n         WHEN MOD(sub2.total, 2) = 0 THEN (SUBSTRING_INDEX(sub2.mid_quantities, ',', 1) + SUBSTRING_INDEX(sub2.mid_prices, ',', -1)) / 2\n    END AS median_of_quantity\nFROM \n    (\n        SELECT \n            sub1.total,\n            CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', CEIL(sub1.total/2)), ',', '-1')\n                 WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', sub1.total/2 + 1), ',', '-2')\n            END AS mid_prices,\n            CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.quantities, ',', CEIL(sub1.total/2)), ',', '-1')\n                 WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.quantities, ',', sub1.total/2 + 1), ',', '-2')                 \n            END AS mid_quantities\n        FROM \n            (\n                SELECT\n                    COUNT(*) AS total,\n                    GROUP_CONCAT(o.price ORDER BY o.price) AS prices,\n                    GROUP_CONCAT(o.quantity ORDER BY o.quantity) AS quantities\n                FROM products.orders o\n            ) sub1\n    ) sub2\n;\n\n\nmedian_of_price|median_of_quantity|\n---------------+------------------+\n3              |30                |\n\nSimilar to previous example\nVariables: order_id, price, quantity",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-trans",
    "href": "qmd/sql.html#sec-sql-trans",
    "title": "SQL",
    "section": "Transactions",
    "text": "Transactions\n\nMisc\n\nAlso see\n\nTerms &gt;&gt; Transaction\nDatabase, Warehouses &gt;&gt; Database Triggers - Shows how to efficiently transfer data from a transactional database to a warehouse/relational database by setting up event triggers and staging tables.\n\nWhen the transaction is successful, COMMIT is applied. When the transaction is aborted, incorrect execution, system failure ROLLBACK occurs.\n\nOnly used with INSERT, UPDATE and DELETE\nBEGIN TRANSACTION: It indicates the start point of an explicit or local transaction.\n\nRepresents a point ast which the data referenced by a connection is logically and physically consistent.\nIf errors are encountered, all data modifications made after the BEGIN TRANSACTION can be rolled back to return the data to this known state of consistency\nSyntax: BEGIN TRANSACTION transaction_name ;\n\nSET TRANSACTION: Places a name on a transaction.\n\nSyntax: SET TRANSACTION [ READ WRITE | READ ONLY ];\n\nCOMMIT: used to permanently save the changes done in the transaction in tables/databases. The database cannot regain its previous state after its execution of commit.\n\nIf everything is in order with all statements within a single transaction, all changes are recorded together in the database is called committed. The COMMIT command saves all the transactions to the database since the last COMMIT or ROLLBACK command\nExample: Delete records\nDELETE FROM Student WHERE AGE = 20;\nCOMMIT;\n\nDeletes those records from the table which have age = 20 and then commits the changes in the database.\n\n\nROLLBACK: used to undo the transactions that have not been saved in the database. The command is only been used to undo changes since the last commit\n\nIf any error occurs with any of the SQL grouped statements, all changes need to be aborted. The process of reversing changes is called rollback. This command can only be used to undo transactions since the last COMMIT or ROLLBACK command was issued.\nSyntax: ROLLBACK;\n\nSAVEPOINT: creates points within the groups of transactions in which to ROLLBACK.\n\nSyntax: SAVEPOINT &lt;savepoint_name&gt;;\nA savepoint is a point in a transaction in which you can roll the transaction back to a certain point without rolling back the entire transaction.\nRemove a savepoint: RELEASE SAVEPOINT &lt;savepoint_name&gt;\nExample: Rollback a deletion\nSAVEPOINT SP1;\n//Savepoint created.\nDELETE FROM Student WHERE AGE = 20;\n//deleted\nSAVEPOINT SP2;\n//Savepoint created.\nROLLBACK TO SP1;\n//Rollback completed.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-procexp",
    "href": "qmd/sql.html#sec-sql-procexp",
    "title": "SQL",
    "section": "Processing Expressions",
    "text": "Processing Expressions\n\nUse multiple conditions in a WHERE expression\nselect\n    *\nfrom XXX_table\nwhere 1=1\n    (if condition A) and clause 1 \n    (if condition B) and clause 2 \n    (if condition C) and clause 3\n;\n\nThe “1=1” prevents errors that would occur when the first condition doesn’t apply to any rows.\n\nCan also use “true”\n\n\nSelect unique rows without using DISTINCT\n\nUsing UNION\nSELECT employee_id,\n      employee_name,\n      department\nFROM Dummy_employees\nUNION\nSELECT employee_id,\n      employee_name,\n      department\nFROM Dummy_employees\n\nthere must be same number and order of columns in both the SELECT statements\n\nUsing INTERSECT\nSELECT employee_id,\n      employee_name,\n      department\nFROM Dummy_employees\nINTERSECT\nSELECT employee_id,\n      employee_name,\n      department\nFROM Dummy_employees\n\nThere must be same number and order of columns in both the SELECT statements\n\nUsing ROW_NUMBER\nWITH temporary_employees as (\n  SELECT\n    employee_id,\n    employee_name,\n    department,\n    ROW_NUMBER() OVER(PARTITION BY employee_name,\n                                  department,\n                                  employee_id) as row_count\n  FROM Dummy_employees\n)\n\nSELECT *\nFROM temporary_employees\nWHERE row_count = 1\nUsing GROUP BY\nSELECT employee_id,\n      employee_name,\n      department\nFROM Dummy_employees\nGROUP BY employee_id,\n        employee_name,\n        department\n\nJust need to group by all the columns. Useful to use in conjunction with aggregate functions.\n\n\nCASE WHEN\nSELECT OrderID,\n      OrderDate,\n      Sales_Manager,\n      Quantity,\n      CASE WHEN Quantity &gt; 51 THEN 'High'\n            WHEN Quantity &lt; 51 THEN 'Low'\n            ELSE 'Medium' \n      END AS OrderVolume\nFROM Dummy_Sales_Data_v1\n\nEND AS specifies the name of the new column, “OrderVolume”\nELSE specifies the value when none of the conditions are met\n\nIf you did not mention ELSE clause and no condition is satisfied, the query will return NULL for that specific record\n\n\nPivot Wider\n\nSELECT Sales_Manager,\n      COUNT(CASE WHEN Shipping_Address = 'Singapore' THEN OrderID\n            END) AS Singapore_Orders,\n\n      COUNT(CASE WHEN Shipping_Address = 'UK' THEN OrderID\n            END) AS UK_Orders,\n\n      COUNT(CASE WHEN Shipping_Address = 'Kenya' THEN OrderID\n            END) AS Kenya_Orders,\n\n      COUNT(CASE WHEN Shipping_Address = 'India' THEN OrderID\n            END) AS India_Orders\nFROM Dummy_Sales_Data_v1\nGROUP BY Sales_Manager\n\nDepending on your use-case you can also use different aggregation such as SUM, AVG, MAX, MIN with CASE statement.\n\n\n\nNULLs\n\nDivision and NULLS\n\nAny division with NULL values with have a result of NULL.\nisNull allows to get a different resulting value\nSELECT IsNull(&lt;column&gt;, 0) / 45\n\nAll NULL values in the column will replaced with 0s during the division operation.\n\n\nCOALESCE\n\nSubstitute a default value in place of NULLs\nSELECT COALESCE(column_name, 'Default Value') AS processed_column\nFROM table_name;\n\nSELECT COALESCE(order_date, current_date) AS processed_date\nFROM orders;\n\nSELECT\n  product ||' - '||\n  COALESCE(subcategory, category, family, 'no product description ')\n    AS product_and_subcategory\nFROM stock\n\n3rd Expression: If there is a NULL in subcategory, then it looks in category, then into family, and finally if all those fields have NULLs, it uses “no product description” as the value.\n\nConcantenating Strings where NULLs are present\nSELECT COALESCE(first_name, '') || ' ' || COALESCE(last_name, '') AS full_name\nFROM employees;\n\nNULLs are replaced with an empty string so transformation doesn’t break\n\nPerforming calculations involving numeric columns where there are NULLs\nSELECT COALESCE(quantity, 0) * COALESCE(unit_price, 0) AS total_cost\nFROM products;\n\nSELECT product,\n  quantity_available,\n  minimum_to_have,\n  COALESCE(minimum_to_have, quantity_available * 0.5) AS threshold\nFROM stock\n\nNULLs are substituted with 0s so the calcuation doesn’t break\n\nAs part of a join in case keys have missing values\nSELECT *\nFROM employees e\nLEFT JOIN departments d ON COALESCE(e.department_id, 0) = COALESCE(d.id, 0);\nWith Aggregate Functions\nSELECT department_id, COALESCE(SUM(salary), 0) AS total_salary\nFROM employees\nGROUP BY department_id;\nMake hierarchical subtotals output more readable\n\nSELECT COALESCE(family,'All Families') AS family,\n COALESCE(category,'All Categories') AS category,\n COALESCE(subcategory,'All Subcategories') AS subcategory,\n SUM(quantity_available) as quantity_in_stock\nFROM stock\nGROUP BY ROLLUP(family, category, subcategory)\nORDER BY family, category, subcategory\n\nROLLUP clause assumes a hierarchy among the columns family, category, and subcategory. Thus, it generates all the grouping sets that make sense considering the hierarchy: GROUP BY family, GROUP BY family, category and GROUP BY family, category, subcategory.\n\nThis is the reason why ROLLUP is often used to generate subtotals and grand totals for reports.\n\nWithout COALESCE , the text in the unused columns for the subtotals would be NULLs.\n\n\n\n\n\nDuplicated Rows\n\nUses QUALIFY as a window function to filter out duplicates\n/* removes duplicate rows at the order_id level */\nSELECT * FROM orders\nQUALIFY row_number() over (partition by order_id order by created_at) = 1\n\nMore verbose example of what’s happening\nWITH temporary_employees as \n(SELECT \n  employee_id, \n  employee_name, \n  department, \n  ROW_NUMBER() OVER(PARTITION BY employee_name, \n                                department, \n                                employee_id) as row_count \nFROM Dummy_employees)\n\nSELECT * \nFROM temporary_employees \nWHERE row_count = 1\n\nUse a hash column as id column, then test for duplicates, remove them or investigate them (BigQuery)\n\nWITH\n    inbound_zoo_elephants AS (\n        SELECT *\n        FROM flowfunctions.examples.zoo_elephants\n    ),\n    add_row_hash AS (\n        SELECT\n            *,\n            TO_HEX(MD5(TO_JSON_STRING(inbound_zoo_elephants))) AS hex_row_hash\n        FROM inbound_zoo_elephants\n    )\n\nSELECT\n    COUNT(*) AS records,\n    COUNT(DISTINCT hex_row_hash) AS unique_records\nFROM add_row_hash\n\nNo duplicate records found, since “records” = 9 and “unique_records” = 9\n\nif records &gt; unique_records, duplicates exist\n\nCan select distinct hex_row_hash if you want to remove duplicates\nCan count hex_row_hash then filter where hex_row_hash &gt; 1 to find which rows are duplicates\nNotes from link\nDescription\n\nflowfunctions is the project name\nexamples is a directory (?)\nzoo_elephants is the dataset\n\nSteps\n\nTO_JSON_STRING - creates column with json string for each row\nMD5 hashes that string\nTO_HEX makes it alpha-numeric and gets rid of the symbols in the hash\n\nEasier to deal with in BigQuery\nAssume this is still unique (?)\n\n\nNote: By adding “true” value, TO_JSON_STRING(inbound_zoo_elephants, true) , TO_JSON_STRING adds line breaks to the json string for easier readability.\nHashing function options\n\nMD5 -  shortest one (16 characters), fine for this use case\n\ncryptographically broken, returns 16 characters and suffices for our use-case. Other options are\n\nFARM_FINGERPRINT - returns a signed integer of variable length\nSHA1, SHA256 and SHA512, which return 20, 32 and 64 bytes respectively and are more secure for cryptographic use cases.\n\n\n\n\n\nNested Data\n\nRecursive CTE\n\nRecursive CTEs are used primarily when you want to query hierarchical data or graphs. This could be a company’s organizational structure, a family tree, a restaurant menu, or various routes between cities\nAlso see\n\nWhat Is a Recursive CTE in SQL?\n\nTutorial, 3 examples, and links to other articles\n\n\nSyntax\nWITH RECURSIVE cte_name AS (\n    cte_query_definition (the anchor member)\n    UNION ALL\n    cte_query_definition (the recursive member)\n)\n\nSELECT *\nFROM  cte_name;\nExample: : postgres\nWITH RECURSIVE category_tree(id, name, parent_id, depth, path) AS (\n  SELECT id, name, parent_id, 1, ARRAY[id]\n  FROM categories\n  WHERE parent_id IS NULL\n  UNION ALL\n  SELECT categories.id, categories.name, categories.parent_id, category_tree.depth + 1, path || categories.id\n  FROM categories\n  JOIN category_tree ON categories.parent_id = category_tree.id\n)\n\nSELECT id, name, parent_id, depth, path\nFROM category_tree;\n\nCTE (WITH) + RECURSIVE says it’s a recursive query.\nUNION ALLcombines the results of both statements.\n\nExample is defined by 2 Select statements\n\nAnchor Member: First SELECT statement selects the root nodes of the category tree (nodes with no parent)\n\nRoot node is indicated by “parent_id” = NULL\n\nRecursive member: Second SELECT statement selects the child nodes recursively\n\nAlso see Arrays for further examples of the use of UNION ALL\n\nThe “depth” column is used to keep track of the depth of each category node in the tree.\n\n“1” in the first statement\n“category_tree.depth + 1” in the second statement\n\nWith every recursion, the CTE will add 1 to the previous depth level, and it will do that until it reaches the end of the hierarchy\n\n\nThe “path” column is an array that stores the path from the root to the current node.\n\n“ARRAY[id]” in the first statement\n“path || categories.id” in the second statement\n\n“||” concatenates “path” and “id” columns (See Strings)\n\n\n\n\n\n\n\nBinning\n\nCASE WHEN\n\nSELECT\n Name, \n Grade,\n CASE\n  WHEN Grade &lt; 10 THEN '0-9'\n  WHEN Grade BETWEEN 10 and 19 THEN '10-19'\n  WHEN Grade BETWEEN 20 and 29 THEN '20-29'\n  WHEN Grade BETWEEN 30 and 39 THEN '30-39'\n  WHEN Grade BETWEEN 40 and 49 THEN '40-49'\n  WHEN Grade BETWEEN 50 and 59 THEN '50-59'\n  WHEN Grade BETWEEN 60 and 69 THEN '60-69'\n  WHEN Grade BETWEEN 70 and 79 THEN '70-79'\n  WHEN Grade BETWEEN 80 and 89 THEN '80-89'\n  WHEN Grade BETWEEN 90 and 99 THEN '90-99'\n  END AS Grade_Bucket\n FROM students\n\nBETWEEN is inclusive of the end points\nFlexible for any size of bin you need\n\nFLOOR\nSELECT\n Name,\n Grade,\n FLOOR(Grade / 10) * 10 AS Grade_Bucket\nFROM students\n\nCan easily scale up the number of bins without having to increase the lines of code\nOnly useful for evenly spaced bins\n\nLEFT JOIN on preformatted table\nCREATE OR REPLACE TABLE bins (\n    Lower_Bound INT64,\n    Upper_Bound INT64,\n    Grade_Bucket STRING\n);\n\nINSERT bins (Lower_Bound, Upper_Bound, Grade_Bucket)\nVALUES\n (0, 9, '0-9')\n (10, 19, '10-19')\n (20, 29, '20-29')\n (30, 39, '30-39')\n (40, 49, '40-49')\n (50, 59, '50-59')\n (60, 69, '60-69')\n (70, 79, '70-79')\n (80, 89, '80-89')\n (90, 99, '90-99');\n\nSELECT\n A.Name, \n A.Grade,\n B.Grade_Bucket\nFROM students AS A\nLEFT JOIN bins AS B\nON A.Grade BETWEEN B.Lower_Bound AND B.Upper_Bound\n\n“bins” table acts a template that funnels the values from your table into the correct bins\n\n\n\n\nTime Series\n\nExtract components from date-time columns\n/* MySQL */\nEXTRACT(part_of_date FROM date_time_column_name)\nYEAR(date_time_column_name)\nMONTH(date_time_column_name)\nMONTHNAME(date_time_column_name)\nDATE_FORMAT(date_time_column_name)\n\n/* SQLte */\nSELECT strftime('%m', OrderDate) as Month\n\nstrftime codes\n\n\nPreprocess Time Series with 4 Lags (article)\nWITH top_customers as (\n    --- select the customter ids you want to track\n),\ntransactions as (\n    SELECT \n      cust_id, \n      dt, \n      date_trunc('hour', cast(event_time as timestamp)) as event_hour, \n      count(*) as transactions\n    FROM ourTable\n    WHERE\n        dt between cast(date_add('day', -7, current_date) as varchar) \n        and cast(current_date as varchar)\n    GROUP BY 1,2,3 Order By event_hour asc\n)\n\nSELECT transactions.cust_id,\n      transactions.event_hour,\n      day_of_week(transactions.event_hour) day_of_week,\n        hour(transactions.event_hour) hour_of_day,\n        transactions.transactions as transactions,\n        LAG(transactions,1) OVER \n          (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag1,\n        LAG(transactions,2) OVER \n          (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag2,\n        LAG(transactions,3) OVER \n          (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag3,\n        LAG(transactions,4) OVER \n          (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag4\nFROM transactions \n    join top_customers \n      on transactions.cust_id = top_customers.cust_id\n\n/* output */\n\"cust_id\", \"event_hour\", \"day_of_week\", \"hour_of_day\", \"transactions\", \"lag1\", \"lag2\", \"lag3\", \"lag4\"\n\"Customer-123\",\"2023-01-14 00:00:00.000\",\"6\",\"0\",\"4093\",,,,,,\n\"Customer-123\",\"2023-01-14 01:00:00.000\",\"6\",\"1\",\"4628\",\"4093\",,,,,\n\"Customer-123\",\"2023-01-14 02:00:00.000\",\"6\",\"2\",\"5138\",\"4628\",\"4093\",,,,\n\"Customer-123\",\"2023-01-14 03:00:00.000\",\"6\",\"3\",\"5412\",\"5138\",\"4628\",\"4093\",,,\n\"Customer-123\",\"2023-01-14 04:00:00.000\",\"6\",\"4\",\"5645\",\"5412\",\"5138\",\"4628\",\"4093\",\n\"Customer-123\",\"2023-01-14 05:00:00.000\",\"6\",\"5\",\"5676\",\"5645\",\"5412\",\"5138\",\"4628\",\n\"Customer-123\",\"2023-01-14 06:00:00.000\",\"6\",\"6\",\"6045\",\"5676\",\"5645\",\"5412\",\"5138\",\n\"Customer-123\",\"2023-01-14 07:00:00.000\",\"6\",\"7\",\"6558\",\"6045\",\"5676\",\"5645\",\"5412\",\n\nDataset contains number of transactions made per customer per hour.\n2 WITH clauses: the first just extracts a list of customers we are interested in. Here you can add any condition that is supposed to filter in or out specific customers (perhaps you want to filter new customers or only include customers with sufficient traffic). The second WITH clause simply creates the first data set — Dataset A, which pulls a week of data for these customers and selects the customer id, date, hour, and number of transactions.\nFinally, the last and most important SELECT clause generates Dataset B, by using SQL lag() function on each row in order to capture the number of transactions in each of the hours that preceded the hour in the row.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-tools",
    "href": "qmd/sql.html#sec-sql-tools",
    "title": "SQL",
    "section": "Tools",
    "text": "Tools\n\nChatSQL: Convert plain text to MySQL query by ChatGPT\n{{sqlglot}} - no dependency Python SQL parser, transpiler, optimizer, and engine\n\nFormat SQL or translate between nearly twenty different SQL dialects.\n\nIt doesn’t just transpile active SQL code, too. Moves comments from one dialect to another.\n\nThe parser itself can be customized\nCan also help you analyze queries, traverse parsed expression trees, and incrementally (and, programmatically) build SQL queries.\nsupport for optimizing SQL queries, and performing semantic diffs.\nCan be used to unit test queries through mocks based on Python dictionaries.\nExample: : translate duckdb to hive\nimport sqlglot\nsqlglot.transpile(\n  \"SELECT EPOCH_MS(1618088028295)\", \n  read = \"duckdb\", \n  write = \"hive\"\n)[0]\n---\n'SELECT FROM_UNIXTIME(1618088028295 / 1000)'",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/bayes-reporting.html",
    "href": "qmd/bayes-reporting.html",
    "title": "Reporting",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Bayes",
      "Reporting"
    ]
  },
  {
    "objectID": "qmd/bayes-reporting.html#sec-bayes-rep-misc",
    "href": "qmd/bayes-reporting.html#sec-bayes-rep-misc",
    "title": "Reporting",
    "section": "",
    "text": "Also see Mathematics, Statistics &gt;&gt; Descriptive &gt;&gt; Understanding CI, sd, and sem Bars\n{posterior} rvars class\n\nobject class that’s designed to interoperate with vectorized distributions in {distributional}, to be able to be used inside data.frame()s and tibble()s, and to be used with distribution visualizations in the {ggdist}.\nDocs\n\nRemember CIs of parameter estimates including zero are not evidence of the null hypothesis (i.e. β = 0).\n\nEspecially if CIs are broad and most of the posterior probability distribution is massed away from zero\n\nVisualization for differences (Thread)",
    "crumbs": [
      "Bayes",
      "Reporting"
    ]
  },
  {
    "objectID": "qmd/bayes-reporting.html#sec-bayes-rep-sdfe",
    "href": "qmd/bayes-reporting.html#sec-bayes-rep-sdfe",
    "title": "Reporting",
    "section": "Significant Digits for Estimates",
    "text": "Significant Digits for Estimates\n\nMisc\n\nNotes from: Bayesian workflow book - Digits\n\nBefore we can answer how many chains and iterations we need to run, we need to know how many significant digits we want to report\nMCMC in general doesn’t produce independent draws and the effect of dependency affects how many draws are needed to estimate different expectations\nGuidelines in general\n\nIf the posterior would be close to a normal(μ,1), then\n\nFor 2 significant digit accuracy,\n\n2000 independent draws from the posterior would be sufficient for that 2nd digit to only sometimes vary.\n4 chains with 1000 iterations after warmup is likely to give near two significant digit accuracy for the posterior mean. The accuracy for 5% and 95% quantiles would be between one and two significant digits.\nWith 10,000 draws, the uncertainty is 1% of the posterior scale which would often be sufficient for two significant digit accuracy.\n\nFor 1 significant digit accuracy, 100 independent draws would be often sufficient, but reliable convergence diagnostics may need more iterations than 100.\nFor posterior quantiles, more draws may be needed (need more draws to get values towards the tails of the posterior)\n\nSome quantities of interest may have posterior distribution with infinite variance, and then the ESS and MCSE are not defined for the expectation.\n\nIn such cases, use median instead of mean and mean absolute deviation (MAD) instead of standard deviation.\nVariance of parameter posteriors\nas_draws_rvars(brms_fit) %&gt;%\n    summarise_draws(var = distributional::variance) \n#&gt;    variable  var\n#&gt;    &lt;chr&gt;    &lt;dbl&gt;\n#&gt;  1 mu        11.6\n#&gt;  2 tau      12.8\n#&gt;  3 theta[1]  39.7\n#&gt;  4 theta[2]  21.5\n\n\nSteps\n\nCheck convergence diagnostics for all parameters\n\ne.g. RHat, ESS, autocorrelation plots (see Diagnostics, Bayes)\n\nLook at the posterior for quantities of interest and decide how many significant digits is reasonable taking into account the posterior uncertainty (using SD, MAD, or tail quantiles)\n\nYou want to be able to distinguish you upper or lower CI from the point estimate\n\ne.g. Point estimate is 2.1 and you upper CI is 2.1 then you want at least another significant digit.\n\n\nCheck that MCSE is small enough for the desired accuracy of reporting the posterior summaries for the quantities of interest.\n\nCalculate the range of variation due to MC sampling for your paramter (See MCSE example)\n\nMC sampling error is the average amount of variation that’s expected from changing seeds and re-running the analysis\n\nIf the accuracy is not sufficient (i.e. range is too wide), report less digits or run more iterations.\n\n\nMonte Carlo standard error (MCSE) - uncertainty about a parameter estimate due to MCMC sampling error\n\nPackages\n\n{posterior} is the preferred package for brms objects\n{mcmcse} - methods to calculate MCMC standard errors for means and quantiles using sub-sampling methods. (Different calculation than used by Stan)\nbayestestR::mcse uses Kruschke 2015 method of calculation\n\nExample: brms, MCSE quantiles\n# Coefficient and CI estimates for the \"beta100\" variable\nas_draws_rvars(brms_fit) %&gt;%\n  subset_draws(\"beta100\") %&gt;%\n  summarize_draws(mean, ~quantile(.x, probs = c(0.05, 0.95)))\n#&gt; variable  mean      5%   95%\n#&gt; beta100   1.966  0.673 3.242\n\nas_draws_rvars(brms_fit) %&gt;%\n  subset_draws(\"beta100\") %&gt;% # select variable\n  summarize_draws(mcse_mean, ~mcse_quantile(.x, probs = c(0.05, 0.95)))\n#&gt; variable  mcse_mean  mcse_q5 mcse_q95\n#&gt; beta100       0.013    0.036    0.033\n\nSpecification\n\n“mcse_mean” and “mean” are available as preloaded functions that summary_draws can use out of the box\n“mcse_quantile” (also in {posterior}) and “quantile” are not preloaded functions so they’re called as lambda functions\n\nThese are MCSE values for\n\nthe summary estimate (aka point estimate) which is the mean of the posterior in this case\nAnd the CI values of that summary estimate\n\nTail quantiles will have greater amounts of error sampling in the tails of the posterior than in the bulk (i.e. less accurate tail estimates)\nFewer points, more uncertainty\n\n\nCalculate the range of variation due to Monte Carlo\n\nMultiply the MCSE values by 2, the likely range of variation due to Monte Carlo is ±0.02 for mean and ±0.07 for 5% and 95% quantiles\n\nMultiplying by 2, since I guess they’re assuming a normal distribution posterior, therefore estimate ± 1.96 * SE\n\n\nConclusion for “beta100” coefficient\n\nIf the mean estimate for beta100 is reported as 2 (rounded up from 1.966), then there is unlikely to be any variation in that estimate due to MCMC sampling. (i.e. okay to report the estimate as 2)\n\nThis is because\n\n1.966 + 0.02 = 1.986 which would still be rounded up to 2\n1.966 - 0.02 = 1.946 which would still be rounded up to 2\n\n\n\nDraws and iterations\n\nWith an MCSE in the 100ths (e.g. 0.07), 4 times more iterations would halve the MCSEs\nWith an MCSE in the 1000ths (e.g. 0.007), 64 times more iterations would halve the MCSEs\nMCSEs depend on the quantity type. Continuous quantities (e.g. parameter estimates) have more information than discrete quantities (e.g. indicator values used to calculate probabilities).\n\nFor example, above, the estimate for whether the temperature increase is larger than 4 degrees per century has high ESS, but the indicator variable contains less information (than continuous values) and thus much higher ESS would be needed for two significant digit accuracy.",
    "crumbs": [
      "Bayes",
      "Reporting"
    ]
  },
  {
    "objectID": "qmd/bayes-reporting.html#probabilistic-inference-of-estimates",
    "href": "qmd/bayes-reporting.html#probabilistic-inference-of-estimates",
    "title": "Reporting",
    "section": "Probabilistic Inference of Estimates",
    "text": "Probabilistic Inference of Estimates\n\nMisc\n\nNotes from: Bayesian workflow book - Digits\n\nExample: probability that an estimate is positive\nas_draws_rvars(brms_fit) %&gt;%\n  # binary 1/0, posterior samples &gt; 0\n  mutate_variables(beta0p = beta100 &gt; 0) %&gt;% \n  subset_draws(\"beta0p\") %&gt;% # select variable\n  summarize_draws(\"mean\", mcse = mcse_mean)\n\n#&gt; variable  mean  mcse\n#&gt;   beta0p 0.993 0.001\n99.3% probability the estimate is above zero +/- 0.2% (= 2*MCSE)\nMCSE indicates that we have enough MCMC iterations for practically meaningful reporting that the probability that the variable (e.g. temperature) is increasing (i.e. slope is positive) is larger than 99%\nExample: probability that an estimate &gt; 1,2,3,4\nas_draws_rvars(brms_fit) %&gt;%\n  subset_draws(\"beta100\") %&gt;%\n  # binary 1/0 variable\n  mutate_variables(beta1p = beta100 &gt; 1,\n                  beta2p = beta100 &gt; 2,\n                  beta3p = beta100 &gt; 3,\n                  beta4p = beta100 &gt; 4) %&gt;%\n  subset_draws(\"beta[1-4]p\", regex=TRUE) %&gt;%\n  summarize_draws(\"mean\", mcse = mcse_mean, ESS = ess_mean)\n\n#&gt; variable  mean mcse  ESS\n#&gt;  beta1p 0.896 0.006 3020\n#&gt;  beta2p 0.487 0.008 4311\n#&gt;  beta3p 0.088 0.005 3188\n#&gt;  beta4p 0.006 0.001 3265\nTaking into account MCSEs given the current posterior sample, we can summarise these as\n\np(beta100&gt;1) = 88%–91%,\np(beta100&gt;2) = 46%–51%,\np(beta100&gt;3) = 7%–10%,\np(beta100&gt;4) = 0.2%–1%.\n\nTo get these probabilities estimated with 2 digit accuracy would again require more iterations (16-300 times more iterations depending on the quantity), but the added iterations would not change the conclusion radically.perature in the center of the time range (instead defining prior for temperature at year 0).",
    "crumbs": [
      "Bayes",
      "Reporting"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html",
    "href": "qmd/post-hoc-analysis-anova.html",
    "title": "ANOVA",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-misc",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-misc",
    "title": "ANOVA",
    "section": "",
    "text": "Packages\n\n{car} - Anova function that computes all 3 types of ANOVA table\n\nCan also be applied to glm models to produce Analysis of Deviance tables (e.g. logistic, poisson, etc.)\nThink the other packages wrap this function, so they can be used instead in order to advantage of their plotting, testing conveniences.\n\n{grafify} - ANOVA wrappers, plotting, wrappers for {emmeans}\n{afex} - Analysis of Factorial EXperiments\n\nANOVA helper functions that fit the lm, center, apply contrasts, etc. in one line of code\n\nExample: afex::aov_car(Y ~ group * condition + Error(id), data = d)\nType III used, Factor variables created, Sum-to-Zero contrast is applied\n\nEffect plotting functions\n\n\nNotes from\n\nEverything You Always Wanted to Know About ANOVA\n\nANOVA vs. Regression (GPT-3.5)\n\nDifferent Research Questions:\n\nANOVA is typically used when you want to compare the means of three or more groups to determine if there are statistically significant differences among them. It’s suited for situations where you’re interested in group-level comparisons (e.g., comparing the average test scores of students from different schools).\nRegression, on the other hand, is used to model the relationship between one or more independent variables and a dependent variable. It’s suitable for predicting or explaining a continuous outcome variable.\n\nData Type:\n\nANOVA is traditionally used with categorical independent variables and a continuous dependent variable. It helps assess whether the categorical variable has a significant impact on the continuous variable.\n\nThere are other variants such as ANCOVA (categorical and continuous IVs) and Analysis of Deviance (discrete outcome)\n\nRegression can be used with both categorical and continuous independent variables to predict a continuous dependent variable or to examine the relationship between variables.\n\nMultiple Factors:\n\nANOVA is designed to handle situations with multiple categorical independent variables (factors) and their interactions. It is useful when you are interested in understanding the combined effects of several factors.\nRegression can accommodate multiple independent variables as well, but it focuses on predicting the value of the dependent variable rather than comparing groups.\n\nHypothesis Testing:\n\nANOVA tests for differences in means among groups and provides p-values to determine whether those differences are statistically significant.\nRegression can be used for hypothesis testing, but it’s more often used for estimating the effect size and making predictions.\n\nAssumptions:\n\nANOVA assumes that the groups are independent and that the residuals (the differences between observed values and group means) are normally distributed and have equal variances.\nRegression makes similar assumptions about residuals but also assumes a linear relationship between independent and dependent variables.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-gen",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-gen",
    "title": "ANOVA",
    "section": "General",
    "text": "General\n\nFamily of procedures which summarizes the relationship between the underlying model and the outcome by partitioning the variation in the outcome into components which can be uniquely attributable to different sources according to the law of total variance.\nEssentially, each of the model’s terms is represented in a line in the ANOVA table which answers the question how much of the variation in Y can be attributed to the variation in X?\n\nWhere applicable, each source of variance has an accompanying test statistic (oftenF), sometimes called the omnibus test, which indicates the significance of the variance attributable to that term, often accompanied by some measure of effect size.\n\nOne-Way ANOVA - 1 categorical, independent variable\n\nDetermines whether there is a statistically significant difference in the means of the dependent variable across the different levels of the independent variable.\nExample: A researcher wants to compare the average plant height grown using three different types of fertilizer. They would use a one-way ANOVA to test if there is a significant difference in height between the groups fertilized with each type.\n\nTwo-Way ANOVA - 2 categorical, independent variables\n\nExample: 3 treatments are given to subjects and the researcher thinks that females and males will have different responses in general.\n\nTest whether there are treatment differences after accounting for sex effects\nTest whether there are sex differences after accounting for treatment effects\nTest whether the treatment effect is different for females and males if you allow the treatment \\(\\times\\) sex interaction to be in the model\n\n\nTypes\n\nTL;DR;\n\nI don’t see a reason not to run type III every time.\nType I: Sequential Attribution of Variation\nType II: Simultaneous Attribution of Variation\n\nFor interactions: Sequential-Simultaneous Attribution of Variation\n\nType III: Simultaneous Attribution of Variation for Main Effects and Interactions\nIf the categorical explanatory variables in the analysis are balanced, then all 3 types will give the same results. The results for each variable will be it’s unique contribution.\n\nExample:\n# balanced\ntable(d$Rx, d$condition)\n#&gt;           Ca Cb\n#&gt;   Placebo  5  5\n#&gt;   Dose100  5  5\n#&gt;   Dose250  5  5\n\n# imbalanced\ntable(d$group, d$condition)\n#&gt;      Ca Cb\n#&gt;   Gb  6  6\n#&gt;   Ga  5  6\n#&gt;   Gc  4  3\n\n\nType I: Sequential Sum of Squares\n\nVariance attribution is calculated sequentially so the order of variables in the model matters. Each term is attributed with a portion of the variation (represented by its SS) that has not yet been attributed to any of the previous terms.\nRarely used in practice because the order in which variation is attributed isn’t usually important\nExample: Order of terms matters\nanova(lm(Y ~ group + X, data = d))\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Response: Y\n#&gt;           Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \n#&gt; group      2    8783    4391  0.0918 0.912617   \n#&gt; X          1  380471  380471  7.9503 0.009077 **\n#&gt; Residuals 26 1244265   47856                    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm(Y ~ X + group, data = d))\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Response: Y\n#&gt;           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \n#&gt; X          1  325745  325745  6.8067 0.01486 *\n#&gt; group      2   63509   31754  0.6635 0.52353  \n#&gt; Residuals 26 1244265   47856                  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSum of Squares values change based on the order of the terms in the model\nIn the first model,\n\nThe effect of group does not represent its unique contribution to Y’s variance, but instead its total contribution.\n\nThis reminds me of a dual path DAG where group is influenced by X. Here X’s variance contribution is included in group’s contribution since X is not conditioned upon. (See Causal Inference &gt;&gt; Dual Path DAGs)\n\nThe effect of X represents only what X explains after removing the contribution of group — the variance attributed to X is strictly the variance that can be uniquely attributed to X, controlling for group\n\n\n\nType II: Simultaneous Sum of Squares\n\nThe variance attributed to each variable is its unique contribution — variance after controlling for the other variables. Order of terms does not matter.\nExample\ncar::Anova(m, type = 2)\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: Y\n#&gt;            Sum Sq Df F value   Pr(&gt;F)   \n#&gt; group       63509  2  0.6635 0.523533   \n#&gt; X          380471  1  7.9503 0.009077 **\n#&gt; Residuals 1244265 26                    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSum of Squares values are equal to values of the Type 1 results when each variable is last.\nNote that factor variables, e.g. group, are treated as 1 term and not broken down into dummy variables for each level.\n\nWith interactions, the method of calculation could be called, Sequential-Simultaneous.\n\nTerms are evaluated simultaneously in groups based on type of term, e.g. main effects, 2-way interactions, 3-way interactions, etc., but sequentially according to the order of that term where the order of main effects &lt; 2-way interactions &lt; 3-way interactions, etc.\nAll main effects (1st order) are tested simultaneously (accounting for one another), then all 2-way interactions (2nd order) are tested simultaneously (accounting for the main effects and one another), and finally the 3-way interaction is tested (accounting for all main effects and 2-way interactions).\nSo, if you use this way to test a model with interactions, only the highest order term’s Sum of Squares represents a unique variance contribution.\n\n\nType III: Simultaneous-Simultaneous Sum of Squares\n\nThe Sum-of-Squares for each main effect and interaction is calculated as its unique contribution (i.e. takes into account all other terms of the model).\nUnlike Type II, it allows you compare variance contributions for every term in your model.\nWithout centering continuous variables and applying sum-to-zero contrasts to categorical variables, tests results can change depending on the categorical level of the moderator. (Also see Regression, Linear &gt;&gt; Contrasts &gt;&gt; Sum-to-Zero)\n\nExample\n\nNo Centering, No Sum-to-Zero Contrasts\nm_int &lt;- lm(Y ~ group * X, data = d)\n\nd$group &lt;- relevel(d$group, ref = \"Gb\")\nm_int2 &lt;- lm(Y ~ group * X, data = d)\n\ncar::Anova(m_int, type = 3)\n#&gt;             Sum Sq Df F value    Pr(&gt;F)    \n#&gt; (Intercept) 538630  1 22.9922 6.994e-05 ***\n#&gt; group       738108  2 15.7536 4.269e-05 ***\n#&gt; X           101495  1  4.3325   0.04823 *  \n#&gt; group:X     682026  2 14.5566 7.246e-05 ***\n#&gt; Residuals   562240 24     \n\ncar::Anova(m_int2, type = 3)\n#&gt;             Sum Sq Df F value    Pr(&gt;F)    \n#&gt; (Intercept) 219106  1  9.3528  0.005402 ** \n#&gt; group       738108  2 15.7536 4.269e-05 ***\n#&gt; X           910646  1 38.8722 1.918e-06 ***\n#&gt; group:X     682026  2 14.5566 7.246e-05 ***\n#&gt; Residuals   562240 24  \n\nThe sum of squares and p-value change for X when the categorical variable’s reference level changed which shouldn’t matter given this is an omnibus test (i.e. the categorical variable is treated as 1 entity and not set of dummy variables).\n\nCentered, Sum-to-Zero Contrasts Applied\n# center, contr.sum\nd_contr_sum &lt;- d |&gt; \n  mutate(X_c = scale(X, scale = FALSE))\ncontrasts(d_contr_sum$group) &lt;- contr.sum\nm_int_cont_sum &lt;- lm(Y ~ group * X_c, data = d_contr_sum)\ncar::Anova(m_int_cont_sum, type = 3)\n#&gt;              Sum Sq Df  F value    Pr(&gt;F)    \n#&gt; (Intercept) 4743668  1 202.4902 3.401e-13 ***\n#&gt; group         19640  2   0.4192   0.66231    \n#&gt; X_c          143772  1   6.1371   0.02067 *  \n#&gt; group:X_c    682026  2  14.5566 7.246e-05 ***\n#&gt; Residuals    562240 24\n\n# change reference level\nd_rl &lt;- d_contr_sum |&gt; \n  mutate(group_rl = relevel(group, ref = \"Gb\"))\ncontrasts(d_rl$group_rl) &lt;- contr.sum\ncar::Anova(lm(Y ~ group_rl * X_c, data = d_rl),\n           type = 3)\n#&gt;               Sum Sq Df  F value    Pr(&gt;F)    \n#&gt; (Intercept)  4743668  1 202.4902 3.401e-13 ***\n#&gt; group_rl       19640  2   0.4192   0.66231    \n#&gt; X_c           143772  1   6.1371   0.02067 *  \n#&gt; group_rl:X_c  682026  2  14.5566 7.246e-05 ***\n#&gt; Residuals     562240 24   \n\nNow when the reference level is changed, the sum-of-squares and p-value for X remain the same.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#assumptions",
    "href": "qmd/post-hoc-analysis-anova.html#assumptions",
    "title": "ANOVA",
    "section": "Assumptions",
    "text": "Assumptions\n\nEach group category has a normal distribution.\nEach group category is independent of each other and identically distributed (iid)\nGroup categories have of similar variance (i.e. homoskedastic variance)\n\nIf this is violated\n\nIf the ratio of the largest variance to the smallest variance is less than 4, then proceed with one-way ANOVA (robust to small differences)\nIf the ratio of the largest variance to the smallest variance is greater than 4, perform a Kruskal-Wallis test. This is considered the non-parametric equivalent to the one-way ANOVA. (example)\n\nEDA\ndata %&gt;%\n  group_by(program) %&gt;%\n  summarize(var=var(weight_loss))\n#&gt; A tibble: 3 x 2\n#&gt;   program  var   \n#&gt; 1 A      0.819\n#&gt; 2 B      1.53 \n#&gt; 3 C      2.46\nPerform a statisical test to see if these variables are statistically significant (See Post-Hoc Analysis, Difference-in-Means &gt;&gt; EDA &gt;&gt; Tests for Equal Variances)",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-math",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-math",
    "title": "ANOVA",
    "section": "Mathematics",
    "text": "Mathematics\n\nAsides:\n\nThis lookd like the variance formula except for not dividing by the sample size to get the “average” squared distance\nSSA formula - the second summation just translates to multiplying by ni, the group category sample size, since there is no j in that formula\n\nCalculate SSA and SSE\n\\[\n\\begin{align}\n\\text{SST} &= \\text{SSA} + \\text{SSE} \\\\\n&= \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (x_{i,j} - \\mu)^2 \\\\\n&= \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (\\bar x_i - \\mu)^2 + \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (x_{i,j} - \\bar x_i)^2\n\\end{align}\n\\]\n\n\\(\\text{SST}\\): Sum of Squares Total\n\\(\\text{SSA}\\): Sum of Squares between categories, treatments, or factors\n\n“A” stands for attributes (i.e. categories)\n\n\\(\\text{SSE}\\): Sum of Squares of Errors; randomness within categories, treatments, or factors\n\\(x_{ij}\\): The jth observation of the ith category\n\\(\\bar x_i\\): The sample mean of category i\n\\(\\mu\\): The overall sample mean\n\\(n_i\\): The group category sample size\n\\(a\\): The number of group categories\n\nCalculate MSA and MSE\n\\[\n\\begin{align}\n\\text{MSE} &= \\frac{\\text{SSE}}{N-a} \\\\\n\\text{MSA} &= \\frac{\\text{SSA}}{a-1}\n\\end{align}\n\\]\n\nWhere N is the total sample size\n\nCalculate the F statistic and P-Value\n\\[\nF = \\frac{\\text{MSA}}{\\text{MSE}}\n\\]\n\nFind the p-value (need a table to look it up)\nIf our F statistic is less than the critical value F statistic for a \\(\\alpha = 0.05\\) than we cannot reject the null hypothesis (no statistical difference between categories)\n\nDiscussion\n\nIf there is a group category that has more variance than the others’ attribute error (SSA), we should then pick that up when we compare it to the random error (SSE)\n\nIf a group is further away from the overall mean, then it will increase SSA and thus influence the overall variance but might not always increase random error",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#diagnostics",
    "href": "qmd/post-hoc-analysis-anova.html#diagnostics",
    "title": "ANOVA",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nEta Squared\n\nMetric to describe the effect size of a variable\nRange: [0, 1]; values closer to 1 indicating that a specific variable in the model can explain a greater fraction of the variation\nlsr::etaSquared(anova_model) (use first column of output)\nGuidelines\n\n0.01: Effect size is small.\n0.06: Effect size is medium.\nLarge effect size if the number is 0.14 or above\n\n\nPost-ANOVA Tests\n\nAssume approximately Normal distributions\nFor links to more details about each test, https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/post-hoc/\nDuncan’s new multiple range test (MRT)\n\nWhen you run Analysis of Variance (ANOVA), the results will tell you if there is a difference in means. However, it won’t pinpoint the pairs of means that are different. Duncan’s Multiple Range Test will identify the pairs of means (from at least three) that differ. The MRT is similar to the LSD, but instead of a t-value, a Q Value is used.\n\nFisher’s Least Significant Difference (LSD)\n\nA tool to identify which pairs of means are statistically different. Essentially the same as Duncan’s MRT, but with t-values instead of Q values.\n\nNewman-Keuls\n\nLike Tukey’s, this post-hoc test identifies sample means that are different from each other. Newman-Keuls uses different critical values for comparing pairs of means. Therefore, it is more likely to find significant differences.\n\nRodger’s Method\n\nConsidered by some to be the most powerful post-hoc test for detecting differences among groups. This test protects against loss of statistical power as the degrees of freedom increase.\n\nScheffé’s Method\n\nUsed when you want to look at post-hoc comparisons in general (as opposed to just pairwise comparisons). Scheffe’s controls for the overall confidence level. It is customarily used with unequal sample sizes.\n\nTukey’s Test\n\nThe purpose of Tukey’s test is to figure out which groups in your sample differ. It uses the “Honest Significant Difference,” a number that represents the distance between groups, to compare every mean with every other mean.\n\nDunnett’s Test\n\nLike Tukey’s this post-hoc test is used to compare means. Unlike Tukey’s, it compares every mean to a control mean.\n{DescTools::DunnettTest}\n\nBenjamin-Hochberg (BH) Procedure\n\nIf you perform a very large amount of tests, one or more of the tests will have a significant result purely by chance alone. This post-hoc test accounts for that false discovery rate.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-oneway",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-oneway",
    "title": "ANOVA",
    "section": "One-Way",
    "text": "One-Way\n\nMeasures if there’s a difference in means between any group category\nExample: 1 control, 2 Test groups\n\nData\ndata &lt;- data.frame(Group = rep(c(\"control\", \"Test1\", \"Test2\"), each = 10),\nvalue = c(rnorm(10), rnorm(10),rnorm(10)))\ndata$Group&lt;-as.factor(data$Group)\nhead(data)\n#&gt;   Group      value\n#&gt; 1 control  0.1932123\n#&gt; 2 control -0.4346821\n#&gt; 3 control  0.9132671\n#&gt; 4 control  1.7933881\n#&gt; 5 control  0.9966051\n#&gt; 6 control  1.1074905\nFit model\nmodel &lt;- aov(value ~ Group, data = data)\nsummary(model)\n#&gt;             Df    Sum Sq   Mean Sq  F value  Pr(&gt;F) \n#&gt; Group        2     4.407    2.2036     3.71  0.0377 *\n#&gt; Residuals   27    16.035    0.5939\n\n# or\nlm_mod &lt;- lm(value ~ Group, data = data)\nanova(lm_mod)\n\nP-Value &lt; 0.05 says at least 1 group category has a statistically significant different mean from another category\n\nDunnett’s Test\nDescTools::DunnettTest(x=data$value, g=data$Group)\n\n#&gt; Dunnett's test for comparing several treatments with a control : \n#&gt;     95% family-wise confidence level\n#&gt; $control\n#&gt;                     diff    lwr.ci      upr.ci  pval   \n#&gt; Test1-control -0.8742469 -1.678514 -0.06998022 0.0320 * \n#&gt; Test2-control -0.7335283 -1.537795  0.07073836 0.0768 .\n\nMeasures if there is any difference between treatments and the control\nThe mean score of the test1 group was significantly higher than the control group. The mean score of the test2 group was not significantly higher than the control group.\n\nTukey’s HSD\nstats::TukeyHSD(model, conf.level=.95)\n\nMeasures difference in means between all categories and each other",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ancova",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ancova",
    "title": "ANOVA",
    "section": "ANCOVA",
    "text": "ANCOVA\n\nAnalysis of Covariance is used to measure the main effect and interaction effects of categorical variables on a continuous dependent variable while controlling the effects of selected other continuous variables which co-vary with the dependent variable.\nMisc\n\nAnalysis of covariance is classical terminology for linear models but we often use the term for nonlinear models (Harrell)\nSee also\n\nHarrell - Biostatistics for Biomedical Research Ch. 13\n\n\nAssumptions\n\nIndependent observations (i.e. random assignment, avoid is having known relationships among participants in the study)\nLinearity: the relation between the covariate(s) and the dependent variable must be linear.\nNormality: the dependent variable must be normally distributed within each subpopulation. (only needed for small samples of n &lt; 20 or so)\nHomogeneity of regression slopes: the beta-coefficient(s) for the covariate(s) must be equal among all subpopulations. (regression lines for these individual groups are assumed to be parallel)\n\nFailure to meet this assumption implies that there is an interaction between the covariate and the treatment.\nThis assumption can be checked with an F test on the interaction of the independent variable(s) with the covariate(s).\n\nIf the F test is significant (i.e., significant interaction) then this assumption has been violated and the covariate should not be used as is.\nA possible solution is converting the continuous scale of the covariate to a categorical (discrete) variable and making it a subsequent independent variable, and then use a factorial ANOVA to analyze the data.\n\n\nThe covariate (adjustment variable) and the treatment are independent\nmodel &lt;- aov(grade ~ technique, data = data)\nsummary(model)\n\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; technique    2    9.8    4.92    0.14  0.869\n#&gt; Residuals  87 3047.7  35.03\n\nH0: variables are independent\n\n\nHomogeneity of variance: variance of the dependent variable must be equal over all subpopulations (only needed for sharply unequal sample sizes)\n# response ~ treatment\nleveneTest(exam ~ technique, data = data)\n\n#&gt;       Df F value    Pr(&gt;F)   \n#&gt; group  2  13.752 6.464e-06 ***\n#&gt;       87\n\n# alt test\nfligner.test(size ~ location, my.dataframe)\n\nH0: Homogeneous variance\nThis one fails\n\nFit\nancova_model &lt;- aov(exam ~ technique + grade, data = data)\ncar::Anova(ancova_model, type=\"III\")\n\n#&gt;                 Sum Sq Df F value    Pr(&gt;F)   \n#&gt;     (Intercept) 3492.4  1 57.1325 4.096e-11 ***\n#&gt;     technique  1085.8  2  8.8814 0.0003116 ***\n#&gt;     grade          4.0  1  0.0657 0.7982685   \n#&gt;     Residuals  5257.0 86\n\nWhen adjusting for current grade (covariate), study technique (treatment) has a significant effect on the final exam score (response).\n\nDoes the effect differ by treatment\npostHocs &lt;- multicomp::glht(ancova_model, linfct = mcp(technique = \"Tukey\"))\nsummary(postHocs)\n\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   \n#&gt; B - A == 0   -5.279      2.021  -2.613  0.0284 * \n#&gt; C - A == 0    3.138      2.022   1.552  0.2719   \n#&gt; C - B == 0    8.418      2.019   4.170  &lt;0.001 ***\n\nAlso see Post-Hoc Analysis, Multilevel &gt;&gt; Tukey’s Test\n\\(A\\), \\(B\\), and \\(C\\) are the study techniques (treatment)\nSignificant differences between \\(B\\) and \\(A\\) and a pretty large difference between \\(B\\) and \\(C\\).\n\nExample: RCT\n\\[\n\\begin{align}\n\\text{post}_i &\\sim \\mathcal{N}(\\mu_i, \\sigma_\\epsilon)\\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{tx}_i + \\beta_2 \\text{pre}_i\n\\end{align}\n\\]\nw2 &lt;- glm(\n  data = dw,\n  family = gaussian,\n  post ~ 1 + tx + pre)\n\nSpecification\n\npost, pre: The post-treatment and pre-treatment measurement of the outcome variable\ntx: The treatment indicator variable\n\\(\\beta_0\\): Population mean for the outcome variable in the control group\n\\(\\beta_1\\): Parameter is the population level difference in pre/post change in the treatment group, compared to the control group.\n\nAlso a causal estimate for the average treatment effect (ATE) in the population, τ\n\nBecause pre is added as a covariate, both \\(\\beta_0\\) and \\(\\beta_1\\) are conditional on the outcome variable, as collected at baseline before random assignment.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html",
    "href": "qmd/visualization-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-misc",
    "href": "qmd/visualization-general.html#sec-vis-gen-misc",
    "title": "General",
    "section": "",
    "text": "Notes from\n\nFriends Don’t Let Friends\n\nMicrosoft Paint 3D\n\nLocation: Start &gt;&gt; All Programs &gt;&gt; Paint 3D\nHightlight Text\n\nClick 2D Shapes (navbar) &gt;&gt; Select square (side panel)\nLeft click and hold &gt;&gt; Extend area around text you want to highlight &gt;&gt; Release\nChoose Line Type color and Sticker Opacity level (37%)\nOn area surrrounding text\n\nIf needed, make area size adjustment dragging little box-shaped icons that are along the outside\nOn the right side, click the check mark icon to finalize\n\nClick Menu (left-side on navbar) &gt;&gt; save as &gt;&gt; Image\n\nIt adds a png extension, but you just need to type the name.\n\n\n\nAlt Text\n\nThe guiding principle is to write alt text that gives disabled readers as close to the same experience as nondisabled readers as possible.\n\nggplot2\n\nDon’t use stat calculating geoms and set axis limits with scale_y_continuous\n\n\nSee examples of the behavior in this thread\n\nDefaults for any {ggplot2} geom using the default_aes field (i.e. GeomBlah$default_aes )\n\nFractional Data\n\nUse Stacked Bars instead of Pie or Circular or Donut\n\nHumans are better at judging lengths than angles (article)\n\n\nFactorial Experiments\n\nDon’t use bars factorial experiments\n\nCheck outcome ranges by group when facetting",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-concepts",
    "href": "qmd/visualization-general.html#sec-vis-gen-concepts",
    "title": "General",
    "section": "Concepts",
    "text": "Concepts\n\nExploration and Analysis\n\nGoal: explore a new dataset, gertan overview, find answers to specific questions\nFast iteration of many generic charts, don’t customize or worry about color schemes, etc.\n\nExplanation\n\nGoal: help others understand a relationship in the data\nUse as few charts as possible, carefully chosen\nSequence so that they are easy to understand\nAdd interaction to help people get a better understanding\n\nPresentation\n\nGoal: walk your audience through an argument, help them come to a decision\nFocus on polishing charts: colors, legends, titles, etc.\nHighlighting of key elements (which might be considered biasing in Exploration)\nPossibly use of unusual charts for memorability\nSequence to make a specific point",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-svg",
    "href": "qmd/visualization-general.html#sec-vis-gen-svg",
    "title": "General",
    "section": "SVG",
    "text": "SVG\n\nBetter for doing post-processing in Inkscape and gimp\nSVGs won’t be pixelated when you zoom in like PNGs are\nD3 outputs SVG\nsvglite PKG\n\nusing svglite instead of base::svg( ) allows you alter text in Inkscape or Illustrator\nrequires the used fonts to be present on the system it is viewed on.\n\nThe vast majority of interactive data visualizations on the web are now based on D3.js which often renders to SVG and it all seems to behave. Still, this is something to be mindful of, and a reason to use svg() if exactness of the rendered text is of prime importance\n\nFile size will be dramatically smaller",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-layout",
    "href": "qmd/visualization-general.html#sec-vis-gen-layout",
    "title": "General",
    "section": "Layout",
    "text": "Layout\n\nFacetting vs Single Graph\n\nLayout based on experiement design\n\nAlign title ALL the way to the left (ggplot: plot.title.position = “plot”)\nremove legends\n\nuse colored text in title (ggtext)\nlabel points or lines\nlast resort: place legend underneath title/subtitle\n\ngrid lines\n\nremove if possible\nsparse and faint if needed\n\naxis labels\n\nremove if obvious (e.g brands of cars)\ncreate a title that informs about the axis labels\nshould always be horizontal\n\nflip axis, don’t angle them 45 degrees\n\n\ntext\n\nleft-align most text\ncan center a subtitle if it helps with making the graph more symmetrical\nsome labels can be right-aligned\n\nRemove all borders\nMaximize white space\n\ndon’t cram visuals together\n\nWorking memory. A cognitive limitation that affects plot comprehension is the limit on working memory. Typically, working memory is limited to approximately seven (plus or minus two) items, or chunks. In practice, this means that categorical scales with more than seven categories decrease readability, increase comprehension time, and require significant attentional resources, because it is not possible to hold the legend mapping in working memory.\nThe use of redundant aesthetics that activate the same gestalt principles (such as color and shape in a scatter plot, which both activate similarity) results in higher identification of corresponding data features. In addition, dual encoding increases the accessibility of a chart to individuals who have impaired color vision or perceptual processing (e.g., dyslexia, dysgraphia). This experimental evidence directly contradicts the guidelines popularized by Tufte (1991), which suggest the elimination of any feature that is not dedicated to representing the core data, including redundant encoding and other unnecessary graphical elements.\nggplot themes\n\nCedric Sherer (article)\ntheme_set(theme_minimal(base_size = 15, base_family = \"Anybody\"))\ntheme_update(\n  axis.title.x = element_text(margin = margin(12, 0, 0, 0), color = \"grey30\"),\n  axis.title.y = element_text(margin = margin(0, 12, 0, 0), color = \"grey30\"),\n  panel.grid.minor = element_blank(),\n  panel.border = element_rect(color = \"grey45\", fill = NA, linewidth = 1.5),\n  panel.spacing = unit(.9, \"lines\"),\n  strip.text = element_text(size = rel(1)),\n  plot.title = element_text(size = rel(1.4), face = \"bold\", hjust = .5),\n  plot.title.position = \"plot\"\n)",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-ar",
    "href": "qmd/visualization-general.html#sec-vis-gen-ar",
    "title": "General",
    "section": "Aspect Ratio",
    "text": "Aspect Ratio\n\nMisc\n\nGolden Rectangle\n```{{{r, fig.width = 6, fig.asp = 1.618}}}\n```\nGet consistent outputs\n\nRStudio pane displays in 72dpi which can mislead you on what your output looks like.\nThink {ragg} is supposed to have taken care of the inconsistency in terms of printing on different OSes\nUsing {camcorder}\n\nStart “recording” plots\ncamcorder::gg_record(\n  dir = \"imgs\",\n  width = 12,\n  height = 12*9/16,\n  dpi = 300,\n  bg = \"white\"  # Makes sure background is actually white an not transparent\n)\n\nAll plots will immediately be exported as a .png-file to the directory specified\nAll plots will be displayed in the viewer with dimensions and resolution that you specified and not in the plots pane in RStudio\n300 dpi is pretty standard and default of ggsave\n\nDo work. Export final png file in directory when done and delete the rest\nRegarding Fonts\n\nIf using {ragg}, then all is fine.\nIf using {showtext}, then you have to set resolution in options, showtext_opts(dpi = 300)\n\n\n\n\nTwitter\n\nVideo: 1105 x 1920\n\nLine Charts\n\nMatters most if two different line charts are being compared\n\nThe core idea of “banking” is that the slopes in a line chart are most readable if they average to 45°.\nUse ggthemes::bank_slopes(x, y, method = c(\"ms\", \"as\"))\n\n2 methods (that req. no optimization) from Jeer, Maneesh who followed Cleveland’s 45° guideline\ndocs\n\n“The problem with banking is that sometimes you need the chart in a certain aspect ratio to fit into a page layout. Especially if banking produces portrait sized charts. But why not let the optimal chart ratio define your layout? For instance, you can put the additional information to the side of the chart. Remember that the main goal of banking is to increase the readability of the line slopes. In the following example, the slopes for Nuclear and Renewables would have been much more difficult to see, if the chart would have been ‘squeezed’ to a landscape aspect.” (article)",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-typo",
    "href": "qmd/visualization-general.html#sec-vis-gen-typo",
    "title": "General",
    "section": "Typography",
    "text": "Typography\n\nCSS Length Units\n\nAbsolute Lengths\n\n* Pixels (px) are relative to the viewing device. For low-dpi devices, 1px is one device pixel (dot) of the display. For printers and high resolution screens 1px implies multiple device pixels.\n\n\ncm\ncentimeters\n\n\nmm\nmillimeters\n\n\nin\ninches (1in = 96px = 2.54cm)\n\n\npx*\npixels (1px = 1/96th of 1in)\n\n\npt\npoints (1pt = 1/72 of 1in)\n\n\npc\npicas (1pc = 12 pt)\n\n\n\nRelative Lengths\n\nThe em and rem units are practical in creating perfectly scalable layout! * Viewport = the browser window size. If the viewport is 50cm wide, 1vw = 0.5cm.\n\n\n\n\n\n\nem\nRelative to the font-size of the element (2em means 2 times the size of the current font)\n\n\nex\nRelative to the x-height of the current font (rarely used)\n\n\nch\nRelative to the width of the “0” (zero)\n\n\nrem\nRelative to font-size of the root element\n\n\nvw\nRelative to 1% of the width of the viewport*\n\n\nvh\nRelative to 1% of the height of the viewport*\n\n\nvmin\nRelative to 1% of viewport’s* smaller dimension\n\n\nvmax\nRelative to 1% of viewport’s* larger dimension\n\n\n%\nRelative to the parent element\n\n\n\n\nFont Weight\n\n400 is the same as normal, and 700 is the same as bold\n\nFonts\n\nAdelle\n\nA serif font that doesn’t go overboard. Good for short paragraphs.\n\nAlegreya\nBarlow\n\nSlender font\n\nFira Code Retina\n\ncode syntax highlighting\n@import url(“https://cdn.rawgit.com/tonsky/FiraCode/1.205/distr/fira_code.css”);\n\nLora\n\nbody\nUsed in COVID-19 project &gt;&gt; Static Charts, Hospitals\n@import url(‘https://fonts.googleapis.com/css2?family=Lora&display=swap’);\n\nMerriweather\n\nSimilar to Adelle, but has a bit more pronounced hooks\n\nMontserrat\n\nSimple design that can handle long lines of text. I like it for minimal plots.\n\nPrata\n\nheader\nUsed in ericbook-distill\n@import url(‘https://fonts.googleapis.com/css2?family=Cinzel&display=swap’);\n\nReforma family\n\nonly one I have is Roboto, need to import and load the rest using extrafont pkg\n\nRoboto family\n\nDancho shiny apps\n\np, body: 100 wt\nHeaders, (h1, h2, etc.): 400 wt\n\nRoboto Slab\n\nNot sure if this is exact font used but it’s very similar. Only difference I spotted was the “3.”\n\n\nTitillium Web Bold\n\nheaders\nUsed in ebtools\n@import url(‘https://fonts.googleapis.com/css?family=Titillium+Web&display=swap’);\n\n\nNumbers\n\nshould all have the same height (Lining)\nshould all have the same width (Tabular)\n\nUsing {showtext}\nlibrary(showtext)\n#load font\nfont_add_google(name = \"Metal Mania\", family = \"metal\")\nfont_add_google(name = \"Montserrat\", family = \"montserrat\")\nshowtext_auto()",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-annot",
    "href": "qmd/visualization-general.html#sec-vis-gen-annot",
    "title": "General",
    "section": "Annotation",
    "text": "Annotation\n\nPeople love annotations (thread, paper). More text, the better.\n\nTheir takeaway from the chart is more likely to resemble the annotation if it takes the form of L2 and/or L4 and is close to the data\n\nExample: Financial Times\n\n\nTitle (L2) is used for part of the takaway message\n\nSubtitle used to describe the Y-Axis\n\nChart annotation paragraph (L4) gives contextual information\n\n\nWhen to annotate\n\na design element in your visualization that needs explaining\na data point or series that you want readers to see, like an outlier\nreaders should know something to better understand why certain data points look the way they do\n\nRemove the color key/legend and directly label your categories\n\nIf the screen is small (e.g. mobile), then it’s better to keep the legend\n\nMake it obvious which units your data uses.\n\nDon’t just put units in the description, but also in axis labels, tooltips, and annotations\n\nFor large numbers (e.g. 20 million), try to use B, M, K instead of an annotation somewhere that says something like “in thousand”\nTooltips\n\nConsider not just stating the numbers in tooltips, but also the category\n\ne.g. “3.4% unemployed” instead of “3.4%,” or “+16% revenue” instead of “+16%”\n\nUse a transparent background by setting the alpha channel of CSS background-colorto a number less than 1\n\ne.g. 0.3 using rgba(255, 255, 255, 0.3)\n\nWith a transparent background, text behind the tooltip can interfere with the text in the tooltip, so also apply backdrop-filter\n\nExample:\n.tooltip {\n  background-color: rgba(255, 255, 255, 0.3);\n  -webkit-backdrop-filter: blur(2px);\n  backdrop-filter: blur(2px);\n}\n@media (prefers-contrast: more) {\n  .tooltip {\n    background-color: white;\n    -webkit-backdrop-filter: none;\n    backdrop-filter: none;\n  }\n}\n\nExample shows a tooltip that has an HTML class of “tooltip”.\nblur is measured in pixels and the image size varies with screen width, so the optimal blur size here may vary for you depending on the dimensions of your browser window.\n\nApplies a Gaussian blur to the target element’s background with the standard deviation specified as the argument (e.g. two pixels).\n\nAs of Mar 2023, doesn’t work on Safari, so adding -webkit-backdrop-filter allows it to work on Safari\n@media (prefers-contrast: more)checks if your user has informed their operating system or browser that they prefer increased contrast. When they do, this chunk then overrides the applied styles.\n\n\n\nTransparent backgrounds might work better with thematic maps and less with scatter plots\nDon’t center-align your text\nUse straightforward phrasings\nMove axis labels nearest the most important chart objects (e.g. bars)\n\n\nIf the higher bars are what’s most important and they’re on the right, then usea right-side axis\n\nFonts for annotation\n\n\nUse what readers are most used to (e.g. sans-serif regular, &gt;12px, (almost) black text\nIf you need to need a lot of words and they don’t fit, don’t use smaller font, use a tooltip instead\n\nOn mobile screens you can also hide the least important annotations, or move them below the visualization\n\n\nLead the eye with font sizes, styles, and colors\n\n\nThe biggest and boldest text with the highest contrast against the background should be reserved for the most important information.\n\nDon’t overdo it though\n\nUse only two levels of hierarchy that are clearly different from each other — like a 12px gray and a 14px black\nEmphasize within the annotations using boldness\n\nKeep labels horizontal\n\n\nUse a text outline\n\n\nSet the stroke around your letters, using the background color of your chart.\n\nBe conversational first and precise later",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-color",
    "href": "qmd/visualization-general.html#sec-vis-gen-color",
    "title": "General",
    "section": "Color",
    "text": "Color\n\nMisc\n\nWhen choosing bg and fg colors, keep in mind that it’s generally a good idea to pick colors with a similar hue but a large difference in their luminance.\nDatawrapper guide\nWhen using several subplots together to tell a story and they each have their own color scheme. Blend a color into each color scheme to produce a more unified look\n\nExample: Blending blue into a plot with green color scheme.\n\n\nBreakpoints for scales\n\nHow to choose an interpolation for your color scale\n\nCharts (see prismatic PKG to do this manipulation within ggplot)\nPalette composition methods\n\nComplimentary\n\nopposite sides of the color wheel (2 colors)\ncontrast\n\nAnalogous\n\nsame side of the color wheel (multiple)\ngradient\n\nTriadic\n\nforms triangle on the color wheel\nvibrant, contrast\n\nOthers\n\nsplit complimentary (popular)\n\nComprised of one color and two colors symmetrically placed around it. This strategy adds more variety than complementary color schemes by including three hues without being too jarring or bold. Using this method, we end up with combinations that include warm and cool hues that are more easily balanced than the complementary color schemes\n\nquadratic\n\n\nAdjustments once you chosen a color (hue) to create variations\n\nMove brightness up for lighter variations and down for darker variations\nThen, move saturation in the opposite way you moved brightness\n\nSave colors you find attractive\n\ninstant eyedropper (windows)\nThen use HSL (hue, saturation, lightness) slider for adjustments\n\nBackgrounds\n\nWhite\n\nbright, used a lot\ntry ivory or a light gray\nshades of eggshell, link\n\nAvoid black (or REALLY dark) unless situation calls for it\n\ndark is fine\n\n\nLightest and darkest colors should have meaning (e.g. min, max, mean, zero) and not just some arbitrary numbers\n\nWhat to do when you have a lot of categories\n\nSimply don’t show different colors Does your chart work without colors?\n\n1 color and a discrete axis with the categories\n\nShow shades, not hues Can you make the chart less confetti-like?\n\nAlthough, consider not using shades when the parts are as or more important than the totals\n\nEmphasize Can you only use color for your most important categories?\nLabel directly Can you use the same or similar colors but label them?\nMerge categories Can you put categories together?\nGroup categories, but keep showing them Can strokes help to tell categories apart?\n\nChange the chart type Will another chart type rely less on colors?\n“Small multiply” it Can you split the categories into multiple charts? (i.e. facet by category)\nAdd other indicators Can you add symbols, patterns, line widths, or dashes?\n\n\nDoesn’t use any color — just opacity, thickness, and dotted lines.\n\nUse tooltips and hover effects Can smaller categories be hidden with them?\n\nColor scales should be chosen to best match the data values and plot type: If the goal is to show magnitude, a univariate color scheme is typically preferable, while a double-ended color scale is typically more effective when showing data that differ in sign and magnitude. Where possible, color scales should use a minimal number of hues, varying intensity or lightness of the color to show magnitude, and transitioning through neutral colors (white, light yellow) when utilizing a gradient. Cognitive load can also be reduced by selecting colors with cultural associations that match the data display, such as the use of blue for men and red (or pink) for women, or the use of blue for cold temperatures and red/orange for warm temperatures.\n\nIt is also important to consider the human perceptual system, which does not perceive hues uniformly: We can distinguish more shades of green than any other hue, and fewer shades of yellow, so green univariate color schemes will provide finer discriminability than other colors because the human perceptual system evolved to work in the natural world, where shades of green are plentiful.\n\n\nFigure above shows the International Commission on Illumination (CIE) 1931 color space, which maps the wavelength of a color to a physiologically based perceptual space; a significant portion of the color space is dedicated to greens and blues, while much smaller regions are dedicated to violet, red, orange, and yellow colors. This unevenness in mapping color is one reason that the multi-hued rainbow color scheme is suboptimal—the distance between points in a given color space may not be the same as the distance between points in perceptual space. As a result of the uneven mapping between color space and perceptual space, multi-hued color schemes are not recommended.",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-types",
    "href": "qmd/visualization-general.html#sec-vis-gen-types",
    "title": "General",
    "section": "Chart Types",
    "text": "Chart Types\n\nBar Graphs\n\nDon’t use bar graphs for anything except counts. Audiences have trouble with the abstraction.\nFor averages, used errorbar charts or use median + raincloud.\nGuide\n\nDesign Examples (link)\n\n\nStacked Bar\n\nReplacement for pie charts et al when dealing with fractional data\nAlways reorder stacks\n\n\nRmd tutorial for reordering optimation\n\n\nBox Plots\n\n\nSmall data - emphasize the points\nLarge data - emphasize the box\n\nLine Charts\n\nSometimes it’s appropriate not to use zero as the baseline\nHaving the y-axis not intersect the x-axis can minimize the risk of confusing the readers with a non-zero baseline chart\n\nTime Series of ordinal discrete data by category\n\n\nordinal data has 3 levels\n\n\nHeatmaps\n\nReorder rows and columns to produce a more meaningful visualization\n\n\nGuide on reordering heatmaps\nIf order is important, then this may not be possible\n\nReorder by clustering\n\n\nNetwork Graphs\n\nAlways try different multiple layout methodologies",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-maps",
    "href": "qmd/visualization-general.html#sec-vis-gen-maps",
    "title": "General",
    "section": "Maps",
    "text": "Maps\n\nAbove rules also apply\nRemove as many extraneous elements as possible\n\nHard because maps have so many necessary elements\n\nBorders, Labels, etc.\n\nIn cloropleths, remove unnecessary borders (e.g. along coastlines)\n\n\n“Borders as lines” is much less cluttered\nArticle, rmapshaper::ms_innerlines() keeps only the necessary inner borders in the “geometry” column of the spatial dataset.\n\n\nPay close attention to typography hierarchy\n\nBold, Font size, etc\n\nUse iconography to help users identify what you want them to see\nNumeric values (thread)\n\nPalettes: use a sequential (top row) or diverging (bottom row)\n\n\nFor diverging palettes\n\n\nThe middle value should be light on a light background (top left) or dark on a dark background (bottom left)\n\n\nBackgrounds:\n\n\nLight background: darker color on the value of interest (usually the higher value) (top left)\nDark background: lighter color on the value of interest (usually the higher value) (bottom left)\n\n\nTry not to use Rainbow palettes, because they are misleading\n\nThe rainbow and jet colors are problematic as the change in color is not perceptually uniform, leading to distinct ‘bands’ of certain colors. This causes misleading jumps and emphasizes certain values, most likely without the intention to highlight them. (Cedric Scherer)\n(acceptable) rainbow called “Turbo” if you need one (article)\n\n\nCode - see comments for links to R scripts and improved versions of Turbo\n\nOther Alternatives",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-area",
    "href": "qmd/visualization-general.html#sec-vis-gen-area",
    "title": "General",
    "section": "Area",
    "text": "Area\n\nIn general, these charts aren’t good for noisy data and data with many categories\n\nHave issues when values increase sharply (see video. around 50:13)\n\nExperiment with the order of the groups\n\nEvents that you’re looking for are probably only visable when there’s a particular order\nMost of the time, putting the most stable groups at the bottom produces the best results",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-ts",
    "href": "qmd/visualization-general.html#sec-vis-gen-ts",
    "title": "General",
    "section": "Time Series",
    "text": "Time Series\n\nHorizon Charts\n\nSee Anomaly Detection &gt;&gt; Charts\nEspecially useful for showing data with large amplitudes in a short vertical space",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-uncert",
    "href": "qmd/visualization-general.html#sec-vis-gen-uncert",
    "title": "General",
    "section": "Uncertainty",
    "text": "Uncertainty\n\nVisualizing only inferential uncertainty can lead to significant overestimates of treatment effects\n\n\nWhen possible, plot individual data points alongside statistical estimates\n\nTranslate percentages into counts (e.g. “a 1 out of 5 chance” rather than “a 20% chance”)\n\n\n{riskyr} - icon arrays and less sophisticated viz for the above chart\nicon arrays\n\nExamples\n\nbase rates and error rates (paper)\nrelative risks (paper)\n\n\nWaffle plots are similar to icon arrays\n\nquantile dotplots\n\n{ggdist} (many examples and flavors)\n\nhypothetical outcome plots\n\nConsists of multiple individual plots (frames), each of which depicts one draw from a distribution (use case for animation)\nBest suited for multivariate judgments like how reliable a perceived difference between two random variables is\nIllustration of the process\n\n\nYou create a distribution to sample from or using known distribution and parameters or bootstrapping the sample and sample from each bootstrap.\nEach sample/draw is presented on the right side of the distribution plot (fig 1) (final product)\n\nI think it would be better if after each draw the previous draw remained but was de-emphasized (i.e. turned light gray)\nAnother example would McElreath’s lecture video on posterior prediction distribution.\n\nFigs 2 and 3 show a sequence of draws from a joint distribution of uncorrelated variables (fig 2) and correlated variables (fig 3)\n\nExample: NYT on interpreting jobs reports\n\n\n\n2 facets: accelerating job growth (left), steady job growth (right)\nFor each facet,\n\nthe left plot is static, and the right plot is animated showing different noisy samples of the same underlying dgp\nthe left plot shows what normals perceive the distribution to look like for the given interpretation (e.g. accelerating job growth), and the right plot shows what real (i.e. noisy) data with the same interpretion looks like.\n\n\n\n\nFan charts\n\n\nshows a 90% interval broken divided into 30% increments (left) or 10% increments (right)\n\nShow previous forecasts\n\n\nTruth is in dark blue with light blue branches showing previous forecasts",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-mob",
    "href": "qmd/visualization-general.html#sec-vis-gen-mob",
    "title": "General",
    "section": "Mobile",
    "text": "Mobile\n\nMisc\n\nRStudio plots are displayed in 96 dpi and ggsave uses 300 dpi as default\n\ni.e. viewed plots won’t look the same as the saved plots using default settings\n\n\nUse sharp color contrasts when highlighting\nMinimal readable size is 16, but 22 is recommended\nAspect ratio of 4:3 or 1024 x 768 pixels\n\nAnother article say 1:2\n\nBar Charts should be horizontal to make charts with many categories readable\n\nMobile screens are more tall than wide so labels on the y-axis makes more sense than on the x-axis\n\nR\n\nSet-up external window with aspect ratio (e.g. 1:2)\ndev.new(width=1080, height=2160, unit=\"px\", noRStudioGD = TRUE)\n\nnoRStudioGD = TRUE says any new plots appear in the new graphics window rather than the RStudio graphics device\nCan also use windows(), x11(), or png() from {ragg}\n\n\nUse Quarto (or Rmd) for developement\n#| dpi: 300     \n#| fig.height: 7.2     \n#| fig.width: 3.6     \n#| dev: \"png\"     \n#| echo: false     \n#| warning: false     \n#| message: false`\n\nThis way your dpi and aspect ratio are set and you can view the final output without having to save the png and viewing it separately to see how it looks\nfig.height and fig.width are always given in inches\n\nIf you haven’t set your Quarto document to be self-contained, then the images have also already been saved for you - probably in a folder called documentname_files/figure-html/",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html",
    "href": "qmd/quarto-rmarkdown.html",
    "title": "Quarto",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-misc",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-misc",
    "title": "Quarto",
    "section": "",
    "text": "Packages\n\n{quarto}\n\nResources\n\nDocs\nReference\nTroubleshooting\n\nquarto --version - Must be in RStudio Terminal\nquarto check - Must be in RStudio Terminal - versions and engine checks\n$ quarto check\n[&gt;] Checking versions of quarto binary dependencies...\n      Pandoc version 3.1.1: OK\n      Dart Sass version 1.55.0: OK\n[&gt;] Checking versions of quarto dependencies......OK\n[&gt;] Checking Quarto installation......OK\n      Version: 1.3.340\n      Path: C:\\Users\\tbats\\AppData\\Local\\Programs\\Quarto\\bin\n      CodePage: 1252\n[&gt;] Checking basic markdown render....OK\n[&gt;] Checking Python 3 installation....OK\n      Version: 3.8.1 (Conda)\n      Path: C:/Users/tbats/Miniconda3/python.exe\n      Jupyter: 4.9.1\n      Kernels: python3\n(\\) Checking Jupyter engine render....2023-04-28 10:18:15,018 - traitlets - WARNING - Kernel\nProvisioning: The 'local-provisioner' is not found.  This is likely due to the presence of multiple jupyter_client distributions and a        previous distribution is being used as the source for entrypoints - which does not include 'local-provisioner'.  That distribution should     be removed such that only the version-appropriate distribution remains (version &gt;= 7).  Until then, a 'local-provisioner' entrypoint will     be automatically constructed and used.\nThe candidate distribution locations are: ['C:\\\\Users\\\\tbats\\\\Miniconda3\\\\lib\\\\site-packages\\\\jupyter_client-5.3.4.dist-info',                'C:\\\\Users\\\\tbats\\\\Miniconda3\\\\lib\\\\site-packages\\\\jupyter_client-7.0.6.dist-info']\n[&gt;] Checking Jupyter engine render....OK\n[&gt;] Checking R installation...........OK\n      Version: 4.2.3\n      Path: C:/PROGRA~1/R/R-42~1.3\n      LibPaths:\n        - C:/Users/tbats/AppData/Local/R/win-library/4.2\n        - C:/Program Files/R/R-4.2.3/library\n      knitr: 1.42\n      rmarkdown: 2.20\n[&gt;] Checking Knitr engine render......OK\nCLI\n\nquarto render to compile a document\nquarto preview to render a live preview that automatically updates when the source files are saved\n\nUsing a development verison of Quarto\n\nFirst Usage\n\nChange directories to where you want to store the dev version\nClone repo and change to the cloned directory\ngit clone https://github.com/quarto-dev/quarto-cli\ncd quarto-cli\nDisable Anti-Virus\nRun Configuration Script\n\nWindows Command Prompt\ncmd /k configure.cmd\n\n\\k keeps the window open in case it errors\n\nPowershell\nInvoke-Item configure.cmd\nLinux/MacOS\n./configure.sh\nThis will take a minute or two as it checks versions, installs dependencies like pandoc, etc.\n\nAdd path to quarto.cmd to PATH\n\nAfter the configuration file runs, it will output the path you need to put on PATH, e.g. \"C:\\Users\\erc\\Documents\\Quarto\\quarto-cli\\package\\dist\\bin\"\n\nEnable Anti-Virus\nShould be able to use in RStudio\n\nI was not able to use the RStudio terminal for quarto commands (e.g. quarto check) though.\nTo find the version, I just opened powershell and ran quarto –version just to make sure it was running and on PATH.\n\nNot sure if they use this every time but it was 99.9.9 instead of the verion in the changelog.\n\nI also rendered a qmd file using quarto-cmd from the root directory of quarto-cli to see if it matched the output from RStudio. (cd qmd then quarto preview forecasting-statistical.qmd --to html --no-watch-inputs --no-browse)\n\n\nSubsequent Development Versions\n\nChange directory to quarto-cli and git pull\n\n\nShortcuts\n\nNew R chunk: ctrl + alt + i\nBuild whole book: ctrl+shift b\nRender page and preview book: ctrl+shift k\n\nUsing yaml style for chunk options\n\nConvert Rmd chunk options to Quarto: knitr::convert_chunk_header(\"doc.rmd\", \"doc.qmd\")\nAnchor Link - A link, which allows the users to flow through a website page. It helps to scroll and skim-read easily. A named anchor can be used to link to a different part of the same page (like quickly navigating) or to a specific section of another page.\n\nThis is the “#sec-moose” id that can be added to headers which it allows to be referenced within the document or in other documents.\n\nMathJax commands\n\nFont Size: \\tiny{ }, \\scriptsize{ }, \\small{ }, \\normal{ }, \\large{ }, \\Large{ }, \\LARGE{ }, \\huge{ }, \\Huge{ }\n\nLightbox\n\nDocs\nGrouping images for lightbox carousel: ![A Lovely Image](mv-1.jpg){group=\"my-gallery\"}",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-quarto",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-quarto",
    "title": "Quarto",
    "section": "Syntax",
    "text": "Syntax\n\nAlign code chunk under bullet and add indented comment below chunk\n-   [Example]{.ribbon-highlight} (using a SQL Query; method 1)\n\n    ``` r\n    # open dataset\n    ds &lt;- arrow::open_dataset(dir_out, partitioning = \"species\")\n    # open connection to DuckDB\n    con &lt;- dbConnect(duckdb::duckdb())\n    # register the dataset as a DuckDB table, and give it a name\n    duckdb::duckdb_register_arrow(con, \"my_table\", ds)\n    # query\n    dbGetQuery(con, \"\n      SELECT sepal_length, COUNT(*) AS n\n      FROM my_table\n      WHERE species = 'species=setosa'\n      GROUP BY sepal_length\n    \")\n\n    # clean up\n    duckdb_unregister(con, \"my_table\")\n    dbDisconnect(con)\n    ```\n\n    -   filtering using a partition, the WHERE format is '\\&lt;partition_variable\\&gt;=\\&lt;partition_value\\&gt;'\n\nSpace between bullet and top ticks\nSpace between bottom ticks and bullet\nNote alignment of text\n\nAdd Code Annotations\n-   [Partition a large file and write to arrow format]{.underline}\n\n    ``` r\n    lrg_file &lt;- open_dataset(&lt;file_path&gt;, format = \"csv\") # &lt;1&gt;\n    lrg_file %&gt;%\n        group_by(var) %&gt;% # &lt;2&gt;\n        write_dataset(&lt;output_dir&gt;, format = \"feather\") # &lt;3&gt;\n    ```\n\n    1.  Pass the file path to `open_dataset()`\n\n    2.  Use `group_by()` to partition the Dataset into manageable chunks\n\n    3.  Use `write_dataset()` to write each chunk to a separate Parquet file---all without needing to read the full CSV file into R\n\n    -   `open_dataset` is fast because it only reads the metadata of the file system to determine how it can construct queries",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#chunk-options-and-yaml",
    "href": "qmd/quarto-rmarkdown.html#chunk-options-and-yaml",
    "title": "Quarto",
    "section": "Chunk Options and YAML",
    "text": "Chunk Options and YAML\n\nSet global chunk options in yaml\n\nEnable Margin Notes\n---\n# YAML front matter\nreference-location: margin\n---\n!expr to render code within chunk options\n\ne.g. figure caption: #| fig-cap: !expr glue::glue(\"The mean temperature was {mean(airquality$Temp) |&gt; round()}\")\n\nConditional Code Chunk Evaluation\n\nExample: document output type\n\nSet value in a code chunk\n```{r setup}\n# Include in first chunk of .qmd\n# Get output file type\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\n```\nUse !expr sytax to determine evaluation status\n\nExample: eval chunk based on output type\n```{r}\n#| eval: !expr out_type == \"html\"\n\n# code to create interactive {plotly}\n```\n\n```{r}\n#| eval: !expr out_type == \"docx\"\n\n# code to create static {ggplot2}\n```\n\n\nExample: Use parameterization to set value\n---\ntitle: \"test\"\nformat: html\nparams:\n  my_value: false\n---\n\nmy_value can then be used throughout the document to determine chunk evaluation status\n\n\ncolumn: screen-inset yaml markup is used to show a very wide table\nCLI\n\nquarto render to compile a document\nquarto preview to render a live preview that automatically updates when the source files are saved\n\nGraphics\n\nCode Chunk\n#| dpi: 300\n#| fig.height: 7.2\n#| fig.width: 3.6\n#| dev: \"png\"\n#| echo: false\n#| warning: false\n#| message: false\n\nExample shows settings for a graph for mobile\nfig.height and fig.width are always given in inches\n\n\nIf you haven’t set your Quarto document to be self-contained, then the images have also already been saved for you - probably in a folder called documentname_files/figure-html/\nformat: \n  html:\n    embed-resources: true\nYAML Example\n\nNested Tabs\n\nKnitr Hooks\n\nNotes from Writing knitr hooks\n\nAlso has a knitr hook example that alters cell output (e.g. only prints 4 lines of a vector)\n\nChunk Hooks\n\nChunk hooks get called twice: once before knitr executes the code in the chunk, and once again afterwards\nThe function can take up to four arguments, all of which are optional:\n\nbefore: A logical value indicating whether the function is being called before or after the code chunk is executed\noptions: The list of chunk options\nenvir: The environment in which the code chunk is executed\nname: The name of the code chunk option that triggered the hook function\n\nThe chunk hook is called for its side effects not the return value. However, if it returns a character output, knitr will add that output to the document output as-is.\nExample: Chunk Timer\n\nCode\ncreate_timer_hook &lt;- function() {\n  start_time &lt;- NULL\n  function(before, options) {\n    if (before) {\n      start_time &lt;&lt;- Sys.time()\n    } else {\n      stop_time &lt;- Sys.time()\n      elapsed &lt;- difftime(stop_time, start_time, units = \"secs\")\n      paste(\n        \"&lt;div style='font-size: 70%; text-align: right'&gt;\",\n        \"Elapsed time:\", \n        round(elapsed, 2), \n        \"secs\",\n        \"&lt;/div&gt;\"\n      )\n    }\n  }\n}\nknitr::knit_hooks$set(timer = create_timer_hook())\n\nThe hook is triggered the first time (with before = TRUE) to record the system time somewhere (e.g., in a variable called start_time). Then, when the hook is triggered the second time (with before = FALSE), it records the system time again (e.g., as stop_time), and computes the difference in time.\n\nUse in a cell\n```{r}\n#| timer: true\nrunif(10000)\n```\nOutput",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#r-and-python",
    "href": "qmd/quarto-rmarkdown.html#r-and-python",
    "title": "Quarto",
    "section": "R and Python",
    "text": "R and Python\n\nIf only R or R and Python, the notebook is rendered by {knitr}\nIf only Python, the notebook is rendered by jupyter\nSet-up\n\n{reticulate} automatically comes loaded in Quarto and it knows to use it when it sees a python block, so you don’t need to load the package\nQuarto will select a version of Python using the Python Launcher on Windows or system PATH on MacOS and Linux. You can override the version of Python used by Quarto by setting the QUARTO_PYTHON environment variable.\n\nIn CLI on Windows, type py is see which version the Python Launcher , and therefore Quarto, is using and py –list to see which versions are installed.\n\n\nR\n```{r}\n#| label: read-data\n#| echo: true\n#| message: false\n#| cache: true\nlemurs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-08-24/lemur_data.csv')\n```\nPython\n```{python}\n#| label: modelling \n#| echo: true \n#| message: false\n\nlemur_data_py = r.lemur_data \nimport statsmodels.api as sm \ny = lemur_data_py[[\"Weight\"]] \nx = lemur_data_py[[\"Age\"]] \nx = sm.add_constant(x) \nmod = sm.OLS(y, x).fit() \nlemur_data_py[\"Predicted\"] = mod.predict(x) \nlemur_data_py[\"Residuals\"] = mod.resid`\n```\n\nUse r. to access the data in the R chunk\nThe first execution of a python cell starts reticulate::repl_python() in the terminal\n\n(back to) R\n```{r}\n#| label: plotting \n#| echo: true \n#| output-location: slide \n#| message: false \n#| fig-align: center \n#| fig-alt: \"Scatter plot of predicted and residual values for the fitted linear model.\" \n\nlibrary(reticulate) \nlibrary(ggplot2) \nlemur_residuals &lt;- py$lemur_data_py \nggplot(data = lemur_residuals, aes(x = Predicted, y = Residuals)) +\n  geom_point(colour = \"#2F4F4F\") +\n  geom_hline(yintercept = 0,\n            colour = \"red\") +\n  theme(panel.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"),\n        plot.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"))\n```\n\nUse py$ to access the data in the Python chunk *\nMust call library(reticulate) in order for Quarto to recognize py$",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#layouts",
    "href": "qmd/quarto-rmarkdown.html#layouts",
    "title": "Quarto",
    "section": "Layouts",
    "text": "Layouts\n\n2 cols (1 col: text, 1 col: image)\n\n::: {layout=\"[50,50]\"}\n\n::: column\nEvery Quarto project starts with a Quarto file that has the extension `.qmd`.\n\n\nThis particular one analyzes children's early words, but every `.qmd` includes the same three basic elements inside:\n\n\n- A block of metadata at the top, between two fences of `---`s. This is written in [YAML](https://learnxinyminutes.com/docs/yaml/). \n- Narrative text, written in [Markdown](https://commonmark.org/help/tutorial/). \n- Code chunks in gray between two fences of ```` ``` ````, written with R or another programming language.\n\n\nYou can use all three elements to develop your code and ideas in one reproducible document.\n:::\n\n![](img/01-source.png)\n:::\n2 figures, 2 columns (i.e. side-by-side) with captions at the top\n---\nfig-cap-location: top\n---\n\n-   Words\n    -   Predictions of Standard RF vs Oblique RF\n\n        ::: {layout-ncol=\"2\"}\n        ![Standard Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-axpred-1.png){fig-align=\"left\" width=\"432\"}\n\n        ![Oblique Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-oblpred-1.png){fig-align=\"left\" width=\"432\"}\n        :::\n\n        -   Words  \n\nfig-cap-location: bottom is default;\nfig-cap-location: margin is buggy, at least in for project type book. Captions are added to the margins but bullet points mysteriously disappear during rendering to html\n\n2 charts side-by-side extending past body margins\n```{r}\n#| label: my-figure\n#| layout-ncol: 2\n#| column: page\nggplot() + ...\nggplot() + ...\n```\n\n“layout-ncol” says 2 side-by-side columns\n“column: page” says extend column width to the width of the page",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#webr",
    "href": "qmd/quarto-rmarkdown.html#webr",
    "title": "Quarto",
    "section": "WebR",
    "text": "WebR\n\nSet-Up\n\nInstall the extension alongside your blog post by running quarto add coatless/quarto-webr\nAdd the extension to your blog by adding filters: [\"webr\"] to your post’s frontmatter\nInstead of {r} code chunks, use {webr-r} ones\n\nInstall CRAN packages on page load\nfilters:\n  - \"webr\"\nwebr:\n  packages:\n  - \"dplyr\"\n  - \"tidyr\"\n  - \"purrr\"\n  - \"tibble\"\n  - \"crayon\"\n\nAdd to frontmatter\n\nInstall R-Universe Package\n```{webr-r}\n#| context: setup\nwebr::install(\"collateral\", repos = c(\"https://jimjam-slam.r-universe.dev\"))\n```\n\nR-Universe packages must be installed in code cells",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/data.table.html",
    "href": "qmd/data.table.html",
    "title": "data.table",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#sec-dt-misc",
    "href": "qmd/data.table.html#sec-dt-misc",
    "title": "data.table",
    "section": "",
    "text": "Syntax\nDT[i, j, by]\n\n##   R:                 i                 j        by\n## SQL:  where | order by   select | update  group by\n\nTake data.table DT, subset rows using i, and manipulate columns with j, grouped according to by.\n\nResources\n\nDocs but it’s difficult to find anything.\n\nThe philosophy of the package is highly dependent on syntax, so the reference page is not very useful in finding out how to perform certain operations as it usually is with other packages.\nThe search doesn’t include the articles which contain a lot of information.\nAlso, it’s an old package, and every old article, changelog, etc. is in the docs. So, if you find something you think answers your question, it may be that that syntax is outdated.\n\nIntroduction to data.table (vignette)\nSyntax Reference (link)\nSymbol Reference (link)\n\nsetDT(df)- Fast conversion of a data frame or list to a data.table without copying\n\nUse when working with larger data sets that take up a considerable amount of RAM (several GBs) because the operation will modify each object in place, conserving memory.\nas.data.table(matrix) should be used for matrices\ndat &lt;- data.table(df) can be used for small datasets but there’s no reason to.\nsetDT(copy(df)) if you want to work with a copy of the df instead of converting the original object.\n\nChaining: see Pivoting &gt;&gt; melt &gt;&gt; Multiple variables stored in column names for an example\nPiping\ndt |&gt; \n   _[, do_stuff(column), by = group] |&gt; \n   _[, do_something_else(othr_col), by = othr_grp]\n\nThe _ placeholder allows you to use R’s native pipe.\nExample\npenguins[species == \"Chinstrap\"] |&gt; \n  _[ , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island)]\n# or\npenguins[species == \"Chinstrap\"] |&gt; \n  DT( , .(mean_flipper_length = mean(flipper_length_mm)), by = .(sex, island))\n\nSymbols\n\n.SD is a data.table containing the Subset of DT’s Data for each group, excluding any columns used in by (or keyby). Its usage is still confusing to me.\n:= is the walrus operator. let is an alias. Think it acts like dplyr::mutate or maybe dplyr::summarize. (Docs)\nDT[i, colC := mean(colB), by = colA]\nDT[i,\n   `:=`(colC = sum(colB),\n        colD = sum(colE))\n   by = colF]\nDT[i,\n   let(colC = sum(colB),\n       colD = sum(colE)),\n   by = colF] \n.I is the row index. It’s an integer vector equal to seq_len(nrow(x))\ndt &lt;- data.table(\n  a = 1:3,\n  b = 4:6\n)\ndt[, .(a, b, rowsum = sum(.SD)), by = .I]\n#&gt;        I     a     b rowsum\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1:     1     1     4      5\n#&gt; 2:     2     2     5      7\n#&gt; 3:     3     3     6      9",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#basic-usage",
    "href": "qmd/data.table.html#basic-usage",
    "title": "data.table",
    "section": "Basic Usage",
    "text": "Basic Usage\n\nUsing i\n\nWe can subset rows similar to a data.frame- except you don’t have to use DT$ repetitively since columns within the frame of a data.table are seen as if they are variables.\nWe can also sort a data.table using order(), which internally uses data.table’s fast order for performance.\nWe can do much more in i by keying a data.table, which allows blazing fast subsets and joins. We will see this in the “Keys and fast binary search based subsets” and “Joins and rolling joins” vignette.\n\n\n\nUsing j\n\nSelect columns the data.table way: DT[, .(colA, colB)].\nSelect columns the data.frame way: DT[, c(\"colA\", \"colB\")].\nCompute on columns: DT[, .(sum(colA), mean(colB))].\nProvide names if necessary: DT[, .(sA =sum(colA), mB = mean(colB))].\nCombine with i: DT[colA &gt; value, sum(colB)].\n\n\n\nUsing by\n\nUsing by, we can group by columns by specifying a list of columns or a character vector of column names or even expressions. The flexibility of j, combined with by and i makes for a very powerful syntax.\nby can handle multiple columns and also expressions.\nWe can keyby grouping columns to automatically sort the grouped result.\nWe can use .SD and .SDcols in j to operate on multiple columns using already familiar base functions. Here are some examples:\n\nDT[, lapply(.SD, fun), by = ..., .SDcols = ...] - applies fun to all columns specified in .SDcols while grouping by the columns specified in by.\nDT[, head(.SD, 2), by = ...] - return the first two rows for each group.\nDT[col &gt; val, head(.SD, 1), by = ...] - combine i along with j and by.",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#columns",
    "href": "qmd/data.table.html#columns",
    "title": "data.table",
    "section": "Columns",
    "text": "Columns\n\nRename Columns\nsetnames(DT, \n         old = c(\"SIMD2020v2_Income_Domain_Rank\",\n                 \"SIMD2020_Employment_Domain_Rank\",  \n                 \"SIMD2020_Health_Domain_Rank\",\n                 \"SIMD2020_Education_Domain_Rank\", \n                 \"SIMD2020_Access_Domain_Rank\", \n                 \"SIMD2020_Crime_Domain_Rank\",    \n                 \"SIMD2020_Housing_Domain_Rank\",\n                 \"CP_Name\"),\n\n         new = c(\"Income\", \"Employment\", \n                 \"Health\",   \"Education\",\n                 \"Access\",  \"Crime\", \n                 \"Housing\", \"areaname\"))",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#sec-dt-filter",
    "href": "qmd/data.table.html#sec-dt-filter",
    "title": "data.table",
    "section": "Filtering",
    "text": "Filtering\n\nFast filtering mechanism; reorders rows (increasing) to group by the values in the key columns. Reordered rows make them easier to find and subset.\n\nAll types of columns can be used except list and complex\n\nOperations covered in this section\n\nFiltering\nFilter, select\nFilter, groupby, summarize\nIf-Else\n\nSet Keys - Says order in the increasing direction according to origin and then dest.\nsetkey(flights, origin, dest)\nhead(flights)\n#    year month day dep_delay arr_delay carrier origin dest air_time distance hour\n# 1: 2014     1   2        -2       -25      EV    EWR  ALB      30      143    7\n# 2: 2014     1   3        88        79      EV    EWR  ALB      29      143   23\n# 3: 2014     1   4       220       211      EV    EWR  ALB      32      143   15\n# 4: 2014     1   4        35        19      EV    EWR  ALB      32      143    7\n# 5: 2014     1   5        47        42      EV    EWR  ALB      26      143    8\n# 6: 2014     1   5        66        62      EV    EWR  ALB      31      143   23\nFilter by origin == “JFK” and dest == “MIA”\nflights[.(\"JFK\", \"MIA\")]\n#      year month day dep_delay arr_delay carrier origin dest air_time distance hour\n#    1: 2014    1   1        -1       -17      AA    JFK  MIA      161    1089   15\n#    2: 2014    1   1         7        -8      AA    JFK  MIA      166    1089    9\n#    3: 2014    1   1         2        -1      AA    JFK  MIA      164    1089   12\n#    4: 2014    1   1         6         3      AA    JFK  MIA      157    1089    5\n#    5: 2014    1   1         6       -12      AA    JFK  MIA      154    1089   17\n#  ---                                                                             \n# 2746: 2014   10  31        -1       -22      AA    JFK  MIA      148    1089   16\n# 2747: 2014   10  31        -3       -20      AA    JFK  MIA      146    1089    8\n# 2748: 2014   10  31         2       -17      AA    JFK  MIA      150    1089    6\n# 2749: 2014   10  31        -3       -12      AA    JFK  MIA      150    1089    5\n# 2750: 2014   10  31        29         4      AA    JFK  MIA      146    1089   19\nFilter by only the first key column (origin): flights[\"JFK\"]\nFilter by only the second key column (dest)\nflights[.(unique(), \"MIA\")]\n#      year month day dep_delay arr_delay carrier origin dest air_time distance hour\n#    1: 2014    1   1        -5       -17      AA    EWR  MIA      161    1085   16\n#    2: 2014    1   1        -3       -10      AA    EWR  MIA      154    1085    6\n#    3: 2014    1   1        -5        -8      AA    EWR  MIA      157    1085   11\n#    4: 2014    1   1        43        42      UA    EWR  MIA      155    1085   15\n#    5: 2014    1   1        60        49      UA    EWR  MIA      162    1085   21\n#  ---                                                                             \n# 9924: 2014   10  31       -11        -8      AA    LGA  MIA      157    1096   13\n# 9925: 2014   10  31        -5       -11      AA    LGA  MIA      150    1096    9\n# 9926: 2014   10  31        -2        10      AA    LGA  MIA      156    1096    6\n# 9927: 2014   10  31        -2       -16      AA    LGA  MIA      156    1096   19\n# 9928: 2014   10  31         1       -11      US    LGA  MIA      164    1096   15\nFilter by origin and dest values, then select a arr.delay column: flights[.(\"LGA\", \"TPA\"), .(arr_delay)]\nFilter by origin and dest values, then summarize and pull maximum of arr_delay\nflights[.(\"LGA\", \"TPA\"), max(arr_delay)]\n# [1] 486\nFilter by origin value, group_by month, summarize( max(dep_delay))\nans &lt;- flights[\"JFK\", max(dep_delay), keyby = month]\nhead(ans)\n#    month  V1\n# 1:    1  881\n# 2:    2 1014\n# 3:    3  920\n# 4:    4 1241\n# 5:    5  853\n# 6:    6  798\nkey(ans)\n# [1] \"month\"\n\nkeyby groups and sets the key to month\n\nFilter by three origin values, one dest value, return the last row for each match\nflights[.(c(\"LGA\", \"JFK\", \"EWR\"), \"XNA\"), mult = \"last\"]\n#    year month day dep_delay arr_delay carrier origin dest air_time distance hour\n# 1: 2014     5  23       163       148      MQ    LGA  XNA      158    1147  18\n# 2:   NA    NA  NA        NA        NA      NA    JFK  XNA       NA      NA  NA\n# 3: 2014     2   3       231       268      EV    EWR  XNA      184    1131  12\n\nFiltering by more than one key value returns combinations of the first key and second key\nRemember setting a key reorders (increasing)",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#joins",
    "href": "qmd/data.table.html#joins",
    "title": "data.table",
    "section": "Joins",
    "text": "Joins\n\nLeft Equal Join\nDT &lt;- lookup[DT, on = .(DataZone = Data_Zone)]\nDT &lt;- merge(lookup, DT, by.x = \"DataZone\", by.y = \"Data_Zone\")\n\nDT: A datatable where the id column is Data_Zone\nlookup: A datatable where the id column is DataZone\nBoth datatables have the same number of rows so that makes this an Equal Join\nDT is joined to lookup, so the columns of lookup appear first (farthest left) then DT’s columns (farthest right) of the joined datatable.\nSubset Notation: The output datatable has the id column, DataZone, which is from lookup but the rows are ordered the same way as the input table, DT.\n\nIt’s weird that the output’s rows are ordered according to the input datatable\n\nmerge: The output datatable has the id column, DataZone, which is from lookup, and the rows are ordered according to lookup\nThe subset way is the “data.table” way, because you perform calculations on the output using the j position whereas with merge, it would require a chain or an extra line of code. But if the order of rows of the output matters, then I can’t find a way to reproduce the merge ordering using the subset method.",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#sec-dt-cond",
    "href": "qmd/data.table.html#sec-dt-cond",
    "title": "data.table",
    "section": "Conditionals",
    "text": "Conditionals\n\nIfelse using hour\nsetkey(flights, hour) # hour has values 0-24\nflights[.(24), hour := 0L]\n\nifelse(hour == 24, 0, TRUE)\nConsequence: since a key column value has changed, hour is no longer a key",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#sec-dt-pivot",
    "href": "qmd/data.table.html#sec-dt-pivot",
    "title": "data.table",
    "section": "Pivoting",
    "text": "Pivoting\n\npivot_longer and melt\n\nBasic\nrelig_income |&gt;\n  pivot_longer(!religion, # keep religion as a column\n              names_to = \"income\", # desired name for new column\n              values_to = \"count\") # what data goes into the new column?\nmelt(DT, id.vars = \"religion\",\n    variable.name = \"income\",\n    value.name = \"count\",\n    variable.factor = FALSE) # added to keep output consistent with tidyr\nColumns have a common prefix and missing values are dropped\nbillboard |&gt;\n  pivot_longer(\n    cols = starts_with(\"wk\"),\n    names_to = \"week\",\n    names_prefix = \"wk\",\n    values_to = \"rank\",\n    values_drop_na = TRUE\n  )\nmelt(DT,\n    measure.vars = patterns(\"^wk\"),\n    variable.name = \"week\",\n    value.name = \"rank\",\n    na.rm = TRUE)\nMultiple variables stored in column names\nwho &lt;- data.table(id = 1, new_sp_m5564 = 2, newrel_f65 = 3)\n#         id new_sp_m5564 newrel_f65\n#      &lt;num&gt;        &lt;num&gt;      &lt;num&gt;\n#   1:     1            2          3\n\nmelt(who,\n     measure.vars = measure(diagnosis,\n                            gender,\n                            ages,\n                            pattern = \"new_?(.*)_(.)(.*)\"))\n#       id diagnosis gender   ages value\n#    &lt;num&gt;    &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt;\n# 1:     1        sp      m   5564     2\n# 2:     1       rel      f     65     3\n\n# with tidyr \nwho |&gt; \n  tidyr::pivot_longer(\n    cols = !id,\n    names_to = c(\"diagnosis\", \"gender\", \"age\"),\n    names_pattern = \"new_?(.*)_(.)(.*)\",\n    values_to = \"count\")\n# # A tibble: 2 × 5\n#           id diagnosis gender age   count\n#        &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n# 1          1 sp        m      5564      2\n# 2          1 rel       f      65        3\n\ntstrsplit is DT’s tidyr::separate\n\nMatrix to long\nanscombe |&gt;\n  pivot_longer(\n    everything(),\n    cols_vary = \"slowest\",\n    names_to = c(\".value\", \"set\"),\n    names_pattern = \"(.)(.)\" \n  )\nDT[,melt(.SD,\n            variable.name = \"set\",\n            value.name = c(\"x\",\"y\"),\n            variable.factor = FALSE,\n            measure.vars = patterns(\"^x\",\"^y\"))]\n\n\n\npivot_wider and dcast\n\nData in examples\n\nfish_encounters\n## # A tibble: 114 × 3\n##    fish  station  seen\n##    &lt;fct&gt; &lt;fct&gt;    &lt;int&gt;\n##  1 4842  Release     1\n##  2 4842  I80_1       1\n##  3 4842  Lisbon      1\n##  4 4842  Rstr        1\n##  5 4842  Base_TD     1\n##  6 4842  BCE         1\n##  7 4842  BCW         1\n##  8 4842  BCE2        1\n##  9 4842  BCW2        1\n## 10 4842  MAE         1\n## # … with 104 more rows\n\nBasic\nfish_encounters |&gt;\n  pivot_wider(names_from = station, values_from = seen)\n\ndcast(DT, fish ~ station, value.var = \"seen\")\nFill in missing values\nfish_encounters |&gt;\n  pivot_wider(names_from = station, values_from = seen, values_fill = 0)\n\ndcast(DT, fish ~ station, value.var = \"seen\", fill = 0)\n# alt\nDT[, dcast(.SD, fish ~ station, value.var = \"seen\", fill = 0)]\n\nRather than have the DT inside dcast, we can use .SD and have dcast inside DT, which is helpful for further chaining. (see applied to melt above)\n\nGenerate column names from multiple variables\nus_rent_income |&gt;\n  pivot_wider(\n    names_from = variable,\n    values_from = c(estimate, moe)\n  )\n\ndcast(DT, GEOID + NAME ~ variable, \n          value.var = c(\"estimate\",\"moe\"))\n# alt\ndcast(DT, ... ~ variable, \n      value.var = c(\"estimate\",\"moe\"))\n\nAlternative: pass “…” to indicate all other unspecified columns\n\nSpecify a different names separator\nus_rent_income |&gt;\n  pivot_wider(\n    names_from = variable,\n    names_sep = \".\",\n    values_from = c(estimate, moe)\n  )\n\ndcast(DT, GEOID + NAME ~ variable,\n      value.var = c(\"estimate\",\"moe\"), \n      sep = \".\")\n# alt\nDT[, dcast(.SD, GEOID + NAME ~ variable,\n    value.var = c(\"estimate\",\"moe\"), \n          sep = \".\")]\n\nAlternative: Rather than have the DT inside dcast, we can use .SD and have dcast inside DT, which is helpful for further chaining. (see applied to melt above)\n\nControlling how column names are combined\nus_rent_income |&gt;\n  pivot_wider(\n    names_from = variable,\n    values_from = c(estimate, moe),\n    names_vary = \"slowest\"\n  ) |&gt; names()\n\nDT[, dcast(.SD, GEOID + NAME ~ variable,\n          value.var = c(\"estimate\",\"moe\"))\n  ][,c(1:3,5,4,6)] |&gt; names()\n\n## [1] \"GEOID\"          \"NAME\"            \"estimate_income\" \"moe_income\"     \n## [5] \"estimate_rent\"  \"moe_rent\"\n\nSee {tidyr::pivot_wider} docs and the names_vary arg\n\nAggregation\nwarpbreaks %&gt;%\n  pivot_wider(\n    names_from = wool,\n    values_from = breaks,\n    values_fn = mean\n  )\ndcast(DT, tension ~ wool, \n          value.var = \"breaks\", fun = mean)\n# alt\nDT[, dcast(.SD, tension ~ wool, \n      value.var = \"breaks\", fun = mean)]\n\n## # A tibble: 3 × 3\n##  tension    A    B\n##  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 L        44.6  28.2\n## 2 M        24    28.8\n## 3 H        24.6  18.8\n\nAlternative: Rather than have the DT inside dcast, we can use .SD and have dcast inside DT, which is helpful for further chaining. (see applied to melt above)",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#tidyr",
    "href": "qmd/data.table.html#tidyr",
    "title": "data.table",
    "section": "tidyr",
    "text": "tidyr\n\nseparate via tstrsplit\ndt &lt;- data.table(x = c(\"00531725 Male 2021 Neg\", \"07640613 Female 2020 Pos\"))\n#                           x\n#                      &lt;char&gt;\n# 1:   00531725 Male 2021 Neg\n# 2: 07640613 Female 2020 Pos\n\ncols &lt;- c(\"personID\", \"gender\", \"year\", \"covidTest\")\n\ndt[, tstrsplit(x,\n               split = \" \",\n               names = cols,\n               type.convert = TRUE)]\n#    personID gender  year covidTest\n#       &lt;int&gt; &lt;char&gt; &lt;int&gt;    &lt;char&gt;\n# 1:   531725   Male  2021       Neg\n# 2:  7640613 Female  2020       Pos\n\n\ndt[, tstrsplit(x,\n               split = \" \",\n               names = cols,\n               type.convert = list(as.character = 1,\n                                   as.factor = c(2, 4),\n                                   as.integer = 3)\n               )]\n#    personID gender   year covidTest\n#      &lt;char&gt; &lt;fctr&gt;  &lt;int&gt;    &lt;fctr&gt;\n# 1: 00531725   Male   2021       Neg\n# 2: 07640613 Female   2020       Pos",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#user-defined-functions",
    "href": "qmd/data.table.html#user-defined-functions",
    "title": "data.table",
    "section": "User Defined Functions",
    "text": "User Defined Functions\n\nenv\n\niris_dt &lt;- as.data.table(iris)\nsquare = function(x) x^2\n\niris_dt[filter_col %in% filter_val,\n        .(var1, var2, out = outer(inner(var1) + inner(var2))),\n        by = by_col,\n        env = list(\n          outer = \"sqrt\",\n          inner = \"square\",\n          var1 = \"Sepal.Length\",\n          var2 = \"Sepal.Width\",\n          out = \"Sepal.Hypotenuse\",\n          filter_col = \"Species\",\n          filter_val = I(\"versicolor\"),\n          by_col =  \"Species\"\n        )] |&gt; \n  head(n = 3)\n#       Species Sepal.Length Sepal.Width Sepal.Hypotenuse\n#        &lt;fctr&gt;        &lt;num&gt;       &lt;num&gt;            &lt;num&gt;\n# 1: versicolor          7.0         3.2         7.696753\n# 2: versicolor          6.4         3.2         7.155418\n# 3: versicolor          6.9         3.1         7.564390\n\nVariables are included in the standard i, j, and by syntax\nenv contains the (quoted) variable values\n\ni.e. argument values in the typical R udf syntax (function(x = val1))\nCan use other UDFs as values which is demonstrated by inner = “square”",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/data.table.html#sec-dt-rec",
    "href": "qmd/data.table.html#sec-dt-rec",
    "title": "data.table",
    "section": "Recipes",
    "text": "Recipes\n\nOperations covered in this section\n\ngroup_by, summarize (and arrange)\ncrosstab\n\ngroup_by, summarize (and arrange)\ndt_res &lt;- dtstudy[, .(n = .N, avg = round(mean(y), 1)), keyby = .(male, over65, rx)]\n\ntb_study &lt;- tibble::as_tibble(dtstudy)\ntb_res &lt;- tb_study |&gt;\n  summarize(n = n(),\n            avg = round(mean(y), 1),\n            .by = c(male, over65, rx)) |&gt;\n  arrange(male, over65, rx)\n\ndt automatically orders by the grouping variables, so to get the exact output, you have to add an arrange\n\nCrosstab using cube (Titanic5 dataset)\n# Note that the mean of a 0/1 variable is the proportion of 1s\nmn &lt;- function(x) mean(x, na.rm=TRUE)\n# Create a function that counts the number of non-NA values\nNna &lt;- function(x) sum(! is.na(x))\n\ncube(d, .(Proportion=mn(survived), N=Nna(survived)), by=.q(sex, class), id=TRUE)\n\n#&gt;     grouping    sex class Proportion    N\n#&gt; 1:         0 female     1  0.9652778  144\n#&gt; 2:         0   male     1  0.3444444  180\n#&gt; 3:         0   male     2  0.1411765  170\n#&gt; 4:         0 female     2  0.8867925  106\n#&gt; 5:         0   male     3  0.1521298  493\n#&gt; 6:         0 female     3  0.4907407  216\n#&gt; 7:         1 female    NA  0.7274678  466\n#&gt; 8:         1   male    NA  0.1909846  843\n#&gt; 9:         2   &lt;NA&gt;     1  0.6203704  324\n#&gt; 10:        2   &lt;NA&gt;     2  0.4275362  276\n#&gt; 11:        2   &lt;NA&gt;     3  0.2552891  709\n#&gt; 12:        3   &lt;NA&gt;    NA  0.3819710 1309",
    "crumbs": [
      "data.table"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html",
    "href": "qmd/db-dbt.html",
    "title": "5  dbt",
    "section": "",
    "text": "5.1 Misc",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-misc",
    "href": "qmd/db-dbt.html#sec-db-dbt-misc",
    "title": "5  dbt",
    "section": "",
    "text": "List of available DB Adaptors\n\nRuns on Python, so adaptors are installed via pip Notes from\nAnatomy of a dbt project\nWhat is dbt? Docs\n\nResources\n\nCreate a Local dbt Project\n\nUses docker containers to set up a local dbt project and a local postgres db to play around with\n\n\nArchitecture\n\nTypical Workflow\ndbt deps\ndbt seed\ndbt snapshot\ndbt run\ndbt run-operation {{ macro_name }}\ndbt test\nStyle Guide Components\n\nNaming conventions (the case to use and tense of the column names)\nSQL best practices (commenting code, CTEs, subqueries, etc.)\nDocumentation standards for your models\nData types of date, timestamp, and currency columns\nTimezone standards for all dates\n\nCastor - tool that takes your project and autofills much of the documentation\n\n\nHas a free tier\nVery helpful if you have the same column name in multiple datasets, you don’t have to keep defining it\nTribal Knowledge\n\nWhen a dataset is discussed in a team slack channel, Castor pulls the comments and adds them to the documentation of the dataset\n\n\nLightdash - BI tool for dbt projects - free tier for self hosting",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-setup",
    "href": "qmd/db-dbt.html#sec-db-dbt-setup",
    "title": "5  dbt",
    "section": "5.2 Set-Up",
    "text": "5.2 Set-Up\n\nBasic set-up: Article\n\nExample uses postgres adaptor\n\nWithin a python virtual environment\n\nCreate: python3 -m vevn dbt-venv\nActivate: source dbt-venv/bin/activate\n\nShould be able to see a (dbt-venv) prefix in every line on the terminal\n\nInstall dbt-core: pip install dbt-core\n\nSpecific version: pip install dbt-core==1.3.0\nConfirm installation by checking version: dbt --version\n\nInstall plugins\n\n\npip install dbt-bigquery\npip install dbt-spark\n# etc...",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-desc",
    "href": "qmd/db-dbt.html#sec-db-dbt-desc",
    "title": "5  dbt",
    "section": "5.3 Description",
    "text": "5.3 Description\n\nBuilt for data modeling\n\nmodels are like sql queries\n\nModularizes SQL code and makes it reusable across “models”\n\nRunning the orders “model” also runs the base_orders model and base_payments model (i.e. dependencies for orders)\n\nNot sure this exactly right. Seems like doing this would result in wasting time rerunning the same dependencies multiple times\n\nbase_orders and base_payments are independent in that they can also be used in other models\nCreates more dependable code because you’re using the same logic in all your models\nMakes runs faster since you aren’t wasting time and resources running the same blocks of code over and over again\nYou can schedule running sets of models by tagging them (e.g. #daily, #weekly)\nVersion Control\n\nsnapshots provide mechanism for versioning datasets\nWithin every yaml file is an option to include the version\n\npackage add-ons that allow you to interact with spark, snowflake, duckdb, redshift, etc.\nDocumentation for every step of the way\n\n.yml files can be used to generate a website (localhost:8080) around all of your dbt documentation.\n\ndbt docs generate\ndbt docs serve\n\nCurrent understanding\n\nData is brought in from warehouses via base models and basic transformations are performed (models &gt;&gt; staging directory)\nThen the data is transformed to the desired state via intermediate models and calculations performed (models &gt;&gt; marts directory)\nThen the final product is stored in the data directory",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-opt",
    "href": "qmd/db-dbt.html#sec-db-dbt-opt",
    "title": "5  dbt",
    "section": "5.4 Optimizations",
    "text": "5.4 Optimizations\n\nusing an M1, the new Apple laptops with Apple’s own CPUs improved speed by 3x\nupgrading from dbt 0.15.0 -&gt; 0.20.0 resulted in another 3x speed increase\nmoving dbt out of a container resulted in a 2x speed increase for those using a Intel CPU MacBook Pro\nRuns parallelized\n\nModels that have dependencies aren’t run until their upstream models are completed but models that don’t depend on one another are run at the same time.\nthread  parameter in your dbt_project.yml specifies how many models are permitted to run in parallel\n\nBigQuery",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-comp",
    "href": "qmd/db-dbt.html#sec-db-dbt-comp",
    "title": "5  dbt",
    "section": "5.5 Components",
    "text": "5.5 Components\n\nProject Templates\n\nStyle Guide\n\nMore detailed: link\n\nExample Starter Project\n\n\nprofiles.yml\n\nNot included in project directory\nOnly have to worry about this file if you set up dbt locally.\ndoc\nCreated by dbt init in ~/.dbt/\nContents\n\ndatabase connection, database credentials that dbt will use to connect to the data warehouse\nIf you work on multiple projects locally, the different project names (configured in the dbt_project.yml file) will allow you to set up various profiles for other projects.\n… something about “targets” but not sure what this is or how it’s used\n\n\ndbt_project.yml\n\ndoc\nmain configuration file for your project\nFields where you need to change default values to your project name:\n\nProject Name, Profile Name\n\nVariables\n\nDefined in the project yaml and used in models\n\nAccessed using var\n\nExample: Assigning States to Regions\n\nDefine variables in dbt_project.yml\nvars:\n  state_lookup:\n    Northeast:\n      - CT\n      - ME\n    Midwest:\n      - IL\n      - IN\n\n\n\n\nUsing the variables in a model\n{# Option 1 #}\nSELECT state,\n      CASE {% for k, v in var(\"state_lookup\").items() %}\n            WHEN state in ({% for t in v %}'{{ t }}'{% if not loop.last %}, {% endif %}{% endfor %}) THEN {{ k }}{% endfor %}\n            ELSE NULL END AS region\n  FROM {{ ref('my_table') }}\n\n{# Option 2 #}\nSELECT state,\n      CASE {% for k, v in var(\"state_lookup\").items() %}\n            WHEN state in ({{ t|csl }}) THEN {{ k }}{% endfor %}\n            ELSE NULL END AS region\n  FROM {{ ref('my_table') }}\n\nThis is a complicated example, see docs for something simpler\n{% ... %} are used to encapsulate for-loops and if-then conditions, see docs\n\n{# ... #} is for comments\n\nOption 2 uses a csl filter (comma-separated-list)\n\nModels section\n\nmy_new_project\n\nModels Misc\n\nKey Components of a Well-Written Data Model\n\nModularity where possible\nSame as the functional mindset: “if there’s any code that’s continually repeated, then it should be a function(i.e. its own separate model in dbt).”\nReadability\nComment\nUse CTEs instead of subqueries\nUse descriptive names\n\nExample: if you are joining the tables “users” and “addresses” in a CTE, you would want to name it “users_joined_addresses” instead of “user_addresses”\n\n\nExample: Comments, CTE, Descriptive Naming\nWITH\nActive_users AS (\n  SELECT\n    Name AS user_name,\n    Email AS user_email,\n    Phone AS user_phone,\n    Subscription_id\n  FROM users\n  --- status of 1 means a subscription is active\n  WHERE subscription_status = 1\n),\nActive_users_joined_subscriptions AS (\n  SELECT\n    Active_users.user_name,\n    active_users.user_email,\n    Subscriptions.subscription_id,\n    subscriptions.start_date ,\n    subscriptions.subscription_length\n  FROM active_users\n  LEFT JOIN subscriptions\n    ON active_users.subscription_id = subscriptions.subscription_id\n)\nSELECT * FROM Active_users_joined_subscriptions\n\n\nModel categories: staging, marts, base/intermediate\n* Staging\n    * Contains all the individual components of your project that the other layers will use in order to craft more complex data models.\n    * Each model bears a one-to-one relationship with the source data table it represents (i.e. 1 staging model per source)\n    * Typical Transformations: recasting, column renaming, basic computations (such as KBs to MBs or GBs), categorization (e.g. using CASE WHEN statements).\n        * Aggregations and joins should also be avoided\n    * Usually materialized as views.\n        * Allows any intermediate or mart models referencing the staging layer to get access to fresh data and at the same time it saves us space and reduces costs.\n    * Example\n        * Both the Stripe and Braintree payments are recast into a consistent shape, with consistent column names.\n* Marts\n    * Where everything comes together in a way that business-defined entities and processes are constructed and made readily available to end users via dashboards or applications.\n    * Since this layer contains models that are being accessed by end users it means that performance matters. Therefore, it makes sense to materialize them as tables.\n        * If a table takes too much time to be created (or perhaps it costs too much), then you may also need to consider configuring it as an incremental model.\n    * A mart model should be relatively simple and therefore, too many joins should be avoided\n    * Example\n        * A monthly recurring revenue (MRR) model that classifies revenue per customer per month as new revenue, upgrades, downgrades, and churn, to understand how a business is performing over time.\n            * It may be useful to note whether the revenue was collected via Stripe or Braintree, but they are not fundamentally separate models.\n    Base/Intermediate\n        Base\n            basic transformations (e.g. cleaning up the names of the columns, casting to different data types)\n            \n            Other models use these models as data sources\n                prevents errors like accidentally casting your dates to two different types of timestamps, or giving the same column two different names.\n                    two different timestamp castings can cause all of the dates to be improperly joined downstream, turning the model into a huge disaster\n                    \n            Usually occuring in staging\n            \n            read directly from a source, which is typically a schema in your data warehouse\n                Source object\n                    `{{ source('campaigns', 'channel'){style='color: goldenrod'}[}}]{style='color: goldenrod'}`\n                    \n                * campaigns is the name of the source in the .yml file\n                * channel is the name of a table from that source\n        Intermediate\n            Brings together the atomic building blocks that reside on staging layer such that more complex and meaningful models are constructed\n            \n            Usually occuring in marts\n            \n            Additional transformations that particular marts-models require\n            * Created to isolate complex operations\n                Typically used for joins between multiple base models\n                \n        * Should not be directly exposed to end users via dashboards or applications\n        * Other models should reference them as Common Table Expressions although there may be cases where it makes sense to materialize them as Views\n            * Macros called via `run-operation` cannot reference ephemeral objects such as CTEs\n            * Recommended to start with ephemeral objects unless this doesn’t work for the specific use case\n            * Whenever you decide to materialize them as Views, it may be easier to to do so in a [custom schema](https://docs.getdbt.com/docs/build/custom-schemas), that is a schema outside of the main schema defined in your dbt profile.\n        * If the same intermediate model is referenced by more than one model then it means your design has probably gone wrong.\n            * Usually indicates that you should consider turning your intermediate model into a macro.\n            reference the base models rather than from a source\n            * Reference object\n                * `{{ ref('base_campaign_types'){style='color: goldenrod'}[}}]{style='color: goldenrod'}`\n                * base\\_campaign\\_types is a base model\n\nTagging\n\nAllows you to run groups of models\n\nExample dbt run --models tag:daily\n\n\nDirectories models Sources (i.e. data sources) are defined in src_ .yml files in your models directory * .yml files contain definitions and tests * .doc files contain source documentation * Models (i.e. sql queries) are defined stg__yml * .yml files contain definitions and tests * .doc files contain source documentation * The actual models are the .sql files Example  * staging: * Different data sources will have separate folders underneath staging (e.g. stripe). * marts: * Use cases or departments have different folders underneath marts (e.g. core or marketing) data contains all manual data that will be loaded to the database by dbt.\n      To load the .csv files in this folder to the database, you will have to run the `dbt seed` command.\n\n      For github or other repos, do not put large files or files with sensitive information here\n          acceptable use cases: yearly budget, status mappings, category mappings, etc\n\nsnapshots\n\ncaptures of the state of a table at a particular time\ndoc\nbuild a slowly changing dimension (SCD) table for sources that do not support change data capture (CDC)\nExample\n\nEvery time the status of an order change, your system overrides it with the new information. In this case, there we cannot know what historical statuses that an order had.\nDaily snapshots of this table builds a history and allows you to track order statuses\n\n\n\nMacros\n\nsimilar to functions in excel\ndefine custom functions in the macros folder or override default macros and macros from a package\nSee bkmks for tutorials on writing custom macros with jinja\n{dbtplyr} macros\n\ndplyr tidy selectors, across, etc.\n\n\nSeeds\n\nSeeds are csv files that you add to your dbt project to be uploaded to your data warehouse.\n\nUploaded into your data warehouse using the dbt seed command\n\nBest suited to static data which changes infrequently.\n\nExamples of use cases:\n\nA list of unique codes or employee ids that you may need in your analysis but is not present in your current data.\nA list of mappings of country codes to country names\nA list of test emails to exclude from analysis\n\n\nReferenced in downstream models the same way as referencing models — by using the ref function",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-pkgs",
    "href": "qmd/db-dbt.html#sec-db-dbt-pkgs",
    "title": "5  dbt",
    "section": "5.6 Packages",
    "text": "5.6 Packages\n\npackages.yml\n\nlist of external dbt packages you want to use in your project\n\navailable packages: link\nformat\n\n        packages:\n            - package: dbt-labs/dbt_utils\n              version: 0.7.3\n\nInstall packages - dbt deps\nsome of the packages (there are a lot of packages and I didn’t get through them all)\n\n{audit-helper}\n\ncompares columns, queries; useful if refactoring code or migrating db\n\n{codegen}\n\ngenerate base model, barebones model and source .ymls\n\n{dbt-athena} - adaptor for AWS Athena\n{dbt-expectations}\n\ndata validation based on great expectations py lib\n\n{dbt-utils}\n\nton of stuff for tests, queries, etc.\n\n{dbtplyr} macros\n\ndplyr tidy selectors, across, etc.\n\n{dbt-duckdb} - adapter for duckdb\n{external-tables}\n\ncreate/replace/refresh external tables\nGuessing this means any data source (e.g. s3, spark, google, another db like snowflake, etc.) that isn’t the primary db connected to the dbt project\n\n{logging}\n\nprovides out-of-the-box functionality to log events for all dbt invocations, including run start, run end, model start, and model end.\ncan slow down runs substantially\n\n{re_data}\n\ndashboard for monitoring, macros, models\n\n{profiler}\n\nimplements dbt macros for profiling database relations and creating doc blocks and table schemas (schema.yml) containing said profiles\n\n{spark-utils}\n\nenables use of (most of) the {dbt-utils} macros on spark",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-dvaut",
    "href": "qmd/db-dbt.html#sec-db-dbt-dvaut",
    "title": "5  dbt",
    "section": "5.7 Data Validation and Unit Tests",
    "text": "5.7 Data Validation and Unit Tests\n\n\nBuilt-in support for CI/CD pipelines to test your “models” and stage them before committing to production\n\nMisc\n\nSee also\n\nDocs\npackages (see above)\nHow to do Unit Testing in dbt\n\n\n\nMost of the tests are defined in a models-type .yml file in the models directory\nUses pre-made or custom macros\n\ncustom (aka singular) tests should be located in a tests folder.\n\ndbt will evaluate the SQL statement.\nThe test will pass if no row is returned and failed if at least one or more rows are returned.\nUseful for testing for some obscurity in the data\nExample: Check for duplicate rows when joining two tables\n\n\n\nselect \na.id \nfrom {{ ref(‘table_a’) }} a \nleft join {{ ref(‘table_b’) }} b \non a.b_id = b.id \ngroup by a.id \nhaving count(b.id)&gt;1\n\ni.e. If I join table a with table b, there should only be one record for each unique id in table a\nProcess\n\njoin the tables on their common field\ngroup them by the id that should be distinct\ncount the number of duplicates created from the join.\n\nThis tells me that something is wrong with the data.\nadd a having clause to filter out the non-dups\n\n\nCan be applied to a model or a column\nRun test - dbt test\nMock data\n\nData used for unit testing SQL code\nTo ensure completeness, it’s best if analysts or business stakeholders are the ones provide test cases or test data\nStore in the “data” folder (typically .csv files)\n\neach CSV file represents one source table\nshould be stored in a separate schema (e.g. unit_testing) from production data\ndbt seed (see below, Other &gt;&gt; seeds) command is used to load mock data into the data warehouse\n\n\nTests\n\nfreshness (docs) - used to define the acceptable amount of time between the most recent record, and now, for a table\n\nExample\n\n\n\nsources:\n  - name: users\n    freshness:\n      warn_after:\n        count: 3\n        period: day\n      error_after:\n        count: 5\n        period: day\n\nExample\nExample: Unit Test\n\nFig shows the mock data (.csv) files\nAdd test to dbt_project.yml\n\n\nseeds:\n  unit_testing:\n    revenue:\n      schema: unit_testing\n      +tags:\n        - unit_testing\n\nEvery file in the unit_testing/revenue folder will be loaded into unit_testing\nExecuting dbt build -s +tag:unit_testing will run all the seeds/models/tests/snapshots with tag unit_testing and their upstreams\nCreate macro that switches the source data in the model being tested from production data (i.e. using { source() } ) to mock data (i.e. using ref ) when a unit test is being run\n\n{% macro select_table(source_table, test_table) %}\n      {% if var('unit_testing', false) == true %}\n\n            {{ return(test_table) }}\n      {% else %}\n            {{ return(source_table) }}\n      {% endif %}\n{% endmacro %}\n\nArticle calls this file “select_table.sql”\n2 inputs: “source_table” (production data) and “test_table” (mock data)\nmacro returns the appropriate table based on the variable in the dbt command\n\nIf the command doesn’t provide unit_testing variable or the value is false , then it returns source_table , otherwise it returns test_table.\n\nAdd macro code chunk to model\n\n{{ config\n    (\n        materialized='table',\n        tags=['revenue']\n    )\n}}\n{% set import_transaction = select_table(source('user_xiaoxu','transaction'), ref('revenue_transaction')) %}\n{% set import_vat = select_table(source('user_xiaoxu','vat'), ref('revenue_vat')) %}\nSELECT\n    date\n    , city_name\n    , SUM(amount_net_booking) AS amount_net_booking\n    , SUM(amount_net_booking * (1 - 1/(1 + vat_rate)))  AS amount_vat\nFROM {{ import_transaction }}\nLEFT JOIN {{ import_vat }} USING (city_name)\nGROUP BY 1,2\n\nInside the {%...%} , the macro “select_table” is called to set the local variables, “import_transaction” and “import_vat” which are later used in the model query\nModel file is named “revenue2.sql”\nRun model and test using mock data: dbt build -s +tag:unit_testing --vars 'unit_testing: true'\n\nRun model with production data (aka source data): dbt build -s +tag:revenue --exclude tag:unit_testing\n\nCompare output\n\nversion: 2\nmodels:\n  - name: revenue\n    meta:\n      owner: \"@xiaoxu\"\n    tests:\n      - dbt_utils.equality:\n          compare_model: ref('revenue_expected')\n          tags: ['unit_testing']\n\nmodel properties file that’s named “revenue.yml” in the models directory\nBy including tags: ['unit_testing'] we can insure that we don’t run this test in production (see build code above with --exclude tag:unit_testing\nMacro for comparing numeric output\n\n{% test advanced_equality(model, compare_model, round_columns=None) %}\n{% set compare_columns = adapter.get_columns_in_relation(model) | map(attribute='quoted') %}\n{% set compare_cols_csv = compare_columns | join(', ') %}\n{% if round_columns %}\n    {% set round_columns_enriched = [] %}\n    {% for col in round_columns %}\n        {% do round_columns_enriched.append('round('+col+')') %}\n    {% endfor %}\n    {% set selected_columns = '* except(' + round_columns|join(', ') + \"), \" + round_columns_enriched|join(', ') %}\n{% else %}\n    {% set round_columns_csv = None %}\n    {% set selected_columns = '*' %}\n{% endif %}\nwith a as (\n    select {{compare_cols_csv}} from {{ model }}\n),\nb as (\n    select {{compare_cols_csv}} from {{ compare_model }}\n),\na_minus_b as (\n    select {{ selected_columns }} from a\n    {{ dbt_utils.except() }}\n    select {{ selected_columns }} from b\n),\nb_minus_a as (\n    select {{ selected_columns }} from b\n    {{ dbt_utils.except() }}\n    select {{ selected_columns }} from a\n),\nunioned as (\n    select 'in_actual_not_in_expected' as which_diff, a_minus_b.* from a_minus_b\n    union all\n    select 'in_expected_not_in_actual' as which_diff, b_minus_a.* from b_minus_a\n)\nselect * from unioned\n{% endtest %}\n\nFile called “advanced_equality.sql”",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>dbt</span>"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html",
    "href": "qmd/geospatial-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-misc",
    "href": "qmd/geospatial-general.html#sec-geo-gen-misc",
    "title": "General",
    "section": "",
    "text": "QGIS - free and open source\nArcGIS - expensive and industry-standard\nspatiotemporal data — data cubes with spatial and regular temporal dimensions — such as\n\ne.g. gridded temperature values (raster time series) and vector data with temporal records at regular temporal instances (e.g. election results in states).\n\n{stars} - regular intervals\n{sftime} - irregular intervals\n\n\nSpatial Resampling\n\nCreates cross-validation folds by k-means clustering coordinate variables\nlibrary(tidymodels)\nlibrary(spatialsample)\nset.seed(123)\nspatial_splits &lt;- spatial_clustering_cv(landslides, coords = c(\"x\", \"y\"), v = 5)\n\n# fit a logistic model\nglm_spec &lt;- logistic_reg()\nlsl_form &lt;- lslpts ~ slope + cplan + cprof + elev + log10_carea \nlsl_wf &lt;- workflow(lsl_form, glm_spec)\ndoParallel::registerDoParallel() \nregular_rs &lt;- fit_resamples(lsl_wf, bad_folds)",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-terms",
    "href": "qmd/geospatial-general.html#sec-geo-gen-terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nCensus Block Groups - ~600–3,000 population; the smallest geography reported; Wiki\nCensus Tract - ~4,000 average population; Docs\n\nAlso see Survey, Census Data &gt;&gt; Geographies\n\nGraticules - a network of lines on a map that delineate the geographic coordinates (degrees of latitude and longitude.)\n\nUse of graticules is not advised, unless the graphical output will be used for measurement or navigation, or the direction of North is important for the interpretation of the content, or the content is intended to display distortions and artifacts created by projection. Unnecessary use of graticules only adds visual clutter but little relevant information. Use of coastlines, administrative boundaries or place names permits most viewers of the output to orient themselves better than a graticule\n{sf::st_graticule}\n\nRaster Data - Grid data (instead of point/polygon data in Vector Data) where each square on this grid is a small cell, and each cell holds a single value representing some real-world phenomenon, e.g. elevation, temperature, land cover type, rainfall amount, or color of a pixel in a satellite image. The entire collection of these cells and their values is what we call raster data. Raster data is better for continuous phenomena like elevation, soil moisture, or temperature. Most data from satellites and aerial photography comes in raster form.\nVector Data - Data that uses points, lines, and polygons (instead of grid cells like Raster Data) to represent features like roads, buildings, or country borders. Vector data is precise and good for discrete objects.\nVRT - File format that allows a virtual GDAL dataset to be composed from other GDAL datasets with repositioning, and algorithms potentially applied as well as various kinds of metadata altered or added. VRT descriptions of datasets can be saved in an XML format normally given the extension .vrt.\n\nBasically a metadata XML file describing various properties of the actual raster file, like pixel dimensions, geolocation, etc..",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-opt",
    "href": "qmd/geospatial-general.html#sec-geo-gen-opt",
    "title": "General",
    "section": "Optimization",
    "text": "Optimization\n\nMisc\nVector Tiles\n\nMisc\n\nNotes from Push the limits of interactive mapping in R with vector tiles\n\nMcBain goes through a complete example with plenty of tips on simplification strategies and hosting mbtiles files\n\nIssues (solution: Vector Tiles)\n\nLimited number of features with DOM canvas\n\nThere’s a limit to how many features leaflet maps can handle, because at some point the DOM gets too full and your browser stops being able to parse it.\n\nLimited number of maps on same webpage\n\nOnce you start rendering spatial data on WebGL canvasses instead of the DOM you’ll find there is a low number of WebGL contexts that can co-exist on any one web page, typically limiting you to only around 8 maps.\n\nFile sizes blow up to hundreds of MB\n\nTrying to reuse WebGL maps by toggling on and off different layers of data for the user at opportune times. This is an improvement, but data for all those layers piles up, and your toolchain wants to embed this in your page as reams of base64 encoded text. Page file sizes are completely blowing out.\n\n\n\nUse Cases\n\nSimplification of geometry is not desirable, e.g. because of alignment issues\n\ne.g. The zoomed-in road network has to align with the road network on the basemap, so that viewers can see features that lie along sections of road.\n\nSimplification of geometry doesn’t really help, you still have too many features\nCumulatively your datasets are too large to handle.\n\nVector Tiles  - contain arrays of annotated spatial coordinate data which is combined with a separately transmitted stylesheet to produce the tile image.\n\ni.e. The edges of the roads, the boundaries of buildings etc. Not an image, but the building blocks for one\nDifferent stylesheets can use the same vector data to produce radically different looking maps that either highlight or omit data with certain attributes\nMapbox Vector Tiles (MVT) - specification; the de-facto standard for vector tile files\n\nstored as a Google protocol buffer - a tightly packed binary format.\n\n\nMBTiles - by Mapbox; describe a method of storing an entire MVT tileset inside a single file.\n\nInternally .mbtiles files are SQLlite databases containing two tables: metadata and tiles.\n\ntiles table\n\nindexed by z,x,y\ncontains a tile_data column for the vector tile protocol buffers, which are compressed using gzip\n\n\nSQLite format and gzip compression help with efficient retrieval and transmission\n\nUsing vector tiles we can have unlimited reference layers. Each one contributes nothing to the report file size since it is only streamed on demand when required.\nWorkflow to convert data to .tbtiles\n\nIn R, read source data as an sf, and wrangle\n\nTippecanoe expects by epsg 4326 by default\n\nWrite data out to geojson\nOn the command line, convert geojson to .mbtiles using the tippecanoe command line utility.\n\nTippecanoe sources\n\nMapbox version - repo\n\nMcBain says, he uses this version and hasn’t had any problems\nREADME has helpful cookbook section\n\nActively maintained community forked version - repo\nMay be a headache to get dependencies if using Windows\n\nAlternatively it can output a folder structure full of protocol buffer files.\n\nExample\ntippecanoe -zg \\\n          -o abs_mesh_blocks.mbtiles \\\n          --coalesce-densest-as-needed \\\n          --extend-zooms-if-still-dropping \\\n          mb_shapes.geojson\n\nMapping\n\nExample\nlibrary(mvtview)\nlibrary(rdeck)\n\n# Fire up the server\nserve_mvt(\"abs_mesh_blocks.mbtiles\", port = 8765)\n# Serving your tile data from http://0.0.0.0:8765/abs_mesh_blocks.json.\n# Run clean_mvt() to remove all server sessions.\n\nmesh_blocks &lt;- jsonlite::fromJSON(\"http://0.0.0.0:8765/abs_mesh_blocks.json\")\n\n# Map the data\nrdeck(\n    initial_bounds = structure(meshblocks$bounds, crs = 4326, class = \"bbox\") # set map limits using the tilejson\n) |&gt;\n  add_mvt_layer(\n    data = rdeck::tile_json(\"http://0.0.0.0:8765/abs_mesh_blocks.json\"),\n    get_fill_color = scale_color_linear(\n      random_attribute\n    ),\n    opacity = 0.6\n  )\n\nSee McBain article for options on hosting .mbtiles files\nRegarding “abs_mesh_blocks”: {mvtview} provides a way to fetch the metadata table from .mbtiles as json by querying a json file with the same name as the .mbitles file.\nThe structure of ‘tilejson’ is yet another specification created by Mapbox, and is supported in deck.gl (and therefore {rdeck}) to describe tile endpoints.",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-gridsys",
    "href": "qmd/geospatial-general.html#sec-geo-gen-gridsys",
    "title": "General",
    "section": "Grid Systems",
    "text": "Grid Systems\n\nMisc\n\nExplainer: Why using hexbins to visualize Australian electoral map is better than a typical provincial map.\n\ntl;dr: Geographical size distorts what the value is trying to measure. The value is the party that wins the parliamentary seat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe bar graph shows the values the map is trying to visualize geographically. The hexabins better represent the close race by removing the distorting element which is the geographical size of the provinces.\nEach voting district (hexabin) is voting for 1 representative and has the same number of voters, but districts can have vastly different areas depending on population density.\n\nKeep unit at constant size (like hexabins) but alter hex shape to keep state shape.\n\n\nA better U.S. house election results map?\nResults\n\nstate size depends on number of districts which depends on population and therefore correctly conveys voting results visually across the whole country\nDistricts get distorted but the states retain their shape and so distortion of the overall visualization is minimized\n\n\n\n\nUber’s H3 grid system -\n\nMisc\n\nPackages: {h3r}, {h3-r}\ndocs\nAdd census data to H3 hexagons, calculate overlaps (article)\nFor large areas, you can reduce the number of hexagons by merging some hexagons into larger hexagons.\n\nReduces storage size\nIssue: leaves small gaps between hexagons\n\nmight not matter for your use case\n\nSolution: use Microsoft’s Quadkeys approach (see article)\n\n\nEach hexagon has a series of smaller hexagons that sit (mostly) inside of another, which creates a hierarchy that can be used for consistent referencing and analysis, all the way down to lengths of 2 feet for the edges.\n“Hexagons were an important choice because people in a city are often in motion, and hexagons minimize the quantization error introduced when users move through a city. Hexagons also allow us to approximate radiuses easily.”\nRe other shapes: “We could use postal code areas, but such areas have unusual shapes and sizes which are not helpful for analysis, and are subject to change for reasons entirely unrelated to what we would use them for. Zones could also be drawn by Uber operations teams based on their knowledge of the city, but such zones require frequent updating as cities change and often define the edges of areas arbitrarily”\nGrid systems can have comparable shapes and sizes across the cities that Uber operates in and are not subject to arbitrary changes. While grid systems do not align to streets and neighborhoods in cities, they can be used to efficiently represent neighborhoods by clustering grid cells. Clustering can be done using objective functions, producing shapes much more useful for analysis. Determining membership of a cluster is as efficient as a set lookup operation.\n16 Resolutions\n\n0 - 15 (0 being coarsest and 15 being finest)\nEach finer resolution has cells with one seventh the area of the coarser resolution. Hexagons cannot be perfectly subdivided into seven hexagons, so the finer cells are only approximately contained within a parent cell.\nThe identifiers for these child cells can be easily truncated to find their ancestor cell at a coarser resolution, enabling efficient indexing. Because the children cells are only approximately contained, the truncation process produces a fixed amount of shape distortion. This distortion is only present when performing truncation of a cell identifier; when indexing locations at a specific resolution, the cell boundaries are exact.\nWant a resolution granular enough to introduce variability and wide enough to capture the effects of an area\nExample of resolution 6 in Iowa",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-feats",
    "href": "qmd/geospatial-general.html#sec-geo-gen-feats",
    "title": "General",
    "section": "Features",
    "text": "Features\n\nCarto Spatial Features dataset ($) - https://carto.com/spatial-data-catalog/browser/?country=usa&category=derived&provider=carto\n\nResolution: Quadgrid level 15 (with cells of approximately 1x1km) and Quadgrid level 18 (with cells of approximately 100x100m).\n\nGuessing if the areas you’re interested in have high population density, then maybe 100 x 100 m cells would be more useful\n\nFeatures\n\nTotal population\nPopulation by gender\nPopulation by age and gender (e.g. female_0_to_19)\nPOIs by category\n\nRetail Stores\nEducation\n\nNumber of education related POIs, incuding schools, universities, academies, etc.\n\nFinancial\n\nNumber of financial sector POIs, including ATMs and banks.\n\nFood, Drink\n\nNumber of sustenance related POIs, including restaurants, bars, cafes and pubs.\n\nHealthcare\n\nNumber of healthcare related POIs, including hospitals\n\nLeisure\n\nNumber of POIs related to leisure activities, such as theaters, stadiums and sport centers.\n\nTourism\n\nNumber of POIs related to tourism attractions\n\nTransportation\n\nNumber of transportation related POIs, including parking lots, car rentals, train stations and public transport stations.\n\n\n\n\nCarto Data Observatory ($) - https://carto.com/spatial-data-catalog/browser/dataset/mc_geographic\\_\\_4a11e98c/\n\nFeatures\n\nGeo id\nRegion id\nIndustry\nTotal Transactions Amount Index\nTransaction Count Index\nAccount Count Index\nAverage Ticket Size Index\nAverage Frequency of Transaction per Card Index\nAverage Spend Amount by Account Index",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-interx",
    "href": "qmd/geospatial-general.html#sec-geo-gen-interx",
    "title": "General",
    "section": "Interactions",
    "text": "Interactions\n\nSimilar to interpolation but keeps the original spatial units as interpretive framework. Hence, the map reader can still rely on a known territorial division to develop its analyses\n\nThey produce understandable maps by smoothing complex spatial patterns\nThey enrich variables with contextual spatial information.\n\nMisc\n\nResources\n\nGetting Started with Potential - nice little mathematical summary, some background\n\nPackages\n\n{potential}: spatial interaction modeling via Stewart Potentials. Also capable of interpolation\n\n\nThere are two main ways of modeling spatial interactions: the first one focuses on links between places (flows), the second one focuses on places and their influence at a distance (potentials).\nComparisons ({potential}article)\n\nGDP per capita (cloropleth)\n\n\nTypical cloropleth at the municipality level\nValues have been binned\n\nPotential GDP per Capita (interaction)\n\n\nStewart Potentials have smoothed the values\nMunicipality boundaries still intact, so you could perform an analysis based on these GDP regions\n\nSmoothed GDP per Capita (interpolation)\n\n\nSimilar results as the interaction model except there are no boundaries",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-interp",
    "href": "qmd/geospatial-general.html#sec-geo-gen-interp",
    "title": "General",
    "section": "Interpolation",
    "text": "Interpolation\n\nMeasurements can have strong regional variance, so the geographical distribution of measurements can have a strong influence on statistical estimates.\n\nExample: Temperature\n\n\nTwo different geographical distributions of sensors\n\n\nA concentration of sensors in North can lead to a cooler average regional temperature and vice versa for the South.\n\nDistribution of temperatures across the region for 1 day.\n\n\nWith this much variance in temperature, a density of sensors in one area can distort the overall average.\n\nInterpolation evens out the geographical distribution of measurments\n\n\n\nThe process of using points with known values to estimate values at other points. In GIS applications, spatial interpolation is typically applied to a raster with estimates made for all cells. Spatial interpolation is therefore a means of creating surface data from sample points.\nMisc\n\n{gstat} - Has various interpolation methods.\n\nKriging\n\n\nConsiders not only the distances but also the other variables that have a linear relationship with the estimation variables\nUses a correlated Gaussian process to guess at values between data points\n\n\nUncorrelated - white noise\nCorrelated - smooth\n\nThe closer in distance two points are to each other, the more likely they are to have a similar value (i.e. geospatially correlated)\nExample: Temperature\n\n\nFewer known points means greater unceretainty\n\n\nInputs:\n\nThe measured values at the sampling points,\nThe geometric coordinates of the sampling points,\nThe geometric coordinates of the target points to interpolate,\nThe “calibrated” probabilistic model, with the spatial correlation obtained by data\n\nOutputs:\n\nThe estimated values at the target points,\nThe estimated uncertainty (variance) at the target points.",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/production-tools.html",
    "href": "qmd/production-tools.html",
    "title": "38  Tools",
    "section": "",
    "text": "38.1 Misc",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-misc",
    "href": "qmd/production-tools.html#sec-prod-tools-misc",
    "title": "38  Tools",
    "section": "",
    "text": "Overview of some 2021 tools Descriptions in article \nAWS Batch - Managed service for computational jobs. Alternative to having to maintain a kubernetes cluster\n\nTakes care of keeping a queue of jobs, spinning up EC2 instances, running code and shutting down the instances.\nScales up and down depending on how many jobs submitted.\nAllows you to execute your code in a scalable fashion and to request custom resources for compute-intensive jobs (e.g., instances with many CPUs and large memory) without requiring us to maintain a cluster\nSee bkmks: Hosting &gt;&gt; AWS &gt;&gt; Batch\nPackages:\n\n{crew.aws.batch}",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-stckcomprnk",
    "href": "qmd/production-tools.html#sec-prod-tools-stckcomprnk",
    "title": "38  Tools",
    "section": "38.2 Stack Component Rankings",
    "text": "38.2 Stack Component Rankings\n\nDB format\n\narrow files\n\nELT Operations\n\n*dbt\n\nGoogle’s alternative is Dataform\nAWS’s alternative is Databrew\n\n*Spark\n*Google Big Query SQL\n*AWS Athena\n\nOrchestration and monitoring\n\n*Targets\n\n+ {cronR} for orchestration + scheduling\n\n*Mage-AI\n*AWS Glue\nPrefect\nAirflow\n\nData Ingestion\n\nAirbyte (data ingestion)\nfivetran (data ingestion)\n\nCan “process atomic REST APIs to extract data out of SAAS silos and onto your warehouse”\n\nterraform (multi-cloud management)\n\nTracking/Versioning for Model Building\n\n*DVC\nMLFlow\n\nReporting\n\nblastula (email), xaringan (presentation), RMarkdown (reports), flexdashboard (dashboards),\nRStudio Connect (publishing platform to stakeholders)\n\ndashboards, apps\non-demand and scheduled reports\npresentations\nAPIs (?)\nPublish R and Python\nEnterprise security\nCan stay in RStudio\n\n\nVisualization Platforms\n\nLooker*\nPowerBI, DataStudio",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-depman",
    "href": "qmd/production-tools.html#sec-prod-tools-depman",
    "title": "38  Tools",
    "section": "38.3 Dependency Management",
    "text": "38.3 Dependency Management\n\nR\n\nr2u for linux installations\n\n“for Ubuntu 20.04 and 22.04 it provides _all_ of CRAN (and portion of BioConductor) as binary #Rstats packages with full, complete and automatic resolution of all dependencies for full system integration. If you use `bspm` along with it you can even use this via `install.packages()` and friends. Everything comes from a well connected mirror”",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#data-versioning",
    "href": "qmd/production-tools.html#data-versioning",
    "title": "38  Tools",
    "section": "38.4 Data versioning",
    "text": "38.4 Data versioning\n\nFlat Table by Github\n\nHas a Github action associated with it\nHas a datetime commit message\nLists as a feature that it tracks differences from one commit to the next, but doesn’t a normal data commit doe the same thing?\n\nLumberjack R package\n\nAdd functions to your processing script\ntracks using a log file\noptions for changes you want to track",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-dating",
    "href": "qmd/production-tools.html#sec-prod-tools-dating",
    "title": "38  Tools",
    "section": "38.5 Data Ingestion",
    "text": "38.5 Data Ingestion\n\nFiveTran\n\nFree-tier\nSync raw data sources\n\nevery 1hr for starter plan, every 15 minutes both standard plans, every 5 min for enterprise plan",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-orch",
    "href": "qmd/production-tools.html#sec-prod-tools-orch",
    "title": "38  Tools",
    "section": "38.6 Orchestration",
    "text": "38.6 Orchestration\n\n38.6.1 Airflow\n\nWebpage\nOpen-source platform for authoring, scheduling, and executing data pipelines.\n\nFeatures for managing and monitoring data pipelines, including integration with various data storage and processing technologies. Similar to the Unix cron utility — you write scripts and schedule them to run every X minutes.\nAirflow can be used for any sort of scheduling task, but is often used for scheduling data modeling. schedule, run and monitor the refresh of our data warehouse\nMonitoring on-prem checking Airflow logs is not user-friendly (better in AWS MWAA)\n\ndifferent types of logs for task, web server, scheduler, worker, and DAGs\nhave to SSH into the server and run commands which becomes more complicated when you want to use distributed servers for scalability.\n\nRequires you to create a central logging storage and make additional setup to make all servers write logs into that single place\n\n\nServer-based remains active even when not running jobs –&gt; continually incurring cost\n\nNo latency since servers are always running\n\nProblems\nLong feedback loop\n\nWhile programming, instant feedback of your DAG becomes crucial when you want a sanity check before your code goes too far.\nTo see the graph view, which is mainly for visualizing dependencies in DAGs, your code needs to be in the folder of an Airflow scheduler that can be picked up. The airflow scheduler also takes time to render and parse your DAG until it shows up.\nMakes debugging difficult during the development cycle, so some engineers write more lines of code and test them all together. If the lines of code become unmanageable on one screen, you might vaguely remember what to validate and what dependencies to check.\n\nDifficult with local development\n\na docker image can be used to inject as much production-related information as possible. But it’s still not 100% copy, and it takes tremendous effort to develop and maintain that docker image.\nEven if you set up dev, staging, and production environments for running Airflow, they aren’t totally isolated and developers can end-up interfering with one another. Services/Extensions Astronomer offers a managed Airflow service.\nAmazon Managed Workflows for Apache Airflow (MWAA) - managed Airflow service\n\nOrchestrate jobs in EMR, Athena, S3, or Redshift\n\nGlue\n\nAirflow has the glue operator\n\nCloudFormation can be used to configure and manage\n\nallows for autoscaling which saves on costs by scaling down when usage is low\nstill needs a server running even when not running jobs\nmonitoring much easier since all the logs are written into CloudWatch search certain logs using Logs Insights\n\nhave a dashboard that displays usage of server resources like CPU, memory, and network traffic.\nmonitor numerous other Airflow-specific metrics.\nset up alerts and manage notification recipients programmatically.\n\n\nCost factors\n\nInstance size\nAdditional worker instance\nAdditional scheduler instance\nMeta database storage\n\nPotential Issues:\n\nResources are shared on multiple jobs so performance can suffer if:\n\nDon’t distribute trigger times evenly\nMisconfigure your maximum worker count\n\n\nOperate through AWS SDK\n\nCan\n\ncreate, update, and delete MWAA environments and retrieve their environment information that includes logging policies, number of workers, schedulers\nrun Airflow’s internal commands to control DAGs\n\nCan’t\n\nSome of Airflow’s native commands like backfill (check this AWS document), dags list, dags list-runs, dags next-execution, and more\n\n\n\n\n\n\n\n38.6.2 AWS Glue\n\nCloud-based data integration service that makes it easy to move data between data stores.\n\nIncludes a data catalog for storing metadata about data sources and targets, as well as a ETL (extract, transform, and load) engine for transforming and moving data.\nIntegrates with other AWS services, such as S3 and Redshift, making it a convenient choice for users of the AWS ecosystem. Serverless (i.e. costs only incurred when triggered by event) Each job triggers separate resources, so if one job overloads resources, it doesn’t affect other jobs\nJobs experience latency since instances have to spin-up and install packages\nCost Charged by Data Processing Unit (DPU) multiplied by usage hours\n\nJob types:\n\nPython shell: you can choose either 0.0625 or 1 DPU.\nApache Spark: you can use 2 to 100 DPUs.\nSpark Streaming: you can use 2 DPUs to 100 DPUs.\n\n\n\nCan run Spark\nMonitoring\n\nCloudwatch\nGlueStudio within Glue Clicking number sends you to Cloudwatch where you can drill down into jobs\nCloudFormation can be used to configure and manage\nGlue SDK available\n\n\n\n\n38.6.3 Prefect\n\nEasier to manage for smaller data engineer teams or a single data engineer\nmore user friendly than Airflow; Better UI; more easily discover location and time of errors\npurely python\nMisc\n\nadd slack webhook for notifications\nHas slack channel to get immediate help with issues or questions\n\nautomatic versioning for every flow, within every project\n\nalso document the models deployed with each version in the README they provide with every flow\n\nComponents\n\nTasks - individual jobs that do one unit of work\n\ne.g. a step that syncs Fivetran data or runs a dbt model\n\nFlows - functions that consist of a bunch of smaller tasks, or units of work, that depend on one another\n\ne.g. 1 flow could be multiple tasks running Fivetran syncs and dbt models\n\nExample:\nfrom prefect import flow, task\n@flow(name=\"Create a Report for Google Trends\")\ndef create_pytrends_report(\n    keyword: str = \"COVID\", start_date: str = \"2020-01-01\", num_countries: int = 10\n):\n\nThese flows are then scheduled and run by whatever types of agents you choose to set up.\n\nSome options include AWS ECS, GCP Vertex, Kubernetes, locally, etc.\n\nDeployments (docs)\n\nAlso see Create Robust Data Pipelines with Prefect, Docker, and GitHub\nDefintions\n\nSpecify the execution environment infrastructure for the flow run\nSpecify how your flow code is stored and retrieved by Prefect agents\nCreate flow runs with custom parameters from the UI\nCreate a schedule to run the flow\n\nSteps\n\nBuild the deployment definition file and optionally upload your flow to the specified remote storage location\nCreate the deployment by applying the deployment definition\n\nSyntax: prefect deployment build [OPTIONS] &lt;path-to-your-flow&gt;:&lt;flow-name&gt;\nExample:\nprefect deployment build src/main.py:create_pytrends_report \\\n  -n google-trends-gh-docker \\\n  -q test\n\nDeployment for the flow create_pytrends_report (see flow example) from the file, “src/main.py”\n-n google-trends-gh-docker specifies the name of the deployment to be google-trends-gh-docker.\n-q test specifies the work queue to be test . A work queue organizes deployments into queues for execution.\nOutput\n\n“create_pytrends_report-deployment.yaml” file and a “.prefectignore” created in the current directory.\n\n“create_pytrends_report-deployment.yaml”:  specifies where a flow’s code is stored and how a flow should be run.\n“.prefectignore”:  prevents certain files or directories from being uploaded to the configured storage location.\n\n\n\n\n\n38.6.4 Azure Data Factory\n\nAllows users to create, schedule, and orchestrate data pipelines for moving and transforming data from various sources to destinations.\nData Factory provides a visual designer for building pipelines, as well as a range of connectors for integrating with various data stores and processing technologies.\nExample: Demand Planning Project\n\n\n\n38.6.5 Mage-AI\n\nEnables users to define DAG regardless of the choice of languages (python/SQL/R)\nWeb-based IDE, so its mobility allows working from different devices, and sharing becomes more straightforward.\n\nUI layout feels like using RStudio. It has many sections divided into different areas.\nOne of the areas is the DAG visualization which provides instant feedback to the user on the task relationship.\n\nDAGs\n\nThe pipeline or DAG is constructed with modular blocks—a block maps to a single file.\nBlock Options\n\nExecution with upstream blocks: this triggers all upstream blocks to get the data ready for the current block to run\nExecute and run tests defined in the current block: this focuses on the current block to perform testing.\nSet block as dynamic: this changes the block type into the dynamic block, and it fits better to create multiple downstream blocks at runtime.\n\nManipulate dependencies via drag and drop\n\nmage-ai keeps track of the UI changes the user made and automatically builds the dependencies DAG into the YAML file. (./pipelines/{your_awesome_pipeline_name}/metadata.yaml)\n\nVisualize data in each block\n\nHelpful for inspecting your input data and further validating the transformation.\nOnce the chart has been created, it will also be attached to the current block as the downstream_blocks.\n\n\nR\n\nAllows users to write the main ETL (Extraction, Transformation, and Loading) blocks using R.\n\n\n\n\n38.6.6 kestra\n\nPopular orchestration libraries such as Airflow, Prefect, and Dagster require modifications to the Python code to use their functionalities. You may need to modify the data science code to add orchestration logic\nKestra, an open-source library, allows you to develop your Python scripts independently and then ​​seamlessly incorporate them into data workflows using YAML files.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-eltetl",
    "href": "qmd/production-tools.html#sec-prod-tools-eltetl",
    "title": "38  Tools",
    "section": "38.7 ELT/ETL Operations",
    "text": "38.7 ELT/ETL Operations\n\n38.7.1 Misc\n\ndbt - see DB, dbt\nGoogle Dataform - Docs, Best Practices\n\n\n\n38.7.2 AWS DataBrew\n\nFeatures to clean and transform the data to ready it for further processing or feeding to machine learning models\n\nNo coding; pay for what you use; scales automatically\nover 250 transformations\nAllows you to add custom transformations with lambda functions",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-modexptrk",
    "href": "qmd/production-tools.html#sec-prod-tools-modexptrk",
    "title": "38  Tools",
    "section": "38.8 Model Experimentation/Version Tracking",
    "text": "38.8 Model Experimentation/Version Tracking\n\n38.8.1 DVC\n\nTracks data and models while model building\nStore code and track changes in a Git repository while data/models are in AWS/GCP/Azure/etc. storage\nTracking changes\n\nSteps\n\nhashes every file in the directory data,\nadds it to .gitignore and\ncreates a small file data.dvc that is added to Git.\n\nBy comparing hashes, DVC knows when files change and which version to restore.\n\nInitial Steps\n\nGoto project directory -cd &lt;path to local github repo&gt;\nInitialize DVC - dvc init\nAdd a data path/uri - dvc remote add -d remote path/to/remote\n\ncan be Google Drive, Amazon S3, Google Cloud Storage, Azure Storage, or on your local machine\ne.g. Google Drive: dvc remote add -d remote gdrive://&lt;hash&gt;\n\nThe hash will the last part of the URL, e.g. “https://drive.google.com/drive/u/0/folders/1v1cBGN9vS9NT6-t6QhJG”\n\nConfirm data set-up: dvc config -l\n\nThe config file is located inside “.dvc/”\nTo version your config on github: git add .dvc/config\n\n\nAdd data/ to .gitignore\n\nExample showed adding every file in the repo manually but this seems easier\n\nAdd, commit, and push all files to repo\n\nMain differences to regular project initialization\n\ndata/ directory doesn’t get pushed to github\ndata.dvc file gets pushed to github\n\n\nSet-up DVC data cache\n\nCan be local directory/s3/gs/gdrive/etc\nExample: S3\n            dvc remote add -d myremote s3://mybucket/path\n            git add .dvc/config\n            git commit -m \"Configure remote storage\"\n            git push\n            dvc push\n\n\nI’m guessing .dvc/config is created with dvc remote add  and wasn’t there before. Otherwise in steps 3 and 4, I need to add the files manually.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-modmon",
    "href": "qmd/production-tools.html#sec-prod-tools-modmon",
    "title": "38  Tools",
    "section": "38.9 Model/Data Drift Monitoring",
    "text": "38.9 Model/Data Drift Monitoring\n\nArize AI\n\nDocs\nAccessed through Rest API, Python SDK, or Cloud Storage Bucket\n\nFiddler AI Monitoring: fiddler.ai has a suite of tools that help in making the AI explainable, aid in operating ML models in production, monitor ML models and yes data & model drift detection is one of them\nEvidently: EvidentlyAI is another open-source tool, which helps in evaluating and monitoring models in production. If you are not using Azure ML and looking for a non-commercial tool that is simple to use, evidentlyai is a good place to start.\nAzure ML\n\nMonitors data; uses wasserstein distance\n\nAWS Glue DataBrew\n\nmonitors features\ncalculates full suite of summary stats + entropy\n\nCan be exported to a bucket and then download to measure change over time\n\nAccessed through console or programmatically\nGenerates reports that can be viewed in console or be exported in html, pdf, etc.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-appclustmon",
    "href": "qmd/production-tools.html#sec-prod-tools-appclustmon",
    "title": "38  Tools",
    "section": "38.10 App/Cluster Monitoring",
    "text": "38.10 App/Cluster Monitoring\n\n38.10.1 Prometheus\n\nDon’t use for ML monitoring (from article)(maybe for apps?)\n\nNeed to use multiple Prometheus Metric types for cross-component monitoring\nNeed to define histogram buckets up front for single-component monitoring\nCorrectness of query results depending on scraping interval\nInability to handle sliding windows\nDisgusting-looking PromQL queries\nHigh latency for cross-component metrics (i.e., high-cardinality joins)\n\nMisc\n\nPrometheus is not a time series database (TSDB). It merely leverages a TSDB.\nBecause Prometheus scrapes values periodically, some Metric types (e.g., Gauges) can lose precision if the Metric value changes more frequently than the scraping interval. This problem does not apply to monotonically increasing metrics (e.g., Counters).\nMetrics can be logged with arbitrary identifiers such that at query time, users can filter Metrics by their identifier value.\nPromQL is flexible – users can compute many different aggregations (basic arithmetic functions) of Metric values over different window sizes, and these parameters can be specified at query time.\n\nMetric values (Docs):\n\nCounter: a cumulative Metric that monotonically increases. Can be used to track the number of predictions served, for example.\nGauge: a Metric that represents a single numerical value that can arbitrarily change. Can be used to track current memory usage, for example.\nHistogram: a Metric that categorizes observed numerical values into user-predefined buckets. This has a high server-side cost because the server calculates quantiles at query time.\nSummary: a Metric that tracks a user-predefined quantile over a sliding time window. This has a lower server-side cost because quantiles are configured and tracked at logging time. Also, the Summary Metric doesn’t generally support aggregations in queries.\n\nProcess\n\n\nUsers instrument their application code to log Metric values.\nThose values are scraped and stored in a Prometheus server.\nThe values can be queried using PromQL and exported to a visualization tool like Grafana\n\nThe are R packages that might make querying these metrics easier so you don’t have to learn PromQL",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-oth",
    "href": "qmd/production-tools.html#sec-prod-tools-oth",
    "title": "38  Tools",
    "section": "38.11 Other",
    "text": "38.11 Other\n\nTerraform\n\nProvision infrastructure across 300+ public clouds and services using a single workflow through yaml files\n\nAutomates and makes these workflows reproducible\nArticle on using it with R\n\n\nDatadog - Monitor servers in all cloud hosts in one place — alerts, metrics, logs, traces, security incidents, etc.\nPagerDuty - Automated incident management — alerts, notifications, diagnostics, logging, etc.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html",
    "href": "qmd/project-analyses.html",
    "title": "Analyses",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-misc",
    "href": "qmd/project-analyses.html#sec-proj-anal-misc",
    "title": "Analyses",
    "section": "",
    "text": "Also see\n\nLogistics, Demand Planning &gt;&gt; Stakeholder Questions\n\nThese are questions for the stakeholder(s) when preparing to create a forecasting model, but many apply to other types of projects including Analysis.\n\n\nSee Thread on an analysis workflow using {targets}",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-analy-gen",
    "href": "qmd/project-analyses.html#sec-proj-analy-gen",
    "title": "Analyses",
    "section": "General",
    "text": "General\n\nGeneral Questions\n\n“What variables are relevant to the problem I’m trying to solve?”\n“What are the key components of this data set?”\n“Can this data be categorized?”\n“Is this analysis result out of the ordinary?”\n“What are the key relationships?”\n“Is this the best way this company could be carrying out this task?”\n“What will happen under new conditions?”\n“What factors are best used to determine or predict this eventuality?”\n\nBreak down the problem into parts and focus on those during EDA\n\nAlso see Decison Intelligence &gt;&gt; Mental Models for details on methods to break down components\nExample: Why are sales down?\n\nHow are sales calculated?\n\ne.g. Total Sales = # of Orders * Average Order Value\n\nBreakdown # of orders and average order value\n\nnumber of orders = number of walk-ins * % conversion\n\nHas walk-ins or conversion declined?\n\nAverage Order Value\n\nBin avg order value by quantiles, plot and facet or group by binned groups. Is one group more responsible for the decline than others?\n\n\nIs there regional or store or brand variability? (grouping variables)\n\n\nDrill down into each component until the data doesn’t allow you to go any farther.\nSegment data by groups\n\nColor or facet by cat vars\nPay attention to counts of each category (may need to collapse categories)\nCommon segments in product analytics\n\nFree vs Paid users\nDevice Type (desktop web vs mobile web vs native app)\nTraffic Source (people coming from search engines, paid marketing, people directly typing in your company’s URL into their browser, etc.)\nDay of the Week.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-tropf",
    "href": "qmd/project-analyses.html#sec-proj-anal-tropf",
    "title": "Analyses",
    "section": "TROPICS framework",
    "text": "TROPICS framework\n\nMisc\n\nFor analyzing changes in key performance metrics\nFrom https://towardsdatascience.com/answering-the-data-science-metric-change-interview-question-the-ultimate-guide-5e18d62d0dc6\nComponents: Time, Region, Other Internal Products, Platform, Industry and Competitors, Cannibalization, Segmentation\n\nTime\n\nWhat to explore\n\nHow has our performance been trending over the last few weeks (or months)?\n\nExample: If we saw a 10% increase in the last week, was the percentage change in the weeks before also 10%? In which case the 10% may actually be pretty normal? Or was the change lower? Higher?\n\nIs this change seasonal? Do we see the same spike around this time each year?\n\nExample: Does WhatsApp see a spike in messages sent during the holiday season?\n\nWas the change sudden or gradual? Did we see a sudden spike or drop overnight? Or has the metric gradually been moving in this direction over time?\n\nExample: If product usage jumps by 50% overnight could there be a bug in our logging systems?\n\nAre there specific times during the day or week where this change is more pronounced?\n\nSolution examples\n\nIf the change is seasonal then there may not necessarily be anything you need to ‘solve’ for. But, you can leverage this to your advantage.\n\nExample: Amazon sales may jump up on Black Friday so they would want to make sure they have the proper infrastructure in place so the site doesn’t crash. They may also see if there are certain types of products that are popular purchases and increase their inventory accordingly.\n\nIf there is a sudden decline, there may be a bug in the logging or a new feature or update recently launched that’s creating problems that you may need to roll back.\nIf there’s a gradual decline, it may indicate a change in user behavior.\n\nExample: If the time spent listening to music is declining because people prefer to listen to podcasts then Spotify may want to focus more of their content inventory on podcasts.\n\n\n\nRegion\n\nWhat to explore\n\nIs this change concentrated in a specific region or do we see a similar change across the board?\n\nSolution examples\n\nThere may be newly enforced regulations in countries that are affecting your product metrics. You would need to do further research to assess the impacts of these regulations and potential workarounds.\n\nExample: Uber was temporarily banned in London in 2019 for repeated safety failures which resulted in a series of lawsuits and court cases.\n\nPopular local events may also be potential explanations. While these may not be areas to ‘solve’ for they can be opportunities to take advantage of.\n\nExample: Coachella season means a jump in the number of Airbnb bookings in Southern California that are capitalized on by surge pricing.\n\n\n\nOther Internal Products\n\nWhat to explore\n\nIs this change specific to one product or is it company-wide? How does this metric vary across our other product offerings?\n\nExample: If the Fundraising feature on Facebook is seeing increased usage, is the swipe up to donate feature on Instagram (which Facebook owns) also seeing a similar uptick?\n\nAre there other metrics that have also changed in addition to the one in question?\n\nExample: If the time spent on Uber is going down, is the number of cancellations by drivers also declining (implying people are spending less time on the app because they’re having a more reliable experience)?\n\n\nSolution examples\n\nIf there is a metric change across our other features and products, it’s likely a larger problem we should address with multiple teams and may need a Public Relations consultant.\n\nExample: Elon + Twitter.\n\n\n\nPlatform\n\nWhat to explore\n\nMobile vs Desktop?\nMac vs Windows?\nAndroid vs iOS?\n\nSolution examples\n\nIf there was a positive change in our metric on a specific platform (e.g. iOS) and coincides with an (iOS) update we released, we would want to do a retrospective to determine what about that update was favorable so we can double down on it. Alternatively, if the metric change was negative, we may want to reconsider and even roll back the update.\nIf the change was due to a change in the platform experience (e.g. app store placement, ratings) we may want to seek advice from our marketing team since this is a top of the funnel problem\nIf users are showing astrong preference for a specific platform, we want to make sure that the experience of the preferred platform is up to par. We also need to make sure our platform-specific monetization strategies are switching to follow the trend.\n\nExample: Facebook’s ad model was initially tied to the desktop app only and had to be expanded as mobile became the platform of preference.\n\n\n\nIndustry & Competitors\n\nWhat to explore\n\nWhen our decline began, was there a new competitor or category that emerged?\n\nExample: Did the number of users listening to Apple podcasts go down when Clubhouse came on to the scene?\n\nHave competitors changed their offering lately?\nIs the category as a whole declining?\n\nSolution examples\n\nIf the category is shifting as a whole, we should begin looking at larger-scale changes to the app.\n\nExample: What Kodak should have done.\n\nIf there’s a new competitor taking our market share, we can begin with reactivation campaigns on churned users. We may also want to conduct user research to understand the gap between our offering and those of our competitors\n\n\nCannibalization\n\nWhat to explore\n\nAre other products or features in our offering experiencing growth in the face of our decline or vice versa?\nHave we released a new feature that is drawing users away from our old features? If so, can we fully attribute the release of the new feature with the decline in the metric of our feature in question?\n\nExample: When Facebook released reactions, did the number of comments on a post go down because people found it easier to press a react button instead of writing a comment?\n\n\nSolution examples\n\nCannibalization may not necessarily be a bad thing. We need to determine whether this shift in user interest across our features is favorable by determining whether the new features align better with the goals of the business.\nCannibalization may also be an indication of but it is indicative of a change in user behavior. In which case we may want to consider if perhaps our core metrics need to change as user behaviors change.\n\nExample: If users care more about watching Instagram stories than engaging with the Instagram feed we may want to optimize for retention (because the ephemeral nature of stories is more likely to motivate users to keep coming back to the platform) instead of time spent on the app.\n\nWe can also look at ways to bridge the two features together to create a more unified platform.\n\n\nSegmentation\n\nWhat to explore\n\nHow does this metric vary by user type:\n\nAge, sex, education\nPower users versus casual users * New users versus existing users\n\nHow does this metric vary by different attributes of the product:\n\nExample: If the time spent watching YouTube videos is going down, is it across longer videos or shorter clips? Is it only for DIY videos or interview tutorial content? Is the same number of people that started watching a video the same but a large chunk of them stop watching it halfway through?\n\n\nSolution examples\n\nIf the metric varies between new and existing users then maybe there is a overcrowding effect.\n\nExample: Reddit forums could hit a critical mass where new users feel lost and less likely to engage than existing users resulting in a drop in engagements per user\n\nIf users are dropping off at certain parts of the funnel then maybe the experience at that funnel step is broken.\n\nExample: While the same number of people are starting carts on Amazon there may be a drop in purchases if the payment verification system isn’t working.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-actanl",
    "href": "qmd/project-analyses.html#sec-proj-anal-actanl",
    "title": "Analyses",
    "section": "Actionable Analyses",
    "text": "Actionable Analyses\n\nNotes from: Driving Product Impact With Actionable Analyses\nActionable insights do not only provide a specific data point that might be interesting, but lay out a clear narrative how this insight is connected to the problem at hand, what the ramifications are, as well as possible options and next steps to take with the associated benefits/risks of (not) acting upon these.\nNot Actionable: Users under the age of 25 hardly use audiobooks.\n\nIs this good, bad? Should they be listening to audiobooks and is there anything we should do about it?\n\nActionable: Users under the age of 25 hardly use audiobooks because they never explore the feature in the app. However users who listen to audiobooks have a 20% higher retention rate.\n\nThis information tells us that audiobooks represent a potential opportunity to increase retention amongst younger users, however there seems to be more work to be done to encourage users exploring this feature.\n\nSteps\n\nProblem Statement: High-level business problem to solve (e.g. Increasing Retention, Conversion Rate, Average Order Value)\n\nCan also be in regards to a metric that’s believed to be highly associated with a North Star metric like a Primary metric (See KPIs)\n\nOpportunity Areas: Areas or problems with a strong connection to the problem at hand\n\nInvestigate behaviors of users with the behavior that you’re interested in (i.e. high or low values of the desired metric).\nDiscovering the characteristics of these users can help to figure out ways to encourage other users to act similarily or gain insight into the type of users you want to attract.\n\nLevers: Different ways to work on the opportunity areas\n\nA lever should be data-based and able to be validated on whether working to increase or decrease the lever will lead to a positive solution to the problem statement.\nThere are typically multiple levers for a given opportunity area\n\nThese should be ordered in terms of priority, and priority should be given to the lever that is believed to result in the greatest impact on the opportunity area that will result in the greatest impact on the solution to the problem statement.\n\n\nExperiments [Optional]: Concrete implementation of a specific lever that can help prove/disprove our hypotheses.\n\nOptional but always helpful to convey recommendations and suggestions with concrete ideas for what the team could or should be building.\n\n\nExample\n\n\nProblem Statement: How can we increase daily listening time for premium users in the Spotify app?\n\nHypothesis: Daily Listening Time is strongly connected to retention for premium users and hensce to monthly revenue.\n\nOpportunity Areas:\n\nUsers who use auto-generated playlists have a x% higher daily listening time\nUsers who subscribed to at least 3 podcasts have a x% higher listening time per day than those who did not subscribe to any.\nUsers who listen to audiobooks have a x% higher daily listening time.\n\nLevers:\n\nOpportunity Area: Increase the percentage of users under 25 using audiobooks from x% to y%.\nQuestions:\n\nDo users not see the feature?\nDo users see the feature but don’t engage with the feature?\nDo users engage with the feature but drop off after a short amount of time?\n\nFinding: Users under 25 engage less with the Home Screen, the only screen where Audiobooks are promoted, and hence don’t see this feature in the App. This is likely leading low usage and engagement.\nLever: Increase prominence of Audiobooks within the app\nPrioritzation Table for Report\n\n\nExperiments:\n\n\n“We predict that adding a banner promoting Audiobooks when the App opens [Experiment Change] will increase younger users’ daily listening time [Problem] because more younger users will see and listen to Audiobooks [Lever]. We will know this is true when we see an increase in young users using Audiobooks [Lever], followed by an increase in the daily listening time for younger users [Validation Metrics].”\nIf there is no significant increase in audiobook usage, then there many other ways to increase the visibility of a feature which can be the hypotheses of further experiments.\nIf , however, there is a significant increase in users using Audiobooks (lever) but no effect on daily listening time (main problem), then the lever is invalidated and we can move on to the next one.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-edap",
    "href": "qmd/project-analyses.html#sec-proj-anal-edap",
    "title": "Analyses",
    "section": "Exploratory Data Analysis Research Plan",
    "text": "Exploratory Data Analysis Research Plan\n\nNotes from Pluralsight Designing an Exploratory Data Analysis Research Plan\n\nSee code &gt;&gt; rmarkdown &gt;&gt; reports &gt;&gt; edarp-demo.Rmd\n\nDuring the development of the EDARP, all stakeholders can align their expectiations. Buy-in from the aligned stakeholders can help sell the project to the organization.\nEach section should have an introduction with a description about whats in it\nMock Schedule\n\nWeek 1: Data request by a department\nWeek 2: Data Scientist and department meet to formalize the research questions\n\nWorking backwards from the desired output can help frame the right questions to ask during this period\n\nWeek 3: Clear metrics are established. The use case of the product is defined (i.e. who’s using it and what decisions are to be made). Sponsorship is set. Budgets are allocated.\nWeek 4: EDARP is finalized with everyone understanding the objectves, budget, product design, and product usage\nWeek 6: Data Scientist delivers the product to the department.\n\nSections of the Report\n\nAbstract\n\nHighlights the research questions\nWho the stakeholders are\nMetrics of success\nExample:\n\n“The foundational task was to develop sales insights across stores. Through the identification and inclusion of various business groups, data were gathered and questions were formed. The business groups included are Marketing, IT, Sales and Data Science. From this process we defined the primary goal of this research. This research adds understanding to how sales are driven across stores and develops a predictive model of sales across stores. These outcomes fit within budget and offer an expected ROI of 10%.”\n\n\nFigures and Tables\n\nOptional depending on audience\nSection where all viz is at\n\nIntroduction\n\nDetailed description of metrics of success\n\nExample\n\nROI 8%\nR2 75%\nInterpretability\n\n\n\nStakeholders\n\nMarketing\n\nList of people\n\nIT\nSales\nData Science\n\nBudget and Financial Impact\n\nNot always known, but this section is valuable if you’re able to include it.\nPotential vendor costs\nInfrastructure costs\nApplication developement\nFinancial impact, completed by finance team, result in an expected ROI of blah%\n\nMethods\n\nData description\nData wrangling\n\nWhat were the variables of interest and why (“data wrangling involved looking at trends in sales across stores, store types, and states”)\n\nAutocorrelation\n\n“Testing for autocorrelation was completed leading to insights in seasonality across the stores. We examined by the ACF an PACF metrics in the assessment of autocorrelation”\n\nClustering\nOutliers\n\nDescription of algorithm comparison and model selection\n\nWords not code or results\nExample\n\nInvolved training and testing regression, random forest,…\nRegression model served as a benchmark comparison across 5 models\nA discussion of interpretability and expected ROI guided the choice of the final model\n\n\n\n\nResults and Discussion\n\n“This section highlights the thought process that went into wrangling the data and building the models. A few of the insights gained in observation of the data are shared. Also, the assessment of the model is discussed at the end of the section.”\nVisualizing the Data (i.e. EDA viz - descriptive, outliers, clusters)\n\nFigures\nInsights\nRepeat as needed\n\nVariable Importance\nFinal Model\n\nModel Assessment\n\nAlgorithm comparison metrics\nDynamic visual of model output\n\nSimple shiny graph with a user input and a graph\n\ne.g. Choose store number - graph of sales forecast\n\n\n\n\nConclusion\n\nExample:\n\n“The research explored the possibility of building a predictive model to aid in forecasting sales across stores. We found that, given the established metrics of ROI greater than 8%, R-square of greater than .75 and interpretability in the models, this reasearch has resulted in a viable model for the business. Additionally, it was discovered the presence of some outlier phenomena in the data which has been identified by the stakeholders as acceptable noise. Further we discovered that there is a latent grouping to the stores across sales, store type and assortment. This insight will be used to guide marketings action in the future.”\n\n\nAppendix\n\nSchedule of Maintenance\nFuture Research",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-datmet",
    "href": "qmd/project-analyses.html#sec-proj-anal-datmet",
    "title": "Analyses",
    "section": "Data Meta-Metrics",
    "text": "Data Meta-Metrics\n\nNotes from Data Meta Metrics\nMetrics for categorizing the quality of data being used in your analysis\nYou can be very confident about the methodologies you’re using to analyze data, but if there are issues with the underlying dataset, you might not be so confident in the results of an analysis or your ability to repeat the analysis.\n\nIdeally, we should be passing this information — our confidences and our doubts — on to stakeholders alongside any results or reports we share.\n\nUse Cases\n\nConvey the quality of the data and its collection process to technical and non-technical audience\nHelpful for diagnosing the strengths and weaknesses of data storage and collection across multiple departments.\nDevelop a data improvement process with an understanding of what data you do and don’t have and what you can and can’t collect.\n\nGood data: You know how and when it’s collected, it lives in a familiar database, and represents exactly what you expect it to represent.\nLess-Than-Stellar data: Data that comes with an “oral history” and lots of caveats and exceptions when it comes to using it in practice.\n\ne.g. When you ask a department for data and their responses are “Anna needs to download a report with very specific filters from a proprietary system and give you the data” or “Call Matt and see if he remembers”\n\nPotential Metrics - The type of metrics you use can depend on the analysis your doing\n\n\nRelevance: Ability to answer the question we were asking of it\nTrustworthiness: Will the data be accurate based on how it was collected, stored, and managed?\nRepeatability: How accessible is this data? Can the ETL process be faithfully reproduced?\n\nSlide Report Examples\n\nGood Data\n\nBad Data\n\n\nWith bad data, notes on why the data is bad are included in the slide.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html",
    "href": "qmd/db-postgres.html",
    "title": "Postgres",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-misc",
    "href": "qmd/db-postgres.html#sec-db-pstgr-misc",
    "title": "Postgres",
    "section": "",
    "text": "Notes from\n\nCourse: linkedin.learning.postgresql.client.applications\nCourse: Linux.Academy.PostgreSQL.Administration.Deep.Dive\n\nResources\n\nPostgreSQL is Enough - Links to various applications/extensions resources\n\nEverything is case sensitive, so use lowercase for db and table names\nCheck postgres sql version - psql --version or -V\nSee flag options - psql --help\nIf there’s a “#” in the prompt after logging into a db, then that signifies you are a super-user\nMeta commands (i.e. commands once you’re logged into the db)\n\n\\du - list roles (aka users + permissions)\n\\c  - switches databases\n\\password  - assign a password to a user (prompt will ask for the password twice)\n\nCan also use ALTER ROLE for this but the password will then be in the log\n\n\nUnlogged Table - Data written to an unlogged table will not be logged to the write-ahead-log (WAL), making it ideal for intermediate tables and considerably faster. Note that unlogged tables will not be restored in case of a crash, and will not be replicated.",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-gstgr-ext",
    "href": "qmd/db-postgres.html#sec-db-gstgr-ext",
    "title": "Postgres",
    "section": "Extensions",
    "text": "Extensions\n\npg_analytics\n\nIntro, Repo\nArrow and Datafusion integrated with Postgres\nDelta Lake tables behave like regular Postgres tables but use a column-oriented layout via Apache Arrow and utilize Apache DataFusion, a query engine optimized for column-oriented data\nData is persisted to disk with Parquet\nThe delta-rs library is a Rust-based implementation of Delta Lake. This library adds ACID transactions, updates and deletes, and file compaction to Parquet storage. It also supports querying over data lakes like S3, which introduces the future possibility connecting Postgres tables to cloud data lakes.\n\npg_bm25\n\nIntro, Repo\nRust-based extension that significantly improves Postgres’ full text search capabilities\n\nBuilt to be an Elasticsearch inside of a postgres db\n\nPerformant on large tables, adds support for operations like fuzzy search, relevance tuning, or BM25 relevance scoring (same algo as Elasticsearch), real-time search — new data is immediately searchable without manual reindexing\n\nQuery times over 1M rows are 20x faster compared to tsquery and ts_ran (built-in search and sort)\n\n\npg_sparse\n\nIntro, Repo\nEnables efficient storage and retrieval of sparse vectors using HNSW\n\nSPLADE outputs sparse vectors with over 30,000 entries. Sparse vectors can detect the presence of exact keywords while also capturing semantic similarity between terms.\n\nFork of pgvector with modifications\nCompatible alongside both pg_bm25 and pgvector\n\npgvector\n\nRepo\nAlso see Databases, Vector Databases for alternatives and comparisons\nEnables efficient storage and retrieval of dense vectors using HNSW\n\nOpenAI’s text-embedding-ada-002 model outputs dense vectors with 1536 entries\n\nExact and Approximate Nearest Neighbor search\nL2 distance, Inner Product, and Cosine Distance\nSupported inside AWS RDS\n\npgrx\n\nRepo\nFramework for developing PostgreSQL extensions in Rust\nTo install extensions built in Rust, you need to have this extension installed\n\nplprql\n\nRepo\nEnables you to run PRQL queries. PRQL has a syntax that is similar to {dplyr}\nBuilt in Rust so you have to have pgrx installed. Repo has directions.",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-dock",
    "href": "qmd/db-postgres.html#sec-db-pstgr-dock",
    "title": "Postgres",
    "section": "Docker",
    "text": "Docker\n\nSteps\n\nStart docker desktop\nStart powershell\ndocker run --name pg_database -p 5432:5432 -e POSTGRES_PASSWORD=ericb2022 -d postgres:latest\n\n1st 5432 is local computer port\n2nd 5432 is the required postgres image port\n-e is for defining an environment variable; here its the db password that I set to ericb2022\n-d\n\nRuns the container in the background\nAllows you to run commands in the same terminal window that you used the container run command in\n\n“postgres:latest” is the name of the image to build the container from\n\nClose powershell\nIn docker desktop, the “pg_database” container should be running\n\nConnect to the db\n\nSteps\n\npsql should be in your list of path environment variables\n\nRight-click Start &gt;&gt; System &gt;&gt; advanced settings (right panel) &gt;&gt; environment variables &gt;&gt; highlight path &gt;&gt; edit\n“C:\\Program Files\\PostgreSQL\\14\\bin”\n\n** Note the “14” in the path which is the current version. Therefore, when postgres is updated, this path will have to be updated **\n\n\npsql --host localhost --port 5432 --dbname postgres --username postgres\n\nNote these are all default values, so this is equivalent to psql -U postgres\n–host (-h) is the ip address or computer name that you want to connect to\n\nlocalhost is for the docker container that’s running\n\n5432 is the default –port (-p) for a postgres container\n–dbname (-d) is the name of the database on the server\n\n“postgres” is a db that ships with postgres\n\n–username (-U) is a username that has permission to access the db\n\n“postgres” is the default super-user name\n\n\nA prompt will then ask you for that username’s password\n\nThe container above has the password ericb2022\n\nThis didn’t work for me, needed to use my postgres password that I set-up when I installed postgres and pgAdmin.\nMy local postgres server and the container are listening on the same port, so maybe if I changed the first port number to something else, it would connect to the container.\n\n\nTo exit db, \\q\n\n\nCreate a db\n\nSteps\n\ncreatedb -h localhost -p 5432 -U postgres -O eric two_trees\n\n-U is the user account used to create the db\n-O is used to assign ownership to another user account\n\n“role” (i.e. user account) must already exist\n\n“two_trees” is the name of the new db\nYou will be prompted for user’s password\n\nList of dbs on the server\n\npsql -h localhost -p 5432 -U postgres -l\n\n-l lists all dbs on server\nYou will be prompted for user’s password\n\n\n\n\nRun a sql script\n\npsql -d acweb -f test.sql\n\n-d is for the database name (e.g. acweb)\n-f is for running a file (e.g. test.sql)\n\n\nAdd users\n\nCreate user/role (once inside db)\nCREATE USER &lt;user name1&gt;;\nCREATE ROLE &lt;user name2&gt;;\nALTER ROLE &lt;user name2&gt; LOGIN\n\nCREATE USER will give the user login attribute/permission while CREATE ROLE will not\n\nALTER ROLE gives the user attributes/permissions (e.g. login permission)\n\nCreate user/role (at the CLI) - createuser &lt;user name&gt;",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-pgadm",
    "href": "qmd/db-postgres.html#sec-db-pstgr-pgadm",
    "title": "Postgres",
    "section": "pgAdmin",
    "text": "pgAdmin\n\nCreate a server\n\nRight-click on servers &gt;&gt; create &gt;&gt; server\n\nGeneral tab &gt;&gt; enter name\nConnection tab\n\nHost name/address: computer name or ip address where the server is running\n\nlocal: localhost or 127.0.0.1\n\nPort: default = 5432\nMaintenance database: db you want to connect to\n\nIf you haven’t created it yet, just use default “postgres” which autmatically created during installation\n\nusername/password\n\nu: default is postgres\np: installation password\nTick Save password\n\n\nClick Save\n\n\nCreate a db\n\nRight-click databases &gt;&gt; create &gt;&gt; databases &gt;&gt; enter name (lowercase) and click save\n\nCreate a table\n\nVia gui\n\nClick db name &gt;&gt; schema &gt;&gt; public &gt;&gt; right-click tables &gt;&gt; create &gt;&gt; tables\nGeneral tab\n\nEnter the table name (lower case)\n\nColumns tab\n\nEnter name, data type, whether there should be a “Not Null” constraint, and whether it’s a primary key\nAdd additional column with “+” icon in upper right\nIf you’re going to fill the table with a .csv file, make sure the column names match\n\nClick save\nTable will be located at db name &gt;&gt; schema &gt;&gt; public &gt;&gt; tables\n\nVia sql\n\nOpen query tool\n\nRight-click  or Schemas or Tables &gt;&gt; query tool\nClick Tools menu dropdown (navbar) &gt;&gt; query tool\n\nRun CREATE TABLE statement\n\nIf you don’t include the schema as part of the table name, pgadmin automatically places it into the “public” schema directory (e.g. public.table_name)\n\n\n\nImport csv into an empty table\n\nMake sure the column names match\nRight-click table name &gt;&gt; import/export\nOptions tab\n\nMake sure import is selected\nSelect the file\nIf you have column names in your csv, select Yes for Header\nSelect “,” for the Delimiter\n\nColumns tab\n\nCheck to make sure all the column names are there\n\nClick OK\n\nQuery Table\n\nRight-click table &gt;&gt; query editor\nQuery editor tab\n\nType query &gt;&gt; click ▶ to run query",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-rds",
    "href": "qmd/db-postgres.html#sec-db-pstgr-rds",
    "title": "Postgres",
    "section": "AWS RDS",
    "text": "AWS RDS\n\nMisc\n\nNotes from Create an RDS Postgres Instance and connect with pgAdmin\n\nSteps\n\nSearch AWS services for “RDS” (top left navbar)\nCreate Database\n\nClick “Create Database”\n\nCreate Database\n\nChoose Standard create or Easy Create\n\nEasy Create - uses “best practices” settings\n\nSelect postgres\n\nAlso available: Amazon Aurora, MySQL, MariaDB, Oracle, Microsoft SQL Server\n\nTemplates\n\nProduction\n\nMulti-AZ Deployment - Multiple Availability Zones\nProvisioned IOPS Storage - Increased output\n\nDev/Test\nRree tier\n\n750 hrs of Amazon RDS in a Single-AZ db.t2.micro Instance.\n20 GB of General Purpose Storage (SSD).\n20 GB for automated backup storage and any user-initiated DB Snapshots.\n\nRDS pricing page\n\nSettings\n\nDB Instance Identifier - enter name\nSet master username, master username password\n\nDB Instance\n\ndb.t3.micro or db.t4g.micro for free tier\n\ndev/test, production has many other options\n\n\nStorage\n\nDefaults: SSD with 20GB\nAutoscaling can up the storage capacity to a default 1000GB",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-py",
    "href": "qmd/db-postgres.html#sec-db-pstgr-py",
    "title": "Postgres",
    "section": "Python",
    "text": "Python\n\n{{psycopg2}}\n\nMisc\n\nNotes from Fastest Way to Load Data Into PostgreSQL Using Python\ntl;dr\n\nLarge Data: use copy_to\nMedium to Small Data:\n\nTime and memory isn’t an issue: Use extract_values or maybe copy_to if you don’t have JSON.\n\n\n\nConnect to db\nimport psycopg2\n\nconnection = psycopg2.connect(\n    host=\"localhost\",\n    database=\"testload\",\n    user=\"haki\",\n    password=None,\n)\nconnection.autocommit = True\nCreate a table\ndef create_staging_table(cursor) -&gt; None:\n    cursor.execute(\"\"\"\n        DROP TABLE IF EXISTS staging_beers;\n        CREATE UNLOGGED TABLE staging_beers (\n            id                  INTEGER,\n            name                TEXT,\n            tagline             TEXT,\n            first_brewed        DATE,\n            description         TEXT,\n            image_url           TEXT,\n            abv                 DECIMAL,\n            ibu                 DECIMAL,\n            target_fg           DECIMAL,\n            target_og           DECIMAL,\n            ebc                 DECIMAL,\n            srm                 DECIMAL,\n            ph                  DECIMAL,\n            attenuation_level   DECIMAL,\n            brewers_tips        TEXT,\n            contributed_by      TEXT,\n            volume              INTEGER\n        );\n    \"\"\")\n\nwith connection.cursor() as cursor:\n  create_staging_table(cursor)\n\nThe function receives a cursor and creates a unlogged table called staging_beers.\n\nInsert many rows at once\n\nNotes from Fastest Way to Load Data Into PostgreSQL Using Python\nThe best way to load data into a database is using the copy command (last method in this section). The issue here is that copy needs a .csv file and not json.\n\nThis might be an issue just because of psycopg2 library doesn’t support json or that there is a postgres extension that isn’t supported by the library. This also might not be a problem in the future.\n\nData\nbeers = iter_beers_from_api()\nnext(beers)\n{'id': 1,\n 'name': 'Buzz',\n 'tagline': 'A Real Bitter Experience.',\n 'first_brewed': '09/2007',\n 'description': 'A light, crisp and bitter IPA brewed...',\n 'image_url': 'https://images.punkapi.com/v2/keg.png',\n 'abv': 4.5,\n 'ibu': 60,\n 'target_fg': 1010,\n...\n}\nnext(beers)\n{'id': 2,\n 'name': 'Trashy Blonde',\n 'tagline': \"You Know You Shouldn't\",\n 'first_brewed': '04/2008',\n 'description': 'A titillating, ...',\n 'image_url': 'https://images.punkapi.com/v2/2.png',\n 'abv': 4.1,\n 'ibu': 41.5,\n ...\n }\n\nData is from beers api\niter_beers_from_api is a udf that takes the json from the api and creates a generator object that iterates through each beer.\n\nInsert data in db using execute_values (low memory usage and still pretty fast)\ndef insert_execute_values_iterator(\n    connection,\n    beers: Iterator[Dict[str, Any]],\n    page_size: int = 100,\n) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        psycopg2.extras.execute_values(cursor, \"\"\"\n            INSERT INTO staging_beers VALUES %s;\n        \"\"\", ((\n            beer['id'],\n            beer['name'],\n            beer['tagline'],\n            parse_first_brewed(beer['first_brewed']),\n            beer['description'],\n            beer['image_url'],\n            beer['abv'],\n            beer['ibu'],\n            beer['target_fg'],\n            beer['target_og'],\n            beer['ebc'],\n            beer['srm'],\n            beer['ph'],\n            beer['attenuation_level'],\n            beer['brewers_tips'],\n            beer['contributed_by'],\n            beer['volume']['value'],\n        ) for beer in beers), page_size=page_size)\n\ninsert_execute_values_iterator(page_size=1000)\n\nparse_first_brewed is a udf that transforms a date string to datetime type.\nbeer[‘volume’][‘value’]: Data is in json and the value for volume is subsetted from the nested field.\nBenchmark: At page_size = 1000, 1.468s, 0.0MB of RAM used\nThe generator((bear['id'], … , bear['volume']['value'], for beer in beers) keeps data from being stored in memory during transformation\npage_size: maximum number of arglist items to include in every statement. If there are more items the function will execute more than one statement.\n\nHere arglist is the data in the form of generator\n\n\nInsert data in db using copy_from (Fast but memory intensive)\nimport io\n\ndef clean_csv_value(value: Optional[Any]) -&gt; str:\n    if value is None:\n        return r'\\N'\n    return str(value).replace('\\n', '\\\\n')\n\ndef copy_stringio(connection, beers: Iterator[Dict[str, Any]]) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        csv_file_like_object = io.StringIO()\n        for beer in beers:\n            csv_file_like_object.write('|'.join(map(clean_csv_value, (\n                beer['id'],\n                beer['name'],\n                beer['tagline'],\n                parse_first_brewed(beer['first_brewed']),\n                beer['description'],\n                beer['image_url'],\n                beer['abv'],\n                beer['ibu'],\n                beer['target_fg'],\n                beer['target_og'],\n                beer['ebc'],\n                beer['srm'],\n                beer['ph'],\n                beer['attenuation_level'],\n                beer['contributed_by'],\n                beer['brewers_tips'],\n                beer['volume']['value'],\n            ))) + '\\n')\n        csv_file_like_object.seek(0)\n        cursor.copy_from(csv_file_like_object, 'staging_beers', sep='|')\n\nclean_csv_value: Transforms a single value\n\nEscape new lines: some of the text fields include newlines, so we escape \\n -&gt; \\\\n.\nEmpty values are transformed to \\N: The string \"\\N\" is the default string used by PostgreSQL to indicate NULL in COPY (this can be changed using the NULL option).\n\ncsv_file_like_object: Generate a file like object using io.StringIO. A StringIO object contains a string which can be used like a file. In our case, a CSV file.\ncsv_file_like_object.write: Transform a beer to a CSV row\n\nTransform the data: transformations on first_brewed and volume are performed here.\nPick a delimiter: Some of the fields in the dataset contain free text with commas. To prevent conflicts, we pick “|” as the delimiter (another option is to use QUOTE).\n\n\nInsert data (streaming) in db using copy_from (Fastest and low memory but complicated, at least with json)\n\nBuffering function\nfrom typing import Iterator, Optional\nimport io\n\nclass StringIteratorIO(io.TextIOBase):\n    def __init__(self, iter: Iterator[str]):\n        self._iter = iter\n        self._buff = ''\n\n    def readable(self) -&gt; bool:\n        return True\n\n    def _read1(self, n: Optional[int] = None) -&gt; str:\n        while not self._buff:\n            try:\n                self._buff = next(self._iter)\n            except StopIteration:\n                break\n        ret = self._buff[:n]\n        self._buff = self._buff[len(ret):]\n        return ret\n\n    def read(self, n: Optional[int] = None) -&gt; str:\n        line = []\n        if n is None or n &lt; 0:\n            while True:\n                m = self._read1()\n                if not m:\n                    break\n                line.append(m)\n        else:\n            while n &gt; 0:\n                m = self._read1(n)\n                if not m:\n                    break\n                n -= len(m)\n                line.append(m)\n        return ''.join(line)\n\nThe regular io.StringIO creates a file-like object but is memory-heavy. This function creates buffer that will feed each line of the file into a buffer, stream it to copy, empty the buffer, and load the next line.\n\nCopy to db\ndef clean_csv_value(value: Optional[Any]) -&gt; str:\n    if value is None:\n        return r'\\N'\n    return str(value).replace('\\n', '\\\\n')\n\ndef copy_string_iterator(connection, beers: Iterator[Dict[str,\nAny]]) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        beers_string_iterator = StringIteratorIO((\n            '|'.join(map(clean_csv_value, (\n                beer['id'],\n                beer['name'],\n                beer['tagline'],\n                parse_first_brewed(beer['first_brewed']).isoformat(),\n                beer['description'],\n                beer['image_url'],\n                beer['abv'],\n                beer['ibu'],\n                beer['target_fg'],\n                beer['target_og'],\n                beer['ebc'],\n                beer['srm'],\n                beer['ph'],\n                beer['attenuation_level'],\n                beer['brewers_tips'],\n                beer['contributed_by'],\n                beer['volume']['value'],\n            ))) + '\\n'\n            for beer in beers\n        ))\n        cursor.copy_from(beers_string_iterator, 'staging_beers', sep='|')\n\nSimilar to other code above",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#distributed-architectures",
    "href": "qmd/db-postgres.html#distributed-architectures",
    "title": "Postgres",
    "section": "Distributed Architectures",
    "text": "Distributed Architectures\n\nMisc\n\nNotes from An Overview of Distributed PostgreSQL Architectures\nFeatures to achieve single node availability, durability, and performance - Replication - Place copies of data on different machines - Distribution - Place partitions of data on different machines - Decentralization - Place different DBMS activities on different machines\nIf transactions take on average 20ms, then a single (interactive) session can only do 50 transactions per second. You then need a lot of concurrent sessions to actually achieve high throughput. Having many sessions is not always practical from the application point-of-view, and each session uses significant resources like memory on the database server. Most PostgreSQL set ups limit the maximum number of sessions in the hundreds or low thousands, which puts a hard limit on achievable transaction throughput when network latency is involved.\n\nNetwork-Attached Block Storage (e.g. EBS)\n\n\nCommon technique in cloud-based architectures\nDatabase server typically runs in a virtual machine in a Hypervisor, which exposes a block device to the VM. Any reads and writes to the block device will result in network calls to a block storage API. The block storage service internally replicates the writes to 2-3 storage nodes.\nPros\n\nHigher durability (replication)\nHigher uptime (replace VM, reattach)\nFast backups and replica creation (snapshots)\nDisk is resizable\n\nCons\n\nHigher disk latency (~20μs -&gt; ~1000μs)\nLower IOPS (~1M -&gt; ~10k IOPS)\nCrash recovery on restart takes time\nCost can be high\n\nGuideline: The durability and availability benefits of network-attached storage usually outweigh the performance downsides, but it’s worth keeping in mind that PostgreSQL can be much faster.\n\nRead Replicas\n\n\nThe most common way of using a replica is to set it up as a hot standby that takes over when the primary fails in a high availability set up.\nHelps you scale read throughput when reads are CPU or I/O bottlenecked by load balancing queries across replicas, which achieves linear scalability of reads and also offloads the primary, which speeds up writes!\n\nThe primary usually does not wait for replication when committing a write, which means read replicas are always slightly behind. That can become an issue when your application does a read that, from the user’s perspective, depends on a write that happened earlier.\nFor example, a user clicks “Add to cart”, which adds the item to the shopping cart and immediately sends the user to the shopping cart page. If reading the shopping cart contents happens on the read replica, the shopping cart might then appear empty. Hence, you need to be very careful about which reads use a read replica.\n\nWhen load balancing between different nodes, clients might repeatedly get connected to different replica and see a different state of the database\nPowerful tool for scaling reads, but you should consider whether your workload is really appropriate for it.\nPros\n\nRead throughput scales linearly\nLow latency stale reads if read replica is closer than primary\nLower load on primary\n\nCons\n\nEventual read-your-writes consistency\nNo monotonic read consistency\nPoor cache usage\n\nGuideline: Consider using read replicas when you need &gt;100k reads/sec or observe a CPU bottleneck due to reads, best avoided for dependent transactions and large working sets.\n\nDBMS-Optimized Cloud Storage\n\n\nWhere DBMS is Database Management Software. (e.g. Aurora)\nPostgreSQL is not optimized for this architecture\nWhile the theory behind DBMS-optimized storage is sound. In practice, the performance benefits are often not very pronounced (and can be negative), and the cost can be much higher than regular network-attached block storage. It does offer a greater degree of flexibility to the cloud service provider, for instance in terms of attach/detach times, because storage is controlled in the data plane rather than the hypervisor.\nPros\n\nPotential performance benefits by avoiding page writes from primary\nReplicas can reuse storage, incl. hot standby\nCan do faster reattach, branching than network-attached storage\n\nCons\n\nWrite latency is high by default\nHigh cost / pricing\nPostgreSQL is not designed for it, not OSS\n\nGuideline: Can be beneficial for complex workloads, but important to measure whether price-performance under load is actually better than using a bigger machine.\n\nActive-Active (e.g. BDR)\n\n\nAny node can locally accept writes without coordination with other nodes.\nIt is typically used with replicas in multiple sites, each of which will then see low read and write latency, and can survive failure of other sites.\nActive-active systems do not have a linear history, even at the row level, which makes them very hard to program against.\nPros\n\nVery high read and write availability\nLow read and write latency\nRead throughput scales linearly\n\nCons\n\nEventual read-your-writes consistency\nNo monotonic read consistency\nNo linear history (updates might conflict after commit)\n\nGuideline: Consider only for very simple workloads (e.g. queues) and only if you really need the benefits.\n\nTransparent Sharding (e.g. Citus)\n\n\nTables distributed and/or replicated across multiple primary nodes using a “shard key .”\n\nEach node shows the distributed tables as if they were regular PostgreSQL tables and queries\n\nData are located in “shards” which are regular PostgreSQL tables. Joins and foreign keys that include the shard key can be performed locally.\nScaling out transactional workloads is most effective when queries have a filter on the shard key, such that they can be routed to a single shard group (e.g. single tenant in a multi-tenant app) or compute-heavy analytical queries that can be parallelized across the shards (e.g. time series / IoT).\nWhen loading data, use COPY, instead of INSERT, to avoid waiting for every row.\nPros\n\nScale throughput for reads & writes (CPU & IOPS)\nScale memory for large working sets\nParallelize analytical queries, batch operations\n\nCons\n\nHigh read and write latency\nData model decisions have high impact on performance\nSnapshot isolation concessions\n\nGuideline: Use for multi-tenant apps, otherwise use for large working set (&gt;100GB) or compute heavy queries.\n\nDistributed Key-Value Stores With SQL (e.g. Yugabyte)\n\n\nA bunch of complicated stuff I don’t understand 😅\nTables are stored in the key-value store, with the key being a combination of the table ID and the primary key.\nBetter to use PostgresSQL without this architecture.\nPros\n\nGood read and write availability (shard-level failover)\nSingle table, single key operations scale well\nNo additional data modeling steps or snapshot isolation concessions\n\nCons\n\nMany internal operations incur high latency\nNo local joins in current implementations\nNot actually PostgreSQL, and less mature and optimized\n\nGuideline: Just use PostgreSQL. For simple applications, the availability and scalability benefits can be useful.",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/renv.html",
    "href": "qmd/renv.html",
    "title": "Renv",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Renv"
    ]
  },
  {
    "objectID": "qmd/renv.html#sec-renv-misc",
    "href": "qmd/renv.html#sec-renv-misc",
    "title": "Renv",
    "section": "",
    "text": "Bruno Rodrigues posts on reproducibility\n\nMRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?\n\ntl;dr:\n\nI want to start a project and make it reproducible.\n\n{renv} and Docker\n\nThere’s an old script laying around that I want to run.\n\n{groundhog} and Docker\n\nI want to work inside an environment that enables me to run code in a reproducible way.\n\nDocker and the Posit CRAN mirror.\n\n\n\nCode longevity of the R programming language\n\nAlternatives\n\nGNU Guix\n\nSee Bruno’s “MRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?” post for his thoughts\n\n\nPosit Package Manager - Helps you install binary packages from a certain date\n\ne.g. Adding to .Rprofile “options(repos = c(CRAN = \"https://packagemanager.posit.co/cran/2024-02-06\"))\nMight be able to keep a separate .Rprofie in the project directory.\n\nInstall from github\nrenv::install(\"eddelbuettel/digest\")",
    "crumbs": [
      "Renv"
    ]
  },
  {
    "objectID": "qmd/renv.html#sec-renv-actrest",
    "href": "qmd/renv.html#sec-renv-actrest",
    "title": "Renv",
    "section": "Activating/Restoring a Project",
    "text": "Activating/Restoring a Project\n\nMisc\n\n** Restoring an environment comes with a few caveats: **\n\nFirst of all, renv does not install a different version of R if the recorded and current version disagree. This is a manual step and up to the user.\n\n{rig} can make switching between R versions easy\n\nThe same is true for packages with external dependencies. Those libraries, their headers and binaries also need to be installed by the user in the correct version, which is not recorded in the lockfile.\nFurthermore renv supports restoring packages installed from git repositories, but fails if the user did not install git beforehand.\n\nhttps://rstudio.github.io/renv/articles/faq.html#im-returning-to-an-older-renv-project-what-do-i-do\n\nInit steps (updates lockfile to either latest pkg versions or what you have locally)\n\nRun renv::init()\nChoose “Discard the lockfile and re-initialize the project”\n\nRestore steps (looking to reproduce the original project results)\n\nRun renv::restore()\nUpdate any packages with outside dependencies\n\nRMarkdown depends on pandoc which is usually installed by installing RStudio. So if you updated RStudio, then the rmarkdown version in the project’s lockfile may not work with your current pandoc version. You’ll have to update rmarkdown (or revert your pandoc version… shhhyea, as if)\n\n\nIssues\n\nA pkg installation fails (and therefore restore fails) because some pkg requires another pkg to be a more up-to-date version or some other reason\n\n**If it’s something with a shit ton of dependencies like rstanarm, you might as well use renv::init( ) method**\nExamples:\n\n{pkgload} failed to install because I had rlang v.4.0.6 instead of &gt; v4.09\n{RCurl} failed because it couldn’t find some obsure file\n\nSolution (update the package):\n\nupdate(\"pkg\") (or go to the github and search for the latest stable version) to see what the latest stable version is.\nrenv::modify() allows you edit the lockfile. Change pkg version in lockfile to that latest version\nrerun renv::restore()\nRepeat as necessary",
    "crumbs": [
      "Renv"
    ]
  },
  {
    "objectID": "qmd/renv.html#sec-renv-font",
    "href": "qmd/renv.html#sec-renv-font",
    "title": "Renv",
    "section": "Installing Fonts",
    "text": "Installing Fonts\n\nUsing a package like extrafont, it won’t find any fonts installed so you have to point it to the system path\nSteps\n\nInstall {systemfonts}\nRun systemfonts::match_font(\"&lt;name of font you have installed&gt;\") and copy path (don’t include file (i.e. .ttf))\nRun extrafont::",
    "crumbs": [
      "Renv"
    ]
  },
  {
    "objectID": "qmd/renv.html#using-local-directory-with-package-tarbell-to-install-a-package",
    "href": "qmd/renv.html#using-local-directory-with-package-tarbell-to-install-a-package",
    "title": "Renv",
    "section": "Using Local directory with Package Tarbell to Install a Package",
    "text": "Using Local directory with Package Tarbell to Install a Package\n\nNeeded to install an old XML package version that wasn’t available through CRAN mirror. Errored looking libxml parser. Discussion says you need to use libxml2.\nResources\n\nrenv using local package directory\n\nhttps://rstudio.github.io/renv/articles/local-sources.html\n\nInstructions on compiling old XML package with rtools40\n\nhttps://github.com/r-windows/rtools-installer/issues/3\nhttps://github.com/r-windows/checks/issues/5#issue-335598042\n\nInstall libxml2 library\n\nhttps://github.com/r-windows/docs/blob/master/packages.md#xml\n\nFinding pacman package manager to install libxml2\n\nhttps://github.com/r-windows/docs/blob/master/rtools40.md#readme\n\n\nIn the project directory, create renv/local directory. Then, download/move the package version’s tar.gz file to that “local” folder\n&gt; Sys.setenv(LIB_XML = \"$(MINGW_PREFIX)\")\n&gt; Sys.setenv(LOCAL_CPPFLAGS = \"-I/mingw$(WIN)/include/libxml2\")\n&gt; install.packages('renv/local/XML_3.99-0.3.tar.gz', repos = NULL, type = 'source')",
    "crumbs": [
      "Renv"
    ]
  },
  {
    "objectID": "qmd/renv.html#errors",
    "href": "qmd/renv.html#errors",
    "title": "Renv",
    "section": "Errors",
    "text": "Errors\n\nFails to retrieve package\n\nSolutions:\n\nInstall from github\nrenv::install(\"eddelbuettel/digest\")\nRevert to previous version\nremotes::install_version(\"cachem\", version = \"1.0.3\", repos = \"http://cran.us.r-project.org\")\nrenv::install(\"cachem@1.0.3\")",
    "crumbs": [
      "Renv"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-syntax",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-syntax",
    "title": "Quarto",
    "section": "Syntax",
    "text": "Syntax\n\nInline code\n-   Total number of counties: **`{r} polling_places |&gt; filter(state == \"Alabama\") |&gt; distinct(county_name) |&gt; count()`**\n-   Total number of polling places: **`{r} polling_places |&gt; filter(state == \"Alabama\") |&gt; count()`**\n-   Election Day: **`{r} polling_places |&gt; filter(state == \"Alabama\") |&gt; pull(election_date) |&gt; unique()`**\nAlign code chunk under bullet and add indented comment below chunk\n-   [Example]{.ribbon-highlight} (using a SQL Query; method 1)\n\n    ``` r\n    # open dataset\n    ds &lt;- arrow::open_dataset(dir_out, partitioning = \"species\")\n    # open connection to DuckDB\n    con &lt;- dbConnect(duckdb::duckdb())\n    # register the dataset as a DuckDB table, and give it a name\n    duckdb::duckdb_register_arrow(con, \"my_table\", ds)\n    # query\n    dbGetQuery(con, \"\n      SELECT sepal_length, COUNT(*) AS n\n      FROM my_table\n      WHERE species = 'species=setosa'\n      GROUP BY sepal_length\n    \")\n\n    # clean up\n    duckdb_unregister(con, \"my_table\")\n    dbDisconnect(con)\n    ```\n\n    -   filtering using a partition, the WHERE format is '\\&lt;partition_variable\\&gt;=\\&lt;partition_value\\&gt;'\n\nSpace between bullet and top ticks\nSpace between bottom ticks and bullet\nNote alignment of text\n\nAdd Code Annotations\n-   [Partition a large file and write to arrow format]{.underline}\n\n    ``` r\n    lrg_file &lt;- open_dataset(&lt;file_path&gt;, format = \"csv\") # &lt;1&gt;\n    lrg_file %&gt;%\n        group_by(var) %&gt;% # &lt;2&gt;\n        write_dataset(&lt;output_dir&gt;, format = \"feather\") # &lt;3&gt;\n    ```\n\n    1.  Pass the file path to `open_dataset()`\n\n    2.  Use `group_by()` to partition the Dataset into manageable chunks\n\n    3.  Use `write_dataset()` to write each chunk to a separate Parquet file---all without needing to read the full CSV file into R\n\n    -   `open_dataset` is fast because it only reads the metadata of the file system to determine how it can construct queries\nFootnote\nwords [^1]\n\n[^1]: Data from https://github.com/rfordatascience/tidytuesday\nFor PDF output, you need pagebreaks:\n{{&lt; pagebreak &gt;}}",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-yaml",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-yaml",
    "title": "Quarto",
    "section": "YAML",
    "text": "YAML\n\nSet global chunk options in yaml\n\n\nFor code cells\nexecute:\n  echo: false\n  message: false\n  warning: false\n\nEnable Margin Notes\n---\n# YAML front matter\nreference-location: margin\n---\n!expr to render code within chunk options\n\ne.g. figure caption: #| fig-cap: !expr glue::glue(\"The mean temperature was {mean(airquality$Temp) |&gt; round()}\")\n\ncolumn: screen-inset yaml markup is used to show a very wide table\nIf you haven’t set your Quarto document to be self-contained, then the images have also already been saved for you - probably in a folder called documentname_files/figure-html/\nformat: \n  html:\n    embed-resources: true\nDate first published and date modified using the current date:\n---\ndate: 2024-01-01\ndate-modified: today\n---\nYAML Examples\n\nExample",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-chunk",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-chunk",
    "title": "Quarto",
    "section": "Chunk Options",
    "text": "Chunk Options\n\nGraphics\n\nCode Chunk\n#| label: \"fig-statemap\"\n#| dpi: 300\n#| fig.height: 7.2\n#| fig.width: 3.6\n#| dev: \"png\"\n#| echo: false\n#| warning: false\n#| message: false\n\nExample shows settings for a graph for mobile\nfig.height and fig.width are always given in inches\n\nReference Figure\n1 See polling place locations in @fig-statemap.\n\nConditional Code Chunk Evaluation\n\nExample: document output type\n\nSet value in a code chunk\n```{r setup}\n# Include in first chunk of .qmd\n# Get output file type\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\n```\nUse !expr sytax to determine evaluation status\n\nExample: eval chunk based on output type\n```{r}\n#| eval: !expr out_type == \"html\"\n\n# code to create interactive {plotly}\n```\n\n```{r}\n#| eval: !expr out_type == \"docx\"\n\n# code to create static {ggplot2}\n```\n\n\nExample: Use parameterization to set value\n---\ntitle: \"test\"\nformat: html\nparams:\n  my_value: false\n---\n\nmy_value can then be used throughout the document to determine chunk evaluation status\n\n\nKnitr Hooks\n\nNotes from Writing knitr hooks\n\nAlso has a knitr hook example that alters cell output (e.g. only prints 4 lines of a vector)\n\nChunk Hooks\n\nChunk hooks get called twice: once before knitr executes the code in the chunk, and once again afterwards\nThe function can take up to four arguments, all of which are optional:\n\nbefore: A logical value indicating whether the function is being called before or after the code chunk is executed\noptions: The list of chunk options\nenvir: The environment in which the code chunk is executed\nname: The name of the code chunk option that triggered the hook function\n\nThe chunk hook is called for its side effects not the return value. However, if it returns a character output, knitr will add that output to the document output as-is.\nExample: Chunk Timer\n\nCode\ncreate_timer_hook &lt;- function() {\n  start_time &lt;- NULL\n  function(before, options) {\n    if (before) {\n      start_time &lt;&lt;- Sys.time()\n    } else {\n      stop_time &lt;- Sys.time()\n      elapsed &lt;- difftime(stop_time, start_time, units = \"secs\")\n      paste(\n        \"&lt;div style='font-size: 70%; text-align: right'&gt;\",\n        \"Elapsed time:\", \n        round(elapsed, 2), \n        \"secs\",\n        \"&lt;/div&gt;\"\n      )\n    }\n  }\n}\nknitr::knit_hooks$set(timer = create_timer_hook())\n\nThe hook is triggered the first time (with before = TRUE) to record the system time somewhere (e.g., in a variable called start_time). Then, when the hook is triggered the second time (with before = FALSE), it records the system time again (e.g., as stop_time), and computes the difference in time.\n\nUse in a cell\n```{r}\n#| timer: true\nrunif(10000)\n```\nOutput",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-rpy",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-rpy",
    "title": "Quarto",
    "section": "R and Python",
    "text": "R and Python\n\nIf only R or R and Python, the notebook is rendered by {knitr}\nIf only Python, the notebook is rendered by jupyter\nSet-up\n\n{reticulate} automatically comes loaded in Quarto and it knows to use it when it sees a python block, so you don’t need to load the package\nQuarto will select a version of Python using the Python Launcher on Windows or system PATH on MacOS and Linux. You can override the version of Python used by Quarto by setting the QUARTO_PYTHON environment variable.\n\nIn CLI on Windows, type py is see which version the Python Launcher , and therefore Quarto, is using and py –list to see which versions are installed.\n\n\nR\n```{r}\n#| label: read-data\n#| echo: true\n#| message: false\n#| cache: true\nlemurs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-08-24/lemur_data.csv')\n```\nPython\n```{python}\n#| label: modelling \n#| echo: true \n#| message: false\n\nlemur_data_py = r.lemur_data \nimport statsmodels.api as sm \ny = lemur_data_py[[\"Weight\"]] \nx = lemur_data_py[[\"Age\"]] \nx = sm.add_constant(x) \nmod = sm.OLS(y, x).fit() \nlemur_data_py[\"Predicted\"] = mod.predict(x) \nlemur_data_py[\"Residuals\"] = mod.resid`\n```\n\nUse r. to access the data in the R chunk\nThe first execution of a python cell starts reticulate::repl_python() in the terminal\n\n(back to) R\n```{r}\n#| label: plotting \n#| echo: true \n#| output-location: slide \n#| message: false \n#| fig-align: center \n#| fig-alt: \"Scatter plot of predicted and residual values for the fitted linear model.\" \n\nlibrary(reticulate) \nlibrary(ggplot2) \nlemur_residuals &lt;- py$lemur_data_py \nggplot(data = lemur_residuals, aes(x = Predicted, y = Residuals)) +\n  geom_point(colour = \"#2F4F4F\") +\n  geom_hline(yintercept = 0,\n            colour = \"red\") +\n  theme(panel.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"),\n        plot.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"))\n```\n\nUse py$ to access the data in the Python chunk *\nMust call library(reticulate) in order for Quarto to recognize py$",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-lay",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-lay",
    "title": "Quarto",
    "section": "Layouts",
    "text": "Layouts\n\n2 cols (1 col: text, 1 col: image)\n\n::: {layout=\"[50,50]\"}\n\n::: column\nEvery Quarto project starts with a Quarto file that has the extension `.qmd`.\n\n\nThis particular one analyzes children's early words, but every `.qmd` includes the same three basic elements inside:\n\n\n- A block of metadata at the top, between two fences of `---`s. This is written in [YAML](https://learnxinyminutes.com/docs/yaml/). \n- Narrative text, written in [Markdown](https://commonmark.org/help/tutorial/). \n- Code chunks in gray between two fences of ```` ``` ````, written with R or another programming language.\n\n\nYou can use all three elements to develop your code and ideas in one reproducible document.\n:::\n\n![](img/01-source.png)\n:::\n2 figures, 2 columns (i.e. side-by-side) with captions at the top\n---\nfig-cap-location: top\n---\n\n-   Words\n    -   Predictions of Standard RF vs Oblique RF\n\n        ::: {layout-ncol=\"2\"}\n        ![Standard Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-axpred-1.png){fig-align=\"left\" width=\"432\"}\n\n        ![Oblique Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-oblpred-1.png){fig-align=\"left\" width=\"432\"}\n        :::\n\n        -   Words  \n\nfig-cap-location: bottom is default;\nfig-cap-location: margin is buggy, at least in for project type book. Captions are added to the margins but bullet points mysteriously disappear during rendering to html\n\n2 charts side-by-side extending past body margins\n```{r}\n#| label: my-figure\n#| layout-ncol: 2\n#| column: page\nggplot() + ...\nggplot() + ...\n```\n\n“layout-ncol” says 2 side-by-side columns\n“column: page” says extend column width to the width of the page\n\nNested Tabs",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-auto",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-auto",
    "title": "Quarto",
    "section": "Automation",
    "text": "Automation\n\nIteration and Parameterization\n\nNotes from\n\nVelásquez R-Ladies Nairobi: Code, Slides, Video\n\nIt involves having a “child” document as a template and running it repeatedly with different parameters\nThe “main” document includes the output from the child document\nRendering Options\n\nCLI: e.g. quarto render polling-places-report.qmd -P state:'California'\n{quarto}:\nquarto::quarto_render(\n  input = here::here(\"polling-places-report.qmd\"),\n  execute_params = list(state = \"California\")\n)\n\nExample: Create a report for each parameter value. In each report, use the parameter value (e.g. state) to iterate through a template file that makes a tables (1 for each county) based on that value.\nMain Report Document\n---\ntitle: \"Polling Places Report - `r params$state`\"\nparams:\n  state: \"California\"\n---\n\n```{r}\n#| results: hide\n\nlibrary(dplyr)\n\ncounties &lt;- polling_places |&gt; \n  filter(state == params$state) |&gt; \n  distinct(county_name) |&gt; \n  pull()\n\nexpanded_child &lt;- \n  counties |&gt; \n    purrr::map(\\(county) {\n      knitr::knit_expand(\"../_template.qmd\", \n                         current_county = county))\n      }|&gt; \n    purrr::flatten()\n\nparsed_child &lt;- knitr::knit_child(text = unlist(expanded_child))\n```\n\n`{r} parsed_child`\n\nThe document that gets published, emailed, etc.\nparams specified in YAML\n\nValue can also be used in the title of the document via inline R code\n\nEach county is iterated through the child document (_template.qmd) via current_county variable and knit_expand\nparsed_child is a list of the template file outputs.\nThen, parsed_child is converted to a character vector by unlist and all the results are printed in the document by the inline R code\n\nChild Document (i.e. Template)\n### {{current_county}} COUNTY\n\n-   Total Polling Places: `{r} polling_places |&gt; filter(state == params$state, county_name == \"{{current_county}}\") |&gt; count()`\n-   Example Locations:\n\n```{r}\npolling_places |&gt; \n  filter(state == params$state, \n         county_name == \"{{current_county}}\") |&gt; \n  head(6) |&gt; \n  select(name, address.x) |&gt; \n  kbl(format = \"markdown\")\n```\n\nParameter value is used to get county data and create tables for each.\nNo YAML is necessary in child document\n\nparams values are automatically available through knitr::knit_expand that’s executed in the Main document\n\nThe county variable is utilized by the template file using the double curly braces, {{current_county}}\nkbl outputs in markdown format so the table is correctly rendered in the Main document.\n\nRendering Script\npolling_places &lt;-\n  readr::read_csv(here::here(\"data\", \"geocoded_polling_places.csv\"))\n\n# create quarto::render arguments df\npolling_places_reports &lt;-\n  polling_places |&gt;\n  dplyr::distinct(state) |&gt;\n  dplyr::slice_head(n = 5) |&gt;\n  dplyr::mutate(\n    output_format = \"html\",\n    output_file = paste0(tolower(state),\n                         \"-polling-places\"),\n    execute_params = purrr::map(state,\n                                \\(state) list(state = state))\n  ) |&gt;\n  # default output is html, so that variable not selected\n  dplyr::select(output_file, execute_params) \n\n# iterate through args and create reports\npurrr::pwalk(\n  .l = polling_places_reports,\n  .f = quarto::quarto_render,\n  input = here::here(\"main_report_document.qmd\"),\n  .progress = TRUE\n)\n\nCreates a report for each params value (e.g. state)\nGenerates a dataframe for each set of arguments to be fed to quarto::quarto_render.",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-webr",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-webr",
    "title": "Quarto",
    "section": "WebR",
    "text": "WebR\n\nSet-Up\n\nInstall the extension alongside your blog post by running quarto add coatless/quarto-webr\nAdd the extension to your blog by adding filters: [\"webr\"] to your post’s frontmatter\nInstead of {r} code chunks, use {webr-r} ones\n\nInstall CRAN packages on page load\nfilters:\n  - \"webr\"\nwebr:\n  packages:\n  - \"dplyr\"\n  - \"tidyr\"\n  - \"purrr\"\n  - \"tibble\"\n  - \"crayon\"\n\nAdd to frontmatter\n\nInstall R-Universe Package\n```{webr-r}\n#| context: setup\nwebr::install(\"collateral\", repos = c(\"https://jimjam-slam.r-universe.dev\"))\n```\n\nR-Universe packages must be installed in code cells",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/db-vector.html",
    "href": "qmd/db-vector.html",
    "title": "Vector Databases",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Vector Databases"
    ]
  },
  {
    "objectID": "qmd/db-vector.html#sec-db-vect-misc",
    "href": "qmd/db-vector.html#sec-db-vect-misc",
    "title": "Vector Databases",
    "section": "",
    "text": "Vector databases store embeddings and provide fast similarity searches\nComparison (link)\n\n\nOpen-Source and hosted cloud: If you lean towards open-source solutions, Weviate, Milvus, and Chroma emerge as top contenders. Pinecone, although not open-source, shines with its developer experience and a robust fully hosted solution.\nPerformance: When it comes to raw performance in queries per second, Milvus takes the lead, closely followed by Weviate and Qdrant. However, in terms of latency, Pinecone and Milvus both offer impressive sub-2ms results. If nmultiple pods are added for pinecone, then much higher QPS can be reached.\nCommunity Strength: Milvus boasts the largest community presence, followed by Weviate and Elasticsearch. A strong community often translates to better support, enhancements, and bug fixes.\nScalability, advanced features and security: Role-based access control, a feature crucial for many enterprise applications, is found in Pinecone, Milvus, and Elasticsearch. On the scaling front, dynamic segment placement is offered by Milvus and Chroma, making them suitable for ever-evolving datasets. If you’re in need of a database with a wide array of index types, Milvus’ support for 11 different types is unmatched. While hybrid search is well-supported across the board, Elasticsearch does fall short in terms of disk index support.\nPricing: For startups or projects on a budget, Qdrant’s estimated $9 pricing for 50k vectors is hard to beat. On the other end of the spectrum, for larger projects requiring high performance, Pinecone and Milvus offer competitive pricing tiers.",
    "crumbs": [
      "Databases",
      "Vector Databases"
    ]
  },
  {
    "objectID": "qmd/db-vector.html#sec-db-vect-bran",
    "href": "qmd/db-vector.html#sec-db-vect-bran",
    "title": "Vector Databases",
    "section": "Brands",
    "text": "Brands\n\nQdrant - open source, free, and easy to use (example)\nChroma - can be used as a local in-memory (example)\nPinecone - Data Elixir is using this store for their chatbot; has a free tier\nPostgres with pgvector: Supports exact and approximate nearest neighbor search; L2 distance, inner product, and cosine distance; any language with a Postgres client\n\nAlso see Databases, PostgreSQL &gt;&gt; Extensions &gt;&gt; pgvector and pg_sparse for sparse embeddings (e.g. SPLADE)",
    "crumbs": [
      "Databases",
      "Vector Databases"
    ]
  },
  {
    "objectID": "qmd/scraping.html",
    "href": "qmd/scraping.html",
    "title": "Scraping",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-misc",
    "href": "qmd/scraping.html#sec-scrap-misc",
    "title": "Scraping",
    "section": "",
    "text": "Packages\n\n{rvest}\n{rselenium}\n{selenium}\n{selenider} - Wrapper functions around {chromote} and {selenium} functions that utilize lazy element finding and automatic waiting to make scraping code more reliable\n{shadowr} - For shadow DOMs\n\nIn loops, use Sys.sleep (probably) after EVERY selenium function. Sys.sleep(1) might be all that’s required. ({selenider} fixes this problem)\n\nsee Projects &gt; foe &gt; gb-level-1_9-thread &gt; scrape-gb-levels.R\nMight not always be needed, but absolutely need if you’re filling out a form and submitting it.\nMight even need one at the top of the loop\nIf a Selenium function stops working, adding Sys.sleeps are worth a try.\n\nSometimes clickElement( ) stops working for no apparent reason. When this happens used sendKeysToElement(list(\"laptops\",key=\"enter\"))\nIn batch scripts (.bat), sometimes after a major windows update, the Java that selenium uses will trigger Windows Defender (WD) and cause the scraping script to fail (if you have it scheduled). If you run the .bat script manually and then when the WD box rears its ugly head, just click ignore. WD should remember after that and not to mess with it.\nRSelenium findElement(using = \"\") options “class name” : Returns an element whose class name contains the search value; compound class names are not permitted.\n\n“css selector” : Returns an element matching a CSS selector.\n“id” : Returns an element whose ID attribute matches the search value.\n“name” : Returns an element whose NAME attribute matches the search value.\n“link text” : Returns an anchor element whose visible text matches the search value.\n“partial link text” : Returns an anchor element whose visible text partially matches the search value.\n“tag name” : Returns an element whose tag name matches the search value.\n“xpath” : Returns an element matching an XPath expression.",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-terms",
    "href": "qmd/scraping.html#sec-scrap-terms",
    "title": "Scraping",
    "section": "Terms",
    "text": "Terms\n\nStatic Web Page: A web page (HTML page) that contains the same information for all users. Although it may be periodically updated, it does not change with each user retrieval.\nDynamic Web Page: A web page that provides custom content for the user based on the results of a search or some other request. Also known as “dynamic HTML” or “dynamic content”, the “dynamic” term is used when referring to interactive Web pages created for each user.",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-rvest",
    "href": "qmd/scraping.html#sec-scrap-rvest",
    "title": "Scraping",
    "section": "rvest",
    "text": "rvest\n\nMisc\n\nNotes from: Pluralsight.Advanced.Web.Scraping.Tactics.R.Playbook\n\nUses css selectors or xpath to find html nodes\nlibrary(rvest)\npage &lt;- read_html(\"&lt;url&gt;\")\nnode &lt;- html_element(page, xpath = \"&lt;xpath&gt;\"\n\nFind css selectors\n\nselector gadget\n\nclick selector gadget app icon in Chrome in upper right assuming you’ve installed it already\nclick item on webpage you want to scrape\n\nit will highlight other items as well\n\nclick each item you DON’T want to deselect it\ncopy the selector name in box at the bottom of webpage\nUse html_text to pull text or html_attr to pull a link or something\n\ninspect\n\nright-click item on webpage\nclick inspect\nhtml element should be highlighted in elements tab of right side pan\nright-click element –&gt; copy –&gt; copy selector or copy xpath\n\n\n\nExample: Access data that needs authentication (also see RSelenium version)\n\nnavigate to login page\nsession &lt;- session(\"&lt;login page url&gt;\")\nFind “forms” for username and password\nform &lt;- html_form(session)[[1]]\nform\n\nEvidently there are multiple forms on a webpage. He didn’t give a good explanation for why he chose the first one\n“session_key” and “session_password” are the ones needed\n\nFill out the necessary parts of the form and send it\nfilled_form &lt;- html_form_set(form, session_key = \"&lt;username&gt;\", session_password = \"&lt;password&gt;\")\nfilled_form # shows values that inputed next the form sections\nlog_in &lt;- session_submit(session, filled_form)\nConfirm that your logged in\nlog_in # prints url status = 200, type = text/html, size = 757813 (number of lines of html on page?)\nbrowseURL(log_in$url) # think this maybe opens browser\n\nExample: Filter a football stats table by selecting values from a dropdown menu on a webpage (also see RSelenium version)\n\nAfter set-up and navigating to url, get the forms from the webpage\nforms &lt;- html_form(session)\nforms # prints all the forms\n\nThe fourth has all the filtering menu categories (team, week, position, year), so that one is chosen\n\nFill out the form to enter the values you want to use to filter the table and submit that form to filter the table\nfilled_form &lt;- html_form_set(forms[[4]], \"team\" = \"DAL\", \"week\" = \"all\", \"position\" = \"QB\", \"year\" = \"2017\")\nsubmitted_session &lt;- session_submit(session = session, form = filled_form)\nLook for the newly filtered table\ntables &lt;- html_elements(submitted_session, \"table\")\ntables\n\nUsing inspect, you can see the 2nd one has &lt;table class = “sortable stats-table…etc\n\nSelect the second table and convert it to a dataframe\nfootball_df &lt;- html_table(tables[[2]], header = TRUE)",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-rsel",
    "href": "qmd/scraping.html#sec-scrap-rsel",
    "title": "Scraping",
    "section": "RSelenium",
    "text": "RSelenium\n\nAlong with installing package you have to know the version of the browser driver of the browser you’re going to use\n\nhttps://chromedriver.chromium.org/downloads\nFind Chrome browser version\n\nThrough console\nsystem2(command = \"wmic\",\n        args = 'datafile where name=\"C:\\\\\\\\Program Files         (x86)\\\\\\\\Google\\\\\\\\Chrome\\\\\\\\Application\\\\\\\\chrome.exe\" get Version /value')\n\nList available Chrome drivers\nbinman::list_versions(appname = \"chromedriver\")\n\nIf no exact driver version matches your browser version,\n\nEach version of the Chrome driver supports Chrome with matching major, minor, and build version numbers.\n\nExample: Chrome driver 73.0.3683.20  supports all Chrome versions that start with 73.0.3683\n\n\n\n\nStart server and create remote driver\n\na browser will pop up and say “Chrome is being controlled by automated test software”\n\nlibrary(RSelenium)\ndriver &lt;- rsDriver(browser = c(\"chrome\"), chromever = \"&lt;driver version&gt;\", port = 4571L) # assume the port number is specified by chrome driver ppl.\nremDr &lt;- driver[['client']] # can also use $client\nNavigate to a webpage\nremDr$navigate(\"&lt;url&gt;\")\nremDR$maxWindowSize(): Set the size of the browser window to maximum.\n\nBy default, the browser window size is small, and some elements of the website you navigate to might not be available right away\n\nGrab the url of the webpage you’re on\nremDr$getCurrentUrl()\nGo back and forth between urls\nremDr$goBack()\nremDr$goForward()\nFind html element (name, id, class name, etc.)\nwebpage_element &lt;- remDr$findElement(using = \"name\", value = \"q\") \n\nSee Misc section for selector options\nWhere “name” is the element class and “q” is the value e.g. name=“q” if you used the inspect method in chrome\nAlso see Other Stuff &gt;&gt; Shadow DOM elements &gt;&gt; Use {shadowr} for alternate syntax to search for web elements\n\nHighlight element in pop-up browser to make sure you have the right thing\nwebpage_element$highlightElement()\nExample: you picked a search bar for your html element and now you want to use the search bar from inside R\n\nEnter text into search bar\nwebpage_element$sendKeysToElement(list(\"Scraping the web with R\"))\nHit enter to execute search\nwebpage_element$sendKeysToElement(list(key = \"enter\"))\n\nYou are now on the page with the results of the google search\n\nScrape all the links and titles on that page\nwebelm_linkTitles &lt;- remDr$findElement(using = \"css selector\", \".r\") \n\nInspect showed ”\n\n\n. Notice he used “.r”. Says it will pick-up all elements with “r” as the class.\n\nGet titles\n# first title\nwebelm_linkTitles[[1]]$getElementText()\n\n# put them all into a list\ntitles &lt;- purrr::map_chr(webelm_linkTitles, ~.x$getElementText())\ntitles &lt;- unlist(lapply(\n    webelm_linkTitles, \n    function(x) {x$getElementText()}\n\nExample: Access data that needs user authentication (also see rvest version)\n\nAfter set-up and navigating to webpage, find elements where you type in your username and password\nwebelm_username &lt;- remDr$findElement(using = \"id\", \"Username\")\nwebelm_pass &lt;- remDr$findElement(using = \"id, \"Password\")\nEnter username and password\nwebpage_username$sendKeysToElement(list(\"&lt;username&gt;\"))\nwebpage_pass$sendKeysToElement(list(\"&lt;password&gt;\"))\nClick sign-in button and click it\nwebelm_sbutt &lt;- remDr$findElement(using = \"class\", \"psds-button\")\nwebelm_sbutt$clickElement()\n\nExample: Filter a football stats table by selecting values from a dropdown menu on a webpage (also see rvest version)\n\nThis is tedious — use rvest to scrape this if possible (have to use rvest at the end anyways). html forms are the stuff.\nAfter set-up and navigated to url, find drop down “team” menu element locator using inspect in the browser and use findElement\nwebelem_team &lt;- remDr$findElement(using = \"name\", value = \"team\") # conveniently has name=\"team\" in the html\n\nAlso see Other Stuff &gt;&gt; Shadow DOM elements &gt;&gt; Use {shadowr} for alternate syntax to search for web elements\n\nclick team dropdown\nwebelem_team$clickElement()\nGo back to inspect in the browser, you should be able to expand the team menu element. Left click value that you want to filter team by to highlight it. Then right click the element and select “copy” –&gt; “copy selector”. Paste selector into value arg\nwebelem_DAL &lt;- remDr$findElement(using = \"css\", value = \"edit-filters-0-team &gt; option:nth-child(22)\")\nwebelem_DAL$clickElement()\n\nAlso see Other Stuff &gt;&gt; Shadow DOM elements &gt;&gt; Use {shadowr} for alternate syntax to search for web elements\nRepeat process for week, position, and year drop down menu filters\n\nAfter you’ve selected all the values in the dropdown, click the submit button to filter the table\nwebelem_submit &lt;- remDr$findElement(using = \"css\", value =     \"edit-filters-0-actions-submit\") \nwebelem_submit$clickElement()\n\nFinds element by using inspect on the submit button and copying the selector\n\nGet the url of the html code of the page with the filtered table. Read html code into R with rvest.\nurl &lt;- remDr$getPageSource()[[1]]\nhtml_page &lt;- rvest::read_html(url)\n\nIf you want the header, getPageSource(header = TRUE)\n\nUse rvest to scrape the table. Find the table with the stats\nall_tables &lt;- rvest::html_elements(html_page, \"table\")\nall_tables\n\nUsed the “html_elements” version instead of “element”\nThird one has “&lt;table class =”sortable stats-table full-width blah blah”\n\nSave to table to dataframe\nfootball_df &lt;- rvest::html_table(all_tables[[3]], header = TRUE)",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-ostuff",
    "href": "qmd/scraping.html#sec-scrap-ostuff",
    "title": "Scraping",
    "section": "Other Stuff",
    "text": "Other Stuff\n\nClicking a semi-infinite scroll button (e.g. “See more”)\n\nExample: For-Loop\n# Find Page Element for Body\nwebElem &lt;- remDr$findElement(\"css\", \"body\")\n\n# Page to the End\nfor (i in 1:50) {\n  message(paste(\"Iteration\",i))\n  webElem$sendKeysToElement(list(key = \"end\"))\n\n  # Check for the Show More Button\n  element&lt;- try(unlist(\n      remDr$findElement(\n        \"class name\",\n        \"RveJvd\")$getElementAttribute('class')), silent = TRUE)\n\n  #If Button Is There Then Click It\n  Sys.sleep(2)\n  if(str_detect(element, \"RveJvd\") == TRUE){\n    buttonElem &lt;- remDr$findElement(\"class name\", \"RveJvd\")\n    buttonElem$clickElement()\n  }\n\n  # Sleep to Let Things Load\n  Sys.sleep(3)\n}\n\narticle\nAfter scrolling to the “end” of the page, there’s a “show me more button” that loads more data on the page\n\nExample: Recursive\nload_more &lt;- function(rd) {\n  # scroll to end of page\n  rd$executeScript(\"window.scrollTo(0, document.body.scrollHeight);\", args = list())\n\n  # Find the \"Load more\" button by its CSS selector and ...\n  load_more_button &lt;- rd$findElement(using = \"css selector\", \"button.btn-load.more\")\n\n  # ... click it\n  load_more_button$clickElement()\n\n  # give the website a moment to respond\n  Sys.sleep(5)\n}\n\nload_page_completely &lt;- function(rd) {\n  # load more content even if it throws an error\n  tryCatch({\n    # call load_more()\n    load_more(rd)\n    # if no error is thrown, call the load_page_completely() function again\n    Recall(rd)\n  }, error = function(e) {\n    # if an error is thrown return nothing / NULL\n  })\n}\n\nload_page_completely(remote_driver)\n\narticle\nRecall is a base R function that calls the same function it’s in.\n\n\nShadow DOM elements\n\n#shadow-root and shadow dom button elements\nMisc\n\nTwo options: {shadowr} or JS script\n\nExample: Use {shadowr}\n\nMy stackoverflow post\nSet-up\npacman::p_load(RSelenium, shadowr)\ndriver &lt;- rsDriver(browser = c(\"chrome\"), chromever = chrome_driver_version)\n# chrome browser\nchrome &lt;- driver$client\nshadow_rd &lt;- shadow(chrome)\nFind web element\n\nSearch for element using html tag\n\n\nwisc_dl_panel_button4 &lt;- shadowr::find_elements(shadow_rd, 'calcite-button')\nwisc_dl_panel_button4[[1]]$clickElement()\n\nShows web element located in #shadow-root\nSince there might be more than one element with the “calcite-button” html tag, we use the plural, find_elements, instead of find_element\nThere’s only 1 element returned, so we use [[1]] index to subset the list before clicking it\n\nSearch for web element by html tag and attribute\nwisc_dl_panel_button3 &lt;- find_elements(shadow_rd, 'button[aria-describedby*=\"tooltip\"]')\nwisc_dl_panel_button3[[3]]$clickElement()\n\n“button” is the html tag which is subsetted by the brackets, and “aria-describedby” is the attribute\nOnly part of the attribute’s value is used, “tooltip,” so I think that’s why “*=” instead of just “=” is used. I believe the “*” may indicate partial-matching.\nSince there might be more than one element with this  html tag + attribute combo, we use the plural, find_elements, instead of find_element\nThere are 3 elements returned, so we use [[3]] index to subset the list to element we want before clicking it\n\n\nExample: Use a JS script and some webelement hacks to get a clickable element\n\nMisc\n\n“.class_name”\n\nfill in spaces with periods\n\n“.btn btn-default hidden-xs” becomes “.btn.btn-default.hidden-xs”\n\n\n\nYou can find the element path to use in your JS script by going step by step with JS commands in the Chrome console (bottom window)\n\nSteps\n\nWrite JS script to get clickable element’s elementId\n\nStart with element right above first shadow-root element and use querySelector\nMove to the next element inside the next shadow-root element using shadowRoot.querySelector\nContinue to desired clickable element\n\nIf there’s isn’t another shadow-root that you have to open, then the next element can be selected usingquerySelector\nIf you do have to click on another shadow-root element to open another branch, then used shadowRoot.querySelector\nExample\n\n\n“hub-download-card” is just above shadow-root so it needs querySelector\n“calcite-card” is an element that’s one-step removed from shadow-root, so it needs shadowRoot.querySelector\n“calcite-dropdown” (type = “click”) is not directly (see div) next to shadow-root , so it can selected using querySelector\n\n\nWrite and execute JS script\nwisc_dlopts_elt_id &lt;- chrome$executeScript(\"return document.querySelector('hub-download-card').shadowRoot.querySelector('calcite-card').querySelector('calcite-dropdown');\")\n\nMake a clickable element or just click the damn thing\n\nclickable element (sometimes this doesn’t work; needs to be a button or type=click)\n\nUse findElement to find a generic element class object that you can manipulate\nUse “@” ninja-magic to force elementId into the generic webElement to coerce it into your button element\nUse clickElement to click the button\n\n# think this is a generic element that can always be used\nmoose &lt;- chrome$findElement(\"css\", \"html\")\nmoose@.xData$elementId &lt;- as.character(wisc_dlopts_elt_id)\nmoose$clickElement()\n\nClick the button\nchrome$executeScript(\"document.querySelector('hub-download-card').shadowRoot.querySelector('calcite-card').querySelector('calcite-dropdown').querySelector('calcite-dropdown-group').querySelector('calcite-dropdown-item:nth-child(2)').click()\")\n\n\n\nGet data from a hidden input\n\narticle\nHTML Element\n&lt;input type=\"hidden\" id=\"overview-about-text\" value=\"%3Cp%3E100%25%20Plant-Derived%20Squalane%20hydrates%20your%20skin%20while%20supporting%20its%20natural%20moisture%20barrier.%20Squalane%20is%20an%20exceptional%20hydrator%20found%20naturally%20in%20the%20skin,%20and%20this%20formula%20uses%20100%25%20plant-derived%20squalane%20derived%20from%20sugar%20cane%20for%20a%20non-comedogenic%20solution%20that%20enhances%20surface-level%20hydration.%3Cbr%3E%3Cbr%3EOur%20100%25%20Plant-Derived%20Squalane%20formula%20can%20also%20be%20used%20in%20hair%20to%20increase%20heat%20protection,%20add%20shine,%20and%20reduce%20breakage.%3C/p%3E\"&gt;\nExtract value and decode the text\noverview_text &lt;- webpage |&gt;\n  html_element(\"#overview-about-text\") |&gt;\n  html_attr(\"value\") |&gt;\n  URLdecode() |&gt;\n  read_html() |&gt;\n  html_text()\n\noverview_text\n#&gt; [1] \"100% Plant-Derived Squalane hydrates your skin while supporting its natural moisture barrier.",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html",
    "href": "qmd/db-lakes.html",
    "title": "Lakes",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-misc",
    "href": "qmd/db-lakes.html#sec-db-lakes-misc",
    "title": "Lakes",
    "section": "",
    "text": "Data is stored in structured format or in its raw native format without any transformation at any scale.\n\nHandling both types allows all data to be centralized which means it can be better organized and more easily accessed.\n\nOptimal for fit for bulk data types such as server logs, clickstreams, social media, or sensor data.\nIdeal use cases\n\nBackup for logs\nRaw sensor data for your IoT application,\nText files from user interviews\nImages\nTrained machine learning models (with the database simply storing the path to the object)\n\nLower storage costs due to their more open-source nature and undefined structure\nOn-Prem set-ups have to manage hardward and environments\n\nIf you wanted to separate stuff like test data from production data, you also probably had to set up new hardware.\nIf you had data in one physical environment that had to be used for analytical purposes in another physical environment, you probably had to copy that data over to the new replica environment.\n\nHave to keep a tie to the source environment to ensure that the stuff in the replica environment is still up-to-date, and your operational source data most likely isn’t in one single environment. It’s likely that you have tens — if not hundreds — of those operational sources where you gather data.\n\nWhere on-prem set-ups focus on isolating data with physical infrastructure, cloud computing shifts to focus on isolating data using security policies.\n\nObject Storage Systems\n\nCloud data lakes provide organizations with additional opportunities to simplify data management by being accessible everywhere to all applications as needed\nOrganized as collections of files within directory structures, often with multiple files in one directory representing a single table.\n\nPros: highly accessible and flexible\nMetadata Catalogs are used to answer these questions:\n\nWhat is the schema of a dataset, including columns and data types\nWhich files comprise the dataset and how are they organized (e.g., partitions)\nHow different applications coordinate changes to the dataset, including both changes to the definition of the dataset and changes to data\n\nHive Metastore (HMS) and AWS Glue Data Catalog are two popular catalog options\n\nContain the schema, table structure and data location for datasets within data lake storage\n\n\nIssues:\n\nDoes not coordinate data changes or schema evolution between applications in a transactionally consistent manner.\n\nCreates the necessity for data staging areas and this extra layer makes project pipelines brittle",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-nosql.html",
    "href": "qmd/db-nosql.html",
    "title": "NoSQL",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "NoSQL"
    ]
  },
  {
    "objectID": "qmd/db-nosql.html#sec-db-nosql-misc",
    "href": "qmd/db-nosql.html#sec-db-nosql-misc",
    "title": "NoSQL",
    "section": "",
    "text": "High-Performance data ingestion and retrieval for specific applications, especially with structured or semi-structured data.\n\nData lakes can also store semi-structured data, but they don’t have the responsiveness (i.e. low latency, reading, writing) for apps, etc.\nNoSQL dbs can be specialized for particular tasks like social media, email, gaming, etc.\n\nApplications (Google Gemini)\n\nData Analytics: For real-time log monitoring, frequent analysis, smaller datasets, and short term to medium term storage. For long-term storage, a data lake would be a better choice.\nCaching: Due to their high speed and low latency, they are perfect for caching frequently accessed data, significantly improving application performance. Popular use cases include caching user sessions, search results, and product information in e-commerce platforms.\nSession Management: In web applications, they efficiently store user session data, shopping carts, and temporary preferences. This enables seamless user experience and personalization across sessions.\nIoT and Real-time Data: Sensors and devices in IoT ecosystems generate large volumes of real-time data. These types of dbs excel at capturing and storing sensor readings, timestamps, and device states, offering real-time insights and analytics.\nGaming and Leaderboards: Prominent use in online gaming for storing player profiles, high scores, and game state information. Their fast retrieval and update capabilities ensure smooth gameplay and accurate leaderboards.\nSocial Media and Messaging: Storing user profiles, connections, messages, and notifications benefits greatly from the scalability and efficient retrieval. This enables handling millions of users and delivering real-time interactions.\nAuthentication and Authorization: Securel storage of user credentials, session tokens, and access control information. This enables efficient user authentication and authorization, securing access to sensitive data and functionalities.\nConfiguration Management: Storing application configuration settings, API keys, and environment variables in a key-value store simplifies management and deployment. This allows for dynamic configuration changes and simplifies scaling processes.\n\nBrands: ScyllaDB, Cassandra, MongoDB, DynamoDB",
    "crumbs": [
      "Databases",
      "NoSQL"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-brands",
    "href": "qmd/db-lakes.html#sec-db-lakes-brands",
    "title": "Lakes",
    "section": "Brands",
    "text": "Brands\n\nHadoop\n\nTraditional format for data lakes\n\nAmazon S3\n\nTry to stay &lt;1000 entries per level of hierarchy when designing the partitioning format. Otherwise there is paging and things get expensive.\nAWS Athena ($5/TB scanned)\n\nAWS Athena is serverless and intended for ad-hoc SQL queries against data on AWS S3\n\n\nMicrosoft Azure Data Lake Storage (ADLS)\nMinio\n\nOpen-Source alternative to AWS S3 storage.\nGiven that S3 often stores customer PII (either inadvertently via screenshots or actual structured JSON files), Minio is a great alternative to companies mindful of who has access to user data.\n\nOf course, AWS claims that AWS personnel doesn’t have direct access to customer data, but by being closed-source, that statement is just a function of trust.\n\n\nDatabricks Delta Lake\nGoogle Cloud Storage\n\n5 GB of US regional storage free per month, not charged against your credits.",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-iceb",
    "href": "qmd/db-lakes.html#sec-db-lakes-iceb",
    "title": "Lakes",
    "section": "Apache Iceberg",
    "text": "Apache Iceberg\n\nOpen source table format that addresses the performance and usability challenges of using Apache Hive tables in large and demanding data lake environments.\nInterfaces\n\nDuckDB can query Iceberg tables in S3 with an extension, docs\nAthena can create Iceberg Tables\nGoogle Cloud Storage has something called BigLake that can create Iceberg tables\n\nFeatures\n\nTransactional consistency between multiple applications where files can be added, removed or modified atomically, with full read isolation and multiple concurrent writes\nFull schema evolution to track changes to a table over time\nTime travel to query historical data and verify changes between updates\nPartition layout and evolution enabling updates to partition schemes as queries and data volumes change without relying on hidden partitions or physical directories\nRollback to prior versions to quickly correct issues and return tables to a known good state\nAdvanced planning and filtering capabilities for high performance on large data volumes\nThe full history is maintained within the Iceberg table format and without storage system dependencies\n\nSupports common industry-standard file formats, including Parquet, ORC and Avro\nSupported by major data lake engines including Dremio, Spark, Hive and Presto\nQueries on tables that do not use or save file-level metadata (e.g., Hive) typically involve costly list and scan operations\nAny application that can deal with parquet files can use Iceberg tables and its API in order to query more efficiently\nComparison",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/eda-general.html",
    "href": "qmd/eda-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-",
    "href": "qmd/eda-general.html#sec-eda-gen-",
    "title": "General",
    "section": "",
    "text": "First contact with an unfamiliar database\n\nselect * from limit 50\nLook for keys/fields to connect tables\nMake running list of Q’s, try to answer them by poking around first\nFind team/code responsible for DB and ask for time to review questions – communication can be a superpower here!\n\nUse domain knowledge to assess peculier relationships\n\nExample: Is there a nonlinear relationship between Driver hours and Incentive Level\n\n\nCommon sense says if we raise payment bonuses, we should see more drivers want to work more hours.\nReason behind the relationship shown in this chart is omitted variables: weather and holiday.\n\nIncentives stop having an effect on drivers because they hate going out in shitty weather and want to stay home with their family on the holidays.",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-bascln",
    "href": "qmd/eda-general.html#sec-eda-gen-bascln",
    "title": "General",
    "section": "Basic Cleaning",
    "text": "Basic Cleaning\n\nTidy column names\nShrink long column names to something reasonable enough for an axis label\nMake sure continuous variables aren’t initially coded as categoricals and vice versa\nMake note of columns with several values per cell and will need to be separated into multiple columns (e.g. addresses)\nFind duplicate rows\n\nSee\n\nCode, Snippets &gt;&gt; Cleaning\nSQL &gt;&gt; Processing Expressions &gt;&gt; Duplicates\nPython, Pandas &gt;&gt; Distinct\n\nThese can cause data leakage if the same row is in the test and train sets.\n\nMake a note to remove columns that the target is a function of\n\ne.g. Don’t use monthly salary to predict yearly salary\n\nRemove columns that occur after the target event\n\ne.g. Using info occurring in or after a trial to predict something pre-trial\n\nYou won’t have this info beforehand when you make your prediction\n\n\nOrdinal categorical\n\nReorder by a number in the text (parse_number)\nmutate(income_category = fct_reorder(income_category, parse_number(income_category)),\n      # manually fix category that is still out of order\n      # moves \"Less thatn $40K\" to first place in the levels\n      income_category = fct_relevel(income_category, \"Less than $40K\"))",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-pkgs",
    "href": "qmd/eda-general.html#sec-eda-gen-pkgs",
    "title": "General",
    "section": "Packages",
    "text": "Packages\n\nBase R coplot can be used for quick plots of all combinations of categorical and continuous variables for up to 4 variables\n\nSee Continuous Predictor vs Outcome &gt;&gt; Continuous Outcome for examples\n\n{skimr::skim} - Overall summary, check completion percentage for vars with too many NAs\n{dataexplorer}\ncreate_report(airquality)\ncreate_report(diamonds, y = \"price\") # specify response variable\n\nRuns multiple functions to analyze dataset\n\n{dataxray} - Table with interactive distributions, summary stats, missingness, proportions. (dancho article/video)\n{trelliscope} - Quick, interactive, facetted pairwise plots, built with JS\n{explore} - Interactive data exploration or automated report\n\nexplore - If you want to explore a table, a variable or the relationship between a variable and a target (binary, categorical or numeric). The output of these functions is a plot (automatically checks if an attribute is categorical or numerical, chooses the best plot-type and handles outliers).\ndescribe - If you want to describe a dataset or a variable (number of na, unique values, …) The output of these functions is a text.\nexplain - To create a simple model that explains a target. explain_tree() for a decision tree, explain_forest() for a random forest and explain_logreg() for a logistic regression.\nreport - To generate an automated report of all variables. A target can be defined (binary, categorical or numeric)\nabtest - To test if a difference is statistically significant\n\n{visdat} has decent visualization for group comparison, missingness, correlation, etc.\n{Hmisc::describe}\nsparkline::sparkline(0)\ndes &lt;- describe(d)\nplot(des) # maybe for displaying in Viewer pane\nprint(des, 'both') # maybe just a console df of the numbers\nmaketabs(print(des, 'both'), wide=TRUE) # for Quarto\n\n“both” says display “continuous” and “categorical”\n“continuous”\n\n“categorical”\n\nColumns (from Hmisc Ref Manual)\n\n“Info”: Info which is a relative information measure using the relative efficiency of a proportional odds/Wilcoxon test on the variable relative to the same test on a variable that has no ties. Info is related to how continuous the variable is, and ties are less harmful the more untied values there are. The formula for Info is one minus the sum of the cubes of relative frequencies of values divided by one minus the square of the reciprocal of the sample size. The lowest information comes from a variable having only one distinct value following by a highly skewed binary variable. Info is reported to two decimal places.\n“Mean” and “Sum” (Binary): , the sum (number of 1’s) and mean (proportion of 1’s)\n\n\nLux - Jupyter notebook widget that provides visual data profiling via existing pandas functions which makes this extremely easy to use if you are already a pandas user. It also provides recommendations to guide your analysis with the intent function. However, Lux does not give much indication as to the quality of the dataset such as providing a count of missing values for example.\n{{pandas_profiling}} - Produces a rich data profiling report with a single line of code and displays this in line in a Juypter notebook. The report provides most elements of data profiling including descriptive statistics and data quality metrics. Pandas-profiling also integrates with Lux.\n{{sweetviz}} - Provides a comprehensive and visually attractive dashboard covering the vast majority of data profiling analysis needed. This library also provides the ability to compare two versions of the same dataset which the other tools do not provide.\n{{ydata-profiling}} - Data profiling, automates, and standardizes the generation of detailed reports, complete with statistics and visualizations",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-miss",
    "href": "qmd/eda-general.html#sec-eda-gen-miss",
    "title": "General",
    "section": "Missingness",
    "text": "Missingness\n\nAlso see\n\nMissingness\nModel Building, tidymodels &gt;&gt; Recipe &gt;&gt; Imputation\n\nPackages\n\n{naniar} - tidy ways to summarize, visualize, and manipulate missing data with minimal deviations from the workflows in ggplot2 and tidy data\n{qreport} - Harrell package\n\nA few of the charts aren’t intuitive and don’t have good documentation in terms of explaining how to interpret them.\nFits an ordinal logistic regression model to describe which types of subjects (based on variables with no NAs) tend to have more variables missing.\nHierarchically clusters variables that have similar observations missing\nSee naclus docs, RMS Ch.19.1, R Workflow Ch.2.7 (interprets the clustering), Ch.6 (interpretes the ordinal regression) (possibly more use cases in that ebook)\n\n\nQuestions\n\nWhich features contain missing values?\nWhat proportion of records for each feature comprises missing data?\nIs the missing data missing at random (MAR) or missing not at random (MNAR) (i.e. informative)?\nAre the features with missing values correlated with other features?\n\nCategoricals for binary classification\n\ntrain_raw %&gt;%\n  select(\n    damaged, precipitation, visibility, engine_type,\n    flight_impact, flight_phase, species_quantity\n  ) %&gt;%\n  pivot_longer(precipitation:species_quantity) %&gt;%\n  ggplot(aes(y = value, fill = damaged)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(vars(name), scales = \"free\", ncol = 2) +\n  labs(x = NULL, y = NULL, fill = NULL)\n\nThe NAs (top row in each facet) aren’t 50/50 between the two levels of the target. The target is imbalanced and the NAs seem to be predictive of “no damage,” so they aren’t random.\nSince these NAs look predictive, you can turn them into a category by using step_unknown in the preprocessing recipe.",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-out",
    "href": "qmd/eda-general.html#sec-eda-gen-out",
    "title": "General",
    "section": "Outliers",
    "text": "Outliers\n\nAlso see Outliers\nAbnormalities due to likely data entry errors\n\nExample: store == “open” and sales == 0 or store == “closed” and sales &gt; 0\n\nPotential sol’n: replace 0’s (open) with mean sales and sales &gt;0 (closed) with 0s\n\n\nExtreme counts in charts when grouping by a cat var\n\nWhy is one category’s count so low or so high?\n\nMay need subject matter expert\n\nWhat can be done to increase or decrease that category’s count?\n\nFor prediction, experiment with keeping or removing outliers while fitting baseline models",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-grpcal",
    "href": "qmd/eda-general.html#sec-eda-gen-grpcal",
    "title": "General",
    "section": "Group Calculations",
    "text": "Group Calculations\n\nAlso see Feature Engineering, General &gt;&gt; Domain Specific\nVariance of Value by Group\n\nExample: how sales vary between store types over a year\nimportant to standardize the value by group\n\ngroup_by(group), mutate(sales = scale(sales))\n\nWhich vary wildly and which are more stable\n\nRates by Group\n\nExample: sales($) per customer\n\ngroup_by(group), mutate(sales_per_cust = sum(sales)/sum(customers)\n\n\nAvg by Group(s)\ndat %&gt;%\nselect(cat1, cat2, num) %&gt;%\ngroup_by(cat1, cat2) %&gt;%\nsummarize(freq = n(),\n          avg_cont = mean(num))",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-cont",
    "href": "qmd/eda-general.html#sec-eda-gen-cont",
    "title": "General",
    "section": "Continuous Variables",
    "text": "Continuous Variables\n\nDoes the variable have a wide range. (i.e. values across multiple magnitudes: 101 and 102 and … etc.)\n\nIf so, log the variable\n\nHistogram - Check shape of distribution\nggplot(aes(var)) +\n    geom_histogram()\n\nLooking at skew. Is it roughly normal?\nDoes filter(another_var &gt; certain_value (see below) help it look more normal?\nIs it multi-modal\n\nSee Regression, Other &gt;&gt; Multi-Modal(visuals, tests, modelling, etc.)\n{{gghdr}} - Visualization of Highest Density Regions in ggplot2\nInteractions &gt;&gt; Outcome: Categorical &gt;&gt; Binary Outcome (pct_event) vs Discrete by Discrete (or binary in this case)\n\nIs the variable highly skewed\n\nIf so, try:\n\nChanging units (min to hr),\nfilter(some_var &gt; some_value)\nsome combination of the above make more normal?\n\nNormality among predictors isn’t necessary, but I think it improves fit or prediction somewhat\n\nlog transformation may help some if the skew isn’t too extreme\n\n\n\nQ-Q plot to check fit against various distributions\n\n{ggplot}\nggplot(data)+\n    stat_qq(aes(sample = log_profit_rug_business))+\n    stat_qq_line(aes(sample = log_profit_rug_business))+\n    labs(title = 'log(profit) Normal QQ')\n\nA plot of the sample (or observed) quantiles of the given data against the theoretical (or expected) quantiles.\nSee article for the math and manual code\nstat_qq, stat_qq_line default distributions are Normal\nggplot::stat_qq docs have some good examples on how to use q-q plots to test your data against different distributions using MASS::fitdistr to get the distributional parameter estimates. Available distributions: “beta”, “cauchy”, “chi-squared”, “exponential”, “gamma”, “geometric”, “log-normal”, “lognormal”, “logistic”, “negative binomial”, “normal”, “Poisson”, “t” and “weibull”\n\n{dataexplorer}\n## View quantile-quantile plot of all continuous variables\nplot_qq(diamonds)\n\n## View quantile-quantile plot of all continuous variables by feature `cut`\nplot_qq(diamonds, by = \"cut\") \nSkewed Variables\n\nx &lt;- list()\nn &lt;- 300\nx[[1]] &lt;- rnorm(n)\nx[[2]] &lt;- exp(rnorm(n))\nx[[3]] &lt;- -exp(rnorm(n))\n\npar(mfrow = c(2,3), bty = \"l\", family = \"Roboto\")\n\nqqnorm(x[[1]], main = \"Normal\")\nqqnorm(x[[2]], main = \"Right-skewed\")\nqqnorm(x[[3]], main = \"Left-skewed\")\nlapply(x, function(x){plot(density(x), main = \"\")})\nGood fits\n\nnormal distribution\n\nBad fits\n\nUniform data tested against a normal distibution\n\nUniform data tested against an exponential distribution\n\n\n\n\nIs the mean/median above or below any important threshold?\n\ne.g. CDC considers a BMI &gt; 30 as obese. Health Insurance charges rise sharply at this threshold\n\nIs there an important threshold value?\n\n1 value –&gt; split into a binary\nMultiple values –&gt; Multinomial\n\nExamples\n\nBinary\n\nWhether a user spent more than $50 or didn’t (See Charts &gt;&gt; Categorical Predictors vs Outcome)\nIf user had activity on the weekend or not\n\nMultinomial\n\nTimestamp to morning/afternoon/ night,\nOrder values into buckets of $10–20, $20–30, $30+\n\n\n\nEmpirical Cumulative Density function (ecdf)\n\nggplot(aes(x = numeric_var, color = cat) +\n    stat_ecdf()\n\nShows the percentage of sample (y-axis) that are below a numeric_var value (x-axis)\n{sfsmisc::ecdf.ksCI} - plots the ecdf and 95% CIs (see Harrell for details of the CI calculation)\nCan view alongside a table of group means to see if the different percentiles differ from the story of just looking at the mean.\ndata %&gt;%\n    group_by(categorical_var) %&gt;%\n    summarize(mean(numeric_var))",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-cat",
    "href": "qmd/eda-general.html#sec-eda-gen-cat",
    "title": "General",
    "section": "Categorical/Discrete Variables",
    "text": "Categorical/Discrete Variables\n\nCount number of rows per category level (or use skimr or DataExplorer)\ntbl %&gt;% count(cat_var, sort = True)\nLooking for how skewed data might be (only a few categories have most of the obs)\nIf levels are imbalanced, consider: initial_split(data, strata = imbalanced_var)\nFor cat vars with levels with too few counts, consider lumping together\n\nLevels with too few data will have large uncertainties about the effect and the bloated std.devs can cause some models to throw errors\n\nCount NAs (or use skimr or DataExplorer)\ntbl %&gt;%\n  map_df(~ sum(is.na(.))) %&gt;%\n  gather(key = \"feature\", value = \"missing_count\") %&gt;%\n  arrange(desc(missing_count))\nVars with too many NAs, may need to be dropped or imputed\n\nSome models don’t handle NAs\n\nIf the number of NAs is within tolerance and you decide to impute, you need to find out what kind of “missingness” you have before you choose the imputation method. Some cause issues with certain types of missingness. (e.g. mean and missing-not-at-random (MNAR))\nYear variable\ndata |&gt;\n    count(year) |&gt;\n    arrange(desc(year)) |&gt;\n    ggplot(aes(year, n)) +\n    geom_line()\n\nLooking for skew.\nIs data older or more recent?\n\nFree Text Sometimes these columns are just metadata (a url, product description, etc.), but other times they could have valuable information (e.g. customer feedback). If a column seems like it contains valuable information for your prediction task, you generate features from it text length, appearance/frequency of certain keywords, etc.\n\nTokenize\n\nSee below code for “Facetted bar by variable with counts of the values” and the use of separate_rows to manually tokenize more useful when the columns don’t have stopwords\n\n\nVisualize value counts for multiple variables\n\nFacetted bar by variable with counts of the values\n\ncategorical_variables &lt;- board_games %&gt;%\n      # select all cat vars\n      select(game_id, name, family, category, artist, designer, mechanic) %&gt;%\n      # \"type\" receives all colnames; \"value\" receives their values\n      gather(type, value, -game_id, -name) %&gt;%\n      filter(!is.na(value)) %&gt;%\n      # Some values of vars are free text separated by commas; code makes each value into a separate row\n      separate_rows(value, sep = \",\") %&gt;%\n      arrange(game_id)\ncategorical_counts &lt;- categorical_variables %&gt;%\n      count(type, value, sort = TRUE)\n\ncategorical_counts %&gt;%\n      # type is gathered colnames of the variables\n      group_by(type) %&gt;%\n      # high cardinality variables, so only show top 10\n      top_n(10, n) %&gt;%\n      ungroup() %&gt;%\n      mutate(value = fct_reorder(value, n)) %&gt;%\n      ggplot(aes(value, n, fill = type)) +\n      geom_col(show.legend = FALSE) +\n      facet_wrap(~ type, scales = \"free_y\") +\n      coord_flip() +\n      labs(title = \"Most common categories\")\n\n“type” has the names of the variables, “value” has the levels of the variable",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-corr",
    "href": "qmd/eda-general.html#sec-eda-gen-corr",
    "title": "General",
    "section": "Correlation/Association",
    "text": "Correlation/Association\n\nMisc\n\nAlso see\n\nAssociation, General\nNotebook &gt;&gt; Statistical Inference &gt;&gt; Correlation\nInteractions &gt;&gt; Continuous Outcome &gt;&gt; Correlation Heatmaps\n\n{correlationfunnel} - Dancho’s package; bins numerics, then dummies all character and binned numerics, then runs a pearson correlation vs the outcome variable. Surprisingly it’s useful to use Pearson correlations for binary variables as long as you have a mix of 1s and 0s in each variable. (Cross-Validated post)\nchurn_df %&gt;%\n    binarize() %&gt;%\n    correlate(&lt;outcome_var&gt;) %&gt;%\n    plot_correlation_funnel()\n\ncorrelate returns a sorted? tibble in case you don’t want the plot\nThe funnel plot is a way of combining and ranking all the correlation plots into a less eye-taxing visual.\nUses stats::cor for calculation so you can pass args to it and but changing the method (e.g. method = c(\"pearson\", \"kendall\", \"spearman\") ) won’t matter, since pearson and spearman (and probably kendall) will be identical for binary variables.\n\nFor binary vs. binary, also see Association, General &gt;&gt; Discrete &gt;&gt; Binary Similarity Measures and Cramer’s V\n\nPairwise plots for patterns\n\nOutcome vs Predictor\nPredictor vs Predictor\n\nInteractions\nMulticollinearity\n\nCorrelation/Association scores for linear relationships\nHistograms for variations between categories\nExample: {{ggforce}}\n\nggplot(palmerpenguins::penguins, aes(x = .panel_x, y = .panel_y)) +\n  geom_point(aes(color = species), alpha = .5) +\n  geom_smooth(aes(color = species), method = \"lm\") +\n  ggforce::geom_autodensity(aes(color = species, fill = after_scale(color)), alpha = .7) +\n  scale_color_brewer(palette = \"Set2\", name = NULL) +\n  ggforce::facet_matrix(vars(names), layer.lower = 2, layer.diag = 3)\n\nLinear\n\n{greybox} for testing correlation between different types of variables\n\nMulticollinearity\n\nVIF (performance::check_collinearity(fit) or greybox::determ or vif(fit))\nUse PCA — if only a few (depends on the number of variables) pc explain all or almost all of the variation, then you could have a multicollinearity problem\n\nNonlinear\n\nScatterplots for non-linear patterns,\nCorrelation metrics\nAlso see General Additive Models &gt;&gt; Diagnostics for a method of determining a nonlinear relationship for either continuous or categorical outcomes.\n\nCategorical\n\n2-level x 2-level: Cramer’s V\n2-level or multi-level x multi-level\n\nChi-square or exact tests\n\nLevels vs Levels correlation\n\nMultiple Correspondence Analysis (MCA) (see bkmks &gt;&gt; Features &gt;&gt; Reduction)\n\nBinary outcome vs Numeric predictors\n# numeric vars should be in a long tbl. Use pivot longer to make two columns (e.g. metric (var names) value (value))\n\nnumeric_gathered %&gt;%\n  group_by(metric) %&gt;%\n  # rain_tomorrow is the outcome; event_level says which factor level is the event your measuring\n  roc_auc(rain_tomorrow, value, event_level = \"second\") %&gt;%\n  arrange(desc(.estimate)) %&gt;%\n  mutate(metric = fct_reorder(metric, .estimate)) %&gt;%\n  ggplot(aes(.estimate, metric)) +\n  geom_point() +\n  geom_vline(xintercept = .5) +\n  labs(x = \"AUC in positive direction\",\n      title = \"How predictive is each linear predictor by itself?\",\n      subtitle = \".5 is not predictive at all; &lt;.5 means negatively associated with rain, &gt;.5 means positively associated\")\n\n.5 is not predictive at all; &lt;.5 means negatively associated with rain, &gt;.5 means positively associated\n\n\nOrdinal\n\nPolychoric",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-contout",
    "href": "qmd/eda-general.html#sec-eda-gen-contout",
    "title": "General",
    "section": "Continuous Predictor vs Outcome",
    "text": "Continuous Predictor vs Outcome\n\nMisc\n\nIf the numeric-numeric relation isn’t linear, then the model will be misspecified: an influential variable may be overlooked or the assumption of linearity may produce a model that fails in important ways to represent the relationship.\nAlso see General Additive Models &gt;&gt; Diagnostics for a method of determining a nonlinear relationship for either continuous or categorical outcomes.\n\n\n\nContinuous Outcome\n\nContinuous vs Continuous by Continuous\n\ncoplot(lat ~ long | depth, data = quakes)\n\ncoplot is base R.\n\nExamples from Six not-so-basic base R functions\n\nThe six plots show the relationship of these two variables for different values of depth\nThe bar plot at the top indicates the range of depth values for each of the plots\nFrom lowest depth to highest depth, the default arrangement of the plots is from bottom row, left to right, and upwards\n\ne.g. The 4th lowest depth is on the top row, farthest to the left.\n\nrows = 1 would arrange all plots in 1 row.\noverlap = 0 will remove overlap between bins\n\nContinuous vs Continuous by Continuous by Continuous\n\ncoplot(lat ~ long | depth * mag, data = quakes, number = c(3, 4))\n\nShows the relationship with depth from left to right and the relationship with magnitude from top to bottom.\nnumber = c(3, 4) says you want 3 bins for depth and 4 bins for mag\nFrom lowest depth, mag to highest depth, mag, the arrangement of the plots is from bottom row, left to right, and upwards\n\ne.g. The 2nd lowest depth (columns) and 3rd lowest mag (rows) is in the 3rd from bottom row and 2nd column.\n\n\nContinuous vs Continuous by Categorical by Categorical\n\ncoplot(flowers ~ weight|nitrogen * treat, data = flowers,\n        panel = function(x, y, ...) {\n        points(x, y, ...)\n        abline(lm(y ~ x), col = \"blue\")})\n\nFrom An Introduction to R\nSame arrangement scheme as the plots above\n\ne.g. nitrogen = “medium” and treat = “tip” is the cell at middle column, top row\n\n\nScagnostics (paper) - metrics to examine numeric vs numeric relationships\n\n{scagnostics}\nScagnostics describe various measures of interest for pairs of variables, based on their appearance on a scatterplot. They are useful tool for discovering interesting or unusual scatterplots from a scatterplot matrix, without having to look at every individual plot\nMetrics: Outlying, Skewed, Clumpy, Sparse, Striated, Convex, Skinny, Stringy, Monotonic\n\n“Straight” (paper) seems to have been swapped for “Sparse” (package)\n\nPotential use cases\n\nFinding linear/nonlinear relationships\nClumping or clustered patterns could indicate an interaction with a categorical variable\n\nScore Guide\n\n\nHigh value: Red\nLow value: Blue\nCouldn’t find the ranges of these metrics in the paper or the package docs\nShows how scatterplot patterns correspond to metric values\n\n\n\n\n\nCategorical Outcome\n\nFor binary outcome, look for variation between numeric variables and each outcome level\n\n# numeric vars should be in a long tbl.\n# Use pivot longer to make two columns (e.g. metric (var names) value (value)) with the binary outcome (e.g rain_tomorrow) as a separate column\nnumeric_gathered %&gt;%\n  ggplot(aes(value, fill = rain_tomorrow)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ metric, scales = \"free\")\n# + scale_x_log10()\n\nSeparation between the two densities would indicate predictive value.\nIf one of colored density is further to the right than the other then the interpretation would be:\n\nHigher values of metric result in a greater probability of &lt;outcome category of the right-most density&gt;\n\nNormalize the x-axis with rank_percentile(value)\n\nnumeric_gathered %&gt;%\n    mutate(rank = percent_rank(value)) %&gt;%\n    ggplot(aes(rank, fill = churned)) + \n      geom_density(alpha = 0.5) + \n      facet_wrap(~ metric, scales = \"free\")\n\nNot sure why you’d do this unless there was a reason to compare the separation of densities (i.e. strength of association with outcome) between the predictors.\n\n\nEstimated AUC for binary outcome ~ numeric predictor\nnumeric_gathered &lt;- train %&gt;%\n  mutate(rainfall = log2(rainfall + 1)) %&gt;%\n  gather(metric, value, min_temp, max_temp, rainfall, contains(\"speed\"), contains(\"humidity\"), contains(\"pressure\"), contains(\"cloud\"),        contains(\"temp\"))\n\nnumeric_gathered %&gt;%\n  group_by(metric) %&gt;%\n  # \"rain_tomorrow\" is a binary factor var\n  # \"second\" says the event we want the probability for is the second level of the binary factor variable\n  yardstick::roc_auc(rain_tomorrow, value, event_level = \"second\") %&gt;%\n  arrange(desc(.estimate)) %&gt;%\n  mutate(metric = fct_reorder(metric, .estimate)) %&gt;%\n  ggplot(aes(.estimate, metric)) +\n  geom_point() +\n  geom_vline(xintercept = .5) +\n  labs(x = \"AUC in positive direction\",\n      title = \"How predictive is each linear predictor by itself?\",\n      subtitle = \".5 is not predictive at all; &lt;.5 means negatively associated with rain, &gt;.5 means positively associated\")",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-catout",
    "href": "qmd/eda-general.html#sec-eda-gen-catout",
    "title": "General",
    "section": "Categorical Predictor vs Outcome",
    "text": "Categorical Predictor vs Outcome\n\nContinuous Outcome\n\nBoxplot by Categorical\n\nfct_reorder  says order cat_var by a num_var\n\nMake sure data is NOT grouped\n\ndata %&gt;%\n    mutate(cat_var = fct_reorder(cat_var, numeric_outcome)) %&gt;%\n    ggplot(aes(numeric_outcome, cat_var)) +\n    geom_boxplot()\n\nIf all the medians line up then no relationship. A slope or nonlinear shows relationship.\n\nfct_lump  can be used to create an “other” group.\n\ndata %&gt;%\n    mutate(cat_var = fct_lump(cat_var, 8),\n          cat_var = fct_reorder(cat_var, numeric_outcome)) %&gt;%\n    ggplot(aes(numeric_outcome, cat_var)) +\n    geom_boxplot()\n\nUseful for cat_vars with too many levels which can muck-up a graph\nSays to keep the top 8 levels with the highest counts and put rest in “other”.\n\nAlso takes proportions. Negative values says keep lowest.\n\n\n\nBoxplot by Categorical (Titanic5 dataset)\n\n\nY-Axis is the “Class” categorical with 3 levels\nFor ticket price, only class 1 shows any variation\nFor Age, there’s a clear trend but also considerable overlap between classes\n\n\n\n\nCategorical Outcome\n\nHistograms of cat_vars split by response_var \ndf %&gt;%\n    select(cat_vars) %&gt;%\n    pivot_longer(key, value = cat_vars, response_var) %&gt;%\n    ggplot(aes(value)) +\n    geom_bar(fill = response_var) +\n    facet_wrap( ~key, scales = \"free\")\n\nJust looking for variation in the levels of the cat_var given response var. More variation = more likely to be a better predictor\nEach facet will be a level of the response variable\n\nError Bar Plot\n# outcome variable is a binary for whether or not it rained on that day\ngroup_binary_prop &lt;- function(tbl) {\n    ret &lt;- tbl %&gt;%\n        # count of events for each category (successes)\n        summarize(n_rain = sum(rain_tomorrow == \"Rained\"),\n                  # count of rows for each category (trials)\n                  n = n()) %&gt;%\n        arrange(desc(n)) %&gt;%\n        ungroup() %&gt;%\n        # probability of event for each category\n        mutate(pct_rain = n_rain / n,\n              # jeffreys interval\n              # bayesian CI for binomial proportions\n              low = qbeta(.025, n_rain + .5, n - n_rain + .5),\n              high = qbeta(.975, n_rain + .5, n - n_rain + .5)) %&gt;%\n        # proportion of all events for each category\n        mutate(pct = n_rain / sum(n_rain))\n        # this was the original but this would just be proportion of the total data for each caategory\n        # mutate(pct = n / sum(n))\n    ret\n}\n\n# error bar plot\n# cat vs probability of event w/CIs\ntrain %&gt;%\n    # cat predictor\n    group_by(location = fct_lump(location, 50)) %&gt;%\n    # apply custom function\n    group_binary_prop() %&gt;%\n    mutate(location = fct_reorder(location, pct_rain)) %&gt;%\n    ggplot(aes(pct_rain, location)) +\n    geom_point(aes(size = pct)) +\n    geom_errorbarh(aes(xmin = low, xmax = high), height = .3) +\n    scale_size_continuous(labels = percent, guide = \"none\", range = c(.5, 4)) +\n    scale_x_continuous(labels = percent) +\n    labs(x = \"Probability of raining tomorrow\",\n      y = \"\",\n      title = \"What locations get the most/least rain?\",\n      subtitle = \"Including 95% confidence intervals. Size of points is proportional to frequency\")\n\nBinary Outcome: Group by cat predictors and calculate proportion of event\nThis needs some tidyeval so it can generalize to other binary(?) outcome vars\n\nSimpler (uncommented) version\nsummarize_churn &lt;- function(tbl) {\n    tbl %&gt;%\n        summarize(n = n(),\n                  n_churned = sum(churned == \"yes\"),\n                  pct_churned = n_churned/n,\n                  low = qbeta(.025, n_churned + .5, n - n_churned + .5), \n                  high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\n        arrange(desc(n))\n}\n\nplot_categorical &lt;- function(tbl, categorical, ...) {\n    tbl %&gt;%       \n        ggplot(aes(pct_churned, cat_pred), ...) +\n        geom_col() +\n        geom_errorbar(aes(xmin = low, xmax = high), height = 0.2, color = red) +\n        scale_x_continuous(labels = percent) +\n        labs(x = \"% in category that churned\")\n}\n\ndata %&gt;%\n    group_by(cat_var) %&gt;%\n    summarize_churn() %&gt;%\n    plot_categorical(cat_var)\nBinary Outcome vs Two Binned Continuous\n\nsummarize_churn &lt;- function(tbl) {\n    tbl %&gt;%\n        summarize(n = n(),\n                  n_churned = sum(churned == \"yes\"),\n                  pct_churned = n_churned/n,\n                  low = qbeta(.025, n_churned + .5, n - n_churned + .5), \n                  high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\n        arrange(desc(n))\n}\n\ndata %&gt;%\n    mutate(avg_trans_amt = total_trans_amt / total_trans_ct,\n          total_transactions = ifelse(total_trans_ct &gt;= 50,\n                                        \"&gt; 50 Transactions\",\n                                        \"&lt; 50 Transactions\"),\n          avg_transaction = ifelse(avg_trans_amt &gt;= 50,\n                                      \"&gt; $50 Average\",\n                                      \"&lt; $50 Average\")\n    ) %&gt;%\n    group_by(total_transactions,avg_transaction) %&gt;%\n    summarize_churn() %&gt;%\n    ggplot(aes(total_transactions, avg_transaction)) +\n    geom_tile(aes(fill = pct_churned)) +\n    geom_text(aes(label = percent(pct_churned, 1))) +\n    scale_fill_gradient2(low = \"blue\", high = \"red\", midpoint = 0.3) +\n    labs(x = \"How many transactions did the customer do?\",\n    y = \"What was the average transaction size?\",\n    fill = \"% churned\",\n    title = \"Dividing customers into segments\")\n\nSegmentation chart\nEach customer’s spend is averaged and binned (&gt; or &lt; $50)\nEach customer’s transaction count is binned (&gt; or &lt; 50)\nThe df is grouped by both binned vars, so you get 4 subgroups\n\nProportions of each subgroup that falls into the event category of then binary variable (e.g. churn) are calculated\nLow and high quantiles for churn counts are calculated (typical calc of CIs for the proportions of binary variables)\n\nUsed to add context of whether these are high proportions, low proportions, etc.",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-inter",
    "href": "qmd/eda-general.html#sec-eda-gen-inter",
    "title": "General",
    "section": "Interactions",
    "text": "Interactions\n\nMisc\n\nY-Axis is the response, X-Axis is the explanatory variable of interest, and the Grouping Variable is the moderator\nInterpretation\n\nSignificant Interactions - The lines of the graph cross or sometimes if they converge (if there’s enough data/power)\n\nThis pattern is a visual indication that the effects of one IV change as the second IV is varied.\nIf either line has a non-linear pattern (e.g. U-Shaped), yet still cross, it may indicate a non-linear interaction\n\nNon-Significant Interactions - Lines that are close to parallel.\n\nAlso see\n\nRegression, Interactions for details\nDiagnostics, Model Agnostic &gt;&gt; DALEX &gt;&gt; Instance Level &gt;&gt; Break-Down &gt;&gt; Example: Assume Interactions\n\nTypical Format: outcome_mean vs pred_var by pred_var\ndata %&gt;% \n  group_by(pred1, pred2) %&gt;% \n  summarize(out_mean = mean(outcome)) %&gt;% \n  ggplot(aes(y = out_mean, x = pred1, color = pred2)+\n    geom_point() +\n    geom_line()\n\nMay also need a “group = pred2” in the aes function\n\n\n\n\nContinuous Outcome\n\nContinuous vs Continuous, Scatter with Smoother by a Categorical\n\nggplot(w, aes(x=age, y=price, color=factor(class))) +\n  geom_point() +\n  geom_smooth() +\n  scale_y_continuous(trans='sqrt') +\n  guides(color=guide_legend(title='Class')) +\n  hlabs(age, price)\n\nContinuous outcome has been transformed so that the lower values can be more visible\n“Class” == 1 ⨯ Age shows some variation but the other two classes do not seem to show much. Lookng at the scatter of red dots, I’m skeptical that variation being shown by the curve.\n\nAlthough the decent separation of the “Class” groups may be what indicates an informative interaction\n\n\nContinuous vs Binary by Binary\n\n\nSignificant interaction effect (crossing)\n\nVariable A had no significant effect on participants in Condition B1 but caused a decline from A1 to A2 for those in Condition B2\n\n\nContinuous vs Continuous by Categorical\n\nplot_manufacturer &lt;- function(group) {\n\n  ## check if input is valid\n  if (!group %in% mpg$manufacturer) stop(\"Manufacturer not listed in the data set.\")\n\n  ggplot(mapping = aes(x = hwy, y = displ)) +\n    ## filter for manufacturer of interest\n    geom_point(data = filter(mpg, manufacturer %in% group), \n               color = \"#007cb1\", alpha = .5, size = 4) +\n    ## add shaded points for other data\n    geom_point(data = filter(mpg, !manufacturer %in% group), \n               shape = 1, color = \"grey45\", size = 2) +\n    scale_x_continuous(breaks = 2:8*5) +\n    ## add title automatically based on subset choice\n    labs(x = \"Highway gallons\", y = \"Displacement\", \n         title = group, color = NULL)\n}\n\ngroups &lt;- unique(mpg$manufacturer)\nmap(groups, ~plot_manufacturer(group = .x))\n\nThe grouping variable is the facet variable but also highlights the dots with color\nHighlighting plus using all the data in each chart helps add context with the other groups when you want to compare groups but in a low data situation.\n\nContinuous vs Continuous by Ordinal\n\nplot_scatter_lm &lt;- function(data, var1, var2, pointsize = 2, transparency = .5, color = \"\") {\n\n  ## check if inputs are valid\n  if (!is.data.frame(data)) stop(\"data needs to be a data frame.\")\n  if (!is.numeric(pull(data[var1]))) stop(\"Column var1 needs to be of type numeric, passed as string.\")\n  if (!is.numeric(pull(data[var2]))) stop(\"Column var2 needs to be of type numeric, passed as string.\")\n  if (!is.numeric(pointsize)) stop(\"pointsize needs to be of type numeric.\")\n  if (!is.numeric(transparency)) stop(\"transparency needs to be of type numeric.\")\n  if (color != \"\") { if (!color %in% names(data)) stop(\"Column color needs to be a column of data, passed as string.\") }\n\n  g &lt;- \n    ggplot(data, aes(x = !!sym(var1), y = !!sym(var2))) +\n    geom_point(aes(color = !!sym(color)), size = pointsize, alpha = transparency) +\n    geom_smooth(aes(color = !!sym(color), color = after_scale(prismatic::clr_darken(color, .3))), \n                method = \"lm\", se = FALSE) +\n    theme_minimal(base_family = \"Roboto Condensed\", base_size = 15) +\n    theme(panel.grid.minor = element_blank(),\n          legend.position = \"top\")\n\n  if (color != \"\") { \n    if (is.numeric(pull(data[color]))) {\n      g &lt;- g + scale_color_viridis_c(direction = -1, end = .85) +\n        guides(color = guide_colorbar(\n          barwidth = unit(12, \"lines\"), barheight = unit(.6, \"lines\"), title.position = \"top\"\n        ))\n    } else {\n      g &lt;- g + scale_color_brewer(palette = \"Set2\")\n    }\n  }\n\n  return(g)\n}\n\nmap2(\n  c(\"displ\", \"displ\", \"hwy\"), \n  c(\"hwy\", \"cty\", \"cty\"),\n  ~plot_scatter_lm(\n    data = mpg, var1 = .x, var2 = .y, \n    color = \"cyl\", pointsize = 3.5\n  )\n)\n\nA continuous color scale is used for the ordinal variable\nTrend shows relationship follows the ordinal variable values for the most part which might indicate that this interaction would be predictive\n\nInteresting values might be at dots where the colors are swapped — defying the order of the ordinal variable\n\n\nContinuous vs Continuous by Categorical by Categorical\n\nplot_manufacturer_marginal &lt;- function(group, save = FALSE) {\n\n  ## check if input is valid\n  if (!group %in% mpg$manufacturer) stop(\"Manufacturer not listed in the data set.\")\n  if (!is.logical(save)) stop(\"save should be either TRUE or FALSE.\")\n\n  ## filter data\n  data &lt;- filter(mpg, manufacturer %in% group)\n\n  ## set limits\n  lims_x &lt;- range(mpg$hwy) \n  lims_y &lt;- range(mpg$displ)\n\n  ## define colors\n  pal &lt;- RColorBrewer::brewer.pal(n = n_distinct(mpg$class), name = \"Dark2\")\n  names(pal) &lt;- unique(mpg$class)\n\n  ## scatter plot\n  main &lt;- ggplot(data, aes(x = hwy, y = displ, color = class)) +\n    geom_point(size = 3, alpha = .5) +\n    scale_x_continuous(limits = lims_x, breaks = 2:8*5) +\n    scale_y_continuous(limits = lims_y) +\n    scale_color_manual(values = pal, name = NULL) +\n    labs(x = \"Highway miles per gallon\", y = \"Displacement\") +\n    theme(legend.position = \"bottom\")\n\n  ## boxplots\n  right &lt;- ggplot(data, aes(x = manufacturer, y = displ)) +\n    geom_boxplot(linewidth = .7, color = \"grey45\") +\n    scale_y_continuous(limits = lims_y, guide = \"none\", name = NULL) +\n    scale_x_discrete(guide = \"none\", name = NULL) +\n    theme_void()\n\n  top &lt;- ggplot(data, aes(x = hwy, y = manufacturer)) +\n    geom_boxplot(linewidth = .7, color = \"grey45\") +\n    scale_x_continuous(limits = lims_x, guide = \"none\", name = NULL) +\n    scale_y_discrete(guide = \"none\", name = NULL) +\n    theme_void()\n\n  ## combine plots\n  p &lt;- top + plot_spacer() + main + right + \n    plot_annotation(title = group) + \n    plot_layout(widths = c(1, .05), heights = c(.1, 1))\n\n  ## save multi-panel plot\n  if (isTRUE(save)) {\n    ggsave(p, filename = paste0(group, \".pdf\"), \n           width = 6, height = 6, device = cairo_pdf)\n  }\n\n  return(p)\n}\n\nplot_manufacturer_marginal(\"Dodge\")\n\n{ggside} should be able to add these marginal plots with fewer lines of code.\nThis is one of a set of facetted charts by the categorical, “manufacturer”\nDots are grouped by categorical, “class”\nTop boxplot shows a minivan as an outlier in terms of hwy mpg.\nBox plots and the scatter plot are combined using {patchwork}\n\nCorrelation Heatmaps\n\nFilter data by different levels of a categorical, then note how correlations between numeric predictors and the numeric outcome change\nExample: PM 2.5 pollution (outcome) vs complete dataset and filtered for Wind Direction = NE\n\nComplete\n\nWind Direction = NE\n\nInterpretation\n\nTemperature’s correlation (potentially its predictive strength) would lessen if would be interacted with Wind Direction. So we do NOT want to interact wind direction and temperature\n\nArticle didn’t show whether it increases with other directions\n\nWind Strength’s (cws) correlation with the outcome would increase if interacted with Wind Direction. So we do want to interacted wind direction and wind strength\n\nFor ML, I think you’d dummy the wind direction, then multiply windspeed times each of the dummies.\n\n\n\n\nBoxplot by Discrete (Binned) Continuous\n\npmin can be similarily used as fct_lump (see below) but for discrete integer variables\n\nIf the distribution of the discrete numeric is skewed to the right, then pmin will bin all integers larger than some number\n\nMost of the distribution are small integers and the rest will be binned into a sort of “other” category (e.g. 14)\n\nIf the distribution is skewed to the left, pmax can be used similarily.\n\ndata %&gt;%\n   mutate(integer_var = pmin(integer_var, 14) %&gt;%\n   ggplot(aes(int_var, numeric_outcome, group = int_var)) +\n   geom_boxplot()\n\nIf all the medians line up then no relationship. A slope or nonlinear pattern shows relationship.\n\n\n\n\n\n\nCategorical Outcome\n\nNumeric vs Numeric by Cat Outcome\n\nScatter with 45 degree line\nggplot(aes(num_predictor1, num_predictor2, color = cat_outcome_var)) +\n   geom_point() +\n   geom_abline(color = \"red\")\n\nLook for groupings or other patterns wrt to cat var.\nCat-var colored points above line skew more towards the higher y-var than x-var and vice versa for below the 45 degree line.\nLine also shows how linearly correlated the two num vars are.\nIf clustering present, could indicate a good interaction pair with the numeric : cat_var\n\nScatter with linear smooth (or loess)\n\nggplot(aes(num_predictor1, num_predictor2)) +\n   geom_point(alpha = 0.25) +\n   geom_smooth(aes(color = cat_outcome_var), method = \"lm\")\n\nProduces a lm line for each outcome var category\nLooking for differing trends for ranges of values on the x-axis. A pattern for one line that is substantially different from the other line\nExample: At around 28, the blue line trend rises while the red line continues to slope downwards, and they actually cross to where at some threshold of x, the relationship is the opposite. So an interaction is likely present\n\n\nBinary Outcome (pct_event) vs Discrete by Discrete (or binary in this case)\n\ndata %&gt;%\n    mutate(avg_trans_amt = total_trans_amt / total_trans_ct) %?%\n    group_by(total_trans_ct = cut(total_trans_ct, c(0,30, 40, 50, 60, 80, Inf)),\n            avg_trans_amt = ifelse(avg_trans_amt &gt;= 50, \"&gt; $50\", \"&lt; $50\") %&gt;%\n            # use to figure out best cut point(s) that keeps the ribbon width small-ish on all lines\n            # avg_trans_amt = cut(avg_trans_amt, c(0, 50, 100, 130, Inf)) %&gt;%   \n    summarize(n = n(),\n              n_churned = sum(churned == \"yes\"),\n              pct_churned = n_churned/n,\n              low = qbeta(.025, n_churned + .5, n - n_churned + .5), \n              high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\n        arrange(desc(n)) %&gt;%\n    ggplot(aes(total_trans_ct, pct_churned, color = avg_trans_amt) +\n    geom_point() +\n    geom_line() +\n    geom_ribbon(aes(ymin = low, ymax = high))           \n\nInterpretation:\n\nClear alternating trend from about 0 to 40 on the x-axis says there’s probably an interaction (at least with the binned versions of these variables) between total_trans_ct and avg_trans_amt.\n\ni.e. The relationship between transaction count and churned (binary outcome) (pct_churned) depends on the average transaction amount\n\n\nExample: The cut points for avg_trans_amt were chosen from its distribution\n\nThe distribution was bi-modal and the 3 cutpoints were the 1st mode, point that splits both modal distributions, and the 2nd mode.\n{Upsetr} might be useful to examine bimodal structure and determine cutpoints based on categorical predictor values and not just outcome values\n{gghdr} - viz for multi-modal distribtutions\nAlso see Regression, Other &gt;&gt; Mult-Modal\n\nExample of likely no interaction\n\n\nBlue and red lines move in unison. Same trend directions.\n\nThere is separation, so the mean value of percent churn is different. Also, the slopes are different, so the rates of increase and decrease would be different. I’m not convinced. I’d like to see if an interaction term wouldn’t be significant\nkaggle sliced s01e07 dataset - percent churn (y-axis), revolving balance bucketed (x-axis), color = total_transactions dicotomized. DRob video for the code.\n\n\n\nBinary Outcome (pct_event) vs Categorical by Categorical\n\nSliding Window Continuous vs Binary Outcome (Proportion of Event) by Categorical\n\nggplot(z, aes(x=price, y=`Moving Proportion`, col=factor(class))) +\n  geom_line() + guides(color=guide_legend(title='Class')) +\n  xlab(hlab(price)) + ylab('Survival')\n\n“Moving Proportion” is the mean of the binary outcome (probability of an event) over a sliding window of “Total Price”\n“Total Price” should be sorted in ascending order and grouped by “Class” before the sliding window is applied\nHarrell uses a default window of 15 observations on either side of the target point, but says the results can be noisy. Recommends passing the results through a smoother\n\nSo, might want to add a geom_smooth to the code chunk\nI might like to see the data points to see how many points at the ends of lines there are. Smoothed lines can be misleading on the boundaries.\n\n\nSliding Window Continuous vs Binary Outcome (Proportion of Event) by 2 Categoricals\n\nggplot(d, aes(x=age, y=`Moving Proportion`, col=factor(class))) +\n  geom_smooth() +\n  facet_wrap(~ sex) +\n  ylim(0, 1) + xlab(hlab(age)) + ylab('Survival') +\n  guides(color=guide_legend(title='Class'))\n\nSimilar to above but grouped by 2 variables before the sliding window calculation.\n\nGrouped Bar\n\nsummarize_churn &lt;- function(tbl) {\n    tbl %&gt;%\n        summarize(n = n(),\n                  n_churned = sum(churned == \"yes\"),\n                  pct_churned = n_churned/n,\n                # Jeffrey's Interval (Bayesian CI)\n                  low = qbeta(.025, n_churned + .5, n - n_churned + .5), \n                  high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\n        arrange(desc(n))\n}\nplot_categorical &lt;- function(tbl, categorical, ...) {\n    tbl %&gt;%       \n        ggplot(aes(pct_churned, [{{categorical}}]{style='color: goldenrod'}), ...) + \n        geom_col(position = position_dodge()) + \n        geom_errorbar(aes(xmin = low, xmax = high),\n                      height = 0.2, color = red,\n                      position = position_dodge(width = 1) +\n        scale_x_continuous(labels = percent) +\n        labs(x = \"% in category that churned\")\n}\ndata %&gt;%\n    group_by(cat_var1, cat_var2) %&gt;%\n    summarize_churn() %&gt;%\n    plot_categorical(cat_var1, fill = cat_var2, group = cat_var2)\n\nInterpretation: Probably not an interaction variable. Pct Churned by education Level doesn’t vary (much) by  Gender especially if you take the error bars into account\n\nOnly for “college” do you see a flip in the relationship where females churn more than men, but it’s still within the error bars.\n\n\n\nBinary Outcome vs Binary by Categorical\n\n\nNot certain but I’d think you’d want your outcome on the x-axis. Although, if you swapped the x-axis variable with the grouping variable, you’d probably come to the same conclusion. Therefore, it may not matter that much\nShows percent, and not counts",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/css-general.html",
    "href": "qmd/css-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CSS",
      "General"
    ]
  },
  {
    "objectID": "qmd/css-general.html#sec-css-gen-misc",
    "href": "qmd/css-general.html#sec-css-gen-misc",
    "title": "General",
    "section": "",
    "text": "Resources\n\nhttps://css-tip.com/\nWidget testing parameter values for css styling a div box\n\nColumn Widths in CSS Grid\n\nCSS comment - /* comment */\nSelector formats\n\nSyntax: #&lt;class&gt;.&lt;id&gt;&lt;additional-stuff&gt;\nExample:\n\nCSS\n#header.fluid-row::before{\n}\nHTML\n&lt;div class=\"fluid-row\" id=\"header\"&gt; == $0\n::before\n&lt;/div&gt;\n\n\nInclude css styling directly into a html page\n\nExample: Via HTML style tag\n&lt;style&gt;\nbody {\n  padding: 50px 25px 0px 25px;\n  font-family: 'Roboto', sans-serif;\n  font-size: 19px;\n}\n&lt;/style&gt;\nExample: Via R chunk\nhtmltools::tags\\$link(href = \"https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght\\@0,400;0,700;1,400&display=swap\",\n                      rel = \"stylesheet\")\nExample: styling of a legend html div\n&lt;style type='text/css'&gt;\n  .my-legend .legend-title {\n    text-align: left;\n    margin-bottom: 8px;\n    font-weight: bold;\n    font-size: 90%;\n    }\n  .my-legend .legend-scale ul {\n    margin: 0;\n    padding: 0;\n    float: left;\n    list-style: none;\n    }\n  .my-legend .legend-scale ul li {\n    display: block;\n    float: left;\n    width: 50px;\n    margin-bottom: 6px;\n    text-align: center;\n    font-size: 80%;\n    list-style: none;\n    }\n  .my-legend ul.legend-labels li span {\n    display: block;\n    float: left;\n    height: 15px;\n    width: 50px;\n    }\n  .my-legend .legend-source {\n    font-size: 70%;\n    color: #999;\n    clear: both;\n    }\n  .my-legend a {\n    color: #777;\n    }\n&lt;/style&gt;\n\nSee link for details on the legend div element that uses this CSS",
    "crumbs": [
      "CSS",
      "General"
    ]
  },
  {
    "objectID": "qmd/css-general.html#centering",
    "href": "qmd/css-general.html#centering",
    "title": "General",
    "section": "Centering",
    "text": "Centering\n\nThere are also instructions for placing elements in different positions (e.g. right edge)\nNotes from How To Center a Div\n\nThere’s also code/explainer for centering elements (e.g. images) that have to stacked on top of each other\n\n\n\nElements\n\nCenter Horizontally with auto-margins\n.element {\n  max-width: fit-content;\n  margin-left: auto;\n  margin-right: auto;\n  /* margin-inline: auto*/\n}\n\nUse when you want to horizontally center a single element without disturbing any of its siblings\nmax-width is used because if width is used instead, it would lock it to that size, and the element would overflow when the container is really narrow.\nIncluding only margin-left: auto will force the div flush with the right side and vice verse with margin-right\nmargin-inline: auto can replace both margin-left and margin-right to center the div\n\nCentering Vertically and Horizontally\n.container {\n  align-content: center;\n}\n.element {\n  max-width: fit-content;\n  margin-inline: auto;\n}\nCenter Vertically and Horizontally with Flexbox\n/* single element */\n.container {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n}\n/* multiple elements */\n.container {\n  display: flex;\n  flex-direction: row;\n  justify-content: center;\n  align-items: center;\n  gap: 4px;\n}\n\nThe most versatile method; it can be used to center one or multiple children, horizontally and/or vertically, whether they’re contained or overflowing.\nflex-direction controls the direction in which the items are aligned, and it can have other values: column, row-reverse, column-reverse\n\nText\ncontainer {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n  text-align: center;\n}\n\nBlocks of text can be treated as one element and can be centered using the previous methods. This code (text-align) is for centering the rows of text within a element’s block.\n\n\n\n\nViewports\n\nUseful for elements like dialogs, prompts, and GDPR banners need to be centered within the viewport. (Think pop-ups)\nCentering With Known Sizes\n.element {\n  position: fixed;\n  inset: 0px;\n  width: 12rem;\n  height: 5rem;\n  max-width: 100vw;\n  max-height: 100dvh;\n  margin: auto;\n}\n\nComplex and has more settings that depend on the element. See article for details but there are four main concepts:\n\nFixed positioning\nAnchoring to all 4 edges with inset: 0px\nConstrained width and height\nAuto margins\n\nOmitting top: 0px will anchor the element to the bottom\n\nUse calc with max-width to make sure theres a buffer around the element\nmax-width: calc(\n    100vw - 8px * 2\n  );\n\n\nCentering Elements With Unknown Sizes\n.element {\n  position: fixed;\n  inset: 0;\n  width: fit-content;\n  height: fit-content;\n  margin: auto;\n}\n\nfit-content is doing the work",
    "crumbs": [
      "CSS",
      "General"
    ]
  },
  {
    "objectID": "qmd/code-optimization.html",
    "href": "qmd/code-optimization.html",
    "title": "Optimization",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Code",
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/code-optimization.html#sec-code-opt-misc",
    "href": "qmd/code-optimization.html#sec-code-opt-misc",
    "title": "Optimization",
    "section": "",
    "text": "A while loop is faster than a recursive function\nungroup before performing calculations in mutate or summarize when that calculation doesn’t need to be performed within-group (i.e. per factor level)\nString functions\n\n“fixed” searches, fixed = TRUE are fastest overall\n\nSearches involving fixed strings (e.g. “banana”) that don’t require regular expressions\n\nPCRE2, perl = TRUE , is fastest for regular expressions\n\nAdding Rows to a Matrix (link, link)\n\nrbind is very slow and cbind + t() isn’t much better\nfrbind &lt;- function(n) {\n    res &lt;- vector()\n    for (i in 1:n) {\n        res &lt;- rbind(res, runif(n))\n    }\n    res\n}\nfcbind &lt;- function(n) {\n    res &lt;- vector()\n    for (i in 1:n) {\n        res &lt;- cbind(res, runif(n))\n    }\n    t(res)\n}\nAdd rows to a list then combine into a matrix\n# grow list which is converted to a matrix at the end.\nflist &lt;- function(n) {\n    res &lt;- list()\n    for (i in 1:n) {\n        res[[length(res) + 1]] &lt;- runif(n)\n    }\n    do.call(rbind, res)\n}\nflist2 &lt;- function(n) {\n    res &lt;- list()\n    for (i in 1:n) {\n        res[[length(res) + 1]] &lt;- runif(n)\n    }\n    len &lt;- length(res)\n    res &lt;- res |&gt; purrr::list_c()\n    dim(res) &lt;- c(len, len)\n    res\n}\n\nlist_c uses C under the hood and might be faster on larger matrices.",
    "crumbs": [
      "Code",
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/code-optimization.html#sec-code-opt-bench",
    "href": "qmd/code-optimization.html#sec-code-opt-bench",
    "title": "Optimization",
    "section": "Benchmarking",
    "text": "Benchmarking\n\nMisc\n\nMastering Software Development in R, Ch. 2.71\n\n{bench}\n\nMisc\n\nAutomatically checks that each approach gives the same output, so that you don’t mistakenly compare apples and oranges\n\nExample: Basic\nres &lt;-\n  bench::mark(\n    approach_1 = Reduce(sum, numbers),\n    approach_2 = sum(unlist(numbers))\n  )\n\nres %&gt;% select(expression, median)\n\n#&gt; # A tibble: 2 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 approach_1  2.25µs\n#&gt; 2 approach_2 491.97ns\n\n{microbenchmark}\n\nExample: Basic\n\nrecord_temp_perf &lt;- microbenchmark(find_records_1(example_data, 27), \n                                  find_records_2(example_data, 27))\nrecord_temp_perf\n\n## Unit: microseconds\n##                              expr      min      lq      mean  median      uq\n##  find_records_1(example_data, 27)  114.574  136.680  156.1812  146.132  163.676\n##  find_records_2(example_data, 27) 4717.407 5271.877 6113.5087 5867.701 6167.709\n##        max neval\n##    593.461  100\n##  11334.064  100\n\nlibrary(ggplot2)\nautoplot(record_temp_perf)\n\nDefault: 100 iterations\nTimes are given in a reasonable unit, based on the observed profiling times (units are given in microseconds in this case).\nOutput\n\nmin - min time\nlq - lower quartile\nmean, median\nuq - upper quartile\nmax - max time",
    "crumbs": [
      "Code",
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/code-optimization.html#sec-code-opt-prof",
    "href": "qmd/code-optimization.html#sec-code-opt-prof",
    "title": "Optimization",
    "section": "Profiling",
    "text": "Profiling\n\nMisc\n\nResources\n\nMastering Software Development in R, Ch. 2.72\nAdvanced R, Ch. 23\n\n\n{profvis}\n\nExample: Basic\n\nstep_1 &lt;- function() {\n  pause(1)\n}\nstep_2 &lt;- function() {\n  pause(2)\n}\nslow_function &lt;- function() {\n  step_1()\n\n  step_2()\n\n  TRUE\n}\nresult &lt;- profvis(slow_function())\nresult\n\nBottom Row: outermost function\n2nd Row from the bottom are the functions in the next enviromental layer (e.g. “step_1” and “step_2”)\n\n“step_2” takes about 2/3 of the total function execution time or twice the execution time of “step_1”\n\n3rd Row from the bottom (top row) are the functions in each of those other functions\n\n“pause” in “step_2” takes about 2/3 of the total function execution time or twice the execution time of “pause” in “step_1”",
    "crumbs": [
      "Code",
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/code-optimization.html#sec-code-opt-py",
    "href": "qmd/code-optimization.html#sec-code-opt-py",
    "title": "Optimization",
    "section": "Python",
    "text": "Python\n\nBenchmarking\n\n{{time}}(built-in module)\nimport time\nstart = time.perf_counter()\ntime.sleep(1) # do work\nelapsed = time.perf_counter() - start\nprint(f'Time {elapsed:0.4}')\n#&gt; Time 1.001\n\nMemory Usage\n\n{{memory-profiler}}\n\nBasic usage for a function\nfrom memory_profiler import memory_usage\nmem, retval = memory_usage((fn, args, kwargs), retval=True, interval=1e-7)\n\ninterval: For very quick operations the function fn might be executed more than once. By setting interval to a value lower than 1e-6, we force it to execute only once.\nretval: Tells the function to return the result of fn.\n\nNon-interactive usage\n$ python -m memory_profiler example.py\n#&gt; Line #    Mem usage  Increment   Line Contents\n#&gt; ==============================================\n#&gt;      3                           @profile\n#&gt;      4      5.97 MB    0.00 MB   def my_func():\n#&gt;      5     13.61 MB    7.64 MB       a = [1] * (10 ** 6)\n#&gt;      6    166.20 MB  152.59 MB       b = [2] * (2 * 10 ** 7)\n#&gt;      7     13.61 MB -152.59 MB       del b\n#&gt;      8     13.61 MB    0.00 MB       return a\n\n\nProfile decorator\nimport time\nfrom functools import wraps\nfrom memory_profiler import memory_usage\n\ndef profile(fn):\n    @wraps(fn)\n    def inner(*args, **kwargs):\n        fn_kwargs_str = ', '.join(f'{k}={v}' for k, v in kwargs.items())\n        print(f'\\n{fn.__name__}({fn_kwargs_str})')\n\n        # Measure time\n        t = time.perf_counter()\n        retval = fn(*args, **kwargs)\n        elapsed = time.perf_counter() - t\n        print(f'Time   {elapsed:0.4}')\n\n        # Measure memory\n        mem, retval = memory_usage((fn, args, kwargs), retval=True, timeout=200, interval=1e-7)\n\n        # Get Peak Memory Usage\n        print(f'Memory {max(mem) - min(mem)}')\n        return retval\n\n    return inner\n\n@profile\ndef work(n):\n   for i in range(n):\n       2 ** n\n\nwork(10)\n#&gt; work()\n#&gt; Time   0.06269\n#&gt; Memory 0.0\n\nwork(n=10000)\n#&gt; work(n=10000)\n#&gt; Time   0.3865\n#&gt; Memory 0.0234375",
    "crumbs": [
      "Code",
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/code-optimization.html#sec-code-opt-rftf",
    "href": "qmd/code-optimization.html#sec-code-opt-rftf",
    "title": "Optimization",
    "section": "Replacements for Tidyverse Functions",
    "text": "Replacements for Tidyverse Functions\n\nMisc\n\nNotes from: Writing performant code with tidy tools\n\nAlso provides links to more {vctrs} recipes from their tidymodels github pull requests\n\nFor code that relies on group_by()and sees heavy traffic, see vctrs::list_unchop(), vctrs::vec_chop(), and vctrs::vec_rep_each().\n\nselect\nbench::mark(\n  dplyr = select(mtcars_tbl, hp),\n  `[.tbl_df` = mtcars_tbl[\"hp\"]\n) %&gt;%\n  select(expression, median)\n#&gt; # A tibble: 2 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyr      527.01µs\n#&gt; 2 [.tbl_df    8.08µs\n\nWinner: base R subsetting\n\nfilter\nWres &lt;-\n  bench::mark(\n    dplyr = filter(mtcars_tbl, hp &gt; 100),\n    vctrs = vec_slice(mtcars_tbl, mtcars_tbl$hp &gt; 100),\n    `[.tbl_df` = mtcars_tbl[mtcars_tbl$hp &gt; 100, ]\n  ) %&gt;%\n    select(expression, median)\nres\n#&gt; # A tibble: 3 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyr      289.93µs\n#&gt; 2 vctrs        4.63µs\n#&gt; 3 [.tbl_df    23.74µs\n\nWinner: vctrs::vec_slice \n\nmutate\nbench::mark(\n  dplyr = mutate(mtcars_tbl, year = 1974L),\n  `$&lt;-.tbl_df` = {mtcars_tbl$year &lt;- 1974L; mtcars_tbl}\n) %&gt;%\n  select(expression, median)\n\n#&gt; # A tibble: 2 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyr      302.5µs\n#&gt; 2 $&lt;-.tbl_df  12.8µs\n\nWinner: base R assignment\n\nmutate and relocate\nbench::mark(\n  mutate = mutate(mtcars_tbl, year = 1974L, .after = make_model),\n  relocate = relocate(mtcars_tbl, year, .after = make_model),\n  `[.tbl_df` = \n      mtcars_tbl[\n        c(left_cols, \n          colnames(mtcars_tbl[!colnames(mtcars_tbl) %in% left_cols])\n        )\n      ],\n  check = FALSE\n) %&gt;% \n  select(expression, median)\n\n#&gt; # A tibble: 3 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 mutate        1.2ms\n#&gt; 2 relocate    804.3µs\n#&gt; 3 [.tbl_df    19.1µs\n\nWinner: base R\n\npull\nbench::mark(\n  dplyr = pull(mtcars_tbl, hp),\n  `$.tbl_df` = mtcars_tbl$hp,\n  `[[.tbl_df` = mtcars_tbl[[\"hp\"]]\n) %&gt;%\n  select(expression, median)\n\n#&gt; # A tibble: 3 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyr      101.19µs\n#&gt; 2 $.tbl_df  615.02ns\n#&gt; 3 [[.tbl_df    2.25µs\n\nWinner: base R bracket subsetting\n\nbind_*\nbench::mark(\n  dplyr = bind_rows(mtcars_tbl, mtcars_tbl),\n  vctrs = vec_rbind(mtcars_tbl, mtcars_tbl)\n) %&gt;%\n  select(expression, median)\n\n#&gt; # A tibble: 2 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyr          44µs\n#&gt; 2 vctrs        14.3µs\n\nbench::mark(\n  dplyr = bind_cols(mtcars_tbl, tbl),\n  vctrs = vec_cbind(mtcars_tbl, tbl)\n) %&gt;%\n  select(expression, median)\n#&gt; # A tibble: 2 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyr        60.7µs\n#&gt; 2 vctrs        26.2µs\n\nWinners: vctrs::vec_cbind and vctrs::vec_rbind\n\nCreate Tibble\nbench::mark(\n  tibble = tibble(a = 1:2, b = 3:4),\n  new_tibble_df_list = new_tibble(df_list(a = 1:2, b = 3:4), nrow = 2),\n  new_tibble_list = new_tibble(list(a = 1:2, b = 3:4), nrow = 2)\n) %&gt;% \n  select(expression, median)\n#&gt; # A tibble: 3 × 2\n#&gt;  expression          median\n#&gt;  &lt;bch:expr&gt;        &lt;bch:tm&gt;\n#&gt; 1 tibble            165.97µs\n#&gt; 2 new_tibble_df_list  16.69µs\n#&gt; 3 new_tibble_list      4.96µs\n\nWinner: new_tibble_list\n\nJoins\n\nQuestions:\n\nIf this join happens multiple times, is it possible to express it as one join and then subset it when needed?\n\ni.e. if a join happens inside of a loop but the elements of the join are not indices of the loop, it’s likely possible to pull that join outside of the loop and then vctrs::vec_slice() its results inside of the loop. Am I using the complete outputted join result or just a portion? If I end up only making use of column names, or values in one column (as with joins approximating lookup tables), or pairings between two columns, I may be able to instead use $.tbl_df or [.tbl_df (see above, Pull).\n\n\nFor problems even a little bit more complex, e.g. if there were possibly multiple matching or if I wanted to keep all rows, then expressing this join with more bare-bones operations quickly becomes less readable and more error-prone. In those cases, too, joins in dplyr have a relatively small amount of overhead when compared to the vctrs backends underlying them. So, optimize carefully.\nExample: inner_join vs vctrs::vec_slice (Note: *only 0 or 1 match possible*)\nsupplement_my_cars &lt;- function() {\n  # locate matches, assuming only 0 or 1 matches possible\n  loc &lt;- vec_match(my_cars$make_model, mtcars_tbl$make_model)\n\n  # keep only the matches\n  loc_mine &lt;- which(!is.na(loc))\n  loc_mtcars &lt;- vec_slice(loc, !is.na(loc))\n\n  # drop duplicated join column\n  my_cars_join &lt;- my_cars[setdiff(names(my_cars), \"make_model\")]\n  vec_cbind(\n    vec_slice(mtcars_tbl, loc_mtcars),\n    vec_slice(my_cars_join, loc_mine)\n  )\n}\nsupplement_my_cars()\n#&gt; # A tibble: 1 × 13\n#&gt;  make_model    mpg  cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#&gt;  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Honda Civic  30.4    4  75.7    52  4.93  1.62  18.5    1    1    4    2\n#&gt; # ℹ 1 more variable: color &lt;chr&gt;\n\nbench::mark(\n  inner_join = inner_join(mtcars_tbl, my_cars, \"make_model\"),\n  manual = supplement_my_cars()\n) %&gt;%\n  select(expression, median)\n#&gt; # A tibble: 2 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 inner_join    438µs\n#&gt; 2 manual      50.7µs\n\nnest\nbench::mark(\n  nest = nest(mtcars_tbl, .by = c(cyl, am)),\n  vctrs = {\n    res &lt;- \n      vec_split(\n        x = mtcars_tbl[setdiff(colnames(mtcars_tbl), nest_cols)],\n        by = mtcars_tbl[nest_cols]\n      )\n\n    vec_cbind(res$key, new_tibble(list(data = res$val)))\n  }\n) %&gt;%\n  select(expression, median)\n\n#&gt; # A tibble: 2 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 nest        1.81ms\n#&gt; 2 vctrs      67.61µs\n\n# Results of nesting\n#&gt; # A tibble: 6 × 3\n#&gt;    cyl    am data             \n#&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;           \n#&gt; 1    6    1 &lt;tibble [3 × 10]&gt; \n#&gt; 2    4    1 &lt;tibble [8 × 10]&gt; \n#&gt; 3    6    0 &lt;tibble [4 × 10]&gt; \n#&gt; 4    8    0 &lt;tibble [12 × 10]&gt;\n#&gt; 5    4    0 &lt;tibble [3 × 10]&gt; \n#&gt; 6    8    1 &lt;tibble [2 × 10]&gt;\nglue and paste0\nvec_paste0 &lt;- function (...) {\n  args &lt;- vec_recycle_common(...)\n  exec(paste0, !!!args)\n}\n\nname &lt;- \"Simon\"\nbench::mark(\n  glue = glue::glue(\"My name is [{name}]{style='color: #990000'}.\"),\n  vec_paste0 = vec_paste0(\"My name is \", name, \".\"),\n  paste0 = paste0(\"My name is \", name, \".\"),\n  check = FALSE\n) %&gt;% \n  select(expression, median)\n\n#&gt; # A tibble: 3 × 2\n#&gt;  expression  median\n#&gt;  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 glue        38.99µs\n#&gt; 2 vec_paste0  3.98µs\n#&gt; 3 paste0    861.01ns\n\npaste0() has some tricky recycling behavior. vec_paste0 is a middle ground in terms of both performance and safety.\nUse glue() for errors, when the function will stop executing anyway.\nFor simple pastes that are intended to be called repeatedly, use vec_paste0().",
    "crumbs": [
      "Code",
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html",
    "href": "qmd/code-snippets.html",
    "title": "Snippets",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-misc",
    "href": "qmd/code-snippets.html#sec-code-snippits-misc",
    "title": "Snippets",
    "section": "",
    "text": "Check whether an environment variable is empty\nnzchar(Sys.getenv(\"blopblopblop\"))\n#&gt; [1] FALSE\nwithr::with_envvar(\n  new = c(\"blopblopblop\" = \"bla\"),\n  nzchar(Sys.getenv(\"blopblopblop\"))\n)\nUse a package for a single instance using {withr::with_package}\n\n\nUsing library() will keep the package loaded during the whole session, with_package() just runs the code snippet with that package temporarily loaded. This can be useful to avoid namespace collisions for example\n\nRead .csv from a zipped file\n# long way\ntmpf &lt;- tempfile()\ntmpd &lt;- tempfile()\ndownload.file('https://website.org/path/to/file.zip', tmpf)\nunzip(tmpf, exdir = tmpd)\ny &lt;- data.table::fread(file.path(tmpd,\n                       grep('csv$',\n                            unzip(tmpf, list = TRUE)$Name,\n                            value = TRUE)))\nunlink(tmpf)\nunlink(tmpd)\n\n# quick way\ny &lt;- data.table::fread('curl https://website.org/path/to/file.zip | funzip')\nLoad all R scripts from a directory: for (file in list.files(\"R\", full.names = TRUE)) source(file)\nView dataframe in View as html table using {kableExtra}\ndf_html &lt;- kableExtra::kbl(rbind(head(df, 5), tail(df, 5)), format = \"html\")\nprint(df_html)",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-opts",
    "href": "qmd/code-snippets.html#sec-code-snippits-opts",
    "title": "Snippets",
    "section": "Options",
    "text": "Options\n\n{readr}\noptions(readr.show_col_types = FALSE)",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-cleaning",
    "href": "qmd/code-snippets.html#sec-code-snippits-cleaning",
    "title": "Snippets",
    "section": "Cleaning",
    "text": "Cleaning\n\nRemove all objects except: rm(list=setdiff(ls(), c(\"train\", \"validate\", \"test\")))\nRemove NAs\n\ndataframes\ndf %&gt;% na.omit\ndf %&gt;% filter(complete.cases(.))\ndf %&gt;% tidyr::drop_na()\nvariables\ndf %&gt;% filter(!is.na(x1))\ndf %&gt;% tidyr::drop_na(x1)\n\nFind duplicate rows\n\n{datawizard} - Extract all duplicates, for visual inspection. Note that it also contains the first occurrence of future duplicates, unlike duplicated or dplyr::distinct. Also contains an additional column reporting the number of missing values for that row, to help in the decision-making when selecting which duplicates to keep.\ndf1 &lt;- data.frame(\n  id = c(1, 2, 3, 1, 3),\n  year = c(2022, 2022, 2022, 2022, 2000),\n  item1 = c(NA, 1, 1, 2, 3),\n  item2 = c(NA, 1, 1, 2, 3),\n  item3 = c(NA, 1, 1, 2, 3)\n)\n\ndata_duplicated(df1, select = \"id\")\n#&gt;   Row id year item1 item2 item3 count_na\n#&gt; 1   1  1 2022    NA    NA    NA        3\n#&gt; 4   4  1 2022     2     2     2        0\n#&gt; 3   3  3 2022     1     1     1        0\n#&gt; 5   5  3 2000     3     3     3        0\n\ndata_duplicated(df1, select = c(\"id\", \"year\"))\n#&gt; 1   1  1 2022    NA    NA    NA        3\n#&gt; 4   4  1 2022     2     2     2        0\ndplyr\ndups &lt;- dat %&gt;% \n  group_by(BookingNumber, BookingDate, Charge) %&gt;% \n  filter(n() &gt; 1)\nbase r\ndf[duplicated(df[\"ID\"], fromLast = F) | duplicated(df[\"ID\"], fromLast = T), ]\n\n##        ID value_1 value_2 value_1_2\n## 2  ID-003      6      5      6 5\n## 3  ID-006      1      3      1 3\n## 4  ID-003      1      4      1 4\n## 5  ID-005      5      5      5 5\n## 6  ID-003      2      3      2 3\n## 7  ID-005      2      2      2 2\n## 9  ID-006      7      2      7 2\n## 10 ID-006      2      3      2 3\n\ndf[duplicated(df[\"ID\"], fromLast = F) doesn’t include the first occurence, so also counting from the opposite direction will include all occurences of the duplicated rows\n\n\nRemove duplicated rows\n\n{datawizard} - From all rows with at least one duplicated ID, keep only one. Methods for selecting the duplicated row are either the first duplicate, the last duplicate, or the “best” duplicate (default), based on the duplicate with the smallest number of NA. In case of ties, it picks the first duplicate, as it is the one most likely to be valid and authentic, given practice effects.\ndf1 &lt;- data.frame(\n  id = c(1, 2, 3, 1, 3),\n  item1 = c(NA, 1, 1, 2, 3),\n  item2 = c(NA, 1, 1, 2, 3),\n  item3 = c(NA, 1, 1, 2, 3)\n)\n\ndata_unique(df1, select = \"id\")\n#&gt; (2 duplicates removed, with method 'best')\n#&gt;   id item1 item2 item3\n#&gt; 1  1     2     2     2\n#&gt; 2  2     1     1     1\n#&gt; 3  3     1     1     1\nbase R\ndf[!duplicated(df[c(\"col1\")]), ]\ndplyr\ndistinct(df, col1, .keep_all = TRUE)\n\nShowing all combinations present in the data and creating all possible combinations\n\nFuzzy Join (alt to case_when)\nref.df &lt;- data.frame(\n            bucket = c(“High”, “Medium-High”, “Medium-Low”, “Low”),\n            value.high = c(max(USArrests$Assault), 249, 199, 149),\n            value.low = c(250, 200, 150, min(USArrests$Assault)))\nUSArrests %&gt;% \n  fuzzy_join(ref.df, \n                    by = c(\"Assault\"=\"value.low\",\n                          \"Assault\" = 'value.high'), \n            match_fun = c(`&gt;=`,`&lt;=`)) %&gt;% \n  select(-c(value.high, value.low))\n\nAlso does partial matches\n\n\n\n\nRemove elements of a list by name\npurrr::discard_at(my_list, \"a\")\nlistr::list_remove",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-func",
    "href": "qmd/code-snippets.html#sec-code-snippits-func",
    "title": "Snippets",
    "section": "Functions",
    "text": "Functions\n\nggplot\nviz_monthly &lt;- function(df, y_var, threshhold = NULL) {\n\n  ggplot(df) +\n    aes(\n      x = .data[[\"day\"]],\n      y = .data[[y_var]]\n    ) +\n    geom_line() +\n    geom_hline(yintercept = threshhold, color = \"red\", linetype = 2) +\n    scale_x_continuous(breaks = seq(1, 29, by = 7)) +\n    theme_minimal()\n}\n\naes is on the outside\n\nThis was a function for a shiny module\nIt’s peculier. Necessary for function or module?\n\n\nCreate formula from string\nanalysis_formula &lt;- 'Days_Attended ~ W + School'\nestimator_func &lt;-  function(data) lm(as.formula(analysis_formula), data = data)\nRecursive Function\n\nExample\n# Replace pkg text with html\nreplace_txt &lt;- function(dat, patterns) {\n  if (length(patterns) == 0) {\n    return(dat)\n  }\n\n  pattern_str &lt;- patterns[[1]]$pattern_str\n  repl_str &lt;- patterns[[1]]$repl_str\n  replaced_txt &lt;- dat |&gt;\n    str_replace_all(pattern = pattern_str, repl_str)\n\n  new_patterns &lt;- patterns[-1]\n  replace_txt(replaced_txt, new_patterns)\n}\n\nArguments include the dataset and the iterable\nTests whether function has iterated through pattern list\nRemoves 1st element of the list\nreplace_text calls itself within the function with the new list and new dataset\n\nExample: Using Recall and tryCatch\nload_page_completely &lt;- function(rd) {\n  # load more content even if it throws an error\n  tryCatch({\n      # call load_more()\n      load_more(rd)\n      # if no error is thrown, call the load_page_completely() function again\n      Recall(rd)\n  }, error = function(e) {\n      # if an error is thrown return nothing / NULL\n  })\n}\n\nload_more is a user defined function\nRecall is a base R function that calls the same function it’s in.",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-calcs",
    "href": "qmd/code-snippets.html#sec-code-snippits-calcs",
    "title": "Snippets",
    "section": "Calculations",
    "text": "Calculations\n\nCompute the running maximum per group\n(df &lt;- structure(list(var = c(5L, 2L, 3L, 4L, 0L, 3L, 6L, 4L, 8L, 4L),\n              group = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L),\n                                .Label = c(\"a\", \"b\"), class = \"factor\"),\n              time = c(1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L)),\n          .Names = c(\"var\", \"group\",\"time\"),\n          class = \"data.frame\", row.names = c(NA, -10L)))\n\ndf[order(df$group, df$time),]\n#    var group time\n# 1    5    a    1\n# 2    2    a    2\n# 3    3    a    3\n# 4    4    a    4\n# 5    0    a    5\n# 6    3    b    1\n# 7    6    b    2\n# 8    4    b    3\n# 9    8    b    4\n# 10  4    b    5\n\ndf$curMax &lt;- ave(df$var, df$group, FUN=cummax)\ndf\nvar  |  group  |  time  |  curMax\n5      a        1        5\n2      a        2        5\n3      a        3        5\n4      a        4        5\n0      a        5        5\n3      b        1        3\n6      b        2        6\n4      b        3        6\n8      b        4        8\n4      b        5        8\n\n\nTime Series\n\nBase-R\n\nIntervals\n\nDifference between dates\n# Sample dates\nstart_date &lt;- as.Date(\"2022-01-15\")\nend_date &lt;- as.Date(\"2023-07-20\")\n\n# Calculate time difference in days\ntime_diff_days &lt;- end_date - start_date\n\n# Convert days to months\nmonths_diff_base &lt;- as.numeric(time_diff_days) / 30.44  # average days in a month\n\ncat(\"Number of months using base R:\", round(months_diff_base, 2), \"\\n\")\n#&gt; Number of months using base R: 18.1 \n\n\n\n\n{lubridate}\n\nDocs\nIntervals\n\nLubridate’s interval functions\nNotes from: Wrangling interval data using lubridate\nDifference between dates\n# Load the lubridate package\nlibrary(lubridate)\n\n# Sample dates\nstart_date &lt;- ymd(\"2022-01-15\")\nend_date &lt;- ymd(\"2023-07-20\")\n\n# Calculate months difference using lubridate\nmonths_diff_lubridate &lt;- interval(start_date, end_date) %/% months(1)\n\ncat(\"Number of months using lubridate:\", months_diff_lubridate, \"\\n\")\n#&gt; Number of months using lubridate: 18 \n\n%/% is used for floor division by months. For decimals, just use /\n\nData\n(house_df &lt;- tibble(\n  person_id  = factor(c(\"A10232\", \"A10232\", \"A10232\", \"A39211\", \"A39211\", \"A28183\", \"A28183\", \"A10124\")),\n  house_id   = factor(c(\"H1200E\", \"H1243D\", \"H3432B\", \"HA7382\", \"H53621\", \"HC39EF\", \"HA3A01\", \"H222BA\")),\n  start_date = ymd(c(\"20200101\", \"20200112\", \"20211120\", \"19800101\", \"19900101\", \"20170303\", \"20190202\", \"19931023\")),\n  end_date   = ymd(c(\"20200112\", \"20211120\", \"20230720\", \"19891231\", \"20170102\", \"20180720\", \"20230720\", \"20230720\"))\n))\n\n#&gt;   A tibble: 8 × 4\n#&gt;   person_id house_id start_date end_date  \n#&gt;   &lt;fct&gt;     &lt;fct&gt;    &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 A10232    H1200E   2020-01-01 2020-01-12\n#&gt; 2 A10232    H1243D   2020-01-12 2021-11-20\n#&gt; 3 A10232    H3432B   2021-11-20 2023-07-20\n#&gt; 4 A39211    HA7382   1980-01-01 1989-12-31\n#&gt; 5 A39211    H53621   1990-01-01 2017-01-02\n#&gt; 6 A28183    HC39EF   2017-03-03 2018-07-20\n#&gt; 7 A28183    HA3A01   2019-02-02 2023-07-20\n#&gt; 8 A10124    H222BA   1993-10-23 2023-07-20\nCreate interval column\nhouse_df &lt;- \n  house_df |&gt; \n  mutate(\n    # create the interval\n    int = interval(start_date, end_date), \n    # drop the start/end columns\n    .keep = \"unused\"                      \n  )\n\nhouse_df\n#&gt;   A tibble: 8 × 3\n#&gt;   person_id house_id int                           \n#&gt;   &lt;fct&gt;     &lt;fct&gt;    &lt;Interval&gt;                    \n#&gt; 1 A10232    H1200E   2020-01-01 UTC--2020-01-12 UTC\n#&gt; 2 A10232    H1243D   2020-01-12 UTC--2021-11-20 UTC\n#&gt; 3 A10232    H3432B   2021-11-20 UTC--2023-07-20 UTC\n#&gt; 4 A39211    HA7382   1980-01-01 UTC--1989-12-31 UTC\n#&gt; 5 A39211    H53621   1990-01-01 UTC--2017-01-02 UTC\n#&gt; 6 A28183    HC39EF   2017-03-03 UTC--2018-07-20 UTC\n#&gt; 7 A28183    HA3A01   2019-02-02 UTC--2023-07-20 UTC\n#&gt; 8 A10124    H222BA   1993-10-23 UTC--2023-07-20 UTC\nIntersection Function\n\nint_intersect &lt;- function(int, int_limits) {\n  int_start(int) &lt;- pmax(int_start(int), int_start(int_limits))\n  int_end(int)   &lt;- pmin(int_end(int), int_end(int_limits))\n  return(int)\n}\n\nThe red dashed line is the reference interval and the blue solid line is the interval of interest\nThe function creates an interval thats the intersection of both intervals (segment between black parentheses)\n\nProportion of the Reference Interval\n\nint_proportion &lt;- function(dat, reference_interval) {\n\n  # start with the housing data\n  dat |&gt; \n    # only retain overlapping rows, this makes the following\n    # operations more efficient by only computing what we need\n    filter(int_overlaps(int, reference_interval)) |&gt; \n    # then, actually compute the overlap of the intervals\n    mutate(\n      # use our earlier truncate function\n      int_sect = int_intersect(int, reference_interval),\n      # then, it's simple to compute the overlap proportion\n      prop = int_length(int_sect) / int_length(reference_interval)\n    ) |&gt; \n    # combine different intervals per person\n    summarize(prop_in_nl = sum(prop), .by = person_id)\n\n}\n\nExample\nint_2017  &lt;- interval(ymd(\"20170101\"), ymd(\"20171231\"))\nprop_2017 &lt;- \n  int_proportion(dat = house_df, \n                 reference_interval = int_2017)\n\nprop_2017\n\n#&gt; # A tibble: 3 × 2\n#&gt;   person_id prop_in_nl\n#&gt;   &lt;fct&gt;          &lt;dbl&gt;\n#&gt; 1 A39211       0.00275\n#&gt; 2 A28183       0.832  \n#&gt; 3 A10124       1",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html",
    "href": "qmd/feature-engineering-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-misc",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-misc",
    "title": "General",
    "section": "",
    "text": "Tree-based Models\n\nFrom Uber, “Tree-based models are performing piecewise linear functional approximation, which is not good at capturing complex, non-linear interaction effects.”\n\nWith regression models, you have to be careful about encoding categoricals as ordinal (i.e. integers) which means one-hot encoding is better.\n\nFor example, the raw numerical encoding (0-24) of the “hour” feature prevents the linear model from recognizing that an increase of hour in the morning from 6 to 8 should have a strong positive impact on the number of bike rentals while a increase of similar magnitude in the evening from 18 to 20 should have a strong negative impact on the predicted number of bike rentals.\n\nModels with large numbers (100s) of features increases the opportunity for feature drift\nZero-Inflated Predictors/Features\n\nFor ML, transformations probably not necessary\nFor regression\n\nlog(x + 0.05)\n\nlarger effect on skew than sqrt\n\narcsinh(x) (see Continuous &gt;&gt; Transformations &gt;&gt; Logging)\n\napproximates a log but handles 0s\n\nsqrt maybe Yeo-Johnson (?)\n\n&gt;60% of values = 0, consider binning or binary",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-cont",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-cont",
    "title": "General",
    "section": "Continuous",
    "text": "Continuous\n\nBinning\n\nBenefits\n\nReduces Noise\n\nContinuous variables tend to store information with minute fluctuations that provide no added value for the machine learning task of interest\n\nMakes the feature more intuitive\n\nIs there an important threshold value?\n\n1 value –&gt; split into a binary\nmultiple values –&gt; multinomial\n\n\nMinimizes outlier influence\n\nBin and Embed\n\nSteps\n\nFind bin ranges\n\nIf sufficient data, calculate quantiles of the numeric vector to find the bin ranges\nsklearn.preprocessing.KBinsDiscretizer has a few different methods\nUse some other method to find the number/ranges of bins (see R packages)\n\nUse the indices of the bins (i.e. leftmost bin is 1, 2nd leftmost bin is 2) to discretize each value of the numeric\n\nMight need to be one-hot coded\n\nCreate an embedding of the discretized vector and use the embedding as features.\n\n\nDichotomizing is bad (post, list of papers)\n\nTypical arguments for splitting (even when there’s no underlying reason to do so) include: simplifies the statistical analysis and leads to easy interpretation and presentation of results\n\nExample: splitting at the median—leads to a comparison of groups of individuals with high or low values of the measurement, leading in the simplest case to a t test or χ2 test and an estimate of the difference between the groups (with its confidence interval) on another variable.\n\nUsing multiple categories (to create an “ordinal” variable) is generally preferable , and using four or five groups the loss of information can be quite small\nIssues:\n\nInformation is lost, so the statistical power to detect a relation between the variable and patient outcome is reduced.\n\nDichotomising a variable at the median reduces power by the same amount as would discarding a third of the data\n\nMay increase the risk of a positive result being a false positive\nMay seriously underestimate the extent of variation in outcome between groups, such as the risk of some event, and considerable variability may be subsumed within each group.\nIndividuals close to but on opposite sides of the cutpoint are characterised as being very different rather than very similar.\nConceals any non-linearity in the relation between the variable and outcome\nUsing a stat like median for a cutpoint means studies will have different cutpoints, therefore results cannot easily be compared, seriously hampering meta-analysis of observational studies\nAn “optimal” cutpoint (usually that giving the minimum P value) runs a high risk of a spuriously significant result. Effect will be overestimated and the CI too narrow\nAdjusting for the effect of a confounding variable, dichotomisation will run the risk that a substantial part of the confounding remains\n\n\nHarrell\n\nThink most of these issues are related to inference models like types of logistic regression\nA better approach that maximizes power and that only assumes a smooth relationship is to use a restricted cubic spline (regression spline; piecewise cubic polynomial) function for predictors that are not known to predict linearly. Use of flexible parametric approaches such as this allows standard inference techniques (P -values, confidence limits) to be used (See Feature Engineering, Splines)\nIssues with binning continuous variables\n\nIf cutpoints are chosen by trial and error in a way that utilizes the response, even informally, ordinary P -values will be too small and confidence intervals will not have the claimed coverage probabilities.\n\nThe correct Monte-Carlo simulations must take into account both multiple tests and uncertainty in the choice of cutpoints.\n\nUsing the “minimum p-value approach” often results in multiple cutpoints so ¯\\_(ツ)_/¯ plus multiple testing p-value adjustments need to be used.\n\nThis approach involves testing multiple cutpoints and choosing one that minimizes the p-value below a threshold.\n\nOptimal cutpoints often change from sample to sample\nThe optimal cutpoint for a predictor would necessarily be a function of the continuous values of all the other predictors\nYou’re losing variation (information) which causes a loss of power and precision\nAssumes that the relationship between the predictor and the response is flat within each interval\n\nthis assumption is far less reasonable than a linearity assumption in most cases\n\nPercentiles\n\nUsually estimated from the data at hand, are estimated with sampling error, and do not relate to percentiles of the same variable in a population\nValue of binned variable potentially takes on a different relationship with the outcome\n\ne.g. Body Mass Index has a smooth relationship with every outcome studied, and relates to outcome according to anatomy and physiology. Binning may change that relationship to being how many subjects have a similar BMI.\n\n\nMany bins usually required to make it worth it. Therefore, many dummy variables will end up being created resulting in a loss of power and precision. (i.e. more bins = more variables = more dof used)\nPoor predictive performance with Cox regression models\n\n\nThey might help with prediction using ML or DL models though\n\n“Instead of directly using marketplace health as a continuous feature, we decided to use a form of target-encoding by splitting up the metric into buckets and taking the average historical delivery duration within that bucket as the new feature. With this approach, we directly helped the model learn that very supply-constrained market conditions are correlated with very high delivery times — rather than relying on the model to learn those patterns from the relatively sparse data available.”\n\nImproving ETA Prediction Accuracy for Long Tail Events\nHelps to “represent features in a way that makes it easy for the model to learn sparse patterns.”\n\nThis article was about modeling tail events, so maybe this is most useful for features that have an association with the tail values in the outcome variable\n\n\nXGBoost seems to like numerics much more than dummies\n\nTrees may prefer larger cardinalities. So if you do bin, you’d probably want quite a few bins\nNever really seen a binned age variable do well, so guessing more than 10 at least. Though maybe Age just wasn’t important enough.\n\n\nExamples\n\nBinary\n\nWhether a user spent more than $50 or didn’t\nIf user had activity on the weekend or not\n\nMultinomial or Discrete\n\nTimestamp to morning/afternoon/ night,\nOrder values into buckets of $10–20, $20–30, $30+\nHeight, age\n\nExample: step_discretize\ndata(ames, package = \"modeldata\")\n\nrecipe(~ Lot_Frontage + Lot_Area, data = ames) |&gt;\n  step_discretize(all_numeric_predictors(), num_breaks = 5) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 × 2\n#&gt;    Lot_Frontage Lot_Area\n#&gt;    &lt;fct&gt;        &lt;fct&gt;   \n#&gt;  1 bin5         bin5    \n#&gt;  2 bin4         bin4    \n#&gt;  3 bin5         bin5    \n#&gt;  4 bin5         bin4    \n#&gt;  5 bin4         bin5    \n#&gt;  6 bin4         bin3    \n#&gt;  7 bin2         bin1    \n#&gt;  8 bin2         bin1    \n#&gt;  9 bin2         bin1    \n#&gt; 10 bin2         bin2    \n#&gt; # ℹ 2,920 more rows\n\n\n\n\nTransformations\n\nMisc\n\nAlso see:\n\nRegression, Linear &gt;&gt; Transformations\nFeature Engineering, Splines\n\nCentering\n\nNo matter how a variable is centered (e.g. around the mean, median, or other number), its linear regression coefficient will not change - only the intercept will change.\n\nGuide for choosing a scaling method for classification modeling\n\nNotes from The Mystery of Feature Scaling is Finally Solved (narrator: it wasn’t)\n\nOnly used a SVM model for experimentation so who knows if this carries over to other classifiers\n\ntldr\n\nGot time and compute resources? –&gt; Ensemble different standardization methods using averaging\nNo time and limited compute resources –&gt; standardization\n\nModels that are distribution independent or distance sensitive (e.g. SVM, kNN, ANNs) should use standardization\n\nModels that are distribution dependent (e.g. regularized linear regression, regularized logistic regression, or linear discriminant analysis) weren’t tested\n\nNo evidence that data-centric rules (e.g. normal or non-normal distributed variables, outliers present)\nFeature scaling that is aligned with the data or model can be responsible for overfitting\nEnsembling by averaging (instead of using a model to ensemble) different standarization methods\n\nExperiment used robust scaler (see below) and z-score standardization\n\nWhen they added a 3rd method it created more biased results\n\nRequires predictions to be probabilities\n\nFor ML models, this takes longer because an extra CV has to be run\n\n\n\n\n\n\nStandardization\n\nThe standard method transforms feature to have mean = 0, and standard deviation = 1\n\nNot robust to outliers\n\nFeature will be skewed\n\n\nUsing the median to center and the MAD to scale makes the transformation robust to outliers\nScaling by 2 sd/MAD instead of 1 sd/MAD can be useful to obtain model coefficients of continuous parameters comparable to coefficients related to binary predictors, when applied to the predictors (not the outcome)\nNotes from\n\nWhen conducting multiple regression, when should you center your predictor variables & when should you standardize them?\n\nReasons to standardize\n\nMost ML/DL models require it\n\nMany elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n\nMost Clustering methods require it\nPCA can only be interpreted as the singular value decomposition of a data matrix when the columns have centered\nInterpreting the intercept as the mean of the outcome when all predictors are held at their means\nPredictors with large values (country populations) can have really small regression coefficients. Standardization makes the coefficients have a more managable scale.\nSome types of models are more numerically stable with the predictors have been standardized\nEasier to set priors in Bayesian modeling\nCentering fixes collinearity issues when creating powers and interaction terms\n\nCollinearity between the created terms and the main effects\n\n\nOther Reasons why you might want to:\n\nCreating a composite score\n\nWhen you’re trying to sum or average variables that are on different scales, perhaps to create a composite score of some kind. Without scaling, it may be the case that one variable has a larger impact on the sum due purely to its scale, which may be undesirable.\nOther Examples:\n\nResearch into children’s behavioral disorders - researchers might get ratings from both parents & teachers, & then want to combine them into a single measure of maladjustment.\nStudy on the activity level at a nursing home w/ self-ratings by residents & the number of signatures on sign-up sheets for activities\n\n\nTo simplify calculations and notation.\n\nA sample covariance matrix of values that has been centered by their sample means is simply X′X (correlation matrix)\nIf a univariate random variable, X, has been mean centered, then var(X)=E(X2) and the variance can be estimated from a sample by looking at the sample mean of the squares of the observed values.\n\n\nReasons NOT to standardize\n\nWe don’t want to standardize when the value of 0 is meaningful.\n\nstep_best_normalize\n\nRequires {bestNormalize} and has bestNormalize for use outside of {tidymodels}\nChooses the best standardization method using repeated cross-validation to estimate the Pearson’s P statistic divided by its degrees of freedom (from {nortest}) which indicates closness to the Gaussian distribution.\nPackage features the method, Ordered Quantile normalization (orderNorm, or ORQ). ORQ transforms the data based off of a rank mapping to the normal distribution.\nAlso includes: Lambert W\\(\\times\\)F, Box Cox, Yeo-Johnson, arcsinh, exponential, log, square root, and has a method to add your own.\nExample\nlibrary(bestNormalize)\n\ndata(ames, package = \"modeldata\")\n\nrecipe(Sale_Price ~ Lot_Frontage + Lot_Area, data = ames) |&gt;\n  step_best_normalize(all_numeric_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 × 3\n#&gt;    Lot_Frontage Lot_Area Sale_Price\n#&gt;           &lt;dbl&gt;    &lt;dbl&gt;      &lt;int&gt;\n#&gt;  1        2.48     2.29      215000\n#&gt;  2        0.789    0.689     105000\n#&gt;  3        0.883    1.28      172000\n#&gt;  4        1.33     0.574     244000\n#&gt;  5        0.468    1.19      189900\n#&gt;  6        0.656    0.201     195500\n#&gt;  7       -0.702   -1.27      213500\n#&gt;  8       -0.669   -1.24      191500\n#&gt;  9       -0.735   -1.19      236500\n#&gt; 10       -0.170   -0.654     189000\n#&gt; # ℹ 2,920 more rows\n\nscale(var or matrix)\n\nDefault args: center = T, scale = T\nStandardizes each column of a matrix separately\nFYI scale(var) == scale(scale(var))\n\n{datawizard::standardize} - Can center by median and scale by MAD (robust), can scale by 2sd (Gelman)\n{{sklearn::RobustScaler}}\n\nStandardize by median and IQR instead of mean and sd\n\n(value − median) / IQR\n\nThe resulting variable has a zero mean and median and a standard deviation of 1, although not skewed by outliers and the outliers are still present with the same relative relationships to other values.\nstep_normalize has means, sd args, so it might be able to do this\n\nHarrell recommends substituting the gini mean difference for the standard deviation\n\nGini’s mean difference - the mean absolute difference between any two distinct elements of a vector.\n\n\nHmisc::GiniMd(x, na.rm = F) (doc)\nsjstats::gmd(x or df, ...) (doc)\n\nIf “df” then it will compute gmd for all vectors in the df\n“…” allows for use of tidy selectors\n\nManual\ngmd &lt;- function(x) {\n  n &lt;- length(x)\n  sum(outer(x, x, function(a, b) abs(a - b))) / n / (n - 1)\n  }\n\n\n\n\n\nRescaling/Normalization\n\nMisc\n\nIf the values of the feature get rescaled between 0 and 1, i.e. [0,1], then it’s called normalization\nExcept in min/max, all values of the scaling variable should be &gt; 0 since you can’t divide by 0\n{datawizard::rescale} - Scales variable to a specified range\n\nMin/Max\n\nRange: [0, 1]\n\n\nMake sure the min max value are NOT outliers. If they are outliers, then the range of your data will be more constricted that it needs to be.\n\ne.g. if values are in between 100 and 500 with an exceptional value of 25000, then 25000 is scaled as 1 and all the other values become very close to the lower bound of zero\n\nExample: Age is the predictor and Happiness is the outcome. Imagine a very strong relationship between age and happiness, such that happiness is at its maximum at age 18 and its minimum at age 65. It’ll be easier if we rescale age so that the range from 18 to 65 is one unit. Now this new variable A ranges from 0 to 1, where 0 is age 18 and 1 is age 65. (from Statistical Rethinking section 6.3.1 pg 182)\nd2 &lt;- d[ d$age&gt;17 , ] # only adults\nd2$A &lt;- ( d2$age - 18 ) / ( 65 - 18 )\n\nRange: [a, b]\n\nAlso see notebook for code to transform more than 1 variable at a time.\n\nBy max\nscaled_var = var/max(var)\n\nExample: From Statistical Rethinking, pg 246\n\n“… zero ruggedness is meaningful. So instead terrain ruggedness is divided by the maximum value observed. This means it ends up scaled from totally flat (zero) to the maximum in the sample at 1 (Lesotho, a very rugged and beautiful place).”\n\nExample: From Statistical Rethinking, pg 258\n\n“I’ve scaled blooms by its maximum observed value, for three reasons. First, the large values on the raw scale will make optimization difficult. Second, it will be easier to assign a reasonable prior this way. Third, we don’t want to standardize blooms, because zero is a meaningful boundary we want to preserve.”\n\nblooms is bloom size. So there can’t be a negative but zero makes sense.\nblooms is 2 magnitudes larger than both its predictors.\n\n\n\nBy mean\nscaled_var = var/mean(var)\n\nExample: From Statistical Rethinking, pg 246\n\n“log GDP is divided by the average value. So it is rescaled as a proportion of the international average. 1 means average, 0.8 means 80% of the average, and 1.1 means 10% more than average.”\n\n\n\n\n\nLogging\n\nUseful for skewed variables\nIf you have zeros, then its common to add 1 to the predictor values\n\nTo backtransform: exp(logged_predictor) - 1\narcsinh(x): approximates a log (at large values of x) but handles 0s:\n\nBacktransform: log(x + sqrt(1+x^2))\n\n* Don’t use these for outcome variables (See Regression, Other &gt;&gt; Zero-Inflated/Truncated &gt;&gt; Continuous for methods, Thread for discussion and link to a paper on the alternatives)\n\nThe scale of the outcome matters. The thread links to a discussion of a paper on log transforms.\nProposals in the paper are in Section 4.1. One of the recommendations is log(E[Y(0)] + Y) where (I think) E[Y(0)] is the average value of Y when Treatment = 0 but I’m not sure. Need to read the paper.\n\n\nRemember to back-transform predictions if you transformed the target variable\n# log 10 transformed target variable\npreds_intervals &lt;- predict(\n  workflows::pull_workflow_fit(lm_wf),\n  workflows::pull_workflow_prepped_recipe(lm_wf) %&gt;% bake(ames_holdout),\n  type = \"pred_int\",\n  level = 0.90\n) %&gt;% \n  mutate(across(contains(\".pred\"), ~10^.x))\nCombos\n\nLog + scale by mean\n\nExample From Statistical Rethinking Ch8 pg 246\n\n“Raw magnitudes of GDP aren’t meaningful to humans. Since wealth generates wealth, it tends to be exponentially related to anything that increases it (earlier in chapter). This is like saying that the absolute distances in wealth grow increasingly large, as nations become wealthier. So when we work with logarithms instead, we can work on a more evenly spaced scale of magnitudes”\n“Log GDP is divided by the average value. So it is rescaled as a proportion of the international average. 1 means average, 0.8 means 80% of the average, and 1.1 means 10% more than average.”",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-disc",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-disc",
    "title": "General",
    "section": "Discrete",
    "text": "Discrete\n\nQuantitative variables that are countable with no in-between the values. (e.g. integer value variables)\n\ne.g. Age, Height (depending on your scale), Year of Birth, Counts of things\n\nMany variables can be either discrete or continuous depending on whether they are “exact” or have been rounded (i.e. their scale).\n\nTime since event, distance from location\nA zip code would not be a discrete variable since it is not quantitative (i.e. don’t represent amounts of anything). The values just represent geographical locations and could just as easily be names instead of numbers. There is no inherent meaning to arithmetic operations performed on them (e.g. zip_code1 - 5 has no obvious meaning)\n\nBinning\n\nSee Binning\n\nRange to Average\n\nSo numerical range variables like Age can have greater predictive power in ML/DL algorithms by just using the average value of the range\ne.g. Age == 21 to 30 –&gt; (21+30)/2 = 25.5\n\nRates/Ratios\n\nSee Domain Specific\n\nMin/Max Rescaling\n\nSee Continuous &gt;&gt; Transformations &gt;&gt; Rescaling/Normalization",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feats-general-cats",
    "href": "qmd/feature-engineering-general.html#sec-feats-general-cats",
    "title": "General",
    "section": "Categoricals",
    "text": "Categoricals\n\nMisc\n\nSee Feature Engineering, Embeddings &gt;&gt; Engineering\nOne-Hot Encode Issues:\n\nWith high cardinality, the feature space explodes –&gt;\n\nLess power\nLikely to encounter memory problems\n\nUsing a sparse matrix is memory efficient which might make the one-hot encode feasible\n\nSparse data sets don’t work well with highly efficient tree-based algorithms like Random Forest or Gradient Boosting.\n\nModel can’t determine similarity between categories (embedding does)\nEvery kind of encoding and embedding outperforms it by a lot, especially in tree models\n\n\n\n\nCombine/Lump/Collapse\n\nCollapse categories with similar characteristics to reduce dimensionality\n\nstates to regions (midwest, northwest, etc.)\n\nLump\n\nCat vars with levels with too few counts –&gt; lump together into an “Other” category\nstep_other(cat_var, threshold = 0.01) # see\n\nFor details see Model Building, tidymodels &gt;&gt; Recipe\nLevels with too few data will have large uncertainties about the effect and the bloated std.devs can cause some models to throw errors\n\n\nCombine\n\nThe feature reduction can help when data size is a concern\n\nThink this is equivalent to a cat-cat interaction.  ML models usually algorithmically create interactions but I guess this way you get the interaction but with fewer features.\nAlso might be useful to use the same considerations that you use to choose interactions to choose which cat variables to combine.\n\nSteps\n\nCombine var1 and var2 (e.g. “dog”, “minnesota”) to create a new feature called var3 (“dog_minnesota”).\nRemove individual features (var1 and var2) from the dataset.\nencode (one-hot, dummy, etc.) var 3\n\n\n\n\n\nEncode/Hashing\n\nCat vars with high numbers of levels need encoded\nCan’t dummy var because it creates too many additional variables –&gt; reduces power\nNumeric: as.numeric(as.factor(char_var))\nTarget Encoding\n\n{collinear}\n\ntl;dr; I don’t see a method that stands out as theoretically better or worse than the others. The rnorm method would probably produce the most variance within the predictor.\ntarget_encoding_lab takes a df and encodes all categoricals using all or some of the methods\nRank (target_encoding_rank): Returns the rank of the group as a integer, starting with 1 as the rank of the group with the lower mean of the response variable\n\nwhite_noise argument might be able to used.\n\nMean (target_encoding_mean): Replaces each value of the categorical variable with the mean of the response across the category the given value belongs to.\n\nThe argument, white_noise, limits potential overfitting. Must be a value betwee 0 and 1. The value added depends on the magnitude of the response. If response is within 0 and 1, a white_noise of 0.25 will add to every value of the encoded variable a random number selected from a normal distribution between -0.25 and 0.25\n\nrnorm (target_encoding_rnorm): Computes the mean and standard deviation of the response for each group of the categorical variable, and uses rnorm() to generate random values from a normal distribution with these parameters.\n\nThe argument rnorm_sd_multiplier is used as a multiplier of the standard deviation to control the range of values produced by rnorm() for each group of the categorical predictor. Values smaller than 1 reduce the spread in the results, while values larger than 1 have the opposite effect.\n\nLOO (target_encoding_loo): Replaces each categorical value with the mean of the response variable across the other cases within the same group.\n\nThe argument, white_noise, limits potential overfitting.\n\n\n{{category_encoders}}\npip install category_encoders\nimport category_encoders as ce\ntarget_encoder = ce.TargetEncoder(cols=['cat_col_1', 'cat_col_2'])\ntarget_encoder.fit(X, y)\nX_transformed = target_encoder.transform(X_pre_encoded)\n\nCatboost Encoder\npip install category_encoders\nimport category_encoders as ce\ntarget_encoder = ce.CatBoostEncoder(cols=['cat_col_1', 'cat_col_2'])\ntarget_encoder.fit(X, y)\nX_transformed = target_encoder.transform(X_pre_encoded)\nBinary Encoding\n\nBenchmarks for decision trees:\n\nNumeric best (&lt; 1000 categories)\nBinary best (&gt; 1000 categories)\n\nStore N cardinalities using ceil(log(N+1)/log(2)) features\n\n\nHashing\n\nBeyond security and fast look-ups, hashing is used for similarity search.\n\ne.g. Different pictures of the same thing should have similar hashes\nSo, if these hashes are being binned, you’d want something a hashing algorithm thinks is similar to actually be similar in order for this to be most effective.\n\nzip codes, postal codes, lat + long would be good\nNot countries or counties since I’d think the hashing similarity would be related to how similar they are alphabetically or maybe phonetically\nMaybe something like latin species names since those have similar roots, etc. would work. (e.g. dogs are canis-whatever)\n\n\nCan’t be reversed to the original values\n\nAlthough since you have the original, it seems like you could see which cat levels are in a particular hash and maybe glean some latent variable\n\nCreates dummies for each cat but fewer of them.\n\nIt is likely that multiple levels of the column will map to the same hashed columns (even with small data sets). Similarly, it is likely that some columns will have all zeros.\n\nA zero-variance filter (via recipes::step_zv) is recommended for any recipe that uses hashed columns\n\n\ntextrecipes::step_dummy_hash - Dimension Reduction. Create dummy variables, but instead of giving each level its own column, you run the level through a hashing function (MurmurHash3) to determine the column.\n\nnum_terms: Tuning parameter tha controls the number of indices that the hashing function will map to. Since the hashing function can map two different tokens to the same index, will a higher value of num_terms result in a lower chance of collision.\nExample\ndata(ames, package = \"modeldata\")\n\nrecipe(Sale_Price ~ Neighborhood, data = ames) |&gt;\n  step_dummy_hash(Neighborhood, num_terms = 4) |&gt; # Low for example\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 × 5\n#&gt;    Sale_Price dummyhash_Neighborhood_1 dummyhash_Neighborhood_2\n#&gt;         &lt;int&gt;                    &lt;int&gt;                    &lt;int&gt;\n#&gt;  1     215000                        0                       -1\n#&gt;  2     105000                        0                       -1\n#&gt;  3     172000                        0                       -1\n#&gt;  4     244000                        0                       -1\n#&gt;  5     189900                        0                        0\n#&gt;  6     195500                        0                        0\n#&gt;  7     213500                        0                        0\n#&gt;  8     191500                        0                        0\n#&gt;  9     236500                        0                        0\n#&gt; 10     189000                        0                        0\n#&gt; # ℹ 2,920 more rows\n#&gt; # ℹ 2 more variables: dummyhash_Neighborhood_3 &lt;int&gt;,\n#&gt; #   dummyhash_Neighborhood_4 &lt;int&gt;\n\n\nLikelihood Encodings\n\nEstimate the effect of each of the factor levels on the outcome and these estimates are used as the new encoding. The estimates are estimated by a generalized linear model. This step can be executed without pooling (via glm) or with partial pooling (stan_glm or lmer). Currently implemented for numeric and two-class outcomes.\n{embed}\n\nstep_lencode_glm, step_lencode_bayes , and step_lencode_mixed\n\n\n\n\n\nOrdinal\n\nMisc\n\nIf there are NAs or Unknowns, etc.,\n\nAfter coercing into a numeric/integer, you can convert Unknowns to NA and then impute the variable\n\nAll these encodings will produce the same results for a tree model, since tree-based models rely on variable ranks rather than exact values.\n0 = “0 Children”\n1 = “1 Child”\n2 = “2 Children”\n3 = “3 Children”\n4 = “4 or more Children”\n\n1 = “0 Children”\n2 = “1 Child”\n3 = “2 Children”\n4 = “3 Children”\n5 = “4 or more Children”\n\n-100 = “0 Children”\n-85  = “1 Child”\n0    = “2 Children”\n10  = “3 Children”\n44  = “4 or more Children”\n\nVia {tidymodels}\nstep_mutate(ordinal_factor_var = as_integer(ordinal_factor_var))\n# think this uses as_numeric\nstep_ordinalscore(ordinal_factor_var)\nPolynomial Contrasts\n\nSee the section Kuhn’s book\n\nRainbow Method (article)\n\nMIsc\n\nCreates an artifical ordinal variable from a nominal variable (i.e. ordering colors according the rainbow, roy.g.biv)\nAt worst, it maintains the signal of a one-hot encode, but with tree models, it results in less splits and therefore a simpler, more efficient, and less overfit model.\nTest psuedo ordinal method by constructing a simple bayesian model with response ~ 0 + ordinal. Then, you extract the posterior for each constructed ordinal level. Pass these posteriors through a constraint that labels draws for that level that are less (or not) than the draws of the previous level. Lastly calculate the proportion of those that were less than in order to get a probability that the predictor is ordered (article &gt;&gt; “The Model” section)\n\nCode\ngrid &lt;- data.frame(\n  Layer = c(\"B\", \"C\", \"E\", \"G\", \"I\"),\n  error = 0\n)\n\ngrid_with_mu &lt;- tidybayes::add_linpred_rvars(grid, simple_mod, value = \".mu\")\n\nis_stratified &lt;- with(grid_with_mu, {\n  .mu[Layer == \"B\"] &gt; .mu[Layer == \"C\"] &\n  .mu[Layer == \"C\"] &gt; .mu[Layer == \"E\"] &\n  .mu[Layer == \"E\"] &gt; .mu[Layer == \"G\"] &\n  .mu[Layer == \"G\"] &gt; .mu[Layer == \"I\"]\n})\n\nPr(is_stratified)\n#&gt; [1] 0.78725\n“Layer” is the ordinal variable being tested\nadd_linpred_rvars extracts the mean response posteriors for each level of the variable\nResults strongly suggest that the levels of the variable (“Layer”) are ordered, with a 0.79 posterior probability.\n\n\nMethods:\n\nDomain Knowledge\nVariable Attribute (see examples)\nOthers - Best to compute these on a hold out set, so as not cause data leakage\n\nAssociation with the target variable where the value of association is used to rank the categories\nProportion of the event for a binary target variable where the value of the proportion is used to rank the categories\n\n\nIf it’s possible, use domain knowledge according the project’s context to help choose the ranking of the categories.\nThere are always multiple ways to rank the categories, so it may be worthwhile to try multiple versions of the artificial ordinal variable\n\nNot recommended to use more than log₂(K) versions, so as to not surpass the number of variables creating using One-hot (where k is the number of categories)\n\nExample: Vehicle Type\n\nCategories\n\nC: “Compact Car”\nF: “Full-size Car”\nL: “Luxury Car”\nM: “Mid-Size Car”\nP: “Pickup Truck”\nS: “Sports Car”\nU: “SUV”\nV: “Van”\n\nPotential attributes to order by: vehicle size, capacity, price category, average speed, fuel economy, costs of ownership, motor features, etc.\n\nExample: Occupation\n\nCategories\n\n1: “Professional/Technical”\n2: “Administration/Managerial”\n3: “Sales/Service”\n4: “Clerical/White Collar”\n5: “Craftsman/Blue Collar”\n6: “Student”\n7: “Homemaker”\n8: “Retired”\n9: “Farmer”\nA: “Military”\nB: “Religious”\nC: “Self Employed”\nD: “Other”\n\nPotential attributes to order by: average annual salary, by their prevalence in the geographic area of interest, or variables in a Census dataset or some other data source\n\n\n\n\n\nWeight of Evidence\n\nembed::step_woe",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-inter",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-inter",
    "title": "General",
    "section": "Interactions",
    "text": "Interactions\n\nManually\n\nNumeric ⨯ Cat\n\nDummy the cat, then multiply the numeric times each of the dummies.",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-date",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-date",
    "title": "General",
    "section": "Date",
    "text": "Date\n\nDuration\n\nDays since last purchase per customer\n\nExample: (max(invoice_date) - max_date_overall) / lubridate::ddays(1)\n\nThink ddays converts this value to a numeric\n\n\nCustomer Tenure\n\nExample: (min(invoice_date) - max_date_overall) / lubridate::ddays(1)",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-dom",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-dom",
    "title": "General",
    "section": "Domain Specific",
    "text": "Domain Specific\n\nRates/Ratios\n\nPurchase per Customer\n\nTotal Spent\n\nExample: sum(total_per_invoice, na.rm = TRUE)\n\nAverage Spent\n\nExample: mean(total_per_invoice, na.rm = TRUE)\n\n\nLet the effect of Cost vary by the person’s income\n\nmutate(cost_income = cost_of_product/persons_income)\nIntuition being that the more money you have the less effect cost will have on whether purchase something.\nDividing the feature by income is equivalent to dividing the \\(\\beta\\) by income.\n\n\nPre-Treatment Baseline\n\nExample: From Modeling Treatment Effects and Nonlinearities in A/B Tests with GAMS\n\noutcome = log(profit), treatment = exposure to internation markets, group = store\nBaseline variable is log(profit) before experiment is conducted\n\nShould center this variable",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/json.html",
    "href": "qmd/json.html",
    "title": "JSON",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-misc",
    "href": "qmd/json.html#sec-json-misc",
    "title": "JSON",
    "section": "",
    "text": "Packages\n\n{yyjsonr} - A fast JSON parser/serializer, which converts R data to/from JSON and NDJSON. It is around 2x to 10x faster than jsonlite at both reading and writing JSON.\n{RcppSimdJson} - Comparable to {yyjsonr} in performance.\n\nAlso see\n\nBig Data &gt;&gt; Larger than Memory\nSQL &gt;&gt; Processing Expressions &gt;&gt; Nested Data\nDatabases &gt;&gt; DuckDB &gt;&gt; Misc\n\nhrbmstr recommends trying duckdb before using the cli tools in “Big Data”",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-jsonlite",
    "href": "qmd/json.html#sec-json-jsonlite",
    "title": "JSON",
    "section": "{jsonlite}",
    "text": "{jsonlite}\n\nRead",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-py",
    "href": "qmd/json.html#sec-json-py",
    "title": "JSON",
    "section": "Python",
    "text": "Python\n\nExample: Parse Nested JSON into a dataframe (article)\n\nRaw JSON\n\n\n“entry” has the data we want\n“…” at the end indicates there are multiple objectss inside the element, “entry”\n\nProbably other root elements other than “feed” as well\n\n\nRead a json file from a URL using {{requests}} and convert to list\n\nimport requests\n\nurl = \"https://itunes.apple.com/gb/rss/customerreviews/id=1500780518/sortBy=mostRecent/json\"\n\nr = requests.get(url)\n\ndata = r.json()\nentries = data[\"feed\"][\"entry\"]\n\nIt looks like the list conversion also ordered the elements alphabetically\nThe output list is subsetted by the root element “feed” and the child element “entry”\n\nGet a feel for the final structure you want by hardcoding elements into a df\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    parsed_data[\"author_uri\"].append(entry[\"author\"][\"uri\"][\"label\"])\n    parsed_data[\"author_name\"].append(entry[\"author\"][\"name\"][\"label\"])\n    parsed_data[\"author_label\"].append(entry[\"author\"][\"label\"])\n    parsed_data[\"content_label\"].append(entry[\"content\"][\"label\"])\n    parsed_data[\"content_attributes_type\"].append(entry[\"content\"][\"attributes\"][\"type\"])\n    ... \nGeneralize extracting the properties of each object in “entry” with a nested loop\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    for key, val in entry.items():\n        for subkey, subval in val.items():\n            if not isinstance(subval, dict):\n                parsed_data[f\"{key}_{subkey}\"].append(subval)\n            else:\n                for att_key, att_val in subval.items():\n                    parsed_data[f\"{key}_{subkey}_{att_key}\"].append(att_val)\n\ndefaultdict creates a key from a list element (e.g. “author”) and groups the properties into a list of values where the value may also be a dict.\n\nSee Python, General &gt;&gt; Types &gt;&gt; Dictionaries\n\nFor each item in “entry”, it looks at the first key-value pair knowing that value is always a dictionary (object in JSON)\nThen handles two different cases\n\nFirst Case: The value dictionary is flat and does not contain another dictionary, only key-value pairs.\n\nCombine the outer key with the inner key to a column name and take the value as column value for each pair.\n\nSecond Case: Dictionary contains a key-value pair where the value is again a dictionary.\n\nAssumes at most two levels of nested dictionaries\nIterates over the key-value pairs of the inner dictionary and again combines the outer key and the most inner key to a column name and take the inner value as column value.\n\n\n\nRecursive function that handles json elements with deeper structures\n\ndef recursive_parser(entry: dict, data_dict: dict, col_name: str = \"\") -&gt; dict:\n    \"\"\"Recursive parser for a list of nested JSON objects\n\n    Args:\n        entry (dict): A dictionary representing a single entry (row) of the final data frame.\n        data_dict (dict): Accumulator holding the current parsed data.\n        col_name (str): Accumulator holding the current column name. Defaults to empty string.\n    \"\"\"\n    for key, val in entry.items():\n        extended_col_name = f\"{col_name}_{key}\" if col_name else key\n        if isinstance(val, dict):\n            recursive_parser(entry[key], data_dict, extended_col_name)\n        else:\n            data_dict[extended_col_name].append(val)\n\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    recursive_parser(entry, parsed_data, \"\")\n\ndf = pd.DataFrame(parsed_data)\n\nNotice the check for a deeper structure with isinstance. If there is one, then the function is called again.\nFunction outputs a dict which is coerced into dataframe\nTo get rid of “label” in column names: df.columns = [col if not \"label\" in col else \"_\".join(col.split(\"_\")[:-1]) for col in df.columns]\nobject types can be cast into more efficient types: df[\"im:rating\"] = df[\"im:rating\"].astype(int)",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/package-development.html",
    "href": "qmd/package-development.html",
    "title": "Package Development",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Package Development"
    ]
  },
  {
    "objectID": "qmd/package-development.html#sec-pkgdev-misc",
    "href": "qmd/package-development.html#sec-pkgdev-misc",
    "title": "Package Development",
    "section": "",
    "text": "Packages\n\n{goodpractices} - Give advice about good practices when building R packages. Advice includes functions and syntax to avoid, package structure, code complexity, code formatting, etc\n\nVersion Number Syntax\n\n0.0.0.9000 = the “.9000” means intital experimental or in development version (.9001 would be the second experimental version)\n0.1.0 = the “1” means this is your first “minor” version\n1.0.0 = the “1” means this is your first “major” version\n\nIn function docs, always use @examples and NOT @example\nIf you have internal functions (e.g. little helper functions) that you don’t want your users to have the regular access to then:\n\nDon’t include @export for that function’s script\nDocumentation of the function isn’t required\n\nAdd a file from another repo using usethis::use_standalone\n\nIf another package has a set of functions that you’d like to include in your package but don’t want to import their entire package, this can be used.\nIt always overwrites an existing standalone file of the same name, making it easy to update previously imported code.\n\nUsing “Starting a Package” (below) takes some time when you want to test newly written functions.\n\nAlternative: load_all() - Quickly makes function available for interactive testing, (ctrl + shift + L)\n\nChange package name (if not on CRAN): changer::changer(\"current_pkg_name\", \"new_pkg_name\")\n\nShouldn’t have the package project open when running this. I set the working dir to the root of my “Projects” directory locally (e.g Projects &gt;&gt; current_proj_directory, then set working directory to Projects)\nThis changes EVERYTHING automagically, but be sure and make a copy of the directory before running just in case.\nNeed to change repo name in the github repo’s settings before pushing changes\n\nCreate Citation file for package: usethis::use_citation()\n\nOnly execute after you’ve gone through putting your package on zenodo and have a generated citation to populate the fields of the CITATION file.\nSee indianacovid19data project for example\n\nIf planning to release on CRAN, use usethis::use_release_issue, which creates a checklist of things to fix/do before submitting to CRAN\n\nCRAN requires your pkg run without issues on all platforms. {covr} tracks test coverage for your R package and view reports locally or (optionally) upload the results to codecov or coveralls\n\nDeveloping Internal Packages\n\nDeveloping packages for your company’s specific use cases increases efficiency\n\nExamples\n\nCollection\n\nPulling data from public sources\n\nDatabase\n\nConnections and Querying\nAPI requests\nETL processes\n\nReport building\n\nData manipulation\nVisualization\n\n\n\nSee Building a team of internal R packages | Emily Riederer and VIDEO How to make internal R packages part of your team - RStudio\n\nAlso {RDepot} for management",
    "crumbs": [
      "Package Development"
    ]
  },
  {
    "objectID": "qmd/package-development.html#starting-a-package",
    "href": "qmd/package-development.html#starting-a-package",
    "title": "Package Development",
    "section": "Starting a Package",
    "text": "Starting a Package\n\nCheck package name, Create pkg, DESCRIPTION, Set-Up Git\n\nCheck if package name already taken on CRAN: available::available(\"package_name\", browse = FALSE)\n\nAsks you stuff about using “urban dictionary” to see if your package name might be offensive. Just say yes. (or it keeps asking)\nYou want it to say that your package name is available on CRAN\nShows sentiment analysis of your package name according to different dictionaries\n\nCreate directory, a project, a basic package skeleton in that directory\nsetwd(\"~/R/Projects\")\nusethis::create_package(\"package_name\")\nOpen project and fill out some of DESCRIPTION\n\nTitle, Authors, Description\n\nGo to Build pane in RStudio \\(\\rightarrow\\) more \\(\\rightarrow\\) Configure Build Tools \\(\\rightarrow\\) Make sure the “Generate documentation with Roxygen” box is ticked (tick it and click ok)\nSet-Up Git: usethis::use_git()\n\nIt will ask to commit the package skeleton to github \\(\\rightarrow\\) Choose yes to commit locally\nIt’ll ask you to restart RStudio to activate the git pane \\(\\rightarrow\\) Choose yes\nSet-up of GH Auth token, if you don’t have one\n\nThen use usethis::create_github_token() and follow steps\nRefresh session once you’ve updated .Renviron\n\nFor private repo: usethis::use_github(private = TRUE)\nFor public repo: usethis::use_github(private = FALSE, protocol = \"ssh\")\n\nChoose 1 to use ssh key\n“Are title and description okay?” \\(\\rightarrow\\) choose 3. Yes\n\n\n\n\n\nBasic Set-up\n\nUse Markdown for documentation: usethis::use_roxygen_md()\nAdd license: usethis::use_mit_license(copyright_holder = \"Eric Book\")\nAdd Readme.Rmd: usethis::use_readme_rmd()\n\nYou’ll still need to render/knit README.Rmd regularly.\nTo keep README.md up-to-date, devtools::build_readme() is handy.\nYou could also use GitHub Actions to re-render README.Rmd every time you push. An example workflow can be found here: https://github.com/r-lib/actions/tree/master/examples.\n\nAdd News/Changlog file: usethis::use_news_md()\nAdd Article/Vignette: usethis::use_vignette\nDocument, Install, and Check package\n\nBuild pane \\(\\rightarrow\\)\n\nmore \\(\\rightarrow\\) Run Document\nRun Install and Restart\nRun Check\n\n\nCommit files and Push\n\n\n\nDevelopment\n\nAdd common imported functions to DESCRIPTION\nusethis::use_tidy_eval()\nusethis::use_tibble()\nAdd data\n# read data into environment\npkg_dat &lt;- readr::read_rds(\"../path/to/data.rds\")\nusethis::use_data(pkg_dat, overwrite = TRUE)\n\nCreates data directory and adds data as an .rda file\nDocument the data: usethis::use_r(\"data\")\n\nSee indianacovid19data for examples\n\nIf you want to store binary data and make it available to the user, put it in data/. This is the best place to put example datasets. If you want to store parsed data, but NOT make it available to the user, put it in R/sysdata.rda. This is the best place to put data that your functions need.\nIf you want to store raw data, put it in inst/extdata.\n\nAdd Functions\n\nCreate R file: usethis::use_r(\"name-function-file\")\nWrite function\n\nUse pkg::function format for every external package function\n\nAdd documentation (e.g. @description, @params, @returns, @details, @references, @export, and @examples\n\nWhen pasting code into examples, use the RStudio multiple cursors feature (ctrl + alt + up/down) to add “#&gt;” to all the lines of code at once\n\nAdd packages used in function to DESCRIPTION: usethis::use_package(\"package_name\")\nMake sure hardcoded variable names are in global variables file\nMake sure any data used in @examples is added and documentedf\nIf submitting to CRAN: need examples and they should be wrapped in \\dontrun{} and \\donttest{}.\nTest\n\nBuild pane \\(\\rightarrow\\) more \\(\\rightarrow\\) Run Document (Ctrl+Shift+D) \\(\\rightarrow\\) Run Install and Restart (ctrl+shift+B) \\(\\rightarrow\\) Run Check (ctrl+shift+E)\n\nAfter writing a function or two, set-up {pkgdown}, and return here.\nAdd new function to _pkgdown.yml\nAdd new function to any additional pages of the site (e.g. vignettes, Readme.Rmd)\n\nRemember to knit\n\nRun pkgdown::build_site()\n\nKnits html pages for reference function pages\n\nCommit and Push\nRinse and Repeat for each function\n\n\n\n\npkgdown\n\nDocs\nSet-Up: usethis::use_pkgdown()\n\nCreates yaml file: _pkgdown.yml\nFill out yaml\n\nPublish website and set-up Github Actions: usethis::use_pkgdown_github_pages()\nCustomize site\n\nName css file, “extra.css”, and place in folder called, “pkgdown” (also extra.js, extra.scss)\nWhen testing out themes, css features, etc., iterating is much faster when using build_home_index(); init_site() instead of build_site(). Then, refresh html page in browser.\nSee docs for bslib variables that can be used in _pkgdown.yml (requires using bootsrap 5).",
    "crumbs": [
      "Package Development"
    ]
  },
  {
    "objectID": "qmd/package-development.html#sec-pkgdev-depend",
    "href": "qmd/package-development.html#sec-pkgdev-depend",
    "title": "Package Development",
    "section": "Dependencies",
    "text": "Dependencies\n\nImports and Depends\n\nImports just loads the package\n\nUnless there is a good reason otherwise, you should always list packages in Imports not Depends. That’s because a good package is self-contained, and minimises changes to the global environment (including the search path)\n\nDepends attaches it.\n\nLoading and Attaching\n\nLoading\n\nThe package is available in memory, but because it’s not in the search path (path that R searches for functions), you won’t be able to access its components without using ::.\n\nAttaching\n\nPuts the package in the search path. You can’t attach a package without first loading it\nBoth library() (throws error when pkg not installed )or require() (just returns false when pkg not installed) load then attach the package",
    "crumbs": [
      "Package Development"
    ]
  },
  {
    "objectID": "qmd/package-development.html#sec-pkgdev-test",
    "href": "qmd/package-development.html#sec-pkgdev-test",
    "title": "Package Development",
    "section": "Testing",
    "text": "Testing\n\nPackages\n\n{testthat} explicitly evaluates the outputs of your function but you can add a test that makes sure the checks on inputs within the function are working\n{doctext} - Generate tests from examples using {roxygen} and {testthat}\n\nRun tests (Ctrl+Shift+T): test()\n\nAlso ran when using, check()\n\nSet-up - Run usethis::use_testthat within the project directory\n\nCreates:\n\nA tests folder in your working directory\nA testthat folder in the tests folder where your tests will live\nA testthat.R file in the tests folder which organizes things for running. Don’t modify this manually.\n\n\nNames of your testing files must begin with ‘test’\n\ne.g. testing file, ‘test-my-function.R’, which lives in testthat folder\n\nWriting Tests\n\nWrite tests using the test_that function, and with each test, there’s an “expectation.”\nYou can have one or more tests in each file and one or more expectations within each test.\nExample\ntest_that(\"description of the test\", {\n        test_output1 &lt;- dat %&gt;%\n            your_package_function()\n\n        expect_equal(nrow(test_output1), 6)\n        # etc...\n})\nExample\n\n\nWhere sse is the function you’re testing\n\nExample expect_error(compute_corr(data = faithful, var1 = erruptions, var2 = waiting)\n\nerruptions isn’t a column in the dataset and should throw an error because of a check inside the function\nAlso expect_warning() available\n\nExample: Compare optional outputs of a nested function to the individual functions within that nested function\n\n\nerror is the nested function with optional outputs of sse error or mape error\nSecond chunk should say “mape calculations work”\n1st chunk checks if error with sse option output is the same as sse output\n2nd chunk checks if error with mape option output is the same as mape output\n\n\nRun Tests\n\nUse the test_file function\n\nUse the test_dir function: e.g. test_dir(wd$test_that) for running all tests.\nPress the Run Tests button in R Studio if you open the test file.\nHighlight and run the code.",
    "crumbs": [
      "Package Development"
    ]
  },
  {
    "objectID": "qmd/git-general.html#config-options",
    "href": "qmd/git-general.html#config-options",
    "title": "25  General",
    "section": "25.2 Config Options",
    "text": "25.2 Config Options\n\nNotes from: Popular git config options - More options listed that are not presented here.\nSetting Options\n\nAdd via CLI: git config --global &lt;name&gt; &lt;value&gt;\n\nExample: git config --global diff.algorithm histogram\n\nDelete by going into ~/.gitconfig and delete the parameter and value\n\nmerge.conflictstyle diff3 - Provides extra information on merge conflicts\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\ndef parse(input):\n    return input.split(\"\\n\")\n||||||| b9447fc\ndef parse(input):\n    return input.split(\"\\n\\n\")\n=======\ndef parse(text):\n    return text.split(\"\\n\\n\")\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; somebranch\n\nBelow &lt;&lt;&lt;&lt;&lt;&lt; HEAD: This is your local code that you’re trying to push\nBetween |||||||| b9447fc and =======: This is the original version of the code\nAbove &lt;&lt;&lt;&lt;&lt;&lt; somebranch: This is code from the branch that got merged before yours (I think)\nTherefore, the correct merge conflict resolution is return text.split(\"\\n\"), since that combines the changes from both sides.\n\nmerge.conflictstyle zdiff3 - A newer version of merge.conflictstyle diff3\nA\nB\nC\nD\nE\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; ours\nF\nG\n||||||| base\n# Add More Letters\n=======\nX\nY\nZ\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; theirs\n\nAbove &lt;&lt;&lt;&lt;&lt;&lt; ours: This is the original code plus the code that belongs to the branch that got merged that is not in conflict with your code\nBelow &lt;&lt;&lt;&lt;&lt;&lt; ours: This is the code that is in conflict with the branch (e.g. main) your merging into.\nBelow |||||||| base: This is the code that has been removed from the original code for both mergers\nAbove &lt;&lt;&lt;&lt;&lt;&lt; theirs: This is code for another branch that was merged before yours that is in conflict with your code.\n\npush.default current - Says that when using git push to always push the local branch to a remote branch with the same name.\n\npush.default simple is the default in Git. Means git push only works if your branch is already tracking a remote branch.\nI guess it’s possible to push a local branch to a remote branch of a different name.\n\ninit.defaultBranch main - Create a main branch instead of a master branch when creating a new repo. I normally do this on Github.\ncommit.verbose true - This adds the whole commit diff in the text editor where you’re writing your commit message, to help you remember what you were doing.\nrerere.enabled true - This enables rerere (”reuse recovered resolution”), which remembers how you resolved merge conflicts during a git rebase and automatically resolves conflicts for you when it can.\ncore.pager delta - The “pager” is what git uses to display the output of git diff, git log, git show, etc.\n\nValues:\n\ndelta: A fancy diff viewing tool with syntax highlighting\nless -x5,9 - Sets tabstops, which I guess helps if you have a lot of files with tabs in them?\nless -F -X - Not sure about this one, -F seems to disable the pager if everything fits on one screen if but her git seems to do that already anyway\ncat - To disable paging altogether\n\nDelta also suggests that you set up interactive.diffFilter delta –color-only to syntax highlight code when you run git add -p.\n\ndiff.algorithm histogram - Improves the Patience algorithm for presenting diffs. See link in article for more details.\n\nDefault (I think the default algorithm is Myers.)\n-.header {\n+.footer {\n     margin: 0;\n }\n\n-.footer {\n+.header {\n     margin: 0;\n+    color: green;\n }\n\nfooter didn’t actually have margin: 0 and color: green in the original code like this diff makes it seem. In reality, the two rules have switched order with header gaining the additional property, color: green.\n\nHistogram\n-.header {\n-    margin: 0;\n-}\n-\n .footer {\n     margin: 0;\n }\n\n+.header {\n+    margin: 0;\n+    color: green;\n+}\n\nThis shows header’s old rule without color: green at the top and being removed. footer is accurately depicted as unchanged. Then, it shows header with the addtional property, color: green, added below footer.\n\n\nincludeIf - Allows you to use different options depending which directory your project is in.\n\nExample: Use this config file only if you’re in the “work” directory\n[includeIf \"gitdir:~/code/&lt;work&gt;/\"]\n    path = \"~/code/&lt;work&gt;/.gitconfig\"\n\nGood if, for example, you want to have a work email set for work repos and personal email for set for personal repos\n\n\ninsteadOf - Useful to correct little mistakes often you make\n\nSee article for other usecases\nExample: If you accidently clone using http when you want to use SSH\n[url \"git@github.com:\"]\n    insteadOf = \"https://github.com/\"\n\nNow when you accidently clone a repo using the http address, it’ll change it to the ssh address in .git/config. Now you’ll be using ssh to push changes which is more secure.\n\n\nSubmodules\nstatus.submoduleSummary true\ndiff.submodule log\nsubmodule.recurse true\n\nSee thread for details\nThe top two “make git status and git diff display some more useful information on how things differ in submodules.”\nThe bottom one aids in the updating of submodules when switching branches\n\ndiff.colorMoved default - Uses different colours to highlight lines in diffs that have been “moved”\n\ndiff.colorMovedWS allow-indentation-change - With diff.colorMoved set, also ignores indentation changes\n\ngpg.format ssh - Allows you to sign commits with SSH keys\nmerge.tool meld (or nvim, or nvimdiff) - Enables use git mergetool to help resolve merge conflicts",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-time-series.html",
    "href": "qmd/feature-engineering-time-series.html",
    "title": "Time Series",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Engineering",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-misc",
    "href": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-misc",
    "title": "Time Series",
    "section": "",
    "text": "Also see\n\nForecasting, ML &gt;&gt; Feature Engineering\n\nPackages\n\n{feasts}\n\nThe package works with tidy temporal data provided by the tsibble package to produce time series features, decompositions, statistical summaries and convenient visualisations. These features are useful in understanding the behaviour of time series data, and closely integrates with the tidy forecasting workflow used in the fable package\n\n{theft}\n\nProvides a single point of access to &gt; 1200 time-series features from a range of existing R and Python packages. The packages which theft ‘steals’ features from currently are:\n\ncatch22 (R; see Rcatch22 for the native implementation on CRAN)\nfeasts (R)\ntsfeatures (R)\nKats (Python)\ntsfresh (Python)\nTSFEL (Python)\n\n\n{timetk}\n\nIncorporates tsfeatures package, timetk::tk_tsfeatures\nExample: Take weekly dataset and compute tsfeatures for each quarter\nnew_dat &lt;- dat %&gt;%\n    mutate(date_rounded = lubridate::round_date(date, \"quarter\")) %&gt;%\n    group_by(date_rounded) %&gt;%\n    timetk::tk_tsfeatures(\n        .date_var = date,\n        .value = price,\n        .features = c(\"median\", \"frequency\", \"stl_features\", \"entropy\", \"acf_features\"),\n        .prefix = \"tsfeat_\"\n    ) %&gt;%\n    ungroup()\n\n.features is for specifying the names of the features from tsfeatures that you want to include\n.prefix is the prefix for the newly created column names\ndate_rounded is a column that has the date for each quarter\n\n\n{fractaldim} (paper) - The fractal dimension of a series measures its roughness or smoothness.\n{tsfeatures}\n\nExample\nlibrary(tsfeatures)\nts_fts &lt;- \n  tsfeatures(\n    ts_data,\n    features = c(\n        \"acf_features\", \"outlierinclude_mdrmd\",\n        \"arch_stat\", \"max_level_shift\",\n        \"max_var_shift\", \"entropy\",\n        \"pacf_features\", \"firstmin_ac\",\n        \"std1st_der\", \"stability\",\n        \"firstzero_ac\", \"hurst\",\n        \"lumpiness\", \"motiftwo_entro3\"\n      )\n  )\n\n{{kats}}\n\nTime series analysis, including detection, forecasting, feature extraction/embedding, multivariate analysis, etc. Kats is released by Facebook’s Infrastructure Data Science team.\n\n{{pytimetk}} - Python version of {timetk}\n{{temporian}} - Similar to {{pytimetk}}\nhctsa - a Matlab software package for running highly comparative time-series analysis. It extracts thousands of time-series features from a collection of univariate time series and includes a range of tools for visualizing and analyzing the resulting time-series feature matrix. Can be ran through CLI. Calculates like 7000 features.\n\ncatch22 (paper) extracts only 22 canonical features (so much faster) used in hctsa and can be used in R {Rcatch22}, Python {{pycatch22}}, or Julia Catch22.jl.\n\n\nIssue: Features that aren’t available during the forecast horizon aren’t useful.\n\nSolutions:\n\nBut you can use a lagged value of predictor or an aggregated lagged value e.g. averages, rolling averages, tiled/windowed averages\n\nExample: average daily customers for the previous week.\n\nSome features are one value per series, but the functions could be fed a lagged window (length of horizon?) of the whole series and generate a value for each window.\n\n\nDifference features that are really linear or have little variation. Change in value can be more informative\nForecasting shocks is difficult for an algorithm\n\nIt can better to smooth out (expected) shocks (Christmas) in the training data and then add an adjustment to the predictions during the dates of the shocks.\nThe smoothed out data will help the algorithm produce more accurate predictions for days when there isn’t an expected shock.\nExamples of shocks that may need training data to have manual adjustments and not be smoothed by an algorithm\n\none-time spikes due to abnormal weather conditions\none-off promotions\na sustained marketing campaign that is indistinguishable from organic growth.\n\n\nModels with large numbers (100s) of features increases the opportunity for feature drift\n\nPackage feature set comparison\n\nFrom paper, An Empirical Evaluation of Time Series Feature Sets\nMisc\n\nI think feasts ⊆ tsfeatures ⊆ catch22 ⊆ hctsa\nkats is a facebook python library\ntheft package integrates all these packages\n\nfeature redundancy\n\n\nSays catch22 features have fewer things in common with each other that the other packages\n\nComputation time\n\n\nNo surprise hctsa takes the most time. It’s like 7K features or something stupid\ntsfeatures,feasts are pretty disappointing\ncatch22 is excellent",
    "crumbs": [
      "Feature Engineering",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-tran",
    "href": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-tran",
    "title": "Time Series",
    "section": "Tranformations",
    "text": "Tranformations\n\nLogging a feature can create more compact ranges, which then enables more efficient neural network training\nLog before differencing (SO post)\nstep_normalize(all_predictors)\n\nActually standardizes the variables\nIf you’re using predictors on different scales\n\nlm (and AR) are scale invariant, so not really necessary for those models\n\n\nSmoothing\n\nLOESS (LOcally WEighted Scatter-plot Smoother)\n\nWeights are applied to the neighborhood of each point which depend on the distance from the point\nA polynomial regression is fit at each data point with points in the neighborhood as explanatory variables\nSome robustness to outliers (by downweighting large residuals and refitting)\nspan: the distance from each data that considered the neighborhood is controlled by this argument\n\nDefault: 0.75\n&lt; 1: the value represents the proportion of the data that is considered to be neighbouring x, and the weighting that is used is proportional to 1-(distance/maximum distance)3)3, which is known as tricubic\nChoosing a value that’s too small will result in insufficient data near x for an accurate fit, resulting in a large variance\nChoosing a value that’s too large will result in over-smoothing and a loss of information, hence a large bias.\n\ndegree: degree of the polynomial regression used to fit the neighborhood data points\n\nDefault: 2 (quadratic)\nHigh degree: provides a better approximation of the population mean, so less bias, but there are more factors to consider in the model, resulting in greater variance.\n\nHigher than 2 typically doesn’t improve the fit very much.\n\nLower degree: (i.e. 1, linear) has more bias but pulls back variance at the boundaries.\n\nExample: {ggplot}\nggplot(data, aes(x = time, y = price)) +\n    geom_line(alpha = 0.55, color = \"black\") + \n    geom_smooth(aes(color = \"loess\"), formula = y ~ x, method = \"loess\", se = FALSE, span = 0.70) +\n    scale_color_manual(name = \"smoothers\", values = c(\"ma\" = \"red\", \"loess\" = \"blue\"))\n\n\nExample: base r\nloess_mod &lt;- stats::loess(price ~ time, data = dat, span = 0.05, degree = 4)\ndat$loess_price &lt;- fitted(loess_mod)\nCubic Regression Splines\n\nExample: {mgcv}: mgcv::gam(price ~ s(time, bs = \"cs\"), data = data, method = \"REML\")$fitted.values",
    "crumbs": [
      "Feature Engineering",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-eng",
    "href": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-eng",
    "title": "Time Series",
    "section": "Engineered",
    "text": "Engineered\n\nMisc\n\nTidymodels\n\ndate variable needs to be role = ID for ML models\n\n\n\n\nDecimal Date\n\nrecipe::step_date(year_month_var, features = c(\"decimal\"))\n\n\n\nCalendar Features\n\nCalandar Variables\n\nday of the month, day of the year, week of the month, week of the year, month, and year\nhour of the week (168 hours/week)\nminute, hour\nmorning/afternoon/ night\nExample: step_date, step_time\nexample_data &lt;- tibble(date = Sys.time() + 9 ^ (1:10))\n\nrecipe(~ ., data = example_data) |&gt;\n  step_date(all_datetime(), \n            features = c(\"year\", \"doy\", \"week\", \"decimal\", \"semester\", \n                         \"quarter\", \"dow\", \"month\")) |&gt;\n  step_time(all_datetime(),\n            features = c(\"am\", \"hour\", \"hour12\", \"minute\", \"second\", \n                         \"decimal_day\")) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\n#&gt; Rows: 10\n#&gt; Columns: 15\n#&gt; $ date             &lt;dttm&gt; 2023-12-07 11:46:02, 2023-12-07 11:47:14, 2023-12-07…\n#&gt; $ date_year        &lt;int&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2024, 2025, 2036,…\n#&gt; $ date_doy         &lt;int&gt; 341, 341, 341, 341, 342, 347, 31, 108, 77  155\n#&gt; $ date_week        &lt;int&gt; 49, 49, 49, 49, 49, 50, 5, 16, 11, 23\n#&gt; $ date_decimal     &lt;dbl&gt; 2023.933, 2023.933, 2023.933, 2023.933, 2023 .935, 202…\n#&gt; $ date_semester    &lt;int&gt; 2, 2, 2, 2, 2, 2, 1, 1, 1, 1\n#&gt; $ date_quarter     &lt;int&gt; 4, 4, 4, 4, 4, 4, 1, 2, 1, 2\n#&gt; $ date_dow         &lt;fct&gt; Thu, Thu, Thu, Thu, Fri, Wed, Wed, Fri, Mon,  Fri\n#&gt; $ date_month       &lt;fct&gt; Dec, Dec, Dec, Dec, Dec, Dec, Jan, Apr, Mar,  Jun\n#&gt; $ date_am          &lt;lgl&gt; TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE,  FALSE, F…\n#&gt; $ date_hour        &lt;int&gt; 11, 11, 11, 13, 4, 15, 20, 18, 13, 19\n#&gt; $ date_hour12      &lt;int&gt; 11, 11, 11, 1, 4, 3, 8, 6, 1, 7\n#&gt; $ date_minute      &lt;int&gt; 46, 47, 58, 35, 10, 23, 22, 11, 34, 59\n#&gt; $ date_second      &lt;dbl&gt; 2.890565, 14.890565, 2.890565, 14.890565, 2 .890565, 1…\n#&gt; $ date_decimal_day &lt;dbl&gt; 11.76747, 11.78747, 11.96747, 13.58747, 4 .16747, 15.3…\nmodeltime::step_timeseries_signature creates a similar set of calendar features\n\nDaylight Savings - At one point in the year, we have 23 hours in a day, and in another time, we have 25 hours in a day\n\nIf using a smooth::adam model, then it shifts seasonal indices, when the time change happens. All you need to do for this mechanism to work is to provide an object with timestamps to the function (for example, zoo).\n\nLeap Year\n\nBecomes less important when we model week of year seasonality instead of the day of year or hour of year\n\nHolidays\n\nstep_holiday_signature\n# Sample Data\ndates_in_2017_tbl &lt;- tibble::tibble(\n    index = tk_make_timeseries(\"2017-01-01\", \"2017-12-31\", by = \"day\")\n)\n\n# Add US holidays and Non-Working Days due to Holidays\n# - Physical Holidays are added with holiday pattern (individual) and locale_set\nrec_holiday &lt;- recipe(~ ., dates_in_2017_tbl) %&gt;%\n    step_holiday_signature(index,\n                           holiday_pattern = \"^US_\",\n                           locale_set      = \"US\",\n                           exchange_set    = \"NYSE\")\nbake(rec_holiday_prep, dates_in_2017_tbl)\n#&gt; # A tibble: 365 × 21\n#&gt;    index      index_exch_NYSE index_locale_US index_US_NewYearsDay\n#&gt;    &lt;date&gt;               &lt;dbl&gt;           &lt;dbl&gt;                &lt;dbl&gt;\n#&gt;  1 2017-01-01               0               1                    1\n#&gt;  2 2017-01-02               1               0                    0\n#&gt;  3 2017-01-03               0               0                    0\n#&gt;  4 2017-01-04               0               0                    0\n#&gt;  5 2017-01-05               0               0                    0\n#&gt;  6 2017-01-06               0               0                    0\n#&gt;  7 2017-01-07               0               0                    0\n#&gt;  8 2017-01-08               0               0                    0\n#&gt;  9 2017-01-09               0               0                    0\n#&gt; 10 2017-01-10               0               0                    0\n#&gt; # ℹ 355 more rows\n#&gt; # ℹ 17 more variables: index_US_MLKingsBirthday &lt;dbl&gt;,\n#&gt; #   index_US_InaugurationDay &lt;dbl&gt;, index_US_LincolnsBirthday &lt;dbl&gt;,\n#&gt; #   index_US_PresidentsDay &lt;dbl&gt;, index_US_WashingtonsBirthday &lt;dbl&gt;,\n#&gt; #   index_US_CPulaskisBirthday &lt;dbl&gt;, index_US_GoodFriday &lt;dbl&gt;,\n#&gt; #   index_US_MemorialDay &lt;dbl&gt;, index_US_DecorationMemorialDay &lt;dbl&gt;,\n#&gt; #   index_US_IndependenceDay &lt;dbl&gt;, index_US_LaborDay &lt;dbl&gt;, …\n\nIndicators for holidays based on locales\nIndicators for when business is off based on stock exchanges\n\n\nAs splines\n\nExample: {tidymodels}\nstep_mutate(release_year = year(release_date),\n            release_week = week(release_date)) %&gt;%\nstep_ns(release_year, deg_free = tune(\"deg_free_year\")) %&gt;%\nstep_ns(releas_week, deg_free = tune(\"deg_free_week\"))\n\nMay need lubridate loaded for the mutate part\nCan also use a basis spline (step_bs)\n\nExample: {mgcv}\n\nctamm &lt;- \n  gamm(temp ~ s(day.of.year, bs = \"cc\", k=20) + s(time, bs = \"cr\"),\n       data = cairo,\n       correlation = corAR1(form = ~1|year))\n\nFrom pg 371, “Generalized Additive Models: An Introduction with R, 2nd Ed” (See R/Documents/Regression)\nHighly seasonal so uses a cyclic penalized cubic regression spline for “day.of.year”\n10 peaks and 10 valleys probably explains “k = 20”\n\nWith regression models, you have to be careful about encoding categoricals/discretes as ordinal (i.e. integers). Linear regression does not model non-monotonic relationships between the input features and the target while tree models do.\n\nFor example, the raw numerical encoding (0-24) of the “hour” feature prevents the linear model from recognizing that an increase of hour in the morning from 6 to 8 should have a strong positive impact on the number of bike rentals while a increase of similar magnitude in the evening from 18 to 20 should have a strong negative impact on the predicted number of bike rentals.\nOptions\n\nOne-hot encoding for small cardinality (e.g. hours) and binning for large cardinality features (e.g. minutes)\nSpline tranformation\n\nReduces number of features with comparable performance to one-hot\nPeriod isn’t used, just here (examples) for reference to the number of splines chosen\nnum_knots = num_splines + 1\nDegree 3 was used in the sklearn tutorial\n\nSKLearn also has an extrapolation=“periodic” arg\n\n“Periodic splines with a periodicity equal to the distance between the first and last knot are used. Periodic splines enforce equal function values and derivatives at the first and last knot. For example, this makes it possible to avoid introducing an arbitrary jump between Dec 31st and Jan 1st in spline features derived from a naturally periodic”day-of-year” input feature. In this case it is recommended to manually set the knot values to control the period.”\nDon’t see this arg in step_bs or step_ns\n\n\nExamples\n\nHour feature (period = 24, num_splines = 12)\nWeekday (day of the week, numeric) feature (period=7, num_splines=3)\nMonth (period=12, num_splines=6)\n\n\nSpline transform + step_kpca_poly or step_kpca_rbf\n\nProduced best results in sklearn tutorial\nKernel function smooths out the spline\nAdd kpca_poly allows a regression model to capture non-linear (spline) interactions (kpca_poly)\n\nBoosted trees naturally capture these features\n\nExample in sklearn tutorial used n_components=300 in elasticnet regression which seems crazy, but their point was that if you were to create non-linear interaction features manually it’d be in the thousands\nUsing one-hot features instead of splines would require 3 or 4 times the number of components to reach the same performance which substantially increases training time.\n\n\n\n\n\n\n\nClustering\n\nSee Clustering, Time Series for details\n\n\n\nLags\n\nIf there is a gap period between the training and the validation (or test) set, all lags should be larger than this period\n\nSometimes predictions will have to be made with data that isn’t up-to-date. So your model training should mimic this.\n\n{recipe}\nrecipe::step_lag(var, lag: 4:7)\n\nCreates 4 lagged variables with lags 4:7\n\n{slider} - more sophisticated way without data leakage\nSCORE_recent &lt;- \n  slider::slide_index_dbl(SCORE,\n                          date,\n                          mean,\n                          na.rm = TRUE,\n                          .before = lubridate::days(365*3),\n                          .after = -lubridate::days(1),\n                          .complete = FALSE)\n\nafter is -lubridate::days(1) says don’t include current day\n\n“Prevents data leakage by ensuring that this feature does not include information from the current day in its calculation”\n\nNot sure exactly why including the current day would cause data leakage\n\n\n\n{dplyr} + {purrr}\ncalculate_lags &lt;- function(df, var, lags){\n  map_lag &lt;- lags %&gt;% map(~partial(lag, n = .x))\n  return(df %&gt;% mutate(across(.cols = [{{var}}]{style='color: goldenrod'}, .fns = map_lag, .names = \"{.col}_lag[{lags}]{style='color: #990000'}\")))\n}\ntsla %&gt;%\n  calculate_lags(close, 1:3)\n\n\n\nRolling\n\n{feasts} functions\n\nTiling (non-overlappping) features\nSliding window (overlapping) features\n\nExponentially weighted moving average on lags (more recent values get more weight)\n\nHelps smooth out you lags if your data is noisy\nMight be easier to just smooth the outcome\nH2O’s weight parameter, alpha,  has a range between 0.9 and 0.99\n\nExample: smoothed over 2 days\n\nsales, 1 day ago = 3; 2 days ago = 4.5; and alpha = 0.95\nsmoothed sales = [3.0*(0.95^1) + 4.5*(0.95^2)] / [(0.95^1) + (0.95^2)] = 3.73\n\n\n\nMoving Averages\ndt_ma = data.table::frollmean(data[, 2], n = window, align = \"right\", fill = NA, algo = \"fast\")\nrcpp_ma = RcppRoll::roll_mean(data[, 2], n = window, align = \"right\", fill = NA\n\n{data.table} looks to be slightly faster than {RcppRoll} in a benchmark\n\nBoth are substantially faster than {TTR} and base R. {zoo}’s was slowest\n\n{TTR} produced different values (floating point precision) than the other packages\n\n\n\nDescriptive statistics\n\nExample: (weekly) units sold\n\nMetrics: mean, standard deviation, minimum, maximum, 25th percentile, 50th percentile, and 75th percentile\nRolling time windows: 1, 2, 3, 4, 12, 26, 52 weeks prior\n\nExample: rolling average sales from the previous year\n\ne.g. 2-week average sales of the previous year\n\n\n\n\n\nInteractions\n\nExamples\n\nInteractions between lags\nBetween workday (indicator) and hour\n\nDistinguishes between 8am on a monday and 8am on a sunday\nSpline transform hour and create interaction with workday\nMain effects(x + y) and interactions (xy) only are included, not variables of the form, x2\n\n\n\n\n\n“Days Since” a Specific Date\n\nCheck scatter (e.g. price vs last review date) and see if variance is heteroskedastic but should probably be log transformed\n# think this data only has year and month variables (numeric?)\nstep_mutate(days_since = lubridate::today() - lubridate::ymd(year, month, \"01\"))\n\n# last review is a date var\nstep_mutate(last_review = as.integer(Sys.Date() - last_review))\nExample: {lubridate}\nlength_of_stay &lt;- start_date %--% end_date / ddays(1) \nis_los_g90 &lt;- start_date %--% current_date &gt;= ddays(90)\n** consider using the inverse: 1 / number of days since **\n\nThis keeps the value between 0 and 1\nHave to watch out for 0s in the denominator and replace those values with 0 or replace the count with a really small fraction\n\nExamples\n\nLog days since the brand or product first appeared on the market\nThe number of weeks since the product was last sold\n\n\n\n\nInterval Groups\n\nCreating group indicators for different intervals in the series\nProbably involves some sort of clustering of the series or maybe this is what “wavelets” are.\n\n\n\nFourier Transform (sine, cosine)\n\nDecision trees based algorithms (Random Forest, Gradient Boosted Trees, XGBoost) build their split rules according to one feature at a time. This means that they will fail to process these two features simultaneously whereas the cos/sin values are expected to be considered as one single coordinates system.\nHandled differently in different articles so ¯\\_(ツ)_/¯\nFrom https://towardsdatascience.com/how-to-handle-cyclical-data-in-machine-learning-3e0336f7f97c\n\\[\n\\begin{align}\n\\text{Hour}_{\\sin} &= \\sin \\left(\\frac{2\\pi\\cdot\\text{Hour}}{\\max (\\text{Hour})}\\right) \\\\\n\\text{Hour}_{\\cos} &= \\cos \\left(\\frac{2\\pi\\cdot\\text{Hour}}{\\max (\\text{Hour})}\\right)\n\\end{align}\n\\]\n\nExample: Tranforming an hour variable instead one-hot encoding\nThe argument is that since time features are cyclical, their transform should be a cyclical function. This way the difference between transformed 1pm and 2pm values are more closely related than if they were one-hot encoded where the difference between 1pm and 2pm is the same as 1pm and 8pm.\n\nFrom sklearn article\nperiod &lt;- 24 # hours variable\nvar &lt;- dat$hour           \nsin_encode &lt;- function (x, period) {sin(x / period * 2 * pi)}\ncos_encode &lt;- function (x, period) {cos(x / period * 2 * pi)}\nsin_encode(x = var, period = period)\nstep_harmonic\nexample_data &lt;- tibble(\n  year = 1700:1988,\n  n_sunspot = sunspot.year\n)\n\nsunspots_rec &lt;- \n  recipe(n_sunspot ~ year, data = sun_train) |&gt; \n    step_harmonic(year, \n                  frequency = 1 / 11, \n                  cycle_size = 1, # sunspots happen once every 11 yrs\n                  role = \"predictor\",\n                  keep_original_cols = FALSE) |&gt; \n    prep() |&gt;\n    bake(new_data = NULL)\n\n#&gt; # A tibble: 289 × 3\n#&gt;    n_sunspot year_sin_1 year_cos_1\n#&gt;        &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1         5  -2.82e- 1     -0.959\n#&gt;  2        11  -7.56e- 1     -0.655\n#&gt;  3        16  -9.90e- 1     -0.142\n#&gt;  4        23  -9.10e- 1      0.415\n#&gt;  5        36  -5.41e- 1      0.841\n#&gt;  6        58   6.86e-14      1    \n#&gt;  7        29   5.41e- 1      0.841\n#&gt;  8        20   9.10e- 1      0.415\n#&gt;  9        10   9.90e- 1     -0.142\n#&gt; 10         8   7.56e- 1     -0.655\n#&gt; # ℹ 279 more rows\n\n\n\nChange Point Detection\n\nPersistent Homology Notes from Topological Change Point Detection\n\nTopological method\nClassifies the state of a set of features over a sliding window where the states are normal (incoherent), onset (partially coherent), synchronized (fully coherent)\nPython has a library that does this stuff, {geotto-tda}\n\nFor each window a 2-D pearson dissimilarity matrix is computed and a Vietoris-Rips persistence score is calculated “up to homology 2.” Repeat for each window.\nThe amplitude, persistent entropy and number of diagram points per homology dimension are calculated for each resulting persistence diagram resulting in a feature vector (or maybe its a matrix)\nTuning parameters\n\nWindow Size\nStride (how many steps the window slides)\nCoherence threshold (classification threshold for “synchronized”)\n\nExample: 0.8\n\nOnset threshold (classification threshold for “onset”)\n\nExample: 0.5\n\n\n\nUse the trained model to predict classification categories that can be used as a feature.\nThis state is supposed to be predictive of whether a change point is about to happen in the time series\n\nBayesian Change Point Indicator Regression\n\n\n\nDomain Specific\n\nNet calculation: recipe::step_mutate(monthly_net = monthly_export - monthly_import)\nAmount of the last sale\nTotal volume of units sold for each product up to that date\n\nIndicates of long-term historical sales performance\n\nCustomer Age - How long a customer has been with the company\ndat_prep_tbl &lt;- \n  dat_raw |&gt; \n    mutate(dt_customer = dmy(dt_customer),\n           dt_customer_age = -1 * (dt_customer - min(dt_customer)) / ddays(1)) |&gt;\n    select(-dt_customer)\n\nSubtracts the minimum of the customer-first-bought-from-the-company date variable from each customer’s first-bought date.\ndt_customer is a date the customer first bought from the company but in the raw dataset was a character type, so lubridate::dmy coerces it to a Date type\nWhy multiply by -1 instead of reversing the objects being substracted? Why divide by ddays(1)? I dunno. The resultant object is a dbl type, so maybe it’s a formatting thing.\n\n\n\n\nMissing Values\n\nWhen aggregating values (e.g. daily to weekly), information about missing values is lost\nCreate a variable that is the sum of missing values over the aggregated period\nMissing rows in the original dataset (due to a lack of sales on those particular dates) can also be counted and added to the dataset",
    "crumbs": [
      "Feature Engineering",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/python-pandas.html",
    "href": "qmd/python-pandas.html",
    "title": "Pandas",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Python",
      "Pandas"
    ]
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-misc",
    "href": "qmd/python-pandas.html#sec-py-pandas-misc",
    "title": "Pandas",
    "section": "",
    "text": "Examples of using numeric indexes to subset dfs\n\nindexes &gt;&gt; Syntax for using indexes\nselect\nfiltering &gt;&gt; .loc/.iloc\n\ndf.head(8) to see the first 8 rows\ndf.info() is like str in R\ndf.describe() is like summary in R\n\ninclude='all' to include string columns",
    "crumbs": [
      "Python",
      "Pandas"
    ]
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-series",
    "href": "qmd/python-pandas.html#sec-py-pandas-series",
    "title": "Pandas",
    "section": "Series",
    "text": "Series\n\na vector with an index (ordered data object)\ns = pd.Series([20, 21, 12],\n              index=['London', 'New York', 'Helsinki'])\n&gt;&gt; s\nLondon      20\nNew York    21\nHelsinki    12\ndtype: int64\nMisc\n\nTwo Pandas Series with the same elements in a different order are not the same object\nList vs Series: list can only use numeric indexes, while the Pandas Series also allows textual indexes.\n\nArgs\n\ndata: Different data structures can be used, such as a list, a dictionary, or even a single value.\nindex: A labeled index for the elements in the series can be defined.\n\nIf not set, the elements will be numbered automatically, starting at zero.\n\ndtype: Sets the data types of the series\n\nUseful if all data in the series are of the same data type\n\nname: Series can be named.\n\nUseful if the Series is to be part of a DataFrame. Then the name is the corresponding column name in the DataFrame.\n\ncopy: True or False; specifies whether the passed data should be saved as a copy or not\n\nSubset: series_1[\"A\"] or series_1[0]\nFind the index of a value: list(series_1).index(&lt;value&gt;)\nChange value of an index: series_1[\"A\"] = 1 (1 is now the value for index, A)\nAdd a value to a series: series_1[\"D\"] = \"fourth_element\" (D is the next index in the sequence)\nFilter by condition(s): series_1[series_1 &gt; 4] or series_1[series_1 &gt; 4][series_1 != 8]\ndict to Series: pd.Series(dict_1)\nSeries to df: pd.DataFrame([series_1, series_2, series_3])\n\nSeries objects should either all have the same index or no index. Otherwise, a separate column will be created for each different index, for which the other rows have no value",
    "crumbs": [
      "Python",
      "Pandas"
    ]
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-cats",
    "href": "qmd/python-pandas.html#sec-py-pandas-cats",
    "title": "Pandas",
    "section": "Categoricals",
    "text": "Categoricals\n\nMisc - Also see - Conversions for converting between types - Optimizations &gt;&gt; Memory Optimizations &gt;&gt; Variable Type\nCategorical (docs)\n\npython version of factors in R\n\nR’s levels are always of type string, while categories in pandas can be of any dtype.\nR allows for missing values to be included in its levels (pandas’ categories). pandas does not allow NaN categories, but missing values can still be in the values.\n\nSee cat.codes below\n\n\nCreate a categorical series: s = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\nCreate df of categoricals\ndf = pd.DataFrame({\"A\": list(\"abca\"), \"B\": list(\"bccd\"){style='color: #990000'}[}]{style='color: #990000'}, dtype=\"category\")\ndf[\"A\"]\n0    a\n1    b\n2    c\n3    a\n\nSpecify categories and add to dataframe (also see Set Categories below)\nraw_cat = pd.Categorical(\n    [\"a\", \"b\", \"c\", \"a\"], categories=[\"b\", \"c\", \"d\"], ordered=False   \n)\ndf[\"B\"] = raw_cat\ndf \n  A    B\n0  a  NaN\n1  b    b\n2  c    c\n3  a  NaN\n\nNote that categories not in the specification get NaNs\n\nSee categories, check if ordered: cat.categories, cat.ordered\ns.cat.categories\nOut[61]: Index(['c', 'b', 'a'], dtype='object')\ns.cat.ordered\nOut[62]: False\nSet categories for a categorical: s = s.cat.set_categories([\"one\", \"two\", \"three\", \"four\"])\nRename categories w/cat.rename_categories\n# with a list of new categories\nnew_categories = [\"Group %s\" % g for g in s.cat.categories]\ns = s.cat.rename_categories(new_categories)\n\n# with a dict\ns = s.cat.rename_categories({1: \"x\", 2: \"y\", 3: \"z\"})\ns\n0    Group a\n1    Group b\n2    Group c\n3    Group a\nAdd a category (doesn’t have to be a string, e.g. 4): s = s.cat.add_categories([4])\nRemove categories\ns = s.cat.remove_categories([4])\ns.cat.remove_unused_categories()\nOrdered\n\nCreate ordered categoricals\nfrom pandas.api.types import CategoricalDtype\ns = pd.Series([\"a\", \"b\", \"c\", \"a\"])\ncat_type = CategoricalDtype(categories=[\"b\", \"c\", \"d\"], ordered=True)\ns_cat = s.astype(cat_type)\ns_cat \n0    NaN\n1      b\n2      c\n3    NaN\ndtype: category\nCategories (3, object): ['b' &lt; 'c' &lt; 'd']\n\n# alt\ns = pd.Series([\"a\", \"b\", \"c\", \"a\"]).astype(CategoricalDtype(ordered=True))\nReorder: cat.reorder_categories\ns = pd.Series([1, 2, 3, 1], dtype=\"category\")\ns = s.cat.reorder_categories([2, 3, 1], ordered=True)\ns\n0    1\n1    2\n2    3\n3    1\ndtype: category\nCategories (3, int64): [2 &lt; 3 &lt; 1]\n\nCategory codes: .cat.codes\ns = pd.Series([\"a\", \"b\", np.nan, \"a\"], dtype=\"category\")\ns\n0      a\n1      b\n2    NaN\n3      a\ndtype: category\nCategories (2, object): ['a', 'b']\n\ns.cat.codes\n0    0\n1    1\n2  -1\n3    0\ndtype: int8",
    "crumbs": [
      "Python",
      "Pandas"
    ]
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-ops",
    "href": "qmd/python-pandas.html#sec-py-pandas-ops",
    "title": "Pandas",
    "section": "Operations",
    "text": "Operations\n\nRead\n\nMisc\n\nFor large datasets, better to use data.table::fread (see Python, Misc &gt;&gt; Misc)\n\nCSV df = pd.read_csv('wb_data.csv', header=0)\n\n“usecols=[‘col1’, ‘col8’]” for only reading certain columns\n\nRead and process data in chunks\nfor chunk in pd.read_csv(\"dummy_dataset.csv\", chunksize=50000): \n    print(type(chunk)) # process data\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\nIssue: Cannot perform operations that need the entire DataFrame. For instance, say you want to perform a groupby() operation on a column. Here, it is possible that rows corresponding to a group may lie in different chunks.\n\n\n\n\nWrite\n\nCSV: df.to_csv(\"file.csv\", sep = \"|\", index = False)\n\n“sep” - column delimiter (assume comma is default)\n“index=False” -  instructs Pandas to NOT write the index of the DataFrame in the CSV file\n\n\n\n\nCreate/Copy\n\nDataframes\n\nSyntaxes\ndf = pd.DataFrame({\n    \"first_name\": [\"John\",\"jane\",\"emily\",\"Matt\",\"Alex\"],\n    \"last_name\": [\"Doe\",\"doe\",\"uth\",\"Dan\",\"mir\"],\n    \"group\": [\"A-12\",\"B-15\",\"A-18\",\"A-12\",\"C-15\"],\n    \"salary\": [\"$75000\",\"$72000\",\"£45000\",\"$77000\",\"£58,000\"]\n})\n\ndf = pd.DataFrame(\n    [\n        (1, 'A', 10.5, True),\n        (2, 'B', 10.0, False),\n        (3, 'A', 19.2, False)       \n    ],\n    columns=['colA', 'colB', 'colC', 'colD']\n)\n\ndata = pd.DataFrame([[\"A\", 1], [\"A\", 2], [\"B\", 1], \n                    [\"C\", 4], [\"A\", 10], [\"B\", 7]], \n                    columns = [\"col1\", \"col2\"])\n\ndf = pd.DataFrame([('bird', 389.0),\n                  ('bird', 24.0),\n                  ('mammal', 80.5),\n                  ('mammal', np.nan)],\n                  index=['falcon', 'parrot', 'lion', 'monkey'],\n                  columns=('class', 'max_speed'))\ndf\n        class  max_speed\nfalcon    bird      389.0\nparrot    bird      24.0\nlion    mammal      80.5\nmonkey  mammal        NaN\nCreate a copy of a df: df2 = df1.copy()\n\n\n\n\nIndexes\n\nMisc\n\nRow labels\nDataFrame indexes do NOT have to be unique\nThe time it takes to search a dataframe is shorter with unique index\nBy default, the index is a numeric row index\n\nSet\ndf = df.set_index('col1')\ndf.set_index(column, inplace=True)\n\nprevious index is discarded\ninplace=True says modify orginal df\n\nChange indexes but keep previous index\ndf.reset_index() # converts index to a column, index now the default numeric\ndf = df.set_index('col2')\n\nreset_index\ninplace=True says modify orginal df (default = False)\ndrop=True discards old index\n\nChange to new indices or expand indices: df.reindex\nSee indexes: df.index.names\nSort: df.sort_index(ascending=False)\nSyntax for using indexes\n\nMisc\n\n“:”  is not mandatory as a placeholder for the column position\n\n.loc\n\ndf.loc[one_row_label] # Series\ndf.loc[list_of_row_labels] # DataFrame\ndf.loc[:, one_column_label] # Series\ndf.loc[:, list_of_column_labels] # DataFrame\ndf.loc[first_row:last_row]\n\n.iloc\n\ndf.iloc[one_row_position, :] # Series\ndf.iloc[list_of_row_positions, :] # DataFrame\n\n\nExample: df.iloc[0:3] or df.iloc[:3]\n\noutputs rows 0, 1, 2  (end point NOT included)\n\nExample: df.iloc[75:]\n\n(e.g. 78 rows) outputs rows 75, 76, 77 (end point, i.e., last row, NOT included)\n\nExample: df.loc['The Fool' : 'The High Priestess']\n\noutputs all rows from “The Fool” to “The High Priestess” (end point included)\n\nExample: Filtering cells\ndf.loc[\n    ['King of Wands', 'The Fool', 'The Devil'],\n    ['image', 'upright keywords']\n]\n\nindex values: ‘King of Wands’, etc.\ncolumn names: ‘image’, etc.\n\nMulti-indexes\n\nUse list of variables to create multi-index: df.set_index(['family', 'vowel_inventories'], inplace=True)\n\n\n“family” is the 0th index; “vowel_inventories” is the 1st index\n\nSee individual index values: df.index.get_level_values(0)\n\nGets the 0th index values (e.g. “family”)\n\nSort by specific level: df.sort_index(level=1)\n\nNot specifying a level means df will be sorted first by level 0, then level 1, etc.\n\nReplace index values: df.rename(index={\"Arawakan\" : \"Maipurean\", \"old_value\" : \"new_value\"})\n\nReplaces instances in all levels of the multi-index\nFor only a specific level, use “levels=‘level_name’”\n\nSubsetting via multi-index\n\nBasic\n\n\nrow_label = ('Indo-European', '2 Average (5-6)')\ndf.loc[row_label, :].tail(3)\nNote: Using “:” or specifying columns may be necessary using a multi-index\nWith multiple multi-index values\nrow_labels = [\n    ('Arawakan', '2 Average (5-6)'),\n    ('Uralic', '2 Average (5-6)'),\n]\nrow_labels = (['Arawakan', 'Uralic'], '2 Average (5-6)') # alternate method\ndf.loc[row_labels, :]\nWith only 1 level of the multi-index\nrow_labels = pd.IndexSlice[:, '1 Small (2-4)']\ndf.loc[row_labels, :]\n\n# drops other level of the multi-index\ndf.loc['1 Small (2-4)']\n\n# more verbose\ndf.xs('1 Small (2-4)', level = 'vowel_inventories', drop_level=False)\n\nIgnores level 0 of the multi-index and filters only by level 1\nCould also use an alias, e.g. idx = pd.IndexSlice if writing the whole thing gets annoying\n\n\n\n\n\nSelect\n\nAlso see Indexes\nSelect by name:\ndf[[\"name\",\"note\"]]\n\ndf.filter([\"Type\", \"Price\"])\n\ndat.loc[:, [\"f1\", \"f2\", \"f3\"]]\n\ntarget_columns = df.columns.str.contains('\\+') # boolean array: Trues, Falses\nX = df.iloc[:, ~target_columns]\nY = df.iloc[:, target_columns]\n\n“:” is a place holder in this case\n\nSubset\nts_df\n    t-3  t-2  t-1  t-0\n10  10.0  11.0  12.0  13.0\n11  11.0  12.0  13.0  14.0\n12  12.0  13.0  14.0  15.0\nAll but last column\nts_df.iloc[:, :-1]\n    t-3  t-2  t-1\n10  10.0  11.0  12.0\n11  11.0  12.0  13.0\n12  12.0  13.0  14.0\n\nFor [:, :-2], the last 2 columns would not be included \n\nAll but first column\nts_df.iloc[:, 1:]\n    t-2  t-1  t-0\n10  11.0  12.0  13.0\n11  12.0  13.0  14.0\n12  13.0  14.0  15.0\n\ni.e. the 0th indexed column is not included\nFor [:, 2:], the 0th and 1st indexed column would not be included\n\nAll columns between the 2nd col and 4th col\nts_df.iloc[:, 1:4]\n    t-2  t-1  t-0\n10  11.0  12.0  13.0\n11  12.0  13.0  14.0\n12  13.0  14.0  15.0\n\nleft endpoint (1st index) is included but the right endpoint (4th index) is not\ni.e. the 1st, 2nd, and 3rd indexed columns are included\n\nAll columns except the first 2 cols and the last col\nts_df.iloc[:, 2:-1]\n    t-1\n10  12.0\n11  13.0\n12  14.0\n\ni.e. 0th and 1st indexed columns not included and “-1” says don’t include the last column\n\n\n\n\nRename columns\ndf.rename({'variable': 'Year', 'value': 'GDP'}, axis=1, inplace=True)\n-   \"variable\" is renamed to \"Year\" and \"value\" is renamed to \"GDP\"\n\n\nDelete columns\n\ndf.drop(columns = [\"col1\"])\n\n\n\nFiltering\n\nMisc\n\nAlso see Indexes\nGrouped and regular pandas data frames have different APIs, so it’s not a guarantee that a method with the same name will do the same thing. 🙃\n\nOn value: df_filtered = data[data.col1 == \"A\"]\nVia query: df_filtered = data.query(\"col1 == 'A'\")\n\nMore complicated\npurchases\n  .query(\"amount &lt;= amount.median() * 10\")\n# or\n.loc[lambda df: df[\"amount\"] &lt;= df[\"amount\"].median() * 10]\n\nBy group\npurchases\n  .groupby(\"country\")                                               \n  .apply(lambda df: df[df[\"amount\"] &lt;= df[\"amount\"].median() * 10]) \n  .reset_index(drop=True)\n\nFIlters each country by the median of its amount times 10.\nThe result of apply is a dataframe, but that dataframe has the index, country (i.e. rownames). The problem is that the result also has a country column. Therefore using reset_index would move the index to a column, but since there’s already a country column, it will give you a ValueError. Including drop=True fixes this by dropping the index entirely.\n\nOn multiple values: df_filtered = data[(data.col1 == \"A\") | (data.col1 == \"B\")]\nBy pattern: df[df.profession.str.contains(\"engineer\")]\n\nOnly rows of profession variable with values containing “engineer”\nAlso available\n\nstr.startswith: df_filtered = data[data.col1.str.startswith(\"Jo\")]\nstr.endswith: df_filtered = data[data.col1.str.endswith(\"n\")]\n\nIf column has NAs or NaNs, specify arg, “nan=False” to ignore them, else you’ll get a valueError\nMethods are case-sensitive unless you specify, “case=False:”\n\nBy negated pattern: df[~df.profession.str.contains(\"engineer\")]\nBy character type: df_filtered = data[data.col1.str.isnumeric()]\n\nAlso available\n\nupper-case: isupper()\nlower-case: islower()\nalphabetic: isalpha()\ndigits: isdigit()\ndecimal: isdecimal()\nwhitespace: isspace()\ntitlecase: istitle()\nalphanumeric: isalnum()\n\n\nBy month of a datetime variable: df[df.date_of_birth.dt.month==11]\n\ndatetime variable has yy-mm-dd format; dt.month accessor used to filter rows of “date_of_birth” variable wit\n\nConditional: df[df.note &gt; 90]\n\nBy string length: df_filtered = data[data.col1.str.len() &gt; 4]\n\nMulti-conditional\ndf[(df.date_of_birth.dt.year &gt; 2000) & \n  (df.profession.str.contains(\"engineer\"))]\n\ndf.query(\"Distance &lt; 2 & Rooms &gt; 2\")\n%in%: df[df.group.isin([\"A\",\"C\"])]\nSmallest 2 values of note column: df.nsmallest(2, \"note\")\n\nnlargest also available\n\nOnly rows in a column with NA values: df[df.profession.isna()]\nOnly rows in a column that aren’t NA: df[df.profession.notna()]\nFind rows by index value: df.loc['Tony']\n\nloc is for the value, iloc is for position (numerical or logical)\n\nSee Select columns Example for iloc logical case\n\nWith groupby\ndata_grp = data.groupby(\"Company Name\")\ndata_grp.get_group(\"Amazon\")\n\nReturn 1st 3 rows (and 2 columns): df.loc[:3, [\"name\",\"note\"]]\nReturn 1st 3 rows and 3rd column: df.iloc[:3, 2]\n\nh 11th month\n\nUsing index to filter single cell or groups of cells\ndf.loc[\n    ['King of Wands', 'The Fool', 'The Devil'],\n    ['image', 'upright keywords']\n]\n\nindex values: ‘King of Wands’, etc. (aka row names)\ncolumn names: ‘image’, etc.\n\n\n\n\nMutate\n\nLooks like all these functions do the same damn thing\nassign e.g. Add columns\ndf[\"col3\"] = df[\"col1\"] + df[\"col2\"]\ndf = df.assign(col3 = df[\"col1\"] + df[\"col2\"])\napply  an operation to column(s)\n\nFunction\ndf[['colA', 'colB']] = df[['colA', 'colB']].apply(np.sqrt)\ndf[\"col3\"] = df.apply(my_func, axis=1)\n\ndocs: Series, DataFrame\naxis - arg for applying a function across rowwise or columnwise (default is 0, which is for columnwise)\nThe fact that mutated columns are assigned to the columns of the df are what keeps it from being like summarize\n\nIf np.sum were the function, then the output would be two numbers and this probably wouldn’t work.\n\n\nWithin a chain\npurchases\n  .groupby(\"country\")\n  .apply(lambda df: (df[\"amount\"] - df[\"discount\"]).sum())\n  .reset_index()\n  .rename(columns={0: \"total\"})\n\nWhen the result of the apply function is coerced to a dataframe by reset_index, the column will be named “0”. rename is used to rename it total.\n\n\nmap\ndf['colE'] = df['colE'].map({'Hello': 'Good Bye', 'Hey': 'Bye'})\n\nEach case of “Hello” is changed to “Good By”, etc.\nIf you give it a dict, it acts like a case_when or plyr::mapvalues or maybe recode\nValues in the column but aren’t included in the dict get NaNs\nThis also can take a lambda function\nDocs (only applies to Series, so I guess that means only 1 column at a time(?))\n\napplymap\ndf[['colA', 'colD']] = df[['colA', 'colD']].applymap(lambda x: x**2)\n\nDocs\nSays it applies a function elementwise, so its probably performing a loop\n\nTherefore better to avoid if a vectorized version of the function is available\n\n\n\n\n\nPivot\n\npivot_longer\n\nExample\nyear_list=list(df.iloc[:, 4:].columns)\ndf = pd.melt(df, id_vars=['Country Name','Series Name','Series Code','Country Code'], value_vars=year_list)\n\nyear_list has the variable names of the columns you want to merge into 1 column\n\n\npivot_wider\n\nExample\n# Step 1: add a count column to able to summarize when grouping\nlong_df['count'] = 1\n\n# Step 2: group by date and type and sum\ngrouped_long_df = long_df.groupby(['date', 'type']).sum().reset_index()\n\n# Step 3: build wide format from long format\nwide_df = grouped_long_df.pivot(index='date', columns='type', values='count').reset_index()\n\n“long_df” has two columns: type and date\n\n\n\n\n\nBind_Rows\ndf1 = pd.DataFrame({'strata': 1, 'y': np.random.normal(loc=10, scale=1, size=size){style='color: #990000'}[}]{style='color: #990000'})\ndf2 = pd.DataFrame({'strata': 2, 'y': np.random.normal(loc=15, scale=2, size=size){style='color: #990000'}[}]{style='color: #990000'})\ndf3 = pd.DataFrame({'strata': 3, 'y': np.random.normal(loc=20, scale=3, size=size){style='color: #990000'}[}]{style='color: #990000'})\ndf4 = pd.DataFrame({'strata': 4, 'y': np.random.normal(loc=25, scale=4, size=size){style='color: #990000'}[}]{style='color: #990000'})\ndf = pd.concat([df1, df2, df3, df4])\n\ndf_ls = [df1, df2, df3, df4]\ndf_all = pd.concat([df_ls[i] for i in range(8)], axis=0)\n\n2 variables are created, “strata” and “y”, then merged into a df\nMake sure no rows are duplicates\n\ndf_loans = pd.concat([df, df_pdf], verify_integrity=True)\n\n\nBind_Cols\npd.concat([top_df, more_df], axis=1)\n\n\nCount\n\nvalue.counts: df[\"col_name\"].value_counts()\n\noutput arranged in descending order of frequencies\nfaster than groupby + size\nargs\n\nnormalize=False\ndropna=True\n\n\ngroupby + count\ndf.groupby(\"Product_Category\").size()\ndf.groupby(\"Product_Category\").count()\n\n“size” includes null values\n“count” doesn’t include null values\narranged by the index column\n\n\n\n\nArrange\n\ndf.sort_values(by = \"col_name\")\n\nargs\n\nascending=True\nignore_index=False (True will reset the index)\n\n\n\n\n\nGroup_By\n\nTo get a dataframe, instead of a series, after performing a grouped calculation, apply reset_index() to the result.\nFind number of groups\ndf_group = df.groupby(\"Product_Category\")\ndf_group.ngroups\ngroupby + count\ndf.groupby(\"Product_Category\").size()\ndf.groupby(\"Product_Category\").count()\n\n“size” includes null values\n“count” doesn’t include null values\n\nOnly groupby observed categories\ndf.groupby(\"col1\", observed=True)[\"col2\"].sum()\ncol1\nA    49\nB    43\nName: col2, dtype: int64\n\ncol1 is a category type and has 3 levels specified (A, B, C) but the column only has As and Bs\nobserved=False (default) would include C and a count of 0.\n\nCount NAs\ndf[\"col1\"] = df[\"col1\"].astype(\"string\")\ndf.groupby(\"col1\", dropna=False)[\"col2\"].sum()\n# output\ncol1\nA      49.0\nB      43.0\n&lt;NA&gt;    30.0\nName: col2, dtype: float64\n\n** Will not work with categorical types **\n\nSo categorical variables, you must convert to strings to be able to group and count NAs\n\n\nCombo\ndf.groupby(\"col3\").agg({\"col1\":sum, \"col2\":max})\n\n      col1  col2\ncol3           \nA        1    2\nB        8    10\nAggregate with only 1 function\ndf.groupby(\"Product_Category\")[[\"UnitPrice(USD)\",\"Quantity\"]].mean()\n\nSee summarize for aggregate by more than 1 function\n\nExtract a group category\ndf_group = df.groupby(\"Product_Category\")\ndf_group.get_group('Healthcare')\n# or\ndf[df[\"Product_Category\"]=='Home']\nGroup objects are iterable\ndf_group = df.groupby(\"Product_Category\")\nfor name_of_group, contents_of_group in df_group:\n    print(name_of_group)\n    print(contents_of_group)\nGet summary stats on each category conditional on a column\ndf.groupby(\"Product_Category\")[[\"Quantity\"]].describe()\n\n\n\nSummarize\n\nagg can only operate on one column at time, so for transformations involving multiple columns (e.g. col3 = col1 - col2), see Mutate section.\nWith groupby and agg\n\ndf.groupby(\"cat_col_name\").agg(new_col_name = (\"num_col_name\", \"func_name\"))\n\nTo reset the index  (i.e. ungroup)\n\nuse .groupby arg, as_index=False\nuse .reset_index() at the end of the chain (outputs a dataframe instead of a series)\n\narg: drop=TRUE …does something (maybe drops grouping columns)\n\n\n\n\nGroup aggregate with more than 1 function\ndf.groupby(\"Product_Category\")[[\"Quantity\"]].agg([min,max,sum,'mean'])\n\nWhen you mention ‘mean’ (with quotes), .aggregate() searches for a function mean belonging to pd.Series i.e. pd.Series.mean().\nWhereas, if you mention mean (without quotes), .aggregate() will search for function named mean in default Python, which is unavailable and will throw an NameError exception.\n\nUse a different aggregate function on specific columns\nfunction_dictionary = {'OrderID':'count','Quantity':'mean'}\ndf.groupby(\"Product_Category\").aggregate(function_dictionary)\nFilter by Group, then Summarize by Group (link)\n\nOption 1\npurchases\n  .groupby(\"country\")                                               \n  .apply(lambda df: df[df[\"amount\"] &lt;= df[\"amount\"].median() * 10]) \n  .reset_index(drop=True)                                           \n  .groupby(\"country\")\n  .apply(lambda df: (df[\"amount\"] - df[\"discount\"]).sum())\n  .reset_index()\n  .rename(columns={0: \"total\"})\n\nFIlters each country by the median of its amount times 10.\nFor each country, subtracts a discount from amount, then sums and renames the column from “0” to total.\n\nOption 2\npurchases\n  .assign(country_median=lambda df:                         \n      df.groupby(\"country\")[\"amount\"].transform(\"median\")   \n  )\n  .query(\"amount &lt;= country_median * 10\")                                  \n  .groupby(\"country\")\n  .apply(lambda df: (df[\"amount\"] - df[\"discount\"]).sum())\n  .reset_index()\n  .rename(columns={0: \"total\"})\n\nHere the median amount per country is calculated first and assigned to each row in purchases.\nquery is used to filter by each country’s median.\nThen, apply and groupby are used in the same manner as option 1.\n\n\n\n\n\nJoin\n\nUsing merge: pd.merge(df1,df2)\n\nMore versatile than join\nAutomatically detects a common column\nMethod: “how = ‘inner’” (i.e. default is inner join)\n\n‘outer’ , ‘left’ , ‘right’ are available\n\nOn columns with different names\n\non = “col_a”\nleft_on = ‘left df col name’\nright_on = ‘right df col name’\n\ncopy = True (default)\n\nUsing join\ndf1.set_index('Course', inplace=True)\ndf2.set_index('Course', inplace=True)\ndf3 = df1.join(df2, on = 'Course', how = 'left')\n\ninstance method that joins on the indexes of the dataframes\nThe column that we match on for the left dataframe doesn’t have to be its index. But for the right dataframe, the join key must be its index\nCan use multiple columns as the index by passing a list, e.g. [“Course”, “Student_ID”]\n* indexes do NOT have to be unique *\nFaster than merge\n\n\n\n\nDistinct\n\nFind number of unique values: data.Country.nunique()\nDisplay unique values: df[\"col3\"].unique()\nCreate a boolean column to indicated duplicated rows: df.duplicated(keep=False)\nCheck for duplicate ids: df_loans[df_loans.duplicated(keep=False)].sort_index()\nCount duplicated ids: df_check.index.duplicated().sum()\nDrop duplicated rows: df.drop_duplicates()\n\n\n\nReplace values\n\nIn the whole df\n\n1 value\ndf.replace(to_replace = '?', value = np.nan, inplace=True)\n\nreplaces all values == ? with NaN\nfaster than loc method\n\nMultiple values\ndf.replace([\"Male\", \"Female\"], [\"M\", \"F\"], inplace=True) # list\ndf.replace({\"United States\": \"USA\", \"US\": \"USA\"}, inplace=True) # dict\n\nOnly in specific columns\ndf.replace(\n    {\n        \"education\": {\"HS-grad\": \"High school\", \"Some-college\": \"College\"},\n        \"income\": {\"&lt;=50K\": 0, \"&gt;50K\": 1},\n    },\n    inplace=True,\n)\n\nreplacement only occurs in “education” and “income” columns\n\nUsing Indexes\n\nExample (2 rows, 1 col): df.loc[['Four of Pentacles', 'Five of Pentacles'], 'suit'] = 'Pentacles'\n\nindex values: “Four of Pentacles”, “Five of Pentacles”\ncolumn name: “suit”\nReplaces the values in those cells with the value “Pentacles”\n\nExample (1 row, 2 cols):\ndf.loc['King of Wands', ['suit', 'reversed_keywords']] = [\n    'Wands', 'impulsiveness, haste, ruthlessness'\n]\n\nindex value: “King of Wands”\ncolumn names: “suit”, :reversed_keywords”\nIn “suit,” replaces value with “Wands”\nIn “reversed words,” replaces value with “impulsiveness, haste, ruthlessness”\n\n\nReplace Na/NaNs in a column with a constant value\ndf['col_name'].fillna(value = 0.85, inplace = True)\nReplaces Na/NaNs in a column with the value of a function/method\ndf['price'].fillna(value = df.price.median(), inplace = True)\nReplaces Na/NaNs in a column with a group value (e.g. group by fruit, then use price median)\n# median\ndf['price'].fillna(df.groupby('fruit')['price'].transform('median'), inplace = True)\nForward-Fill\ndf['price'].fillna(method = 'ffill', inplace = True)\ndf['price'] = df.groupby('fruit')['price'].ffill(limit = 1)\ndf['price'] = df.groupby('fruit')['price'].ffill()\n\nforward-fill: fills Na/NaN with previous non-Na/non-NaN value\nforward-fill, limited to 1: only fills with the previous value if there’s a non-Na/non-NaN 1 spot behind it.\n\nMay leave NAs/NaNs\n\nforward-fill by group\n\nBackward-Fill\ndf['price'].fillna(method = 'bfill', inplace = True)\n\nbackward-fill: fills Na/NaN with next non-Na/non-NaN value\n\nAlternating between backward-fill and forward-fill\ndf['price'] = df.groupby('fruit')['price'].ffill().bfill()\n\nalternating methods: for the 1st group a forward fill is performed; for the next group, a backward fill is performed; etc.\n\nInterpolation\n\ndf['price'].interpolate(method = 'linear', inplace = True)\ndf['price'] = df.groupby('fruit')['price'].apply(lambda x: x.interpolate(method='linear')) # by group\ndf['price'] = df.groupby('fruit')['price'].apply(lambda x: x.interpolate(method='linear')).bfill() # by group with backwards-fill\n\nInterpolation (e.g. linear)\n\nMay leave NAs/NaNs\n\nInterpolation by group\nInterpolation + backwards-fill\n\nApply a conditional: says fill na with mean_price where “weekday” column is TRUE; if FALSE, fill with mean_price*1.25\nmean_price = df.groupby('fruit')['price'].transform('mean')\ndf['price'].fillna((mean_price).where(cond = df.weekday, other = mean_price*1.25), inplace = True)\n\n\n\nStrings\n\nMisc\n\nregex is slow\nStored as “object” type but “StringDtype” is available. This new Dtype is optional for now but it may be required to do so in the future\nRegex with Examples in python and pandas\n\nFilter (see Filter section)\nReplace pattern using regex\ndf['colB'] = df['colB'].str.replace(r'\\D', '')\ndf['colB'] = df['colB'].replace(r'\\D', r'', regex=True)\n\n“r” indicates you’re using regex\nreplaces all non-numeric patterns (e.g. letters, symbols) with an empty string\nReplace pattern with list comprehension (more efficient than loops)\n\nWith {{re}}\nimport re\ndf['colB'] = [re.sub('[^0-9]', '', x) for x in df['colB']]\n\nre is the regular expressions library\nReplaces everything not a number with empty string (carrot inside is a negation)\n\nSplit w/list output\n&gt;&gt; df[\"group\"].str.split(\"-\")\n0    [A, 1B]\n1    [B, 1B]\n2    [A, 1C]\n3    [A, 1B]\n4    [C, 1C]\n\n“-” is the delimiter. A-1B –&gt; [A, 1B]\n\nSplit into separate columns\ndf[\"group1\"] = df[\"group\"].str.split(\"-\", expand=True)[0]\ndf[\"group2\"] = df[\"group\"].str.split(\"-\", expand=True)[1]\n\nexpand = True splits into separate columns\nBUT you have to manually create the new columns in the old df by assigning each column to a new column name in the old df.\n\nConcatenate strings from a list\nwords = ['word'] * 100000 # ['word', 'word', ...]\nsentence = \"\".join(words)\n\nMore efficient than “+” operator\n\nConcatenate string columns\n\nList comprehension is fastest\ndf['all'] = [p1 + ' ' + p2 + ' ' + p3 + ' ' + p4 for p1, p2, p3, p4 in zip(df['a'], df['b'], df['c'], df['d'])]\n“+” operator with a space, ” “, as the delimiter\ndf['all'] = df['a'] + ' ' + df['b'] + ' ' + df['c'] + ' ' + df['d']\n\nAlso fast and have read that this is most efficient for larger datasets\n\ndf['colE'] = df.colB.str.cat(df.colD) can be used for relatively small datasets (up to 100–150 rows)\narr1 = df['owner'].array\narr2 = df['gender'].array\narr3 = []\nfor i in range(len(arr1)):\n    if arr2[i] == 'M':\n        arr3.append('Mr. ' + arr1[i])\n    else:\n        arr3.append('Ms. ' + arr1[i])\ndf['name5'] = arr3\n\nVectorizes columns then concantenates with a +\nFor-loop w/vectorization was faster than apply + list comprehension or for-loop + itertuples or for-loop + iterrows\n\n\nConcatenate string and non-string columns\ndf['colE'] = df['colB'].astype(str) + '-' + df['colD']\nExtract pattern with list comprehension\nimport re\ndf['colB'] = [re.search('[0-9]', x)[0] for x in df['colB']]\n\nre is the regular expressions library\nextracts all numeric characters\n\nExtract all instances of pattern into a list\nresults_ls = re.findall(r'\\d+', s)\n\nFinds each number in string, “s,” and outputs into a list\nCan also use re.finditer\nfyi re.search only returns the first match\n\nExtract pattern using regex\ndf['colB'] = df['colB'].str.extract(r'(\\d+)', expand=False).astype(\"int\")\n\n“r” indicates you’re using regex\nextracts all numeric patterns and changes type to integer\n\nRemove pattern by map loop\nfrom string import ascii_letters\ndf['colB'] = df['colB'].map(lambda x: x.lstrip('+-').rstrip(ascii_letters))\n\npluses or minuses are removed from if they are the leading character\nlstrip removes leading characters that match characters provided\nrstrip removes trailing characters that match characters provided\n\nDoes a string contain one or more digits\nany(c.isdigit() for c in s)\n\n\n\nConversions\n\nConvert dict to pandas df (**slow for large lists**)\ndf = pd.DataFrame.from_dict(acct_dict, orient=\"index\", columns=[\"metrics\"])\n\nresult_df = DataFrame.from_dict(search.cv_results_, orient='columns') \nprint(result_df.columns)\n\nkey of the dict is used as the index and value is column named “metrics”\n\nConvert pandas df to ndarray\nndarray = df.to_numpy()\n# using numpy\nndarray = np.asarray(df)\nConvert df to list or dict\ndf\n  ColA ColB  ColC\n0    1    A    4\n1    2    B    5\n2    3    C    6\n\nresult = df.values.tolist()\n[[1, 'A', 4], [2, 'B', 5], [3, 'C', 6]]\n\ndf.to_dict()\n{'ColA': {0: 1, 1: 2, 2: 3},\n'ColB': {0: 'A', 1: 'B', 2: 'C'},\n'ColC': {0: 4, 1: 5, 2: 6}}\n\ndf.to_dict(\"list\")\n{'ColA': [1, 2, 3], 'ColB': ['A', 'B', 'C'], 'ColC': [4, 5, 6]}\n\ndf.to_dict(\"split\")\n{'index': [0, 1, 2],\n'columns': ['ColA', 'ColB', 'ColC'],\n'data': [[1, 'A', 4], [2, 'B', 5], [3, 'C', 6]]}\nConvert string variable to datetime\ndf.date_of_birth = df.date_of_birth.astype(\"datetime64[ns]\")\ndf['Year'] = pd.to_datetime(df['Year'])\nConvert all “object” type columns to “category”\nfor col in X.select_dtypes(include=['object']):\n  X[col] = X[col].astype('category')\nConvert column to numeric\ndf['GDP'] = df['GDP'].astype(float)\n\n\n\nSample and Simulate\n\nSimulate\n\nRandom normal variable (and group variable named strata)\ndf1 = pd.DataFrame({'strata': 1, 'y': np.random.normal(loc=10, scale=1, size=size))\n\n2 columns “strata” (all 1s) and “y”\nloc = mean, scale = sd, size = number of observations to create\n\n\nSample\n\nRows\ndf.sample(n = 15, replace = True, random_state=2) # with replacement\n\nsample_df = df.sample(int(len(tps_df) * 0.2)) # sample 20% of the data\n\nThis method is faster than sampling using random indices with NumPy\n\nRows and columns\ntps.sample(5, axis=1).sample(7, axis=0)\n\n5 columns and 7 rows\n\n\n\n\n\nChaining\n\nNeed to encapsulate code in parentheses\n# to assign to an object\nnew_df = (\n    melb\n    .query(\"Distance &lt; 2 & Rooms &gt; 2\") # query equals filter in Pandas\n    .filter([\"Type\", \"Price\"]) # filter equals select in Pandas\n    .groupby(\"Type\")\n    .agg([\"mean\", \"count\"]) # calcs average price and row count for each Type; creates subcolumns mean and count under Price\n    .reset_index() # converts matrix to df\n    .set_axis([\"Type\", \"averagePrice\", \"numberOfHouses\"], # renames Price to averagePrice and count to numberOfHouses\n              axis = 1,\n              inplace = False)\n    .assign(averagePriceRounded = lambda x: x[\"averagePrice\"] # assign equals mutate in Pandas (?)\n                                              .round(1))\n    .sort_values(by = [\"numberOfHouses\"],\n                ascending = False)\n)\n\nif agg[\"mean\"] then there wouldn’t be a subcolumn mean, just the values of Price would be the mean\n\nUsing pipe\n\nargs\n\nfunc: Function to apply to the Series/DataFrame\nargs: Positional arguments passed to func\nkwargs: Keyword arguments passed to func\n\nReturns: object, the return type of func\nSyntax\ndef f1(df, arg1):\n# do something return # a dataframe\n\ndef f2(df, arg2):\n# do something return # a dataframe\n\ndef f3(df, arg3):\n# do something return # a dataframe\n\ndf = pd.DataFrame(..) # some dataframe\n\ndf.pipe(f3, arg3 = arg3).pipe(f2, arg2 = arg2).pipe(f1, arg1 = arg1)\n\nfunction 3 (f3) is executed then function 2 then function 1\n\n\n\n\n\nCrosstab\n\nExample\ndf = pd.DataFrame([[\"A\", \"X\"], \n                  [\"B\", \"Y\"], \n                  [\"C\", \"X\"],\n                  [\"A\", \"X\"]], \n                  columns = [\"col1\", \"col2\"])\nprint(pd.crosstab(df.col1, df.col2))\ncol2  X  Y\ncol1     \nA    2  0\nB    0  1\nC    1  0\n\n\n\nPivot Table\n\nExample\nprint(df)\n    Name  Subject  Marks\n0  John    Maths      6\n1  Mark    Maths      5\n2  Peter    Maths      3\n3  John  Science      5\n4  Mark  Science      8\n5  Peter  Science    10\n6  John  English    10\n7  Mark  English      6\n8  Peter  English      4\n\npd.pivot_table(df, \n              index = [\"Name\"],\n              columns=[\"Subject\"], \n              values='Marks',\n              fill_value=0)\nSubject  English  Maths  Science\nName                           \nJohn          10      6        5\nMark          6      5        8\nPeter          4      3      10\nExample: drop lowest score for each letter grade, then calculate the average score for each letter grade\ngrades_df.pivot_table(index='name',\n                      columns='letter grade',\n                      values='score',\n                      aggfunc = lambda series : (sorted(list(series))[-1] + sorted(list(series))[-2]) / 2)\n\nletter grade    A    B\nname\nArif          96.5  87.0\nKayla        95.5  84.0\n\ngrades_df\n\n2 names (“name”)\n6 scores (“score”)\nOnly 2 letter grades associated with these scores (“letter grade”)\n\nindex: each row will be a “name”\ncolumns: each column will be a “letter grade”\nvalues: value in the cells will be from the “score” column according to each combination columns in the index and columns args\naggfunc: uses a lambda to compute the aggregated values\n\n“series” is used a the variable  in the lambda function\nsorts series (ascending), takes the top two values (using negative list indexing), and averages them\n\nIterate over a df\n\nBetter to use a vectorized solution if possible\n\n\n\n\n\nIteration\n\niterrows\ndef salary_iterrows(df):\n    salary_sum = 0\n\n    for index, row in df.iterrows():\n        salary_sum += row['Employee Salary']\n\n    return salary_sum/df.shape[0]\n\nsalary_iterrows(data)\niteruples\ndef salary_itertuples(df):\n    salary_sum = 0\n\n    for row in df.itertuples(): \n        salary_sum += row._4\n\n    return salary_sum/df.shape[0]\n\nsalary_itertuples(data)\n\nFaster than iterrows",
    "crumbs": [
      "Python",
      "Pandas"
    ]
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-ts",
    "href": "qmd/python-pandas.html#sec-py-pandas-ts",
    "title": "Pandas",
    "section": "Time Series",
    "text": "Time Series\n\nMisc\n\nAlso see\n\nFeature Engineering, Time Series &gt;&gt; Misc has a list of python libraries for preprocessing\nA Collection of Must-Know Techniques for Working with Time Series Data in Python\n\nCollection of preprocessing recipes\n\n{{datetime}} in bkmks\n\n\n\n\nOperations\n\nLoad and set index frequency\n\nExample\n#Load the PCE and UMCSENT datasets\ndf = pd.read_csv(\n    filepath_or_buffer='UMCSENT_PCE.csv',\n    header=0,\n    index_col=0,\n    infer_datetime_format=True,\n    parse_dates=['DATE']\n)\n#Set the index frequency to 'Month-Start'\ndf = df.asfreq('MS')\n\n“header=0” is default, says 1st row of file is the column names\n“index_col=0” says use the first column as the df index\n“infer_datetime_format=True” says infer the format of the datetime strings in the columns\n“parse_dates=[‘DATE’]” says convert “DATE” to datetime and format\n\n\nCreate date variable\n\nExample: w/date_range\n# DataFrame\ndate_range = pd.date_range('1/2/2022', periods=24, freq='H')\nsales = np.random.randint(100, 400, size=24)\nsales_data = pd.DataFrame(\n    sales,\n    index = date_range,\n    columns = ['Sales']\n)\n# Series\nrng = pd.date_range(\"1/1/2012\", periods=100, freq=\"S\")\nts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)\nMethods (article)\n\npandas.date_range — Return a fixed frequency DatetimeIndex.\n\nstart: the start date of the date range generated\nend: the end date of the date range generated\nperiods: the number of dates generated\nfreq: default to “D” (daily), the interval between dates generated, it could be hourly, monthly or yearly\n\npandas.bdate_range — Return a fixed frequency DatetimeIndex, with the business day as the default frequency.\npandas.period_range — Return a fixed frequency PeriodIndex. The day (calendar) is the default frequency.\npandas.timedelta_range — Return a fixed frequency TimedeltaIndex, with the day as the default frequency.\n\n\nCoerce to datetime\n\nSource has month first or day first\n# month first\n# e.g 9/16/2015 --&gt; 2015-09-16\ndf['joining_date'] = pd.to_datetime(df['joining_date'])\n\n# day first\n# e.g 16/9/2015 --&gt; 2015-09-16\ndf['joining_date'] = pd.to_datetime(df['joining_date'], dayfirst=True)\n\ndefault is month first\n* If the first digit is a number that can NOT be a month (e.g. 25), then it will parse it as a day instead. *\n\nFormat conditional on source’s delimiter\ndf['joining_date_clean'] = np.where(df['joining_date'].str.contains('/'),\n                                    pd.to_datetime(df['joining_date']),\n                                    pd.to_datetime(df['joining_date'], dayfirst=True)\n                          )\n\nLike an ifelse. Source dates that have “/” separating values are parsed as month-first and everything else as day-first\n\n\nTransform partial date columns\n\nExample: String 09/2007 will be transformed to date 2007-09-01\nimport datetime\n\ndef parse_first_brewed(text: str) -&gt; datetime.date:\n    parts = text.split('/')\n    if len(parts) == 2:\n        return datetime.date(int(parts[1]), int(parts[0]), 1)\n    elif len(parts) == 1:\n        return datetime.date(int(parts[0]), 1, 1)\n    else:\n        assert False, 'Unknown date format'\n\n&gt;&gt;&gt; parse_first_brewed('09/2007')\ndatetime.date(2007, 9, 1)\n\n&gt;&gt;&gt; parse_first_brewed('2006')\ndatetime.date(2006, 1, 1)\n\nExtract time components\n\nCreate “year-month” column: df[\"year_month\"] = df[\"created_at\"].dt.to_period(\"M\")\n\n“M” is the “offset alias” string for month\n(Docs)\n\n\nFilter a range\ngallipoli_data.loc[\n        (gallipoli_data.DateTime &gt;= '2008-01-02') \n        & \n        (gallipoli_data.DateTime &lt;= '2008-01-03')\n    ]\nFill in gaps\n\nExample: hacky way to do it\npd.DataFrame(\n    sales_data.Sales.resample('h').mean()\n)\n\nfrequency is already hourly (‘h’), so taking the mean doesn’t change the values. But NaNs will be added for datetime values that don’t exist.\n.fillna(0) can be added to .mean() if you want to fill the NaNs with something meaningful (e.g. 0)\n\nExplode interval between 2 date columns into a column with all the dates in that interval\n\nExample: 1 row\npd.date_range(calendar[\"checkin_date\"][0], calendar[\"checkout_date\"][0])\n# output\nDatetimeIndex(['2022-06-01', '2022-06-02', '2022-06-03', \n              '2022-06-04', '2022-06-05', '2022-06-06', \n              '2022-06-07'],\n              dtype='datetime64[ns]', freq='D')\nExample: all rows\n\ncalendar.loc[:, \"booked_days\"] = calendar.apply(\n\n    lambda x: list(\n        pd.date_range(\n            x.checkin_date, \n            x.checkout_date + pd.DateOffset(days=1)\n        ).date\n    ),\n    axis = 1\n\n)\nExample: pivot_longer\n\n# explode \ncalendar = calendar.explode(\n    column=\"booked_days\", ignore_index=True\n)[[\"property\",\"booked_days\"]]\n# display the first 5 rows\ncalendar.head()\n\n\n\n\n\nCalculations\n\nGet the min/max dates of a dataset: print(df.Date.agg(['min', 'max'])) (“Date” is the date variable)\nFind difference between to date columns\ndf[\"days_to_checkin\"] = (df[\"checkin_date\"] - df[\"created_at\"]).dt.days\n\nnumber of days between the check-in date and the date booking was created (i.e. number of days until the customer arrives)\n\nAdd 1 day to a subset of observations\ndf.loc[df[\"booking_id\"]==1001, \"checkout_date\"] = df.loc[df[\"booking_id\"]==1001, \"checkout_date\"] + pd.DateOffset(days=1)\n\nadds 1 day to the checkout date of the booking with id 1001\n\nAggregation\n\nMisc\n\nresample (docs) requires a datetime type column set as the index for the dataframe: df.index = df[‘DateTime’]\n\nbtw this function doesn’t resample in the bootstrapping sense of the word. Just a function that allows you to do window calculations on time series\n\nCommon time strings (docs)\n\ns for seconds\nt for minutes\nh for hours\nw for weeks\nm for months\nq for quarter\n\n\nRolling-Window\n\nExample: 30-day rolling average\nwindow_mavg_short=30\nstock_df['mav_short'] = stock_df['Close'] \\\n    .rolling(window=window_mavg_short) \\\n    .mean()\n\nStep-Window\n\nExample: Mean temperature every 3 hours\n\ngallipoli_data.Temperature.resample(rule = ‘3h’).mean()\n\n“1.766667” is the average from 03:00:00 to 05:00:00\n“4.600000” is the average from 06:00:00 to 08:00:00\n“h” is the time string from hours\nBy default the calculation window starts on the left index (see next Example for right index) and doesn’t include the right index\n\ne.g. index, “09:00:00”, calculation window includes “09:00:00”, “10:00:00”, and “11:00:00”\n\n\nExample: Max sunshine every 3 hrs\n\ngallipoli_data['Sunshine Duration'].resample(rule = '3h', closed= 'right').max()\n\n“closed=‘right’” says include the right index in the calculation but not the left\n\ne.g. index, “09:00:00”, calculation window includes “10:00:00”, “11:00:00”, and “12:00:00”\n\n\n\n\n\n\n\nSimulation\n\nExample: simulation, gaussian\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(987)\ntime_series = [np.random.normal()*0.1, np.random.normal()*0.1]\nsigs = [0.1, 0.1]\nfor t in range(2000):\n    sig_t = np.sqrt(0.1 + 0.24*time_series[-1]**2 + 0.24*time_series[-2]**2 + 0.24*sigs[-1]**2 + 0.24*sigs[-2]**2)\n    y_t = np.random.normal() * sig_t\n    time_series.append(y_t)\n    sigs.append(sig_t)\n\ny = np.array(time_series[2:])\nplt.figure(figsize = (16,8))\nplt.plot(y, label = \"Simulated Time-Series\")\nplt.grid(alpha = 0.5)\nplt.legend(fontsize = 18)\nStandard GARCH time-series that’s frequently encountered in econometrics\n\nExample: simulation, beta\n\nfrom scipy.stats import beta\nnp.random.seed(321)\ntime_series = [beta(0.5,10).rvs()]\nfor t in range(2000):\n    alpha_t = 0.5 + time_series[-1] * 0.025 * t\n    beta_t = alpha_t * 20\n    y_t = beta(alpha_t, beta_t).rvs()\n    time_series.append(y_t)\n\ny = np.array(time_series[1:])\nplt.figure(figsize = (16,8))\nplt.plot(y, label = \"Simulated Time-Series\")\nplt.grid(alpha = 0.5)\nplt.legend(fontsize = 18)",
    "crumbs": [
      "Python",
      "Pandas"
    ]
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-opt",
    "href": "qmd/python-pandas.html#sec-py-pandas-opt",
    "title": "Pandas",
    "section": "Optimization",
    "text": "Optimization\n\nPerformance\n\nPandas will typically outperform numpy ndarrays in cases that involve significantly larger volume of data (say &gt;500K rows)\nBad performance by iteratively creating rows in a dataframe\n\nBetter to iteratively append lists then coerce to a dataframe at the end\nUse itertuples instead of iterrows in loops\n\nIterates through the data frame by converting each row of data as a list of tuples. Makes comparatively less number of function calls and hence carry less overhead.\ntqdm::tqdm is a progress bar for loops\n\n\nLibraries\n\n{{pandarallel}} - A simple and efficient tool to parallelize Pandas operations on all available CPUs.\n\narticle, article\n\n{{parallel_pandas}} -  A simple and efficient tool to parallelize Pandas operations on all available CPUs.\n\narticle\n\n{{modin}} - multi-processing package with identical APIs to Pandas, to speed up the Pandas workflow by changing 1 line of code. Modin offers accelerated performance for about 90+% of Pandas API. Modin uses Ray and Dask under the hood for distributed computing.\n\narticle\n\n{{numba}} - JIT compiler that translates a subset of Python and NumPy code into fast machine code.\n\nworks best with functions that involve many native Python loops, a lot of math, and even better, NumPy functions and arrays\nExample\n@numba.jit\ndef crazy_function(col1, col2, col3):\n    return (col1 ** 3 + col2 ** 2 + col3 * 10) ** 0.5\n\nmassive_df[\"f1001\"] = crazy_function(\n    massive_df[\"f1\"].values, massive_df[\"f56\"].values, massive_df[\"f44\"].values\n)\nWall time: 201 ms\n\n9GB dataset\nJIT stands for just in time, and it translates pure Python and NumPy code to native machine instructions\n\n\n\nUse numpy arrays\ndat['col1001'] = some_function(\n          dat['col1'].values, dat['col2'].values, dat['col3'].values\n        )\n\nAdding the .values to the column vectors coerces to ndarrays\nNumPy arrays are faster because they don’t perform additional calls for indexing, data type checking like Pandas Series\n\neval for non-mathematical operations (e.g. boolean indexing, comparisons, etc.)\n\nDocs\nExample\nmassive_df.eval(\"col1001 = (col1 ** 3 + col2 ** 2 + col3 * 10) ** 0.5\", inplace=True)\n\nUsed a mathematical operation for his example for some reason\n\n\niloc vs loc\n\niloc faster for filtering rows: dat.iloc[range(10000)]\nloc faster for selecting columns: dat.loc[:, [\"f1\", \"f2\", \"f3\"]]\nnoticeable as data size increases\n\n\n\n\nMemory Optimization\n\nSee memory size of an object\ndata = pd.read_csv(\"dummy_dataset.csv\")\ndata.info(memory_usage = \"deep\")\ndata.memory_usage(deep=True)  / 1024 ** 2 # displays col sizes in MBs\n\n# or for just 1 variable\ndata.Country.memory_usage()\n\n# a few columns\nmemory_usage = df.memory_usage(deep=True) / 1024 ** 2 # displays col sizes in MBs\nmemory_usage.head(7)\nmemory_usage.sum() # total\nUse inplace transformation (i.e. don’t create new copies of the df) to reduce memory load\ndf.fillna(0, inplace = True)\n# instead of\ndf_copy = df.fillna(0)\nLoad only the columns you need from a file\ncol_list = [\"Employee_ID\", \"First_Name\", \"Salary\", \"Rating\", \"Company\"]\ndata = pd.read_csv(\"dummy_dataset.csv\", usecols=col_list)\nVariable datatypes\n\n\nConvert variables to smaller types when possible\n\nVariables always receive largest memory types\n\nPandas will always assign int64 as the datatype of the integer-valued column, irrespective of the range of current values in the column.\n\n\nByte ranges (same bit options for floats)\n\n\nuint refers to unsigned, only positive integers\nint8: 8-bit-integer that covers integers from [-2⁷, 2⁷].\nint16: 16-bit-integer that covers integers from [-2¹⁵, 2¹⁵].\nint32: 32-bit-integer that covers integers from [-2³¹, 2³¹].\nint64: 64-bit-integer that covers integers from [-2⁶³, 2⁶³].\n\nConvert integer column to smaller type: data[\"Employee_ID\"] = data.Employee_ID.astype(np.int32)\nConvert all “object” type columns to “category”\nfor col in X.select_dtypes(include=['object']):\n  X[col] = X[col].astype('category')\n\nobject datatype consumes the most memory. Either use str or category if there are few unique values in the feature\npd.Categorical data type can speed things up to 10 times while using LightGBM’s default categorical handler\n\nFor datetime or timedelta, use the native formats offered in pandas since they enable special manipulation functions\nFunction for checking and converting all numerical columns in dataframes to optimal types\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min &gt; np.finfo(np.float16).min\n                    and c_max &lt; np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min &gt; np.finfo(np.float32).min\n                    and c_max &lt; np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df\n\nBased on the minimum and maximum value of a numeric column and the above table, the function converts it to the smallest subtype possible\n\nCheck memory usage before and after conversion\nprint(\"Memory usage before changing the datatype:\", data.Country.memory_usage())\ndata[\"Country\"] = data.Country.astype(\"category\")\nprint(\"Memory usage after changing the datatype:\", data.Country.memory_usage())\nUse sparse types for variables with NaNs\n\n\nExample: data[\"Rating\"] = data.Rating.astype(\"Sparse[float32]\")\n\nSpecify datatype when loading data\ncol_list = [\"Employee_ID\", \"First_Name\", \"Salary\", \"Rating\", \"Country_Code\"]\ndata = pd.read_csv(\"dummy_dataset.csv\", usecols=col_list, \n                  dtype = {\"Employee_ID\":np.int32, \"Country_Code\":\"category\"})",
    "crumbs": [
      "Python",
      "Pandas"
    ]
  },
  {
    "objectID": "qmd/apache-arrow.html",
    "href": "qmd/apache-arrow.html",
    "title": "Arrow",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Apache",
      "Arrow"
    ]
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-misc",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-misc",
    "title": "Arrow",
    "section": "",
    "text": "Resources\n\nDocs\nApache Arrow R Cookbook\nCheatsheet\n\nArrow/Feather format built for speed, not compression. so larger files than parquet\n\nFeature for short term storage and parquet for longer term storage\nThe arrow format requires ~ten times more storage space.\n\ne.g. For nyc-taxi data set, parquet takes around ~38GB, but arrow would take around 380GB. Although with arrow, you could see ~10x speed increase in operations.\n\n\nEven with S3 support enabled, network speed will be a bottleneck unless your machine is located in the same AWS region as the data.\nTo create a multi-source dataset, provide a list of datasets to open_dataset instead of a file path, or simply concatenate them like big_dataset &lt;- c(ds1, ds2)\nMore verbose installation + get compression libraries and AWS S3 support\nSys.setenv(\n  ARROW_R_DEV = TRUE,\n  LIBARROW_MINIMAL = FALSE\n)\ninstall.packages(\"arrow\")\n\nInstallation takes some time, so this lets you monitor progress to make sure it isn’t locked.\n\nInfo about your Arrow installation - arrow_info()\n\nVersion, Compression libs, C++ lib, Runtime, etc.\n\nCreating an Arrow dataset\n\nHas a script that downloads monthly released csv files; creates a Hive directory structure; and converts them to parquet files with partitioning based on that structure.\n\nStatically hosted parquet files provide one of the easiest to use and most performant APIs for accessing bulk¹ data, and are far simpler and cheaper to provide than custom APIs. (article)\n\nPros and cons\nList of cases when are static files inappropriate",
    "crumbs": [
      "Apache",
      "Arrow"
    ]
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-apis",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-apis",
    "title": "Arrow",
    "section": "APIs",
    "text": "APIs\n\nSingle File\n\nContains functions for each supported file format (CSV, JSON, Parquet, Feather/Arrow, ORC).\n\nStart with read_  or write_  followed by the name of the file format.\ne.g. read_csv_arrow() , read_parquet() , and read_feather()\n\nWorks on one file at a time, and the data is loaded into memory.\n\nDepending on the size of your file and the amount of memory you have available on your system, it might not be possible to load the dataset this way.\nExample\n\n111MB RAM used - Start of R session\n135MB - Arrow package loaded\n478MB - After using read_csv_arrow(\"path/file.csv\", as_data_frame = FALSE) to load a 108 MB file\n\n525MB with “as_data_frame = TRUE” (data loaded as a dataframe rather than an Arrow table)\n\n\n\n\nDataset\n\nCan read multiple file formats\nCan point to a folder with multiple files and create a dataset from them\nCan read datasets from multiple sources (even combining remote and local sources)\nCan be used to read single files that are too large to fit in memory.\n\nData does NOT get loaded into memory\nQueries will be slower if the data is not in parquet format\n\ne.g. dat &lt;- open_dataset(\"~/dataset/path_to_file.csv\")",
    "crumbs": [
      "Apache",
      "Arrow"
    ]
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-datobjs",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-datobjs",
    "title": "Arrow",
    "section": "Data Objects",
    "text": "Data Objects\n\nScalar - R doesn’t have a scalar class (only vectors)\nScalar$create(value, type)\nArray and ChunkedArray\nChunkedArray$create(..., type)\nArray$create(vector, type)\n\nOnly difference is that one can be chunked\n\nRecordBatch and Table\nRecordBatch or Table$create(...)\n\nSimilar except Table can be chunked\n\nDataset - list of Tables with same schema\nDataset$create(sources, schema)\nData Types (?decimal) (Table$var$cast(decimal(3,2))\n\nint8(), 16, 32, 64\nuint8(), …\nfloat(), 16, 32, 64\nhalffloat()\nbool(), boolean()\nutf8(), large_utf8\nbinary(), large_binary, fixed_size_binary(byte_width)\nstring()\ndate32(), 64\ntime32(unit = c(\"ms\", \"s\")), 64\ntimestamp(unit, timezone)\ndecimal()\nstruct()\nlist_of(), large_list_of(), fixed_size_list_of()",
    "crumbs": [
      "Apache",
      "Arrow"
    ]
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-ops",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-ops",
    "title": "Arrow",
    "section": "Operations",
    "text": "Operations\n\nread_csv_arrow(&lt;csv_file&gt;, as_data_frame = FALSE)\n\nReads csv into memory as an Arrow table\nas_data_frame - if TRUE (default), reads into memory as a tibble which takes up more space instead of an Arrow Table\n\nwrite_parquet\n\ncompression\n\ndefault “snappy” - popular\n“uncompressed”\n“zstd” (z-standard)\n\nHigh performance from Google\nCompresses to smaller size than snappy\n\n\nuse_dictionary\n\ndefault TRUE - encode column types e.g. factor variables\nFALSE - increases file size dramatically (e.g. 9 kb to 86 kb)\n\nchunk_size\n\nHow many rows per column (aka row group)\n\nThe data is compressed per column, and inside each column, per chunk of rows, which is called the row group\n\nIf the data has fewer than 250 million cells (rows x cols), then the total number of rows is used.\nIf performing batch tasks, you want the largest file sizes possible\nif accessing randomly (?), you might want smaller chunck sizes\n\n\nConvert large csv to parquet\nmy_data &lt;- read_csv_arrow(\n  \"~/dataset/path_to_file.csv\",\n  as_data_frame = FALSE\n)\nwrite_parquet(data, \"~/dataset/my-data.parquet\")\ndat &lt;- read_parquet(\"~/dataset/data.parquet\", as_data_frame = FALSE) # loaded into memory as an Arrow table\n\nReduces size of data stored substantially (e.g. 15 GB csv to 9.5 GB parquet)\n\nLazily download subsetted dataset from S3 and locally convert to parquet with partitions\ndata_nyc = \"data/nyc-taxi\"\nopen_dataset(\"s3://voltrondata-labs-datasets/nyc-taxi\") |&gt;\n    dplyr::filter(year %in% 2012:2021) |&gt; \n    write_dataset(data_nyc, partitioning = c(\"year\", \"month\"))\n\nopen_dataset doesn’t used RAM, so subsetting a large dataset (e.g. 40GB) before writing is safe.\nformat = “arrow” also available",
    "crumbs": [
      "Apache",
      "Arrow"
    ]
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-part",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-part",
    "title": "Arrow",
    "section": "Partitioning",
    "text": "Partitioning\n\nPartitioning increases the number of files and it creates a directory structure around the files.\nHive Partitioning - Folder/file structure based on partition keys (i.e. grouping variable). Within each folder, the key has a value determined by the name of the folder. By partitioning the data in this way, it makes it faster to do queries on data slices.\n\nExample: Folder structure when partitioning on year and month\ntaxi-data\n   year=2018\n     month=01\n      file01.parquet\n     month=02\n      file02.parquet\n      file03.parquet\n     ...  \n   year=2019\n     month=01\n     ...\n\nPros\n\nAllows Arrow to construct a more efficient query\nCan be read and written with parallelism\n\nCons\n\nEach additional file adds a little overhead in processing for filesystem interaction\nCan increase the overall dataset size since each file has some shared metadata\n\nBest Practices\n\nAvoid having individual Parquet files smaller than 20MB and larger than 2GB.\n\nHaving files beyond this range will cancel out the benefit of your query grouping by a partition column. (see article for benchmarks)\n\nAvoid partitioning layouts with more than 10,000 distinct partitions.\nOptimal Size is 512MB — 1GB (docs)\n\nView metadata of a partitioned dataset\nair_data &lt;- open_dataset(\"airquality_partitioned_deeper\")\n\n# View data\nair_data\n\n## FileSystemDataset with 153 Parquet files\n## Ozone: int32\n## Solar.R: int32\n## Wind: double\n## Temp: int32\n## Month: int32\n## Day: int32\n##\n## See $metadata for additional Schema metadata\n\nThis is a “dataset” type so data won’t be read into memory\nAssume $metadata will indicate which columns the dataset is partitioned by\n\nPartition a large file and write to arrow format\nlrg_file &lt;- open_dataset(&lt;file_path&gt;, format = \"csv\")\nlrg_file %&gt;%\n    group_by(var) %&gt;%\n    write_dataset(&lt;output_dir&gt;, format = \"feather\")\n\nPass the file path to open_dataset()\nUse group_by() to partition the Dataset into manageable chunks\n\nCan also use partitioning in write_dataset\n\nUse write_dataset() to write each chunk to a separate Parquet file—all without needing to read the full CSV file into R\nopen_dataset is fast because it only reads the metadata of the file system to determine how it can construct queries\n\nPartition Columns\n\nPreferrably chosen based on how you expect to use the data (e.g. important group variables)\nExample: partition on county because your analysis or transformations will largely be done by county even though since some counties may be much larger than others and will cause the partitions to be substantially imbalanced.\nIf there is no obvious column, partitioning can be dictated by a maximum number of rows per partition\nwrite_dataset(\n  data,\n  format = \"parquet\",\n  path = \"~/datasets/my-data/\",\n  max_rows_per_file = 1e7\n)\ndat &lt;- open_dataset(\"~/datasets/my-data\")\n\nFiles can get very large without a row count cap, leading to out-of-memory errors in downstream readers.\nRelationship between row count and file size depends on the dataset schema and how well compressed (if at all) the data is\nOther ways to control file size.\n\n“max_rows_per_group” - splits up large incoming batches into multiple row groups.\n\nIf this value is set then “min_rows_per_group” should also be set or else you may end up with very small row groups (e.g. if the incoming row group size is just barely larger than this value).",
    "crumbs": [
      "Apache",
      "Arrow"
    ]
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-fpdn",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-fpdn",
    "title": "Arrow",
    "section": "Fixed Precision Decimal Numbers",
    "text": "Fixed Precision Decimal Numbers\n\nComputers don’t store exact representations of numbers, so there are floating point errors in calculations. Doesn’t usually matter in analysis, but it can matter in transaction-based operations.\ntxns &lt;- tibble(amount = c(0.1, 0.1, 0.1, -0.3)) %&gt;%\n    summarize(balance = sum(amount, na.rm = TRUE\n# Should be 0\ntxns\n# 5.55e-17\nThe accumulation of these errors can be costly.\nArrow can fix this with fixed precision decimals\n# arrow table (c++ library)\n# collect() changes it to a df\ntxns &lt;- Table$create(amount = c(0.1, 0.1, 0.1, -0.3))\ntxns$amount &lt;- txns$amount$cast(decimal(3,2))\ntxns\n# blah, blah, decimal128, blah\n\nwrite_parquet(txns, \"data/txns_decimal.parquet\")\ntxns &lt;- spark_read_parquet(\"data/txns_decimal.parquet\")\ntxns %&gt;%\n    summarize(balance = sum(ammount, na.rm = T))\n# balance\n#    0",
    "crumbs": [
      "Apache",
      "Arrow"
    ]
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-quer",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-quer",
    "title": "Arrow",
    "section": "Queries",
    "text": "Queries\n\nExample: Filter partitioned files\nlibrary(dbplyr)\n# iris dataset was written and partitioned to a directory path stored in dir_out\nds &lt;- arrow::open_dataset(dir_out, partitioning = \"species\") \n# query the dataset\nds %&gt;% \n  filter(species == \"species=setosa\") %&gt;%\n  count(sepal_length) %&gt;% \n  collect()\n\nformat “&lt;partition_variable&gt;=&lt;partition_value&gt;”\ncompute stores the result in Arrow\ncollect brings the result into R\n\nExample: libarrow functions\narrowmagicks %&gt;% \n  mutate(days = arrow_days_between(start_date, air_date)) %&gt;% \n  collect()\n\n“days_between” is a function in libarrow but not in {arrow}. In order to use it, you only have to put the “arrow_” prefix in front of it.\nUse list_compute_functions to get a list of the available functions\n\nList of potential functions available (libarrow function reference)\n\n\nWhen the query is also larger than memory\nlibrary(arrow)\nlibrary(dplyr)\nnyc_taxi &lt;- open_dataset(\"nyc-taxi/\")\nnyc_taxi |&gt;\n  filter(payment_type == \"Credit card\") |&gt;\n  group_by(year, month) |&gt;\n  write_dataset(\"nyc-taxi-credit\")\n\nIn the example, the input is 1.7 billion rows (70GB), output is 500 million (15GB). Takes 3-4 mins.\n\nUser-defined functions\n\n\nregister_scalar_function - accepts base R functions inside your function",
    "crumbs": [
      "Apache",
      "Arrow"
    ]
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-cloud",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-cloud",
    "title": "Arrow",
    "section": "Cloud",
    "text": "Cloud\n\nAccess files in Amazon S3 (works for all file types)\ntaxi_s3 &lt;- read_parquet(\"s3://ursa-labs-taxi-data/2013/12/data.parquet)\n# multiple files\nds_s3 &lt;- open_dataset(s3://ursa-labs-taxi-data/\", partitioning = c(\"year\", \"month\"))\n\nAs of 2021, only works for Amazon uri\nread_parquet can take a minute to load\nYou can see the folder structure in the read_parquet S3 uri\nExample Query\n# over 125 files and 30GB\nds_s3 %&gt;%\n    filter(total_amount &gt; 100, year == 2015) %&gt;%\n    select(tip_amount, total_amount, passenger_count) %&gt;%\n    mutate(tip_pct = 100 * tip_amount / total_amount) %&gt;%\n    group_by(passenger_count) %&gt;%\n    summarize(median_tip_pct = median(tip_pct),\n              n = n()) %&gt;%\n    print() # is this necessary?\n\nPartitioning allowed Arrow to bypass all files that weren’t in year 2015 directory and only perform calculation on those files therein.\n\n\nAccess Google Cloud Storage (GCS)\n\nDocs",
    "crumbs": [
      "Apache",
      "Arrow"
    ]
  },
  {
    "objectID": "qmd/nlp-llms.html",
    "href": "qmd/nlp-llms.html",
    "title": "33  NLP, LLMs",
    "section": "",
    "text": "33.1 Misc",
    "crumbs": [
      "NLP",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>NLP, LLMs</span>"
    ]
  },
  {
    "objectID": "qmd/nlp-llms.html#misc",
    "href": "qmd/nlp-llms.html#misc",
    "title": "33  NLP, LLMs",
    "section": "",
    "text": "What chatGPT is:\n\n“What would a response to this question sound like” machine Researchers build (train) large language models like GPT-3 and GPT-4 by using a process called “unsupervised learning,” which means the data they use to train the model isn’t specially annotated or labeled. During this process, the model is fed a large body of text (millions of books, websites, articles, poems, transcripts, and other sources) and repeatedly tries to predict the next word in every sequence of words. If the model’s prediction is close to the actual next word, the neural network updates its parameters to reinforce the patterns that led to that prediction.\nConversely, if the prediction is incorrect, the model adjusts its parameters to improve its performance and tries again. This process of trial and error, though a technique called “backpropagation,” allows the model to learn from its mistakes and gradually improve its predictions during the training process. As a result, GPT learns statistical associations between words and related concepts in the data set.\nIn the current wave of GPT models, this core training (now often called “pre-training”) happens only once. After that, people can use the trained neural network in “inference mode,” which lets users feed an input into the trained network and get a result. During inference, the input sequence for the GPT model is always provided by a human, and it’s called a “prompt.” The prompt determines the model’s output, and altering the prompt even slightly can dramatically change what the model produces.Iterative prompting is limited by the size of the model’s “context window” since each prompt is appended onto the previous prompt.  ChatGPT is different from vanilla GPT-3 because it has also been trained on transcripts of conversations written by humans. “We trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sides—the user and an AI assistant,”\nChatGPT has also been tuned more heavily than GPT-3 using a technique called “reinforcement learning from human feedback,” or RLHF, where human raters ranked ChatGPT’s responses in order of preference, then fed that information back into the model. This has allowed the ChatGPT to produce coherent responses with fewer confabulations than the base model. The prevalence of accurate content in the data set, recognition of factual information in the results by humans, or reinforcement learning guidance from humans that emphasizes certain factual responses.\nTwo major types of falsehoods that LLMs like ChatGPT might produce. The first comes from inaccurate source material in its training data set, such as common misconceptions (e.g., “eating turkey makes you drowsy”). The second arises from making inferences about specific situations that are absent from its training material (data set); this falls under the aforementioned “hallucination” label.\nWhether the GPT model makes a wild guess or not is based on a property that AI researchers call “temperature,” which is often characterized as a “creativity” setting. If the creativity is set high, the model will guess wildly; if it’s set low, it will spit out data deterministically based on its data set. If creativity is set low, “[It] answers ‘I don’t know’ all the time or only reads what is there in the Search results (also sometimes incorrect). What is missing is the tone of voice: it shouldn’t sound so confident in those situations.”\nIn some ways, ChatGPT is a mirror: It gives you back what you feed it. If you feed it falsehoods, it will tend to agree with you and “think” along those lines. That’s why it’s important to start fresh with a new prompt when changing subjects or experiencing unwanted responses.\n“One of the most actively researched approaches for increasing factuality in LLMs is retrieval augmentation—providing external documents to the model to use as sources and supporting context,” said Goodside. With that technique, he explained, researchers hope to teach models to use external search engines like Google, “citing reliable sources in their answers as a human researcher might, and rely less on the unreliable factual knowledge learned during model training.” Bing Chat and Google Bard do this already by roping in searches from the web, and soon, a browser-enabled version of ChatGPT will as well. Additionally, ChatGPT plugins aim to supplement GPT-4’s training data with information it retrieves from external sources, such as the web and purpose-built databases.\nOther things that might help with hallucination include, “a more sophisticated data curation and the linking of the training data with ‘trust’ scores, using a method not unlike PageRank… It would also be possible to fine-tune the model to hedge when it is less confident in the response.” (arstechnica article)\n\nOpenAI models\n\ndavinci (e.g. davinci-003) text-generation models are 10x more expensive than their chat counterparts (e.g. gpt-3.5-turbo)\nFor lower usage in the 1000’s of requests per day range ChatGPT works out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests per day, open-sourced models deployed in AWS work out cheaper. (As of April 24th, 2023.) (article)\n\nUsed AWS Lambda for deployment\n\ndavinci hasn’t been trained using reinforcement learning from human feedback (RLHF}\nchatgpt 3.5 turbo models\n\nPros\n\nPerforms better on 0 shot classification tasks than Davinci-003\nOutperforms Davinci-003 on sentiment analysis\nSignificantly better than Davinci-003 at math\ncheaper than davinci\n\nCons\n\nTends to produce longer responses than Davinci-003, which may not be ideal for all use cases\nIncluding k-shot examples can lead to inefficient resource usage in multi-turn use cases\n\n\ndavinci-003\n\nPros\n\nPerforms slightly better than GPT-3.5 Turbo with k-shot examples\nProduces more concise responses than GPT-3.5 Turbo, which may be preferable for certain use cases\n\nCons\n\nLess accurate than GPT-3.5 Turbo on 0 shot classification tasks and sentiment analysis\nPerforms significantly worse than GPT-3.5 Turbo on math tasks\n\n\n\nUse Cases\n\nUnderstanding code (Can reduce cognative load)(article)\n\nDuring code reviews or onboarding new programmers\nunder-commented code\n\nGenerating the code scaffold for a problem where you aren’t sure where or how to start solving it.\nLLMs don’t require removing stopwords during preprocessing of documents\n\nCost\n\nFor lower usage in the 1000’s of requests per day range ChatGPT works out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests per day, open-sourced models deployed in AWS work out cheaper. (article, April 24th, 2023.)\n\nMethods for giving chatGPT data\n\nThink you can upload a file\nThrough prompt\n\nSee bizsci video\n\npaste actual data\npaste column names and types (glimse() with no values)\n\nGenerate a string for each row of data that contains the column name and value\n\nExample\n\n“The  is . The  is . …”\n“The fico_score is 578.0. The load_amount is 6000.0.  The annual income is 57643.54.”\n\n\n\n\nEvolution of LLMs",
    "crumbs": [
      "NLP",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>NLP, LLMs</span>"
    ]
  },
  {
    "objectID": "qmd/nlp-llms.html#langchain",
    "href": "qmd/nlp-llms.html#langchain",
    "title": "33  NLP, LLMs",
    "section": "33.2 LangChain",
    "text": "33.2 LangChain\n\nFramework for developing applications powered by language models; connect a language model to other sources of data; allow a language model to interact with its environment\nImplementation of the paper ReAct: Synergizing Reasoning and Acting in Language Models which demonstrates a prompting technique to allow the model to “reason” (with a chain-of-thoughts) and “act” (by being able to use a tool from a predefined set of tools, such as being able to search the internet). This combination is shown to drastically improve output text quality and give large language models the ability to correctly solve problems.\nMisc\n\nNotes from:\n\nA Gentle Intro to Chaining LLMs, Agents, and utils via LangChain\n\nSee article for workflow for multi-chains\nAlso shows some diagnostic methods that are included in the library\n\n\nAvailable vector stores for document embeddings\n\nAlso see Databases, Vector Databases\n\nSee article for description and link to code for a manual, more controlled parsing of markdown files to get text and code blocks\n\nComponents\n\nDocument loader: Facilitate the data loading from various sources, including CSV files, SQL databases, and public datasets like Wikipedia.\nAgent: Use the language model as a reasoning engine to determine which actions to take and in which order. It repeats through a continuous cycle of thought-action-observation until the task is completed.\nChain: Different from agents, they consist of predetermined sequences of actions, which are hard coded. It addresses complex and well-defined tasks by guiding multiple tools with high-level directions.\nMemory: Currently the beta version supports accessing windows of past messages, this provides the application with a conversational interface\n\nIn general, chains are what you get by sequentially connecting one or more large language models (LLMs) in a logical way.\nChains can be built using LLMs or “Agents”.\n\nAgents provide the ability to answer questions that require recent or specialty information the LLM hasn’t been trained on.\n\ne.g. “What will the weather be like tomorrow?”\nAn agent has access to an LLM and a suite of tools for example Google Search, Python REPL, math calculator, weather APIs, etc. (list of supported agents)\nLLMs will use tools to interact with Agents (list of tools)\n\nTool process\n\nUses a LLMChain for building the API URL based on our input instructions and makes the API call.\nUpon receiving the response, it uses another LLMChain that summarizes the response to get the answer to our original question.\n\n\nReAct (Reason + Act) is a popular agent that picks the most usable tool (from a list of tools), based on what the input query is.\n\nOutput Components: observation, a thought, or it takes an action. This is mainly due to the ReAct framework and the associated prompt that the agent is using (See example with ZERO_SHOT_REACT_DESCRIPTION)\n\nserpapi is useful for answering questions about current events.\n\n\nChains can be simple (i.e. Generic) or specialized (i.e. Utility).\n\nGeneric — A single LLM is the simplest chain. It takes an input prompt and the name of the LLM and then uses the LLM for text generation (i.e. output for the prompt).\n\nGeneric chains are more often used as building blocks for Utility chains\n\nUtility — These are specialized chains, comprised of many LLMs to help solve a specific task. For example,\n\nLangChain supports some end-to-end chains (such as AnalyzeDocumentChain for summarization, QnA, etc) and some specific ones (such as GraphQnAChain for creating, querying, and saving graphs). Programme Aided Language Model reads complex math problems (described in natural language) and generates programs (for solving the math problem) as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.\n2-Chain Examples\n\nChain 1 is used to clean the prompt (remove extra whitespaces, shorten prompt, etc) and chain 2 is used to call an LLM with this clean prompt. (link)\nChain 1 is used to generate a synopsis for a play and chain is used to write a review based on this synopsis. (link)\n\n\n\nDocument Loaders (docs)- various helper functions that take various formats and types of data and produce a document output\n\nFormats like like markdown, word docs, text, PowerPoint, images, HTML, PDF, csvs, AsciiDoc (adoc), etc.\nExamples\n\nGitLoader function clones the repository and load relevant files as documents\nYoutubeLoader - gets subtitles from videos\nDataFrameLoader - converts text columns in panda dfs to documents\n\nAlso, tons of other functions for googledrive or dbs like bigquery, duckdb or cloud storage like s3 or confluence or email or discord, etc.\n\nText Spitters (Docs) - After loading the documents, they’re usually fed to text splitter to create chunks of text due to LLM context constraints. The chunks of text can then be transformed into embeddings.\n\nFrom “Sales and Support Chatbot” article in example below\n\n\n# Define text chunk strategy\nsplitter = CharacterTextSplitter(\n  chunk_size=2000,\n  chunk_overlap=50,\n  separator=\" \"\n)\n# GDS guides\ngds_loader = GitLoader(\n    clone_url=\"https://github.com/neo4j/graph-data-science\",\n    repo_path=\"./repos/gds/\",\n    branch=\"master\",\n    file_filter=lambda file_path: file_path.endswith(\".adoc\")\n    and \"pages\" in file_path,\n)\ngds_data = gds_loader.load()\n# Split documents into chunks\ngds_data_split = splitter.split_documents(gds_data)\nprint(len(gds_data_split)) #771\n\nEmbeddings\n\nOpenAI’s text-embedding-ada-002 model is easy to work with, achieves the highest performance out of all of OpenAI’s embedding models (on the BEIR benchmark), and is also the cheapest ($0.0004/1K tokens).\nHuggingFace’s sentence-transformers , which reportedly has better performance than OpenAI’s embeddings, but that involves downloading the model and running it on your own server.\n\nExample: Generic\n\nBuild Prompt\n\n\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes [{product}]{style='color: #990000'}?\",\n)\nprint(prompt.format(product=\"podcast player\"))\n# OUTPUT\n# What is a good name for a company that makes podcast player?\n\nIf using multiple variables, then you need, e.g. print(prompt.format(product=\"podcast player\", audience=\"children”), to get the updated prompt.\nCreate LLMChain instance and run\n\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMChain\nllm = OpenAI(\n          model_name=\"text-davinci-003\", # default model\n          temperature=0.9) #temperature dictates how whacky the output should be\nllmchain = LLMChain(llm=llm, prompt=prompt)\nllmchain.run(\"podcast player\")\n\nIf you had more than one input_variables, then you won’t be able to use run. Instead, you’ll have to pass all the variables as a dict.\n\ne.g., LLMchain({“product”: “podcast player”, “audience”: “children”}).\n\nUsing the less expensive chat models: chatopenai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\nExample: Multiple Chains and Multiple Input Variables\n\nGoal: create an age-appropriate gift generator\nChain 1: Find age\n\n\n# Chain1 - solve math problem, get the age\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.agents import load_tools\n\nllm = OpenAI(temperature=0)\ntools = load_tools([\"pal-math\"], llm=llm)\nagent = initialize_agent(tools,\n                        llm,\n                        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n                        verbose=True)\n\npal-math is a math-solving tool\nReact agent uses the tool to answer the age problem\nChain 2: Recommend a gift\n\ntemplate = \"\"\"You are a gift recommender. Given a person's age,\\n\nit is your job to suggest an appropriate gift for them. If age is under 10,\\n\nthe gift should cost no more than [{budget}]{style='color: #990000'} otherwise it should cost atleast 10 times [{budget}]{style='color: #990000'}.\nPerson Age:\n[{output}]{style='color: #990000'}\nSuggest gift:\"\"\"\nprompt_template = PromptTemplate(input_variables=[\"output\", \"budget\"], template=template)\nchain_two = LLMChain(llm=llm, prompt=prompt_template)\n\n“{output}” is the name of the output from the 1st chain\n\nFind the name of the output of a chain: print(agent.agent.llm_chain.output_keys)\n\nThe prompt includes a conditional that transforms {budget} (more below)\nLLMchain is used when there are multiple variable in the template\nCombine Chains and Run\n\noverall_chain = SequentialChain(\n                input_variables=[\"input\"],\n                memory=SimpleMemory(memories={\"budget\": \"100 GBP\"}),\n                chains=[agent, chain_two],\n                verbose=True)\noverall_chain.run(\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\")\n\nThe prompt is only for the 1st chain, and it’s output, Age, will be input for the second chain.\nSimpleMemory is used pass the variable for the second prompt which adds some additional context to the second chain — the {budget} for the gift.\nOutput\n\n#&gt; Entering new SequentialChain chain...\n#&gt; Entering new AgentExecutor chain...\n# I need to figure out my dad's current age and then divide it by two.\n#Action: PAL-MATH\n#Action Input: What is my dad's current age if he is going to be 60 next year?\n#Observation: 59\n#Thought: I now know my dad's current age, so I can divide it by two to get my age.\n#Action: Divide 59 by 2\n#Action Input: 59/2\n#Observation: Divide 59 by 2 is not a valid tool, try another one.\n#Thought: I can use PAL-MATH to divide 59 by 2.\n#Action: PAL-MATH\n#Action Input: Divide 59 by 2\n#Observation: 29.5\n#Thought: I now know the final answer.\n#Final Answer: My current age is 29.5 years old.\n#&gt; Finished chain.\n# For someone of your age, a good gift would be something that is both practical and meaningful. Consider something like a nice watch, a piece of jewelry, a nice leather bag, or a gift card to a favorite store or restaurant.\\nIf you have a larger budget, you could consider something like a weekend getaway, a spa package, or a special experience.'}\n#&gt; Finished chain.\n\nExample: Sales and Support Chatbot (article)\n\nCreate embeddings from text data sources and store in Chroma vector store\n\n\n# Define embedding model\nOPENAI_API_KEY = \"OPENAI_API_KEY\"\nembeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\nsales_data = medium_data_split + yt_data_split\nsales_store = Chroma.from_documents(\n    sales_data, embeddings, collection_name=\"sales\"\n)\nsupport_data = kb_data + gds_data_split + so_data\nsupport_store = Chroma.from_documents(\n    support_data, embeddings, collection_name=\"support\"\n)\n\nSales data is from Medium articles and YouTube subtitles\nSupport data is from docs in a couple github repos and stackoverflow\nInstantiate chatgpt\n\nllm = ChatOpenAI(\n    model_name=\"gpt-3.5-turbo\",\n    temperature=0,\n    openai_api_key=OPENAI_API_KEY,\n    max_tokens=512,\n)\n\nSales prompt template\n\nsales_template = \"\"\"As a Neo4j marketing bot, your goal is to provide accurate \nand helpful information about Neo4j, a powerful graph database used for \nbuilding various applications. You should answer user inquiries based on the \ncontext provided and avoid making up answers. If you don't know the answer, \nsimply state that you don't know. Remember to provide relevant information \nabout Neo4j's features, benefits, and use cases to assist the user in \nunderstanding its value for application development.\n[{context}]{style='color: #990000'}\nQuestion: [{question}]{style='color: #990000'}\"\"\"\nSALES_PROMPT = PromptTemplate(\n    template=sales_template, input_variables=[\"context\", \"question\"]\n)\nsales_qa = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=sales_store.as_retriever(),\n    chain_type_kwargs={\"prompt\": SALES_PROMPT},\n)\n\n“{context}” in the template is the data stored in the vector store\n“retriever” arg points to vector store (e.g. sales_store) and uses the as_retriever method to get the embeddings\nSupport prompt template\n\nsupport_template = \"\"\"\nAs a Neo4j Customer Support bot, you are here to assist with any issues \na user might be facing with their graph database implementation and Cypher statements.\nPlease provide as much detail as possible about the problem, how to solve it, and steps a user should take to fix it.\nIf the provided context doesn't provide enough information, you are allowed to use your knowledge and experience to offer you the best possible assistance.\n[{context}]{style='color: #990000'}\nQuestion: [{question}]{style='color: #990000'}\"\"\"\nSUPPORT_PROMPT = PromptTemplate(\n    template=support_template, input_variables=[\"context\", \"question\"]\n)\nsupport_qa = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=support_store.as_retriever(),\n    chain_type_kwargs={\"prompt\": SUPPORT_PROMPT},\n)\n\nSee Sales template\nAdd React agent to determine whether LLM should use Sales or Support templates and contexts\n\ntools = [\n    Tool(\n        name=\"sales\",\n        func=sales_qa.run,\n        description=\"\"\"useful for when a user is interested in various Neo4j information, \n                      use-cases, or applications. A user is not asking for any debugging, but is only\n                      interested in general advice for integrating and using Neo4j.\n                      Input should be a fully formed question.\"\"\",\n    ),\n    Tool(\n        name=\"support\",\n        func=support_qa.run,\n        description=\"\"\"useful for when when a user asks to optimize or debug a Cypher statement or needs\n                      specific instructions how to accomplish a specified task. \n                      Input should be a fully formed question.\"\"\",\n    ),\n]\n\nagent = initialize_agent(\n    tools, \n    llm, \n    agent=\"zero-shot-react-description\", \n    verbose=True\n)\nagent.run(\"\"\"What are some GPT-4 applications with Neo4j?\"\"\")\n\nIn this example, the tools used by the Agent are custom data sources and prompt templates",
    "crumbs": [
      "NLP",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>NLP, LLMs</span>"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html",
    "href": "qmd/db-duckdb.html",
    "title": "DuckDB",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-misc",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-misc",
    "title": "DuckDB",
    "section": "",
    "text": "High performance embedded database for analytics which provides a few enhancements over SQLite such as increased speed and allowing a larger number of columns\n\nFaster than sqlite for most analytics queries (sums, aggregates etc).\n\nVectorizes query executions (columnar-oriented), while other DBMSs (SQLite, PostgreSQL…) process each row sequentially\n\n\nUnlike some other big data tools it is entirely self-contained. (aka embedded, in-process)\n\nNo external dependencies, or server software to install, update, or maintain\n\nCan directly run queries on Parquet files, CSV files, SQLite files, postgres files, Pandas, R and Julia data frames as well as Apache Arrow sources\nExtensions\n\nDocs, List of Official Extensions\n\nTools\n\nSQL Workbench - Query parquet files locally or remotely. Can also produce charts of results. Uses DuckDB-WASM so browser based.\n\nTutorial - Along with explaining the features of the tool, it has complete normalization example and analysis.\nFor visualizations, click the configure button on the right side of the Results sections (bottom main), click Data Grid, choose a chart type, drag column names from the bottom to various areas (similar to Tableau). Click the Reset button in the toolbar close to the configure button to return to Table mode.\nFor tables, if you right-click their name in the Schema pane (far-left), you get a list of options including Summarize which gives summary stats along with uniques and null % for missing data.\nIf tables have foreign keys, data models can be visualized in a mermaid diagram by clicking Data Modes in the bottom-left of the schema panel",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-setup",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-setup",
    "title": "DuckDB",
    "section": "Set-up",
    "text": "Set-up\n\nInstallation: install.packages(\"duckdb\")\nCreate db and populate table from csv\n\nExample \nExample\n# includes filename/id\nwithr::with_dir(\"data-raw/files/\", {\n  dbSendQuery(\n    con, \"\n    CREATE TABLE files AS\n    SELECT *, regexp_extract(filename, '\\\\d{7}') AS file_number\n    FROM read_csv_auto('*Control*File-*.txt', FILENAME = TRUE);\"\n  )\n})",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-dbplyr",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-dbplyr",
    "title": "DuckDB",
    "section": "d/dbplyr",
    "text": "d/dbplyr\n\nExample: Connect, Read in Parallel, and Summarize\ncon &lt;- \n  dbConnect(duckdb(), \n            \":memory:\")\ndf &lt;- \n  dplyr::tbl(con, \n             paste0(\"read_csv('\",\n                    file_name,\n                    \"',\n                    parallel = true,\n                    delim = ',',\n                    header = true,\n                    columns = {\n                        'measurement': 'DOUBLE',\n                        'state': 'VARCHAR'\n                    })\"), \n             check_from = FALSE)\ndf &lt;- df |&gt;\n  summarize(\n    .by = state,\n    mean = mean(measurement),\n    min = min(measurement),\n    max = max(measurement)\n  ) |&gt;\n  collect()\ndf &lt;- NULL\ndbDisconnect(con, shutdown = TRUE)\ngc()\n\nCompetative with running the operation in SQL\n\nExample Connect to db; Write a df to table; Query it\nlibrary(dbplyr)\n\nduck = DBI::dbConnect(duckdb::duckdb(), dbdir=\"duck.db\", read_only=FALSE)\nDBI::dbWriteTable(duck, name = \"sales\", value = sales)\nsales_duck &lt;- tbl(duck, \"sales\")\n\nsales_duck %&gt;%\n  group_by(year, SKU) %&gt;%\n  mutate(pos_sales = case_when(\n          sales_units &gt; 0 ~ sales_units,\n          TRUE ~ 0)) %&gt;%\n  summarize(total_revenue = sum(sales_units * item_price_eur),\n            max_order_price = max(pos_sales * item_price_eur),\n            avg_price_SKU = mean(item_price_eur),\n            items_sold = n())\n\nDBI::dbDisconnect(duck)",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-arrow",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-arrow",
    "title": "DuckDB",
    "section": "Apache Arrow",
    "text": "Apache Arrow\n\nto_duckdb() and to_arrow(): Converts between using {arrow} engine and {duckdb} engieg in workflow without paying any cost to (re)serialize the data when you pass it back and forth\n\nUseful in cases where something is supported in one of Arrow or DuckDB but not the other\n\nBenefits\n\nUtilization of a parallel vectorized execution engine without requiring any extra data copying\nLarger Than Memory Analysis: Since both libraries support streaming query results, we are capable of executing on data without fully loading it from disk. Instead, we can execute one batch at a time. This allows us to execute queries on data that is bigger than memory.\nComplex Data Types: DuckDB can efficiently process complex data types that can be stored in Arrow vectors, including arbitrarily nested structs, lists, and maps.\nAdvanced Optimizer: DuckDB’s state-of-the-art optimizer can push down filters and projections directly into Arrow scans. As a result, only relevant columns and partitions will be read, allowing the system to e.g., take advantage of partition elimination in Parquet files. This significantly accelerates query execution.\n\nExample (using a SQL Query; method 1)\n# open dataset\nds &lt;- arrow::open_dataset(dir_out, partitioning = \"species\")\n# open connection to DuckDB\ncon &lt;- dbConnect(duckdb::duckdb())\n# register the dataset as a DuckDB table, and give it a name\nduckdb::duckdb_register_arrow(con, \"my_table\", ds)\n# query\ndbGetQuery(con, \"\n  SELECT sepal_length, COUNT(*) AS n\n  FROM my_table\n  WHERE species = 'species=setosa'\n  GROUP BY sepal_length\n\")\n\n# clean up\nduckdb_unregister(con, \"my_table\")\ndbDisconnect(con)\n\nfiltering using a partition, the WHERE format is ‘&lt;partition_variable&gt;=&lt;partition_value&gt;’\n\nExample (using SQL Query; method 2)\nlibrary(duckdb)\nlibrary(arrow)\nlibrary(dplyr)\n\n# Reads Parquet File to an Arrow Table\narrow_table &lt;- arrow::read_parquet(\"integers.parquet\", as_data_frame = FALSE)\n\n# Gets Database Connection\ncon &lt;- dbConnect(duckdb::duckdb())\n\n# Registers arrow table as a DuckDB view\narrow::to_duckdb(arrow_table, table_name = \"arrow_table\", con = con)\n\n# we can run a SQL query on this and print the result\nprint(dbGetQuery(con, \"SELECT SUM(data) FROM arrow_table WHERE data &gt; 50\"))\n\n# Transforms Query Result from DuckDB to Arrow Table\nresult &lt;- dbSendQuery(con, \"SELECT * FROM arrow_table\")\nExample (using dplyr)\nlibrary(duckdb)\nlibrary(arrow)\nlibrary(dplyr)\n\n# Open dataset using year,month folder partition\nds &lt;- arrow::open_dataset(\"nyc-taxi\", partitioning = c(\"year\", \"month\"))\n\nds %&gt;%\n  # Look only at 2015 on, where the number of passenger is positive, the trip distance is\n  # greater than a quarter mile, and where the fare amount is positive\n  filter(year &gt; 2014 & passenger_count &gt; 0 & trip_distance &gt; 0.25 & fare_amount &gt; 0) %&gt;%\n  # Pass off to DuckDB\n  to_duckdb() %&gt;%\n  group_by(passenger_count) %&gt;%\n  mutate(tip_pct = tip_amount / fare_amount) %&gt;%\n  summarize(\n    fare_amount = mean(fare_amount, na.rm = TRUE),\n    tip_amount = mean(tip_amount, na.rm = TRUE),\n    tip_pct = mean(tip_pct, na.rm = TRUE)\n  ) %&gt;%\n  arrange(passenger_count) %&gt;%\n  collect()\n\nIn the docs, the example has to_duckdb after the group_by. Not sure if that makes a difference in speed. \n\nExample (Streaming Data)\n# Reads dataset partitioning it in year/month folder\nnyc_dataset = open_dataset(\"nyc-taxi/\", partitioning = c(\"year\", \"month\"))\n\n# Gets Database Connection\ncon &lt;- dbConnect(duckdb::duckdb())\n\n# We can use the same function as before to register our arrow dataset\nduckdb::duckdb_register_arrow(con, \"nyc\", nyc_dataset)\n\nres &lt;- dbSendQuery(con, \"SELECT * FROM nyc\", arrow = TRUE)\n# DuckDB's queries can now produce a Record Batch Reader\nrecord_batch_reader &lt;- duckdb::duckdb_fetch_record_batch(res)\n\n# Which means we can stream the whole query per batch.\n# This retrieves the first batch\ncur_batch &lt;- record_batch_reader$read_next_batch()",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-sql",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-sql",
    "title": "DuckDB",
    "section": "SQL",
    "text": "SQL\n\nMisc\n\nDocs\n\nExample: Connect, Read in Parallel, and Query\nsqltxt &lt;- paste0(\n  \"select\n        state, min(measurement) as min_m,\n        max(measurement) as max_m,\n        avg(measurement) as mean_m\n  from read_csv('\", file_name, \"',\n        parallel = true,\n        delim = ',',\n        header = true,\n        columns = {\n            'measurement': 'DOUBLE',\n            'state': 'VARCHAR'\n        }\n  )\n  group by state\"\n)\ncon &lt;- \n  dbConnect(duckdb(), \n            dbdir = \":memory:\")\ndbGetQuery(con, \n           sqltxt)\ndbDisconnect(con, \n             shutdown = TRUE)\ngc()\n\nFastest method besides polars for running this operation in this benchmark\n\nStar Expressions\n\nAllows you dynamically select columns\n-- select all columns present in the FROM clause\nSELECT * FROM table_name;\n-- select all columns from the table called \"table_name\"\nSELECT table_name.* FROM table_name JOIN other_table_name USING (id);\n-- select all columns except the city column from the addresses table\nSELECT * EXCLUDE (city) FROM addresses;\n-- select all columns from the addresses table, but replace city with LOWER(city)\nSELECT * REPLACE (LOWER(city) AS city) FROM addresses;\n-- select all columns matching the given expression\nSELECT COLUMNS(c -&gt; c LIKE '%num%') FROM addresses;\n-- select all columns matching the given regex from the table\nSELECT COLUMNS('number\\d+') FROM addresses;",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-remcon",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-remcon",
    "title": "DuckDB",
    "section": "Remote Connections",
    "text": "Remote Connections\n\nMisc\n\nNotes from\n\nQuery Remote Parquet Files with DuckDB\n\n\nhttpfs Extension\n\nCreate a db in memory since the data is stored remotely.\nconn &lt;- \n  DBI::dbConnect(\n    duckdb::duckdb(),\n    dbdir = \":memory:\"\n  )\nInstall and Load httpfs extension\nDBI::dbExecute(conn, \"INSTALL httpfs;\")\nDBI::dbExecute(conn, \"LOAD httpfs;\")\n\nCurrently not available for Windows\n\nQuery\nparquet_url &lt;- \"url_to_parquet_files\"\nres &lt;- DBI::dbGetQuery(\n  conn, \n  glue::glue(\"SELECT carrier, flight, tailnum, year FROM '{parquet_url}' WHERE year = 2013 LIMIT 100\")\n)\n\nQueries that needs more data and return more rows takes longer to run, especially transmitting data over the Internet. Craft carefully your queries with this in mind.\n\nTo use {dplyr}, a View must first be created\nDBI::dbExecute(conn, \n               glue::glue(\"CREATE VIEW flights AS SELECT * FROM PARQUET_SCAN('{parquet_url}')\"))\nDBI::dbListTables(conn)\n#&gt; [1] \"flights\"\n\ntbl(conn, \"flights\") %&gt;%\n  group_by(month) %&gt;%\n  summarise(freq = n()) %&gt;%\n  ungroup() %&gt;%\n  collect()\nClose connection: DBI::dbDisconnect(conn, shutdown = TRUE)\n\n{duckdbfs}\n\nCreate dataset object\nparquet_url &lt;- \"url_to_parquet_files\" #e.g. AWS S3\nds &lt;- duckdbfs::open_dataset(parquet_url)\nQuery\nds %&gt;%\n  group_by(month) %&gt;%\n  summarise(freq = n()) %&gt;%\n  ungroup() %&gt;%\n  collect()",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-ext",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-ext",
    "title": "DuckDB",
    "section": "Extensions",
    "text": "Extensions\n\nVS Code extension\n\nConnect to a local DuckDB instance\nCreate new in-memory DuckDB instance\nView DuckDB tables, columns, and views\nRun SQL queries on open DuckDB connections\nAttach SQLite database files to in-memory DuckDB instances\nQuery remote CSV and Parquet data files with DuckDB HTTPFS extension\nCreate in-memory DuckDB tables from remote data sources and query results\nManage DuckDB connections in SQLTools Database Explorer\nAutocomplete SQL keywords, table names, column names, and view names on open database connections in VSCode SQL editor\nSave named SQL query Bookmarks\nUse SQL Query History\nExport SQL query results in CSV and JSON data formats\nintegrate with the equally spiffy SQL Tools extension\n\nJSON extension\n\nExample: From hrbrmstr drop\nINSTALL 'json';\nLOAD 'json';\n\nCOPY (\n  SELECT * FROM (\n    SELECT DISTINCT\n      cve_id,\n      unnest(\n        regexp_split_to_array(\n          concat_ws(\n            ',',\n            regexp_extract(case when cweId1 IS NOT NULL THEN cweId1 ELSE regexp_replace(json_extract_string(problem1, '$.description'), '[: ].*$', '') END, '^(CWE-[0-9]+)', 0),\n            regexp_extract(case when cweId2 IS NOT NULL THEN cweId2 ELSE regexp_replace(json_extract_string(problem2, '$.description'), '[: ].*$', '') END, '^(CWE-[0-9]+)', 0)\n          ),\n          ','\n        )\n      ) AS cwe_id\n    FROM (\n      SELECT \n        json_extract_string(cveMetadata, '$.cveId') AS cve_id, \n        json_extract(containers, '$.cna.problemTypes[0].descriptions[0]') AS problem1,\n        json_extract(containers, '$.cna.problemTypes[0].descriptions[1]') AS problem2,\n        json_extract_string(containers, '$.cna.problemTypes[0].cweId[0]') AS cweId1,\n        json_extract_string(containers, '$.cna.problemTypes[0].cweId[1]') AS cweId2\n      FROM \n        read_json_auto(\"/data/cvelistV5/cves/*/*/*.json\", ignore_errors = true) \n    )\n    WHERE \n      (json_extract_string(problem1, '$.type') = 'CWE' OR\n       json_extract_string(problem2, '$.type') = 'CWE')\n    )\n  WHERE cwe_id LIKE 'CWE-%'\n) TO '/data/summaries/cve-to-cwe.csv' (HEADER, DELIMETER ',')\n\nProcesses a nested json\nClones the CVE list repo, modify the directory paths and run it. It burns through nearly 220K hideous JSON files in mere seconds, even with some complex JSON operations.\n\nDBs\n\nMySQL, Postgres, SQLite\n\nMight need to use FORCE INSTALL postgres\n\nAllows DuckDB to connect to those systems and operate on them in the same way that it operates on its own native storage engine.\nUse Cases\n\nExport data from SQLite to JSON\nRead data from Parquet into Postgres\nMove data from MySQL to Postgres\nDeleting rows, updating values, or altering the schema of a table in another DB\n\nNotes from\n\nMulti-Database Support in DuckDB\n\nHas other examples including transaction operations\n\n\nExample: Open SQLite db file\nATTACH 'sakila.db' AS sakila (TYPE sqlite);\nSELECT title, release_year, length FROM sakila.film LIMIT 5;\n\nATTACH opens the db file and TYPE says that it’s a SQLite db file\nMultiple dbs without using TYPE\nATTACH 'sqlite:sakila.db' AS sqlite;\nATTACH 'postgres:dbname=postgresscanner' AS postgres;\nATTACH 'mysql:user=root database=mysqlscanner' AS mysql;\nIn python\nimport duckdb\ncon = duckdb.connect('sqlite:file.db')\n\nExample: Switch between attached dbs\nUSE sakila;\nSELECT first_name, last_name FROM actor LIMIT 5;\n\nUSE switches from the previous db to the “sakila” db\n\nExample: View all attached dbs\nSELECT database_name, path, type FROM duckdb_databases;\nExample: Copy table from one db type to another\nCREATE TABLE mysql.film AS FROM sqlite.film;\nCREATE TABLE postgres.actor AS FROM sqlite.actor;\nExample: Joins\nSELECT first_name, last_name\nFROM mysql.film\nJOIN sqlite.film_actor ON (film.film_id = film_actor.film_id)\nJOIN postgres.actor ON (actor.actor_id = film_actor.actor_id)\nWHERE title = 'ACE GOLDFINGER';",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html",
    "href": "qmd/surveys-census-data.html",
    "title": "51  Census Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Census Data</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#misc",
    "href": "qmd/surveys-census-data.html#misc",
    "title": "51  Census Data",
    "section": "",
    "text": "FIPS GEOID\n\npopular variable calculations from variables in ACS\nCensus Geocoder (link)\n\nEnter an address and codes for various geographies are returned\nBatch geocoding available for up to 10K records\n\nCodes for geographies returned in a .csv file\n\n\nTIGERweb (link)\n\nAllows you to get geography codes by searching for an area on a map\nOnce zoomed-in on your desired area, you turn on geography layers to find the geography code for your area.\n\nUS Census Regions",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Census Data</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#geographies",
    "href": "qmd/surveys-census-data.html#geographies",
    "title": "51  Census Data",
    "section": "51.2 Geographies",
    "text": "51.2 Geographies\n\n\nMisc\n\n{tidycensus} docs on various geographies, function arguments, and which surveys (ACS, Census) they’re available in.\nACS Geography Boundaries by Year (link)\n\nTypes\n\nLegal/Administrative\n\nCensus gets boundaries from outside party (state, county, city, etc.)\ne.g. election areas, school districts, counties, county subdivisions\n\nStatistical\n\nCensus creates these boundaries\ne.g. regions, census tracts, ZCTAs, block groups, MSAs, urban areas\n\n\nNested Areas\n\n\nCensus Tracts\n\nAreas within a county\nAround 1200 to 8000 people\nSmall towns, rural areas, neighborhoods\n** Census tracts may cross city boundaries **\n\nBlock Groups\n\nAreas within a census tract\nAround 600 to 3000 people\n\nCensus Blocks\n\nAreas within a block group\nNot for ACS, only for the 10-yr census\n\n\nPlaces\n\nMisc\n\nOne place cannot overlap another place\nExpand and contract as population or commercial activity increases or decreases\nMust represent an organized settlement of people living in close proximity.\n\nIncorporated Places\n\ncities, towns, villages\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\nCensus Designated Places (CDPs)\n\nAreas that can’t become Incorporated Places because of state or city regulations\nConcentrations of population, housing, commericial structures\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\n\nCounty Subdivisions\n\nMinor Civil Divisions (MCDs)\n\nLegally defined by the state or county, stable entity. May have elected government\ne.g. townships, charter townships, or districts\n\nCensus County Divisions (CCDs)\n\nno population requirment\nSubcounty units with stable boundaries and recognizable names\n\n\nZip Code Tabulation Areas (ZCTAs)\n\n\nMisc\n\n{crosswalkZCTA} - Contains the US Census Bureau’s 2020 ZCTA to County Relationship File, as well as convenience functions to translate between States, Counties and ZIP Code Tabulation Areas (ZCTAs)\n\nApproximate USPS Code distribution for housing units\n\nThe most frequently occurring zip code within an census block is assigned to a census block\nThen blocks are aggregated into areas (ZCTAs)\n\nZCTAs do NOT nest within any other geographies\n\nI guess the aggregated ZCTA blocks can overlap block groups\n\n2010 ZCTAs exclude large bodies of water and unpopulated areas",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Census Data</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#american-community-survey",
    "href": "qmd/surveys-census-data.html#american-community-survey",
    "title": "51  Census Data",
    "section": "51.3 American Community Survey",
    "text": "51.3 American Community Survey\n\nAbout\n\nYearly estimates based on samples of the population over a 5yr period\n\nTherefore a Margin of Error (MoE) is included with the estimates.\n\nDetailed social, economic, housing, and demographic characteristics\ncensus.gov/acs\n\nACS Release Schedule (releases)\n\nSeptember - 1-Year Estimates (from previous year’s collection)\n\nEstimates for areas with populations of &gt;65K\n\nOctober - 1-Year Supplemental Estimates\n\nEstimates for areas with populations between 20K-64999\n\nDecember - 5-Year Estimates\n\nEstimates for areas including census tract and block groups\n\n\nData Collected\n\nPopulation\n\nSocial\n\nAncestry, Citizenship, Citizen Voting Age  Population, Disability, Education Attainment, Fertility, Grandparents, Language, Marital Status, Migration, School Enrollment, Veterans\n\nDemographic\n\nAge, Hispanic Origin, Race, Relationship, Sex\n\nEconomic\n\nClass of worker, Commuting, Employment Status, Food Stamps (SNAP), Health Insurance, Hours/Week, Weeks/Year, Income, Industry & Occupation\n\n\nHousing\n\nComputer & Internet Use, Costs (Mortgage, Taxes, Insurance), Heating Fuel, Home Value, Occupancy, Plumbing/Kitchen Facilities, Structure, Tenure (Own/Rent), Utilities, Vehicles, Year Built/Year Movied In",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Census Data</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#dicennial-us-census",
    "href": "qmd/surveys-census-data.html#dicennial-us-census",
    "title": "51  Census Data",
    "section": "51.4 Dicennial US Census",
    "text": "51.4 Dicennial US Census\n\n51.4.1 Misc\n\nA complete count and not based on samples like the ACS\nApplies differential privacy to preserve respondent confidentiality\n\nAdds noise to data. Greater effect at lower levels (i.e. block level)\nThe exception is that is no differetial privacy for household-level data.\n\n\n\n\n51.4.2 PL94-171\n\nPopulation data which the government needs for redistricting\nsumfile = “pl”\nState Populations\npop20 &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"P1_001N\",\n    year = 2020\n  )\n\nFor 2020, default is sumfile = “pl”\n\n\n\n\n51.4.3 DHC\n\nAge, Sex, Race, Ethnicity, and Housing Tenure (most popular dataset)\nsumfile = “dhc”\nCounty\ntx_population &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\nCensus Block (analogous to a city block)\nmatagorda_blocks &lt;- \n  get_decennial(\n    geography = \"block\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    county = \"Matagorda\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\n\n\n\n51.4.4 Demographic Profile\n\nPretabulated percentages from dhc\nsumfile = \"dp\"\nC suffix variables is counts while P suffix variables are percentages\n\n0.4 is 0.4% not 40%\n\nExample: Same-sex married and partnered in California by County\nca_samesex &lt;- \n  get_decennial(\n    geography = \"county\",\n    state = \"CA\",\n    variables = c(married = \"DP1_0116P\",\n                  partnered = \"DP1_0118P\"),\n    year = 2020,\n    sumfile = \"dp\",\n    output = \"wide\"\n  )\nTabulations for 118th Congress and Island Areas (i.e. congressional districts)\n\nsumfile = \"cd118\"\n\n\n\n\n51.4.5 Detailed DHC-A\n\nDetailed demographic data; Thousands of racial and ethnic groups; Tabulation by sex and age.\nDifferent groups are in different tables, so specific groups can be hard to locate.\nAdaptive design means the demographic group (i.e. variable) will only be available in certain areas. For privacy, data gets supressed where the area has low population.\n\nConsiderable sparsity especially when going down census tract\n\nArgs\n\nsumfile = “ddhca”\npop_group - Population group code (See get_pop_groups below)\n\n“all” for all groups\npop_group_label = TRUE - Adds group labels\n\n\nget_pop_groups(2020, \"ddhca\") - Gets group codes for ethnic groups\n\nFor various groups there could be at least two variable (e..g Somaili, Somali and any combination)\nFor time series analysis, analagous groups for 2000 is SF2/SF4 and for 2010 is SF2. (SF stands for Summary File)\n\ncheck_ddhca_groups - Checks which variables are available for a specific group\n\nExample: Somali\ncheck_ddhca_groups(\n  geography = \"county\", \n  pop_group = \"1325\", \n  state = \"MN\", \n  county = \"Hennepin\"\n)\n\nExample: Minnesota group populations\nload_variables(2020, \"ddhca\") %&gt;% \n  View()\nmn_population_groups &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"all\", # for all groups\n    pop_group_label = TRUE\n  )\n\nIncludes aggregate categories like European Alone, Other White Alone, etc., so you can’t just aggregate the value column to get the total population in Minnesota.\n\nSo, in order to calculate ethnic group ratios of the total state or county, etc. population, you need to those state/county totals from other tables (e.g. PL94-171)\n\n\nUse dot density and not chloropleths to visualize these sparse data\n\nExample: Somali populations by census tract in Minneapolis\nhennepin_somali &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    county = \"Hennepin\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"1325\", # somali\n    pop_group_label = TRUE,\n    geometry = TRUE\n  )\n\nsomali_dots &lt;- \n  as_dot_density(\n    hennepin_somali,\n    value = \"value\", # column name which is by default, \"value\"\n    values_per_dot = 25\n  )\n\nmapview(somali_dots, \n        cex = 0.01, \n        layer.name = \"Somali population&lt;br&gt;1 dot = 25 people\",\n        col.regions = \"navy\", \n        color = \"navy\")\n\nvalues_per_dot = 25 says make each dot worth 25 units (e.g. people or housing units)\n\n\n\n\n\n51.4.6 Time Series Analysis\n\n{tidycensus} only has 2010 and 2020\n\nSee https://nhgis.org for previous census values\n\nIssue: county names and boundaries change (e.g. Alaska redraws a lot)\n\nCensus gives a different GeoID to counties that rename even though they’re it’s the same county\nNA values showing up after you calculate how the value changes over time is a good indication of this problem: filter(county_change, is.na(value10))\n\nExample: Join 2010 and 2020\ncounty_pop_10 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P001001\", \n    year = 2010,\n    sumfile = \"sf1\"\n  )\n\ncounty_pop_10_clean &lt;- \n  county_pop_10 %&gt;%\n    select(GEOID, value10 = value) \n\ncounty_pop_20 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    year = 2020,\n    sumfile = \"dhc\"\n  ) %&gt;%\n    select(GEOID, NAME, value20 = value)\n\ncounty_joined &lt;- \n  county_pop_20 %&gt;%\n    left_join(county_pop_10_clean, by = \"GEOID\") \n\ncounty_joined\n\ncounty_change &lt;- \n  county_joined %&gt;%\n    mutate( \n      total_change = value20 - value10, \n      percent_change = 100 * (total_change / value10) \n    ) \nExample: Age distribution over time in Michigan\n\n\nCode available in the github repo or R/Workshops/tidycensus-umich-workshop-2024-main/census-2020/bonus-chart.R\nDistribution shape remains pretty much the same but decreasing for most age cohorts.\n\ne.g. The large hump representing the group of people in there mid-40s in 2000 steadily decreases over time.",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Census Data</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#tidycensus",
    "href": "qmd/surveys-census-data.html#tidycensus",
    "title": "51  Census Data",
    "section": "51.5 tidycensus",
    "text": "51.5 tidycensus\n\nGet an API key\n\nRequest a key, then activate the key from the link in your email.(https://api.census.gov/data/key_signup.html)\nSet as an environment variable: census_api_key(\"&lt;api key&gt;\", install = TRUE)\n\nOr add this line to .Renviron file, CENSUS_API_KEY=‘&lt;api key’\n\n\nSearch Variables\n\nColumns\n\nName - ID of the variable (Use this in the survey functions)\nLabel - Detailed description of the variable\nContext - Subject of the table that the variable is located in.\n\nPrefixes (Variables can have combinations of prefixes)\n\nP: i.e. Person; Data available at the census block and larger\nCT: Data available at the census track and larger\nH: Data available at the Housing Unit level\n\nI think housing unit is an alternatve unit. So instead of the unit being a person, which I assume is the typical unit, it’s a housing unit (~family).\nNot affected by Differential Privacy (i.e. no noise added; true value)\nExample: Total Deleware housing units at census block level\ndp_households &lt;- \n      get_decennial(\n            geography = \"block\",\n            variables = \"H1_001N\",\n            state = \"DE\",\n            sumfile = \"dhc\",\n            year = 2020\n      )\n\n\nExample: DHC data in census for 2020\n\nvars &lt;- load_variables(2020, \"dhc\")\n\nView(vars)\n\nView table, click filter, and then search for parameters (e.g. Age, Median, etc.) with the Label, Context boxes, and overall search box\n\n\nsummary_var - Argument for supplying an additional variable that you need to calculate some kind of summary statistic\n\nExample: Race Percentage per Congressional District\n\nrace_vars &lt;- c(\n  Hispanic = \"P5_010N\", # all races identified as hispanic\n  White = \"P5_003N\", # white not hispanic\n  Black = \"P5_004N\", # black not hispanic\n  Native = \"P5_005N\", # native american not hispanic\n  Asian = \"P5_006N\", # asian not hispanic\n  HIPI = \"P5_007N\" # hawaiian, islander not hispanic\n)\n\ncd_race &lt;- \n  get_decennial(\n    geography = \"congressional district\",\n    variables = race_vars,\n    summary_var = \"P5_001N\", # total population for county\n    year = 2020,\n    sumfile = \"cd118\"\n)\n\ncd_race_percent &lt;- \n  cd_race %&gt;%\n    mutate(percent = 100 * (value / summary_value)) %&gt;% \n    select(NAME, variable, percent)\n\ngeometry = TRUE- Joins shapefile with data and returns a SF (Simple Features) dataframe for mapping\n\nMisc\n\nYou can create a discrete color palette with the at argument in the mapview function.\n\nExample\n# check min and max of your data to select range of bins\nmin(iowa_over_65, na.rm = TRUE) # 0\nmax(iowa_over_65, na.rm = TRUE) # 38.4\n\nm1 &lt;- \n  mapview(iowa_over_65, \n          zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1),\n          at = c(0, 10, 20, 30, 40))\n\nThis will result in a discrete palette with bins of 0-10, 10-20, etc. Looks like an overlap, so I’m sure which bin contains the endpoints.\n\n\n\nExample: Over 65 in Iowa by census tract\nlibrary(mapviw); library(viridisLite)\n\niowa_over_65 &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"DP1_0024P\",\n    state = \"IA\",\n    geometry = TRUE,\n    sumfile = \"dp\",\n    year = 2020\n  )\nm1 &lt;- \n  mapview(iowa_over_65, zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1))\nExport as an HTML file\nhtmlwidgets::saveWidget(m1@map, \"iowa_over_65.html\")\n\nCan embed it elsewhere (html report or website) by adding it as an asset",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Census Data</span>"
    ]
  },
  {
    "objectID": "qmd/algorithms-learn-to-rank.html",
    "href": "qmd/algorithms-learn-to-rank.html",
    "title": "Learn-to-Rank",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Algorithms",
      "Learn-to-Rank"
    ]
  },
  {
    "objectID": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-misc",
    "href": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-misc",
    "title": "Learn-to-Rank",
    "section": "",
    "text": "Defines a ranking function to score each document based on a given query. The documents are then ranked in descending order of these scores, representing the relative relevance of the documents to the query.\n\nThe target variable is some kind of relevance score.\nFor recommendation systems, users are the queries and the products they are interested in are the documents.\n\nTL;DR: LambdaLoss is the most state-of-the-art method available in current py, r packages (See Listwise Ranking &gt;&gt; LambdaLoss), but maybe not overall\nNotes from\n\nLearning to Rank: A Complete Guide to Ranking using Machine Learning\nWhat Is Learning to Rank: A Beginner’s Guide to Learning to Rank Methods\nHow to evaluate Learning to Rank Models\n\nShows some manual calculations of the metrics to get a feel for how they work.\n\n\nResources\n\nList of approaches and their papers\nApproaches used by popular search engines\n\nUse Cases:\n\nInformation retrieval problems, such as document retrieval, collaborative filtering, sentiment analysis, and online advertising\nTravel Agencies — Given a user profile and filters (check-in/check-out dates, number and age of travelers, …), sort available rooms by relevance.\n\nVector Space Models (basic)\n\nCompute a vector embedding (e.g. using Tf-Idf or BERT) for each query and document, and then compute the relevance score f(x) = f(q, d) as the cosine similarity between the vectors embeddings of q and d.\nAlso see: wiki\nExample:  Learning to rank is good for your ML career - Part 1: background and word embeddings - Embracing the Random\n\nexplainer, uses a python Keras model to create embeddings, compares embeddings with cosine similarity.",
    "crumbs": [
      "Algorithms",
      "Learn-to-Rank"
    ]
  },
  {
    "objectID": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-preproc",
    "href": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-preproc",
    "title": "Learn-to-Rank",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nFeature Engineering\n\nCount the number of times a word in the query has occurred in the document\n\ne.g “Sushi” appears once in d1",
    "crumbs": [
      "Algorithms",
      "Learn-to-Rank"
    ]
  },
  {
    "objectID": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-diag",
    "href": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-diag",
    "title": "Learn-to-Rank",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nMisc\n\nAlso see Algorithms, Recommendation &gt;&gt; Metrics\nUse binary relevance metrics if the goal is to assign a binary relevance score to each document.\nUse graded relevance metrics if the goal is to set a continuous relevance score for each document.\n\nMean Average Precision (MAP)\n\n\nMean average precision within the top k highest-ranked documents\nQ is the total number of queries and r is the relevance scroe\nIssues\n\nIt does not consider the ranking of the retrieved items, only the presence or absence of relevant documents.\nIt may not be appropriate for datasets where the relevance of items is not binary, as it does not consider an item’s degree of relevance.\n\n\nMean Reciprocal Rank (MRR)\n\nWhere Q is the total number of queries and r is the relevance score\nIssue: considers only the first relevant document for the given query\n\nNormalized Discounted Cumulative Gain (NDCG)\n\nAccounts for both the relevance and the position of the results\nIn the DCG equation, the numerator increases when the document’s relevance is high; the denominator increases when the position of the document increases. Altogether, DCG value will go high when highly relevant items are ranked higher.\nBound between 0 and 1, where 1 is the best",
    "crumbs": [
      "Algorithms",
      "Learn-to-Rank"
    ]
  },
  {
    "objectID": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-ptrank",
    "href": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-ptrank",
    "title": "Learn-to-Rank",
    "section": "Pointwise Ranking",
    "text": "Pointwise Ranking\n\nThe total loss is computed as the sum of loss terms defined on each document dᵢ (hence pointwise) as the distance between the predicted score sᵢ and the ground truth yᵢ, for i=1…n.\n\nBy doing this, we transform our task into a regression problem, where we train a model to predict y.\n\nData is made up of queries (q) and documents (d) associated with those queries\nIssues\n\nTrue relevance (aka absolute relevance) scores are needed to train the model\n\nTo get the ground truth relevance score per each document in the query, we can use human annotators or the number of clicks received for a particular document.",
    "crumbs": [
      "Algorithms",
      "Learn-to-Rank"
    ]
  },
  {
    "objectID": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-prwse",
    "href": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-prwse",
    "title": "Learn-to-Rank",
    "section": "Pairwise Ranking",
    "text": "Pairwise Ranking\n\nThe total loss is computed as the sum of loss terms defined on each pair of documents dᵢ, dⱼ (hence pairwise) , for i, j=1…n.\nThe objective function on which the model is trained is to predict whether yᵢ &gt; yⱼ or not, i.e. which of two documents is more relevant.\n\nBy doing this, we transform our task into a binary classification problem, (1 if yᵢ &gt; yⱼ, 0 otherwise).\n\nIn many scenarios training data is available only with partial information, e.g. we only know which document in a list of documents was chosen by a user (and therefore is more relevant), but we don’t know exactly how relevant is any of these documents\n\nThis method only requires a partial ordering (i.e. relevance) of the documents for each query in contrast to the absolute relevance required for pointwise ranking. (see LambdaRank)\n\nIssues\n\nbiased towards queries with large document pairs\n\nFirst used by RankNet, which used a Binary Cross Entropy (BCE)",
    "crumbs": [
      "Algorithms",
      "Learn-to-Rank"
    ]
  },
  {
    "objectID": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-lstwse",
    "href": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-lstwse",
    "title": "Learn-to-Rank",
    "section": "Listwise Ranking",
    "text": "Listwise Ranking\n\n\nThe loss is directly computed on the whole list of documents (hence listwise) with corresponding predicted ranks. It sums over all pairs of items within a query.\nMaximizes the evaluation metric in contrast to pointwise and pairwise ranking methods\nSoftRank (2008 paper)\n\nInstead of predicting a deterministic score s = f(x) like lambdaRank, a smoothened probabilistic score s~ 𝒩(f(x), σ²) is predicted. \n\nThe ranks k are non-continuous functions of predicted scores s, but thanks to the smoothening, probability distributions can be calculated for the ranks of each document.\nFinally, SoftNDCG, the expected NDCG over this rank distribution, is optimized, which is a smooth function.\n\n\nListNet (2007 paper)\n\nEach ranked list corresponds to a permutation, and loss is defined over the permutation space\nGiven a list of scores, s, the probability of any permutation is defined using the Plackett-Luce model.\nLoss is computed as the Binary Cross-Entropy distance between true and predicted probability distributions over the space of permutations.\nExample: Learning to rank is good for your ML career - Part 2: let’s implement ListNet! - Embracing the Random\n\nexplainer, uses a keras model to implement listnet\n\n\nLambdaLoss (2018 paper)\n\n\nIntroduced a generalized framework that uses as a mixture model, where the ranked list, π , is treated as a hidden variable. Then, the loss is defined as the negative log likelihood of such model.\nResults\n\nAll other listwise methods (RankNet, LambdaRank, SoftRank, ListNet, …) are special configurations of this general framework. Indeed, their losses are obtained by accurately choosing the likelihood p(y | s, π) and the ranked list distribution p(π | s).\nThis framework allows us to define metric-driven loss functions directly connected to the ranking metrics that we want to optimize.\n\nPackages\n\n{{allRank}}: PyTorch-based framework for training neural Learning-to-Rank (LTR) models, featuring implementations of:\n\ncommon pointwise, pairwise and listwise loss functions\nfully connected and Transformer-like scoring functions\ncommonly used evaluation metrics like Normalized Discounted Cumulative Gain (NDCG) and Mean Reciprocal Rank (MRR)\nclick-models for experiments on simulated click-through data\n\n{{pytorchltr}}: support the infrastructure necessary for performing LTR experiments in PyTorch\n\nUtilities to automatically download and prepare several public LTR datasets\nSeveral pairloss and LambdaLoss functions associated with a few different metrics\n\n{{tensorflow_ranking}}: TensorFlow LTR library\n\nCommonly used loss functions including pointwise, pairwise, and listwise losses.\nCommonly used ranking metrics like Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG).\nMulti-item (also known as groupwise) scoring functions.\nLambdaLoss implementation for direct ranking metric optimization.\nUnbiased Learning-to-Rank from biased feedback data.",
    "crumbs": [
      "Algorithms",
      "Learn-to-Rank"
    ]
  },
  {
    "objectID": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-lmdrnk",
    "href": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-lmdrnk",
    "title": "Learn-to-Rank",
    "section": "LambdaRank",
    "text": "LambdaRank\n\nLambdaRank is a gradient, despite being closely related to the gradient of the classic pairwise loss function, and It’s a pointwise scoring function, meaning that the LightGBM ranker “takes a single document at a time as its input, and produces a score for every document separately.”\n\nSo, a bit of a hybrid between pointwise and pairwise ranking\n\nMisc\n\nXGBoost can also perform lambdarank (docs)\n\nDemo\nobjective\n\nrank:pairwise: Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\nrank:ndcg: Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\nrank:map: Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized\n\neval_metric (docs)\n\ndefault: mean average precision\nauc: When used with LTR task, the AUC is computed by comparing pairs of documents to count correctly sorted pairs. This corresponds to pairwise learning to rank. The implementation has some issues with average AUC around groups and distributed workers not being well-defined.\naucpr: Area under the PR curve. Available for classification and learning-to-rank tasks.\n\nFor ranking task, only binary relevance label is supported. Different from map (mean average precision), aucpr calculates the interpolated area under precision recall curve using continuous interpolation.\n\n\ngroup input format (docs)\n\nFor a ranking task, XGBoost requires an file that indicates the group information.\n\nquery id columns (docs)\n\nFor a ranking task, you may embed query group ID for each instance in the LIBSVM file by adding a token of form qid:xx in each row\n\n\n\nLightGBM:\n\nlambdarank objective: LGBMRanker(objective=\"lambdarank\")\nExpects the target (relevance score) to be an integer\n\nIf your score is a decimal, then you can apply a transformation to get an integer\n\nmultiply by 10, 100, etc. to get an integer (e.g. 0.10 -&gt; 0.14 * 100 = 14) (See Example: anime recommendation)\nBin by quantile, then apply a ranking function (See Example: stock portfolio)\n\nUseful if the variable has negative values\n\n\n\nDatasets should be sorted by user id or query id (might be required for group parameter, see below)\n\nDidn’t see this in every example, so I don’t know if it’s required or not. None of the authors gave me complete confidence that they knew what they were doing.\n\n\nExample: py, LightGBM, anime recommendation (article)\n\ntest_size = int(1e5)\nX,y = train_processed[features],train_processed[target].apply(lambda x:int(x * 10))\ntest_idx_start = len(X)-test_size\nxtrain,xtest,ytrain,ytest = X.iloc[0:test_idx_start],X.iloc[test_idx_start:],y.iloc[0:test_idx_start],y.iloc[test_idx_start:]\n\nget_group_size = lambda df: df.reset_index().groupby(\"user_id\")['user_id'].count()\ntrain_groups = get_group_size(xtrain)\ntest_groups = get_group_size(xtest)\nprint(sum(train_groups) , sum(test_groups))\n#(4764372, 100000)\n\nmodel = LGBMRanker(objective=\"lambdarank\")\nmodel.fit(xtrain,ytrain,group=train_groups,eval_set=[(xtest,ytest)],eval_group=[test_groups],eval_metric=['ndcg'])\n\nIn this example, “relevance_score” is the relevance score (i.e. target), users (“user_id”) are the queries and the anime (“anime_id”) they are interested in are the documents.\n\nOther predictors included\n\nThe “group” parameter enables the model to learn the relative importance of different features within each group, which can improve the model’s overall performance\n\nA query (e.g. user) can have many documents (e.g. products) associated with it. So in this example, a user and the products associate with him is a “group”.\nThe total groups should be the number of samples (i.e. total number of users or queries)\ne.g. group = [3, 2, 1] says the first user has 3 products associated with her, the seconder user has 2 products, and the third user has 1 product.\n“So it’s vital to sort the dataset by user_id(query_id) before creating the group parameter.”\n\nPredicting relevance scores in production for the same users in the training set\n\nYou need to have a selection of products that the user hasn’t used before.\n\ne.g. movies: if you’re recommending movies to a user, you shouldn’t recommend ones they’ve already seen. (i.e. movies mapped to their user id in the training data)\n\nIn this example\n\nSelect the user’s favorite N number of genres.\nFor each genre in the above-selected genres, pick the highest-rated m animes. Now you have M* N animes to rank for that user.\nJust create the user base and anime-based features. And finally, call the .predict() method with the created feature vector.\n\n\nExample: py, LightGBM, score interpretation (article)\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n\nquery_train = [X_train.shape[0]]\nquery_val = [X_val.shape[0]]\nquery_test = [X_test.shape[0]]\n\ngbm = lgb.LGBMRanker()\ngbm.fit(X_train, y_train, group=query_train,\n        eval_set=[(X_val, y_val)], eval_group=[query_val],\n        eval_at=[5, 10, 20], early_stopping_rounds=50)\n\nNo clue what the data is… could be movies for a recommendation\n“early_stopping_rounds=50” says training continues until there’s no improvement for 50 rds.\n“eval_at”  are the k values used to evaluate nDCG@k over the validation set\nPredictions\n\n\ntest_pred = gbm.predict(X_test)\nX_test[\"predicted_ranking\"] = test_pred\nX_test.sort_values(\"predicted_ranking\", ascending=False)\n\n“This numbers can be interpreted as probabilities of a item being relevant (or being at the top).”\n\nMost are negative, though! (See below, Example: ranking stocks for a portfolio &gt;&gt; Predictions on the test set)\n\nThe scores are ordered into a ranking and can be evaluated using a ranking metric (Precision@k, MAP@K, nDCG@K)\nExample: py, LightGBM, ranking stocks for a portfolio (article)\n\nTarget is “Target.” It might be daily returns that being used as the relevance score. Queries is “Date” and the documents are stocks (“SecuritiesCode”)\nThe target variable is a real-valued float and has negative values, so it was ranked per Date, then binned into quantiles.\n\n\ndf[\"Target\"] = df.groupby(\"Date\")[\"Target\"].rank(\"dense\", ascending=False).astype(int)\ndf[\"Target\"] = pd.qcut(df.Target, 30).cat.codes\n\ncat.codes will give what are essentially ranks after the binning\nTime Series Splits\n\n# Just some arbitrary dates\ntime_config = {'train_split_date': '2021-12-06',\n              'val_split_date'  : '2022-02-10',\n              'test_split_date' : '2022-02-20'}\ntrain = df[(df.Date &gt;= time_config['train_split_date']) & (df.Date &lt; time_config['val_split_date'])]\nval = df[(df.Date &gt;= time_config['val_split_date']) & (df.Date &lt; time_config['test_split_date'])]\ntest = df[(df.Date &gt;= time_config['test_split_date'])]\n\nGroup parameter\n\nquery_train = [train.shape[0] /2000] * 2000  # Because we have 2000 stocks in each time group\nquery_val = [val.shape[0] / 2000] * 2000\nquery_test = [test.shape[0] / 2000] *2000\n\nModel\n\nfrom lightgbm import LGBMRanker\ncol_use = [c for c in df.columns if c not in [\"RowId\",\"Date\", \"Target\"]] # predictors\nmodel_return = LGBMRanker(n_estimators=15000,\n                          random_state=42,\n                          num_leaves=41,\n                          learning_rate=0.002,\n                          #max_bin =20,\n                          #subsample_for_bin=20000,\n                          colsample_bytree=0.7,\n                          n_jobs=2)\nmodel_return.fit(train[col_use], train['Target'],\n            group = query_train,\n            verbose=100,\n            early_stopping_rounds=200,\n            eval_set=[(val[col_use], val['Target'])],\n            eval_group=[query_val],\n            eval_at=[1] #Make evaluation for target=1 ranking, I choosed arbitrarily\n                )\n\n“early_stopping_rounds=200” says training continues until there’s no improvement for 200 rds.\n“eval_at”  are the k values used to evaluate nDCG@k over the validation set\nPredictions on the test set\n\nfor (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\n    try:\n        sample_prediction['Rank'] = model_return.predict(prices[col_use]) * -1\n        # Get the ranks from prediction first and for the duplicated ones, just rank again\n        sample_prediction['Rank'] = sample_prediction.groupby(\"Date\")[\"Rank\"].rank(\"dense\", \n                                                                                  ascending=False).astype(int)\n        sample_prediction['Rank'] = sample_prediction.groupby(\"Date\")[\"Rank\"].rank(\"first\").astype(int) - 1\n    except:\n        sample_prediction['Rank'] = 0\n    sample_prediction = sample_prediction.replace([-np.inf, np.inf], np.nan).fillna(0.0)\n    # register your predictions\n    env.predict(sample_prediction)\n    display(sample_prediction)\n\n“iter_test” is some kind of list or maybe json object imported from the kaggle (this example is a kaggle notebook) that has the unseen data to be used to predict with your final model (“# get iterator to fetch data day by day”)\nPreds are multiplied by -1 because most are probably negative for some reason (see prev. example)\nThey’re ranked per query (“Date”)\nThe stock ranked first for each query (“Date”) is pulled",
    "crumbs": [
      "Algorithms",
      "Learn-to-Rank"
    ]
  },
  {
    "objectID": "qmd/algorithms-marketing.html",
    "href": "qmd/algorithms-marketing.html",
    "title": "Marketing",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Algorithms",
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/algorithms-marketing.html#sec-alg-mark-misc",
    "href": "qmd/algorithms-marketing.html#sec-alg-mark-misc",
    "title": "Marketing",
    "section": "",
    "text": "Also see Business Plots\nPropensity Model and Uplift Score Model are basically predicting the same thing with the Uplift Score Model notes going a bit further in detail with the marketing experiment\nPopulation targeting based on your average conversion rate (Thread)\n\nLow avg conversion rate: Treat those with higher probabilities of buying, because the extra nudge helps them get over the finish line.\nHigh avg conversion rate (&gt;80%): Treat those that are less likely to convert\nMiddling avg conversion rate: Treat those with predictions closer to 0.5\n\nLimitations of a simple rules based approach (typical baseline model)\n\nIt is likely not exploiting all the data you have at your disposal whether it be more precise information on the customer journey or your website or other data sources you may have at your disposal like CRM data.\nWhile it seems obvious that customers classified as “Hot” are more likely to purchase than “Warm” which are more likely to purchase than “Cold”, this approach does not give us any specific figures on how likely they are to purchase. Do “warm” customers have 3% chance to purchase ? 5%? 10% ?\nUsing simple rules, the number of classes you can obtain is limited, which limits how customized your targeted response can be.",
    "crumbs": [
      "Algorithms",
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/algorithms-marketing.html#sec-alg-mark-prop",
    "href": "qmd/algorithms-marketing.html#sec-alg-mark-prop",
    "title": "Marketing",
    "section": "Propensity Model",
    "text": "Propensity Model\n\nUses GA data for your website to model probabilities of a customer purchasing\nHelps marketers to decrease cost per acquisition (CPA) and increase ROI\n\nYou might want to have a different marketing approach with a customer that is very close to buying than with one who might not even have heard of your product.\nAlso if you have a limited media budget , you can focus it on customers that have a high likelihood to buy and not spend too much on the ones that are long shots\n\nExample: Using Google Analytics data\n\nNotes from Scoring Customer Propensity using Machine Learning Models on Google Analytics Data\nData\n\nUsed GA360 so the raw data is nested at the session-level\n\nSee Google, Analytics &gt;&gt; Misc &gt;&gt; Google Analytics data in BigQuery for more details on this type of data\n\nAfter processing you want 1 row per customer\nGA keeps data for 3 months by default\n\nCreate features\n\nGeneral Features - metrics that give general information about a session\nFavorite Features - Categorical data\n\nA user can have multiple categories so you’ll have to count instances for each category and choose a favorite and do something about ties\nSome variables may have high cardinality\n\nAfter choosing a “favorite” for each user, group categories with low counts into an Other category.\nDepending on how many categories are left, you’ll need to choose a encoding method\n\n\nProduct Features - numerics that help answer if a customer is likely to buy a specific product\nSimilar Product Features - numerics for substitute products which are products similar to the product of interest\n\nKnowing that the customer interacted with other products with similar function and price range can definitely be useful.\nSimilar products for a given product are defined using business inputs\n\nNotes\n\nNot familiar with marketing campaigns but product, similar product features might be optional depending if the campaign is for a specific product or a group of products or general sale (e.g. black friday) or maybe the campaign is taylored to each group of customers.\n\n\nProcessing\n\nEach row is a customer\nCompute the features by aggregating values over a 3 month time window for each customer\nCompute the target (purchase/no purchase) using the sessions in a 3 weeks time window subsequent to the feature time window to detect whether the customer purchased or didn’t purchase over this window.\n\nIf there is at least one purchase of the product in the time window, Target it equal to 1 (defined as Class 1), else Target is equal to 0 (defined as Class 0)\nLikely a strong class imbalance. Potential solutions:\n\nupsample/downsample as appropriate\nSwitch the target variable from “making a purchase” to making an “add to cart”\n\nmodel looses a bit in terms of business signification but increasing the volume of Class 1 more than compensates\n\n\n\n\nFit a binary classification model (add_to_cart/didn’t add_to_cart)\n\nAlso see Cross-Validation &gt;&gt; Sliding Window CV\nInstead of each fold using a different block of observations for the validation/test set, each successive fold slides the interval of the target variable interval (e.g. 3 weeks) from the previous fold\nSplit the data before creating the folds to avoid leakage.\nFor each algorithm average loss (e.g. PR-AUC score) across test folds.\nChose your algorithm and fit the final model on the whole dataset\n\nCalculate Model Uplift\n\nSort customers by their probability score by dividing the customers into ventiles (i.e. 20 bins).\nUplift is defined as the Class 1 Rate in the top 5% (1st bin) / the Class 1 Rate across all the dataset (all bins).\n\nClass 1 Rate of top 5% = number of customers in the 1st bin that added_to_cart / total number of customers in the 1st bin\nClass 1 Rate of whole dataset = number of customers that added_to_cart in the dataset / total number of customers in the dataset\n\nExample:\n\nIf we have 21% Add to Cart in the top Top 5 % of the dataset vs 3% Add to cart Rate in whole dataset\nuplift = 7, which means our model is 7 times more effective than a random model.\n\nI’m not sure if I’d say a “random” model since this implies that customers are chosen at randomly. If the whole dataset was used, then doesn’t this imply that no “choice” made? So, I’d say this is the uplift over no model being used\n\n\n\nGo live with Ad campaign and calculate ROAS (Return on Ad Spend)\n\nArticle didn’t really touch on the graphic but these are my assumptions\nSplit into groups\n\nLooks like only group is chosen to take part in the experiment and the rest are control or rule-based (see Misc section).\n\nSplit into Segments\n\nThis looks like the binning of probabilities that occurred during the uplift calculation earlier\nOnly 8 bins are shown but that is probably just because of the size of the graphic. Should probably use 20 bins like before\n\nActivation\n\nThe first 3 bins get ad money. This would map to the top 5%, 10%, and 15% bins with the 5% bin getting twice the ad spend as the others.\n\nEvaluation\n\nIt says it’s based on conversions but I’m guessing they get to Revenue somehow since ROAS should be in dollars\nIf you can’t get the Revenue number directly see Uplift Model &gt;&gt; 3 types of Uplift calculations for each type of offer\n\nShows how to get from Conversion Uplift to and estimate of Revenue Uplift\n\n\n\nCompare the model’s ROAS with Control or Rules-based groups\n\nGoogle also has Session Quality Score which comes which is available to compare the ROAS too.",
    "crumbs": [
      "Algorithms",
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/algorithms-marketing.html#sec-alg-mark-uplift",
    "href": "qmd/algorithms-marketing.html#sec-alg-mark-uplift",
    "title": "Marketing",
    "section": "Uplift Score model",
    "text": "Uplift Score model\n\nAlso see\n\nMarketing &gt;&gt; Customer Segmentation, Customer Journey\n\nSimilar goals to Propensity Model\nNotes from https://towardsdatascience.com/uplift-modeling-e38f96b1ef60\nGoal:\n\nidentify which customers are most likely to purchase when given an offer (e.g. buy one get one free).\nHelps to control marketing costs by efficiently targeting the customers where marketing spending will be most effective.\n\nSteps:\n\nRun an experiment where you’ve taken a group of customers and randomly assigned them into treatment and control groups and recorded whether or not they purchased after receiving the offer and a period of time. Here the treatment is the custormer receiving a discount offer or a buy-one-get-one free offer.\nCalculate the three uplift metrics for each offer type.\nFit a binary class model that provides conversion probabilities for each customer.\n\nAlso see Propensity Model\n\nFit a multiclass model whose probabilities will be used to calculate an uplift score for each customer.\n\nThe conversion probabilities will be used to investigate which features are driving the conversion uplift.\nThe upper quantile uplift scores can be used to target customers that are more likely to respond to a particular offer.\n\n\nMetric Calculations\n\nUplift\n\nConversion Uplift (%) = Conversion rate of treatment group - conversion rate of control group.\n\n“conversion” means a purchase was made.\n\nLarger is better.\n\nPurchase rate increase/decrease resulting from the offer campaign\n\nOrder Uplift = Conversion uplift * # converted customers in treatment group.\n\nThe number of purchases as a result of the offer campaign\n\nRevenue Uplift ($) = Order Uplift * Average order $ value.\n\nExpected revenue as a result of the offer campaign\n\n\nbase conversion rate:\n\n\n        df %&gt;%\n            filter(offer = \"no offer\") %&gt;%\n            summarize(base_conv = mean(conversion))\n\noffer has 3 categories: no offer, discount, buy_one_get_one (bogo)\n\ndiscount and bogo conversion rates similar. Calc by filtering the different offers\n\ndiscount conversion uplift = discount conversion rate - base conversion rate\n\nbogo conversion uplift similar\n\n# of converted customers in discount group:\n\n        df %&gt;%\n            filter(offer = \"discount\", conversion == 1) %&gt;%\n            count( )\n\ndiscount order uplift = discount conversion uplift * # of converted customers in discount group \nFor Revenue Uplift equation, “Average order $ value” was a constant which I guess is just a descriptive statistic you could easily calc from your sales data.\nViz: group_by predictor var –&gt; mutate calc conversion mean –&gt; bar plot with x = pred var, y = conversion mean\n\nbars with the higher conversion means are more important in driving the conversion uplift metric\n\nModels\n\nModel conversion probabilities: xgboost with conversion as the target var\n\npredict on the test set\nModel GOF: on the test set, compare the predicted and real order upticks for discount\n\nreal_order_upticks = length(test_dat) * (conversion mean for offer == discount - conversion mean for offer == no offer)\npredicted_order_upticks = length(test_dat) * (predicted conversion probabilities mean for offer == discount - predicted conversion probabilities mean for offer == no offer)\nerror = abs(real - predicted)/real\nreal revenue = real_order_upticks * avg order $ value (similar for predicted revenue)\ndo the same for bogo \n\n\n  add probabilities from a multi-class model where the classes of your target variable are:\n\nTreatment Responders (TR): Customers were given offer and did purchase\nTreatment Non-Responders (TN): Customer were given offer and didn’t purchase\nControl Responders (CR): Customers weren’t given offer and purchased\nControl Non-Responders (CN): Customers weren’t given offer and didn’t purchase\n\nCreate a target variable with the classes above.\n\nTreatment will include customer who received either a discount or bogo offer and control is made up of the “no offer” customers.\n\nMay need to make the target variable numeric for xgboost.\n\n\nIn the example, there was a history variable which was the total amount($) that was purchased by the customer in the past. This variable was k-means clustered into 5 clusters, and the clusters were used as a variable. e.g. customer id# 12 belongs to cluster 4. I don’t know if this is a better way of binning a continuous variable or what. Need to research if maybe it’s some sort of Kaggle thing or something shown to work well with xgboost. 5 clusters were used with no explanation of how that number was reached.\nPredictor variables used:\n\nrecency: months since last purchase\nclustered_history: clustered history variable which was the $value of the historical purchases\nused_discount: indicator on whether the customer had used a “discount” offer before this experiment\nused_bogo: indicator on whether a customer had used a “buy one get one” offer before this experiment\nzip_code: class of the zip code as Suburban/Urban/Rural\nis_new: indicates if the customer is new\nchannel: methods of contact that the customer is using, Phone/Web/Multichannel\n\nXGBoost model trained. Predict( ) using training data to get class probabilities.\nTake target var predicted probabilities and calculate the uplift score for each customer using the above formula.\nModel results analysis: compare uplift metrics of customers with an upper quantile (&gt; 0.75) uplift score to those with a lower quantile (&lt; 0.25). Use test set.\n\nTake the Revenue Uplift from the first section and calc the revenue uplift per targeted customer. This will be used as a baseline to compare the two quantiles against.\n\n base_Discount Revenue Uplift per targeted = Discount Revenue Uplift / # of customers with offer == discount\n\nfilter test data where uplift_score &gt; quantile(uplift_score, 0.75) –&gt; calc the Discount Uplift metrics –&gt; calc the test_Discount Revenue Uplift per targeted customer.\nEstimated percentage change in the Discount Revenue Uplift per targeted customer = (test_DRU_per_cust - base_DRU_per_cust) / base_DRU_per_cust\nSays that the model estimates that targeting the upper quantile uplift scores with a discount offer will lead to a  increase in revenue uplift per customer.",
    "crumbs": [
      "Algorithms",
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/algorithms-marketing.html#sec-alg-mark-clv",
    "href": "qmd/algorithms-marketing.html#sec-alg-mark-clv",
    "title": "Marketing",
    "section": "Customer Lifetime Value (CLV)",
    "text": "Customer Lifetime Value (CLV)\n\nThe present value of all future cash flows of a current customer (inflation adjusted and usually cost of capital adjusted)\n\nThink the cost of capital adjustment is taken care of by the discount rate used in the DERL calculation below.\nAlso see\n\nAlgorithms, Product &gt;&gt; Cost Benefit Analysis (CBA) &gt;&gt; Internal Rate of Return\nProject, Planning &gt;&gt; Decision Models &gt;&gt; Terms &gt;&gt; Cost of Capital, Internal Rate of Return\nMorgan Stanley’s Guide to Cost of Capital\n\nThread about the guide\n\n\n\nAllows marketers to ensure that the most valuable customers have the least churn probability in the long term\nThe more valuable the customer is, the more likely you should be to build products they’d buy, to buy ad space in the magazines they read, to provide a dedicated sales person to them, and to proactively address their needs.\nMisc\n\nNotes from:\n\nhttps://towardsdatascience.com/calculating-customer-lifetime-values-using-a-shifted-beta-geometric-model-86bf538444f4\nhttp://www.brucehardie.com/notes/018/DERL_in_Excel.pdf\n\nAlso see\n\nMarketing &gt;&gt; Conversion Lift Model &gt;&gt; Cost per Incremental Conversion\nOverview of paper, A Deep Probabilistic Model for Customer Lifetime Value Prediction\nVideo: Bizsci\n\nlab-58-customer-lifetime-value-rfm-calc\npython customer lifetime value, rfm + xgboost (youtube)\n\n\nPackages: {CLVTools}, {BTYD}, {BTYDplus}, {{btyd}}\nAccountants discount the result to today’s dollar value so that you know how much each customer is worth now (that is, taking future inflation into account).\n\nWhat is it used for?\n\nmarketing campaigns — how much should we spend to acquire or retain customers?\n\nacquire a customer:\n\nSelect a marketing campaign whose acquisition cost is lower than the CLV for an average active customer\n\nacquisition cost = total_campaign_spend / # of new customers acquired \n\n\nretain: spend on each customer according to their current years (retention length) as an active customer\n\nGuessing marketing decides what percentage of the clv is appropriate to spend\nUse retention rate to judge new features to apps or webpages\nDoes this marketing campaign attract users with high retention rates?\n\n\ncustomer segmentation — who are our most valuable customers and what are their demographic and behavioral traits?\n\nSee Marketing &gt;&gt; Customer Segmentation\n\noverall health of the business — how is our CLV changing over time? (growth metric)\ntotal company value — how can we estimate the value of our existing and future customers?\nused as a constant in loss functions for ML models\n\nIf your model outputs a false positive and your resultant actions cause you to lose a good customer, then the CLV of that customer is a cost related to the false positive.\n\n\nPrimary ways to increase CLV\n\nDecrease churn\nIncrease prices\nSell more to each customer\n\nBusiness model elements must be taken into account when calculating CLV\n\ncontractual vs non-contractual\n\ncontractual: subscription, credit cards, software-as-a-service\nnon-contractual: grocery store, car sales\n\ncontinuous vs discrete (transaction frequency)\n\ncontinuous: credit card payments to a retail business, i.e. unpredictable\ndiscrete: customers pay on a fixed cycle, e.g. monthly, yearly payments\n\n\nnon-contractual and continuous\n\nDaily activity rate for an app used as a retention rate\n\nUsers are grouped by start date which is their cohort\nFor each cohort, the number of users that are active on the app is counted each day after their start date\nRentention rate_day = user_count_day/cohort_size\n\nSo, there’s a “retention rate” for each day after the start date\n\nThis data can be used to perform an A/B test to monitor how each cohort reacts in terms of app activity after a push notification or marketing campaign of some sort.\n\nBG/NBD Gamma-Gamma model\n\nBG/NBD model predicts the future number of transactions per customer, which is then fed into the Gamma-Gamma model to predict their monetary value\nMisc\n\nNotes from: Buy Till You Die: Understanding Customer Lifetime Value\n\nExample using {{btyd}}\n\nAlso see:\n\nBeta Geometric Negative Binomial Distribution (BG-NBD) model: explainer, py example\n\npredicts the future number of purchases made by a customer as well as their probability of being alive, i.e. not churned yet\n\n\nPackages\n\n{BTYDplus} - various types of Bayesian NBD models\n{{btyd}}\n\n\nBG/NBD\n\nData needed for each customer: time, volume, and value of each of their purchases\nCalculate these features:\n\nRecency, length of time since a customer’s last purchase or last visit to the website or the mobile app\n\nCan be in units of hours, days, weeks, etc (Assume Time and Recency should have the same units)\n(customers[\"max\"] - customers[\"min\"]) / np.timedelta64(1, freq) / freq_multiplier\n\nFrequency, or the count of time periods the customer made a purchase in\n\nRemove all the rows in which frequency is zero, that is all customers who have made only one purchase\nImportant note: some resources around the web claim that frequency is the number of repeat purchases the customer has made which makes it one less than the total number of purchases. This is incorrect, since frequency should ignore multiple purchases in the same time period\n\nTime, or the customer’s age (this is the time difference between the customer’s first purchase and the end date of our analysis, often the last date available in the data)\n\nCan be in units of hours, days, weeks, etc (Assume Time and Recency should have the same units)\n\nMonetary value, or the average revenue or income from the customer’s repeat purchases\n\nAssumptions\n\nTransactions\n\nNumber of transactions has a Poisson dgp\n\nλ = 2 means that a customer makes two purchases per time period, on average\n\nThe rate λ is different for each customer, and its distribution over all customers is assumed to be the Gamma distribution\nWhen the number of transactions is 0, the customer has churned\n\nChurn\n\nAfter each purchase, a customer might churn with some probability p. This probability is different for each customer, and its distribution over all customers is assumed to be the Beta distribution.\n\n\nUse model to estimate distribution parameters:\n\nModel takes Recency, Frequency, and (I think) Time variables\nGamma (customer transaction rates): r and α\nBeta (churn probability): a and b\n\nPlot distributions with parameter estimates to see if they look reasonable\nFrequency-Recency Heatmap\n\n\nColored by the expected number of future purchases in a predefined time span (e.g. 7 days) for a given Recency and Frequency value of a customer\nNote that 0 on the y-axis is at the top\nInterpretation\n\nBottom-Right: we can expect the most purchases from customers who have historically featured high frequency and high recency\n\nTop-Right: The usually high-freq customers that haven’t bought in a while\n\nBottom-Left: Customers that buy infrequently, but we have seen them recently, so they might or might not buy again — hence the tail of decreasing predictions stretching towards the bottom-left corner\n\n\nChurn Heatmap\n\n\nColored by churn probability for a given Recency and Frequency value of a customer\nNote that 0 on the y-axis is at the top\nInterpretation\n\nBottom-Right: high-freq-recently-seen customers are the most likely to have not churned (“still alive”)\n\n\n\n\n\ncontractual and discrete\n\ncustomers are cohorted based on how many consecutive years they’ve been active customers. An active customer is a customer with a current contract/subscription.\n\nCohort 1 has customers who have their initial contract + 3 contract renewals.\n\n\nGreen represents current active customers\n\n\nThe retention rate, r, for each cohort, k, and number of active years , t,  is the ratio of active customers, c, for that subscription year and the previous subscription year.\n\n\n\nexample for cohort 1, 2nd interval:  \n\n\nThe churn rates which are just 1-r.\nThe annual survival rates for each cohort\n\n  where k is the cohort and t is the final contract year for which r is being calculated.\n\nFor cohort 0:\n\n\n\nForecasting survival rates\n\nChurn rates can be modeled with Beta Regression which can be used to calculate retention rates and therefore survival rates\n\nA basic forecast model would be to use the average retention rate of all the cohorts\n\nExample: If   , then  \nUsing this method neglects the fact that as customer loyalty (# of consecutive subscriptions) increases the likelihood a customer subscribes the next year (i.e. survives) increases\n\nTechnically, the literature says that a “shifted-beta-geometric (sBG) distribution” is used to calculate the α and β parameters of a Beta Distribution. Unless this is a different calculation from a standard Beta distribution, the parameters can be calculated using the sample mean and sample standard deviation. Compare “DERL PAPER” bkmk’d in the CLV folder with calculation from bkmk in Beta regression folder.\nCurves and interpretations wrt to various Beta Regression parameter values, α and β\n\n\n\n\nA survival rate can be calculated using the retention rates from formula using the model estimates for α and β.\n\n\n\n\n\nCompute Discounted Expected Residual Lifetime (DERL)\n\nA CLV measure for contractual businesses\n\n\nFormula assumes a constant net cashflow per period (e.g. contract = $1000 per customer per year)\n“active in n” refers to number of years as an active customer\n\nsee first sentence of the “contractual and discrete” section above for an active customer definition\nsee 3rd bullet below for an example\n\nd: discount rate (aka hurdle rate)\n\nMinimum expected return on investment. 10% is common and 5% is usually the minimum, i.e. a replacement-level ROI. If the discount rate is 10% percent, then that says that the current investment strategies are yielding at least 10%, so this one should be at least that good. The thinking is, “why spend money on this investment if it has a lower discount rate instead of putting it to work in one of the other strategies that’s yielding at least 10%?”\n\nThe summation goes from n to infinity but in practice, is stopped when term values approach zero.\n\nExample goes out to t = 200 and the last term is equal to 5 x 10^-12. which seems like extreme overkill to me.\n\nExample for cohort 2: 3 active years with 2 renewals\n\nCalculate the Survival Rates out to the specified number of terms\n\nS(0) = 1, S(1) = [(beta + (t=1) -1) / (alpha + beta + (t=1) - 1)] * S(0), S(2) = same equation with t=2 * S(1) … compute until S(200)\n\nConceptually, S(0) makes sense because t = 0 is the initial contract, so everyone “survived.” Therefore the survival rate = 1. But it doesn’t really follow if you set t = 0 in the retention rate equation\n\n\nCalculate the Survival Rate Ratios,  , first half of the DERL equation\n\nWe calculating for cohort 2, where custormers have n = 3 active years, so the denominator with always be n-1 =2. The number just follows the index, t, in the summation.\nS(3)/S(2), S(4)/s(2), …, S(200)/S(2)\n\nCalculate the Discount Rate half of the equation using a 10% discount rate,  :\n\n(1/(1+0.10))^(3-3), same^(4-3), …, same^(200-3).\nSo this ratio acts a scaling factor than makes the terms smaller and smaller.\n\nderl_2 = sum(element-wise product of last two steps) \n\nIn R, this would some kind of recursive loop? Maybe a map2 function with a vector with t-values and a constant n vector (so as not to hard-code) as arguments. Or a pmap and include a constant, d vector\nfor a customer who has made two contract renewals (evaluated just before the point in time at which the third contract renewal decision is to be made)\n\n\n\n\nValue of Consumer Base\n\n\nValue = [(# number of consumers in cohort 0) * contract_value * derl_0] + [(number of consumers in cohort 1) * contract_value * derl_1] + …",
    "crumbs": [
      "Algorithms",
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/algorithms-marketing.html#sec-alg-mark-churn",
    "href": "qmd/algorithms-marketing.html#sec-alg-mark-churn",
    "title": "Marketing",
    "section": "Churn",
    "text": "Churn\n\nMisc\n\nAlso see\n\nMarketing &gt;&gt; Workflow &gt;&gt; Churn examples\nDiagnostics, Classification &gt;&gt; Scoring &gt;&gt; Comparison of similar scoring models & Custom Cost Functions\nGoogle, Analytics &gt;&gt; Explore &gt;&gt; User Lifetime & Segment overlap\nProduct Development &gt;&gt;\n\nWhy do leave and stay?\nMetrics &gt;&gt; Growth Metrics\n\nAlgorithms, Product &gt;&gt; Retention Analysis\n\n\nIt usually costs more to acquire a customer than it does to retain a customer. Focusing on customer retention enables companies to maximize customer revenue over their lifetime.\nPotential targets\n\nCancellation of last product, no transactions in the last three months\nAlso contractual (e.g., bank) and non-contractual (e.g., e-shop) client relationships.\n\nPotential features\n\nBeware of look-ahead bias when selecting features\n\nIf you’re trying to predict an event and your labels are determined with knowledge of future events, you’re also introducing look-ahead bias.\nExample: if you label a customer’s eventual outcome as “churned” during a time period when they hadn’t actually churned yet, then your model has access to future information in the training data.\n\nSocio-Demographic\n\nProducts Owned,\nHistorical Transactions\nClient-Company Interaction (i.e. active user?),\ne-Commerce Behaviour\n\n\nAlso important to be careful about how far in advance we want to estimate the propensity to leave. In other words, how long is the time between the day we look at clients through the available features and the day we can tell if they have left? If that time is too short, we won’t have much time to make any kind of response. If, on the other hand, it is too long, the model will be less accurate and up to date.\nCHAID (chi-squared automated interaction detection)\n\nold school, interpretable method\nBins continuous variables, integers coerced to factors, and seems to be a decision tree model where the splits are chosen by a chi-square test between each predictor and the churn outcome variable.\nNeed to distinguish between predictors that are nominal or ordinal\nThis article has other tutorial links and good visuals, https://ibecav.netlify.com/post/analyzing-churn-with-chaid/\n\n2 stage model approach\n\nDon’t model who was most likely to leave, model who could best be persuaded to stay—in other words, which customers considering jumping ship would be most likely to respond to a promotion. More efficient marketing spend\n\ni.e. swing customers - like politicians looking for swing voters because they are persuadable.\n\nIf you model the wrong objective, you can squander money on swaths of customers who were going to defect anyway and underinvest in customers they should have doubled down on.\n\nThink this has to be 2 stages\n\nFilter data before promotions over some window –&gt; model traditional churn –&gt; filter data after promotion –&gt; label which likely churns left and which ones didn’t –&gt; model churn with new churn labels using data after promotion because you want probability of churn given promotion\n\nSo you’d have 2 models: 1 to identify churners and 1 to identify swing customers from churners\n\nCan we determine which types of incentive would work best with each type of customer?\n\n\n\nSurvival Model\n\nPredict probability of leaving (i.e. churn) at several time points over the next months\nAllows you to anticipate and prioritize your marketing actions more effectively in time and, ultimately, reduce the churn rate.\nExample: Given a customer has contacted support, predict the probability of unsubscribing over time. (article)\n\nData\n\nthe interaction: the date, the reason for the call (sign-up/support), and the channel (email/phone).\nthe customer: age and gender.\nsubscription: product, price, billing cycle (monthly/annual), sign-up\n\nFeature engineering: the number of times the customer has contacted the company in the past, the duration since the customer subscribed, and cyclical date-related features.\nK-M survival curve\n\nCustomers with a monthly subscription are more volatile, and they tend to churn more often and faster during the first years after their subscription\n\nCox proportional hazards\n\nProportional Hazards Assumption (for churn):\n\nIf a customer has a risk of churn at an initial observation that is twice as low as another customer, then for all subsequent time observations, the risk of churn remains twice as low.\n\nSurvival Function for 5 randomly selected customers\n\nCustomer 2 is most likely to churn in the first few days, while customers 1, 3, and 4 are at much less risk\n\nCumulative Hazards Function for 5 randomly selected customers\n\nAgrees with results of the survival function\n\nCoefficients\n\nML methods\n\nAlso see Regression, Survival &gt;&gt; ML\nUsing Dynamic AUC as the metric makes it possible to evaluate each model only on the time points that are most important in the context\n\nSee Regression, Survival &gt;&gt; Diagnostics\n\nResults on the validation set\n\nDynamic AUC scores over the first 2 years (see other charts)\nNot really much difference between the RF, GBM, and Cox PH models after about 200 days, but the GBM had the best average AUC over the 2-year period of around 0.80\n\n\n\n\nDiagnostics\n\nsensitivity/recall (how many of the clients who actually leave were detected by the model)\n\nin churn prediction we usually have higher costs on false negatives\n\nprecision (how many of the clients identified by the model actually left)",
    "crumbs": [
      "Algorithms",
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/algorithms-ml.html",
    "href": "qmd/algorithms-ml.html",
    "title": "1  ML",
    "section": "",
    "text": "Discriminant Analysis",
    "crumbs": [
      "Algorithms",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ML</span>"
    ]
  },
  {
    "objectID": "qmd/algorithms-ml.html#sec-alg-ml-discrim",
    "href": "qmd/algorithms-ml.html#sec-alg-ml-discrim",
    "title": "1  ML",
    "section": "",
    "text": "Misc\n\nThe features to train Quadratic Discriminant Analysis (QDA) should be strictly normally distributed, making it easy for QDA to calculate and fit an ellipsoid shape around the distribution\nVery fast even on a 1M row dataset\n\nLinear Discriminant Analysis (LDA)\n\nNotes from StatQuest: Linear Discriminant Analysis (LDA) clearly explained video\nGoal is find an axis (2 dim) for a binary outcome or plane (3 dim) for a 3-category outcome, etc. that separates the predictor data which is grouped by the outcome categories.\nHow well the groups are separated is determined by projecting the points on this lower dim object (e.g. axis, plane, etc.) and looking at these criteria:\n\nDistance (d) between the means of covariates (by outcome group) should be maximized\nScatter (s2) ,i.e. variation, of data points per covariate (by outcome group) should be minimized\n\nMaximizing the ratio of Distance to Scatter determines the GoF of the separator\n\n\nFigure shows a example of a 2 dim predictor dataset that been projected onto a 1 dim axis. Dots are colored according to a binary outcome (green/red)\nIn the binary case, the difference between the means is the distance, d.\n\nFor multinomial outcomes, there are a couple differences:\n\nA centroid between all the predictor data is chosen, and centroids within each category of the grouped predictor data are chosen. For each category, d is the distance between the group centroid and the overall centroid\n\n\nThe chosen group predictor centroids are determined by maximizing the distance-scatter ratio\n\nUsing the coordinates of the chosen group predictor centroids, a plane is determined.\n\n\nFor a 3 category outcome, 2 axes (i.e. a plane) are determined which will optimally separate the outcome categories\n\n\nBy looking at which predictors are most correlated with the separator(s), you can determine which predictors are most important in the discrimination between the outcome categories.\nThe separation can be visualized by using charting the data according to the separators\n\n\nExample shows the data is less overlap between black and blue dots and therefore grouped better using LDA than PCA.",
    "crumbs": [
      "Algorithms",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ML</span>"
    ]
  },
  {
    "objectID": "qmd/algorithms-ml.html#sec-alg-ml-svm",
    "href": "qmd/algorithms-ml.html#sec-alg-ml-svm",
    "title": "1  ML",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\n\n\nMisc\n\nPackages: {e1071}, {kernlab}, {LiblineaR}, {{sklearn}}\nAlso see\n\nModel Building, tidymodels &gt;&gt; Model Specification &gt;&gt; Support Vector Machines\nModel Building, sklearn &gt;&gt; Misc &gt;&gt; Tuning\n\n\nProcess\n\nWorks on the principle that you can linearly separate a set of points from another set of point simply by transforming the dataset from dimension n to dimension n + 1.\n\nThe transformation is made by a feature transformation function, φ(x). For two dimensions, a particular φ(x) might transform the vector, x = {x₁, x₂}, which is in 2 dimensions, to {x²₁, √(2x₁x₂), x²₂}, which is 3 dimensions\n\nTransforming a set of vectors into a higher dimension, performing a mathematical operation (e.g. dot product), and transforming the vectors back to lower dimension is involves many steps and therefore is computationally expensive.\n\nThe problem can become computationally intractable fairly quickly. Kernels are able perform these operations in much fewer steps.\n\nCreates a hyperplane at a threshold that is equidistant between classes of the target variable\nEdge observations are called Support Vectors and the distance between them and the threshold is called the Maximum Margin\n\nKernels\n\nGaussian Kernel\n\\[\nK(x,y) = e^{-\\gamma \\lVert x - y \\rVert^2}\n\\]\n\nK(x,y) performs the dot product in the higher dimensional space without having to first transform the vectors\n\n\nHyperparameters\n\ngamma – All the kernels except the linear one require the gamma parameter. ({e1071} default: 1/(data dimension)\ncoef0 – Parameter needed for kernels of type polynomial and sigmoid ({e1071} default: 0).\ncost – The cost of constraints violation ({e1071} default: 1)—it is the ‘C’-constant of the regularization term in the Lagrange formulation.\n\nC = 1/λ (R) or 1/α (sklearn)\nWhen C is small, the regularization is strong, so the slope will be small\n\ndegree - Degree of the polynomial kernel function ({e1071} default: 3)\nepsilon - Needed for insensitive loss function (see Regression below) ({e1071} default: 0.1)\n\nWhen the value of epsilon is small, the model is robust to the outliers.\nWhen the value of epsilon is large, it will take outliers into account.\n\nnu - For {e1071}, needed for types: nu-classification, nu-regression, and one-classification\n\nRegression\n\n\nStochastic Gradient Descent is used in order minimize MAE loss\n\nAlso see\n\nModel building, sklearn &gt;&gt; Stochaistic Gradient Descent (SGD)\nLoss Functions &gt;&gt; Misc &gt;&gt; Mean Absolute Error (MAE)\n\n\nEpsilon Insensitive Loss - The idea is to use an “insensitive tube” where errors less than epsilon are ignored. For errors &gt; epsilon, the function is linear.\n\nEpsilon defines the “width” of the tube.\nSee Loss Functions &gt;&gt; Huber loss for something similar\nSquared Epsilon Insensitive loss is the same but becomes squared loss past a tolerance of epsilon\n\nL2 typically used the penalty\nIn SVM for classification, “margin maximization” is the focus which is equivalent to the coefficient minimization with a L2 norm. For SVR, usually the focus is on “epsilon insensitive.”\n\nVisualization\n\nDecision Boundary\n\nExample\n\n# Create a grid of points for prediction\nx1_grid &lt;- seq(min(data$x1), max(data$x1), length.out = 100)\nx2_grid &lt;- seq(min(data$x2), max(data$x2), length.out = 100)\ngrid &lt;- expand.grid(x1 = x1_grid, x2 = x2_grid)\n\npredicted_labels &lt;- predict(svm_model, newdata = grid)\n\nplot(data$x1, data$x2, col = factor(data$label), pch = 19, main = \"SVM Decision Boundary\")\npoints(grid$x1, grid$x2, col = factor(predicted_labels), pch = \".\", cex = 1.5)\nlegend(\"topright\", legend = levels(data$label), col = c(\"red\", \"blue\"), pch = 19)\n\n(Legend colors in the wrong order)\nx1_grid and x2_grid provide equally spaced points within the range of sample data.\ngrid is a df of all combinations of these points\nThe predicted labels from [grid] are colored and visualize the decision boundary.\n{e1071} provides a plot function that also does this: plot(svm_model, data = data, color.palette = heat.colors)",
    "crumbs": [
      "Algorithms",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ML</span>"
    ]
  },
  {
    "objectID": "qmd/algorithms-ml.html#sec-alg-ml-trees",
    "href": "qmd/algorithms-ml.html#sec-alg-ml-trees",
    "title": "1  ML",
    "section": "1.1 Trees",
    "text": "1.1 Trees\n\nAlgorithmic models that recursively split the data into smaller and more homogeneous subgroups. Predictions are the same for every member of the subgroup (aka piece-wise constant). Forests smooth out the piecewise predictions by averaging over groups of trees.\nHow Tree models get probabilities\n\nMethod 1: Each tree predicts the class of x according to the leaf node x falls within. The leaf node output is the majority class of the training points it contains. The predictions of all trees are considered as votes, and the class with the most votes is taken as the output of the forest. This is the original formulation of random forests proposed by Breiman (2001).\nMethod 2: Each tree outputs a vector [p1,…,pk], where k is the number of classes, representing the predicted probability of each class given x. This may be estimated as the relative class frequencies of training points in the leaf node x falls within. The forest output is the average of these vectors across trees, representing a conditional distribution over classes given x.\nExample: Data point falls into a tree’s leaf where yellow is the predicted class (2nd child split) \n\nFor this tree in the ensemble, there are 3 yellow and 1 green in the terminal leaf (2nd child split). Therefore the probability of Yellow is 75%.\nFor each class, the trees that predict that class have their probabilites averaged to produced the predicted probability for that class.\n\n\n\n\n1.1.1 Decision Trees\n\nMisc\n\nPCA improves DT predictive performance in two important ways (py example):\n\nOrients key features together (that explain the most variance)\n\nDTs create orthogonal decision boundaries and PCs are orthogonal to each other.\n\nReduces the feature space\n\n\nClassification\n\nShowing entropy but misclassification error or the gini index can be used.\nCalculate Shannon Entropy of dependent variable (Y):\n where P(Y) is the marginal probability\n\nFor a binary variable, this would be\n\nIn general the Shannon Entropy equation is\n where p is a probability and c is the number of classes for the variable and S = subset of data or the node.\nProbabilities are between (0,1) and taking a log of numbers in this interval produces a negative value. Hence, the negative at the beginning of the expression.\nIf the natural log, ln, is used then it’s called deviance\n\nCalculate the entropy of the target, Y, with respect to each independent variable, x. For variable,\n\n\n, with number of classes, c :\n\nI do NOT like the way the equation is written above. In videos, this type of entropy isn’t given a name, but I think it matches conditional entropy in its description and calculation.\n\nConditional Entropy (for a particular x),\n\n\nThis definition uses\n\n\nWhere H is used to as the symbol for entropy.\n\n\n\n\n\nThe marginal probability for that class of that variable, i.e. ratio of instances of that class in the entire dataset.\n\nExample: \n\nNot explicitly shown above, but for the entropy calculations, it uses the sum of the rows as the denominator in probability calculations. This fits with a “conditional” type of entropy.\n\n\nCalculate information gain for variable  \n\nRepeat for all independent variables\n\nSelect the independent variable with the largest gain for first split (\n\nFirst split, i.e. root node, is the most influential variable\n\nIf categorical variable chosen, leaves are all levels of that variable\n\nSubset dataset by var == level (for each branch\nRepeat entropy and information gain calculations on the subsetted data set\n\nBranches with entropy &gt; 1 are split unless some other stopping criteria is reached\n\nChoose variable with largest information gain and split by that variable\nKeeping repeating until maxdepth reached or minimum node size (number of rows in subset) reached\n\nNumerical vars are binned and treated like categorical vars\nPredicted class is the mode of the classes in the appropriate terminal node\n\nRegression\n\nFor each predictor var, choose a separator value, s\n\ne.g var1 &gt; 5 and var1 &lt;= 5 where s = 5\n\nCalculate the mean y value for both regions then calculate the MSE ((obs - mean)^2) of both regions. Sum of both MSEs. The optimal separator produces the lowest sum MSE.\nWhichever predictor has lowest sum MSE is chosen as the split variable.\nRecursively repeat. For example, repeat on region where var1 &gt;5 and repeat on region where var1 &lt;= 5.\nContinue until max.depth, max splits reached or data points in created region is less than a minimum or MSEs being calculated are all greater than a chosen amount, or… etc. (Hyperparameters)\nPrediction is the mean in the appropriate terminal node\n\n\n\n\n1.1.2 Random Forest\n\n\nSeveral independent decision trees are fitted. Each tree just gets a part of the variable and then splits the outcome space according to the features in X\nWhen we ‘’drop down’’ a new point x, it will end up in a leaf for each tree. A leaf is a set with observations i and taking the average over all yi in that leaf gives the prediction for one tree. These predictions are then averaged to give the final result. Thus, for a given x if you want to predict the conditional mean of Y given that x, you:\n\n“Drop down” the x each tree (this is indicated in red in the above figure). Since the splitting rules were made on X, your new point x will safely land somewhere in a leaf node.\nFor each tree you average the responses yi in that leaf to get an estimate of the conditional mean of each tree.\nYou average each conditional mean over the trees to get the final prediction.\n\nAveraging the prediction of all trees leads to a marked reduction in variance.\nMissing Predictor Data\n\nSee StatQuest: Random Forests Part 2: Missing data and clustering video for more details\nProcess: Classification model\n\nMissingness is in the training data\n\nChoose intial values for the missing data\n\nLooks at that predictor’s values that have the same outcome value as the observation with the missing data\n\nCategorical: For example, if the row has an observed outcome of 1 (i.e. event), then it will look at that predictor’s values with outcome = 1 and choose the most popular category for the missing value\nNumeric: same as categorical, except the median value of predictor is chosen for the missing value\n\n\nCreate a “Proximity Matrix” to determine which observation is most similar to observation with the missing data\n\nThe matrix values are counts of how many times each row ends up in the node as the missing data row across all the trees in the forest\nThe counts are then divided by the number of trees in the forest\n\nCategorical: Weights for each category are calculated (see video). These weights are multiplied times the observed frequency of the category in the training data. The category with the highest weighted frequency becomes the new value for the missing data.\nNumerical: The weights are used to calculate a weighted average. So, weight * median is the new value for the missing data\nProcess is repeated until the values don’t change within a tolerance\n\nMissingness in the out-of-sample data\n\nA copy of the observation with the missingness is made for each outcome category.\nThe proximity matrix procedure is done for each copy\nThen a prediction for each copy with it’s new value is made in each tree of the forest. (of course the label for each copy has now been stripped)\nWhichever copy had it’s (stripped) outcome label predicted correctly by the most trees wins and that label is prediction for that observation\n\n\n\n\n\n\n1.1.3 Isolation Forests\n\nUsed for anomaly detection. Algorithm related to binary search.\nNotes from paper: https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf\nAlso see Anomaly Detection &gt;&gt; Isolation Forests\nThe tree algorithm chooses a predictor at random for the root node. Then randomly chooses either the minimum or the maximum of that variable as the splitting value. The algorithm recursively subsamples like normal trees (choosing variables and split points in the same manner) until each terminal node has one data point or replicates of the same data point or preset maximum tree height is reached. Across the trees of a forest, anomalies with have a shorter average path length from root to terminal node.\n\nThe algorithm is basically looking for observations with combinations of variables that have extreme values. The process of continually splitting subsamples of data will run out data points and be reduced to a single observation more quickly for an anomalous observation than a common observation.\nMakes sense. Picturing a tree structure, there shouldn’t be too many observations with more that a few minimums/maximums of variable values. The algorithm weeds out these observations as it moves down the tree structure.\n\nAny or all of these wouldn’t necessarily be global minimum/maximums since we’re dealing with subsamples of variable values as we move down the tree.\n\nPaper has some nice text boxes with pseudocode that goes through the steps of the algorithm.\n\nAnomaly scores range from 0 to 1. Observations with a shorter average path length will have a larger score.\n\nAnomaly score,\n\n\nWhere E(h(xi)) is the average path length across the isolation forest for that observation\n\n\n\nWhere H(i) is the Harmonic number,\n\n\nGuidelines\n\nThe closer an observation’s score is to 1 the more likely that it is an anomaly\nThe closer to zero, the more likely the observation isn’t an anomaly.\nObservations with scores around 0.5 means that the algorithm can’t find a distinction.\n\n\n\n\n\n1.1.4 Distributional Trees/Forests\n\n\nBlends the distributional modeling of gamlss (additive, nonlinear,  location, scale, shape) and the abrupt-change detection, additive + multiplicative effects capability, inclusion of interaction effects of decision trees / random forests. For regression trees, estimating all the distributional parameters instead of just the mean makes calculating the uncertainty easier.\n\nCART trees don’t have a concept of statistical signiﬁcance, and so cannot distinguish between a signiﬁcant and an insigniﬁcant improvement in the information measure.\nCART tree predictions are piecewise-constant (every observation in the node has the same prediction), it will not be accurate unless the tree is large. But a large tree is harder to interpret than a small one.\nLinear trends are difficult for trees with piecewise-constant predictions\n\nAlgorithm splits based on changes in the mean and higher moments. So able to capture things like changes in variance.\nConditional distributions allow for the computation of prediction intervals.\nThis framework embeds recursive partitioning into statistical model estimation and variable selection\nThe statistical formulation of the algorithm ensures the validity of interpretations drawn from the resulting model\n{partykit}\n\nNotes from: https://arxiv.org/pdf/1804.02921.pdf\ntl;dr procedure\n\nFor each distributional parameter (e.g. mean, sd), a “score” matrix is computed using the target values and the distribution’s likelihood function\nThe score matrix is used to create a test statistic for each predictor\nThe predictor with the lowest p-value associated with its test statistic is the splitting variable\nThe split point is determined by the point that produces the lowest p-value in one of the split regions\nProcess continues for each leaf until no variables produce a p-value below a certain threshold (e.g. α = 0.05)\nThe distributional parameters associated with the leaf that a new observation falls into is used as the prediction for a tree.\n\nTree procedure\n\nFor each distributional parameter (e.g. mean, std.dev), calculate the value of the maximum likelihood estimator (MLE)\nTake the derivative of the log-likelihood function. Plug in the value(s) of the MLE parameter(s) and a yi value to get a “score” for every value of the response variable. Repeat for each distributional parameter. (The score should fluctuate around zero.)\n\n\nand\n\n\nWhere k is the number of distributional parameters and n is the number of training observations\n\nGaussian example with\n :\n\nSolve for the MLE of the mean and calculate the value:\n\nSolve for the MLE of the variance and calculate the value:\n\nWe’re calculating a score for each value of the outcome variable so we can remove the summation symbol from the derivative of the log-likelihood function w.r.t. the mean. This leaves us with the mean score function:\n\nSame thing but with the derivative of the log-likelihood function w.r.t. the variance:\n\n\n\nNull Hypothesis test each predictor variable vs the parameter score matrix where H0 = independence — Two methods: CTree and MOB\n\nCTree is permutation test based\n\nEach test statistic vector, T, for 1,…,l predictors and n observations is calculated by:\n\n\nwhere\n\n\nis the 1xk row of the score matrix and v is a transformation function that depends on whether the predictor variable, Z, is a numeric or character type.\n\n\n\nIf the predictor variable, Z, is a numeric:\n\nv is an identity function, so Z remains unchanged.\nCorresponding to the first observation, the first row of the score matrix is multiplied by the first value of the predictor variable resulting in a 1xk row vector.\nn 1xk row vectors are added together\nThe summed 1xk vector is transposed by the vec function into the k-vector, T.\n\nIf the predictor variable, Z, is a character variable with H categories:\n\nv creates an indicator variable where the hth value is 1 indicates that Zi’s value is the hth category.\nCorresponding to the first observation, we multiply this Hx1 vector times the 1xk, first row of the score matrix which results in a sparse Hxk matrix.\nn Hxk matrices are added together\nvec then stacks each column of the summed Hxk matrix to create a column vector, T, with H*k rows.\n\n\nT is standardized by maximum or quadratic method.\n\nt just represents a statistic that’s calculated from a permutation of the scores. T is handled in the same way.\npartykit::ctree.pdf shows the calculations for μ and Σ\n\n\nwhere Σ+ is the pseudo-inverse of the covariance matrix\n\nCalculating the pseudo-inverse makes this method more computationally intensive\n\nUsing this quadratic method, c is Chi-Square test statistic.\n\n\n\nFor the maximum method, c is Normal test statistic (partykit::ctree.pdf)\nno idea why the numerator has one “k” and the bottom has “kk.” Maybe it’s a typo.\n\n\nFind the p-value associated with each predictor’s c statistic\n\n\n\nThis says the p-value, P, is the probability,\n\n\n, (associated with the null hypothesis for this particular variable) that the standardized T stat is as or more extreme than the group of standardized t stats of the permuted scores.\n\n\n\nis the symmetric group of permutations and weights. Weights being either 0 or 1 depending on whether the observation is present in that node’s data subset.\n\n\n\n\nMOB stands for model based method\n\nUses a M-Fluctuation test to test for an “instability” by calculating a supLM test statistic. An instability is what’s interpreted from a p-value &lt; 0.05\n\n\n\n\nis a minimum amount of scores that you choose, then\n\nNo guidelines for\n\n\nIn addition to choosing a minimum, the paper does mention also trimming node data points at each end by 10%.\n\n\n\nis called “its variance function.”\n\n\n\n\n\n\nmeans floor of nt, which means round down to the integer.\n\nThe subscript\n\n\nsays the scores are ordered from highest to lowest (aka anti-rank) according to the predictor variable, Zj, values\n\n\ncovariance matrix\n\n\nThe distribution from which the p-value is calculated has something to do with a Bessel process and stuff converging to a Brownian Bridge, so I decided to shut it down here.\n\nSee https://eeecon.uibk.ac.at/~zeileis/papers/Zeileis%2BHothorn%2BHornik-2008.pdf for details\n\n\n\nBonferonni adjust the p-value according the number of variables, m\n\n\n\nSelect predictor variables with adjusted p-values lower than the threshold\nFrom that selection, the predictor with the lowest p-value is chosen as the splitting variable.\n\nIf no p-values are lower than the threshold, splitting is halted and the terminal node is reached for that branch (aka pre-pruning).\nPre-pruning not usually done in distributional forests (mincriterion = 0).\n\nChoose the optimal split point for the chosen variable where the lowest p-value is produced in one of the two created sub-regions (i.e the maximum test statistic).\nProcedure is repeated (like traditional trees) in the created leaves and continues until stopping criteria reached (e.g. no p-values lower than threshold, number of observations in node is below minimum, etc)\nIn practice, predicting, using just a tree, involves finding the node with the criterion that fits the new observation and using the estimated distributional parameters of the subsample belonging to that node as the model prediction.\n\nThe paper says this can be thought of as a weighted maximum likelihood estimation. The mathematical notation is similar to what I show below for forests.\n\n\nForest procedure\n\nThe idea of random forests is to train an ensemble of trees, each on diﬀerent training data obtained through resampling or subsampling. In each node only a random subset of the covariates is considered for splitting to reduce the correlation among the trees and to stabilize the variance of the model\nNotes from: https://arxiv.org/pdf/1701.02110.pdf\nPretty good explainer of the weight system used below to calculate predicted means across all leaves in the forest (from {drf} explainer)\n\nInstead of directly calculating the mean in a leaf node, one calculates the weights that are implicitly used when doing the mean calculation. The weight wi(x) is a function of (1) the test point x and (2) an observation i. That is, if we drop x down a tree, we observe in which leaf it ends up. All observations that are in that leaf get a 1, all others 0.\n\nSo if we end up in a leaf with observations (1,3,10), then the weight of observations 1,3,10 for that tree is 1, while all other observations get 0.\n\nWe then further divide that weight by the number of elements in the leaf node.\n\nIn the example before, we had 3 observations in the leaf node, so the weights for observations 1,3, 10 are 1/3 each, while all other observations still get a weight of 0 in this tree. Averaging these weights over all trees gives the final weight wi(x).\n\nCalculating the mean as we do in a traditional Random Forest, is the same as summing up wi(x)*yi\n\nFor predictions:\n\nFor a training observation and a tree, determine whether a new observation z belongs in the same node as the training observation, zi.\nCalculate a weight according to whether the new observation and the training observation are in the same node.\n\nIf they are in the same node\n\n where i denotes the training observation and n is the number of observations in that node of that particular tree, t.\n\nIf zi, the training observation, and the new observation, z, aren’t in the same node\n\nwit = 0\n\n\nCalculate wit for each tree the training observation belongs to.\n\nResampling or subsampling may exclude a training observation from some of the trees\n\nSum all tree weights, wit, for that training observation and divide by the number of trees to get the forest-weight for that training observation.\n\n\n\nWhere Ti is the total trees that use that training observation in its learning sample.\nSo the forest weight is the average weight per tree for that training observation\n\nIn the paper, this process is described by a more compact notation:\n\n\nThe numerator in the end part of this equation indicates whether the bth terminal node, Bb, contains the training observation, zi, and the new observation, z, for tree, t.\n is the number of observations in that terminal node\n\n\nThe paper describes predictions being calculated by a weighted MLE as it did for a single tree, but for forests it didn’t explicitly give an “in practice” description of process.\n\n\n\nEach parameter that has been calculated for each terminal node of each tree has a subset of the learning data associated with it. The forest weight-likelihood products of each observation are summed over this subset. The parameter with the largest sum is chosen as the prediction.\n\nRepeat for each distributional parameter.\n\n\n\n\n\n\n{drf}\n\n\nNotes from DRF: A Random Forest for (almost) everything\nUltimately when a RF finishes splitting data, each leaf should ideallly contain a homogeneous set of points in terms of approximating a the conditional distribution, P(Y|X=xi), but this only applies to the conditional mean of that leaf. As seen in the pic, the mean doesn’t fully describe that leaf’s conditional distribution.\n\nEvery distribution except x2 has a similar means, but sets (x1,x4,x6) and (x3, x5, x7) have different variances.\n\ndrf is able to fit a RF with these more homogeneous leaves by transforming the leaf’s yi subsamples into a Reproducing Kernel Hilbert Space with a kernel.\n\nIn this infinite-dimensional space, conditional means are able to fully represent conditional distributions and the Maximum Mean Discrepancy (MMD) is (efficiently) calculated.\nThe MMD measures the similarity between distributions\nThus if the conditional distribution of Y given xi and xj are similar, they will be grouped in the same leaf.\n\ndrf uses the same weighting system for its forest as {partykit} in order to produce predictions.",
    "crumbs": [
      "Algorithms",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ML</span>"
    ]
  },
  {
    "objectID": "qmd/algorithms-ml.html#sec-alg-ml-boost",
    "href": "qmd/algorithms-ml.html#sec-alg-ml-boost",
    "title": "1  ML",
    "section": "1.2 Boosting",
    "text": "1.2 Boosting\n\nfrom https://www.economist.com/graphic-detail/2021/03/11/how-we-built-our-covid-19-risk-estimator\n\nIn order to capture such complexity, we needed to allow for the possibility that comorbidities do not have constant effects that can simply be added together, but instead interact with each other, producing overall risk levels that are either higher or lower than the sum of their parts.\n\nSays main effects weren’t good enough and needed to use interactions\n\nGradient-boosted trees make predictions by constructing a series of “decision trees”, one after the other. The first tree might begin by checking if a patient has hypertension, and then if they are older or younger than 65. It might find that people over 65 with hypertension often have a fatal outcome. If so, then whenever those conditions are met, it will move predictions in this direction. The next tree then seeks to improve on the prediction produced by the previous one. Relationships between variables are discovered or refined with each tree.\n\nDifference between XGBoost and LightGBM (Raschka)\n\nXGBoost’s trees are based on breadth-first search, comparing different features at each node.\nLightGBM performs depth-first search, focusing on a single feature & growing the tree from there.\n\n\n\n1.2.1 Gradient boosted machines (GBM)\n\nChoose a differentiable loss function, ρ, such as \n\nIn gradient boosting, 1/n is exchanged for 1/2, to make it differentiable. The mean of the loss function, SSE in this case, calculated over all observations for a model is called the “empirical risk” which is what boosting is trying to minimize.\n\nCalculate the negative gradient, aka first derivative. For regression trees, this turns out to be\n\n\nwhich is just the residuals. Pg 360 of The Elements of Statistical Learning has other loss functions and their negative gradients. Classification uses a multinomial deviance loss function.\n\nInitialize using the optimal constant model, which is just a single terminal node tree. Think this means the initial predictions are just mean of the target,\n\nCalculate the negative gradient vector (residuals), r, by plugging in the predicted values.\nFit a regression tree with the residuals, r, as the target variable. The mean of the residuals for that region (terminal node) is the prediction,\n .\nAdd the predicted residuals vector to the initial predictions vector,\n\n\nto get the next set of predictions to feed into the negative gradient equation.\n\nRepeat steps 4 - 6 until some stopping criteria is met.\n\n\n\n1.2.2 LightGBM\n\nFind optimal split points using a histogram based algorithm\n\nGOSS (Gradient Based One Side Sampling)\n\nretains instances with large gradients while performing random sampling on instances with small gradients.\n\nExample: Gaussian Regression - observations with small residuals are downsampled by random selection while those observations with large residuals remain\n\n\n\nEFB (Exclusive Feature Bundling)\n\nReduce feature space by bundling features together that are “mutually exclusive” (i.e. varA doesn’t take a value of 0 in the same observation as varB).\n\ni.e. Bundles sparse features together\n\nCreate bundles and assign features\n\nConstruct a graph with weighted (measure of conflict between features) edges. Conflict is measure of the fraction of exclusive features which have overlapping non zero values.\nSort the features by count of non zero instances in descending order.\nLoop over the ordered list of features and assign the feature to an existing bundle (if conflict &lt; threshold) or create a new bundle (if conflict &gt; threshold).\n\nMerging\n\nArticle wasn’t coherent on this precedure\n\n\n\n\n\n1.2.3 XGBoost\n\nFYI has various gradient function families: binomial, poisson, tweedie, softmax (multi-category classification)\nUtilizes histogram-based algorithm for finding optimal split points.\n\nBuckets continuous features into discrete bins to construct feature histograms during training. It costs O(#data * #feature) for histogram building and O(#bin * #feature) for split point finding.\n\nRegularization: It penalizes more complex models through both LASSO (L1) and Ridge (L2) regularization to prevent overfitting.\n\nRegularization function\n\n\nThis function gets minimized during training\nT is the total number of trees\nw is a leaf weight\nTuning parameters\n\nα controls how much we want to penalize the sum of the absolute value of leaf weights (L1 regularization)\nλ controls how much we want to penalize the sum of squared leaf weights  (L2 regularization)\nγ is used to control how the number of trees in the model is penalized\n\n\n\nSparsity Awareness: XGBoost naturally admits sparse features for inputs by automatically ‘learning’ best missing value depending on training loss and handles different types of sparsity patterns in the data more efficiently.\nWeighted Quantile Sketch: XGBoost employs the distributed weighted Quantile Sketch algorithm to effectively find the optimal split points among weighted datasets.\nMultinomial: all the trees are constructed at the same time, using a vector objective function instead of a scalar one, i.e. there is an objective for each class. (i.e. Classifying n classes generate trees n times more complex)\n\nThe objective name is multi:softprob when using the integrated objective in XGBoost. Although, the aim is not really the softprob , but the log loss of the softmax. But softmax is not the gradient of softmax , but the gradient of its log loss\n\\[\n\\begin {align}\n\\mbox{soft}_\\mbox{max}(x_i) &= \\frac{e^{x_i}}{\\sum_j e^{x_j}}\\\\\n\\mbox{log}_\\mbox{loss}(x_i) &= -\\ln(\\mbox{soft}_\\mbox{max}(x_i)) = -ln(e^{x_i}) + \\ln\\left(\\sum_j e^{x_j}\\right) = -x_i + \\ln\\left(\\sum_j e^{x_j}\\right) \\\\\n\\frac{\\partial\\mbox{log}_\\mbox{loss}(x_i)}{\\partial x_i} &= \\frac{\\partial (-x_i + \\ln\\left(\\sum_j e^{x_j}\\right)}{\\partial x_i} = -1 + \\frac{e^{x_i}}{\\sum_j e^{x_j}} = \\mbox{soft}_\\mbox{max}(x_i)\n\\end {align}\n\\]\n\ni.e. the objective optimized is not softmax or softprob, but their log loss.\n\n\nCross-validation: The algorithm comes with built-in cross-validation method at each iteration, taking away the need to explicitly program this search and to specify the exact number of boosting iterations required in a single run.\nComponent-wise Boosting (From mboost_PKG tutorial docs)\n\ntl;dr - Same as GBM except instead of fitting a tree model to the residuals, you’re fitting many types of small models (few predictors). Best small model updates the predictions after each iteration.\nThe goal is to minimize empirical risk\n\n\nwhere\n\n\nis the loss function and\n\n\nthe predictor function\n\nThe loss function is usually the negative log-likelihood function for the distribution of the outcome variable. For a Gaussian distribution, this is equivalent to the least squares objective function\n\nSteps\n\nCompute the negative gradient of the loss function which is the negative first derivative with respect to the predictor function,\n .\n\n\ncan be thought of as the vector of predictions at the mth iteration of the algorithm. So to begin, we create an initial vector,\n\n\nwith “offset values.”\n\nFor glmboost, the offset is the mean of the outcome variable. I’d guess it’s probably the same for gamboost.\n\nCompute the negative gradient vector:\n\n\nFor the first iteration, m = 1 and\n\n\nFit each baselearner to the negative gradient vector.\n\nA baselearner is like a subset statistical model of the overall statistical model. They can be linear regressors, penalized linear regressors, shallow trees, penalized splines, etc. Each one using one or more predictors with or without interaction terms.\n\nFor each baselearner, calculate the residual sum of squares,\n\n\nfrom it’s predictions.\n\nWhichever baselearner has the smallest RSS, scale it’s predictions with a learning rate factor and combine them with the previous iteration’s predictions\n\n\n\nwhere\n\n\nis the learning rate.\nOptimization of ν isn’t critical. Only required to be low enough as to not overshoot the minimum empirical risk, e.g. ν = 0.1.\n\n\nAfter the predictions are updated, steps 3-6 are repeated until the number of iterations, set by the value of the mstop, is reached.\n\nmstop is a hyperparameter that you optimize to prevent overfitting. The value can be chosen by cv or AIC\n\n\nEach baselearner’s contribution to the final prediction vector is\n\n\nover all iterations where that baselearner was selected\nI think this is the value of the\n\n\non the y-axis of the partial dependence plots (pdp).\nIf the variables have been centered (maybe need to be completely standardized), then the magnitude of the y-axis (and the variable range within it) can be used as a signifier of variable importance and variables can be compared that way.\nIf a variable has multiple baselearners selected, you can combine all the contributions and plot the combined effect pdp by predict(which = “variable”), row summing the values, and plotting. See the end of mboost.pdf for details.\n\n\nComponent-wise boosting performs variable selection unlike some other boosting algorithms",
    "crumbs": [
      "Algorithms",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ML</span>"
    ]
  },
  {
    "objectID": "qmd/algorithms-recommendation.html",
    "href": "qmd/algorithms-recommendation.html",
    "title": "Recommendation",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Algorithms",
      "Recommendation"
    ]
  },
  {
    "objectID": "qmd/algorithms-recommendation.html#sec-alg-recom-misc",
    "href": "qmd/algorithms-recommendation.html#sec-alg-recom-misc",
    "title": "Recommendation",
    "section": "",
    "text": "Resources\n\nSee this series of posts, https://towardsdatascience.com/recsys-series-part-8-the-14-properties-to-take-into-account-when-evaluating-real-world-recsys-f71cc6e1f195\n\nPackages\n\n{{cornac}} - Focuses on making it convenient to work with models leveraging auxiliary data (e.g., item descriptive text and image, social network, etc). Cornac enables fast experiments and straightforward implementations of new models. It is highly compatible with existing machine learning libraries (e.g., TensorFlow, PyTorch).\n{{lightfm}} - Python implementation of a number of popular recommendation algorithms for both implicit and explicit feedback.\n{{merlin}} - Open source library providing end-to-end GPU-accelerated recommender systems, from feature engineering and preprocessing to training deep learning models and running inference in production.\n\nModel Evaluation\n\nA temporal split is used, so you have the same users in train as in test. (See article in LightFM)\nSort by rating in descending order for each user. Otherwise, the metrics will be bad.\nFrom the model predictions, items already “seen” by the user should be removed from the predictions\nIf you have low metrics, for example PrecisionK below 0.1, and you don’t know what the reason is — the data or the model, or maybe the metric calculation\n\nCheck the sparsity index of your user/item matrix (see Metrics &gt;&gt; Misc)\nUse the MovieLens (.zip) dataset and train your model on it.\n\nIf its metrics are low on MovieLens too, then the cause is in the model\nif the metrics are good, then the likely cause lies in the preprocessing and postprocessing stages.\n\n\nIf Random model and Most popular model (baseline models) metrics are close to ML models, it is worth checking the data — maybe the number of unique items is too low.\n\nThis can also happen if we have very little data, or maybe there is a bug in the training code.\n\nIf values higher than 0.5 for Precision@k look too high and it is worth checking if there is a bug in the script or if we are lowering the sparsity index too much.\nAlways compare how many users and items you have left after lowering the sparsity index. Sometimes in the pursuit of quality you can lose almost all users, so you should look for a compromise.\nIf you already have a model in production, select the most promising candidates for online AB testing on KPIs. For testing recommender models, also see Experiments, A/B Testing &gt;&gt; Interleaving Experiments\n\nBaseline models\n\nMost Popular\ndf[item_col].value_counts()[:top_n]\n\nThis model may produce suprisingly good results but it will have low coverage.\n\nIf your company decides to expand the number of items it offers in the future, coverage for this model will be even lower.\n\n\nRandom Model\n\nOutputs random item ids\n\n\n“Rich get Richer” problem\n\nThe most popular items get more and more recommendations which creates a feedback loop. This prevents new items or new vendors from getting exposure.\nSolution: Initially make the probability of a recommended item uniform (with some regard to category, etc.) and collect ratings. After enough ratings have been collected include rating as a feature in the ML model\n\nKroger’s “Forget Something”\n\nFeature on checkout page that shows items the customer has previously frequently bought\n\nShould items be ordered by recency or most frequent?\n\nFilters\n\nSeasonal - frequently bought items they’ve bought during major holiday time intervals\n\nThis probably should be put into a separate tab or part of a filter list\nKroger has this but it’s more of a recommendation list that has seasonal foods. Maybe this should be part of a separate recommendation feature\n\nItems not on their current grocery list\n\nNot automatic on Kroger’s but it should be part of preprocessing before the list is presented to the customer\n\n\n\nEthics\n\nYglesias post on Facebook’s (and all social media) only recommending to drive engagement. Makes me think there could be a score based on hate-speech or radicalism using sentiment analysis that could be included into the ranking algorithm that would make more ethical recommendations\n\nComplimentary item recommendation\n\nRecommender that’s similar to market basket analysis in that it recommends items that are often bought together with item being purchased.\nResources\n\nHow to provide complementary item recommendations in e-commerce marketplaces\n\n\nFiltering Features Based on Contextual Dimensions\n\ne.g. When the movie is watched or if the user is watching the movie with family or friends (which room it’s watched in might hint at this?)\nNotes from Engineering Features for Contextual Recommendation Engines\nTraditional embedding techniques grows exponentially with the number of contextual dimensions (i.e. see multiverse as an example), alternative approaches like factorization machines can be employed to preserve tractability.\nUsing multi-layer perceptrons vs. matrix factorization, there is more flexibility in how embeddings are derived and used to find the most similar items to users. For example, embeddings can be learned whereby different dimensions have different weights in determining the users location in space.\nAn outcome could be explicit feedback like a rating, implicit feedback like watch time, or a non-feedback quantity like price.\nScenario: Outcome = F(I,C)\n\nItems (I) and context (C) play a dominant role in the recommender, whereas the user takes a backseat.\n\ne.g. New users with limited user-item history\n\nCategoricals are 1-hot encoded and aggregation calculations of numerics per category. Then, the category and the new numeric are combined.\n\nExample\n\n\ne.g.\n\navg_rating_work_photography = photography (1-hot) x AVG Rating (for photography work) = 2.92 for this prospective employee\n\n\nTherefore, each unit/subject has only 1 row.\n\nTo avoid unnecessary complexity and large dimension spaces, features are filtered based on the user’s context\n\nExample: Filling a medium scope, medium budget photography job using a jobs website (see above table)\n\nIf prospective employee has other skills besides those pertaining to photography, then those are filtered out\nSo, in the above table, variables with “work_photography”, “adobe_photoshop”, “scope_medium”, “budget_medium”, and all the experience variables would be included in the dimension space and the rest would be discarded\n\nInteractions like AVG Ratings when both the Budget == medium and the experience level == expert are not considered.\nNon-linearities can still be derived by the training algorithm, or additional contextual dimensions can be created as the the product of existing ones.\n\nA tree model (e.g. XGBoost) is trained using this dataset with the outcome being some kind of feedback column (e.g. from employers that hired the person for a job).\n\nThis model’s predictions are used to produce recommendations with this particular context but I’m not sure how that’s supposed to work.\n\n\nScenario: Outcome = F(U, C)\n\nThe recommendation isn’t for a discrete item, but rather a continuous value.\n\ne.g. a platform recommending the price a property rental company should charge for a property on a weekend.\n\n\nQuestions\n\n“We can rebuild this representation now considering each past job as the current job, stacking each past job as a set of additional rows. For past jobs, we can create an additional column to capture explicit feedback received by the freelancer such as being hired, or implicit feedback like being messaged.”\nNo idea what “talent” is",
    "crumbs": [
      "Algorithms",
      "Recommendation"
    ]
  },
  {
    "objectID": "qmd/algorithms-recommendation.html#sec-alg-recom-metrics",
    "href": "qmd/algorithms-recommendation.html#sec-alg-recom-metrics",
    "title": "Recommendation",
    "section": "Metrics",
    "text": "Metrics\n\nMisc\n\nAlso see\n\nAlgorithms, Learn-to-Rank &gt;&gt; Diagnostics\nSimilarity Scoring bkmk folder\n\nJaccard Index, Map@k\n\n\nCompare your recommender click-through rates of your recommender to those of the baseline.\nIf your model is producing extremely low metrics, then you could have a sparsity issue\n\nSparsity Index\n\\[\n\\mbox{index} = 1 - \\frac{\\mbox{interactions} \\times \\mbox{items}}{\\mbox{users}}\n\\]\n\nGuessing “interactions” are the number of 1s in your user/item matrix\n\n“Users” and “Items” are the sizes of those two dims in your user-item matrix, so users*items would be the number of entries in your matrix\nTherefore interactions / (users*items) would be the proportion of 1s in your matrix and subtracting 1 makes it the proportion of 0s\n\nHigher values = sparser user/item matrix\nTypically, a sparsity-index of about 98% is sufficient for training\n\nReducing overly-sparse user/item matrices can have major effects on metric scores\n\nCheck user/item distribution\n\n\n\nCoverage\n\nThe metric allows you to see percentage of items used by the recommendation system.\nUsually very important for businesses to make sure that the content (e.g. movies) they have on their site is used to its full potential.\nCoverage = number of unique items in recommendations / all unique items\n\nDiversity\n\nHigh diversity values mean that users have an opportunity to discover new genres, diversify their experience, and spend more time on the site. As a rule, it increases the retention rate and has a positive impact on revenue.\nCan be measured in many different ways\n\nAverage similarity for top_n.\nMedian value of the number of unique genres (e.g. movies, books) or another category in hierarchy of product categories.\n\n\nMean Reciprocal Rank\n\\[\n\\mbox{score}_i = \\frac{1}{\\mbox{rank}_i}\n\\]\n\n(Wiki)\nScores based the rank the recommender model gave the “correct” recommendation when it supplied it’s list of recommendations. (i.e. The second recommendation is the recomendation the model felt was 2nd most likely to be correct)\nThe average score is calculated across all recommendations to represent the score of the model.\nExample: 3 queries\n\n\nNormalized Discounted Cumulative Gain (NDCG) (Mhaskar, 2015)\n\\[\n\\begin{align}\n&\\mbox{NDCG}_{\\mbox{pos}} = \\frac{\\mbox{DCG}_{\\mbox{pos}}}{\\mbox{iDCG}} \\\\\n&\\mbox{where}\\;\\; \\mbox{DCG} = \\sum_{\\mbox{pos}=1}^n \\frac{\\mbox{relevance}_{\\mbox{pos}}}{\\ln(\\mbox{pos}+1)}\n\\end{align}\n\\]\n\nRelevant documents appearing lower in the recommendation list are penalized as the graded relevance value is reduced logarithmically proportional to the position of the result\nNotes\n\nVery relevant results are more useful than somewhat relevant results which are more useful than irrelevant results (cumulative gain)\nRelevant results are more useful when they appear earlier in the set of results (discounting).\nThe result of the ranking should be irrelevant to the query performed (normalization).\n\n\nMAP@k - Mean average precision within the top k highest-ranked impressions\n\nExample: If a user watched movie recommendations ranked 1,2,3 and 5 but not 4th ranked recommendation, then the average score for that user would be (1/1 + 2/2 + 3/3 + 3/4 + 4/5)/5 = 0.91\n\nPrecision/Recall@k (article)\n\nPrecision@k\n\nThe proportion of recommended items in the top-k set that are relevant\nPrecision@k = (# out of k recommended items that match the observed relevant scores) / k\nk is a user definable integer that is set by the user to match the top-n recommendations objective (i.e. k = n)\nExample: 80% precision means on average that 80% of the top-n recommendations predicted by the model are relevant to the user.\nIssue: If there are no items recommended. i.e. number of recommended items at k is zero, we cannot compute precision at k since we cannot divide by zero.\n\nIn that case we set precision@k to 1. This makes sense because in that case we do not have any recommended item that is not relevant.\n\n\nRecall@k\n\nThe proportion of relevant items found in the top-k recommendations\nRecall@k = (# of k recommended items that match the observed relevant scores) / (total # of true relevant items)\nk is a user definable integer that is set by the user to match the top-n recommendations objective (i.e. k = n)\nExample: 40% recall means on average 40% of the total number of the (observed) relevant items appear in the top-n results.\nIssue: If there are no items recommended. i.e. number of recommended items at k is zero, we cannot compute precision at k since we cannot divide by zero.\n\nIn that case we set recall@k to be 1. This makes sense because we do not have any relevant items that are not identified in our top-k results.\n\n\nRelevance - A relevant item for a specific user-item pair means that this item is a good recommendation for the user in question.\n\nExample: User/Movie\n\n\nAssume that any true (i.e. observed) rating above 3.0 corresponds to a relevant item and any true rating below 3.0 is irrelevant.\n\nThe threshold is subjective. There are multiple ways to set this threshold value such as taking into consideration the history of ratings given by the user.\n\nUnder the above definition, the movies in the first 2 rows are “relevant” to those users, and the last three movies would be “irrelevant” to those users\n\n\nRecommended - Items are generated by recommendation algorithm with a predicted rating greater than the relevance threshold\nIn calculations, ignore all the ratings where the actual value is not known\n\nRecList (article, article) - Behavior tests; black-box-ish py library that takes a sort of sentiment analysis approach to grading recommenders\n\n{checklist} for recommender systems (see Diagnostics, NLP &gt;&gt; Behavioral Tests &gt;&gt; Misc &gt;&gt; Packages)\nPrinciples\n\nComplementary and similar items satisfy different logical relations\n\nWhile similar items are interchangeable, complementary ones may have a natural ordering\ne.g. Recommend hdmi cable if a tv is bought, but not a tv if a hdmi cable is bought.\n\nNot all mistakes are equally bad\n\nExample: Truth: When Harry Met Sally\n\nRecommending Terminator is a worse miss than You’ve Got Mail\n\n\nSome groups are more important than others\n\nTolerate a small decrease in overall accuracy if a subset of users we care about is happier\nExample: Promoting Nike products so if the recommender results in substantial increased Nike Sales, for Italian users on iphones it becomes terrible, then that’s an acceptable trade-off\n\n\n\nRGRecSys (Salesforce)\n\nEvaluation toolkit that encompasses multiple dimensions - robustness with respect to sub-populations, transformations, distributional disparity, attack, and data sparsity\nGithub\nPaper",
    "crumbs": [
      "Algorithms",
      "Recommendation"
    ]
  },
  {
    "objectID": "qmd/algorithms-recommendation.html#sec-alg-recom-colfil",
    "href": "qmd/algorithms-recommendation.html#sec-alg-recom-colfil",
    "title": "Recommendation",
    "section": "Collaborative Filtering",
    "text": "Collaborative Filtering\n\nFocuses on the relationship of users and items in question (ideal when the data contains ratings for the various items offered).\nExample person A and B listen to Song X\n\nIf person B listens often to song Y, then A is very likely to like song Y as well.\n\nDifferences in embeddings can be used to find analagous product relationships\n\nI think for this to work, you’d have some sort of product description (e.g. ingredients, materials, comments, consumers describing taste, feel, look, etc.) embedding and not just product name embeddings.\nExample: Find the closest diet soft drink to a brand that doesn’ t offer a diet version of their soft drink\n\nCalculate the difference between the diet coke embedding and coke embedding\nFind a diet soft drink embedding that has a similar delta with the soft drink embedding that doesn’t offer a diet version.\n\n\nAlso read about applying regularization after SVD to control for overfitting(?)\nInstead of K-NN, a correlation matrix can be used to find similar items after an item has been inputted.\nClustering embeddings\n\nkNN is has O(N*K) complexity, where N is the number of items and K is the size of each embedding.\n\nApproximate nearest neighbor (ANN) algorithms typically drop the complexity of a lookup to O(log(n))\n\nSee Clustering, General\n\n\n\n\n\nNon-Negative Matrix Factorization (NMF)\n\nMisc\n\nPackages:\n\n{NMF}\n\nMethods\n\nAlternating Least Squares (ALS)\n\nSpark has recommendation model that uses this algorithm\n\nHierarchical Alternating Least Squares (HALS)\n\n\nDecomposes the sparse user-item matrix (N_users ⨯ N_items) to 2 lower dimensional matrices (W & H)\n\nThe original matrix must have all non-negative entries (I think) The product of W and H is an approximation of the original user-item matrix. Being an approximation is how it’s different from SVD This is because the 2 resulting matrices are constrained to be non-negative (every elt is non-negative) instead of orthogonal\n\n\n\n\n\n__\nSKU1\nSKU2\nSKU3\n\n\nCUST1\n0\n1\n1\n\n\nCUST2\n0\n0\n1\n\n\nCUST3\n1\n0\n0\n\n\n\n\nI don’t think these are indicator columns. I think they’re quantities, so each cell can have any positive integer value (or zero).\nCustomer Segment Matrix, W: N_users ⨯ N_latent_factors\n\nEach column is a “segment” or “basis”\nEach cell is the customer’s score for that segment\n\nProduct Segment Matrix, H: N_latent_factors ⨯ N_items\n\nEach row is a “segment” or “basis”\nEach cell is the product’s score for that segment\n\nThe dimension of “N_latent_factors” is a tuning parameter\n\nIf a customer and product have high scores for the same segments, then our factorization is implying that this cell in the customer-by-product matrix has a high value.\n\nA customer’s predicted rating for an item is calculated by multiplying the customer vector by the item vector\nIn general, by multiplying both W and H matrices, we end up with an estimate of the original matrix, but more densely filled with all of our predicted ratings\n\nThe resulting latent dimensions will capture the most relevant information or characteristics about each customer and item and therefore improve the performance of the downstream clustering task\nResults can be visualized with a heatmap\n\nExample had 2 heatmaps (customer segment matrix, product segment matrix)\nA dark cell for customer i and segment j says that the customer prefers products with high scores in segment j (dark cells in the product heatmap)\n\nI think the scores are normalized, so closer to 1 means a stronger association\n\n\nIssues\n\nCold Start Problem - When you have new users or items that you want to make predictions for since the model had no possibility to learn anything for them, leaving you with basically random recommendations for them\n\nSee LightFM for potential solution\n\nUser-Item matrices can be too sparse\n\nSee Metrics &gt;&gt; Misc &gt;&gt; Sparsity Index\nSolutions:\n\nRemove some users or items that have low interaction counts (i.e. 1s in user/item matrix)\nDevelop a hierarchy of information about the product—from brand, to style, to size (or SKU), and choose the appropriate level of the hierarchy to use\n\n\nFactorization parameters\n\nDetermine the starting state of the matrix in the estimation process (i.e. which product information to use? see 1st issue)\nloss function\n\nFrobenius norm and the Kullback-Leibler divergence commonly used (at least in NLP) to minimize the distance between the user-item matrix and its approximation (W x H)\n\n\\(k\\) - Number of segments\n\nToo many segments, and the information is hard to digest; too few, and you are not explaining the data well\n\n\\(\\alpha\\) - A regularization parameter for learning W and H\n\nRegularization is an optional technique that can be used to have more control over how outliers can influence the algorithm\n\n\\(\\mbox{tolerance}\\) - Stopping condition for the NMF algorithm. i.e. how close is the approximation X ≈ W x H.\n\nLower tolerance means more training iterations\n\n\n\nUse Cases\n\nDeveloping product-based customer segments to build need-based personas\nDeciding which products should be offered together as a bundle\nBuilding a product recommendation engine that uses a customer’s segment to determine which products should be merchandised\n\nSimilar items beget similar ratings from similar users\n\nTopic Analysis\n\nAlso see NLP, Topic &gt;&gt; NMF\nPick a couple columns of a df (e.g. movie titles, plot descriptions)\ntf-idf the plot descriptions and remove stopwords, names, etc.\nApply NMF\n\nFor each segment (rows) in H, find the k most important words (columns) (i.e. words with top k scores)\n\nEach segment is a genre. Use the words to label the genres for the movies.\n\nAdjusting the number of segments might produce more coherent results if necessary.\n\n\n\nExample (word2vec, GMM, HDBSCAN)\n\nNotes from https://towardsdatascience.com/using-gaussian-mixture-models-to-transform-user-item-embedding-and-generate-better-user-clusters-a46e1062d621\nMusic playlist data: Users hash ids (users), Artist names (items)\nSteps\n\nItems are characters so apply word2vec to get embeddings\n\nEach unique item will have a numeric vector (item_vec) associated with it.\n\nFor each user’s playlist, compute average vector of all the item vectors associated with it.\n\n(item1_vec + item2_vec + …)/n\nEach user is represented by a (averaged) vector\n\nWe have item_df which has each item and its word2vec numeric representation, and we have a user_df which has each user and their playlists represented by a mean embedding numeric vector\nFit a gaussian mixture model, gmm_mod, with the item_df\nFor each user, get vector of probabilities of that user belonging to each cluster by user_probs &lt;– predict(gmm_mod, data = user_df)\n\nBy increasing the number user_df features by creating gmm cluster probabilities, it helps other clustering algorithms separate users more easily.\n\nCluster user_probs using HDBSCAN\n\nWhere user_probs should look like col1 = users, col2 = gmmclust1_probs, col3 = gmmcluster2_probs, etc.\n\n\nOptional processing that can improve results\n\nLogging and standardizing before clustering\nRemoving outliers (e.g. very popular artists) at the start\n\n\n\n\n\nLightFM\n\nCreates embeddings for additional features of the user and the product. Then, adds all the user embeddings together and adds all the product embeddings together and proceeds as normal\nIn the cold start case, the new user gets a default embedding for “user_id” and “product_id”, but by using the additional feature embeddings, you get an informative rating.\nMisc\n\nNotes from A Performant Recommender System Without Cold Start Problem\nPaper (2015)(code)\n\nResults (only tested on 2 datasets w/binary labels)\n\n“In both cold-start and low-density scenarios, LightFM performs at least as well as pure content-based models, substantially outperforming them when either (1) collaborative information is available in the training set or (2) user features are included in the model.”\n“When collaborative data is abundant (warm-start, dense user-item matrix), LightFM performs at least as well as the MF model.”\n“Embeddings produced by LightFM encode important semantic information about features and can be used for related recommendation tasks such as tag recommendations.”\n\n\n\nMethod Comparison\n\nTypical collaborative filtering (e.g. user ids, movie ids)\n\nLightFM",
    "crumbs": [
      "Algorithms",
      "Recommendation"
    ]
  },
  {
    "objectID": "qmd/algorithms-recommendation.html#sec-alg-recom-contb",
    "href": "qmd/algorithms-recommendation.html#sec-alg-recom-contb",
    "title": "Recommendation",
    "section": "Content-based",
    "text": "Content-based\n\nMeasure similarities in product characteristics and matching based on the strength of the measures.\nVarious similarity scoring algorithms\n\nDensity Adjusting\nVector Embedding\nCo-Occurrence\n\nCollaborative Topic Modeling/Regression (CTM/CTR) for text-based items with enhanced accuracy and out-of-matrix prediction capabilities. * out-of-matrix capability: able to generalize the recommendations for new, completely unrated items: since no ratings are observed for item j, a model without this capability cannot derive its latent vector of qualities.\n\nBuilt on top of Latent Dirchlet Allocation (LDA) and Probabilistic Matrix Factorization (PMF)",
    "crumbs": [
      "Algorithms",
      "Recommendation"
    ]
  },
  {
    "objectID": "qmd/algorithms-recommendation.html#sec-alg-recom-comprec",
    "href": "qmd/algorithms-recommendation.html#sec-alg-recom-comprec",
    "title": "Recommendation",
    "section": "Company Recommenders",
    "text": "Company Recommenders\n\nSpotify - Truncated SVD of user-item matrix then K-NN to find similar groupings\nYoutube\n\n\nPaper\nSimilar to GMM example (above) except:\n\na DL model takes the embeddings and outputs class probabilities instead of a GMM\napprox-kNN is used instead of HDBSCAN to cluster users\n\nCombines two different deep neural networks, a first to select good candidate videos that you’d like if you watched, and a second to pick the best candidates that you are most likely to watch the longest. (Think the candidate NN is the one shown in the figure)\n\nTikTok\n\n\nNotes from:\n\nThe Batch\n\nSummarizes a nytimes article of a leaked internal document\n\nTikTok’s Secret Sauce\n\nFlow chart starts with the primary goal on the left (increase Daily Active Use). From left to right, it breaks down variables into other variables that are probably predictive of the variable preceding it. Therefore maximizing the right-most variables will increase the primary KPI (daily active use)\nFor each user, videos are ranked according to an expected value equation (i.e sum of probabilities and values): Plike x Vlike + Pcomment x Vcomment + Eplaytime x Vplaytime + Pplay x Vplay\n\nPs are likely predicted values that the user will do the action (e.g. like, comment, play) for a particular video.\nVs are values that the tiktok has determined for that particular action\n\nMaybe determined by a regression standardized coefficient (daily_active_use ~ comment + …) or variable importance value\n\nE may be “estimated.” Might be a descriptive statistic for a user. e.g. avg playtime given var1, var2, etc.\n\nmaybe the vars are the “Other features” or Creative quality variables, etc.\n\n\nThe Batch article discusses ways to penalize videos\nUI design: Swiping instead of scrolling\nTreats each video more or less independently to assess its viral potential, caring relatively little about how many followers the creator has\nMore exploration, less exploitation\n\nThere is a tradeoff between safe but somewhat boring recommendations that are similar to recommendations that worked well in the past (“exploitation”), and risky recommendations that are unlikely to be good but have a high payoff if they do turn out to be good (“exploration”).\nPlaces a relatively high emphasis on exploration compared to other platforms\nTikTok is able to take risks because all it takes is a swipe (see UI design and article for details)\n\n\nNetflix\n\nUses a Embarrassingly Shallow Autoencoders (EASE) for embeddings\nUses Propensity Correction (Paper) to prevent feedback loops which can bias the recommender\n\nInstagram\n\nHow Instagram suggests new content",
    "crumbs": [
      "Algorithms",
      "Recommendation"
    ]
  },
  {
    "objectID": "qmd/algorithms-recommendation.html#sec-alg-recom-otnot",
    "href": "qmd/algorithms-recommendation.html#sec-alg-recom-otnot",
    "title": "Recommendation",
    "section": "Other Notes",
    "text": "Other Notes\n\nRecSys 2020 — Takeaways and Notable Papers\n\nInverse propensity scoring was a popular approach taken to debias recommendations\nIncreased shift towards sequence models (with SASRec (2018) and BERT4Rec (2019) being common benchmarks) and bandit and reinforcement learning for recommender systems\nThree sources of recommendation complexity, namely:\n\nPlacement: Where is the recommendation located on the user interface?\nPerson: Who is seeing the recommendation? What are her past experiences with the recommendation placement?\nContext: What is going on at that moment? What are the user’s needs?\n\nThere are many contexts where similarity is not required or can worsen recommendations. For example, users might want a change of pace or mood from that horror movie they just watched. Also, does the user stick to a specific genre (e.g., Korean dramas) or hop around diverse genres? A better understanding will help improve the user experience.\n\n\nDifferent data splitting strategies (for train and validation) can affect the relative performance of recommendation systems in offline evaluation\n\nSplitting strategies\n\nLeave-one-last: Leave one last item, leave one last basket/session\nTemporal: Temporal split within each user, temporal split (on same date) globally\nRandom: For each user, split interactions into train and test data\nUser: Split some users into train, the rest into test\n\nRelative performance of recommenders changed often across splitting strategies (indicated by the rank swaps)\n\n\nModel Evaluation\n\nSR-GNN: Best hit rate, mean reciprocal rank, and nDCG\nV-STAN: Best precision, recall, and mean average precision\nV-SKNN, GRU4Rec: Best coverage and popularity\nSTAMP: Satisfactory in all metrics\nUsing human experts to evaluate recommendations\n\nIn contrast to the offline evaluation metrics, human experts found GRU4Rec to have very relevant recommendations. However, because its recommendations did not match the IDs of products added to cart, GRU4Rec did not perform as well on offline evaluation metrics.\nSTAMP and GRU4Rec performed best in the second step and STAMP was put through an A/B test. This led to a 15.6% increase in CTR and an 18.5% increase in revenue per session.\n\n\nSimilarity\n\nMatrix factorization faster w/sufficient regularization outperformed and was faster than DL approaches\n\nAdding unexpectedness to recommendations to add freshness\n\nTwo kinds\n\nPersonalized: Some users are variety seekers and thus more open to new videos\nSession-based: If a user finishes the first episode of a series, it’s better to recommend the next episode. If the user binged on multiple episodes, it’s better to recommend something different.\n\n\nBad idea to use default parameters for Word2vec-based recommendations",
    "crumbs": [
      "Algorithms",
      "Recommendation"
    ]
  },
  {
    "objectID": "qmd/anomaly-detection.html",
    "href": "qmd/anomaly-detection.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "qmd/anomaly-detection.html#sec-anomdet-misc",
    "href": "qmd/anomaly-detection.html#sec-anomdet-misc",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Also see\n\nOutliers\nbkmks\n\nTime Series &gt;&gt; Cleaning/Processing &gt;&gt; Outliers\nBusiness Applications &gt;&gt; Fraud/Anomaly Detection\n\n\nPackages\n\n{{pyod}}: A Comprehensive and Scalable Python Library for Outlier Detection (Anomaly Detection)\n\nResources\n\nDeep Learning for Anomaly Detection: A Review (2021 Pang et al)\nA Unifying Review of Deep and Shallow Anomaly Detection (2021 Ruff et al)\n\nProposes a formal mathematical definition of the notion of outlier\nReviews “a broad area of the field - including deep neural networks - with a probabilistic perspective on the methods”\n\nThere and Back Again: Outlier Detection Between Statistical Reasoning and Data Mining Algorithms (2019 Zimek and Filzmoser)\n\nHigh-Level perspective on outlier detection\nStatistical view on the methods and a probabilistic interpretation of the anomaly scores\n\n\nA dataset that contains anomalies and has them labeled by a human expert is the ideal case since the problem then turns into a classification task.\n\nThe human can augment a sample with an annotation of which exact features are responsible for the anomaly (e.g., a segmentation mask in the context of image data).\n\nFrom Sci Kit Learn (link)\n\nCommon Questions\n\n“I need to know why your model detected an anomaly. I need sound root cause analysis before I adjust my manufacturing process.” (see variable importance tracking)\n“Anomaly detection is not enough: when a model detects an anomaly, it’s already too late. I need prediction to justify investing time and effort into such an approach.” (see event rates)\n“I need prescription: tell me what I should do to prevent a failure from happening.” (see variable importance tracking)\n\nCommon Criticisms\n\n“There are some false positives, I don’t have time to investigate each event!” (see event rates)\n“Your model only detects anomalies when they already happened, it’s useless!“ (see event rates)\n“I have hundreds of sensors: when an anomaly is detected by your model, I still have to investigate my whole operations, I’m not saving any time here!” (see variable importance tracking)\n\nAmazon Lookout for Equipment - managed service from AWS dedicated to anomaly detection\nML Algorithms (Descriptions from BRDAD paper)\n\nBagged Regularized k-distances for Anomaly Detection (BRDAD) is a distance algorithm that converts the unsupervised anomaly detection problem into a convex optimization problem. Is able to address the sensitivity challenge of the hyperparameter choice in distance-based algorithms. It has two hyperparameters, including the bagging rounds B and the subsampling size s. For the sake of convenience, s = [n/B] is fixed so the bagging rounds B is the only one hyper-parameter and is set to be B = 5 as default.\nElliptic Envelope is suitable for normally-distributed data with low dimensionality. As its name implies, it uses the multivariate normal distribution to create a distance measure to separate outliers from inliers. {{sklearn.covariance.EllipticEnvelope}}\nDistance-To-Measure (DTM) is a distance-based algorithm which employs a generalization of the k nearest neighbors named “distance-to-measure”. As suggested by the authors, the number of neighbors k is fixed to be k = 0.03 × sample size.\nk-Nearest Neighbors (k-NN) is a distance-based algorithm that uses the distance of a point from its k-th nearest neighbor to distinguish anomalies.\nLocal Outlier Factor (LOF) is a distance-based algorithm that measures the local deviation of the density of a given data point with respect to its neighbors. {{sklearn.neighbors.LocalOutlierFactor}}\nPartial Identification Forest (PIDForest) is a forest-based algorithm that computes the anomaly score of a point by determining the minimum density of data points across all subcubes partitioned by decision trees. Authors’ implementation uses number of trees T = 50, the number of buckets B = 5, and the depth of trees p = 10 .\nIsolation Forest (iForest) is a forest-based algorithm that works by randomly partitioning features of the data into smaller subsets and distinguishing between normal and anomalous points based on the number of “splits” required to isolate them, with anomalies requiring fewer splits. {{sklearn.ensemble.IsolationForest}}\nOne-class SVM (OCSVM) is a kernel-based algorithm which tries to separate data from the origin in the transformed high-dimensional predictor space. An O(n) approximate solution to the One-Class SVM. Note that the O(n²) One-Class SVM works well on our small example dataset. {{sklearn.linear_model.SGDOneClassSVM}}\n\nAny abnormal event visible in your time series will either be a:\n\nPrecursor Event\nDetectable Anomaly (forewarning about a future event)\nA Failure\nMaintenance Activity\nHealing Period (while your industrial process recovers after an issue)\n\nTypes\n\nShocks - abrupt changes, spikes\nLevel Shifts - can happen when a given time series shifts between range of values based on underlying conditions or operating modes.\n\nLevel – The average value for a specific time period\nIf you want to consider all operating modes when detecting anomalies, you need to take care to include all of them in your training data\n\nTrending: a set of signals can change over time (not necessarily in the same direction).\n\nWhen you want to assess the condition of a process or of a piece of equipment, these trending anomalies will be great precursors events to search. They will help you build forewarning signals before actual failures may happen.\n\n\nUse average event rates to filter out false positives and predict an upcoming event\n\n\nTake action if the event rate (i.e. rate of predicted events) starts to grow too large (allowing you to move from detecting to predicting)\n\ne.g. You can decide to only notify an operator after the daily event rate reaches at least 200 per day. This would allow you to only react to 3 events out of the 41 detected (aka predicted) during this period\n\nUse historical event data to calculate an event rate threshold\nBy only reacting after a threshold predicted event rate has been reached, you filter out false positives (when scarce events are detected)\n\nTrack variable importance over time to narrow the field of potential causes of an event\n\n\n2 stacked column charts represent two anomalous events.\nEach color is a predictor variable (e.g. sensor) that was important to the prediction of the event.\n\nOnly need to focus on a few (e.g. top five predictor variables)\n\nUse to examine false positives and actual events\n\n\nFor the false positive (left)\n\nThe percentage of importance attributed the top 5 is much less than that for an actual event\nRed is less important and Yellow is more important than when there’s an actual event\n\n\n\nRolling Spectral Entropy\n\n\n\nSpectral Entropy is the normalized (power) spectral density )(PSD)\nMisc\n\nNotes from Anomaly Detection in Univariate Stochastic Time Series with Spectral Entropy\n\nGuidelines\n\nA series which has strong trend and seasonality (and so is easy to forecast) will have entropy close to 0.\n\nIn the case of noisy time series, this indicates an anomaly.\n\nA series that is very noisy (and so is difficult to forecast) will have entropy close to 1.\n\nExample\n\ndef spectral_entropy(x, freq, nfft=None):   \n    _, psd = periodogram(x, freq, nfft = nfft)   \n    # calculate shannon entropy of normalized psd\n    psd_norm = psd / np.sum(psd)\n    entropy = np.nansum(psd_norm * np.log2(psd_norm))\n    return -(entropy / np.log2(psd_norm.size))\n\nwindow = 200\nnfft = None\ndf = pd.DataFrame(data=x, columns=['x'])\ndf['x_roll_se'] = df['x'].rolling(window).apply(lambda x: spectral_entropy(x,freq=100,nfft=nfft))\n\nIf the FFT size is not specified, we will use the window size",
    "crumbs": [
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "qmd/anomaly-detection.html#sec-anomdet-charts",
    "href": "qmd/anomaly-detection.html#sec-anomdet-charts",
    "title": "Anomaly Detection",
    "section": "Charts",
    "text": "Charts\n\nThe goal to breakdown an anomaly (e.g. manufacturing process outage) into constituent parts (sensor readings).\n\n\nBy analyzing the sensor readings, you can potentially find the area causing the anomaly\nThere also might be leading indicators that are predictive of an anomaly.\n\nOften you’re looking at many time series (e.g. a manufacturing process) when performing EDA for anomaly detection, and conventional time series charts are insufficient\nHorizon Chart\n\n\nHue (e.g. blue or red) represents values above or below a certain value\nDarkness/lightness of the color represents the extremeness of the value\n\ni.e. the darker the color the larger the magnitude of the y-axis value\n\nVertical layers in the normal chart become horizontal layers of the horizon chart\n\nlayer feature may provide more detail than the strip chart\n\n\nStrip Chart\n\nSimilar to horizon in that y-axis values get binned and represented by colors\n\nExample\n\n\nColors\n\nBlue - low, Gold - medium, Red - high\n\nColumns of red indicate shocks (e.g. around 2022-11-15)\n\nExample\n\n\nColors\n\nBlue - low, Gold - medium, Red - high\n\nMajor color changes in color indicate trend/level shifts\n\ne.g. the change from a lot of red to a lot of blue after December 2017",
    "crumbs": [
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "qmd/anomaly-detection.html#sec-anomdet-isofor",
    "href": "qmd/anomaly-detection.html#sec-anomdet-isofor",
    "title": "Anomaly Detection",
    "section": "Isolation Forests",
    "text": "Isolation Forests\n\nAlso see Algorithms, ML &gt;&gt; Trees &gt;&gt; Isolation Forests\nA tree-based approach where outliers are more quickly isolated by random splits than inliers\nNotes from paper: https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf\nThe tree algorithm chooses a predictor at random for the root node. Then randomly chooses either the minimum or the maximum of that variable as the splitting value. The algorithm recursively subsamples like normal trees (choosing variables and split points in the same manner) until each terminal node has one data point or replicates of the same data point or preset maximum tree height is reached. Across the trees of a forest, anomalies with have a shorter average path length from root to terminal node.\n\nThe algorithm is basically looking for observations with combinations of variables that have extreme values. The process of continually splitting subsamples of data will run out data points and be reduced to a single observation more quickly for an anomalous observation than a common observation.\nMakes sense. Picturing a tree structure, there shouldn’t be too many observations with more that a few minimums/maximums of variable values. The algorithm weeds out these observations as it moves down the tree structure.\n\nAny or all of these wouldn’t necessarily be global minimum/maximums since we’re dealing with subsamples of variable values as we move down the tree.\n\nPaper has some nice text boxes with pseudocode that goes through the steps of the algorithm.\n\nAnomaly scores range from 0 to 1. Observations with a shorter average path length will have a larger score.\n\nAnomaly score\n\\[\n\\begin{aligned}\n&s(x_i, n) = 2^{-\\frac{\\mathbb{E}(h(x_i))}{c(n)}}\\\\\n&\\begin{aligned}\n\\text{where}\\quad c(n) &= 2H(n-1) - \\frac{2(n-1)}{n} \\\\\nH(i) &= \\ln(i) + \\gamma\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\\(\\mathbb{E}(h(x_i))\\) is the average path length across the isolation forest for that observation\n\\(H(i)\\) is the harmonic number and \\(\\gamma\\) is Euler’s constant\n\nGuidelines\n\nThe closer an observation’s score is to 1 the more likely that it is an anomaly\nThe closer to zero, the more likely the observation isn’t an anomaly.\nObservations with scores around 0.5 means that the algorithm can’t find a distinction.",
    "crumbs": [
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "qmd/anomaly-detection.html#sec-anomdet-autoenc",
    "href": "qmd/anomaly-detection.html#sec-anomdet-autoenc",
    "title": "Anomaly Detection",
    "section": "Autoencoders",
    "text": "Autoencoders\n\nAn outlier is something that the autoencoder has not seen often during training, so it might have trouble finding a good encoding for them.\n\nAn autoencoder tries to learn good encodings for a given dataset. Since most data points in the dataset are not outliers, the autoencoder will be influenced most by the normal data points and should perform well on them.\n\nMisc\n\nAlso see Feature Reduction &gt;&gt; Autoencoders",
    "crumbs": [
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "qmd/apache-spark.html",
    "href": "qmd/apache-spark.html",
    "title": "Spark",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Apache",
      "Spark"
    ]
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-misc",
    "href": "qmd/apache-spark.html#sec-apache-spark-misc",
    "title": "Spark",
    "section": "",
    "text": "Packages\n\n{sparklyr}\n{sparklyr.nested} - Extension for nested data\n{sparkxgb} - XGBoost in Spark\n{sparklyr.flint} - Time Series Computation\n{multiplyr}\n\nAlternative option for data with &gt; 10M rows and you only have access to one machine\nSpreads data over local cores\n\n\nResources\n\n2022 sparklyr cheatsheet\nMastering Apache Spark with R\n\nPotential issues with transferring R code to Spark\n\nCan’t subset reference dataframe columns\n# doesn't work\nresults &lt;- your_function(products_ref$order_id)\n\n# Have to collect the column variable and bring the data into the R environ\norder_id &lt;- product_ref %&gt;%\n                select(order_id) %&gt;%\n                collect()\nresults &lt;- your_function(order_id)\n\nAlso becomes an issue within nested functions. Might need to add collect( ) to the end of the inner function code.\nCluster Manager - Responsible to allocate the resources across the Spark Application. This architecture has several advantages. Each application run is isolated from other application run, because each gets its own executor process. Driver schedules its own tasks and executes it in different application run on different JVM. Downside is, that data can not be shared across different Spark applications, without being written (RDD) to a storage system, that is outside of this particular application\nDrill - A lightweight version of Spark (no ML stuff, just query). It was orphaned after Dec 19 after some company bought some other company. Can be used through {sergeant} PKG.\nHive - Apache Hive is a data warehouse open-source project which allows querying of large amounts of data. Like SQL it uses an easy-to-understand language called Hive QL\nHive partitions - Used to split the larger table into several smaller parts based on one or multiple columns (partition key, for example, date, state e.t.c). The hive partition is similar to table partitioning available in SQL server or any other RDBMS database tables.\n\nEmbeds field names and values in path segments, such as “/year=2019/month=2/ or part_.parquet”. HDFS - Hadoop Distributed File System is a data storage system used by Hadoop. It provides flexibility to manage structured or unstructured data. Storing large amounts of financial transactional data in an HDFS to query using Hive QL.\n\nKafka - More complex to work with than NiFi as it doesn’t have a user interface (UI), mainly used for real-time streaming data. It is a messaging system first created by LinkedIn engineers. Streaming real-time weather events using Kafka\nSpark integration services for the Cloud: AWS EMR, Azure HDInsight, GCP Dataproc.\nIf Spark is used with Databricks, another particularly interesting format is the delta format which offers automatic optimisation tools.",
    "crumbs": [
      "Apache",
      "Spark"
    ]
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-procov",
    "href": "qmd/apache-spark.html#sec-apache-spark-procov",
    "title": "Spark",
    "section": "Process Overview",
    "text": "Process Overview\n\nDistributed computation is performed by\n\nConfiguring - Requests the cluster manager for resources: total machines, memory, and so on.\nPartitioning - Splits the data available for computation across each compute instance.\nExecuting - Means running an arbitrary transformation over each partition.\nShuffling - Redistributes data to the correct machine.\nCaching - Preserves data in memory across different computation cycles.\nSerializing - Transforms data to be sent over the network to other workers or back to the driver node.\n\n\nSpark Driver (aka master node) - Orchestrates the execution of the processing and its distribution among the Spark Executors.\n\nNot necessarily hosted by the computing cluster, it can be an external client.\n\nA Spark Executor is a process that runs on a Worker Node and is responsible for executing the tasks of a specific Spark application.\n\nExecutors are allocated a certain amount of memory and CPU cores on the Worker Node. They can run multiple tasks concurrently, using threads to parallelize execution.\nExecutors typically persist for the duration of the Spark application, reusing cached data and avoiding overheads of starting new processes for each job.\n\nCluster Manager - Manages the available resources of the cluster in real time.\n\nWith a better overview than the Spark application, it allocates the requested resources to the Spark driver if they are available\ne.g. Standalone cluster manager (Spark’s own manager that is deployed on private cluster), Apache Mesos, Hadoop YARN, or Kubernetes\n\nWorker Node - The physical compute or virtual machine that is responsible for executing tasks and storing data.\n\nManaged by the Cluster Manager, such as Apache Mesos or YARN, which allocates resources to them.\nProvides CPU, memory, and storage for Spark Executors to run and can host multiple executors from different Spark applications, depending on its available resources.\n\n\nExample (process)\ndata &lt;- copy_to(sc, \n  data.frame(id = c(4, 9, 1, 8, 2, 3, 5, 7, 6)),\n             repartition = 3)\n\ndata %&gt;% arrange(id) %&gt;% collect()\n\nThe numbers 1 through 9 are partitioned across three storage instances.\nEach worker node loads this implicit partition; for instance, 4, 9, and 1 are loaded in the first worker node.\nA task is distributed to each worker to apply a transformation to each data partition in each worker node.\nThe task executes a sorting operation within a partition.\n\nTasks are denoted as f(x) in the Spark webui DAG\n\nThe result is then shuffled to the correct machine to finish the sorting operation across the entire dataset, which completes a stage.\n\nThe sorted results can be optionally cached in memory to avoid rerunning this computation multiple times.\n\nFinally, a small subset of the results is serialized, through the network connecting the cluster machines, back to the driver node to print a preview of this sorting example.\n\n\nSpark Job\n\n\nStages are a set of operations that Spark can execute without shuffling data between machines\n\nOften delimited by a data transfer in the network between the executing nodes\ne.g. A join operation between two tables.\n\nTask: A unit of execution in Spark that is assigned to a partition of data.\n\nSpark Transformation types:\n\n\nShuffle: A redistribution of the data partitions in the network between the executing nodes\n\nIsn’t usually 1:1 copying as hash keys are typically used to determine how data is grouped by and where to copy. This process usually means data is copied through numerous executors and machines.\nComputationally expensive\n\nSpeed Factors: Data size and latency within cluster\n\nWide transformations require shuffling\n\n\nLazy Evaluation - Triggers processing only when a Spark action is run and not a Spark transformation\n\nAllows Spark to prepare a logical and physical execution plan to perform the action efficiently.",
    "crumbs": [
      "Apache",
      "Spark"
    ]
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-setup",
    "href": "qmd/apache-spark.html#sec-apache-spark-setup",
    "title": "Spark",
    "section": "Set-Up",
    "text": "Set-Up\n\nJava\n\nInstall Java or see which version you have installed\nSee which versions of Spark are compatible with your version of Java\n\nVersions that available to install - spark_available_versions()\nVersions that have been installed - spark_installed_versions()\nInstall\n\nspark_install has a version argument if you want a specific version\nspark_install(version = \"3.1\")\n\nVersion 3.1 is compatible with Java 11\nUninstall a version - spark_uninstall(\"2.2.0\")\nConnection\n\nConnect\nsc &lt;- spark_connect(\n  master = \"local\",\n  version = \"3.1.1\",\n  config = conf)\n\nsc is the connection object\nlocal means that the cluster is set-up on your local machine\nconf is the list of configuration parameter:value pairs (see below, Optimization &gt;&gt; Configuration)\n\n\nDisconnect from cluster - spark_disconnect_all()",
    "crumbs": [
      "Apache",
      "Spark"
    ]
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-errors",
    "href": "qmd/apache-spark.html#sec-apache-spark-errors",
    "title": "Spark",
    "section": "Errors",
    "text": "Errors\n\njava.lang.OutOfMemoryError: Java heap space\n\nout-of-memory (OOM) error\nSolutions:\n\nadd more executor memory\nrebalance executor memory through parameters\n\n\nService 'sparkDriver' failed after 16 retries\n\nWorker node networking error\nSee article for details on implementing these solutions\nSolutions\n\nExporting the corresponding SPARK_LOCAL_IP environment variable that is loaded when the JVM is initialized on worker nodes\nSetting the corresponding spark.driver.bindAddress configuration in the SparkSession (note that this approach will override SPARK_LOCAL_IP environment variable)\nUpdating the hostname on your local machine\nCheck whether you have enabled a Virtual Private Network (VPN) — or any other tool that may be affecting the networking on your local machine — as this may sometimes affect binding addresses\n\n\n\n\nData Skew\n\nThe OOM error can occur on joins of very large tables or very large table + medium table with skewed data because of unevenly distributed keys\n\nWhen a key (also see shuffle in Process Overview &gt;&gt; Spark Transformation types above) has considerably more volume than the others, this “HOT KEY” causes a data skew\nWhen they say “skewed”, they mean something closer to imbalanced. Partitions and join/group_by variables are usually discrete and this seems like a imbalanced categorical variable issue\n\nExamples:\n\nA sales analysis that requires a breakdown by city. The cities with more populations like New York, Chicago, San Fransico have a higher chance to get data skew problems.\nA table is partitioned by month and it has many more records in a few months than all the rest\nToo many null values in a join or group-by key\n\nOther symptoms\n\nFrozen stages and tasks\nLow utilization of CPU\ne.g. most tasks finish within a reasonable amount of time, only to have one task take forever\n\n\nIn the webui, the bottom bar is a task that takes substantially more time to complete than the other tasks\n\n\nSolution: Adaptive Query Execution (AQE)\n\nImplemented in Spark 3.0 and default in 3.2.\n\nMay not provide the most optimized settings for edge cases\n\nSee Deep Dive into Handling Apache Spark Data Skew\n\nDetails on diagnosing, and manually tuning settings (includes salting code and links)\n\n\n\nVideo\nSee Five Tips to Fasten Skewed Joins in Apache Spark for explainer on the process\nSet spark.sql.adaptive.enabled = TRUE in the config\nConfiguration parameters\n\nspark.sql.adaptive.skewJoin.enabled : This boolean parameter controls whether skewed join optimization is turned on or off. Default value is true.\nspark.sql.adaptive.skewJoin.skewedPartitionFactor: This integer parameter controls the interpretation of a skewed partition. Default value is 5.\nspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes: This parameter in MBs also controls the interpretation of a skewed partition. Default value is 256 MB.\nA partition is considered skewed when both (partition size &gt; skewedPartitionFactor * median partition size) and (partition size &gt; skewedPartitionThresholdInBytes) are true.\n\nIssues\n\nCannot handle ‘Full Outer Join’\nCannot handle skewedness on both input datasets\n\nCan handle skew only in the left dataset in the Left Joins category (Outer, Semi and Anti)\nCan handle skew in the right dataset in the Right Joins category\n\n\n\nSolution: Use broadcast joins (See Optimization &gt;&gt; Shuffling &gt;&gt; Broadcast Joins)\n\nFor inner joins, See Optimization &gt;&gt; Shuffling &gt;&gt; Iterative Broadcast Joins\n\nSolution: “Salt” the join key(s)\n\nSee video (haven’t found any good step-by-step resources)\nFor the largest table, you concantenate the join variable with “_” + random number or letter\n\nMay be better to create a new column with the salted join key so it can be deleted later\nI think the range of the random numbers might need to be the number of threads you’re using but it may not matter as long as its larger than the cardinality of your join variable\n\nFor the other table(s) involved in the join, each join variable value should have the same range of numbers as it’s counterpart in the largest table. So rows will essentially be replicated.\n\nThis increases the size of the table.\n\nIt might increase it to the size of the largest table as all other column values get replicated.\n\n\nJoin tables then perform calculations (e.g. group_by(join_id) %&gt;% count)\nDelete the join column or remove the concantenated part\n\nSolution: Salted Sort Merge Join\n\nNotes from\n\nFive Tips to Fasten Skewed Joins in Apache Spark\n\nAjay Gupta (says contact him for code snippets)\n\n\nAlso see Deep Dive into Handling Apache Spark Data Skew\n\nDetails on diagnosing, and manually tuning settings (includes salting code and links)\n\nUseful when joining a large skewed dataset with a smaller non-skewed dataset but there are constraints on the executor’s memory\nCan be used to perform Left Join of smaller non-skewed dataset with the larger skewed dataset which is not possible with Broadcast Hash Join even when the smaller dataset can be broadcasted to executors\nIssues\n\nCannot handle Full Outer Join\nCannot handle skewness on both input datasets\n\nCan handle skew only in the left dataset in the Left Joins category (Outer, Semi and Anti)\nCan handle skew only in the right dataset in the Right Joins category\n\n\nHave to turn off the ‘Broadcast Hash Join’ approach. This can be done by setting ‘spark.sql.autoBroadcastJoinThreshold’ to -1\nSimilar to ‘Iterative Broadcast Hash’ Join (Optimization &gt;&gt; Shuffling &gt;&gt; Iterative Broadcast Joins)\nProcess Option 1\n\nAn additional column ‘salt key’ is introduced in one of the skewed input datasets.\nA number is randomly assigned from a selected range of salt key values for the ‘salt key’ column to every record\nA for-loop is initiated on salt key values in the selected range. For every salt key value:\n\nFIlter the salted input dataset for the iterated salt key value\nJoin the salted filtered input dataset with the other unsalted input dataset to produce a partial joined output.\n\nTo produce the final joined output, all the partial joined outputs are combined together using the Union operator.\n\nProcess Option 2\n\n\nAn additional column ‘salt key’ is introduced in one of the skewed input datasets.\nA number is randomly assigned from a selected range of salt key values for the ‘salt key’ column to every record\nA for-loop is initiated on salt key values in the selected range. For every salt key value:\n\nThe second non skewed input dataset is enriched with the current iterated salt key value by repeating the the same value in the new ‘salt’ column to produce a partial salt enriched dataset.\n\nAll these partial enriched datasets are combined using the Union operator to produce a combined salt enriched dataset version of the second non-skewed dataset.\nThe first skewed salted dataset is Joined with the second salt enriched dataset to produce the final joined output\n\nProcess Option 3\n\nSolution: Broadcast MapPartitions Join\n\nOnly method to handle a skewed ‘Full Outer Join’ between a large skewed dataset and a smaller non-skewed dataset.\n\nsupports all type of Joins and can handle skew in either or both of the dataset\n\nIssues\n\nRequires considerable memory on executors.\n\nLarger executor memory is required to broadcast the smaller input dataset and to support intermediate in-memory collection for manual Join provision.\n\n\nThe smaller of the two input dataset is broadcasted to executors while the Join logic is manually provisioned in the ‘MapPartitions’ transformation which is invoked on the larger non-broadcasted dataset.\n\n\n\n\nDisk Spill\n\nNotes from Spark Performance Tuning: Spill\n\nArticle also has a nice breakdown of the memory types, parameters, etc for executors and where spill occurs.\n\nSpark is designed to leverage in-memory processing. If you don’t have enough memory, spark will try to write the extra data to disk to prevent your process from crashing. (i.e. data is “spilling” over because memory is “full”)\nMonitoring\n\nIf there is disk spill, the spill metrics will be included in the various monitoring tables\n\nSpill (Memory): Captures the size of data in memory before it’s spilled to disk and serialized.\n\nMore directly gauges the volume of data being processed in memory before spilling.\n\nSpill (Disk): The size of the serialized data on disk after it was spilled from memory during shuffle operations.\n\nMore directly reflects the actual disk usage during spilling.\nBefore spilling, Spark serializes the data (converts it into a compact format for efficient disk storage).\n\n\nTables (e.g. large partitions leading to disk spillage)\n\n\nSummary\n\nAggregated\n\nTask\n\nSQL Plan (On sort step)\n\n\n\nActions that can generate spill\n\nData Skew\nReducing spark.sql.shuffle.partitionsleads to bigger file sizes per partition.\n\nExample\n// Create a large partition by mismanaging shuffle partitions\nspark.conf.set(“spark.sql.shuffle.partitions”, 60)\n// The location of our non-skewed set of transactions\nval trxPath = “./transactions/2011-to-2018_int-only_60gb.delta”\nspark\n .read.format(“delta”).load(trxPath) // Load the transactions table\n .orderBy(“trx_id”) // Some wide transformation\n .write.format(“noop”).mode(“overwrite”).save() // Execute a noop \n\njoin or crossJoin between 2 data tables.\nSetting spark.sql.files.maxPartitionBytes to high (default is 128 MBs).\nexplode on multiple columns in data tables.\n\nExample\n// An array that will grow each partions by a factor of 8\nval data = Seq(0,1,3,4,5,6,7,8).toArray\nval count = spark\n .read.format(“delta”).load(trxPath) // Load the transactions table\n .withColumn(“stuff”, lit(data)) // Add an array of N items\n .select($”*”, explode($”stuff”)) // Explode dataset in size by N-1 \n .distinct() // Some wide transformation\n .write.format(“noop”).mode(“overwrite”).save() // Execute a noop\n\n\nSolutions:\n\nFix Skew\nProcess less data per task, which can be achieved reducing file size per partition via spark.shuffle.partitions or repartition.\n\nSee Optimization &gt;&gt; Partitioning\n\nIncrease the RAM to core ratio in your compute, i.e. add more memory per worker (increase worker size).\nAdjust spark.sql.files.maxPartitionBytes - Influences size of partitions when reading in files. Parquet files are already efficient in this respect but CSVs and others may benefite.\n\nSee Optimization &gt;&gt; Partitioning\n\nIncrease spark.shuffle.spill.fraction: Specifies the fraction of executor memory that can be used for shuffle data before spilling starts. Default: 0.20 (20%)\n\nExample: If executor memory is 10GB and spark.shuffle.spill.fraction is 0.2, then spilling to disk starts when shuffle data reaches 2GB (20% of 10GB).\nIncreasing:\n\nDelays spilling, potentially improving performance by keeping more data in memory.\nRisks out-of-memory errors if memory is limited.",
    "crumbs": [
      "Apache",
      "Spark"
    ]
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-opt",
    "href": "qmd/apache-spark.html#sec-apache-spark-opt",
    "title": "Spark",
    "section": "Optimization",
    "text": "Optimization\n\n\nUse event timeline in the UI to find bottlenecks\n\n\nConfiguration\n\nValues of parameters can be viewed in the UI under the Environment tab\nConfiguration object used as input to the config arg in spark_connect\nMisc\n\nRecommended to request significantly more memory for the driver than the memory available over each worker node.\nIn most cases, you will want to request one core per worker.\n\nParameters\n\ncores.local: (local mode) number of cores you want used by the cluster\nshell.driver-memory: amount RAM you want used by the driver node (or cluster if local)\nspark.memory.fraction: percentage of that RAM you want dedicated to jobs that will be run in the cluster and storing RDDs\n\nDefault: 0.60\nProbably want to hold some back for monitoring, administering the cluster, etc.\n\nspark.memory.storageFraction: percentage of RAM you want specifically for storing RDDs\n\nIf you’re mostly doing things like quickly filtering and retrieving subsets, then you can set this equal to your spark.memory.fraction\nSpark will borrow execution memory from storage and vice versa if needed and if possible; therefore, in practice, there should be little need to tune the memory settings\n\nspark.executor.memory: percentage of RAM you want specifically for executing actions\n\nExample (method 1)\n conf &lt;- list()\n conf$`sparklyr.cores.local` &lt;- 6\n conf$`sparklyr.shell.driver-memory` &lt;- \"6G\"\n conf$`spark.memory.fraction` &lt;- 0.9\nExample (method 2)\n# Initialize configuration with defaults\nconfig &lt;- spark_config()\n\n# Memory\nconfig[\"sparklyr.shell.driver-memory\"] &lt;- \"2g\"\n\n# Cores\nconfig[\"sparklyr.connect.cores.local\"] &lt;- 2\nExample (method 3)\ndefault:\n  sparklyr.shell.driver-memory: 2G\n\nAllows you to have cleaner code\nCreate a config.yml file in the working dir or a parent dir. Then there’s no need to specify the config parameter when creating a connection\n\nYou can also specify an alternate configuration filename or location by setting the file parameter in spark_config()\nCan also change the default configuration by changing the value of the R_CONFIG_ACTIVE environment variable. See the GitHub rstudio/config repo for additional information\n\n\n\n\n\nPartitioning\n\nIdeally, Spark organises one thread per task and per CPU core\nIncreasing partitions can help performance but don’t create too many partitions\n\nThere is a deterioration of I/O performance due to the operations performed by the file system (e.g. opening, closing, listing files), which is often amplified with a distributed file system like HDFS.\nScheduling problems can also be observed if the number of partitions is too large.\n\nTypes\n\nImplicit: type of partitioning already present in the storage system\n\ne.g. directory tree of Arrow files that have been partitioned; table schema in a relational db (?)\n(default) it’s more effective to run computations where the data is already located\n\nExplicit: Manaully setting the number of partitions (i.e. repartitioning)\n\nUseful when when you have many more or far fewer compute instances than data partitions. In both cases, it can help to repartition data to match your cluster resources\nVarious data functions, like spark_read_csv() or copy_to(), already support a repartition parameter to request that Spark repartition data appropriately\n\nAlso,spark_df %&gt;% sdf_repartition(4)(e.g. repartitions a Spark DataFrame to have 4 partitions)\n\nShould be shown as the spark.sql.shuffle.partitions parameter in the webui or config\n\nsdf_coalesce(x, partitions) can reduce the number of partitions without shuffling\n\n\n\nCheck number of partitions for a data object\nsdf_len(sc, 10) %&gt;% sdf_num_partitions()\n&gt;&gt; 2\n\nsdf_len creates a spark df with a sequence length (e.g. 10 in this toy example)\n\nI think this a numeric vector in R but they just call everything a df in Spark\n\nI think the default is set the partitions to the number of cores/instances that are being used in the cluster\n\nspark.sql.files.maxPartitionBytes - Influences size of partitions when reading in files. Parquet files are already efficient in this respect but CSVs and others may benefite.\n\nData Size and Distribution:\n\nLarge Files: For massive files, smaller partitions can improve parallelism and processing speed by distributing work across more cores.\nSmall Files: Too many small partitions can introduce overhead, so larger partitions might be better.\nSkewed Data: If data distribution is uneven (some partitions much larger than others), adjusting the parameter can help balance workload and prevent bottlenecks.\n\nFile Format:\n\nColumnar Formats (Parquet, ORC): Efficiently handle larger partitions due to compression and selective column reads.\nText-based Formats (CSV, JSON): Might benefit from smaller partitions for faster parsing and processing.\n\nMemory Constraints:\n\nLimited Memory: Smaller partitions can reduce memory pressure, but excessive partitioning can increase overhead.\n\nComputation Type:\n\nShuffle-intensive Operations (joins, aggregations): Benefit from larger partitions to minimize data movement and shuffling overhead.\nTransformations on individual partitions: Less affected by partition size.\n\nHardware Configuration:\n\nNumber of Cores: More cores can handle larger partitions effectively.\nDisk I/O Speed: Slower disks might favor smaller partitions to reduce disk seeks.\n\nGeneral Recommendations:\n\nDefault (128MB): Often a good starting point.\nLarger Files: Consider increasing the value (e.g., 1GB).\nSmall Files: Decrease the value (e.g., 64MB).\nSkewed Data: Experiment with different values to find the best balance.\n\nBest Practices:\n\nMonitor performance metrics to assess partitioning effectiveness.\nAdjust the parameter based on specific data characteristics and workload requirements.\nConsider other configuration options like spark.sql.files.openCostInBytes and spark.sql.files.minPartitionNum for fine-tuning.\n\n\n\n\n\nCaching\n\nFunctions like spark_read_parquet() or copy_to() load/cache data objects into memory (aka resilient distributed dataset (RDD))\n\nSave data from being lost when a worker fails\ntbl_uncache(sc, \"iris\") can be used to free up memory by removing a RDD from cache\n\nInspect RDDs through the UI (Storage Tab &gt;&gt; )\n\nCheckpoints\n\nsdf_checkpoint caches a spark dataframe or an object thats coerceable to a spark dataframe\n\nUseful during analysis after computationally intensive calculations so that you don’t have to repeat them\n\nThere is a cost to writing these results to disk, so you might need to test whether the calculation is intensive enough for checkpointing to be more efficient.\n\nspark_set_checkpoint_dir() spark_get_checkpoint_dir() are for setting and getting the location of the cache directory\nCache is deleted after the end of the spark session\n\n\n\n\n\nShuffling\n\n\nSpark stores the intermediate results of a shuffle operation on the local disks of the executor machines, so the quality of the disks, especially the I/O quality, is really important.\n\nThe use of SSD disks will significantly improve performance for this type of transformation\n\nSort Merge (Default) requires full shuffling of both data sets via network which is heavy task for Spark\n\nOther Joins: Docs\n\n\n\nBroadcast Joins\n\n** Think this only needs to be explicitly specified for Spark 2.x. Spark 3.0 has Adaptive Query Execution (AQE) which uses the optimal type of join automatically. **\n\nVideo\nMay need Databricks (runtime &gt;7.0) to get AQE and set spark.sql.adaptive.enabled = TRUE in the config\n\nUseful when one df is orders of magnitude smaller than the one you’re joining it too\n\nIt pushes one of the smaller DataFrames to each of the worker nodes to reduce shuffling the bigger DataFrame\nBy duplicating the smallest table, the join no longer requires any significant data exchange in the cluster apart from the broadcast of this table beforehand\nIf you are joining two data sets and both are very large, then broadcasting any table would kill your spark cluster and fails your job.\n\nCan be set according to difference in size in bytes using spark.sql.autoBroadcastHashJoin (not sure if this is available in {sparklyr}\n\nDefault = 10 MB difference in sizes\nSetting to -1 means spark always uses a broadcast join (maybe useful with sufficient memory available)\n\nAdjust (increase?) spark.sql.autoBroadcastJoinThreshold to get smaller tables broadcasted\n\nShould be done to ensure sufficient driver and executor memory\nSetting to -1 disables it\n\nIssues\n\nNot Applicable for Full Outer Join.\nFor Inner Join, executor memory should accommodate at least smaller of the two input dataset. (See Iterative Broadcast Joins below as a potential solution)\nFor Left , Left Anti and Left Semi Joins, executor memory should accommodate the right input dataset as the right one needs to be broadcasted.\nFor Right , Right Anti and Right Semi Joins, executor memory should accommodate the left input dataset as the left one needs to be broadcasted.\nThere is also a considerable demand of execution memory on executors based on the size of broadcasted dataset.\n\nExample:\nsdf_len(sc, 10000) %&gt;%\n  sdf_broadcast() %&gt;%\n  left_join(sdf_len(sc, 100))\n\n\n\nIterative Broadcast Joins\n\n** Limited to Inner Joins only**\nAn adaption of ‘Broadcast Hash’ join in order to handle larger skewed datasets. It is useful in situations where either of the input dataset cannot be broadcasted to executors. This may happen due to the constraints on the executor memory limits.\nBreaks downs one of the input data set (preferably the smaller one) into one or more smaller chunks thereby ensuring that each of the resulting chunk can be easily broadcasted.\n\nOutputs from these multiple joins is finally combined together using the ‘Union’ operator to produce the final output.\n\nProcess\n\n\nAssign a random number out of the desired number of chunks to each record of the Dataset in a newly added column, ‘chunkId’.\nA for-loop is initiated to iterate on chunk numbers. For each iteration:\n\nRecords are filtered on the ‘chunkId’ column corresponding to current iteration chunk number.\nThe filtered dataset, in each iteration, is then joined with the unbroken other input dataset using the standard ‘Broadcast Hash’ Join to get the partial joined output.\nThe partial joined output is then combined with the previous partial joined output.\n\nAfter the loop is exited, one would get the overall output of the join operation of the two original datasets\n\n\n\n\n\nSerialization\n\nKryo Serializer that can provide performance improvements over the default Java Serializer\nconfig &lt;- spark_config()\nconfig$spark.serializer &lt;- \"org.apache.spark.serializer.KryoSerializer\"\n\n\n\nAvoid Loops\n\nIn the planning phase, spark creates a directed acyclical graph (DAG) which indicates how your specified transformations will be carried out. The planning phase is relatively expensive and can sometimes take several seconds, so you want to invoke it as infrequently as possible.\nExample: pyspark\nimport functools\nfrom pyspark.sql import DataFrame\n\npaths = get_file_paths()\n\n# BAD: For loop\nfor path in paths:\n  df = spark.read.load(path)\n  df = fancy_transformations(df)\n  df.write.mode(\"append\").saveAsTable(\"xyz\")\n\n# GOOD: functools.reduce\nlazily_evaluated_reads = [spark.read.load(path) for path in paths]\nlazily_evaluted_transforms = [fancy_transformations(df) for df in lazily_evaluated_reads]\nunioned_df = functools.reduce(DataFrame.union, lazily_evaluted_transforms)\nunioned_df.write.mode(\"append\").saveAsTable(\"xyz\")\n\nI assume pyspark can do the same trick with wildcards that sparklyr uses, so not sure how necessary this is.\n\nExample: sparklyr\nlibrary(sparklyr)\n\n# wildcard\nfile_paths &lt;- \"/path/to/directory/*.csv\"\n# directory\nfile_paths &lt;- \"/path/to/directory\"\n# explicitly\nfile_paths &lt;- c(\"/path/to/file1.csv\", \"/path/to/file2.csv\")\n# arrow\ndata &lt;- arrow::open_dataset(sources = \"/path/to/directory\")\n\ndata &lt;- spark_read_csv(sc = sc, name = \"my_data\", path = file_paths)\n\n\n\nUnion Operator\n\ne.g. df = df1.union(df2).union(df3) (article)\nIf users don’t use union on entirely different data sources, union operators will face a potential performance bottleneck — Catalyst isn’t “smart” to identify the shared data frames to reuse.\nEnsure rows follow the same structure:\n\nThe number of columns must be identical.\nColumn data types should match\nThe column names should follow the same sequence for each data frame. Nevertheless, that’s not mandatory.\n\nThe first data frame will be chosen as the default for the column name. So mixing order can potentially cause an undesired result.\nSpark unionByName is intended to resolve this issue.\n\n\nunionAll is an alias to union that doesn’t remove duplication. We’d need to add distinct after performing union to perform SQL-like union operations without duplication.\nTypical Use Case\n\n\n“Fact 1” and “Fact 2” are 2 large tables that have been joined and split (e.g. filtering) into subsets. Each subset uses different transformations, and eventually, we combine those 4 data frames into the final one.\nSince Catalyst doesn’t recognize these are the same dfs, it performs the two big table join four times!\n\nSolutions\n\nDouble the number of executors to run more concurrent tasks\nHint to Catalyst and let it reuse the joined data frame from memory through caching\n\nExample: Caching\n## Perform inner join on df1 and df2\ndf = df1.join(df2, how=\"inner\", on=\"value\")\n\n## add cache here\ndf.cache()\n\n## Split the joined result into two data frames: one only contains the odd numbers, another one for the even numbers\ndf_odd = df.filter(df.value % 2 == 1)\ndf_even = df.filter(df.value % 2 == 0)\n\n## Add a transformation with a field called magic_value which is generated by two dummy transformations.\ndf_odd = df_odd.withColumn(\"magic_value\", df.value+1)\ndf_even = df_even.withColumn(\"magic_value\", df.value/2)\n\n## Union the odd and even number data frames\ndf_odd.union(df_even).count()\n\nReduces the plan from 50 to 32 stages",
    "crumbs": [
      "Apache",
      "Spark"
    ]
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-dating",
    "href": "qmd/apache-spark.html#sec-apache-spark-dating",
    "title": "Spark",
    "section": "Data Ingestion",
    "text": "Data Ingestion\n\nSend data from R to the Spark Cluster\niris_ref &lt;- dplyr::copy_to(sc, iris_df, \"iris_tbl\")\n\niris_ref\n\nList object in R environment that is a reference to iris_tbl in spark\nWork with this object in R like normal\n\niris_df\n\nData object in the R environment that you want to load into spark\n\niris_tbl\n\nName that you want the spark table to have in the spark cluster\n\n\nManipulate ref objs and create new object in spark and R\nfull_products &lt;- \n  dplyr::copy_to(sc, products_ref) %&gt;%\n      inner_join(departments_ref) %&gt;%\n      inner_join(aisles_ref,\n                 name = \"product_full\",\n                 overwrite = TRUE)\n# or use the spark object name to create an ref object\ntbl(sc, \"product_full\") %&gt;% select(var_name)\n\ncopy_to also can be used to create new objects in spark from reference objects in R\n\nproducts_ref, departments_ref, and aisles_ref are reference objects for tables inside spark\nproducts_full is the name we want for the new table inside spark\nfull_products will be the reference object for products_full\n\n\nRead csv file into spark cluster\nproducts_ref &lt;- spark_read_csv(sc, \"products_tbl\", \"folder/folder/products.csv\")\n\nproducts_ref is the reference object\nsc is the connection object\nproducts_tbl is the name we want for the table inside spark\nLast part is the path to the file\n\nRead multiple files\nlibrary(sparklyr)\n\n# wildcard\nfile_paths &lt;- \"/path/to/directory/*.csv\"\n# directory\nfile_paths &lt;- \"/path/to/directory\"\n# explicitly\nfile_paths &lt;- c(\"/path/to/file1.csv\", \"/path/to/file2.csv\")\n# arrow\ndata &lt;- arrow::open_dataset(sources = \"/path/to/directory\")\n\ndata &lt;- spark_read_csv(sc = sc, name = \"my_data\", path = file_paths)\nView tables in cluster - src_tbl(sc)\nBring Data from the Cluster into R\npred_iris &lt;- \n  ml_predict(model_iris, test_iris) %&gt;%\n    dplyr::collect()\n\ncollect brings the predictions data into the R environment",
    "crumbs": [
      "Apache",
      "Spark"
    ]
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-func",
    "href": "qmd/apache-spark.html#sec-apache-spark-func",
    "title": "Spark",
    "section": "Functions",
    "text": "Functions\n\nnrow \\(\\rightarrow\\) sdf_nrow(ref_obj)\n\nUsing nrow on a reference object with output a NA\n\nquantile \\(\\rightarrow\\) percentile\nExample:\ncars %&gt;%\n  summarize(mpg_percentile = percentile(mpg, array(0, 0.25, 0.5, 0.75, 1))) %&gt;%\n  mutate(mpg_percentile = explode(mpg_percentile))\n\ncars is a reference object\npercentile returns a list so explode (unnesting/flattening) coerces it into a vector\n\nI found an example of “explode” in {sparklyr} docs but no function documentation. Function also available in {sparkr} and {sparklyr.nested}\n\n\ncor \\(\\rightarrow\\) ml_corr(ref_obj)\n\nPearson correlation on a reference object",
    "crumbs": [
      "Apache",
      "Spark"
    ]
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-spsql",
    "href": "qmd/apache-spark.html#sec-apache-spark-spsql",
    "title": "Spark",
    "section": "Spark SQL",
    "text": "Spark SQL\n\nGet sql query from a dplyr operations\ncount(ref_obj) %&gt;%\n  show_query()\n\nCan use mutate, group_by, summarize, across, etc.\n\nUse SQL to query spark object\nDBI::dbGetQuery(\n  conn = sc,\n  statement = \"SELECT COUNT(*) AS 'n' FROM `full_product`\"\n)\n\nWhere full_product is the name of the table in the cluster",
    "crumbs": [
      "Apache",
      "Spark"
    ]
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-mod",
    "href": "qmd/apache-spark.html#sec-apache-spark-mod",
    "title": "Spark",
    "section": "Modeling",
    "text": "Modeling\n\nPartition, Register, Create Reference Object\n\nsdf_partition creates partition reference object for train, test sets\nsdf_register takes the partition reference object and “registers” the splits in the cluster\n\nNot sure if partition_iris is in the cluster, but I think when the “registering” occurs is when the splitting takes place inside the cluster and separate objects are created.\n\ntbl creates a separate reference object for train data in R\npartition_iris &lt;- sdf_partition(iris_ref, training = 0.8, testing = 0.2)\nsdf_register(partition_iris, c(\"spark_training_iris\", \"spark_testing_iris\"))\ntidy_iris &lt;- \n  tbl(sc, \"spark_iris_training\") %&gt;%\n      select(Species, Petal_Length, Petal_Width)\n\nTrain Basic Spark ML model\nmodel_iris &lt;- \n  tidy_iris %&gt;%\n    ml_decision_tree(response = \"Species\", \n                     features = c(\"Petal_Width\", \"Petal_Length\"))\ntest_iris &lt;- tbl(sc, \"spark_testing_iris\")\n\ntest_iris and tidy_iris are reference objects\nI don’t think model_iris is a reference object. It’s a spark class objecti but I believe it’s in your environment and not on the cluster\n\nPredict\npred_iris &lt;- \n  ml_predict(model_iris, test_iris) %&gt;%\n    dplyr::collect()\nSummary - summary(model)\nModels\n\nOLS - ml_linear_regression(cars, mpg ~ hp)\n\nWhere cars is a reference object",
    "crumbs": [
      "Apache",
      "Spark"
    ]
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-stream",
    "href": "qmd/apache-spark.html#sec-apache-spark-stream",
    "title": "Spark",
    "section": "Streaming",
    "text": "Streaming\n\nIntro to Spark Streaming with sparklyr\nSpark server will continue to run and detect changes/updates\n\nTo files in a folder\nTo Apache Kafka stream\n\nStart a stream - stream_read_csv(sc, \"&lt;input folder path&gt;/\")\n\nMake sure path to folder (e.g. “stream_input/”) ends with a backslash\n\nWrite output of processed stream - stream_write_csv(&lt;processed_stream_obj&gt;, \"&lt;output folder path&gt;/\")\nStop stream\n\nMight have to stop connection to spark server with spark_disconnect_all()\n\nExample:\nstream &lt;- stream_read_csv(sc, \"stream_input/\")\nstream %&gt;%\n    select(mpg, cyl, disp) %&gt;%\n    stream_write_csv(\"stream_output/\"",
    "crumbs": [
      "Apache",
      "Spark"
    ]
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-mon",
    "href": "qmd/apache-spark.html#sec-apache-spark-mon",
    "title": "Spark",
    "section": "Monitoring",
    "text": "Monitoring\n\nSparklint\n\n\nGithub\nProvides advance metrics and better visualization about your spark application’s resource utilization\n\nVital application life time stats like idle time, average vcore usage, distribution of locality\nView task allocation by executor\nVCore usage graphs by FAIR scheduler group\nVCore usage graphs by task locality\nCurrent works-in-progress (WIP)\n\nFind rdds that can benefit from persisting\nAutomated report on application bottle neck\nOpportunity to give the running application real-time hints on magic numbers like partitions size, job submission parallelism, whether to persist rdd or not\n\n\n\nwebui\n\nspark_web(sc)\n\nWhere sc is the spark connection object\n\nResources\n\nDocs: https://spark.apache.org/docs/latest/web-ui.html\nR in Spark (sections on DAGs, event timeline, and probably more)\n\nDefault URL: http:driver_host:4040\n\nSpark metrics also available through a REST API\n\nAPI Metrics\n\nSparkListener API docs: https://spark.apache.org/docs/2.4.8/api/java/org/apache/spark/scheduler/SparkListener.html\nExamples: start/end, job start/end, stage start/end, execution time, records read/written, bytes read/written, etc.\n\nMight require knowing Java to take advantage of the api\n\nExample of a “listener” was written in Java but it’s an API, so not why you can’t use any language\n\n\nPrometheus\n\nExperimental support\n\nREST API: /metrics/executors/Prometheus\n\nConditional to “spark.ui.prometheus.enabled=true”\n\n\n\nGraphana\n\nVisualizes metrics streaming to it \n\nExample: This sends metrics to influxdb using a Graphite “protocol” which can be visualized in Graphana\n\n\n\nMonitor tasks and memory usage\n\nMight be able do determine more efficient memory allocations\n\n\n\nPlugins\n\nAllow you to monitor\n\nCloud Storage\nOS metrics (important for kubernetes)\nImproved HDFS monitoring",
    "crumbs": [
      "Apache",
      "Spark"
    ]
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-cloud",
    "href": "qmd/apache-spark.html#sec-apache-spark-cloud",
    "title": "Spark",
    "section": "Cloud",
    "text": "Cloud\n\nDatabricks\n\nMisc\n\nFree community edition available\n{sparkR} and {sparklyr} are already installed on a databricks instance\nUsing RStudio and databricks\n\nVia driver node\n\nR runtime including required packages installed on every node in the cluster while RStudio Server only runs on the driver node where also the web UI is provided\nNo network latency occurs by transferring data between R and Databricks as the JVM processes run on the same machine\nAvoids having to set up another infrastructure\nIf your Databricks cluster is constantly running and you haven’t lot going on besides the interactive workloads, then this option might even be more convenient\n\nSeparate environment\n\nAvoids resource contention\nAllows you to connect to any other remote storage or compute resources if required\nFor details on this connection see RStudio and Databricks Better Together\n\n\n\nConnect\n\nsparklyr::spark_connect(method = \"databricks\")\n\nMay require some extra information on the location of the databricks spark location\n\nExample:\ndatabricks_connect_spark_home &lt;- system(\"databricks-connect get-spark-home\", intern = TRUE)\nsc &lt;- spark_connect(\n    method = \"databricks\",\n    spark_home = databricks_connect_spark_home,\n    config = conf\n)\n\nMonitoring\n\nDatabricks workspace provides through its UI a fairly easy and intuitive way of visualizing the run history of individual jobs\nDatabricks REST API can be used to extract you jobs data\n\ndocs\nMonitoring Databricks jobs through calls to the REST API\n\n\n\n\n\nAWS\n\nAWS Glue allows you to spin up in a couple of seconds a spark cluster up to 100 executors where each executor is 4 CPUs and 16GB RAM.\nAWS EMR is a managed service if you want a more configurable cluster\nEMR\n\nCons\n\nOver-Provisioning or under-provisioning leads to wasted resources or queued jobs\n\ne.g. Ad-Hoc exploration of the researchers with bursts of high usage.\n\nCan’t run multiple versions of Spark simultaneously.\n\nExample: The consequence of this was a burden for migration and testing. Routinely creating new EMR clusters to test out against new dependencies. The added infrastructure burden delayed upgrades and increased the accumulation of technical debt.\n\nScattered visibility\n\nNo single place to visualize Spark job metrics, Executor/Driver Logs, and the Application UI\nDebugging executor failures due to out-of-memory was often tricky\n\nSince Spark is a distributed computational framework - failure stack traces wouldn’t always be informative about the root cause since it may be a red herring.\n\n\nLack of configuration agility\n\nNo ability to change the underlying execution environment to remediate the failure reason(s) automatically\n\n\n\n\n\n\nKubernetes\n\nOptions\n\nSpark’s integration with Kubernetes (non-AWS I think)\nEMR on EKS\n\nGood binding for IAM, EMRFS, and S3, and AWS-provided integrations and container images\n\n\nEMR on EKS\n\nStitchfix configuration\n\nMisc\n\nSaw a 45-55% reduction in infrastructure cost for all our Spark-based compute by switching from EMR to EMR on EKS\nThey’re running thousands of daily spark jobs, so this may not be a solution for everyone plus it would require some substantial programming expertise to implement some of this stuff\n\nCluster Autoscaler - As Spark pods are scheduled, EKS cluster nodes are added or removed based on load\n\nSet to cull nodes after an unused interval of 5 minutes and a scanning interval of 1 minute\n\nContainer images use official EMR on EKS images as base images and then add other dependencies\n\nSame cluster can run multiple versions of Spark\n\nTesting a new version only requires producing a new container image base and running an ad hoc testing job\n\nEach cluster node uses the image pre-puller pattern to cache the Spark images to improve job startup time\n\nMonitoring\n\nEKS pod events + EMR on EKS CloudWatch events\n\nEKS pod events: provide granular visibility into the setup, execution, and tear down of a job.\nEMR cloudwatch: provide high-level overview of the job state (i.e., queued, started, or finished).\n\nStream logs from S3 buckets to Flotilla for monitoring and debugging\n\nEach of the container logs comes from the driver, executor, and controller pods.\n\nMetrics\n\nCluster instrumented with Victoria Metrics to collect detailed container metrics and visualized via Grafana\n\nCan diagnose difficult shuffles, disk spills, needed or unneeded caching, mis-sized executors, poor concurrency, CPU throttling, memory usage, network throttling, and many other job execution details\n\n\nActive and Completed jobs\n\nSomething technical (see article for details)\n\n\nAdaptive Behaviors\n\nExecutor and Driver Memory\n\nIf a job fails due to Executor or Driver running out of memory (exit code 137), automatically schedule the next run with more memory. (link)\n\nOther stuff (see article for details)\n\n\n\n\n\n\nGoogle Cloud Platform (GCP)\n\nRead data from cloud storage to local spark cluster\nbucket_name &lt;- \"ndir-metis-bucket\"\npath &lt;- glue::glue(\"gs://{bucket_name}/asteroid/Asteroid_Updated.csv\")\ndf &lt;- spark_read_csv(path, sep=',', inferSchema=True, header=True)\n\nI adapted this from a pyspark example but should be similar for R",
    "crumbs": [
      "Apache",
      "Spark"
    ]
  },
  {
    "objectID": "qmd/apis.html",
    "href": "qmd/apis.html",
    "title": "APIs",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "APIs"
    ]
  },
  {
    "objectID": "qmd/apis.html#sec-apis-misc",
    "href": "qmd/apis.html#sec-apis-misc",
    "title": "APIs",
    "section": "",
    "text": "Definition\n\nREST API\n\nDesign questions\n\nShould the API receive the entire datapoint (e.g sensitve customer info) or just an ID for you to query in a database itself?\nWhere should the model be loaded from? Disk? Cloud? (see Production, Deployment &gt;&gt; Model Deployment Strategies)\nWhat diagnostic output should be returned along with result?\n\nUse CI/CD to unit test, rebuild, and deploy the API every time there’s a push a commit to the production branch of your repo.\nBest Practices Thread\n\nVersioning\nIDs vs UUIDs\nNested resources\nJSON API\nLet the client decide what it wants\n\nImportant to create unit tests to use before code goes into production\n\nTest all endpoints\nCheck data types\n{testthat}\n\nExample\nlibrary(testthat)\nsource(\"rest_controller.R\")\ntestthat(\"output is a probability\", {\n    input &lt;- list(id = 123, name = \"Ralph\")\n    result &lt;- make_prediction(input)\n    expect_gte(result, 0)\n    expect_lte(result, 1)\n})\n\n\nThe only difference here between GET and POST is that you can’t put parameters and their values in the URL for POST. The parameters and values are passed in the request body as JSON.\n\nGET is a request for data from a server POST sends data to a server and also can receive data.\n\nAn IO-bound task spends most of its time waiting for IO responses, which can be responses from webpages, databases, or disks. For web development where a request needs to fetch data from APIs or databases, it’s an IO-bound task and concurrency can be achieved with either threading or async/await to minimize the waiting time from external resources.",
    "crumbs": [
      "APIs"
    ]
  },
  {
    "objectID": "qmd/apis.html#sec-apis-terms",
    "href": "qmd/apis.html#sec-apis-terms",
    "title": "APIs",
    "section": "Terms",
    "text": "Terms\n\nAsync/Await —  Unlike threading where the OS has control, with this method, we can decide which part of the code can be awaited and thus control can be switched to run other parts of the code. The tasks need to cooperate and announce when the control will be switched out. And all this is done in a single thread with the await command. (article)\nThreading — Uses multiple threads and takes turns to run the code. It achieves concurrency with pre-emptive multitasking which means we cannot determine when to run which code in which thread. It’s the operating system that determines which code should be run in which thread. The control can be switched at any point between threads by the operating system. This is why we often see random results with threading (article)\nBody — information that is sent to the server. (Can’t use with GET requests.)\nEndpoint — a part of the URL you visit. For example, the endpoint of the URL https://example.com/predict is /predict\nHeaders — used for providing information (think authentication credentials, for example). They are provided as key-value pairs\nMethod — a type of request you’re sending, can be either GET, POST, PUT, PATCH, and DELETE. They are used to perform one of these actions: Create, Read, Update, Delete (CRUD)",
    "crumbs": [
      "APIs"
    ]
  },
  {
    "objectID": "qmd/apis.html#sec-apis-meth",
    "href": "qmd/apis.html#sec-apis-meth",
    "title": "APIs",
    "section": "Methods",
    "text": "Methods\n\n\nMisc\n\nIf you’re writing a function or script, you should check whether the status code is in the 200s before additional code runs.\n\nGET\n# example 1\nargs &lt;- list(key = \"&lt;key&gt;\", id = \"&lt;id&gt;\", format = \"json\", output = \"full\", count = \"2\")\napi_json &lt;- GET(url = URL, query = args)\n\n# example 2 (with headers)\nres = GET(\"https://api.helium.io/v1/dc_burns/sum\",\n          query = list(min_time = \"2020-07-27T00:00:00Z\"\n                      , max_time = \"2021-07-27T00:00:00Z\"),\n          add_headers(`Accept`='application/json'\n                      , `Connection`='keep-live'))\n\n# example 3\nget_book &lt;- function(this_title, this_author = NA){\n  httr::GET(\n    url = url,\n    query = list(\n      apikey = Sys.getenv(\"ACCUWEATHER_KEY\"),\n      q = ifelse(\n        is.na(this_author),\n        glue::glue('intitle:{this_title}'),\n        glue::glue('intitle:{this_title}+inauthor:{this_author}')\n        )))\n}\nPOST\n# base_url from get_url above\nbase_url &lt;- \"https://tableau.bi.iu.edu/\"\nvizql &lt;- dashsite_json$vizql_root\nsession_id &lt;- dashsite_json$sessionid\nsheet_id &lt;- dashsite_json$sheetId\n\npost_url &lt;- glue(\"{base_url}{vizql}/bootstrapSession/sessions/{session_id}\")\n\ndash_api_output &lt;- POST(post_url,\n                        body = list(sheet_id = sheet_id),\n                        encode = \"form\",\n                        timeout(300))\nExample: Pull parsed json from raw format\nmy_url &lt;- paste0(\"http://dataservice.accuweather.com/forecasts/\",\n                  \"v1/daily/1day/571_pc?apikey=\", \n                 Sys.getenv(\"ACCUWEATHER_KEY\"))\nmy_raw_result &lt;- httr::GET(my_url)\n\nmy_content &lt;- httr::content(my_raw_result, as = 'text')\n\ndplyr::glimpse(my_content) #get a sense of the structure\ndat &lt;- jsonlite::fromJSON(my_content)\n\ncontent has 3 option for extracting and converting the content of the GET output.\n\n“raw” output asis\n“text” can be easiest to work with for nested json\n“parsed” is a list\n\n\nExample: json body\n\n\n\nFrom thread\n“use auto_unbox = TRUE; otherwise there are some defaults that mess with your API format”\n“url” is the api endpoint (obtain from api docs)\nheaders",
    "crumbs": [
      "APIs"
    ]
  },
  {
    "objectID": "qmd/apis.html#httr2",
    "href": "qmd/apis.html#httr2",
    "title": "APIs",
    "section": "{httr2}",
    "text": "{httr2}\n\nPOST\n\n\nContacts Home Assistant API and turns off a light.",
    "crumbs": [
      "APIs"
    ]
  },
  {
    "objectID": "qmd/apis.html#sec-apis-plumb",
    "href": "qmd/apis.html#sec-apis-plumb",
    "title": "APIs",
    "section": "{plumber}",
    "text": "{plumber}\n\nServes R objects as an API\n3 Main Components: Function Definition, Request Type, API Endpoint\nMisc\n\nAdding `host = “0.0.0.0” to run_pr() opens the API to external traffic\n{valve} - Auto-scales plumber APIs concurrently using Rust libraries Axum, Tokio, and Deadpool — similar to how gunicorn auto-scales fastapi and Flask apps\n\nCloud options for serving Plumber APIs\n\nInstall everything on an Amazon EC2 instance\nUsing a Docker image\n\nSaturn Cloud Deployments\n\nGoogle Cloud Run\nDocker/Kubernetes\n\nManaged Solutions\n\nRStudio Connect\nDigital Ocean\n\n\nLoad Testing\n\n{loadtest}\n\nTest how your API performs under various load scenarios\nOutputs tibble of various measurements\nExample:\nlibrary(loadtest)\nresults &lt;- loadtest(url = &lt;api_url&gt;, method = \"GET\", threads = 200, loops = 1000)\n\nSays simulate 200 users hitting the API 1000 times\n\n\n\nDocumentation\n\nPlumber creates an OpenAPI (aka Swagger) YAML file that documents parameters, tags, description, etc. automatically for users to know how to use your API\nAccess\n\nView webui, e.g .(http://127.0.0.1:9251/__docs__/)\n\nEdit the yaml\n\ne.g. (http://127.0.0.1:9251/openapi.json)\n\n\nScaling\n\nNatively can only handle 1 request at a time\n{valve} - Parallelize your plumber APIs. Redirects your plumbing for you.\n{future} - can be used to spawn more R processes to handle multiple requests\n\nResource: Rstudio Global 2021\nExample\n# rest_controller.R\nfuture::plan(\"multisession\")\n\n@* @post /make-prediction\nmake_prediction &lt;- function (req) {\n    future::future({       \n        user_info &lt;- req$body\n        df_user &lt;- clean_data(user_info) # sourced helper function\n        result &lt;- predict(model, data = df_user)\n        result\n    })\n}\n\n\nLogging\n\nUseful for debugging, monitoring performance, monitoring usage\nProvides data for ML monitoring to alert in case of data/model drift\n{logger}\n\nExample:\n#* @post /make-prediction\nmake_predicition &lt;- function(req) {\n    user_info &lt;- req$body\n    df_user &lt;- clean_data(user_info) # sourced helper function\n    result &lt;- predict(model, data = df_user)\n    logger::log_info(glue(\"predicted_{user_info$id}_[{result}]{style='color: #990000'}\"))\n    aws.s3::s3save(data.frame(id = user_info$id, result = result), ...)\n    result\n}\n\n\nExample: Basic Get request\n\nrest_controller.R\n#* @get /sum\nfunction(a, b) {\n    as.numeric(a) + as.numeric(b)\n}\n\n“/sum” is an endpoint\n\nRun Plumber on rest_controller.R\nplumber::pr(\"rest_controller.R\") %&gt;%\n    plumber::pr_run(port = 80)\n\n80 is a standard browser port\n\nGet the sum  of 1 + 2 by sending a Get request\n\nType “127.0.0.1/sum?a=1&b=2” into your browser\nhttr::GET(\"127.0.0.1/sum?a=1&b=2\")\n\n\nExample: Basic Model Serving\n\nrest_controller.R\nsource(\"helper_functions.R\")\nlibrary(tidyverse)\n\nmodel &lt;- read_rds(\"trained_model.rds\")\n\n#* @post /make-prediction\nmake_predicition &lt;- function(req) {\n    user_info &lt;- req$body\n    df_user &lt;- clean_data(user_info) # sourced helper function\n    result &lt;- predict(model, data = df_user)\n    result\n}",
    "crumbs": [
      "APIs"
    ]
  },
  {
    "objectID": "qmd/apis.html#sec-apis-reqlib",
    "href": "qmd/apis.html#sec-apis-reqlib",
    "title": "APIs",
    "section": "{{requests}}",
    "text": "{{requests}}\n\nUse Session to make a pooled request to the same host (Video, Docs)\n\nExample\nimport pathlib\nimport requests\n\nlinks_file = pathilib.Path.cwd() / \"links.txt\"\nlinks = links_file.read_text().splitlines()[:10]\nheaders = {\"User-Agent\": \"Mozilla/5.0 (X!!; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0}\n\n# W/o Session (takes about 16sec)\nfor link in links:\n  response = requests.get(link, headers=headers)\n  print(f\"{link} - {response.status_code}\")\n\n# W/Session (takes about 6sec)\nwith requests.Session() as session:\n  for link in links:\n    response = session.get(link, headers=headers)\n    print(f\"{link} - {response.status_code}\")\n\nThe first way syncronously makes a get request to each URL\n\nMakes several requests to the same host\n\nThe second way reuses the underlying TCP connection, which can result in a significant performance increase.\n\n\nRetrieve Paged Results One at a Time\n\nGenerator\nfrom typing import Iterator, Dict, Any\nfrom urllib.parse import urlencode\nimport requests\n\n\ndef iter_beers_from_api(page_size: int = 5) -&gt; Iterator[Dict[str, Any]]:\n    session = requests.Session()\n    page = 1\n    while True:\n        response = session.get('https://api.punkapi.com/v2/beers?' + urlencode({\n            'page': page,\n            'per_page': page_size\n        }))\n        response.raise_for_status()\n\n        data = response.json()\n        if not data:\n            break\n\n        yield from data\n\n        page += 1\nIterate through each page of results\n&gt;&gt;&gt; beers = iter_beers_from_api()\n&gt;&gt;&gt; next(beers)\n{'id': 1,\n 'name': 'Buzz',\n 'tagline': 'A Real Bitter Experience.',\n 'first_brewed': '09/2007',\n 'description': 'A light, crisp and bitter IPA brewed...',\n 'image_url': 'https://images.punkapi.com/v2/keg.png',\n 'abv': 4.5,\n 'ibu': 60,\n 'target_fg': 1010,\n...\n}\n&gt;&gt;&gt; next(beers)\n{'id': 2,\n 'name': 'Trashy Blonde',\n 'tagline': \"You Know You Shouldn't\",\n 'first_brewed': '04/2008',\n 'description': 'A titillating, ...',\n 'image_url': 'https://images.punkapi.com/v2/2.png',\n 'abv': 4.1,\n 'ibu': 41.5,",
    "crumbs": [
      "APIs"
    ]
  },
  {
    "objectID": "qmd/apis.html#sec-apis-httplib",
    "href": "qmd/apis.html#sec-apis-httplib",
    "title": "APIs",
    "section": "{{http.client}}",
    "text": "{{http.client}}\n\nDocs\nThe Requests package is recommended for a higher-level HTTP client interface.\nExample:\n\nGET\nimport http.client\n\nurl = '/fdsnws/event/1/query'\nquery_params = {\n    'format': 'geojson',\n    'starttime': \"2020-01-01\",\n    'limit': '10000',\n    'minmagnitude': 3,\n    'maxlatitude': '47.009499',\n    'minlatitude': '32.5295236',\n    'maxlongitude': '-114.1307816',\n    'minlongitude': '-124.482003',\n}\nfull_url = f'https://earthquake.usgs.gov{url}?{\"&\".join(f\"{key}={value}\" for key, value in query_params.items())}'\n\nprint('defined params...')\n\nconn = http.client.HTTPSConnection('earthquake.usgs.gov')\nconn.request('GET', full_url)\nresponse = conn.getresponse()\nJSON response\nimport pandas as pd\nimport json\n\nif response.status == 200:\n    print('Got a response.')\n    data = response.read()\n    print('made the GET request...')\n    data = data.decode('utf-8')\n    json_data = json.loads(data)\n    features = json_data['features']\n    df = pd.json_normalize(features)\n\n    if df.empty:\n        print('No earthquakes recorded.')\n    else:\n        df[['Longitude', 'Latitude', 'Depth']] = df['geometry.coordinates'].apply(lambda x: pd.Series(x))\n        df['datetime'] = df['properties.time'].apply(lambda x : datetime.datetime.fromtimestamp(x / 1000))\n        df['datetime'] = df['datetime'].astype(str)\n        df.sort_values(by=['datetime'], inplace=True)\nelse:\n  print(f\"Error: {response.status}\")",
    "crumbs": [
      "APIs"
    ]
  },
  {
    "objectID": "qmd/association-copulas.html",
    "href": "qmd/association-copulas.html",
    "title": "3  Copulas",
    "section": "",
    "text": "3.1 Misc",
    "crumbs": [
      "Association",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Copulas</span>"
    ]
  },
  {
    "objectID": "qmd/association-copulas.html#sec-assoc-cop-misc",
    "href": "qmd/association-copulas.html#sec-assoc-cop-misc",
    "title": "3  Copulas",
    "section": "",
    "text": "Notes from: https://hudsonthames.org/copula-for-pairs-trading-introduction/\nAlso see\n\nForecasting, Nonlinear &gt;&gt; Misc &gt;&gt; packages, copulas\nFinance &gt;&gt; Mean Reversion Strategy or Pairs Trading\n\nPackages\n\n{{latentcor}}: semi-parametric latent Gaussian copula models\n\nUsed in finance for non-linear and tail risk qualities but currently doesn’t take autocorrelation into account\nUsed to create joint distributions that can be used to describe associated “entities” that may not be from the same distribution\n\nIf each entity has a different behavior, we cannot assume they follow the same distribution.\nAnd most importantly, each entity is likely to influence the others — we cannot assume they are independent. Take product cannibalization, for example: In retail, a successful product pulls demand away from similar items in its category.\nHence, each entity may have a different distribution. Plus, we should find a way to model their correlation, since independence is seldom feasible in most practical scenarios.\n\nNotes\n\nA copula is a multivariate distribution that can be formed from a variety of underlying distributions (e.g. gamma, normal, beta) with a specified correlation structure (depending on the type of copula you choose). You create one of these copulas from your data, and sample from it. These samples are used to run simulations on.\n\nExample:\n\nTake return prices from a few correlated stocks and create a copula.\nTrain a model with economic predictors and your sampled copula data as the response.\nFeed values for you economic predictors that indicate an economic state (e.g. recession) to your model and forecast the response to see how that group of stocks reacts.\n\n\nThe specified correlation stucture is called the “dependence structure.”\n\ne.g. asymetrical correlation or tail correlation\n\nThe Gaussian copula is typically described as \\(\\Phi_R (\\Phi^{-1}(u_1), \\ldots, \\Phi^{-1}(u_d))\\), but each \\(u\\) is NOT a variable in your data. Each \\(u\\) is the result of feeding a variable of your data through its ECDF. That result is always a uniform random variable with 0,1 parameters, \\(\\mathcal{U}(0,1)\\), hence the “u.” So that copula definition is equivalent to \\(\\Phi_R (\\Phi^{-1}(F_1(X_i), \\ldots, \\Phi^{-1}(F_d(X_d))\\) where \\(X_i\\) is one of your data variables and \\(F_i\\) is its ECDF.\nEach ECDF of your data is a marginal distribution is often simply referred to as a “marginal.”\n{vinecopula} and {copula} have a function called pobs which feeds your data through an ecdf and scales it by (n+1). Didn’t know it had to be scaled, so maybe an ecdf doesn’t always output values between 0 and 1 like a cdf does.",
    "crumbs": [
      "Association",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Copulas</span>"
    ]
  },
  {
    "objectID": "qmd/association-copulas.html#sec-assoc-cop-its",
    "href": "qmd/association-copulas.html#sec-assoc-cop-its",
    "title": "3  Copulas",
    "section": "3.2 Inverse Transform Sampling",
    "text": "3.2 Inverse Transform Sampling\n\nNotes from video\nUnderstanding IVS will help with understanding the copula mathematics\nRelationship between PDF and CDF (e.g. exponential distribution)\n\n\nProbability x ≤ 2 is\n\nthe shaded area of the PDF\nIs the output of CDF(x) where x = 2\n\nEquivalence between the PDF and CDF shown in the top integral\n\nMore general form shown in the bottom integral\n\n\nThe inverse of the CDF give you the value of x for any probability\n\n\ne.g. CDF-1(0.7) = 2 and CDF-1(0.5) = 0.7\nWhichever distribution’s CDF-1 is used, the output of that function will be from that distribution (e.g. 2, 0.7)\n\nMathematically:\n\\[\nu_i \\sim \\mathcal{U}(0, 1)\\\\\nx_i = \\mbox{CDF}^-1(u_i)\n\\]\n\n\nExpression says to take a sample from a Uniform distribution, plug that into the inverse CDF, and get a sample from the that CDF’s distribution\n\n\n\nExample: Gamma Distribution\ngamma1 &lt;- rgamma(1e6, shape=1)\n\nhist(gamma1, main='gamma distribution', cex.main=1.3, cex.lab=1.3, cex.axis=1.3, prob='true')\n\n# pgamma is the cdf of gamma\nu &lt;- pgamma(gamma1, shape=1)\nhist(u, main='Histogram of uniform samples from gamma CDF', cex.main=1.3, cex.lab=1.3, cex.axis=1.3, prob='true')\n\n# qgamma is the inverted cdf of gamma\ngamma_transformed &lt;- qgamma(u, shape=1)\nhist(gamma_transformed, main='Histogram of transformed gamma', cex.main=1.3, cex.lab=1.3, cex.axis=1.3,prob='true')",
    "crumbs": [
      "Association",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Copulas</span>"
    ]
  },
  {
    "objectID": "qmd/association-copulas.html#sec-assoc-cop-sklar",
    "href": "qmd/association-copulas.html#sec-assoc-cop-sklar",
    "title": "3  Copulas",
    "section": "3.3 Sklar’s Theorem",
    "text": "3.3 Sklar’s Theorem\n\nGuarantees the existence and uniqueness of a copula for two continuous random variables\nFor two random variables \\(S_1\\), \\(S_2\\) in \\([-\\infty, \\infty]\\). \\(S_1\\) and \\(S_2\\) have their own fixed, continuous CDFs, \\(F_1\\), \\(F_2\\).\n\nConsider their (cumulative) joint distribution\n\\[\nH(s_1, s_2) := P(S_1 \\leq s_1, S_2 \\leq s_2)\n\\]\nNow take the uniformly distributed quantile random variable, \\(U_1(S_1)\\), \\(U_2(S_2)\\). For every pair, \\((u_1, u_2)\\), drawn from the pair’s quantile, we define the *bivariate copula, \\(C: [0,1] \\times [0,1] \\rightarrow [0,1]\\) as:\n\\[\n\\begin {align}\nC(u_1, u_2) &= P(U_1 \\leq u_1, U_2 \\leq u_2) \\\\\n            &= P(S_1 \\leq F_1^{-1}(u_1), S_1 = F_2^{-1}(u2)) \\\\\n            &= H(F_1^{-1}(u_1), F_2^{-1}(u_2))\n\\end {align}\n\\]\n\nWhere \\(F_1^{-1}\\) and \\(F_2^{-1}\\) are inverses (i.e. solved for S) of the marginal CDFs, \\(F_1\\) and \\(F_2\\).\nA copula is just the joint cumulative density for quantiles of a pair of random variables.\n\\(H\\) is “some” function. It varies with the type of copula (see types section).\nSee Probability notebook, “Simulation of a random variable values of a distribution using the distribution’s cdf” section and bookmarks similarly named for some intuition behind what’s happening in Sklar’s Theorem\n\nIf the 1st quantile is from 0 to 0.25 then randomly select some numbers in that range using U(0,1)\n\nThis is unclear to me, he might be calling every number drawn from U(0,1) a “quantile”\nBut I think because you’re using quantiles it’s non-linear which kind a makes sense (thinking about why quantile regression is used sometimes), so maybe he is talking about deciles, quartiles, etc.\nMaybe (see third line of this section) each quantile is treated as a separate dateset where numbers are drawn from U(0,1) and copula calculated.\n\nFind the inverse CDF of the distribution\nPlug those numbers from the 1st quantile into the inverse CDF to get the simulated values\nRepeat for other random variable\n\n\n\n\n\nThe scatter plot crosshair says of the value of the inverse CDF of variable U using input 0.1 corresponds to the value of the inverse CDF of variable V using input 0.3. The closer the points are to the y = x line the greater the association between them (like a Q-Q plot)\nMathematically the cumulative conditional probabilities shown in the scatter plot are given by taking partial derivatives of the joint inverse CDF:\n\n\nAside: taking the derivative of a (not inverse) marginal (not joint) CDF is the pdf\n\nThe copula density is defined as:\n\n\nWhich is a probability density. Larger the copula density, the denser the clump of points in the scatter plot\n\n\nCoefficients of Lower and Upper Tail Dependence: quantifies the strength of association during joint tail events for each random variable’s distribution\n\nNot discernible from plots, so needs to be calculated\nUpper tail dependencies refers to the how closely two variables increase together during an extreme “positive” event\n\ne.g. How strongly 2 stocks move together during an huge gain\nLower tail dependencies are similar except the event is in extreme “negative” direction\n\nFor stocks at least, lower tail dependencies tend to be much stronger than upper tail tendencies\n\nTypes\n\nNot including the actual bivariate copula formulas because I’m not sure how the “H” (see bivariate copula def above) works in practice (and I don’t want to frustrate future me). I am including descriptions and important characteristics which should have practical applicability. See article for copula formulas.\nArchimedean\n\nParametric and uniquely determined by generator functions, φ, that use a parameter, θ\n\nθ measures how “closely” the two random variables are “related”, and its exact range and interpretation are different across different Archimedean copulas\nGenerators seem to act like the inverse CDFs in the bivariate copula formula\nGenerators:\n\n\nSymmetric and scalable to multiple variables, although a closed-form solution may not be available in higher dimensions\n\nElliptical\n\nSymmetric and easily extended to multiple variables\nAssumes symmetry on both upward co-moves and downward moves (i.e. lacks flexibility)\nGaussian - uses Gaussian inverse CDF and a correlation matrix\nStudent-t - similar as Gaussian but with degrees of freedom\n\nMixed\n\nWeighted ensemble of the copulas above\nHelps with overfitting and more finely calibrating upper and lower tail dependencies\nWeights should sum to 1\n\nOther Notes\n\nThe wording below is a bit confusing\n\n“Don’t have” I think means doesn’t have the capability to detect or isn’t sensitive to\n“Stronger center dependence” might mean a greater ability to detect or maybe a center dependence bias\n\nI’m not even sure what a “center” dependency means\n\n\nFrank and Gaussian copulas don’t have tail dependencies\n\nGaussian contributed to 2008 financial crisis\n\nFrank copula has a stronger center dependence than a Gaussian (?)\nCopulas with upper tail dependence: Gumbel, Joe, N13, N14, Student-t.\nCopulas with lower tail dependence: Clayton, N14 (weaker than upper tail), Student-t.\nStudent t copula emphasizes extreme results: it is usually good for modelling phenomena where there is high correlation in the extreme values (the tails of the distribution).\n\nNote also that the correlation is symmetrical, so the strength of correlation is the same for both tails. This might be an issue for some applications.\n\n\n\nNotes\n\nDefinition\n\n\nCopulas are joint cumulative distribution functions (c.d.f.) for unit-uniform random variables\n\nProbability integral transform\n\n\nStates that we can transform any continuous random variable to a uniform one by plugging it into its own c.d.f.\nTransform a uniform random variable to any continuous random variable\n\n\nSo plugging a Uniform random variable into the quantile function which is the inverse of the cdf and outputs a continuous random variable\n\n\nGaussian Copula\n\n\nExample\n\nDefining a (generic) Copula (aka joint cdf) for two random variables\n\n\nWhere FX(x), FY(y) are cdfs of Gamma, Beta distributions respectively\n\nDefining a Gaussian Copula for these 2 random variables\n\n\nFormat: Copula = joint cdf(quantile(cdf(gamma_random_variable), cdf(gamma_random_variable))\n\nConstruct a Copula\n\nTransform the Gamma and Beta marginals into Uniform marginals via the respective c.d.f.s\nTransform the Uniform marginals into standard Normal marginals via the quantile functions\nDefine the joint distribution via the multivariate Gaussian c.d.f. with zero mean, unit variance and non-zero covariance (covariance matrix R)\n\nSample from a Copula\n\nThe bi-variate random variable has the above properties. (standard Gamma/Beta marginals with Gaussian Copula dependencies)\nSteps (reverse of the copula process)\n\nDraw a sample from a bi-variate Gaussian with mean zero, unit variance and non-zero covariance (covariance matrix R).\n\nYou now have two correlated standard Gaussian variables.\n\nTransform both variables with the standard Gaussian c.d.f. (i.e. plugging each into a gaussian cdf)\n\nYou now have two correlated Uniform variables. (via probability integral transform)\n\nTransform one variable with the standard Beta quantile function and the other variable with the Gamma quantile function\n\nCode (Julia)\nusing Measures\nRandom.seed!(123)\n\n# Step 1: Sample bi-variate Gaussian data with zero mean and unit variance\nmu = zeros(2)\nR = [1 0.5; 0.5 1]\nsample = rand(MvNormal(mu,R),10000)\n\n# Step 2: Transform the data via the standard Gaussian c.d.f.\nsample_uniform = cdf.(Normal(), sample)\n\n# Step 3: Transform the uniform marginals via the standard Gamma/Beta quantile functions\nsample_transformed = sample_uniform\nsample_transformed[1,:] = quantile.(Gamma(),sample_transformed[1,:])\nsample_transformed[2,:] = quantile.(Beta(),sample_transformed[2,:])\n\n\nVisuals\n\nNote: We could drop the zero-mean, unit-variance assumption on the multivariate Gaussian.\n\nIn that case we would have to adjust the Gaussian c.d.f. to the corresponding marginals in order to keep the integral probability transform valid.\nSince we are only interested in the dependency structure (i.e. covariances), standard Gaussian marginals are sufficient and easier to deal with\n\nExample: Gaussian Copula derived from beta and gamma vectors (i.e. “marginals”) (article)\n\nIn this example, marginal 1 and marginal 2 are sampled from the beta and gamma distributions, respectively\n# draw our data samples from 2 distributions, a beta and a gamma - \nbeta1 = stats.distributions.beta(a=10, b=3).rvs(1000)\ngamma1 = stats.distributions.gamma(a=1, loc=0).rvs(1000)\n\n# - we use the emprical cdf instead of beta's or gamma's cdf\n# - we do this to show that copulas can be computed regardless of the\n#  underlying distributions\necdf1 = ECDF(beta1)      # F(beta1) = u1\necdf2 = ECDF(gamma1)      # F(gamma1) = u2\n# small correction to remove infinities\necdf1.y[0]=0.0001\necdf2.y[0]=0.0001\n\nx1=stats.norm.ppf(ecdf1.y) # Φ^-1(u1)\nx2=stats.norm.ppf(ecdf2.y) # Φ^-1(u1)\n\n# Parameters of Φ2\nmu_x = 0\nvariance_x = 1\nmu_y = 0\nvariance_y = 1\ncov=0.8\n\n# I think this is just some preprocessing to get the vectors into the correct shape for the mvn function\nX, Y = np.meshgrid(x1,x2)\npos = np.empty(X.shape + (2,))\npos[:, :, 0] = X; pos[:, :, 1] = Y\n\n#remember phi2 is just a multivariate normal CDF\nrv = stats.multivariate_normal([mu_x, mu_y], [[variance_x, cov], [cov, variance_y]])\nphi2=rv.cdf(pos)\n\nNote how Empirical CDFs are used which what you’d use if you didn’t know the underlying distribution of your two vectors\n\nSee Distributions &gt;&gt; Terms &gt;&gt; Empirical CDFs\n\nSteps\n\nCompute ECDFs of vectors with “unknown” distributions\nApply Gaussian Copula formula\n\nCompute gaussian inverse CDFs for each vector\nDecide on parameter values of multivariate gaussian distribution\n\nMean, Variance, and Covariance\n\nCreate multivariate gaussian distribution\nApply multivariate gaussian CDF to the inverse CDFs of the two vectors",
    "crumbs": [
      "Association",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Copulas</span>"
    ]
  },
  {
    "objectID": "qmd/association-general.html",
    "href": "qmd/association-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-misc",
    "href": "qmd/association-general.html#sec-assoc-gen-misc",
    "title": "General",
    "section": "",
    "text": "Also see\n\nEDA &gt;&gt; Correlation\nNotebook &gt;&gt; Statistical Inference &gt;&gt; Correlation\n\nE(υ|x)=0 is equivalent to Cov(x,υ)=0 or Cor(x,υ)=0\nA negative correlation between variables is also called anticorrelation or inverse correlation\nIndependence - Two random variables are independent if the product of their individual probability density functions equals the joint probability density function",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-partcor",
    "href": "qmd/association-general.html#sec-assoc-gen-partcor",
    "title": "General",
    "section": "Partial Correlation",
    "text": "Partial Correlation\n\nStatistical Formula\n\\[\n\\frac{\\mbox{Cov}(X, Y) - \\mbox{Cov}(X, Z) \\cdot \\mbox{Cov}(Y, Z)}{\\sqrt{\\mbox{Var}(X) - \\mbox{Cov}(X, Z)^2}\\cdot \\sqrt{\\mbox{Var}(Y) - \\mbox{Cov}(Y, Z)^2}}\n\\]\nMeasures the association (or correlation) between two variables when the effects of one or more other variables are removed from such a relationship.\n\nIn the above equation, I think it’s the partial correlation between x and y given z.\n\nMisc\n\nResources\n\nDealing with correlation in designed field experiments: part I\n\nExcellent tutorial on partial, joint correlations in block design\n\nppcor pkg: An R Package for a Fast Calculation to Semi-partial Correlation Coefficients\n\nExplainer for semi-partial, partial correlation\n\nAlso see notebook for a method using regression models\n\n\nExample: psych::partial.r(y ~ x - z, data)\nExample: {correlation}\nhead(correlation::correlation(mtcars, partial = TRUE))\n\n#&gt; # Correlation Matrix (pearson-method)\n\n#&gt; Parameter1 | Parameter2 |     r |         95% CI | t(30) |      p\n#&gt; -----------------------------------------------------------------\n#&gt; mpg        |        cyl | -0.02 | [-0.37,  0.33] | -0.13 | &gt; .999\n#&gt; mpg        |       disp |  0.16 | [-0.20,  0.48] |  0.89 | &gt; .999\n#&gt; mpg        |         hp | -0.21 | [-0.52,  0.15] | -1.18 | &gt; .999\n#&gt; mpg        |       drat |  0.10 | [-0.25,  0.44] |  0.58 | &gt; .999\n#&gt; mpg        |         wt | -0.39 | [-0.65, -0.05] | -2.34 | &gt; .999\n#&gt; mpg        |       qsec |  0.24 | [-0.12,  0.54] |  1.34 | &gt; .999\n#&gt; \n#&gt; p-value adjustment method: Holm (1979)\n#&gt; Observations: 32\n\nVisualization\n\npacman::p_load(see, ggraph)\ncorrelation::correlation(mtcars, partial = TRUE) |&gt; \n  plot()\n\nGraphical LASSO\n\nComputing covariance matrices are computationally expensive while computing its inverse can be less so. This algorithm calculates the inverse covariance matrix (ICT), aka Precision Matrix, and it’s based on an interplay between probability theory and graph theory, in which the properties of an underlying graph specify the conditional independence properties of a set of random variables.\n\nSee Statistical Learning With Sparsity (Hastie, Tibshirani, Wainright)\n\nMathematical introduction to graphical models and Graphical LASSO, pg 241 (252 in pdf), See R &gt;&gt; Documents &gt;&gt; Regression\n\n\nAssumes that the observations have a multivariate Gaussian distribution\nMisc\n\nPackages\n\n{glasso} - The original package by the authors of the algorithm. Estimation of a sparse inverse covariance matrix using a lasso (L1) penalty. Facilities are provided for estimates along a path of values for the regularization parameter. Can be slow or nonconvergent for large dimension datasets.\n{huge} - Provides functions for estimating high dimensional undirected graphs from data. Also provides functions for fitting high dimensional semiparametric Gaussian copula models (Vignette)\n{cglasso} - Conditional Graphical Lasso Inference with Censored and Missing Values (Vignette)\n\n\nPreprocessing: All variables should be standardized.\nThe terms in the ICT are not equivalent but are proportional to the partial correlation between the two corresponding variables\n\nTransform the ICT, \\(\\Omega\\) into a partial correlation matrix, \\(R\\)\n\\[\nR_{j,k} = \\frac{-\\Omega_{i,j}}{\\sqrt{\\Omega_{j,j}\\Omega_{k,k}}}\n\\]\nparr.corr &lt;- matrix(nrow=nrow(P), ncol=ncol(P))\nfor(k in 1:nrow(parr.corr)) {\n  for(j in 1:ncol(parr.corr)) {\n    parr.corr[j, k] &lt;- -P[j,k]/sqrt(P[j,j]*P[k,k])\n  }\n}\ncolnames(parr.corr) &lt;- colnames(P)\nrownames(parr.corr) &lt;- colnames(P)\ndiag(parr.corr) &lt;- 0\nSetting the terms on the diagonal to zero prevents variables from having connections with themselves in a network graph if you want to visualize the relationships\n\nWhere the nodes are variables and edges are the partial correlations.\n\n\nHyperparameter, \\(\\rho\\) , adjusts the sparsity of the matrix output\n\nHigher: Isolates the strongest relationships in your data (more sparse)\nLower: Preserving more tenuous connections, perhaps identifying variables with connections to multiple groups (less sparse)\n\nCheck symmetry. Assymmetry in the ICT can arise due to numerical computation and rounding errors, which can cause problems later depending on what you want to do with the matrix.\nExample: Stock Analysis using {glasso} (link)\nrho &lt;- 0.75\ninvcov &lt;- glasso(S, rho=rho)  \n\n# inverse covariance matrix\nP &lt;- invcov$wi\ncolnames(P) &lt;- colnames(S)\nrownames(P) &lt;- rownames(S)\n\n# check symmetry\nif(!isSymmetric(P)) {\n  P[lower.tri(P)] = t(P)[lower.tri(P)]  \n}\n\nGoal: Remove stocks relationship with market Beta and other confounding stocks to get the true relationsip between stock pairs.\nPost also has a network visualization. Data was put through PCA, then DBSCAN to get clusters. The cluster assignments were used to color the clusters in the network graph.\nPost also examines output from a lower \\(\\rho\\) and has an interesting analysis of the non-connected variables (i.e. no partial correlation).",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-cont",
    "href": "qmd/association-general.html#sec-assoc-gen-cont",
    "title": "General",
    "section": "Continuous",
    "text": "Continuous\n\nSpearman’s Rank\n\\[\n\\rho = 1 - \\frac{6\\sum_i d_i^2}{n(n^2-1)}\n\\]\n\n\\(d_i\\): The difference in ranks for the ith observation\nMeasures how well the relationship between the two variables can be described by a monotonic function\nRank correlation measures the similarity of the order of two sets of data, relative to each other (recall that PCC did not directly measure the relative rank).\n\nValues range from -1 to 1 where 0 is no association and 1 is perfect association\nNegative values don’t mean anything in ranked correlation, so just remove the negative\n\nLinear relationship is a specific type of monotonic relationship where the rate of increase remains constant — in other words, unlike a linear relationship, the amount of change (increase or decrease) in a monotonic relationship can vary.\nSee bkmks for CIs\nPackages\n\n{stats::cor.test(method = “spearman”)}\n{DescTools::SpearmanRho}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\n\nKendall’s Tau\n\nNon-parametric rank correlation\n\nNon-parametric because it only measures the rank correlation based on the relative ordering of the data (and not the specific values of the data).\n\nShould be pretty close to Sspearman’s Rank but a potentially faster calculation\nFlavors: a, b (makes adjustment for ties), c (for different sample sizes for each variable)\n\nUse Tau-b if the underlying scale of both variables has the same number of possible values (before ranking) and Tau-c if they differ.\ne.g. One variable might be scored on a 5-point scale (very good, good, average, bad, very bad), whereas the other might be based on a finer 10-point scale. In this case, Tau-c would be recommended.\n\nPackages\n\n{stats::cor.test(method = “kendall”)} - Doesn’t state specifically but I think it calculates a and b depending on whether ties are present or not\n{DescTools} - has all 3 flavors\n\n\nHoeffding’s D\n\nRank-based approach that measures the difference between the joint ranks of (X,Y) and the product of marginal ranks.(?) A non-parametric test of independence. the product of their marginal ranks.\nUnlike the Pearson or Spearman measures, it can pick up on nonlinear relationships.\nRange: [-.5,1]\nGuidelines: Larger values indicate a stronger relationship between the variables.\nPackages\n\n{Hmisc::hoeffd}\n{DescTools::HoeffD}\n\n\nBayesian\n\nSteps: {brms}\n\nList the variables you’d like correlations for within mvbind().\nPlace the mvbind() function within the left side of the model formula.\nOn the right side of the model formula, indicate you only want intercepts (i.e., ~ 1).\nWrap that whole formula within bf().\nThen use the + operator to append set_rescor(TRUE), which will ensure brms fits a model with residual correlations.\nUse non-default priors and the resp argument to specify which prior is associated with which criterion variable\n\nGaussian\n\nExample: multiple variables\nf9 &lt;- \n   brm(data = d,\n    family = gaussian,\n    bf(mvbind(x_s, y_s, z_s) ~ 0,\n       sigma ~ 0) +\n    set_rescor(TRUE),\n    prior(lkj(2), class = rescor),\n    chains = 4, cores = 4,\n    seed = 1)\n\n## Residual Correlations: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## rescor(xs,ys)    0.90      0.02    0.87    0.93 1.00    3719    3031\n## rescor(xs,zs)    0.57      0.07    0.42    0.69 1.00    3047    2773\n## rescor(ys,zs)    0.29      0.09    0.11    0.46 1.00    2839    2615\nStandardized data is used here but isn’t required\n\nWill need to set priors though (see article for further details)\n\nSince the data is standardized, the sd can be fixed at 1\n\nbrms models log of sd by default, hence sigma ~ 0 since log 1 = 0\n\nCorrelations are the estimates for rescor(xs,ys), rescor(xs,zs) rescor(ys,zs)\n\nStudent t-distribution\n\nIf the data has any outliers, pearson’s coefficient is substantially biased.\nExample: correlation between x and y\n\\\nf2 &lt;- \n    brm(data = x.noisy, \n    family = student,\n    bf(mvbind(x, y) ~ 1) + set_rescor(TRUE),\n    prior = c(prior(gamma(2, .1), class = nu),\n              prior(normal(0, 100), class = Intercept, resp = x),\n              prior(normal(0, 100), class = Intercept, resp = y),\n              prior(normal(0, 100), class = sigma, resp = x),\n              prior(normal(0, 100), class = sigma, resp = y),\n              prior(lkj(1), class = rescor)),\n    iter = 2000, warmup = 500, chains = 4, cores = 4, \n    seed = 210191)\n\n## Population-Level Effects: \n##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## x_Intercept    -2.07      3.59    -9.49    4.72 1.00    2412    2651\n## y_Intercept    1.93      7.20  -11.31    16.81 1.00    2454    2815\n## \n## Family Specific Parameters: \n##        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma_x    18.35      2.99    13.12    24.76 1.00    2313    2816\n## sigma_y    36.52      5.90    26.13    49.49 1.00    2216    3225\n## nu          2.65      0.99    1.36    4.99 1.00    3500    2710\n## nu_x        1.00      0.00    1.00    1.00 1.00    6000    6000\n## nu_y        1.00      0.00    1.00    1.00 1.00    6000    6000\n## \n## Residual Correlations: \n##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## rescor(x,y)    -0.93      0.03    -0.97    -0.85 1.00    2974    3366\n\nN = 40 simulated from a multivariate normal with 3 outliers\nCorrelation is the rescor(x,y) estimate -0.93; true value is -0.96\n\nUsing a pearson coefficient, cor = -0.6365649\nUsing brms::brm with family = gaussian, rescor(x,y) estimate -0.61",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-disc",
    "href": "qmd/association-general.html#sec-assoc-gen-disc",
    "title": "General",
    "section": "Discrete",
    "text": "Discrete\n\nMisc\n\nAlso see\n\nMultiple Correspondence Analysis (MCA) (see bkmks &gt;&gt; Features &gt;&gt; Reduction)\nDiscrete Analysis Notebook\n\nPackages\n\n{PAsso} - Assesses the Partial Association Between Ordinal Variables\n\nAllows users to perform a wide spectrum of assessments, including quantification, visualization, and hypothesis testing.\nVignette\n\n\nBinary vs Binary Similarity measures (paper)\n\nNote that a pearson correlation between binaries can be useful (see EDA &gt;&gt; Misc &gt;&gt; {correlationfunnel})\nTypes:\n\nJaccard-Needham\nDice\nYule\nRussell-Rao\nSokal-Michener\nRogers-Tanimoto\nKulzinsky\n\nPackages\n\n{{scipy}} - Also has other similarity measures\n\n\n\nPhi Coefficient - Used for binary variables when the categories are truly binary and not crudely measuring some underlying continuous variable (i.e. dichotomization of a continuous variable)\n\n“A Pearson correlation coefficient estimated for two binary variables will return the phi coefficient” (Phi coefficient wiki)\n(Contingency Table) Two binary variables are considered positively associated if most of the data falls along the diagonal cells. In contrast, two binary variables are considered negatively associated if most of the data falls off the diagonal\nAlso see StackExchange discussion on the difference between Phi Coefficient and Tetrachoric correlation\n{DescTools::Phi}\n\nCramer’s V - Association between two nominal variables\n\nSee Discrete Analysis notebook\n{DescTools::CramerV}\n\nPolychoric - Suppose each of the ordinal variables was obtained by categorizing a normally distributed underlying variable, and those two unobserved variables follow a bivariate normal distribution. Then the (maximum likelihood) estimate of that correlation is the polychoric correlation.\n\n{polycor}\n{psych::polychoric}\n\nFor correct=FALSE, the results agree perfectly with {polycor}\nFor very small data sets, the correction for continuity for the polychoric correlations can lead to difficulties, particularly if using the global=FALSE option, or if doing just one correlation at a time. Setting a smaller correction value (i.e., correct =.1) seems to help.\n\n{DescTools::CorPolychor}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\nTetrachoric - Used for binary variables when those variables are a sort of crude measure of an underlying continuous variable\n\nAlso see StackExchange discussion on the difference between Phi Coefficient and Tetrachoric correlation\nExample of appropriate use case: Suppose there are two judges who judge cakes, say, on some continuous scale, then based on a fixed, perhaps unknown, cutoff, pronounce the cakes as “bad” or “good”. Suppose the latent continuous metric of the two judges has correlation coefficient ρ.\n“the contingency tables are ‘balanced’ row-wise and col-wise, you get good correlation between the two metrics, but the tetrachoric tends to be a bit larger than the phi coefficient. When the cutoffs are somewhat imbalanced, you get slightly worse correlation between the metrics, and the phi appears to ‘shink’ towards zero.”\nThe estimation procedure is two stage ML.\n\nCell frequencies for each pair of items are found. Cells with zero counts are replaced with .5 as a correction for continuity (correct=TRUE).\nThe marginal frequencies are converted to normal theory thresholds and the resulting table for each item pair is converted to the (inferred) latent Pearson correlation that would produce the observed cell frequencies with the observed marginals\n\n{psych::tetrachoric}\n\nThe correlation matrix gets printed, but the correlations can also be extracted with $rho\nCan be sped up considerably by using multiple cores and using the parallel package. The number of cores to use when doing polychoric or tetrachoric may be specified using the options command. (e.g options(\"mc.cores\"=4);)\nsmooth = TRUE - For sets of data with missing data, the matrix will sometimes not be positive definite. Uses a procedure to transform the negative eigenvalues.\nFor relatively small samples with dichotomous data if some cells are empty, or if the resampled matrices are not positive semi-definite, warnings are issued. this leads to serious problems if using multi.cores. The solution seems to be to not use multi.cores (e.g., options(mc.cores =1)\n\nGoodman and Kruskal’s Gamma\n\nA measure of rank correlation, i.e., the similarity of the orderings of the data when ranked by each of the quantities. It measures the strength of association of the cross tabulated data when both variables are measured at the ordinal level.\nFor 2-way contingincy tables (i.e. 2x2 tables)\nIt makes no adjustment for either table size or ties.\nValues range from −1 (100% negative association, or perfect inversion) to +1 (100% positive association, or perfect agreement). A value of zero indicates the absence of association.\n{DescTools::GoodmanKruskalGamma}",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-mix",
    "href": "qmd/association-general.html#sec-assoc-gen-mix",
    "title": "General",
    "section": "Mixed",
    "text": "Mixed\n\nMisc\n\n{psych::mixedCor} - finds Pearson correlations for the continous variables, polychorics for the polytomous items, tetrachorics for the dichotomous items, and the polyserial or biserial correlations for the various mixed variables (no polydi?)\n\nBiserial - correlation between a continuous variable and binary variable, which is assumed to have resulted from a dichotomized normal variable\n\n{psych::biserial}\n\nPolydi - correlation between multinomial variable and binary variable\n\n{psych::polydi}\n\nPolyserial - polychoric correlation between a continuous variable and ordinal variable\n\nBased on the assumption that the joint distribution of the quantitative variable and a latent continuous variable underlying the ordinal variable is bivariate normal\n{polycor}\n{psych::polyserial}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\nX2Y\n\nHandles types: continuous-continuous, continuous-categorical, categorical-continuous and categorical-categorical\nCalculates the % difference in prediction error after fitting a decision tree between two variables of interest and the mean (numeric) or most frequent (categorical)\nFunction is available through a script (Code &gt;&gt; statistical-testing &gt;&gt; correlation)\n\narticle with documentation and usage, https://rviews.rstudio.com/2021/04/15/an-alternative-to-the-correlation-coefficient-that-works-for-numeric-and-categorical-variables/\n\nAll x2y values where the y variable is continuous will be measuring a % reduction in MAE. All x2y values where the y variable is categorical will be measuring a % reduction in Misclassification Error. Is a 30% reduction in MAE equal to a 30% reduction in Misclassification Error? It is problem dependent, there’s no universal right answer.\n\nOn the other hand, since (1) all x2y values are on the same 0-100% scale (2) are conceptually measuring the same thing, i.e., reduction in prediction error and (3) our objective is to quickly scan and identify strongly-related pairs (rather than conduct an in-depth investigation), the x2y approach may be adequate.\n\nNot symmetric, but can average both scores to get a pseudo-symmetric value\nBootstrap CIs available\n\nCopulas\n\nlatentcor PKG: semi-parametric latent Gaussian copula models",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-nonlin",
    "href": "qmd/association-general.html#sec-assoc-gen-nonlin",
    "title": "General",
    "section": "Non-linear",
    "text": "Non-linear\n\nMisc\n\nAlso see General Additive Models &gt;&gt; Diagnostics for a method of determining a nonlinear relationship for either continuous or categorical outcomes.\n\nξ (xi) coefficient\n\nPaper: A New Coefficient of Correlation\nArticle: Exploring the XI Correlation Coefficient\nExcels at oscillatory and highly non-monotonic dependencies\nXICOR::xicor - calculates ξ and performs a significance test (H0: independent)\n\nXICOR::calculateXI just calculates the ξ coefficient\n\nProperties (value ranges; interpretation)\n\nIf y is a function of x, then ξ goes to 1 asymptotically as n (the number of data points, or the length of the vectors x and y) goes to Infinity.\nIf y and x are independent, then ξ goes to 0 asymptotically as n goes to Infinity.\n\nValues can be negative, but this negativity does not have any innate significance other than being close to zero\nn &gt; 20 necessary\n\nn larger than about 250 probably sufficient to get a good estimate\n\nFairly efficient (O(nlogn), compared to some more powerful methods, which are O(n2))\nIt measures dependency in one direction only (is y dependent on x not vice versa)\nDoesn’t tell you if the relationship is direct or inverse",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-time-series.html",
    "href": "qmd/association-time-series.html",
    "title": "Time Series",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Association",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/association-time-series.html#sec-assoc-ts-misc",
    "href": "qmd/association-time-series.html#sec-assoc-ts-misc",
    "title": "Time Series",
    "section": "",
    "text": "Also see EDA, Time Series &gt;&gt; Association",
    "crumbs": [
      "Association",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/association-time-series.html#sec-assoc-ts-ccf",
    "href": "qmd/association-time-series.html#sec-assoc-ts-ccf",
    "title": "Time Series",
    "section": "Cross-Correlation Function (CCF)",
    "text": "Cross-Correlation Function (CCF)\n\nThe correlation between two stationary series. The cross-correlation function (CCF) helps you determine which lags of time series \\(X\\) predicts the value of time series \\(Y\\).\nFor a set of sample correlations between \\(x_{t+h}\\) and \\(y_t\\) for \\(h = 0, \\pm1, \\pm2, \\pm3, \\ldots\\), a negative value for \\(h\\) is a correlation between \\(x\\) at a time before \\(t\\) (i.e. lag) and \\(y\\) at time \\(t\\).\n\nFor instance, consider \\(h = −2\\). The CCF value would give the correlation between \\(x_{t−2}\\) and \\(y_t\\).\n\nMisc\n\nPackages\n\n{ebtools::prewhitened_ccf}\n{forecast::ggCcf} - You have to manually prewhiten series before using this function.\n{feasts::CCF} - You have to manually prewhiten series before using this function. Data needs to be a tsibble.\n\nWhen calculating correlations between lags of a variable and the variable itself (ACF) or another variable (CCF) you can NOT simply take corr(xt-k, yt) for the correlation of x at lag k and y and repeat for all the lags of x.\n\nThe correlation formula/function requires the mean of the series, you would be using a different mean for xk for each calculation of the correlation of each pair. The assumption is that the series are (second-order?) stationary and therefore have a constant mean (i.e. each series has 1 mean (and variance)), so the mean(s) of (each) original series should be used in the calculations of the correlations.\nSource: Modern Applied Statistics with S, Venables and Ripley\nCompared one against the other in COVID-19 CFR project and they actually produce similar patterns but different values of the CCF. The simple correlation method produced inflated CCF values.\n\n\nTerms\n\nInput and Output Series - In a cross-correlation in which the direction of influence between two time-series is hypothesized or known,\n\nThe influential time-series is called the “input” time-series\nThe affected time-series is called the “output” time-series.\n\nLead - When one or more \\(x_{t+h}\\) , with \\(h\\) negative, are predictors of \\(y_t\\), it is sometimes said that x leads y.\nLag - When one or more \\(x_{t+h}\\), with \\(h\\) positive, are predictors of \\(y_t\\), it is sometimes said that x lags y.\n\nPrewhitening\n\nIf either series contain autocorrelation, or the two series share common trends, it is difficult for the CCF to identify meaningful relationships between the two time series. Pre-whitening solves this problem by removing the autocorrelation and trends.\nHelps to avoid spurious correlations\n\nMake sure there’s a theoretical reason for the two series to be related\nAutocorrelation of at least one series should be removed (transformed to white noise)\n\nWith autocorrelation present:\n\nThe variance of cross-correlation coefficient is high and therefore spurious correlations are likely\nSignificance calculations are no longer valid since the CCF distribution will not be normal and the variance is no longer 1/n\n\n\nA problem even with stationary series (even more so with non-stationary series)\nWe expect about 1.75 false alarms out of the 35 sample cross-correlations even after prewhitening\n\n\nSteps\n\nTest for stationarity\nFind number of differences to make the series stationary\n\nNot sure if each series should be differenced the same number of times\n\nWhy would you test a seasonal series and non-seasonal series for an association?\nThe requirement is stationarity, so maybe try using the highest difference needed between both series\n\nIn dynamic regression, Hyndman says difference all variables if one needs differencing, so proabably applicable here.\n\n\nSome examples also used log transformation , but when I did, it produced nonsensical CCF values. (covid cfr project). So, beware.\n\nSeasonal Difference before differencing for trend\n(Optional) Create lag scatter plots of the differenced series and look for patterns to get an idea of the strength of the linear correlation (See EDA, Time Series &gt;&gt; Association)\n\nIf there’s a nonlinear pattern, might be difficult or might not to use a nonlinear or nonparametric correlation function. See 3rd bullet under CCF header above for discussion of stationarity assumption. May not be necessary with another correlation type.\n\nPrewhiten both series (cryer, chan method)\nApply correlation function\n\nExample COVID-19 CFR project\n# Number of differences\nind_cases_diff &lt;- forecast::ndiffs(ind_cases_ts)\nind_deaths_diff &lt;- forecast::ndiffs(ind_deaths_ts)\n# Number of seasonal differences\nind_cases_sdiff &lt;- forecast::nsdiffs(ind_cases_ts)\nind_deaths_sdiff &lt;- forecast::nsdiffs(ind_deaths_ts)\n# Only detrending differences; No seasonal diffs needed\nind_cases_proc &lt;- diff(ind_cases_ts, ind_cases_diff)\nind_deaths_proc &lt;- diff(ind_deaths_ts, ind_deaths_diff)\n\n# Fit AR model with processed input series\nind_cases_ar &lt;- ind_cases_proc %&gt;%\n  ts_tsibble() %&gt;%\n  model(AR(value ~ order(p = 1:30), ic = \"aicc\"))\n# Pull AR coefs\nind_ar &lt;- coef(ind_cases_ar) %&gt;% \n  filter(stringr::str_detect(term, \"ar\")) %&gt;% \n  pull(estimate)\n# Linearly filter both input and output series using coefs\nind_cases_fil &lt;- \n  stats::filter(ind_cases_proc, \n                filter = c(1, -ind_ar),\n                method = 'convolution', \n                sides = 1)\nind_deaths_fil &lt;- \n  stats::filter(ind_deaths_proc, \n                filter = c(1, -ind_ar),\n                method = 'convolution', \n                sides = 1)\n# spike at -20 with corr = 0.26; nonsensical lags at -4 and -15, -68\nggCcf(ind_cases_fil, ind_deaths_fil)",
    "crumbs": [
      "Association",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/association-time-series.html#sec-assoc-ts-dist",
    "href": "qmd/association-time-series.html#sec-assoc-ts-dist",
    "title": "Time Series",
    "section": "Distances",
    "text": "Distances\n\nThe smaller the total distance between two series, the greater the association.\nDistance algorithms in the {dtw} and {dtwclust} packages\nOther packages\n\n{distantia} - Qantifies dissimilarity between multivariate ecological time-series (METS)\n\nAlso see Clustering, Time Series &gt;&gt; {dtwclust}\n\n\nDynamic Time Warping Distance (DTW)\n\ndtw_basic , dtw2\nThe distance between 2 series is measured by “warping” the time axis to bring them as close as possible to each other and measuring the sum of the distances between the points.\n\n\nFigure shows alignment of two series, x and y.\n\nThe initial and final points of the series must match, but other points along the axis may be “warped” in order to minimize the “cost” (i.e. weighted distance).\nThe dashed blue lines are the warp curves and show how points are mapped to each other.\n\n\nHyperparameters:\n\nStep Pattern - Determines how the algorithm traverses the rows of the LCM to find the optimal path (See Step Patterns Section)\nWindow Range - Limits the number lcm calculations for each point (See Window Constraint Section)\n\nSteps\n\nCalculate LCM matrix for series X and Y where X is the Query or Test Series and Y is the Reference Series. The LCM is a distance matrix where each element is the distance between the ith point of series 1 and the jth point of series 2. (See Local Cost Matrix (LCM) for details)\nSimultaneously move along each row of the LCM using a chosen step pattern within a chosen window (See Step Patterns and Window Constraints sections to get a visual of this process).\n\nThe minimum lcm for each point along x-axis is found. The sequence of minimum lcms or minimum alignment is \\(\\phi\\) .\nCalculate the cost, DTWp, using the LCMs in the minimum alignment\n\\[\n\\text{DTW}_p(x, y) = \\left(\\sum\\frac{m_\\phi \\text{lcm}(k)^p}{M_\\phi}\\right)^{1/p}\n\\]\n\n\\(m_\\phi\\): Per-step weighting coefficient (edge weight in patterns fig)\n\\(M_\\phi\\): Normalization constant\n\\(k\\): Pair of points (or position along the x-axis) in the minimum alignment\n\ndtw_basic sets \\(p = 2\\)\n\nThe dtw function in the dtw package doesn’t use p in this equation\n\n\nChoose the alignment with the lowest cost, \\(\\text{DTW}_p\\) (i.e. sum of lcm distances for that alignment)\n\n\n\nLocal Cost Matrix (LCM)\n\nComputed for each pair of series that are compared\nThe \\(L_p\\) norm (i.e. distance) between the query series and reference series\n\\[\n\\text{lcm}(i, j) = \\left(\\sum_v | x_i^v - y_j^v|^p\\right)^{1/p}\n\\]\n\n\\(x_i\\) and \\(y_j\\) are elements of the test and reference time series respectively\n\\(v\\) stands for “variable” which is for comparing multivariate series\n\ni.e. the \\(L_p\\) norm for each pair of points is summed over all variables in the multivariate series.\n\n\\(p\\) is the order of the norm used\n\ne.g. 1 is Manhattan distance; 2 is Euclidean\nChoice of \\(p\\) only matters if multivariate series are being used\n\n\nEach \\(\\text{lcm}(i , j)\\) value is an element in the \\(n \\times m\\) matrix, \\(\\text{LCM}\\) where \\(1 \\lt i \\lt n\\) and \\(1 \\lt j \\lt m\\)\n\n\n\nStep Patterns\n\nDetermines how algorithm moves across the rows of the LCM to create alignments (warpings of the time axis)\nEach pattern is a set of rules and weights\n\nThe rules are used to create different alignments of the LCM (i.e warping of the time axis)\nThe edge weights, \\(m_\\phi\\), are used in the DTW calculation\n\nPatterns\n\n\nPatterns in fig\n\nTop Row: symmetric1, symmetric2\nBottom Row: asymmetric, rabinerJuangStepPattern(4, “c”, TRUE)\n\ni.e. Rabiner-Juang’s type IV with slope weighting\n\n\nOnly some of the patterns are normalizable (i.e. \\(M_\\phi\\) is used in the DTW equation) (normalize argument)\n\nNormalization may be important when\n\nComparing alignments between time series of different lengths, to decide the best match (e.g., for classification)\nWhen performing partial matches (?)\n\nFor dtw_basic, doc says only supported with symmetric2\nrabinerJuangStepPattern with slope weighting types c and d are normalizable\nsymmetricP* (where * is a number) are all normalizable (not shown in fig)\n\nSymmetric (i.e. dist from A to B equals the distance from B to A) only if:\n\nEither symmetric1 or symmetric2 step patterns are used\nSeries are equal length after any constraints are used\nSome Cluster Validity Indicies (CVI) (See Clustering, Time Series &gt;&gt; Cluster Validity Indicies) require symmetric distance functions\n\n{dtwclust} author says symmetric1 most commonly used. dtw::dtw_basic uses symmetric2 by default.\n\n\n\n\nWindow Constraints\n\nLimits the region that the lcm calculation takes place.\n\nReduces computation time but makes sense that you don’t want to compare points that are separated by to large a time interval\n\nSakoe-Chiba window creates a calculation region along the diagonal (yellow blocks) of the LCM\n\n\n1 set of lcm calculations occurs within the horizontal, rectangular block of the query series and the vertical, rectangular block of the reference series.\nTherefore, pairs are chosen in relatively small neighbors around the ith point of series 1 and the ith point of series 2, and the window size is size of the neighborhood.\nSakoe-Chiba requires equal length series, but a “slanted band” is equivalent and works for unequal length series.\n\n“Slanted Band” is what’s used by dtwclust when the window constraint is used.\n\n\nOptimal window size needs to be tuned\n\nCan marginally speed up the DTW calculation, but they are mainly used to avoid pathological warping\n\\(w\\), the window size, is around half the size of the actual region covered\n\n\\([(i, j - w), (i, j + w)]\\) which has \\(2w + 1\\) elements\n\nA common w is 10% of the sample size, smaller sizes sometimes produce better results\n\n\n\n\n\nLower Bounds (LB)\n\ndtw_lb\nUses a lower bound the dtw distance to speed up computation\n\nA considerably large dataset would be needed before the overhead of DTW becomes much larger than that of dtw_lb’s iterations\nMay only be useful if one is only interested in nearest neighbors, which is usually the case in partitional clustering\n\nSteps\n\nCalculates an initial estimate of a distance matrix between two sets of time series using lb_improved\n\nInvolves the “lower bound” calculation (Didn’t get into it)\n\nUses the estimate to calculate the corresponding true DTW distance between only the nearest neighbors (row-wise minima of dist.matrix) of each series in \\(x\\) found in $y\nUpdates distance matrix with DTW values\nContinues iteratively until no changes in the nearest neighbors occur\n\nOnly if dataset is very large will this method will be faster than dtw_basic in the calculation of DTW\nNot symmetric, no multivariate series\nRequires\n\nBoth series to be equal length\nWindow constraint defined\nNorm defined\n\nValue of LB (tightness of envelope around series) affected by step pattern which is set in dtw_basic and included via … in dtw_lb\n\nSize of envelopes in general: LB_Keoghp &lt; LB_Improvedp &lt; DTWp\n\n\n\n\nSoft DTW\n\nsdtw\n“Regularizes DTW by smoothing it” (¯\\_(ツ)_/¯)\n\n“smoothness” controlled by gamma\n\nDefault: 0.01\nWith lower values resulting in less smoothing\n\n\nUses a gradient to efficiently calculate cluster prototypes\nNot recommended for stand-alone distance calculations\n\nNegative values can happen\n\nSymmetric and handles series of different lengths and multivariate series\n\n\n\nShape-based Distance (SBD)\n\\[\n\\text{SBD} (x,y) = 1-\\frac{\\max{\\text{NCC}_c (x,y)}}{\\lVert x \\rVert_2 \\lVert y \\rVert_2}\n\\]\n\nSBD\nUsed in k-Shape Clustering\n\nBased on the cross-correlation with coefficient normalization (NCCc) sequence between two series\n\nFast (uses FFT to calc), competitive with other distance algorithms, and supports series with different lengths\nSymmetric, no multivariate series\nIn preprocessing argument, set to z-normalization\n\n\n\nTriangular Global Alignment Kernel Distance\n\\[\n\\begin{aligned}\n&\\text{TGAK}(x,y,\\sigma,T) = \\frac{\\omega(i,j)\\kappa(x,y)}{2-\\omega(i,j)\\kappa(x,y)} \\\\\n&\\begin{aligned}\n\\text{where}\\quad &\\omega(i,j) = \\left(1-\\frac{|i-j|}{T}\\right)_+ \\\\\n&\\kappa(x,y)= e^{-\\phi_\\sigma(x,y)} \\\\\n&\\phi_\\sigma(x,y) = \\frac{1}{2\\sigma^2}\\lVert x-y \\rVert^2 + \\log \\left(2-e^{-\\frac{\\lVert x-y \\rVert^2}{2\\sigma^2}}\\right)\n\\end{aligned}\n\\end{aligned}\n\\]\n\nGAK\n“Regularizes DTW by smoothing it” (¯\\_(ツ)_/¯)\nSymmetric when normalized (dist a to b = dist b to a)\nSupports multivariate series and series of different length (as long as one series isn’t half the length of the other)\nSlightly more computationally expensive than DTW (… that equation🥴)\n\n\\(\\sigma\\) can defined by the user but if left as NULL, the function estimates it\n\\(T\\) is the triangular constraint and is similar to the window constraint in DTW but there no argument for it, so I guess it’s taken care of\nNo idea what \\(i\\) and \\(j\\) refer to since x and y are also used.\n\nWould have to look it up in the original paper or there is a separate website and package for it\n\n\nIf normalize = TRUE, then a distance is returned, can be compared with the other distance measure, and used in clustering\n\nIf FALSE, a similarity is returned",
    "crumbs": [
      "Association",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html",
    "href": "qmd/bayes-priors.html",
    "title": "Priors",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-misc",
    "href": "qmd/bayes-priors.html#sec-priors-misc",
    "title": "Priors",
    "section": "",
    "text": "Resources\n\nStan/brms prior distributions (mc-stan/function-ref)\n\nDistributions are towards the bottom of the guide\n{brms} should have all the distributions available in Stan according to their docs (see Details section)\n\n\nUsing meta-analyis or previous studies to create informed priors\n\n“Systematic use of informed studies leads to more precise, but more biased estimates (due to non-linear information flow in the literature). Critical comparison of informed and skeptical priors can provide more nuanced and solid understanding of our findings.” (thread + paper)\n\ni.e. try both and compare the result\n\n\nPrior sensitivity analysis\n\n{priorsense}\n\nVideo, Thread\n\n{BayesSenMC}\n\nFor binary exposure and a dichotomous outcome\nGenerates different posterior distributions of adjusted odds ratio under different priors of sensitivity and specificity, and plots the models for comparison. It also provides estimations for the specifications of the models using diagnostics of exposure status with a non-linear mixed effects model.\nVignette\n\n\nStatistical Rethinking\n\nThe “flatness” of a Normal prior is controlled by the size of the s.d. value\n\nnot in logistic regression (see examples below)\n\nFlat priors result in poor frequency properties (i.e. consistently give bad inferences) in realistic settings where studies are noisy and effect sizes are small. (Gelman post)\nWeakly informative priors: they allow some implausibly strong relationships but generally bound the lines to possible ranges of the variables. (fig 5.3, pg 131)\nWe want our priors to be skeptical of large differences [in treatment effects], so that we reduce overfitting. Good priors hurt fit to sample but are expected to improve prediction. (pg 337)\nWe don’t formulate priors based on the sample data. We want the prior predictive distribution to live in the plausible outcome space, not fit the sample.\nFor logistic regression and poisson regression, a flat prior in the logit space is not a flat prior in the outcome probability space (pg 336)\nAs long as the priors are vague, minimizing the sum of squared deviations to the regression line is equivalent to finding the posterior mean. pg 200\n“As always in rescaling variables, the goals are to create focal points that you might have prior information about, prior to seeing the actual data values. That way we can assign priors that are not obviously crazy, and in thinking about those priors, we might realize that the model makes no sense. But this is only possible if we think about the relationship between measurements and parameters, and the exercise of rescaling and assigning sensible priors helps us along that path. Even when there are enough data that choice of priors is not crucial, this thought exercise is useful.” pg 258\nComparing the posteriors with the priors",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-preproc",
    "href": "qmd/bayes-priors.html#sec-priors-preproc",
    "title": "Priors",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nCentering the predictor\n\nMakes the posterior easier to sample\nReduces covariance among the parameter posterior distributions\nMakes it easier to define the prior on average temperature in the center of the time range (instead defining prior for temperature at year 0).\nLinks to Gelman posts about centering your predictors (article)\n\nIf you standardize your predictors, you can use a mean of 0 for the prior on your intercept\n\nWith flat priors, this doesn’t make much of difference",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-getprecs",
    "href": "qmd/bayes-priors.html#sec-priors-getprecs",
    "title": "Priors",
    "section": "Get Prior Recommendations",
    "text": "Get Prior Recommendations\n\nExample: Fitting a spline\n# get recommended prior specifications\n# s is the basis function brms imports from mgcv pkg\nbrms::get_prior(data = d2, \n                family = gaussian, \n                doy ~ 1 + s(year))\n##                  prior    class    coef group resp dpar nlpar bound      source \n##                  (flat)        b                                          default \n##                  (flat)        b syear_1                            (vectorized) \n##  student_t(3, 105, 5.9) Intercept                                          default \n##    student_t(3, 0, 5.9)      sds                                          default \n##    student_t(3, 0, 5.9)      sds s(year)                            (vectorized) \n##    student_t(3, 0, 5.9)    sigma                                          default\n\n# applying the recommendations\n# multi-level method for spline fitting\nb4.11 &lt;- brm(data = d2, \n            family = gaussian, \n            # k = 19, corresponds to 17 basis functions I guess ::shrugs:: \n            # The default for s() is to use what’s called a thin plate regression spline \n            # bs uses a basis spline \n            temp ~ 1 + s(year, bs = \"bs\", k = 19), \n            prior = c(prior(normal(100, 10), class = Intercept), \n                      prior(normal(0, 10), class = b), \n                      prior(student_t(3, 0, 5.9), class = sds), \n                      prior(exponential(1), class = sigma)), \n            iter = 2000, warmup = 1000, chains = 4, cores = 4, \n            seed = 4, \n            control = list(adapt_delta = .99))\nExample: Multinomial Logistic Regression\n# Outcome categorical variable has k = 3 levels. We fit k-1 models. Hence the 2 intercept priors\n# intercept model\nget_prior(data = d, \n          # refcat sets the reference category to the 3rd level\n          family = categorical(link = logit, refcat = 3),\n          # just an intercept model\n          career ~ 1)\n##                prior    class coef group resp dpar nlpar bound  source\n##                (flat) Intercept                                  default\n##  student_t(3, 3, 2.5) Intercept                  mu1            default\n##  student_t(3, 3, 2.5) Intercept                  mu2            default\n\nb11.13io &lt;-\n  brm(data = d, \n      family = categorical(link = logit, refcat = 3),\n      career ~ 1,\n      prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1),\n                prior(normal(0, 1), class = Intercept, dpar = mu2)),\n      iter = 2000, warmup = 1000, cores = 4, chains = 4,\n      seed = 11,\n      file = \"fits/b11.13io\")\n\nAs of brms 2.12.0, “specifying global priors for regression coefficients in categorical models is deprecated.” Meaning — if we want to use the same prior for both, we need to use the dpar argument for each",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-finterp",
    "href": "qmd/bayes-priors.html#sec-priors-finterp",
    "title": "Priors",
    "section": "Formulating an Intercept Prior",
    "text": "Formulating an Intercept Prior\n\nExample SR 6.3.1 pgs 182-83\n\nA thought process on how to set a predictor prior based on its relationship to the outcome and an intercept prior.\n\nExample SR 7.1.1 pg 200\n\nOutcome variable was scaled, outcome/max(outcome)\n\nValues now between 0 and 1\nUseful for when 0 is a meaningful boundary\n\nNow able to center the intercept prior on mean of outcome, α ∼ Normal(0.5, 1)\n\nSays that the average species with an average body mass (predictor variable) has a brain volume (outcome variable) with an 89% credible interval (± 1.5 sd) from about −1 to 2.\n\nBody mass was centered, so it’s at its average is when its value is zero.\n\n\n\nExample SR 8.3.2 pg 259\n\nSimilar to 7.1.1 example except there’s the observation that a sd = 1 for the intercept prior is too large given that the outcome is bdd between 0 and 1 (after scaling)\na &lt;- rnorm( 1e4 , 0.5 , 1 )\nsum( a &lt; 0 | a &gt; 1 ) / length( a )\n[1] 0.6126\n\n61% of the prior is outside the bounds for the outcome which makes no sense\n\nIf it’s 0.5 units from the mean to zero, then a standard deviation of 0.25 should put only 5% of the mass outside the valid range.\na &lt;- rnorm( 1e4 , 0.5 , 0.25 )\nsum( a &lt; 0 | a &gt; 1 ) / length( a )\n[1] 0.0486\n\nNot sure why you want 5% outside the valid range of the outcome variable\n\n\nExample (Ch 11 pg 335-6)\n\nWith logistic regression, flat Normal priors aren’t priors with a high sd.\n\n\nThe Normal prior on the logit scale with the large sd says that the probabilty of an event is either 0 or 1 which usually isn’t reasonable.\n\nlogit(pi) = α\n\nα ~ Normal(0, 1.5) — the curve for the probability of an event is very flat, looks like a mesa\nα ~ Normal(0, 1.0) —the curve for the probability of an event is a fat hill shape. A little more skeptical of extreme probabilities\n\n\nExample\n\nAlso have ggplot code in Documents &gt;&gt; R &gt;&gt; Code &gt;&gt; Simulations &gt;&gt; sim-prior-predictive-distr.R\nPoisson regression (pg 356)\nIn poisson regression, flat normal priors aren’t priors with high s.d.\n\n# prior predictive distribution\ncurve( dlnorm( x , 3 , 0.5 ) , from=0 , to=100 , n=200 )\n\nSince poisson regression uses a log link, the outcome is log-normal. We’re simulating the effect of a normal prior on a log-normal outcome which is why the simulation code uses dlnorm.\n“number of tools” is the outcome variable\nThe prior with s.d. 10 has almost all the probability density at zero and huge mean\na &lt;- rnorm(1e4,0,10)\nlambda &lt;- exp(a)\nmean(lambda)\n[1] 9.622994e+12\nThis usually doesn’t make sense for a prior\nThe prior with s.d. 0.5 has a mean around 20 and a more spread out probability density which makes much more sense given the literature on the subject.",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-fslopp",
    "href": "qmd/bayes-priors.html#sec-priors-fslopp",
    "title": "Priors",
    "section": "Formulating a Slope Prior",
    "text": "Formulating a Slope Prior\n\nExample SR pg 259\n\nslopes centered on zero, imply no prior information about direction\nHow big could can the slopes be in theory?\n\nAfter centering, range of each predictor is 2—from −1 to 1 is 2 units.\nTo take us from the theoretical minimum of outcome variable = 0 on one end to the observed maximum of 1—a range of 1 unit—on the other would require a slope of 0.5 from either predictor variable—0.5 × 2 = 1.\n\nAssign a standard deviation of 0.25, then 95% of the prior slopes are from −0.5 to 0.5, so either predictor could in principle account for the entire range, but it would be unlikely\n\nExample SR pg 336-7\n\nWith logistic regression, flat Normal priors aren’t priors with a high sd.\n\n\nShows difference between two levels of the treatment effect (i.e. 2 different treatments) on the 0/1 outcome\nThe prior with large sd has all the probability density massed at 0 and 1\n\nSays that the 2 treatments are completely alike or completely different\n\nThe prior with the small sd (e.g. Normal(0, 0.5) is concentrated from about 0 to about 0.4\n\nAlthough 0 difference in treatments has the highest probability, the mean is at a difference around 0.10\n\nPrior says that large differences between treatments are very unlikely, but if the data contains strong evidence of large differences, they will shine through\n\nPairs nicely with an intercept prior, α ~ Normal(0, 1.5)\nAn example of a weakly informative prior that reduces overfitting the sample data\n\n\n\nExample: pg 357\n\nset.seed(11)\n## TOP ROW\n\n# how many lines would you like?\nn &lt;- 100\n# simulate and wrangle\ntibble(i = 1:n,\n       a = rnorm(n, mean = 3, sd = 0.5)) %&gt;%\n  mutate(`beta%~%Normal(0*', '*10)` = rnorm(n, mean = 0 , sd = 10),\n         `beta%~%Normal(0*', '*0.2)` = rnorm(n, mean = 0 , sd = 0.2)) %&gt;%\n  pivot_longer(contains(\"beta\"),\n               values_to = \"b\",\n               names_to = \"prior\") %&gt;%\n  expand(nesting(i, a, b, prior),\n         x = seq(from = -2, to = 2, length.out = 100)) %&gt;%\n\n  # plot\n  ggplot(aes(x = x, y = exp(a + b * x), group = i)) +\n  geom_line(size = 1/4, alpha = 2/3,\n            color = wes_palette(\"Moonrise2\")[4]) +\n  labs(x = \"log population (std)\",\n       y = \"total tools\") +\n  coord_cartesian(ylim = c(0, 100)) +\n  facet_wrap(~ prior, labeller = label_parsed)\n\n## BOTTOM ROW\nprior &lt;-\n  tibble(i = 1:n,\n         a = rnorm(n, mean = 3, sd = 0.5),\n         b = rnorm(n, mean = 0, sd = 0.2)) %&gt;% \n  expand(nesting(i, a, b),\n         x = seq(from = log(100), to = log(200000), length.out = 100))\n# left\np1 &lt;-\n  prior %&gt;%\n  ggplot(aes(x = x, y = exp(a + b * x), group = i)) +\n  geom_line(size = 1/4, alpha = 2/3,\n            color = wes_palette(\"Moonrise2\")[4]) +\n  labs(subtitle = expression(beta%~%Normal(0*', '*0.2)),\n       x = \"log population\",\n       y = \"total tools\") +\n  coord_cartesian(xlim = c(log(100), log(200000)),\n                  ylim = c(0, 500))\n# right\np2 &lt;-\n  prior %&gt;%\n  ggplot(aes(x = exp(x), y = exp(a + b * x), group = i)) +\n  geom_line(size = 1/4, alpha = 2/3,\n            color = wes_palette(\"Moonrise2\")[4]) +\n  labs(subtitle = expression(beta%~%Normal(0*', '*0.2)),\n       x = \"population\",\n       y = \"total tools\") +\n  coord_cartesian(xlim = c(100, 200000),\n                  ylim = c(0, 500))\n# combine\np1 | p2\n\nWith poisson regression, flat Normal priors aren’t priors with a high sd.\nOutcome: total_tools, predictor: log_population\nBottom row fig titles have a typo. Should be a ~ dnorm(3, 0.5) since it’s the Intercept prior\nVariables have been standardized; total_tools simulated with intercept + predictor priors. So the y axis is simulating the potential fitted values.\nTop Left (0 is mean of log_population):\n\nlarge sd: mostly results in explosive growth of tools just after mean of log_population or explosive decline just before mean log_population (unlikely)\n\nTop Right (0 is mean of log_population)\n\nsmall sd (flatter): most results are around the mean of the intercept prior results (see above) but still allows for more extreme estimates. (reasonable)\n\nBottom Left\n\n100 trend lines between total tools and un-standardized log population\n\nViewing prior predictive trends with un-standardized variables is more natural to see what’s happening\n\n100 total tools is probably the most we expect to ever see in these data\n\nLooks like 80-85% of the trend lines are under 100. still keeps some explosive possibilities.\n\n\nBottom Right\n\n100 trend lines between total tools and un-standardized, un-logged population\n\nViewing prior predictive trends with un-standardized, un-transformed variables is even more natural to see what’s happening\n\nWhen a predictor variable is logged in a regression with a log-link (i.e. log-log), this means we are assuming diminishing returns for the raw predictor variable.\n\nEach additional person contributes a smaller increase in the expected number of tools\nDiminishing returns as a predictor value continues to increase makes sense in many situations which is why logging predictors is a popular transformation\n\n\nThoughts\n\nBottom-right seems like the right way to visualize the prior to think about the association between the outcome and predictor\nTop row and bottom-left seem to give a better sense of how many explosive possibilities and their patterns that your allowing for with different transformations",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-fsigp",
    "href": "qmd/bayes-priors.html#sec-priors-fsigp",
    "title": "Priors",
    "section": "Formulating a Sigma Prior",
    "text": "Formulating a Sigma Prior\n\nCommon to start with exponential(1)\nTightening the spread of the Exponential distribution by using a Gamma distribution (Thread)\n\n\nYou can keep “mean = 1” (aka exponential(1) and adjust the “sd”.\n\nSee Distributions &gt;&gt; Gamma for details on the process\n\nAlso allows you to move most of the mass of the prior a littler further away from 0.\nAnother alternative is the Weibull distribution",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-eapfam",
    "href": "qmd/bayes-priors.html#sec-priors-eapfam",
    "title": "Priors",
    "section": "Extracting a Prior From a Model",
    "text": "Extracting a Prior From a Model\n\nExample: Logistic Regression (SR sect 11.1.1 pg 336)\n\nIntercept\n\n# prior_samples and inv_logit_scaled are brms functions\n# theme is from ggthemes\nprior_samples(b11.1) %&gt;%\n  mutate(p = inv_logit_scaled(Intercept)) %&gt;%\n\n  ggplot(aes(x = p)) +\n  geom_density(fill = wes_palette(\"Moonrise2\")[4],\n               size = 0, adjust = 0.1) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"prior prob pull left\")",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-simppd",
    "href": "qmd/bayes-priors.html#sec-priors-simppd",
    "title": "Priors",
    "section": "Simulating a Prior Predictive Distribution",
    "text": "Simulating a Prior Predictive Distribution\n\nExample: SR pg 176\n# log-normal prior\nsim_p &lt;- rlnorm( 1e4 , 0 , 0.25 )\n\n# \"this prior expects anything from 40% shrinkage up to 50% growth\"\nrethinking::precis( data.frame(sim_p) )\n# 'data.frame': 10000 obs. of 1 variables:\n#       mean    sd  5.5% 94.5% histogram\n# sim_p 1.03  0.26  0.67  1.48 ▁▃▇▇▃▁▁▁▁▁▁\n\n# tidy-way\nsim_p &lt;-\n  tibble(sim_p = rlnorm(1e4, meanlog = 0, sdlog = 0.25))sim_p %&gt;%\n    mutate(`exp(sim_p)` = exp(sim_p)) %&gt;%\n    gather() %&gt;%\n    group_by(key) %&gt;%\n    tidybayes::mean_qi(.width = .89) %&gt;%\n    mutate_if(is.double, round, digits = 2)\n\n## # A tibble: 2 x 7\n##  key         value .lower .upper .width .point .interval\n##  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;chr&gt;     &lt;chr&gt;\n## 1 exp(sim_p)  2.92   1.96   4.49   0.89   mean        qi\n## 2 sim_p       1.03   0.67    1.5   0.89   mean        qi\n\nVisualize with ggplot\n\n# wrangle\nsim_p %&gt;%\n  mutate(`exp(sim_p)` = exp(sim_p)) %&gt;%\n  gather() %&gt;%\n  # plot\n  ggplot(aes(x = value)) +\n  geom_density(fill = \"steelblue\") +\n  scale_x_continuous(breaks = c(0, .5, 1, 1.5, 2, 3, 5)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(0, 6)) +\n  theme(panel.grid.minor.x = element_blank()) +\n  facet_wrap(~key, scale = \"free_y\", ncol = 1)\n\nExample:\n\nPossible Intercept Priors\n\ngrid &lt;- seq(-3, 3, \n             length.out = 1000) # evenly spaced values from -3 to 3\nb0_prior &lt;- \n   map_dfr(.x = c(0.5, 1, 2), # .x represents the three sigmas \n           ~ data.frame(grid = grid,\n                        b0 = dnorm(grid, mean = 0, sd = .x)),\n                        .id = \"sigma_id\")\n# Create Friendlier Labels\nb0_prior &lt;- b0_prior %&gt;%\n  mutate(sigma_id = factor(sigma_id, \n         labels = c(\"normal(0, 0.5)\",\n                    \"normal(0, 1)\",\n                    \"normal(0, 2)\")))\nggplot(b0_prior, aes(x = grid, y = b0)) +\n  geom_area(fill = \"cadetblue4\", color = \"black\", alpha = 0.90) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.85)) +\n  labs(x = NULL,\n       y = \"probability density\",\n       title = latex2exp::TeX(\"Possible $\\\\beta_0$ (intercept) priors\")) +\n       facet_wrap(~sigma_id, nrow = 3)\nSampling of lines from \\(\\beta_0\\) and \\(\\beta_1\\) priors\n\nb0b1 &lt;- \n  map2_df(.x = c(0.5, 1, 2), \n          .y = c(0.25, 0.5, 1), \n          ~ data.frame(\n              b0 = rnorm(100, mean = 0, sd = .x),\n              b1 = rnorm(100, mean = 0, sd = .y)), \n          .id = \"sigma_id\"\n  )\n\n# Create friendlier labels\nb0b1 &lt;- \n  b0b1 %&gt;%\n    mutate(sigma_id = factor(sigma_id, \n                             labels = c(\"b0 ~ normal(0, 0.5); b1 ~ normal(0, 0.25)\",\n                                        \"b0 ~ normal(0, 1); b1 ~ normal(0, 0.50)\",\n                                        \"b0 ~ normal(0, 2); b1 ~ normal(0, 1)\")))\n\nggplot(b0b1) +\n  geom_abline(aes(intercept = b0, slope = b1), color = \"cadetblue4\", alpha = 0.75) +\n  scale_x_continuous(limits = c(-2, 2)) +\n  scale_y_continuous(limits = c(-3, 3)) +\n  labs(x = \"x\",\n       y = \"y\",\n  title = latex2exp::TeX(\"Sampling of lines from $\\\\beta_0$ and $\\\\beta_1$ priors\")) +\n  facet_wrap(~sigma_id, nrow = 3)",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-conjp",
    "href": "qmd/bayes-priors.html#sec-priors-conjp",
    "title": "Priors",
    "section": "Conjugate Priors",
    "text": "Conjugate Priors\n\nIf the posterior distributions p(θ | x) are in the same probability distribution family as the prior probability distribution p(θ), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function p(x | θ)\nBenefits\n\nBayesian updates no longer need to compute the product of the likelihood and prior (only addition is needed).\n\nThis product is computationally expensive and sometimes not feasible.\nOtherwise numerical integration may be necessary\n\nMay give intuition, by more transparently showing how a likelihood function updates a prior distribution.\n\nAll members of the exponential family have conjugate priors.\nList\n&lt;Beta posterior&gt;\nBeta prior * Bernoulli likelihood → Beta posterior\nBeta prior * Binomial likelihood → Beta posterior\nBeta prior * Negative Binomial likelihood → Beta posterior\nBeta prior * Geometric likelihood → Beta posterior\n\n&lt;Gamma posterior&gt;\nGamma prior * Poisson likelihood → Gamma posterior\nGamma prior * Exponential likelihood → Gamma posterior\n\n&lt;Normal posterior&gt; \nNormal prior * Normal likelihood (mean) → Normal posterior\n\n&lt;Others&gt;\nDirichlet prior * Multinomial likelihood → Dirichlet posterior",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-pelic",
    "href": "qmd/bayes-priors.html#sec-priors-pelic",
    "title": "Priors",
    "section": "Prior Elicitation",
    "text": "Prior Elicitation\n\nTranslating expert opinion to a probability density than you can use as a prior\nMisc\n\nNotes from:\n\nVideo: On using expert information in Bayesian statistics (R &gt;&gt; Videos)\nPaper: Methods for Eliciting Informative Prior Distributions (R &gt;&gt; Documents &gt;&gt; Bayes)\n\nPackages\n\n{SHELF} - shiny apps for eliciting for various distributions and via various methods\n\nwebpage also has links to papers\n\n\nPrior sensitivity analysis should done especially if you’re using expert-informed priors.\n\nBest Practice\n\nUse domain experts to set constraints for your priors (e.g. upper and lower limits) instead of formulating a prior\n\nBest to use data, previous studies, etc. to formulate priors\n\nUse domain experts to inform you on relationships between variables\n\nElicitation process is resource intensive for what is probably minimal gain in comparison to data\n\nIt takes a lot of your time and their time to get this right\nExperts may not have the statistical knowledge to understand what you need or how to convey the information\n\nIn this case, you’ll need to school them on basic statistical concepts\n\n\nWhen to use expert information to formulate a prior\n\nIf your DAG specifies a model that requires more data than you have.\n\nWhile it might be necessary to augment your data, be aware that expert knowledge has been shown to much less useful in complex models rather than simpler models.\n\nIf the your data is really noisy\nIf experts know something that isn’t represented by your data or can’t be captured by the data\nIf domain expertise is required given your research question\n\nInterviews with experts\n\nQuality Control: If your subject matter allows, try to create “calibrating” questions to weed-out the experts that aren’t really experts\n\nShould be questions that you are certain of the answer and are things any expert should know\n\nThis can be difficult for some subject matter.\n\nMaybe consult with an expert that you’re confident is an expert to help come up with some questions.\n\nQuestions should be standardized, so you know that the results from each expert are consistent.\nFace-to-face elicitation produces greater quality results, because the facilitator can clarify the questions from the experts if needed.\nTry to keep experts from biasing the information they give you\n\nDon’t use experts that have seen the results of your model\n\nIf they’ve seen your raw data that’s okay. (I dunno about this, even if they’ve seen eda plots?)\n\nDon’t provide them with any estimates you may have from previous studies or other experts\nDon’t let them fixate on outlier scenarios they may have encountered\n\nRecord conversations with video and/or audio\n\nIf problems surface when evaluating the expert’s information, these can be useful to go back over the information collection process\n\nWas there a misunderstanding between you and the expert on what information you wanted\nWas the information biased? (see above)\nIf using mulitiple experts, maybe subgroups have different viewpoints/experiences which is causing a divergence in opinion (e.g nurses vs psychologists treating PTSD)\n\n\nIf problems surface when evaluating the expert’s information, it can be useful to gather specific experts that differ and have them discuss why they hold their substantially differing opinions. Afterwards, they may adjust their opinions and you’ll have a greater consensus.\nProcess\n\nElicit location parameter (e.g. via Trial Roulette) from the expert\n\nTrial Roulette (see paper in Misc for details)\n\nRequires the expert to have sufficient statistical knowledge to be able to place the blocks to form an appropriate distribution\nUser should be aware that distribution output may be inappropriate for sample data\n\nExample: Algorithm may output a Gamma distribution which is inappropriate for percentage data since the upper bound can be greater than one\n\nParameter space is split into subsections (e.g. quantiles)\nUser assigns blocks to each subsection\nExample From MATCH website which was an earlier implementation of {SHELF}\n\nTop chart is a histogram where each cell is a “block” (called “chips” at the bottom). The right panel shows the options for setting the axis ranges and number of bins\nBottom chart evidently estimates distribution parameters from the histogram in the top chart which are your prior’s parameters\n\n\nWith experts with less statistical training, it may be better for you to give them scenarios (e.g. combinations of quantiles of the predictor variables) and have them predict the outcome variable.\n\nCompute the statistics given their answers. Show them the results. Ask them to give an uncertainty range around that statistic.\nExample: From their predictions, you calculate the mean. Then you present them with this average and ask them about their uncertainty?\n\ni.e. What is the range around this value they expect the average to be in?\n\n\nAlso try combinations of methods\nSee paper in Misc for other options\n\nFeedback session\n\nExplain to the expert how you’re interpreting their information. Do they agree with your interpretation? Refine information based on their feedback.\n\nElicit scale and shape parameters (upper and lower bounds)\nFeedback session\nEvaluate distribution\n\n\nEvaluating Expert Distribution(s)\n\nMisc\n\nMight be better to use another measure instead of K-L divergence (see Inspect the distributions visually section below)\n\ne.g Jensen-Shannon Divergence, Population Stability Index (see Production, ML Monitoring for details)\n\n\nCalculate K-L divergence between the expert distribution and the computed posterior using the expert distribution as a prior\n\nSmaller K-L divergence means the 2 distributions are more similar\nLarger K-L divergence means the 2 distributions are more different\n\nCreate a benchmark distribution\n\nShould be a low information distribution as compared to the sample data distribution\ne.g. uniform distribution\n\nCalculate K-L divergence between the benchmark distribution and the computed posterior using the benchmark distribution as the prior\nCalculate ratio of K-L divergences (expert K-L/benchmark K-L)\n\nGreater than 1 is bad. Indicates a “prior data conflict” and it may be better to drop this expert’s distribution\nLess than 1 is good. Potentially an informative prior\n\nInspect the distributions visually (Expert prior distributions and computed posterior from benchmark prior)\n\nK-L divergence penalyzes more certain distributions (i.e. skinny, tall) than less certain distribtutions (fatter, shorter) even if they have the same mean/median and mostly the same information\n\nSo, an expert that is more certain may have a disqualifying ratio of K-L difference while a less certain expert with a very similar distribution has a qualifying ratio.\n\nAfter inspecting the distributions, you may determine that distributions really are too different and the expert is far too certain to keep.\n\n\n\n\nAggregate distributions if you’re eliciting from multiple experts\n\nAverage the distributions (i.e. equal weights for all experts)\nRank experts (e.g. by K-L ratio), weight them, then calculate a weighted average distribution\nUse aggregated distribution(s) as your prior(s)",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/classification.html",
    "href": "qmd/classification.html",
    "title": "Classification",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/classification.html#sec-class-misc",
    "href": "qmd/classification.html#sec-class-misc",
    "title": "Classification",
    "section": "",
    "text": "Also see Regression, Logistic\nGuide for suitable baseline models: link\nIf you have mislabelled target classes, try AdaSampling to correct the labels (article)\nSample size requirements\n\nLogistic Regression: (Harrell, link)\n\nThese are conservative estimates. Sample size estimates assume an event probability of 0.50.\nFor just estimating the intercept and a margin of error for predicted probabilities of 0.1\n\nWith no covariates (i.e. population is homogeneous), n = 96\nWith 1 categorical covariate, n = 96 for each level of the covariate\n\ne.g. For gender, you need 96 males and 96 females\n\n\nFor just estimating the intercept and a margin of error for predicted probabilities of 0.05\n\nWith no covariates (i.e. population is homogeneous), n = 384\nIf true probabilities of event (and non-event) are known to be extreme, i.e. \\(p \\notin [0.2, 0.8]\\), n = 246\n\nFor estimating predicted probabilities with 1 continuous predictor\n\nFor a margin of error of 0.1, n = 150\nFor a margin of error of 0.07, n = 300\n\n\nRF: 200 events per candidate feature (Harrell, link)\n\nUndersampling non-events(0s) is the popular way to balance the target variable in data sets but other ma be worth exploring while building the model.\nSpline — don’t bin continuous, baseline, adjustment variables, where “baseline” means the patients measurements before treatment. Lack of fit will then come only from omitted interaction effects. (Harrell)\n\ne.g.: if older males are much more likely to receive treatment B than treatment A than what would be expected from the effects of age and sex alone, adjustment for the additive propensity would not adequately balance for age and sex.\nAlso see\n\nFeature Engineering, General &gt;&gt; Continuous &gt;&gt; Binning\nFeature Engineering, General &gt;&gt; Continuous &gt;&gt; Splines\n\n\nThe best information to present to the patient is the estimated individualized risk of the bad outcome separately under all treatment alternatives. That is because patients tend to think in terms of absolute risk, and differences in risks don’t tell the whole story (Harrell)\n\nA risk difference (RD, also called absolute risk reduction) often means different things to patients depending on whether the base risk is very small, in the middle, or very large.\n\nRecommended metrics to be reported for medical studies (Harrell). This is perhaps generalizable to any RCT with a binary outcome.\n\nThe distribution of Risk Difference (RD)\nCovariate-Adjusted OR\nAdjusted marginal RD (mean personalized predicted risk as if all patients were on treatment A minus mean predicted risk as if all patients were on treatment B) (emmeans?)\nMedian RD",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/classification.html#sec-class-diag",
    "href": "qmd/classification.html#sec-class-diag",
    "title": "Classification",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nAlso see\n\nDiagnostics, Classification\nRegression, Logistic &gt;&gt; Diagnostics\n\n“During the initial phase of model building, a good strategy for data sets with two classes is to focus on the AUC statistics from these curves instead of metrics based on hard class predictions. Once a reasonable model is found, the ROC or precision-recall curves can be carefully examined to find a reasonable cutoff for the data and then qualitative prediction metrics can be used.” – 3.2.2 Classification Metrics” Kuhn and Kjell\n“Stable” AUC requirements for 0/1 outcome:\n\nPaper: Modern modelling techniques are data hungry: a simulation study for predicting dichotomous endpoints | BMC Medical Research Methodology | Full Text\nLogistic Regression: 20 to 50 events per predictor variable\nRandom Forest and SVM: greater than 200 to 500 events per predictor variable",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/classification.html#sec-class-imbal",
    "href": "qmd/classification.html#sec-class-imbal",
    "title": "Classification",
    "section": "Class Imbalance",
    "text": "Class Imbalance\n\nMisc\n\nAlso see\n\nModel Building, tidymodel &gt;&gt; Recipe &gt;&gt; up/down-sampling\nSurveys, Analysis &gt;&gt; Modeling &gt;&gt; Tidymodels &gt;&gt; Importance weights\nPaper: Subsampling without calibration will likely be more harmful than without subsampling\n\nSubsampling can refer to up/oversampling or down/undersampling.\nIt is perfectly ok to train a model on 1M negatives and 10K positives (i.e. plenty of events), as long as you avoid using accuracy as a metric\n\n1K to 10K events might be enough for the ML algorithm to learn from\nFor a RF model: 200 events per candidate feature (Harrell, link)\n\nUnless recalibration is applied, applying subsampling to correct class imbalance will lead to overpredicting the minority class (discrimination) and worse calibration when using logistic regression or ridge regression (paper)\n\nPaper used random undersampling (RUS), random oversampling (ROS), and SMOTE (Synthetic Minority Oversampling Technique)\nEvent Fractions: 1%, 10%, 30%\nN: 2500, 5000; p: 3, 6, 12, 24\n“We anticipate that risk miscalibration will remain present regardless of type of model or imbalance correction technique, unless the models are recalibrated. However, class imbalance correction followed by recalibration is only worth the effort if imbalance correction leads to better discrimination of the resulting models.”\n\nThey used what looked to be Platt Scaling for recalibration\n\nAlso see Model Building, tidymodels &gt;&gt; Tune &gt;&gt; Tune Model with Multiple Recipes for an example of how downsampling (w/o calibration) + glmnet affects class prediction and GOF metrics\n\n\nIssues\n\nUsing Accuracy as a metric\n\nIf the positivity rate is just 1%, then a naive classifier labeling everything as negative has 99% accuracy by definition\n\nIf you’ve used subsampling, then the training data is not the same as the data used in production\nLow event rate\n\nIf you only have 10 to 100 positive samples, the model may easily memorize these samples, leading to an overfit model that generalized poorly\nMay result in large CIs for your effect estimate (see Gelman post)\n\n\nCheck Imbalance\ndata %&gt;%\n  count(class) %&gt;%\n  mutate(prop = n / sum(n)) %&gt;%\n  pretty_print()\nDownsampling\n\nUse cases for downsampling the majority class\n\nwhen the training data doesn’t fit into memory (and your ML training pipeline requires it to be in memory), or\nwhen model training takes unreasonably long (days to weeks), causing too long iteration cycles, and preventing you from iterating quickly.\n\nUsing a domain knowledge filter for downsampling\n\na simple heuristic rule that cuts down most of the majority class, while keeping nearly all of the minority class.\n\ne.g. if a rule can retain 99% of positives but only 1% of the negatives, this would make a great domain filter\n\nExamples\n\ncredit card fraud prediction: filter for new credit cards, i.e. those without a purchase history.\nspam detection: filter for Emails from addresses that haven’t been seen before.\ne-commerce product classification: filter for products that contain a certain keyword, or combination of keywords.\nads conversion prediction: filter for a certain demographic segment of the user population.\n\n\n\nCV\n\nIt is extremely important that subsampling occurs inside of resampling. Otherwise, the resampling process can produce poor estimates of model performance.\n\ndata leakage: if you first upsample the data and then split the data into training (aka analysis set) and validation (aka assessment set) folds, your model can simply memorize the positives from the training data and achieve artificially strong performance on the validation data, causing you to think that the model is much better than it actually is.\nThe subsampling process should only be applied to the analysis (aka training) set. The assessment (aka validation) set should reflect the event rates seen “in the wild.”\n\nDoes {recipe} handle this correctly?\n\n\nProcess\n\nSubsample the data only in the analysis set\nPerform CV algorithm selection and tuning using a suitable metric for class imbalance\nUse same metric to get score on the test set\n\n\nML Methods\n\nSynthetic Data Approaches\n\nTabDDPM: Modeling Tabular Data with Diffusion Models (Raschka Thread)\n\nCategorical and Binary Features: adds uniform noise via multinomial diffusion\nNumerical Features: adds Gaussian noise using Gaussian diffusion\n\nSynthetic Data Vault (Docs)\n\nData generated with variational autoencoders adapted for tabular data (TVAE) (paper)\nCan Synthetic Data Boost Machine Learning Performance?\n\n\nImbalanced Classification via Layered Learning (ICLL)\n\nFor code, see article\nA hierarchical model composed of two levels:\n\nLevel 1: A model is built to split easy-to-predict instances from difficult-to-predict ones.\n\nThe goal is to predict if an input instance belongs to a cluster with at least one observation from the minority class.\n\nLevel 2: We discard the easy-to-predict cases. Then, we build a model to solve the original classification task with the remaining data.\n\nThe first level affects the second one by removing easy-to-predict instances from the training set.\n\n\nIn both levels, the imbalanced problem is reduced, which makes the modeling task simpler.\n\n\nTuning parameters (last resort)\n\nXGBoost and LightGBM have a parameter called scale_pos_weight, which up-weighs positive samples when computing the gradient at each boosting iteration\nUnlikely to have a major effect and probably won’t generalize well.",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/classification.html#sec-class-calib",
    "href": "qmd/classification.html#sec-class-calib",
    "title": "Classification",
    "section": "Calibration",
    "text": "Calibration\n\nCalibrated- When the predicted probabilities from a model match the observed distribution of probabilities for each class.\n\nCalibration Plots visualize this comparison of distributions\n\nCalibration mesasure are important for validating a predictive model.\n\nLogistic Regression models are usually well-calibrated, but most ML and DL model predicted probabilities aren’t directly produced by the algorithms and aren’t usually calibrated\nRF models can also benefit from calibration although given enough data, they are already pretty well calibrated\nSVM, Naive Bayes, boosted tree algorithms, and DL models benefit most from calibration\n\nAn unintended consequence of applying calibration modeling can be the worsening of calibration for models that are already well calibrated\nIf a model isn’t calibrated, then the magnitude of the predicted probability cannot be interpreted as the likelihood of an event\n\ne.g. If 0.66 and 0.67 are two predicted probabilities from an uncalibrated xgboost model, the 0.67 prediction cannot be interpreted as more likely to be an event than the 0.66 prediction.\nMiscalibrated predictions don’t allow you to have more confidence in a label with a higher probability than a label with a lower probability.\nSee (below) the introduction in the paper, “A tutorial on calibration measurements …” for examples of scenarios where calibration of risk scoring model is essential. If you can’t trust the predicted probabilities (i.e. risk) then decision-making is impossible.\n\nalso this paper which also explains some sources of miscalibration (e.g. dataset from region with low incidence, measurement error, etc.)\n\n\nEven if a model isn’t caibrated and depending on the metric, it can still be more accurate than a calibrated model.\n\nPoor calibration may make an algorithm less clinically useful than a competitor algorithm that has a lower AUC but is well calibrated (paper)\nCalibration doesn’t affect the AUROC (does not rely on predicted probabilities) but does affect the Brier Score (does rely on predicted probabilities) (paper)(Harrell)\nSince thresholds are based on probabilities, I don’t see how a valid, optimized threshold can be established for an uncalibrated model\nWonder how this affects model-agnostic metrics, feature importance, shap, etc.\n\n\n\nMisc\n\nAlso see Diagnostics, Classification &gt;&gt; Calibration\nPackages\n\n{probably} - tidymodels calibration package\n\nCalibrating Binary Probabilities - Nice tutorial\n\n\nNotes from:\n\nHow and When to Use a Calibrated Classification Model with scikit-learn\nCan I Trust My Model’s Probabilities? A Deep Dive into Probability Calibration\n\nPython, multiclass example\n\nA tutorial on calibration measurements and calibration models for clinical prediction models (paper)\n\nCalibration curves for nested cv (post)\nFor calibrated models, sample size affects the how well they’re calibrated\n\n\nEven for a logistic model, N &gt; 1000 is desired for good calibration\nFor a rf, closer to N = 10,000 is probably needed.\n\nDistributions of predicted probabilities\n\n\nRandom Forest pushes the probabilities towards 0.0 and 1.0, while the probabilities from the logistic regression are less skewed.\nDecision Trees are even more skewed than RF\nSays how rare a prediction is.\n\nIn a rf, really low or high probability predictions aren’t rare. So, if the model gives you a 0.93, you shouldn’t interpet it the way you normally would such a high probability (i.e. high certainty), because a rf inherently pushes its probabilities towards 0 and 1.\n\n\n\n\n\nBasic Workflow\n\nMisc\n\nIdeally you’d want each model (i.e. tuning parameter combination) being scored in a CV or a nested CV to have its own calibration model, but it’s not practical. But also, it’s unlikely an algorithm with a slightly different parameter value combination with have a substantially different predicted probability distribution, and it’s the algorithm itself that’s the salient factor. Therefore, for now, I’d go with 1 calibration model per algorithm.\n\nProcess\n\nSplit data into Training, Calibration, and Test\nFor each algorithm, train the algorithm on the Training set, and create the calibration model using it and the Calibration set.\n\nEach algorithm with have it’s own calibration model\nSee below, Example: AdaBoost in Py using CV calibrator\n\nI like sklearn’s ideas for training a calibration model\n\n\nUse the Training set for CV or Nested CV\nFor each split during CV or Nested CV (outer loop)\n\nTrain the algorithm on the training fold, predict on the validation fold, calibrate predictions with calibration model, score the calibrated predictions for that fold\n\nThe calibration model could be used in the tuning process in the inner loop of Nested CV as well\n\nScores should include calibration metrics (See Diagnostics, Classification &gt;&gt; Calibration &gt;&gt; Basic Workflow)\n\nThe rest is normal CV/Nested CV procedure\n\ni.e. average scores across the splits then select the algorithm with the best score. Predict and score algorithm on the Test set\nSee Calibration curves for nested cv for details and code on averaging calibration curves\n\n\n\n\n\nMethods\n\n\nMisc\n\nTLDR; Both Platt Scaling and Isotonic Regression methods are the essentially the same except:\n\nPlatt Scaling uses a logistic regression model as the calibration model\nIsotonic Regression uses an isotonic regression model on ordered data as the calibration model\n\n\nPlatt Scaling\n\nMisc\n\nSuitable for smaller data and for calibration curves with the S-shape.\nMay fail when model is already well calibrated (e.g. logistic regression models)\nPerforms best under the assumption that the predicted probabilities are close to the midpoint and away from the extremes\n\nSo, might be bad for tree models but okay for SVM, Naive Bayes, etc.\n\n\nProcess\n\nSplit data into Training, Calibration, and Test Sets\nTrain your model on the Training Set\nGet the predicted probabilities from your model on the Test Set\nFit a logistic regression using your model’s predicted probabilities for the Calibration Set as the predictor and the outcome variable in the Calibration Set as the outcome\nCalibrated probabilities are the predicted probabilities of the LR model using your model’s predicted probabilites on the Test set as new data.\n\nExample: SVM in R\nsvm_platt_recal = svm_platt_recal_func(model_fit, calib_dat, test_preds, test_dat)\n\nsvm_platt_recal_func = function(model_fit, calib_dat, test_preds, test_dat){\n\n  # Predict on Calibration Set \n  cal_preds_obj &lt;- predict(model_fit, \n                          calib_dat[, -which(names(calib_dat) == 'outcome_var')], \n                          probability = TRUE) \n  # e1071 has a funky predict output; just getting probabilities \n  cal_preds &lt;- as.data.frame(attr(cal_preds_obj, 'probabilities')[, 2])\n  # Create calibration model\n  cal_obs_preds_df = data.frame(y = calib_dat$outcome_var, yhat = cal_preds[, 1])\n  calib_model &lt;- glm(y ~ yhat, data = cal_obs_preds_df, family = binomial) \n\n  # Recalibrate classifiers predicted probabilities on the test set   \n  colnames(test_preds) &lt;- c(\"yhat\")\n  recal_preds = predict(calib.model, test_preds, type='response')   \n\n  return(recal_preds)\n\n}\n\nSee Istotonic Regression for “model_fit”, “calib_dat”, “test_preds”, and “test_dat”\n\nExample: AdaBoost in py\nX_, X_test, y_, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_train, X_calib, y_train, y_calib = train_test_split(X_, y_, test_size=0.4, random_state=42)\n\n# uncalibrated model\nclf = AdaBoostClassifier(n_estimators=50)\ny_proba = clf.fit(X_train, y_train).predict_proba(X_test)\n\n# calibrated model\ncalibrator = LogisticRegression()\ncalibrator.fit(clf.predict_proba(X_calib), y_calib)\ny_proba_calib = calibrator.predict_proba(y_proba)\n\nIsotonic Regression\n\nMore complex, requires a lot more data (otherwise it may overfit), but can support reliability diagrams with different shapes (is nonparametric).\n\nTried on palmer penguins (n = 332) and almost all the probabilites were pushed to the edges\nTried on mlbench::PimaIndiansDiabetes (n = 768). Probabilites were mostly clumped into 3 modes.\nPaper used a simulated (n = 5000) dataset. Probabilities moved a little more towards the center but the movement was much less dramatic that the other two datasets.\nI didn’t calculate brier scores but I’d guess you’d need over a 1000 or so observations for this method to have a significantly positive effect.\n\nLacks continuousness, because the fitted regression function is a piecewise function.\n\nSo, a slight change in the uncalibrated predictions can result in dramatic difference in the calibrated predictions (i.e. a change in step)\n\nProcess\n\nSplits: train (50%), Calibration (25%), Test (25%)\nTrain classifier model on train data\nGet predictions from the classifier on the test data\nCreate calibration model dataset from the calibration data\n\nGet predictions from the classifier on the calibration data\nCreate df with observed outcome of calibration data and predictions on calibration data\nOrder df rows according the predictions column\n\nLowest to largest probabilities in the paper but I’m not sure it matters\n\n\nFIt calibration model\n\nFit isotonic regression model on calibration model dataset\nCreate a step function using the isotonic regression fitted values and the predictions on the calibration data\n\nCalibrate the classifier’s predictions on the test set with the step function\n\nExample: SVM in R\nlibrary(dplyr)\nlibrary(e1071)\ndata(PimaIndiansDiabetes, package = \"mlbench\")\n\n# isoreg doesn't handle NAs\n# e1071::svm doesn't identify the event correctly in non-0/1 factor variables\ndat_clean &lt;- \n  PimaIndiansDiabetes %&gt;% \n  mutate(diabetes = ifelse(as.numeric(diabetes) == 2, 1, 0)) %&gt;% \n  rename(outcome_var = diabetes) %&gt;% \n  tidyr::drop_na()\n\n# Data splits Training, Calibration, Test (50% - 25% - 25%)\nsmp_size &lt;- floor(0.50 * nrow(dat_clean))\nval_smp_size = floor(0.25 * nrow(dat_clean))\ntrain_idx &lt;- sample(seq_len(nrow(dat_clean)), size = smp_size)\ntrain_dat &lt;- dat_clean[train_idx, ]\ntest_val_dat &lt;- dat_clean[-train_idx, ]\nval_idx &lt;- sample(seq_len(nrow(test_val_dat)), size = val_smp_size)\ncalib_dat &lt;- test_val_dat[val_idx, ]\ntest_dat &lt;- test_val_dat[-val_idx, ]\nrm(list=setdiff(ls(), c(\"train_dat\", \"calib_dat\", \"test_dat\")))\n\n# Fit classifier; predict on Test Set\n# e1071::svm needs a factor outcome, probability=T to output probabilities\nmodel_fit &lt;- svm(factor(outcome_var) ~ .,\n            data = train_dat, \n            kernel = \"linear\", cost = 10, scale = FALSE, probability = TRUE)\ntest_preds_obj &lt;- predict(model_fit,\n                test_dat[, -which(names(test_dat) == 'outcome_var')],\n                probability = TRUE)\n# e1071 has a funky predict output; just getting probabilities\ntest_preds &lt;- as.data.frame(attr(test_preds_obj, 'probabilities')[, 2])\n\n# Predict on Calibration Set\ncal_preds_obj &lt;- predict(model_fit,\n                        calib_dat[, -which(names(calib_dat) == 'outcome_var')],\n                        probability = TRUE)\n# e1071 has a funky predict output; just getting probabilities\ncal_preds &lt;- as.data.frame(attr(cal_preds_obj, 'probabilities')[, 2])\n\n# Create Calibration Model dataset\ncal_obs_preds_mtx = cbind(y = calib_dat$outcome_var, yhat = cal_preds[, 1])\n# order training data by predicted probabilities\niso_train_mtx &lt;- cal_obs_preds_mtx[order(cal_obs_preds_mtx[, 2]), ]\n\n# Fit Calibration Model\n# (predicted probabilities, observed outcome)\ncalib_model &lt;- isoreg(iso_train_mtx[, 2], iso_train_mtx[, 1]) \n# yf are the fitted values of the outcome variable\nstepf_data &lt;- cbind(calib_model$x, calib_model$yf) \nstep_func &lt;- stepfun(stepf_data[, 1], c(0, stepf_data[, 2])) \n# recalibrate classifiers predicted probabilities on the test set\nrecal_preds &lt;- step_func(test_preds[, 1])\n\nhead(recal_preds, n = 20)\nhist(recal_preds)\nhist(test_preds[, 1])\n\nisoreg doesn’t handle NAs\nFor a binary classification model that outputs probabilities, e1071::svm needs:\n\nfactored 0/1 outcome variable\nprobability = TRUE\n\n\nExample: AdaBoost in Py using CV calibrator\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn_evaluation import plot\nX, y = datasets.make_classification(10000, 10, n_informative=8,\n                                    class_sep=0.5, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nclf = AdaBoostClassifier(n_estimators=100)\nclf_calib = CalibratedClassifierCV(base_estimator=clf,\n                                  cv=3,\n                                  ensemble=False,\n                                  method='isotonic')\nclf_calib.fit(X_train, y_train)\ny_proba = clf_calib.predict_proba(X_test)\ny_proba_base = clf.fit(X_train, y_train).predict_proba(X_test)\nfig, ax = plt.subplots()\nplot.calibration_curve(y_test,\n                      [y_proba_base[:, 1], y_proba[:, 1]],\n                      clf_names=[\"Uncalibrated\", \"Calibrated\"],\n                      n_bins=20)\nfig.set_figheight(4)\nfig.set_figwidth(6)\nfig.set_dpi(150)\n\nCalibratedClassifierCV has both platt scaling (“sigmoid”)(default) and isotonic (“isotonic”) calibration methods (Docs)\nCalibration model is built using the test fold (aka validation fold)\n\nensemble=True\n\nFor each cv split, the base estimator is fit on the training fold, and the calibration model is built using the “test” fold (aka validation fold)\n\nThe test (aka validation) fold is the calibration data described in the isotonic and platt scaling process sections above\n\nFor prediction, predicted probabilities are averaged across these individual calibrated classifiers.\n\nensemble=False\n\nLOO CV is performed using cross_val_predict, and those predictions are used to train the calibration model.\nThe base estimator is trained using all the data (i.e. training and test (aka validation)).\nFor prediction, there is only one classifier and calibration model combo.\nThe benefit is that it’s faster, and since there’s only one combo, it’s smaller in size as compared to ensemble = True which is k combos. (not as accurate or as well-calibrated though)\n\n\n\nOther forms\n\nFor logistic regression models, adjustment using the Calibration Intercept (and Calibration Slope)\n\nSeems similar to Platt Scaling, but has strong overfitting vibes so caveat emptor\nFor calculating the values, see Diagnostics, Classification &gt;&gt; Calibration &gt;&gt; Evaluation of Calibration Levels &gt;&gt; Weak &gt;&gt; Intercept, Slope\npaper, see supplemental material\nProcedure\n\nThe calibration intercept is added to the fitted model’s intercept\nThe calibration slope is multiplied times all (nonexponentiated) coefficients of the fitted model (including interactions)\nPredictions are then calculated using the new formula\n\n\nExample\n\nFrom How We Built Our COVID-19 Estimator\nTarget: Risk of Death (probabilities)\nPredictors: Age, Gender, Comorbidities\nModel: XGBoost\n“Every time our model makes a prediction, we compare the result to what the model would have returned for an individual of the same age and sex and only one of the listed comorbidities, or none at all. If the predicted risk is lower than the risk for a comorbidity taken on its own—such as, say, the estimated risk for heart disease alone being greater than the risk for heart disease and hypertension, or the risk for metabolic disorders being lower than the risk of someone with no listed comorbidities—our tool delivers the higher number instead. We also smoothed our estimates and confidence intervals, using five-year moving averages by age and gender.”",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/classification.html#sec-class-featimp",
    "href": "qmd/classification.html#sec-class-featimp",
    "title": "Classification",
    "section": "Feature Importance",
    "text": "Feature Importance\n\nMisc\n\nSee notebook and bookmarks for proper algorithms (e.g. permutation importance) to specify for variable importance plots\nPermutation Importance\n\nPermutation importance is generally considered as a relatively efficient technique that works well in practice\nImportance of correlated features may be overestimated\n\nIf you have highly correlated features, use partykit::varimp(conditional = TRUE)\n\nProcess\n\nTake a model that was fit to the training dataset\nEstimate the predictive performance of the model on an independent dataset (e.g., validation dataset) and record it as the baseline performance\nFor each feature i:\n\nrandomly permute feature column i in the original dataset\nrecord the predictive performance of the model on the dataset with the permuted column\ncompute the feature importance as the difference between the baseline performance (step 2) and the performance on the permuted dataset\n\n\n\nVariable Importance plots can be useful for explaining the model to clients. They should never be interpreted as causal. Ex. Real estate: variables that are most influential in determining housing price by this model are aspects of a house that could be emphasized by Realtors to their clients.\nMake sure to use “LossFunctionChange” importance type in Catboost.\n\nLooks at how much the loss function changes when a feature is excluded from the model.\nDefault method capable of giving importance to random noise\nRequires evaluation on a testing set\n\n\nxgboost\nxg_wf_best &lt;- xg_workflow_obj %&gt;%\nfinalize_workflow(select_best(xg_tune_obj))\nxg_fit_best &lt;- xg_wf_best %&gt;%\nfit(train)\nimportances &lt;- xgboost::xgb.importance(model = extract_fit_engine(xg_fit_best))\nimportances %&gt;%\nmutate(Feature = fct_reorder(Feature, Gain)) %&gt;%\nggplot(aes(Gain, Feature)) +\ngeom_point() +\nlabs(title = \"Importance of each term in xgboost\",\n  subtitle = \"Even after turning direction numeric, still not *that* important\")\nUsing vip package\nlibrary(vip)\n\n# xg_wf = xgboost workflow object\nfit(xg_wf, whole_training_set) %&gt;% \npull_workflow_fit() %&gt;% \nvip(num_features = 20)\nglmnet\n\nlin_trained &lt;- lin_wf %&gt;%\n    finalize_workflow(select_best(lin_tune)) %&gt;%\n    fit(train) # or split_obj, test_dat, etc.\n\nlin_trained$fit$fit %&gt;%\n    broom::tidy %&gt;%\n    top_n(50, abs(estimate)) %&gt;%\n    filter(term != \"(Intercept)\") %&gt;%\n    mutate(ter = fct_reorder(term, estimate)) %&gt;%\n    ggplot(aes(estimate, term, fill = estimate &gt; 0)) +\n    geom_col() +\n    theme(legend.position = \"none\")",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html",
    "href": "qmd/clustering-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-misc",
    "href": "qmd/clustering-general.html#sec-clust-gen-misc",
    "title": "General",
    "section": "",
    "text": "Also see\n\nNotebook, pgs 57-58\nDiagnostics, Clustering\n\nFor static data, i.e., if the values do not change with time, clustering methods are usually divided into five major categories:\n\nPartitioning (or Partitional)\nHierarchical\nDensity-Based\nGrid-Based\nModel-Based Methods",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-terms",
    "href": "qmd/clustering-general.html#sec-clust-gen-terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nCluster Centroid - The middle of a cluster. A centroid is a vector that contains one number for each variable, where each number is the mean of a variable for the observations in that cluster. The centroid can be thought of as the multi-dimensional average of the cluster.\nHard (or Crisp) Clustering - each point belongs to a single cluster\nSoft (or Fuzzy) Clustering - each point is allowed to belong to multiple clusters",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-clustdesc",
    "href": "qmd/clustering-general.html#sec-clust-gen-clustdesc",
    "title": "General",
    "section": "Cluster Descriptions",
    "text": "Cluster Descriptions\n\nPackages\n\n{parameters} - provides various functions for describing, analyzing, and visualizing clusters for various methods\n{clustereval} - compute the statistical association between the features and the detected cluster labels and whether they are significant.\n\nCategorical: Chi-Square, Fisher’s Exact, or Hypergeometric tests\nContinuous: Mann-Whitney-U test\n\n\nExamine variable values at the centroids of each cluster\n\nA higher absolute value indicates that a certain variable characteristic is more pronounced within that specific cluster (as compared to other cluster groups with lower absolute mean values).\n\nDistributional statistics for each cluster\n\nNumeric variables: mean and sd for each variable in that cluster\nCategorical variables:\n\nbinary: percent where event = 1\nmultinomial: most prominent category\n\n\nRun a decision tree on clusters\n\n\nEach color (orange, blue, green, purple) represents a cluster\nExplains how clusters were generated\n{treeheatr}\n\n\nRadar charts\n\n\n3 clusters: blue (highlighted), red, green\nGuessing the mean values for each variable are the points\n\nScatter\n\nUse clustering variables of interest for a scatter plot then label the points with cluster id",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-gmm",
    "href": "qmd/clustering-general.html#sec-clust-gen-gmm",
    "title": "General",
    "section": "Gaussian Mixture Models (GMM)",
    "text": "Gaussian Mixture Models (GMM)\n\nMisc\n\nSoft clustering algorithm\nNotes from\n\nSerrano video: https://www.youtube.com/watch?v=q71Niz856KE&ab_channel=LuisSerrano\nPackages\n\n{otrimle}\n\nUses Improper Maximum Likelihood Estimator Clustering (IMLEC) method\nHyperparameters automatically tuned; Outliers removed\nRobust gaussian mixture clustering algorithm\nWebpage has links to paper, Coretto and Hennig, 2016\n\n\n\n\nComponents of the Algorithm\n\n“Color” points according to gaussians (clusters)\n\nThe closer a point is to the center of a gaussian the more intensely it matches the color of that gaussian\nPoints in between gaussians are a mixture or proportion of the colors of each gaussian\n\n\nFitting a Gaussian\n\nFind the center of mass\n\n2-dim: calculate the mean of x and the mean of y and that’s the coordinates of your center of mass\n\nFind the spread of the points\n\n2-dim: calculate the x-variance, y-variance, and covariance\n\n\nFirst Equation: Height of Gaussian (Multivariate Gaussian distribution equation).\nSecond Equation: 1-D gaussian equation that’s just being used for reference\n\nPartially “colored” points affect spread and center of mass calculations\n\nFully colored points “weigh” more than partially colored points and pull the center of mass and change the orientation\n\n\n\n\n\nSteps\n\nStart with random Gaussians\n\nEach gaussian has random means, variances\n\nColor points according to distance to the random gaussians\n\nThe heights in the distributions pic above\n\n(Forget about old gaussians) Calculate new gaussians based on the colored points\n(Forget about old colors) Color points according to distance to the new gaussians\nRepeat until some threshold is reached (i.e. gaussians or colors don’t change much)\n\nTuning\n\nInitial Conditions (i.e. Good starting points for the random gaussians at the beginning)\nLimits on the mean and variance calculations\nNumber of gaussians, k, can be chosen by minimizing the Davies-Bouldin score\n\nSee Diagnostics, Clustering &gt;&gt; Spherical/Centroid Based &gt;&gt; Davies-Bouldin Index\n\nRunning algorithm multiple times\n\nLike CV grid search algs or bootstrapping",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-lpa",
    "href": "qmd/clustering-general.html#sec-clust-gen-lpa",
    "title": "General",
    "section": "Latent Profile Analysis (LPA)",
    "text": "Latent Profile Analysis (LPA)\n\nSort of like k-means + GMM\nk number of profiles (i.e. clusters) are chosen\nModel outputs probabilities that an observation belongs to any particular cluster\nGOF metrics available\n“As with Exploratory Factor Analysis (EFA )(and other latent-variable models), the assumption of LPA is that the latent (unobserved) factor”causes” (I’m using the term loosely here) observed scores on the indicator variables. So, to refer back to my initial hypothetical example, a monster being a spell caster (the unobserved class) causes it to have high intelligence, low strength, etc. rather than the inverse. This is a worthwhile distinction to keep in mind, since it has implications for how the model is fit.”\nBin variables that might dominate the profile. This way the profiles will represent a latent variable and not gradations of the dominate variable (e.g. low, middle, high values of the dominate variable).\nCenter other variable observations according to dominant variable bin those observations are in. (e.g. subtract values in bin1 from bin1’s mean)\n# From D&D article where challenge_rating is a likely dominant variable\nmons_bin &lt;- mons_df %&gt;%\n  mutate(cr_bin = ntile(x = challenge_rating, n = 6))\nab_scores &lt;- c(\"strength\", \"dexterity\", \"constitution\", \"intelligence\", \"wisdom\", \"charisma\") \nmons_bin &lt;- mons_bin %&gt;%\n  group_by(cr_bin) %&gt;%\n  mutate(across(.cols = ab_scores, .fns = mean, .names = \"{.col}_bin_mean\")) %&gt;%\n  ungroup()",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-tsne",
    "href": "qmd/clustering-general.html#sec-clust-gen-tsne",
    "title": "General",
    "section": "tSNE",
    "text": "tSNE\n\nPackages\n\n{Rtsne}\n\nt-Distributed Stochastic Neighbor Embedding\nLooks at the local distances between points in the original data space and tries to reproduce them in the low-dimensional representation\n\nBoth UMAP and tSNE attempt to do this but fails (Lior Pachter paper thread, Doesn’t preserve local structure, No theorem says that it preserves topology)\n\nResults depend on a random starting point\nTuning parameters: perplexity",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-umap",
    "href": "qmd/clustering-general.html#sec-clust-gen-umap",
    "title": "General",
    "section": "UMAP",
    "text": "UMAP\n\nPackages:\n\n{umap}\n\nUniform Manifold Approximation and Projection\nSee tSNE section for Lior Pachter threads on why not to use tSNE or UMAP\nPreprocessing\n\nOnly for numeric variables\nStandardize\n\nProjects variables to a nonlinear space\nVariation of tSNE\n\nRandom starting point has less of an impact\n\nCan be supervised (give it an outcome variable)\nComputationally intensive\nLow-dimensional embedding cannot be interpreted\n\nNo rotation matrix plot like in PCA\n\nTry pca - linear method (fast)\n\nIf successful (good separation between categories), then prediction may be easier\nIf not, umap, tsne needed\n\nUMAP can taking training model and apply it to test data or new data (tSNE can’t)\nTuning parameter: neighbors\n\nExample used 500 iterations (n_epochs) as limit for convergence",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-kmeans",
    "href": "qmd/clustering-general.html#sec-clust-gen-kmeans",
    "title": "General",
    "section": "K-Means",
    "text": "K-Means\n\nSeeks to assign n points to k clusters and find cluster centers so as to minimize the sum of squared distances from each point to its cluster center.\nFor choosing the number of clusters, elbow method (i.e. WSS) is usually awful if there are more than few clusters. Recommended: Calinski-Harabasz Index and BIC then Silhouette Coefficient or Davies-Bouldin Index (See Diagnostics, Clustering &gt;&gt; Spherical/Centroid Based (article)\nBase R kmeans uses the Hartigan-Wong algorithm\n\nFor large k and larger n, the density of cluster centers should be proportional to the density of the points to the power (d/d+2). In other words the distribution of clusters found by k-means should be more spread out than the distribution of points. This is not in general achieved by commonly used iterative schemes, which stay stuck close to the initial choice of centers.\n\n{tidyclust}\n\nEngines\n\nstats and ClusterR run classical K-means\nlaR runs K-Modes models which are the categorical analog to K-means, meaning that it is intended to be used on only categorical data\nclustMixType to run K-prototypes which are the more general method that works with categorical and numeric data at the same time.\n\nExample: Mixed K-Means\nlibrary(tidymodels)\nlibrary(tidyclust)\n\ndata(\"ames\", package = \"modeldata\")\n\nkproto_spec &lt;- k_means(num_clusters = 3) %&gt;%\n  set_engine(\"clustMixType\")\n\nkproto_fit &lt;- kproto_spec %&gt;%\n  fit(~ ., data = ames)\n\nkproto_fit %&gt;%\n  extract_centroids() %&gt;%\n  select(11:20) %&gt;%\n  glimpse()\n#&gt; Rows: 3\n#&gt; Columns: 10\n#&gt; $ Lot_Config     &lt;fct&gt; Inside, Inside, Inside\n#&gt; $ Land_Slope     &lt;fct&gt; Gtl, Gtl, Gtl\n#&gt; $ Neighborhood   &lt;fct&gt; College_Creek, North_Ames, Northridge_Heights\n#&gt; $ Condition_1    &lt;fct&gt; Norm, Norm, Norm\n#&gt; $ Condition_2    &lt;fct&gt; Norm, Norm, Norm\n#&gt; $ Bldg_Type      &lt;fct&gt; OneFam, OneFam, OneFam\n#&gt; $ House_Style    &lt;fct&gt; Two_Story, One_Story, One_Story\n#&gt; $ Overall_Cond   &lt;fct&gt; Average, Average, Average\n#&gt; $ Year_Built     &lt;dbl&gt; 1989.977, 1953.793, 1998.765\n#&gt; $ Year_Remod_Add &lt;dbl&gt; 1995.934, 1972.973, 2003.035",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-ann",
    "href": "qmd/clustering-general.html#sec-clust-gen-ann",
    "title": "General",
    "section": "Approximate Nearest Neighbor (ANN)",
    "text": "Approximate Nearest Neighbor (ANN)\n\nkNN runs at O(N*K), where N is the number of items and K is the size of each embedding. Approximate nearest neighbor (ANN) algorithms typically drop the complexity of a lookup to O(log(n)).\nMisc\n\nAlso see Maximum inner product search using nearest neighbor search algorithms\n\nIt shows a preprocessing transformation that is performed before kNN to make it more efficient\nIt might already be implemented in ANN algorithms\n\n\nCommonly used in Recommendation algs to cluster user-item embeddings at the end. Also, any NLP task where you need to do a similarity search of one character embedding to other character embeddings.\nGenerally uses one of two main categories of hashing methods: either data-independent methods, such as locality-sensitive hashing (LSH); or data-dependent methods, such as Locality-preserving hashing (LPH)\nLocality-Sensitive Hashing (LSH)\n\nHashes similar input items into the same “buckets” with high probability.\nThe number of buckets is much smaller than the universe of possible input items\nHash collisions are maximized, not minimized, where a collision is where two distinct data points have the same hash.\n\nSpotify’s Annoy\n\nUses a type of LSH, Random Projections Method (RPM) (article didn’t explain this well)\nL RPM hashing functions are chosen. Each data point, p, gets hashed into buckets in each of the L hashing tables. When a new data point, q, is “queried,” it gets hash into buckets like p did. All the hashes in the same buckets of p are pulled and the hashes within a certain threshold, c*R, are considered nearest neighbors.\n\nWiki article on LSH and RPM clears it up a little, but I’d probably have to go to Spotify’s paper to totally make sense of this.\n\nAlso the Spotify alg might bring trees/forests into this somehow\n\nFacebook AI Similarity Search (FAISS)\n\nHierarchical Navigable Small World Graphs (HNSW)\nHNSW has a polylogarithmic time complexity (O(logN))\nTwo approximations available Embeddings are clustered and centroids are calculated. The k nearest centroids are returned.\n\nEmbeddings are clustered into veroni cells. The k nearest embeddings in a veroni cell or a region of veroni cells is returned.\n\nBoth types of approximations have tuning parameters.\n\nInverted File Index + Product Quantization (IVFPQ)(article)",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-dbscan",
    "href": "qmd/clustering-general.html#sec-clust-gen-dbscan",
    "title": "General",
    "section": "DBSCAN",
    "text": "DBSCAN\n\n\nMisc\n\nNotes from:\n\nUnderstanding DBSCAN and Implementation with Python\nClustering with DBSCAN, Clearly Explained video\n\nPackages\n\n{dbscan}\n{parameters}\n\nn_clusters_dbscan - Given a “min_size” (aka minPts?), the function estimates the optimal “eps”\ncluster_analysis - Shows Sum of Squares metrics and the (standardized) mean value for each variable within each cluster.\n\n\nHDBSCAN is the hierarchical density-based clustering algorithm\nUse Cases\n\nGeospatially Clustering Earthquakes\n\nEvents can occur in irregular shaped clusters (i.e., along faults of different orientations).\nEvents can occur in different densities (i.e. some fault zones are more active than others).\nEvents can occur far away from fault zones (i.e. outliers)\n\n\n\nTuning\n\neps - The maximum distance between two samples for one to be considered to be connected to the other\n\nLarge eps tend to include more points within a cluster,\nToo-large eps will include everything in the same single cluster\nToo-small eps will result in no clustering at all\n\nminPts (or min_samples) - The minimum number of samples in a neighborhood for a point to be considered as a core point\n\nToo-small minPts is not meaningful because it will regard every point as a core point.\nLarger minPts can be better to deal with noisy data\n\n\nAlgorithm\n\nFor each data point, find the points in the neighborhood within eps distance, and define the core points as those with at least minPts neighbors.\n\n\nThe orange circle represents the eps area\nIf minPts = 4, then the top 4 points are core points because they have at least 4 points overlapping the eps area\n\nDefine groups of connected core points as clusters.\n\n\nAll the green points have been labelled as core points\n\nAssign each non-core point to a nearby cluster if it’s directly reachable from a neighboring core point, otherwise define it as an outlier.\n\n\nThe black points are non-core points but are points that overlap the eps area for the outer-most core points.\nAdding these black points finalizes the first cluster\n\nThis process is repeated for the next group of core points and continues until all that’s left are outliers.\n\nAdvantages\n\nDoesn’t require users to specify the number of clusters.\nNot sensitive to outliers.\nClusters formed by DBSCAN can be any shape, which makes it robust to different types of data.\n\nExample: Nested Cluster Structure\n\nK-Means\n\n\nK-Means wants spherical clusters which makes it grab groups of points it shouldn’t\n\nDBSCAN\n\n\nAble correctly identify the oblong shaped cluster\n\n\n\n\nDisadvantages\n\nIf the data has a very large variation in densities across clusters because you can only use one pair of parameters, eps and MinPts, on one dataset\nIt could be hard to define eps without the domain knowledge of the data\nClusters not totally reproducible. Clusters are defined sequentially so depending on which group of core points the algorithm starts with and hyperparameter values, some non-core points that are within the eps area of multiple clusters may be assigned to different clusters on different runs of the algorithm.",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/code-style-guide.html",
    "href": "qmd/code-style-guide.html",
    "title": "Style Guide",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Code",
      "Style Guide"
    ]
  },
  {
    "objectID": "qmd/code-style-guide.html#misc",
    "href": "qmd/code-style-guide.html#misc",
    "title": "Style Guide",
    "section": "",
    "text": "Packages\n\n{styler}",
    "crumbs": [
      "Code",
      "Style Guide"
    ]
  },
  {
    "objectID": "qmd/code-style-guide.html#naming",
    "href": "qmd/code-style-guide.html#naming",
    "title": "Style Guide",
    "section": "Naming",
    "text": "Naming\n\nCharacteristics\n\nGood names are a form of documentation\nNested loop variables should be names, not letters\nNames should be easily searchable\nUse prefixes and positive terms for booleans\nAdd extra detail for test functions\nNames should be pronouncable\nUse consistent lexicon throughout a project\n\nGood names are a form of documentation\n\nNested loop variables should be names, not letters\n\nNames should be easily searchable\n\nUse prefixes and positive terms for booleans\n\nAdd extra detail for test functions\n\nNames should be pronouncable\n\nUse consistent lexicon throughout a project",
    "crumbs": [
      "Code",
      "Style Guide"
    ]
  },
  {
    "objectID": "qmd/code-style-guide.html#refactoring",
    "href": "qmd/code-style-guide.html#refactoring",
    "title": "Style Guide",
    "section": "Refactoring",
    "text": "Refactoring\n\nFlatten nested code\n\n\n\n\n\n\n\n\nNested\n\n\n\n\n\n\n\nFlattened\n\n\n\n\n\n\n\nCondition on the negative\n\n\n\n\n\nNested\nFlattened",
    "crumbs": [
      "Code",
      "Style Guide"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html",
    "href": "qmd/confidence-and-prediction-intervals.html",
    "title": "Confidence & Prediction Intervals",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-misc",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-misc",
    "title": "Confidence & Prediction Intervals",
    "section": "",
    "text": "Also see Mathematices, Statistics &gt;&gt; Descriptive Statistics &gt;&gt; Understanding CI, sd, and sem Bars\nSE used for CIs of the difference in proportion\n\\[\n\\text{SE} = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\n\\]",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-terms",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-terms",
    "title": "Confidence & Prediction Intervals",
    "section": "Terms",
    "text": "Terms\n\nConfidence Intervals: A range of values within which we are reasonably confident the true parameter (e.g mean) of a population lies, based on a sample statistic (e.g. t-stat).\n\nFrequentist Interpretation: The confidence interval is constructed by a procedure, which, if you were to repeat the experiment and collecting samples many many times, in 95% of the experiments, the corresponding confidence intervals would cover the true value of the population mean. (link)\n\\[\n[100\\cdot(1-\\alpha)]\\;\\%\\: \\text{CI for}\\: \\hat\\beta_i = \\hat\\beta_i \\pm \\left[t_{(1-\\alpha/2)(n-k)} \\cdot \\text{SE}(\\hat\\beta_i)\\right]\n\\]\n\n\\(t\\) is the t-stat for\n\n\\(n-k\\) = sample size - number of predictors\n\\(1-\\alpha\\) for 2-sided; \\(1 - (\\alpha/2)\\) for 1 sided (I think)\n\n\\(\\text{SE}(\\beta_i)\\) is the sqrt of the corresponding value on the diagonal of the variance-covariance matrix for the coefficients.\n\nBayesian Interpretation: the true value is in that interval with 95% probability\n\nCoverage or Empirical Coverage: The level of coverage actually observed when evaluated on a dataset, typically a holdout dataset not used in training the model. Rarely will your model produce the Expected Coverage exactly\n\nAdaptive Coverage: Setting your Expected Coverage so that your Empirical Coverage = Target Coverage. A conformal prediction algorithm is adaptive if it not only achieves marginal coverage, but also (approximately) conditional coverage\n\nExample: 90% target coverage\n\nIf our model is slightly overfit, you might see that a 90% expected coverage leads to an 85% empirical coverage on a holdout dataset. To align your target and empirical coverage at 90%, may require setting expected coverage at something like 93%\n\n\nExpected Coverage: The level of confidence in the model for the prediction intervals.\nConditional Coverage: The coverage for each individual class of the outcome variable or subset of data specified by a grouping variable.\nMarginal Coverage: The overall average coverage across all classes of the outcome variable. All conformal methods achieve at or near the Expected Coverage averaged across classes but not necessarily for each individual class.\nTarget Coverage: The level of coverage you want to attain on a holdout dataset\n\ni.e. The proportion of observations you want to fall within your prediction intervals\n\n\nJeffrey’s Interval: Bayesian CIs for Binomial proportions (i.e. probability of an event)\n# probability of event\n# n_rain in the number of events (rainy days)\n# n is the number of trials (total days)\nmutate(pct_rain = n_rain / n, \n       # jeffreys interval\n       # bayesian CI for binomial proportions\n       low = qbeta(.025, n_rain + .5, n - n_rain + .5), \n       high = qbeta(.975, n_rain + .5, n - n_rain + .5))\nPrediction Interval: Used to estimate the range within which a future observation is likely to fall\n\nStandard Procedure for computing PIs for predictions (See link for examples and further details)\n\\[\n\\hat Y_0 \\pm t^{n-p}_{\\alpha/2} \\;\\hat\\sigma \\sqrt{1 + \\vec x_0'(X'X)^{-1}\\vec x_0}\n\\]\n\n\\(Y_0\\) is a single prediction\n\\(t\\) is the t-stat for\n\n\\(n-p\\) = sample size - number of predictors\n\\(1 - \\alpha\\) for 2-sided; \\(1 - (\\alpha/2)\\) for 1 sided (I think)\n\n\\(\\hat\\sigma\\) is the variance given by residual standard error, summary(Model1)$sigma\n\\[\nS^2 = \\frac{1}{n-p}\\;||\\;Y-X\\hat \\beta\\;||^2\n\\]\n\n\\(S = \\hat \\sigma\\)\nI think this is also the \\(\\operatorname{MSE}/\\operatorname{dof}\\) that you sometimes see in other formulas\n\n\\(x_0\\) is new data for the predictor variable values for the prediction (also would need to include a 1 for the intercept)\n\\((X'X)^{-1}\\) is the variance covariance matrix, vcov(model)",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-diag",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-diag",
    "title": "Confidence & Prediction Intervals",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nMean Interval Score (MIS)\n\n(Proper) Score of both coverage and interval width\n\nI don’t think there’s a closed range, so it’s meant for model comparison\nLower is better\n\ngreybox::MIS and (scaled) greybox::sMIS\n\nOnline docs don’t have these functions, but docs in RStudio do\n\nAlso scoringutils::interval_score\n\nDocs have formula\n\nThe actual paper is dense Need to take the mean of MIS\n\n\n\nCoverage\n\nExample: Coverage %\ncoverage &lt;- function(df, ...){\n  df %&gt;%\n    mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price pred_upper, 1, 0)) %&gt;% \n    group_by(...) %&gt;% \n    summarise(n = n(),\n              n_covered = sum(\n                covered\n              ),\n              stderror = sd(covered) / sqrt(n),\n              coverage_prop = n_covered / n)\n}\nrf_preds_test %&gt;% \n  coverage() %&gt;% \n  mutate(across(c(coverage_prop, stderror), ~.x * 100)) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt_number(\"stderror\", decimals = 2) %&gt;% \n  gt::fmt_number(\"coverage_prop\", decimals = 1)\n\nFrom Quantile Regression Forests for Prediction Intervals\nSale_Price is the outcome variable\nrf_preds_test is the resulting object from predict with a tidymodels model as input\n\nExample: Test consistency of coverage across quintiles\npreds_intervals %&gt;%  # preds w/ PIs\n  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;%  # quintiles\n  mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price &lt;= .pred_upper, 1, 0)) %&gt;% \n  with(chisq.test(price_grouped, covered))\n\np value &lt; 0.05 says coverage significantly differs by quintile\nSale_Price is the outcome variable\n\n\nInterval Width\n\nNarrower bands should mean a more precise model\nExample: Average interval width across quintiles\nlm_interval_widths &lt;- preds_intervals %&gt;% \n  mutate(interval_width = .pred_upper - .pred_lower,\n        interval_pred_ratio = interval_width / .pred) %&gt;% \n  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;% # quintiles\n  group_by(price_grouped) %&gt;% \n  summarize(n = n(),\n            mean_interval_width_percentage = mean(interval_pred_ratio),\n            stdev = sd(interval_pred_ratio),\n            stderror = stdev / sqrt(n)) %&gt;% \n  mutate(x_tmp = str_sub(price_grouped, 2, -2)) %&gt;% \n  separate(x_tmp, c(\"min\", \"max\"), sep = \",\") %&gt;% \n  mutate(across(c(min, max), as.double)) %&gt;% \n  select(-price_grouped)\n\nlm_interval_widths %&gt;% \n  mutate(across(c(mean_interval_width_percentage, stdev, stderror), ~.x*100)) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt_number(c(\"stdev\", \"stderror\"), decimals = 2) %&gt;% \n  gt::fmt_number(\"mean_interval_width_percentage\", decimals = 1)\n\nInterval width has actually been transformed into a percentage as related to the prediction (removes the scale of the outcome variable)",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-boot",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-boot",
    "title": "Confidence & Prediction Intervals",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nMisc\n\nDo NOT bootstrap the standard deviation\n\narticle\nbootstrap is “based on a weak convergence of moments”\nif you use an estimate based standard deviation of the bootstrap, you are being overly conservative (i.e. overestimate the sd)\n\nbootstrapping uses the original, initial sample as the population from which to resample, whereas Monte Carlo simulation is based on setting up a data generation process (with known values of the parameters of a known distribution). Where Monte Carlo is used to test drive estimators, bootstrap methods can be used to estimate the variability of a statistic and the shape of its sampling distribution\nPackages\n\n{ebtools::get_boot_ci}\n\n\nSteps\n\nResample with replacement\nCalculate statistic of resample\nStore statistic\nRepeat 10K or so times\nCalculate mean, sd, and quantiles for CIs across all collected statistics\n\nCIs\n\nPlenty of articles for means and models, see bkmks\nrsample::reg_intervals is a convenience function for lm, glm, survival models\n\nPIs\n\nBootstrapping PIs is a bit complicated\n\nSee Shalloway’s article (code included)\nonly use out-of-sample estimates to produce the interval\nestimate the uncertainty of the sample using the residuals from a separate set of models built with cross-validation",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-conf",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-conf",
    "title": "Confidence & Prediction Intervals",
    "section": "Conformal Prediction Intervals",
    "text": "Conformal Prediction Intervals\n\nMisc\n\nPackages\n\n{{mapie}} - Handles scikit-learn, tf, pytorch, etc. with wrappers. Computes conformal PIs for Regression, Classification, and Time Series models.\n\nRegression\n\nMethods: naive, split, jackknife, jackknife+, jackknife-minmax, jackknife-after-bootstrap, CV, CV+, CV-minmax, ensemble batch prediction intervals (EnbPI).\n“Since the typical coverage levels estimated by jackknife+ follow very closely the target coverage levels, this method should be used when accurate and robust prediction intervals are required.”\n“For practical applications where N is large and/or the computational time of each leave-one-out simulation is high, it is advised to adopt the CV+ method” even though the interval width will be slightly larger than jackknife+\n“The jackknife-minmax and CV-minmax methods are more conservative since they result in higher theoretical and practical coverages due to the larger widths of the prediction intervals. It is therefore advised to use them when conservative estimates are needed.”\n“The conformalized quantile regression method allows for more adaptiveness on the prediction intervals which becomes key when faced with heteroscedastic data.”\nEnbPI is for time series and residuals must be updated each time new observations are available\n\nClassification\n\nMethods: LAC, Top-K, Adaptive Prediction Sets (APS), Regularized Adaptive Prediction Sets (RAPS), Split and Cross-Conformal methods.\nThe difference between these methods is the way the conformity scores are computed\nLAC method is not adaptive: the coverage guarantee only holds on average (i.e. marginal coverage). Difficult classification cases may have prediction sets that are too small, and easy cases may have sets that are too large. (See below for details on process). Doesn’t seem like to great a task to manually make it adaptive though (See example below).\nAPS’ conformity score used to determine the threshold is a constrained sum of the predicted probabilities for that observation. Only the predicted probabilites \\(\\ge\\) the predicted probability of the true class are included in the sum. Everything else is the same as the LAC algorithm, although the default behavior is to keep the last class that crosses the threshold through the argument, include_last_label = [True, “randomized”, False]. The value of the argument can determine whether conditional coverage is (approximately) attained with True being the most liberal setting. Note that only “randomized” can produce empty predicted class sets. Algorithm tends to produce large predicted class sets when there are many classes in the outcome variable.\nRAPS attenuates the lengthier predicted class sets in APS through regularization. A penalty, \\(\\lambda\\), is added to predicted probabilities with ranks greater that some value, \\(k\\). Everything else is the same as APS.\nNot sure what Split is, but Cross-Conformal is CV applied to LAC and APS.\n\n\n\nNotes from\n\nHow to Handle Uncertainty in Forecasts: A deep dive into conformal prediction\n\nThe conformity score formula used in this article, \\(s_i = |\\;y_i - \\hat p_i(y_i\\;|\\;X_i)\\;|\\) where \\(y_i\\) is the observed class and \\(\\hat p\\) is the predicted probability, has the same results as to the one below, but it’s not workable in production since there is no observed class.\n\nConformal Prediction for Machine Learning Classification — From the Ground Up\n“MAPIE” Explained Exactly How You Wished Someone Explained to You\n\nResources\n\nIntroduction To Conformal Prediction With Python A Short Guide for Quantifying Uncertainty of Machine Learning Models\n\nSee R &gt;&gt; Documents &gt;&gt; Machine Learning\n\n\nNormal PIs require iid data while conformal PIs only require the “identically distributed” part (not independent) and therefore should provide more robust coverage.\nContinuous outcome (link) using quantile regression\n\n\n\nClassification\n\nLAC (aka Score Method) Process\n\nSplit data into Train, Calibration (aka Validation), and Test\nTrain the model on the training set\nOn the calibration (aka validation) set, compute the conformity scores only for the observed class (i.e. true label) for each observation\n\\[\ns_{i, j}  = 1 - \\hat p_{i,j}(y_i | X_i)\n\\]\n\nVariables\n\n\\(s_{i,j}\\): Conformity Score for the ith observation and class \\(j\\)\n\\(y_i\\): Observed Class\n\\(\\hat p_{i,j}\\): Predicted probability by the model for class \\(j\\)\n\\(X_i\\): Predictors\n\\(i\\): Index of the observed data\n\\(j\\): Class of the outcome variable\n\nRange: [0, 1]\nIn general, Low = good, High = bad\nIn R, the predicted probabilities for statistical models are always for the event (i.e. \\(y_i = 1\\)) in a binary outcome context, so when the observed class = 0, the score will be \\(s_{i,0} = 1-(1- \\hat p_{i, 1}(y_i | X_i)) = \\hat p_{i, 1}(y_i | X_i)\\) which is just the predicted probability.\n\nOrder the conformity scores from highest to lowest\nAdjust the chosen the \\(\\alpha\\) using a finite sample correction, \\(q_{\\text{level}} = 1- \\frac{ceil((n_{\\text{cal}}+1)\\alpha)}{n_{\\text{cal}}}\\) and calculate the quantile.\nCalculate the critical value or threshold for the quantile\n\n\nx-axis corresponds to an ordered set of conformity scores\nIf \\(\\alpha = 0.05\\), find the score value at the the 95th percentile (e.g. quantile(scores, 0.95))\nBlue: conformity scores are not statistically significant. They’re within our prediction interval.\nRed: Very large conformity scores indicate high divergence from the true label. These conformal scores are statistically significant and thereby outside of our prediction interval.\n\nPredict on the Test set and calculate conformity scores for each class\nFor each test set observation, select classes that have scores below the threshold score as the model prediction.\n\nAn observation could potentially have both classes or no classes selected. ( Not sure if this is true in a binary outcome situation)\n\n\nExample: LAC Method, Multinomial\n\nModel\nclassifier = LogisticRegression(random_state=42)\nclassifier.fit(X_train, y_train)\nScores calculated using only the predicted probability for the true class on the Validation set (aka Calibration set)\n# Get predicted probabilities for calibration set\ny_pred = classifier.predict(X_Cal)\ny_pred_proba = classifier.predict_proba(X_Cal)\nsi_scores = []\n# Loop through all calibration instances\nfor i, true_class in enumerate(y_cal):\n    # Get predicted probability for observed/true class\n    predicted_prob = y_pred_proba[i][true_class]\n    si_scores.append(1 - predicted_prob) \nThe threshold determines what coverage our predicted labels will have\nnumber_of_samples = len(X_Cal)\nalpha = 0.05\nqlevel = (1 - alpha) * ((number_of_samples + 1) / number_of_samples)\nthreshold = np.percentile(si_scores, qlevel*100)\nprint(f'Threshold: {threshold:0.3f}')\n#&gt; Threshold: 0.598\n\nFinite sample correction for the 95th quantile: multiply 0.95 by (n+1)/n\n\nThreshold is then used to get predicted labels of the test set\n# Get standard predictions for comparison\ny_pred = classifier.predict(X_test)\n# Calc scores, then only take scores in the 95% conformal PI\nprediction_sets = (1 - classifier.predict_proba(X_test) &lt;= threshold)\n\n# Get labels for predictions in conformal PI\ndef get_prediction_set_labels(prediction_set, class_labels):\n    # Get set of class labels for each instance in prediction sets\n    prediction_set_labels = [\n        set([class_labels[i] for i, x in enumerate(prediction_set) if x]) for prediction_set in \n        prediction_sets]\n    return prediction_set_labels\n\n# Compare conformal prediction with observed and traditional preds\nresults_sets = pd.DataFrame()\nresults_sets['observed'] = [class_labels[i] for i in y_test]\nresults_sets['conformal'] = get_prediction_set_labels(prediction_sets, class_labels)\nresults_sets['traditional'] = [class_labels[i] for i in y_pred]\nresults_sets.head(10)\n#&gt;    observed  conformal        traditional\n#&gt; 0  blue      {blue}           blue\n#&gt; 1  green     {green}          green\n#&gt; 2  blue      {blue}           blue\n#&gt; 3  green     {green}          green\n#&gt; 4  orange    {orange}         orange\n#&gt; 5  orange    {orange}         orange\n#&gt; 6  orange    {orange}         orange\n#&gt; 7  orange    {blue, orange}   blue\n#&gt; 8  orange    {orange}         orange\n#&gt; 9  orange    {orange}         orange\n\nconformity scores are calculated for each potential class using the predicted probabilities on the test set\nThe predicted class for an observation is determined by whether a class has a score below the threshold.\nTherefore, an observation may have 1 or more predicted classes or 0 predicted classes.\n\nStatistics (See Statistics section for functions)\n\nOverall\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n#&gt; Overall coverage: 0.947\n#&gt; Average set size: 1.035\n\nOverall coverage is very close to the target coverage of 95%, therefore, marginal coverage is achieved which is expected for this method\n\nPer Class\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n#&gt;         Class counts  Coverage   Average set size\n#&gt; blue    241           0.817427   1.087137\n#&gt; orange  848           0.954009   1.037736\n#&gt; green   828           0.977053   1.016908\n\nOverall coverage (i.e. for all labels) will be at or very near 95% but coverage for individual classes may vary.\n\nAn illustration of how this method lacks Conditional Coverage\nSolution: Get thresholds for each class. (See next example)\n\nNote that the blue class had substantially fewer observations that the other 2 classes.\n\n\n\nExample: LAC-adapted - Threshold per Class\n\nDon’t think {{mapie}} has this option.\nAlso possible do this for subgroups of data, such as ensuring equal coverage for a diagnostic across racial groups, if we found coverage using a shared threshold led to problems.\nCalculate individual class thresholds\n# Set alpha (1 - coverage)\nalpha = 0.05\nthresholds = []\n# Get predicted probabilities for calibration set\ny_cal_prob = classifier.predict_proba(X_Cal)\n# Get 95th percentile score for each class's s-scores\nfor class_label in range(n_classes):\n    mask = y_cal == class_label\n    y_cal_prob_class = y_cal_prob[mask][:, class_label]\n    s_scores = 1 - y_cal_prob_class\n    q = (1 - alpha) * 100\n    class_size = mask.sum()\n    correction = (class_size + 1) / class_size\n    q *= correction\n    threshold = np.percentile(s_scores, q)\n    thresholds.append(threshold)\nApply individual class thresholds to test set scores\n# Get Si scores for test set\npredicted_proba = classifier.predict_proba(X_test)\nsi_scores = 1 - predicted_proba\n\n# For each class, check whether each instance is below the threshold\nprediction_sets = []\nfor i in range(n_classes):\n    prediction_sets.append(si_scores[:, i] &lt;= thresholds[i])\nprediction_sets = np.array(prediction_sets).T\n\n# Get prediction set labels and show first 10\nprediction_set_labels = get_prediction_set_labels(prediction_sets, class_labels)\nStatistics\n\nOverall\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n#&gt; Overall coverage: 0.95\n#&gt; Average set size: 1.093\n\nSimilar to previous example\n\nPer Class\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n#&gt;         Class counts  Coverage   Average set size\n#&gt; blue    241           0.954357   1.228216\n#&gt; orange  848           0.956368   1.139151\n#&gt; green   828           0.942029   1.006039\n\nCoverages now very close to 95% and the average set sizes have increased, especially for Blue.\n\n\n\n\n\n\nContinuous\n\nConformalized Quantile Regression Process\n\nSplit data into Training, Calibration, and Test sets\n\nTraining data: data on which the quantile regression model learns.\nCalibration data: data on which CQR calibrates the intervals.\n\nIn the example, he split the data into 3 equal sets\n\nTest data: data on which we evaluate the goodness of intervals.\n\nFit quantile regression model on training data.\nUse the model obtained at previous step to predict intervals on calibration data.\n\nPIs are predictions at the quantiles:\n\n(alpha/2)*100) (e.g 0.025, alpha = 0. 05)\n(1-(alpha/2))*100) (e.g. 0.975)\n\n\nCompute conformity scores on calibration data and intervals obtained at the previous step.\n\nResiduals are calculated for the PI vectors\nScores are calculated by taking the row-wise maximum of both (upper/lower quantile) residual vectors (e.g s_i &lt;- pmax(lower_pi_res, upper_pi_res))\n\nGet 1-alpha quantile from the distribution of conformity scores (e.g threshold &lt;- quantile(s_i, 0.95)\n\nThis score value will be the threshold\n\nUse the model obtained at step 1 to make predictions on test data.\n\nCompute PI vectors (i.e. predictions at the previously stated quantiles) on Test set\ni.e. Same calculation as with the calibration data in step 2 where you use the model to predict at upper and lower PI quantiles.\n\nCompute lower/upper end of the interval by subtracting/adding the threshold from/to the quantile predictions (aka PIs)\n\nLower conformity interval: lower_pi &lt;- test_lower_pred  - threshold\nUpper conformity interval: upper_pi &lt;- test_upper_pred + threshold\n\n\nExample: Quantile Random Forest\nimport numpy as np\nfrom skgarden import RandomForestQuantileRegressor\n\nalpha = .05\n\n# 1. Fit quantile regression model on training data\nmodel = RandomForestQuantileRegressor().fit(X_train, y_train)\n\n# 2. Make prediction on calibration data\ny_cal_interval_pred = np.column_stack([\n    model.predict(X_cal, quantile=(alpha/2)*100), \n    model.predict(X_cal, quantile=(1-alpha/2)*100)])\n\n# 3. Compute conformity scores on calibration data\ny_cal_conformity_scores = np.maximum(\n    y_cal_interval_pred[:,0] - y_cal, \n    y_cal - y_cal_interval_pred[:,1])\n\n# 4. Threshold: Get 1-alpha quantile from the distribution of conformity scores\n#    Note: this is a single number\nquantile_conformity_scores = np.quantile(\n    y_cal_conformity_scores, 1-alpha)\n\n# 5. Make prediction on test data\ny_test_interval_pred = np.column_stack([\n    model.predict(X_test, quantile=(alpha/2)*100), \n    model.predict(X_test, quantile=(1-alpha/2)*100)])\n\n# 6. Compute left (right) end of the interval by\n#    subtracting (adding) the quantile to the predictions\ny_test_interval_pred_cqr = np.column_stack([\n    y_test_interval_pred[:,0] - quantile_conformity_scores,\n    y_test_interval_pred[:,1] + quantile_conformity_scores])\n\n\n\nStatistics\n\nAverage Set Size\n\nThe average number of predicted classes per observation since there can be more than 1 predicted class in the conformal PI\nExample:\n# average set size for each class\ndef get_average_set_size(prediction_sets, y_test):\n    average_set_size = []\n    for i in range(n_classes):\n        average_set_size.append(\n            np.mean(np.sum(prediction_sets[y_test == i], axis=1)))\n    return average_set_size   \n\n# Overall average set size (weighted by class size)\n# Get class counts\ndef get_class_counts(y_test):\n    class_counts = []\n    for i in range(n_classes):\n        class_counts.append(np.sum(y_test == i))\n    return class_counts\n\ndef get_weighted_set_size(set_size, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_set_size = np.sum((set_size * class_counts) / total_counts)\n    weighted_set_size = round(weighted_set_size, 3)\n    return weighted_set_size\n\nCoverage\n\nClassification: Percentage of correct classifications\nExample: Classification\n# coverage for each class\ndef get_coverage_by_class(prediction_sets, y_test):\n    coverage = []\n    for i in range(n_classes):\n        coverage.append(np.mean(prediction_sets[y_test == i, i]))\n    return coverage\n\n# overall coverage (weighted by class size)\n# Get class counts\ndef get_class_counts(y_test):\n    class_counts = []\n    for i in range(n_classes):\n        class_counts.append(np.sum(y_test == i))\n    return class_counts\n\ndef get_weighted_coverage(coverage, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_coverage = np.sum((coverage * class_counts) / total_counts)\n    weighted_coverage = round(weighted_coverage, 3)\n    return weighted_coverage\n\n\n\n\nVisualization\n\nConfusion Matrix\n\n\nBinary target where labels are 0 and 1\nInterpretation\n\nTop-left: predictions where both labels are not statistically significant (i.e. inside the “prediction interval”).\n\nThe model predicts both classes well since both labels have low scores.\nDepending the threshold, maybe the model could be relatively agnostic (e.g. predicted probabilites like 0.50-0.50, 0.60-0.40)\n\nBottom-right: predictions where both labels are statistically significant  (i.e. outside the “prediction interval”).\n\nModel totally whiffs. Confident it’s one label when it’s actually another.\n\nExample\n\n1 (truth) - low predicted probability = high score -&gt; Red and significant\n0 - high predicted probability = high score -&gt; Red and significant\n\n\n\nTop-right: predictions where all 0 labels are not statistically significant.\n\nModel predicted the 0=class well (i.e. low scores) but the 1-class poorly (i.e. high scores)\n\nBottom-left: predictions where all 1 labels are not statistically significant. Here, the model predicted that 1 is the true class.\n\nVice versa of top-right",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/cross-validation.html",
    "href": "qmd/cross-validation.html",
    "title": "Cross-Validation",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Cross-Validation"
    ]
  },
  {
    "objectID": "qmd/cross-validation.html#sec-cv-misc",
    "href": "qmd/cross-validation.html#sec-cv-misc",
    "title": "Cross-Validation",
    "section": "",
    "text": "Guide for suitable baseline models: link\nIf N &gt; 20,000 & &gt; 60 predictors, use k-fold (Harrell)\nIf N &gt; 20,000 & &lt; 60 predictors, use k-fold or bootstrap resampling (Harrell)\nIf N &lt; 20,000, use bootstrap resampling (Harrell)\nIf N &lt; 10,000, use nested-cv (Raschka)\n\nIf using NCV for algorithm selection and hyperparameter tuning, the prediction error calculation is the standard CV predicition error (used for the algorithm selection) is biased in certain situations. See K-Fold &gt;&gt; Bates, Hastie, Tibshirani 2021 &gt;&gt; CV inflation\nMe: The NCV calculation of the prediction error involves the folds of inner loop that are used in the hyperparameter tuning. It might not be mathematically kosher to still calculate the prediction error using NCV method while tuning but it might still produce a less biased result and better coverage than the CV method. It’s still the same algorithm just with minor tweaks.\n\nSplits\n\nIf &lt; 12 samples per predictor\n\nThe test partition should be no less than 10% of the sample\n\nIf you have enough samples for reasonable predictive accuracy as determined by the sample complexity generalization error,\n\nA 50% test partition size is fine.\n\nBetween these two boundaries, adjust the test size to limit the generalization test error in a tradeoff with training sample size (Abu-Mostafa, Magdon-Ismail, & Lin, 2012, pg. 57).\n\nVehtari (Paper)\n\nRe Modeling Assumptions:\n\nIf you can’t make any, then using CV (and WAIC) is appropriate even with the higher variance\nIf you can make assumptions, then you can reduce variance by examining directly the posterior or using reference models to filter out noise in the data (?) (see, e.g., Piironen, Paasiniemi and Vehtari (2018) and Pavone et al. (2020)).\n\nRe Number of Candidate Models\n\nFor a small number of models, performance bias at the model selection stage is usually negligible, that is, smaller than the standard deviation of the estimate or smaller than what is practically relevant.\nFor a large number of models, the performance bias at the model selection stage can be non-negligible, but this bias can be estimated using nested-CV or bootstrap\n\nThe paper reviews the concepts of selection-induced bias and overfitting, proposes a fast to compute estimate for the bias, and demonstrates how this can be used to avoid selection induced overfitting even when selecting among 10^30 models",
    "crumbs": [
      "Cross-Validation"
    ]
  },
  {
    "objectID": "qmd/cross-validation.html#sec-cv-kfold",
    "href": "qmd/cross-validation.html#sec-cv-kfold",
    "title": "Cross-Validation",
    "section": "K-Fold",
    "text": "K-Fold\n\nMisc\n\nAny preparation of the data prior to fitting the model occur on the CV-assigned training dataset within the loop rather than on the broader data set. This also applies to any tuning of hyperparameters. A failure to perform these operations within the loop may result in data leakage and an optimistic estimate of the model skill.\nIf the effective number of parameters of the model is much less than n, then with K&gt;10, the CV bias is usually negligible compared to the CV variance. (Vehtari)\n\nProcedure\n\nSplit the data into Train and Test sets\nWith the Train set\n\nShuffle the dataset randomly.\nSplit the dataset into k groups (aka folds)\nTrain Models\n\nChoose a fold as a validation dataset that hasn’t previously been chosen\nUse the remaining folds as a training dataset\nFit a model on the training set and calculate the performance metric score on the validation dataset\nRetain the performance metric score and discard the model\nRepeat until each fold has been the validation dataset\n\nSummarize the skill of the model using the set of performance metric scores\n\ne.g. take the mean of the performance metric scores on the validation folds\nPrediction error\n\\[\n\\widehat {\\text{Err}}_{cv} := \\bar e = \\frac{1}{n} \\sum_{i=1}^n e_i\n\\]\nStandard Error of Prediction Error\n\\[\n\\widehat{\\text{SE}} := \\frac{1}{\\sqrt{n}} \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (e_i - \\bar e)^2}\n\\]\nCI for prediction error\n\\[\n\\bar e \\pm z_{1-\\alpha/2} \\cdot \\hat{\\text{SE}}\n\\]\n\n\nRepeat for each model\nCompare models by their mean performance metric scores\nChoose best model and use it to predict on the Test set to get the out-of-sample error\n\nBates, Hastie, Tibshirani 2021\n\nThe estimand of CV is not the accuracy of the model fit on the data at hand, but is instead the average accuracy over many hypothetical data sets\nThe CV estimate of error has larger mean squared error (MSE) when estimating the prediction error of the final model than when estimating the average prediction error of models across many unseen data sets for the special case of linear regression\nCV inflation (the ratio of the true standard error of the point estimate compared to the CV estimate of standard error)\n\nThe number of folds has minimal impact on the inflation, although more folds gives moderately better coverage for small n.\nWe also find that even when n/p is less than/equal 20, there is appreciable inflation, and cross-validation leads to intervals with poor coverage\n\nCI for the prediction error\n\nUsing the variance to compute the width of the error interval does not account for the correlation between the error estimates in different folds, which arises because each data point is used for both training and testing\n\nEstimate of variance is too small and the intervals are too narrow\n\nCV has poor coverage until n &gt; 400.\nThe width of the NCV intervals relative to their CV counterparts—the usual ratio is not that large for samples sizes of n = 100 or greater.\nIt’s expected the standard CV intervals to perform better when n/p is larger and when more regularization is used\n\nRecommends Nested Cross-Validation to calculate the prediction error of the algorithm and the CI for that prediction error. (see details below)\nRegarding hyperparameter tuning and model selection\n\n“[35] suggests a bias correction for the model selected by cross-validation, [36] shows how to return a confidence set for the best model parameters, and [33, 18] show that selecting the best value of the tuning parameter is a statistically easier goal for CV than estimating the prediction error, in some sense.”\n\nSee the paper’s references for details on the papers being referred to.",
    "crumbs": [
      "Cross-Validation"
    ]
  },
  {
    "objectID": "qmd/cross-validation.html#sec-cv-ncv",
    "href": "qmd/cross-validation.html#sec-cv-ncv",
    "title": "Cross-Validation",
    "section": "Nested Cross-Validation (NCV)",
    "text": "Nested Cross-Validation (NCV)\n\n\nAKA Double Cross-Validation\nPackages\n\n{glmnetr} - Performs nested cv using models from ‘glmnet’, ‘survival’, ‘xgboost’, ‘rpart’ and ‘torch’. Fits relaxed Lasso and other models.\n\nHas many similarities to {glmnet}. Recommended that you read glmnet vignettes: “An Introduction to glmnet” and “The Relaxed Lasso”\n\n{nestedcv} - Implements nested k*l-fold cross-validation for lasso and elastic-net regularised linear models via the {glmnet} package and other machine learning models via {caret}. Cross-validation of ‘glmnet’ alpha mixing parameter and embedded fast filter functions for feature selection are provided.\n\nWhen you have a smallish dataset such that having sufficient sized (hold-out) test set is unfeasible, using normal k-fold cv for BOTH tuning and error estimation (algorithm comparison) produces biased error estimation.\n\nThe data used for training in some folds are used for testing in other folds which increases the bias of the average error that is calculated across the folds. This bias means the average error rate across the folds can’t be used as an estimate of the error rate of the model on an independent test set.\n\nRepeats should lower this bias and increase robustness in a nested cv framework so that both tuning and selection can be performed simultaneously. Number of repeats depends on how stable the results are.\nStandard deviations should be recorded along with mean scores on the outer loop. If the standard deviations widely vary, then more repeats may be required.\nIf repeats aren’t included, then the nested cv should only be used for algorithm comparison and a separate k-fold cv should be performed afterwards to find the optimal hyperparameters.\n\n\nUse a larger k, number of outer folds, for smaller sample sizes\n\nIn Kuhn’s example, he used 5 repeats of 10-fold cv with each fold comprised of 25 bootstrap resamples for just a 100 row dataset and the error estimate was pretty much balls on.\n\nKuhn used resamples for the inner-loop, but Raschka used folds. So either should be fine.\n\nSteps for algorithm selection:\n\nEntire nested cv procedure is executed for each candidate algorithm\nIf the algorithms make repeats too computationally intensive, then don’t do the repeats. After the algorithm is selected, perform a k-fold cv for hyperparameter tuning.\nFor each algorithm (using repeats):\n\nA complete grid search of all hyperparameters is computed in each inner-resample of each fold of each repeat\n\nIf we have 5 repeats of 10-fold cv with each fold comprised of 25 bootstrap resamples, then there will be 5 * 10 * 25 = 1250 grid search sessions completed\nIn a fold of a repeat, the mean score (e.g. RMSE) and standard deviation are calculated for each hyperparameter value combination across all the resamples’ assessment sets.\n\nThe hyperparameter value combination with the best mean score (e.g. lowest average RMSE) is selected\n\nRepeat for each fold in each repeat\n\nIn a fold of a repeat, the selected hyperparameter values are used to fit on the entire analysis set of the fold.\n\nThe fitted model is scored on the assessment set of the fold\nRepeat for each fold in each repeat\n\nCalculate the average score and standard deviation across all folds for every repeat (i.e. one score value)(e.g. average RMSE)\n\nWith 5 repeats of 10-fold cv:\n\\[\n\\overline {\\text{RMSE}} = \\frac{1}{50} \\sum_{\\text{fold}=1}^{50} \\text{RMSE}_{\\text{fold}}\n\\]\n\n\nThis score is what will be compared during algorithm selection\nThe standard deviations can be used to get a sense of the stability of the scores\n\nRepeat for each algorithm\nChoose the algorithm with the lowest average score\nFor the final model, use the hyperparameter combination chosen most frequently as best during the inner-loop tuning of the winning algorithm.\n\nBates, Hastie, Tibshirani 2021\n\n.632 Bootstrap intervals are typically, but not always, wider than the NCV intervals. The bootstrap point estimates are typically more biased that the NCV point estimates.\nFound that a large number (e.g., 200) of random splits (aka Repeats) of nested CV were needed to obtain stable estimates of the standard error\n“We anticipate that nested CV can be extended to give valid confidence intervals for the difference in prediction error between two models.”\n\nNo details given on this procedure. Not sure it’s as straightforward as differencing each CI endpoint or what.\n\nProcedure for the prediction error estimate and its CI\n\nPer Fold\n\n\\(e_{\\text{in}}\\) - vector of errors (i.e loss function values) from the calculations of the errors on all validation sets in the inner loop\n\ni.e. Each validation set has a vector of errors, so \\(e_{\\text{in}}\\) is essentially of vector of vectors that’s been coerced into 1 vector\n\n\\(e_{\\text{out}}\\) - Vector of errors from the validation set of the outer loop using a model trained on the entire training set of the fold\n\\(a_{\\text{fold}_i} = (\\bar{e}_{\\text{in}} − \\bar{e}_{\\text{out}})^2\\)\n\\(b_{\\text{fold}_i} = \\text{Var}(e_{\\text{out}})\\)\n\\(\\text{append}(a_{\\text{list}}, a_{\\text{fold}_i})\\)\n\\(\\text{append}(b_{\\text{list}}, b_{\\text{fold}_i})\\)\n\\(\\text{append}(\\text{es}, e_{\\text{in}})\\)\n\n\\(\\widehat{\\text{MSE}} = \\bar a_{\\text{list}} − \\bar b_{\\text{list}}\\)\n\nStandard error of the prediction error estimate\n\n\\(\\widehat{Err}_{\\text{NCV}} = \\overline{\\text{es}}\\)\n\nPrediction error estimate\n\nCompute Bias correction value\n\nFit a standard k-fold CV and compute \\(\\widehat{Err}_{\\text{NCV}}\\)\n\nGuessing K (# of folds) for NCV and CV are equal\n\\(\\widehat{\\text{Err}}_{\\text{cv}}\\) would be the average error on the validation folds\n\\[\n\\widehat {\\text{Err}}_{cv} := \\bar e = \\frac{1}{n} \\sum_{i=1}^n e_i\n\\]\n\nBias\n\\[\n\\widehat {\\text{bias}} := \\left(1 + \\frac{K-2}{K}\\right)\\left(\\widehat{Err}_{\\text{NCV}} - \\widehat{Err}_{\\text{CV}}\\right)\n\\]\n\nCI for the prediction error estimate\n\nRegression\n\\[\n\\widehat{\\text{Err}}_{\\text{NCV}} - \\widehat{\\text{bias}} \\pm q_{1-\\alpha /2} \\cdot \\sqrt{\\frac{K-1}{K}} \\cdot \\sqrt{\\widehat{\\text{MSE}}}\n\\]\n\nThe \\(\\sqrt{\\frac{K-1}{K}} \\cdot \\sqrt{\\widehat{\\text{MSE}}}\\) term needs to be restricted to be between \\(\\widehat{\\text{SE}}\\) and \\(K \\cdot \\widehat{\\text{SE}}\\)\nK is the number of folds\n\nFor binary classification using 0/1 loss\n\\[\n\\sin^{-1}\\left(\\sqrt{\\widehat{\\text{Err}}_{\\text{NCV}}}\\right)\\pm z_{1-\\alpha/2} \\cdot \\frac{\\sqrt{\\widehat{\\text{MSE}}}}{\\widehat{\\text{SE}}} \\cdot \\sqrt{\\frac{1}{4n}}\n\\]\n\nn is the sample size\nWhere \\(\\widehat{\\text{SE}}\\) is the estimate of the width of the confidence interval using standard k-fold CV\n\n\\[\n\\widehat{\\text{SE}} := \\frac{1}{\\sqrt{n}} \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (e_i - \\bar e)^2}\n\\]\nNot sure if this would be viable for some other loss function like cross-entropy loss or not. See Appendix E for a few more details.\n\n\n\nNotes\n\nNo initial split where there’s a hold out set that doesn’t go through the ncv procedure\nPaper acknowledges that hyperparameter tuning might be useful for ncv, but says they’re not getting into it in this paper.",
    "crumbs": [
      "Cross-Validation"
    ]
  },
  {
    "objectID": "qmd/cross-validation.html#sec-cv-swcv",
    "href": "qmd/cross-validation.html#sec-cv-swcv",
    "title": "Cross-Validation",
    "section": "Sliding Window Cross-Validation",
    "text": "Sliding Window Cross-Validation\n\n\nInstead of each fold using a different block of observations for the validation/test set, each successive fold slides the interval of the target variable interval (e.g. 3 weeks) from the previous fold\nSplit the data before creating the folds to avoid leakage.\nExample: Classification\n\nSee Algorithms, Marketing &gt;&gt; Propensity Model\nFrom Scoring Customer Propensity using Machine Learning Models on Google Analytics Data\nEach row is a customer\nEach fold\n\nFeature values are values that have been aggregated over a 3 month window for each customer\nTarget variable is purchase/no purchase which is detected over the next 3 weeks\n\n\nExample: Multi-step time series forecasting with XGBoost\n\nCodes a sliding-window (don’t think it’s a cv approach though) and forecasts each window with the model using the Direct method\nNot exactly sure how this works. No helpful figs in the article or the paper if follows, so would need to examine the code",
    "crumbs": [
      "Cross-Validation"
    ]
  },
  {
    "objectID": "qmd/cross-validation.html#sec-cv-ts",
    "href": "qmd/cross-validation.html#sec-cv-ts",
    "title": "Cross-Validation",
    "section": "Time Series",
    "text": "Time Series\n\nMisc\n\nUsually, the forecast horizon (prediction length), H, equals the number of time periods in the testing data\nAt a minimum, the training dataset should contain at least three times as many time periods as the testing dataset\nIn time series model comparison, CV variance is likely to dominate, and it is more important to reduce the variance than bias. Leave-Few-Observations with joint log score is better than use of Leave-Future-Out (LFO). Cooper et al. (2023) (Vehtari)\n\nPaper: Evaluating time series forecasting models: An empirical study on performance estimation methods\n\nCode in R\nAlgorithm used: rule-base regression system Cubist (trees)\n\n“Other learning algorithms were tested, namely the lasso and a random forest. The conclusions drawn using these algorithms are similar to the ones reported in the next sections.”\nData\n\nEach of the 62 time series had lengths of either 949, 1338, or 3000.\nHalf-Hourly, Hourly, and Daily Frequencies\nIndustries: bike sharing, stock prices from aerospace companies, air quality, solar radiation, and water consumption\n\n\nTypes\n\n\n(Blue) Training set (including validation fold); (Orange) Test set; (White) Unused\nCV Methods\n\nCV (Standard): randomized K-fold cross-validation (4)\nCV-Bl: Blocked K-fold cross-validation (6)\nCV-Mod: Modified K-fold cross-validation (5)\nCV-hvBl: hv-Blocked K-fold cross-validation. Tries to increase the independence between training and validation by introducing a gap between the two samples (7)\n\nOut-of-Sample Methods\n\nHoldout (vice versa Inverse Holdout): A simple OOS approach–the first 70% is used for training and the subsequent 30% is used for testing (1, 2)\nRep-Holdout: OOS tested in nreps testing periods with a Monte Carlo simulation using 70% of the total observations t of the time series in each test. For each period, a random point is picked from the time series. The previous window comprising 60% of t is used for training and the following window of 10% of t is used for testing. (3)\nPreq-Bls: Prequential evaluation in blocks in a growing fashion (8)\nPreq-Sld-Bls: Prequential evaluation in blocks in a sliding fashion–the oldest block of data is discarded after each iteration (9)\nPreq-Bls-Gap: Prequential evaluation in blocks in a growing fashion with a gap block– this is similar to the method above, but comprises a block separating the training and testing blocks in order to increase the independence between the two parts of the data (10)\nPreq-Grow and Preq-Slide: An observation is first used to test the predictive model and then to train it. Method can incoroporate either a growing/landmark window (Preq-Grow) or a sliding window (Preq-Slide). (11, 12)\n\nBest Performing (i.e. cv error closest hold-out error) (i.e better estimation of generalized performance)\n\nRep-Holdout - Repeated holdout. It’s similar to holdout, but it consists of n estimates. Moreover, differently from holdout, not all the observations are used in a single estimate. A block (e.g. 70%) of the dataset is randomly selected, the first part of that block is used for training and the subsequent part for testing. This procedure is repeated n (e.g. 5) times\n\nVaried length of dataset from a size of 100 to a size of 3000, by intervals of 100 (100, 200, …, 3000). The experiments did not provide any evidence that the size of the synthetic time series had a noticeable effect on the error of estimation methods.\nIn a stationary setting the cross-validation variants are shown to have a competitive estimation ability. However, when non-stationarities are present, they systematically provide worse estimations than the out-of-sample approaches.\nBest method based on summary stats of time series\n\n\nSummary Stats used in the Decision Tree (Tree only used 4 of the stats as splits)\n\nSkewness: For measuring the symmetry of the distribution of the time series;\n5th and 95th Percentiles (Perc05, Perc95) of the standardized time series;\nAcceleration (Accel.): As the average ratio between a simple moving average and the exponential moving average;\nInter-Quartile Range (IQR): as a measure of the spread of the standardized time series;\nSerial Correlation - Estimated using a Box-Pierce test statistic;\nLong-Range Dependence: A Hurst exponent estimation with wavelet transform;\nMaximum Lyapunov Exponent: A measure of the level of chaos in the time series;\nStationary: A boolean variable, indicating whether or not the respective time series is stationary according to the wavelet spectrum test\n\nInterpretation: Rep-Holdout, which is the best method most of the time across the 62 time series, should be used unless the acceleration is below 1.2. Otherwise, the tree continues with more tests in order to find the most suitable estimation method for each particular scenario.\n\n\n\nArticle: 12 Ways to Test Your Forecasts like A Pro\n\nCode in Python\nAlgorithm used: Linear Regression with lags and day-of-the-week variable.\nData: Hyndman datasets - “TSDL consists of 648 datasets, but I selected only the univariate time-series with at least 1,000 observations.” (Turns out to be around 58 datasets I think)\nExperiment: 90% used to perform cv method (cv error); model trained on 90% and tested on the 10% hold-out (hold-out error)\nBest Performing (i.e. cv error closest hold-out error) (i.e better estimation of generalized performance)\n\nInverse holdout: (13 wins) It’s the opposite of holdout: the latest part of the data (usually 75% or 80%) is used for training and the preceding part is used for testing.\npreq_grow: (7 wins) Prequential growing window. n iterations are carried out. At the first iteration, the test set is made of the latest part of observations (e.g. the latest 20%), whereas the first part is used for training. In the following iterations, the test set dimension is progressively reduced, and all the remaining observations are used for training.\npreq_sld_bls: (7 wins) Prequential sliding blocks. Each fold is made only of adjacent points. For each model, one fold is used for training and the subsequent fold is used for testing.\n\n\nExpanding Window or Forward Chaining or Online Training\n\n\nSimulates production environment\npreq-grow method above\n{{sklearn.model_selection::TimeSeriesSplit}}\n\nSee How-To: Cross Validation with Time Series Data for coded example\n\n\nBlocked Split\n\n\nOverlapping folds might lead to overly optimistic scores given models are being trained on some of the same data in successive folds.\n\nProposed as solution to the potential scoring bias of the expanding window method\n\nSame as preq-sld-bls above except that the train/test folds don’t overlap\n\nIf the test set overlaps with next training fold, it shouldn’t have the issue that this method is remedying.\n\ni.e. preq-sld-bls code should be fine, if you’re worried about CV score inflation.\n\n\nSee Reduce Bias in Time Series Cross Validation with Blocked Split for a coded example",
    "crumbs": [
      "Cross-Validation"
    ]
  },
  {
    "objectID": "qmd/css-recipes.html",
    "href": "qmd/css-recipes.html",
    "title": "Recipes",
    "section": "",
    "text": "Fit div to text length and center div in page\n\n\nh4, .h4 {\n  border-left: 4px solid #dee2e6;\n  padding-left: 20px;\n  border-right: 4px solid #dee2e6;\n  padding-right: 3px;\n  width: -moz-max-content;\n  width: -webkit-max-content;\n  width: max-content;\n  margin: 0 auto;\n}\n\nI wanted to create the effect with 2 bars on both sides of the text. Since the container stretched across the page, the right bar would be at the edge of the page.\n\nCan also do other things like creating a box or an effect around the text and not across the page\n\nwidth: max-content dynamically adjusts the length of the container to the length of the content\n\nThe multiple width attributes are so this works for different browsers (min-content works for Chrome).\nSee Understanding min-content, max-content, and fit-content in CSS for explanation of other similar attributes\n\nmargin: 0 auto positions the container in the center of the page\n\nNavbar Styling\n.navbar-inverse {\n  background-color: #000000;\n  border-color: #000000;\n  font-family: 'Roboto', sans-serif;\n}\nImport Font\n@import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,400;0,700;1,400&display=swap');\n\nCopied from google fonts site",
    "crumbs": [
      "CSS",
      "Recipes"
    ]
  },
  {
    "objectID": "qmd/diagnostics-bayes.html",
    "href": "qmd/diagnostics-bayes.html",
    "title": "Bayes",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Diagnostics",
      "Bayes"
    ]
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-misc",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-misc",
    "title": "Bayes",
    "section": "",
    "text": "Prior sensitivity analysis\n\n{priorsense}\n\nVideo, Thread",
    "crumbs": [
      "Diagnostics",
      "Bayes"
    ]
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-cbpp",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-cbpp",
    "title": "Bayes",
    "section": "Correlations Between Parameter Posteriors",
    "text": "Correlations Between Parameter Posteriors\n\nCorrelated parameters and their uncertainties will co-vary within the posterior distribution\n\ne.g. High intercepts will often mean high slopes\nCentering/standardization of predictors can remove correlation between parameters\n\nWithout independent parameters\n\nParameters can’t be interpreted independently\nParameter effects on prediction aren’t independent\n\nbrms::pairs(model_fit) (SR Ch 4)\n\nExample: SR Ch 8,9\n\npost &lt;- posterior_samples(mod_obj)\npost %&gt;%\n  select(-lp__ ) %&gt;%\n  ggally::ggpairs()\n\nIgnore first “b_”; no idea why that got added\na_cid1 is the intercept for factor variable, cid = 1\nb_cid1 is the slope for the predictor variable, geological ruggedness, when cid = 1\nSlope and intercept conditional on cid = 1 has the highest correlation at 0.174",
    "crumbs": [
      "Diagnostics",
      "Bayes"
    ]
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-conv",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-conv",
    "title": "Bayes",
    "section": "Convergence",
    "text": "Convergence\n\nMetrics\n\nMisc\n\nNotes from\n\nRank-normalization, folding, and localization: An improved Rˆ for assessing convergence of MCMC\n\nLots of detailed convergence analysis examples\n\n\nbayestestR::diagnostic_posterior has “ESS”, “Rhat”, “MCSE”\n\nAccepts rstanarm, brms models\n\nValues potentially indicate multimodal distribution (Vehtari, Thread)\n\n\n“Chain stacking might help, but would need to know more about the posterior to be more confident on recommendation”\n\n\nRhat - Gelman-Rubin convergence diagnostic\n\nEstimate of the convergence of Markov chains to the target distribution\n\nChecks if the start and end of each chain explores the same region\nChecks that independent chains explore the same region\n\nCan require long chains to work well\n** This diagnostic can fail for more complex models (i.e. bad chains even when value = 1) **\n\nNew metric called R* might be better (docs) but there’s aren’t any guidelines on the values, so probably just useful for model comparison for now.\n\nRatio of variances\n\nAs total variance among all chains shrinks to the average variance within chains, R-hat approaches 1\nIf converges, Rhat = 1+\n\nGuideline\n\nIf value is above 1.00, it usually indicates that the chain has not yet converged, and probably you shouldn’t trust the samples.\n\nSolution\n\nIf you draw more iterations, it could be fine, or it could never converge.\n\n\n\n\nAutocorrelation Metrics\n\nMarkov chains are typically autocorrelated, so that sequential samples are not entirely independent.\nEffects of Autocorrelation\n\nCan be an indicator of non-convergence\nIncreases uncertainty (standard errors)\nWhen chains have high autocorrelation, they can get stuck in regions of the parameter space making the sampling inefficient.\n\nI understand this to mean that less of the parameter space gets sampled\n\n\nSolutions\n\nIf you get warnings, taking more samples usually helps\nIncreasing max tree depth helps if max tree depth is continually being reached\n\nMCMCvis::MCMCdiag(fit, round = 2) produces diagnostics and shows sampler settings your model\n\nAccepts rstan, nimble, rjags, jagsUI, R2jags, rstanarm, and brms model objects\n\n\n\nEffective Sample Size (ESS)\n\nMeasures the amount by which autocorrelation in samples increases uncertainty (standard errors) relative to an independent sample.\n\nTells you how many samples the chain would have if there was 0 autocorrelation between samples in the chain\n\nMore autocorrelation means fewer effective number of samples.\n\nGuidelines for all ESS Metrics (tail or bulk)\n\nLarger is better\nBad: ESS &lt; 400 indicates convergence problems  (Vehtari)\nOkay: ESS ≈ 800 corresponds to low relative efficiency of 1% (Vehtari)\nGood: ESS &gt; 1000 is sufficient for stable estimates (Bürkner, 2017)\nVery Good: ESS ≥ iteration amount\n\nGreater than means that something called anti-correlation is going on which is good\n\nExample: 2000 total samples with 1000 of those used for warm-up which is brms default. 4 chains x 1000 samples = 4000 post-warm-up samples. So for each parameter, the ESS should be around that or above\n\n\nn_eff in {rethinking} precis output\n\nSame as Bulk_ESS\n\nBulk_ESS - effective sample size around the bulk of the posterior (i.e. around the mean or median) (same as McElreath’s n_eff)\n\n“assesses how well the center of the distribution is resolved”\n\ni.e. Measures how well HMC sampled the posterior around the bulk of the distribution in order to determine its shape.\n\nExample: Summary will give ESS stats\nas_draws_rvars(brms_fit) %&gt;%   \n    summarize_draws()\nvariable mean median   sd mad   q5 q95  rhat ess_bulk ess_tail \nlp__       -38.56  -38.20 1.30 1.02  -41.09  -37.21    1    1880    2642 \nalpha_c         9.32 9.32 0.14 0.14 9.09 9.55    1    3364    2436 \nbeta         0.02 0.02 0.01 0.01 0.01 0.03    1    3864    2525 \nsigma         1.12 1.11 0.10 0.10 0.97 1.29    1    3014    2776\nExample: Select variable and ESS Values for Quantiles\nas_draws_rvars(brms_fit) %&gt;%\n  subset_draws(\"beta100\") %&gt;%\n  summarize_draws(ess_mean, ~ess_quantile(.x, probs = c(0.05, 0.95)))\nvariable  ess_mean  ess_q5  ess_q95\nbeta100      3816    2525    3153\n\nThese are ESS values for\n\nThe summary estimate (aka point estimate) which is the mean of the posterior in this case\nAnd the CI values of that summary estimate\n\nSo it makes sense you’d have lower numbers of effective samples in the tails of the posterior than in the bulk since it’s going to get sampled less than the bulk\n\n\nTail_ESS - effective sample size in the tails of the posterior\n\nmeasures how well HMC sampled the posterior in the tails of the distribution in order to determine their shape.\n\n\nAutocorrelation plots for chains\n\npost &lt;- posterior_samples(mod_obj)\npost %&gt;% \nmcmc_acf(pars = vars(b_a_cid1:sigma),\n      lags = 5) +\ntheme_pomological_fancy(base_family = \"Marck Script\")\n\nExample from Ch 8,9 Statistical Rethinking\nL-shaped autocorrelation plots like these are good.\n\nThose are the kinds of shapes you’d expect when you have reasonably large effective samples.",
    "crumbs": [
      "Diagnostics",
      "Bayes"
    ]
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-mix",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-mix",
    "title": "Bayes",
    "section": "Mixing",
    "text": "Mixing\n\nPosteriors and Trace Plots\nbrms::plot(mod_obj)\n\nExample: Ch 8,9 Statistical Rethinking\n\n\nSee above for parameter descriptions\nTrace plots with fat, lazy caterpillars like these are good\n\n\nTrank Plot (Trace Rank Plot)\n\npost &lt;- posterior_samples(b9.1b, add_chain = T)\npost %&gt;% \n  bayesplot::mcmc_rank_overlay(pars = vars(b_a_cid1:sigma)) +\n      scale_color_pomological() +\n      ggtitle(\"My custom trank plots\") +\n      coord_cartesian(ylim = c(25, NA)) +\n      theme_pomological_fancy(base_family = \"Marck Script\") +\n      theme(legend.position = c(.95, .2))",
    "crumbs": [
      "Diagnostics",
      "Bayes"
    ]
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-ppdc",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-ppdc",
    "title": "Bayes",
    "section": "Posterior Predictive Check",
    "text": "Posterior Predictive Check\n\nExample:\n\nBad Fit\n\nbrms::pp_check(model, nsamples = 100) + xlim(0, 20)\nGood Fit\n\n\nWith {bayesplot}\n\nbayesplot::ppc_rootogram(y = testdata$observedResponse,\n              yrep = posterior_predict(model4, nsamples = 1000)) +\n  xlim(0, 20)",
    "crumbs": [
      "Diagnostics",
      "Bayes"
    ]
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-gof",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-gof",
    "title": "Bayes",
    "section": "GOF Plots",
    "text": "GOF Plots\n\nMisc\n\nAlso see Statistical Rethinking, Chapter 5 &gt;&gt; Inferential Plots\n\nPredictor Residual, Counterfactual Plots, and Posterior Predictive Plots\n\n\nData with regression line\n\n# scatter plot of observed pts with the regression line from the model\n# Defined the by the alpha and beta estimate\ndat %&gt;%\n  ggplot(aes(x = weight, y = height)) +\n  geom_abline(intercept = fixef(b4.3)[1],\n              slope    = fixef(b4.3)[2]) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\nPredicted vs Observed with PI and CI around regression line\n\n# x value (weight) range we want for the CI of the line\nweight_seq &lt;- tibble(weight = seq(from = 25, to = 70, by = 1))\n\n# predicted values (height) for each x value\n# 95% CIs generated by default\nmu_summary &lt;-  fitted(b4.3, newdata = weight_seq) %&gt;%\n    as_tibble() %&gt;%\n    # let's tack on the `weight` values from `weight_seq`\n    bind_cols(weight_seq)\n\n# 95% PIs generated by default\npred_height &lt;-  predict(b4.3,\n      newdata = weight_seq) %&gt;%\n    as_tibble() %&gt;%\n    bind_cols(weight_seq)\n\n# includes regression line, CI, and PI\ndat %&gt;%  ggplot(aes(x = weight)) +\n  # PIs\n  geom_ribbon(data = pred_height, aes(ymin = Q2.5,\n              ymax = Q97.5), fill = \"grey83\") +\n  # CIs\n  geom_smooth(data = mu_summary, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\", fill = \"grey70\", color = \"black\",\n              alpha = 1, size = 1/2) +\n  geom_point(aes(y = height), color = \"navyblue\", shape = 1,\n            size = 1.5, alpha = 2/3) +\n  coord_cartesian(xlim = range(d2$weight),\n                  ylim = range(d2$height)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())",
    "crumbs": [
      "Diagnostics",
      "Bayes"
    ]
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-resid",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-resid",
    "title": "Bayes",
    "section": "Residuals",
    "text": "Residuals\n\n{DHARMa}\n\nVignettes:\n\nDHARMa for Bayesians\nIntroduction to DHARMa\n\nExample:\n\nBad Fit\n\npacman::p_load(brms, DHARMa)\n\nmodel.check &lt;- createDHARMa(\n  simulatedResponse = t(posterior_predict(model)),\n  observedResponse = testdata$observedResponse,\n  fittedPredictedResponse = apply(t(posterior_epred(model)), 1, mean),\n  integerResponse = TRUE)\nplot(model.check)\n\nHierarchical dataset fit with a poisson model. Everything is bad in this fit.\n\nGood Fit\n\n\nHierarchical dataset fit with a hierarchical negative binomial model.\n\n\n\nExampleGroup Variation Check\n\n    plot(model.check, form = testdata$group)\n\nThis is hierarchical dataset fit with a hierarchical poisson model. The within-group box-plots show that model has captured the group variance sufficiently as both tests have non-significant (n.s.) results.\n\nExample: Overdispersion\n\nBad fit\n\n    testDispersion(model.check)\n\nThis is hierarchical dataset fit with a hierarchical poisson model. In the previous chart (left panel), the overdispersion test failed. This histogram shows how much the model is off.\n\nGood Fit",
    "crumbs": [
      "Diagnostics",
      "Bayes"
    ]
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-pa",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-pa",
    "title": "Bayes",
    "section": "Predictive Accuracy",
    "text": "Predictive Accuracy\n\nMisc\n\nSee Statistical Rethinking, Chapter 7 for details\nloo package website has some nice CV workflows\n\nAlso ensembling, time series\n\nDon’t compare models with different numbers of observations (SR, Ch 11)\n\ne.g. 1/0 logistic regression model vs aggregated logistic regression model\n\nPSIS-LOO (IS-LOO, WAIC, etc) has difficulties if each observation has their own parameter(s) (aka “random effects”) (Vehtari thread + post)\n\nModel Comparison\n\nMcElreath: To judge whether two models are “easy to distinguish” (i.e. kinda like whether their scores are statistically different), we look at the differences between the model with the best WAIC and the WAICs of the other models along with the standard error of the difference of the WAIC scores\n\nbrms::loo_compare(loo_obj, loo_obj)\n\nWhere a “loo_obj” is a brms::loo(fit) or brms::waic(fit) object\nCan also take a list of loo objects\nsimplify = FALSE gives a more detailed summary\n\nIf the difference in ELPD is much larger or several times the estimated standard error of the difference, then the top model is expected to have better predictive performance\n\n\nPareto-Smoothed Importance Sampling Cross-Validation (PSIS)\n\nWeights observations based on influence on the posterior\nUses highly influential observations to formulate a pareto distribution and sample from it\nbrms::loo - wrappers for loo::loo (docs)\nEstimates out-of-sample LOO-CV lppd\n\n{loo}\n\n“elpd_loo” - Larger is better\n“looic” - is just (-2 * elpd_loo) to convert it to the deviance scale, therefore smaller is better\nMay need to use add_criterion(brms_fit, \"loo\") in order to use the loo function\n\n{Rethinking}: Smaller is better\n\nThe shape parameter of the distribution, k, is estimated. When k &gt; 0.5, then the distribution has infinite variance. PSIS weights perform well as long as k &lt; 0.7. Large k values can be used to identify influential observations (i.e. rare observations/potential outliers).\n\nFor brms, warnings for high k values will show when using add_criterion(brms_mod, \"loo\") \nOutliers make it tough to estimate out-of-sample accuracy, since rare values are unlikely to be in the new sample. (i.e. overfitting risk)\nAlso, warnings about high k values can occur when the sample size is small\n\nWhen looking at the posterior, keep in mind that “influential” data values might be significantly affecting the posterior distribution.\n\nSolutions\n\nIf there are only a few outliers, and you are sure to report results both with and without them, dropping outliers might be okay.\nIf there are several outliers, then a form of Robust Regression can be used or a Mixture Model.\n\nCommon to use a Student’s T distribution instead of a Gaussian for the outcome variable specification\n\nThe Student-t distribution arises from a mixture of Gaussian distributions with different variances. If the variances are diverse, then the tails can be quite thick.\nHas an extra shape parameter, ν, that controls how thick the tails are.\n\nν = ∞ is a Gaussian distribution\nAs v –&gt; 1+ , tails start becoming fat\nν can be estimated with very large datasets that have plenty of rare events\n\n\n\n\nExample\n# shows k values for all data points below 0.5 threshold\nloo::loo(b8.3) %&gt;% \n  plot()\n# K values\ntibble(k = b8.3$criteria$loo$diagnostics$pareto_k, \n      row = 1:170) %&gt;% \n  arrange(desc(k))\n# k value diagnostic table - shows how many are points have bad k values and that group's min n_eff\nloo(b8.3) %&gt;% loo::pareto_k_table()\n\n\nWidely Applicable Information Criterion (WAIC)\n\nDeviance with a penalty term based on the variance of the outcome variable’s observation-level log-probabilities from the posterior\nEstimates out-of-sample deviance\n\n{loo}: brms::waic\n\n“elpd_waic”: Larger is better\n“waic”: is just (-2 * elpd_waic) to convert it to deviance scale, therefore smaller is better\nMay need to use add_criterion(brms_fit, \"waic\") in order to use the waic function\n\n{Rethinking}: Smaller is better\n\nEffective number of parameters, pwaic (aka the penalty term or overfitting penalty)\n\nSays compute the variance in log-probabilities for each observation i, and then sum up these variances to get the total penalty.\nCalled such because in ordinary linear regressions the sum of all penalty terms from all points tends to be equal to the number of free parameters in the model\n\nWhen the sum is larger than the number of free parameters, it can indicate an outlier is present which will increase the overfitting risk.\n\nSee Solutions for outliers under PSIS &gt;&gt; Shape parameter, k\n\n\nWeights\n\nThese weights can be a quick way to see how big the differences are among models.\nEach model weight is essentially a proportion of it’s WAIC or PSIS difference compared to the total of all the WAIC or PSIS differences.\n\nLarger is better\n\nExample\nbrms::model_weights(b8.1b, b8.2, b8.3, weights = \"loo\") %&gt;%\n        round(digits = 2)\n## b8.1b  b8.2  b8.3 \n##  0.00  0.03  0.97\n\nInterpretation:\n\nb8.3 has more than 95% of the model weight. That’s very strong support for including the interaction effect, if prediction is our goal.\nThe modicum of weight given to b8.2 suggests that the posterior means for the slopes in b8.3 are a little overfit.\n\n\n\nBayes Factor\n\nThe ratio (or difference when logged) of the average likelihoods (the denominator of bayes theorem) of two models.\nSince the average likelihood has been averaged over the priors, it has a natural penalty for more complex models\nProblems\n\nEven when priors are weak and have little influence on posterior distributions within models, priors can have a huge impact on comparisons between models.\nNot always possible to compute the average likelihood",
    "crumbs": [
      "Diagnostics",
      "Bayes"
    ]
  },
  {
    "objectID": "qmd/diagnostics-classification.html",
    "href": "qmd/diagnostics-classification.html",
    "title": "Classification",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Diagnostics",
      "Classification"
    ]
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-misc",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-misc",
    "title": "Classification",
    "section": "",
    "text": "Also see Diagnostics, Regression &gt;&gt; Residuals\nTwo main types of quantities to validate (Harrell RMS Ch.5)\n\nCalibration (aka reliability): ability to make unbiased estimates of response (Y^ vs. Y)\nDiscrimination: ability to separate responses\n\nBinary logistic model: e.g. AUROC\n\n(more optional) Centrality of errors: e.g. Brier Score",
    "crumbs": [
      "Diagnostics",
      "Classification"
    ]
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-terms",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-terms",
    "title": "Classification",
    "section": "Terms",
    "text": "Terms\n\nCalibration - When the predicted probabilities from a model match the observed distribution of probabilities for each class. A well-calibrated model is one that minimizes residuals.\nCalibration-in-the-Large - mean calibration\n\nHarrell: “compares the average Y-hat with the average Y (does the model get the right result on average).”\n\naverage predicted risk compared with the overall event rate\n\nConcerned with gross measurements of calibration, such as:\n\nWhether the model’s overall expected number of events exceeds the observed number\nWhether the proportion of expected over observed events departs significantly from “1”\n\n\nCalibration-in-the-Small - means that each of your model’s predicted probabilities, i.e. 0.01, 0.02, …, 0.99, occur exactly at that proportion in the observed data/real world\n\ni.e., for all the times when the predicted risk was 0.4, the outcome happened about 0.4 of the time.\nMore important than calibration-in-the-large. Required for good decision making using risk models.\nHarrell: Assesses the absolute forecast accuracy at the individual levels of Y-hat.\nSmooth calibration curves visualizes this (See Calibration &gt;&gt; Calibration Plots &gt;&gt; Harrell)\n\nA curve close to the diagonal indicates that predicted risks correspond well to observed proportions\n\n“calibration-in-the-tiny” the next step: for males in which the prediction was 0.4 was the outcome present 0.4 of the time, then for females.\n\nDiscrimination - The ability of the predictive model to predict the observed class. It’s not a particular performance measure. Typically AUROC, Somers’ Dxy\n\nPredictive discrimination is the ability to rank order subjects\nExample: The higher the ratio of patients who have high predicted risk probabilities / patients who actually have the disease, the better the discrimination by the model\n\n“High” is determined by a cutpoint or threshold\n\n\nExternal Validity - The extent to which your results can be generalized to other contexts. Assesses the applicability or generalizability of the findings to the real world. So, your study had significant findings in a controlled environment. But will you get the same results outside of the lab?\nInternal Validity - The degree of confidence that the causal relationship you are testing is not influenced by other factors or variables. Evaluates a study’s experimental design and methods. Studies that have a high degree of internal validity provide strong evidence of causality.\n\nUsing CV or bootstrapping is sometimes referred to as performing internal validation\n\nProper Scoring Rule (SO discussion + Harrell links) - A score assesses whether a probabilistic forecast is close to the true distribution. A score that is minimized in expectation if the predictive density is the true density.\n\nLoss functions that map predicted probabilities and corresponding observed outcomes to loss values, which are minimized in expectation by the true probabilities (p,1−p). The idea is that we take the average over the scoring rule evaluated on multiple (best: many) observed outcomes and the corresponding predicted class membership probabilities, as an estimate of the expectation of the scoring rule\nAlso\n\nStrictly proper scoring rules -rules that are only minimized in expectation if the predictive density is the true density\nImproper scoring rules - (e.g. Accuracy)",
    "crumbs": [
      "Diagnostics",
      "Classification"
    ]
  },
  {
    "objectID": "qmd/diagnostics-classification.html#confusion-matrix-statistics",
    "href": "qmd/diagnostics-classification.html#confusion-matrix-statistics",
    "title": "Classification",
    "section": "Confusion Matrix Statistics",
    "text": "Confusion Matrix Statistics\n\nFalse Discovery Rate (FDR)\n\\[\nFDR = \\frac{FP}{TP + FP} = 1 − PPV\n\\]\n\nExpected ratio of the number of false positive classifications (false discoveries) to the total number of positive classifications (rejections of the null)\nFDR-controlling procedures provide less stringent control of Type I errors compared to family-wise error rate (FWER) controlling procedures (such as the Bonferroni correction), which control the probability of at least one Type I error.\n\nThus, FDR-controlling procedures have greater power, at the cost of increased numbers of Type I errors.\n\n\nFalse Positive Rate (FPR)\n\\[\nFPR = 1 - TNR = \\frac{FP}{FP + TN}\n\\]\nFalse Negative Rate (FNR)\n\\[\nFNR = 1 - TPR = \\frac{FN}{FN + TP}\n\\]\nNegative Likelihood Ratio (LR-)\n\\[\nLR_- = \\frac{FNR}{TNR}\n\\]\nNegative Predictive Value (NPV)\n\\[\nNPV = \\frac{TN}{TN + FN}\n\\]\n\n\\(PPV\\) and \\(NPV\\) are useful after the result of the test in know (unlike Sensitivity and Specificity which are useful before the results are known)\nBoth \\(PPV\\) and \\(NPV\\) are useful GoF metrics since they describe performance on future data.\n{yardstick}\n\nPositive Likelihood Ratio (LR+)\n\\[\nLR_+ = \\frac{TPR}{FPR}\n\\]\nPrecision or True Positive Rate (TPR) or Positive Predictive Value (PPV)\n\\[\n\\mbox{Precision} = \\frac{TP}{TP + FP}\n\\]\n\nThe fraction of observations that the model identified as positive that were correct (i.e. the fraction of all the actual positives (\\(TP+FN\\)) that you model labels as positives.)\nFocuses on Type-I error (\\(FP\\))\nInterpretation: 1 is perfect, &lt; 0.5 is low\nMaximize when \\(FPs\\) are more costly\n\nExamples\n\nAn event is mistakenly detected that results in a high value customer getting their account suspended.\nYou want to reduce the number of unsuccessful marketing calls that the model predicts will be conversions.\n\n\n\nPrevalence\n\\[\n\\mbox{Prevalence} = \\frac{P}{P+N}\n\\]\n\nProportion of a particular population found to be affected by a medical condition (typically a disease or a risk factor such as smoking or seatbelt use) at a specific time\nPrevalence Threshold: The prevalence level below which a test’s positive predictive value (PPV) declines most sharply relative to disease prevalence - and thus the rate of false positive results/false discovery rate increases most rapidly.\n\nRecall or Sensitivity\n\\[\n\\mbox{Recall} = \\frac{TP}{TP + FN}\n\\]\n\nThe fraction of all true positives that the model got correct\nFocuses on Type-II error (\\(FN\\))\nProbability of detecting a condition when it’s truly present\n\ni.e. normally want high sensitivity and low specificity\n\nAs the number of \\(FNs\\) towards 0, the value of Recall will tend to \\(TP/TP = 1\\)\nInterpretation: 1 is perfect, &lt; 0.5 is low\nMaximize when \\(FNs\\) are more costly\n\nExamples\n\nA customer gets their card stolen and incurs fraudulent charges, but the model doesn’t detect it.\n\n\nPython:\nfor i in range(n_classes):\n  recall = np.mean(y_pred[y_test == i] == y_test[y_test == i])\n  print(f\"Recall for class {class_labels[i]}: {recall:0.3f}\")\n\nFilter for a specific category, then take the mean.\n\n\nSpecificity or True Negative Rate (TNR)\n\\[\n\\mbox{Specificity} = \\frac{TN}{TN + FP}\n\\]\n\nThe fraction of all the true negatives the model got correct\nProbability of not detecting a condition when it’s truly present\n\ni.e. normally want high sensitivity and low specificity\n\n\nType I Error: False Positives - If \\(FPs\\) are more costly than \\(FNs\\), then the threshold is raised to increase the accuracy of the model in predicting positive cases\nType II Error: False Negatives - If \\(FNs\\) are more costly than \\(FPs\\), then the threshold is lowered to increase the accuracy of the model in predicting negative cases",
    "crumbs": [
      "Diagnostics",
      "Classification"
    ]
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-scores",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-scores",
    "title": "Classification",
    "section": "Scores",
    "text": "Scores\n\nMisc\n\nTypes\n\nThreshold (e.g. F-Scores, Accuracy, Precision, Recall)\n\nDetermining thresholds depends on being able to correctly quantify the costs of type 1 and 2 errors\n\nRanking (ROC, PR) Costs not included An expected utility can’t be calculated\n\nNot intuitive to a non-technical audience\n\n\nComparison of similar scoring models\n\nNotes from https://evidentlyai.com/blog/tutorial-2-model-evaluation-hr-attrition\nIf there’s no clear winner in terms of scores, then you need to drill down.\n\nIf models differ significantly on one or couple scores but not the others. Determine what that score optimizes for (e.g. precision, recall, etc.) and if that fits your use case.\nHow does changing the classification threshold effect the scores?\n\nMakes sure whichever measure (e.g TP, FP, etc.) you’re maximizing for has plenty of observations, so you know you’re seeing a real effect.\n\nExamine how the models predict different groups (aka cohorts).\n\nExample: If it’s a employee churn model, how does the model perform on employees from different departments, skill level, roles, etc.\nOne cohort might be more important to keep from churning than another.\n\n\n\nOut-of-Bag Error (OOB) - Mean prediction error on each training sample (row) using only trees that didn’t use that sample in their bootstrap resample\n\nUseful when there isn’t enough data for a validation set.\n\nCustom Cost Functions\n\nAlso see\n\nLogistics &gt;&gt; Decision Impact Metrics\n\nExamples\n\nCalculating the Business Value of a Data Science Project\nSee notebook\nVisualizing Machine Learning Thresholds to Make Better Business Decisions\n\n\nUses a Telecom subscriptions churn example and incorporates available resources (queue rate) that can review flagged events in order to choose a threshold\n\ni.e. if you can only review 50 cases, then you model need only flag 50 cases\n\nAdds uncertainty by using multiple train/test splits and creating quantiles for CIs\nOptimizes resources, costs, precision, and recall to produce a threshold\n\nExample of model ROI calculation: see Banking/Credit &gt;&gt; Fraud &gt;&gt; Misc\n\n\n\nBalanced Accuracy\n\\[\nBA = \\frac{cTP}{P} + \\frac{(c-1)TN}{N}\n\\]\n\n\\(c\\) is a cost weight between 0 and 1\n\\(P\\) and \\(N\\) are observed Positives and Negatives\nCan also be used when the negative class is more important to get right\n\ne.g. Cost Reductions: Reduce unnecessary treatments of healthy patients that got an FP on their disease test\n\n{yardstick}\n\nBrier Score\n\nHow far your predictions lie from the true values\n\nA mean square error in the probability space\n\nSmaller Brier scores are better\nRandom guess model (i.e. all predicted probabilities = 0.50) has a Brier score of 0.25\n\nWhen the outcome incidence is lower (i.e. event rate, proportion of 1s), the maximum score for a random guess model is lower, eg, for 10%: 0.1 x (1 - 0.1)2 + (1 - 0.1) x 0.12 = 0.090.\n\nWhere the formula is:\n\\[\nY(1- p)^2 + (1-Y)p^2\n\\]\n\n\nIssue: Inadequate for very rare (or very frequent) events, because it does not sufficiently discriminate between small changes in forecast that are significant for rare events.\n\nn &gt; 1000 required for higher-skill forecasts of relatively rare events, whereas only quite modest sample sizes are needed for low-skill forecasts of common events (wiki)\n\nBootstrapping code (article)\nBinary Outcomes\n\\[\nBS = \\frac{1}{N}\\sum_{t=1}^N (f_t - o_t)^2\n\\]\n\n\\(f\\) is the predicted probability\n\\(o\\) is the observed outcome\nRange is from 0 to 1\nMean of brier loss\n\nPolytomous Outcomes\n\\[\nBS = \\frac{1}{N}\\sum_{t=1}^N\\sum_{i=1}^R (f_{ti} - o_{ti})^2\n\\]\n\nR is the number of categories\nRange is from 0 to 2\n\nBrier Skill Score (BSS)\n\\[\nBSS = 1 - \\frac{BS}{BS_{\\text{ref}}}\n\\]\n\nPercentage improvement in the BS compared to the reference model\nStill a strictly proper scoring rule\nValues between 0 and 1\nHigher is better\n\\(BS\\) is the brier score\n\\(BS_{\\text{ref}}\\) is a reference/baseline brier score that your trying to beat\n\nDefault value (no-skill/naive model value)\n\\[\nBS_{\\text{ref}} = \\frac{1}{N} \\sum_{t=1}^N (\\bar o - o_t)^2\n\\]\n\n**for binary outcomes\n\\(o\\) is the observed outcome\n\\(\\bar o\\) is the average observed outcome (i.e. overall proportion of 1s)\n\n\n\nScaled Brier Score\n\\[\n\\begin{align}\n&SBS = 1 - \\frac{BS}{BS_{\\text{max}}} \\\\\n&\\text{where} \\;\\; B_{\\text{max}} = \\bar p (1 - \\bar p)\n\\end{align}\n\\]\n\narticle\nCode\nscaled_brier &lt;- function(x, p, ...){ \n    format(round(1 - (x / (mean(p) * (1 - mean(p)))), digits = 2), nsmall = 2)\n}\nRange between 0% and 100%\n\n\nCohen’s Kappa\n\nA similar measure to accuracy, but is normalized by the accuracy that would be expected by chance alone and is very useful when one or more classes have large frequency distributions.\nExtends naturally to multiclass scenarios\nyardstick::kap, docs\nA measure of agreement between categorical variables X and Y\n\nCalculated from the observed and expected frequencies on the diagonal of a square contingency table\nRange: 0 &lt; \\(\\kappa\\) &lt; 1, although negative values do occur on occasion.\nSuited for nominal (non-ordinal) categories.\n\nWeighted kappa can be calculated for tables with ordinal categories.\n\nSee Penn Stats Stats 509 for further details, equation\n\n\nF scores\n\nEach score weights \\(FP\\) and \\(FN\\) differently. A trade-off between Precision and Recall\nRange: 0 to 1\n\n1 - Perfect precision and recall\n0 - Either precision or recall are zero.\n\ni.e. A low F-score doesn’t tell you whether it’s precision or recall that is the problem\nMaximizing FPR along with F1 will help curb Type-I errors and you’ll get an idea about the villain behind your low F1-score\n\nRule of Thumb\n\n&gt; 0.9 Very good\n0.8 - 0.9 Good\n0.5 - 0.8 OK\n&lt; 0.5 Not good\n\n\nFine for imbalanced classes as long as the positive class (TP and FP) is what’s important and not the negative class (e.g. FN and TN) (article)\n\\(F_\\beta\\)\n\\[\nF_\\beta = \\frac{1+\\beta^2}{\\frac{1}{\\mbox{Precision}}+\\frac{\\beta^2}{\\mbox{Recall}} + }\n\\]\n\nA generalized F score\nChoose your own ratio\n\nThe more you care about recall over precision the higher beta you should choose\ne.g. If you want precision to be twice as important than recall, you’d set \\(\\beta\\) to 0.5\n\nUse when you care more about the positive class\n\n\\(F_{1}\\)\n\\[\n\\begin{align}\nF_1 &= \\frac{2}{\\frac{1}{\\mbox{Precision}} + \\frac{1}{\\mbox{Recall}}} \\\\\n      &= \\frac{TP}{TP + 0.5(FP + FN)}\n\\end{align}\n\\]\n\nHarmonic mean of precision and recall\nRecall and precision are of the same importance\n\n\\(F_2\\)\n\\[\n\\begin{align}\nF_2 &= \\frac{5}{\\frac{4}{\\mbox{Precision}} + \\frac{1}{\\mbox{Recall}}} \\\\\n      &= \\frac{TP}{TP + 0.8FP + 0.2FN}\n\\end{align}\n\\]\n\n\\(F_2\\): 80/20\n\nβ = 2. Therefore you care about Recall twice as much as Precision\n\n\n\\(F_3\\)\n\\[\nF_3 = \\frac{TP}{TP + 0.9FP + 0.1FN}\n\\]\n\n\\(F_3\\): 90/10\n\nβ = 3. Therefore you care about Recall 3 times as much as Precision\n\n\n\nH measure\n\n{hmeasure}\n\nSee bkmks for vignette\n\nInstead of using misclassification cost (c, see above) as a constant it defines it as a distribution\n\nJ index (a.k.a. Youden’s J statistic)\n\\[\nJ = sensitivity + specificity - 1\n\\]\n\nValues near one are best. lower for models with pathological distributions for the class probabilities (i.e. uncalibrated)\n\nLift Score\n\\[\n\\mbox{Lift} = \\frac{\\frac{TP}{TP+FP}}{\\frac{TP+FN}{TP+TN+FP+FN}}\n\\]\n\nThe ratio of correctly predicted positive examples and the actual positive examples in the test dataset.\nRange: 0 to \\(\\infty\\)\nValues greater than 1 represent a valuable model\nExample: Using lift score within {sklearn::GridSearchCV}\nfrom mlxtend.evaluate import lift_score\nscorer = {\n    'lift_score': make_scorer(lift_score)\n}\nclf = GridSearchCV(SVC(), hyperparameters, cv=10,\n                   scoring=lift_scorer)\n\nFrom Raschka library\n\nAlso has examples on how to implement\nmake_scorer is from {{sklearn}}\n\n\n{yardstick::lift_curve}\n{ROCR}\n\nMatthew’s Correlation Coefficient (MCC)\n\\[\nMCC = \\frac{(TP \\cdot TN)-(FP \\cdot FN)}{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}\n\\]\n\nIt’s a correlation between predicted classes and ground truth.\n\nAka Phi Coefficient (i.e. Pearson Correlation for 2x2 tables, i.e. binary variables)\n\n{yardstick}\nRange: -1 to 1\n\n0 is equivalent to a random prediction\n-1 predictions are perfectly negatively correlated with the truth\n1 predictions are perfectly postiviely correlated with the truth (model is perfect)\n\nUses all confusion matrix values\nRobust to class imbalance\n\nMcNemar’s Test\n\nChi-Squared test for symmetry\nSee Post-Hoc Analysis, General &gt;&gt; Dichotomous Data &gt;&gt; for details and code\nUses this matrix format\n\nmatrix(c(9945, 25, 15, 15),\n        nrow = 2,\n        dimnames = list(\"model 1\" = c(\"Correct\", \"Wrong\"),\n                        \"model 2\" = c(\"Correct\", \"Wrong\")))\n\nIf “wrong-correct” or “correct-wrong” have counts &lt; 50, then use the Exact Tests to get accurate p-values\n\nFor more than 2 models, Cochran’s Q test can be used (Generalized McNemar’s Test)\n\nMean Log Loss (aka Cross-Entropy)\n\\[\n\\mbox{MeanLogLoss} = -\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot \\log (\\hat{p}_i) + (1-y_i) \\cdot \\log(1 - \\hat{p}_i))\n\\]\n\n\\(\\hat p\\) is the predicted probability and \\(y\\) is the numeric observed label (0/1)\nThe difference between ground truth and predicted score for every observation and average those errors over all observations\nLower is better\n\nA perfect model has a mean log loss of 0.\n\n{yardstick::mn_log_loss}\nAlso see Loss Functions for issues with this metric\n\nMisclassification Cost\n\\[\n\\mbox{Misclassification Cost} = c_1FP + c_2FN\n\\]\n\nWhere \\(c_1\\) and \\(c_2\\) are costs per \\(FP\\) and per \\(FN\\) respectively; \\(FP\\) is total False Positives and \\(FN\\) is total False Negatives\nSimple method to assign a business value to a classification model\nSee {yardstick::classification_cost}\n\nMisclassification Rate\n\\[\n\\begin{align}\n&\\mbox{Misclassification Rate} = 1 - Accuracy \\\\\n&\\text{where} \\;\\; \\mbox{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\end{align}\n\\]\n\nUses Accuracy so terrible for unbalanced classes.\n\nNo-information rate\nno_information_rate &lt;- function(data) {\n  data %&gt;%\n    count(outcome_var) %&gt;%\n    mutate(proportion = n / sum(n)) %&gt;%\n    top_n(1, proportion) %&gt;%\n    pull(proportion)\n}\n\nThe proportion of the most common class\nUsed as a baseline metric for comparing models in multinomial classification\n\nPR-AUC\n\nThe area under the Precision-Recall curve\nValues near one indicate very good results while values near 0.5 would imply that the model is very poor\nBetter for imbalanced classes\nPR AUC focuses mainly on the positive class (PPV and TPR) it cares less about the frequency of the negative class\n\nUse when you care more about positive than negative class\n\n\nPseudo R2\nlibrary(DescTools)\nPseudoR2(model, which = c(\"CoxSnell\", \"Nagelkerke\", \"Tjur\"))\n##  CoxSnell Nagelkerke      Tjur \n##  0.6583888  0.9198193  0.8795290\n\nExample: How much of the likelihood of promotion does my model explain?\n\nOur model explains more than two thirds of the variation in the promotion outcome.\n\n\nROC-AUC\n\naka AUROC, concordance statistic, or c-statistic\nThe area under the ROC is an overall assessment of performance across all cutoffs.\n\nWhere ROC is Sensitivity vs (1 - Specificity) (aka TPR vs FPR) chart\n\nValues near one indicate very good results while values near 0.5 would imply that the model is very poor.\nUse it when you care equally about positive and negative classes\nDo NOT use it if you have imbalanced classes\n\nSomers’ \\(D_{xy}\\)\n\\[\nD_{xy} = 2(c - 0.5)\n\\]\n\nWhere c is the concordance probability or AUROC\nA pure discrimination measure which is a rank correlation between \\(Y\\) and \\(\\hat Y\\)",
    "crumbs": [
      "Diagnostics",
      "Classification"
    ]
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-climb",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-climb",
    "title": "Classification",
    "section": "Class Imbalance",
    "text": "Class Imbalance\n\nMisc\n\nClass imbalance sampling methods tend to greatly improve metrics based on the hard class predictions (i.e., the categorical outcome variables and not binned numeric variables) because the default cutoff tends to be a better balance of sensitivity and specificity.\n\ne.g J index vs AUC\n\nROC curves and PR curves have inflated performance under strong class imbalance\n\nScores: F-Scores, MCC, PR-AUC, Cohen’s Kappa, precision at fixed recall, recall at fixed precision\nPrecision-Recall\n\nIn cases of balanced classes, Sensitivity and Specificity are often used\nFor less-than-perfect models (FP = FN = 0), if you increase Precision, you reduce Recall and vice versa.\nSensitive to class-imbalance\n\nYou can’t compare curves that have been tested on two different test sets if the class proportions differ\nA test-set with a 50:50 proportion will have higher scores in general as compared to a test-set with a 10:90 proportion for the primary class.",
    "crumbs": [
      "Diagnostics",
      "Classification"
    ]
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-multinom",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-multinom",
    "title": "Classification",
    "section": "Multinomial",
    "text": "Multinomial\n\nMisc\n\nPackages\n\n{MultiClassROC} - computing and visualizing Receiver Operating Characteristics (ROC) and Area Under the Curve (AUC) for multi-class classification problems\n\nAlso see\n\nScores\n\n&gt;&gt; No-information Rate for a generic accuracy baseline\n&gt;&gt; Brier-Score &gt;&gt; Polytomous Outcomes\n&gt;&gt; Cohen’s Kappa\n&gt;&gt; Mean Log Loss\n\n\nsklearn::classification_report calculates the maps and waps for precision, recall, and f1-score\n\n“Support” in this report refers to label and total sample sizes.\nSee Model building, sklearn &gt;&gt; Misc &gt;&gt; Score Model for code\n\n\nConfusion Matrix\n\n\nClass labels are numbers 0-9\nThe decimal places are confusing but these are counts of the predicted label matching each observed label\n\nPrecision for Label 9 = \\(TP / (TP + FP) = 947/ (947 + 1 + 0 + 0 + 0 + 0 + 38 + 0 + 40 + 2) = 0.92\\)\n\nColumns are used for precision calculation\n\nRecall for label 9 = \\(TP / (TP + FN) = 947 / (947 + 0 + 0 + 0 + 0 + 0 +14 + 0 + 36 + 3) = 0.947\\)\n\nRows are used for recall calculation\n\nMacro Average Precision - simple arithmetic average of the precision of all the labels\n\n\\(map = (0.80 + 0.95 + 0.77 + 0.88 + 0.75 + 0.95 + 0.68 + 0.90 + 0.93 + 0.92) / 10 = 0.853\\)\n\nWeighted Average Precision - the precision of each label is multiplied by their sample size and the weighted sum is divided by the total number of samples\n\n\\(wap = (760*0.80 + 900*0.95 +535*0.77 + 843*0.88 + 801*0.75 + 779*0.95 + 640*0.68 + 791*0.90 + 921*0.93 + 576*0.92) / 7546 = 0.86\\)\n\nWhere 760 = sample size for label 0, 900 = sample size for label 1, etc., and 7546 is the total sample size\n\n\nSame techniques can be used for F-Scores and others.\n\nExample: The F-Score would be calculated for each label. Then, the map and wap can be calculated from those individual label scores.",
    "crumbs": [
      "Diagnostics",
      "Classification"
    ]
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-curv",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-curv",
    "title": "Classification",
    "section": "Curves",
    "text": "Curves\n\nMisc\n\n{rtichoke} - Interactive ROC/PR curves; all kinds of additional information provided in the pop-up\n{yardstick}\n{ROCR} - ROC curves, precision/recall plots, lift charts, cost curves, custom curves by freely selecting one performance measure for the x axis and one for the y axis, handling of data from cross-validation or bootstrapping, curve averaging (vertically, horizontally, or by threshold), standard error bars, box plots, curves that are color-coded by cutoff, printing threshold values on the curve, tight integration with Rs plotting facilities (making it easy to adjust plots or to combine multiple plots), fully customizable, easy to use (only 3 commands)\n\nReceiver Operating Curve (ROC)\n\nIf a model is poorly calibrated, the ROC curve value might not show diminished performance. However, the J-index would be lower for models with pathological distributions for the class probabilities.\nA ROC curve plots TPR as a function of FPR at different decision thresholds. We want to minimize our FPR while maximizing our TPR.\nA good model will bend towards the upper-left corner of plot.\nA bad model (i.e., random) will hug the diagonal.\nA ludicrously bad model (i.e., worse than random) will bend towards the lower-right corner of the plot.\nPackages\n\n{runway}\n\n\nPrecision-Recall (PR)\n\n\nWhen the observations with true positive labels are rare, then PR curves are preferred (Also see Class Imbalance)",
    "crumbs": [
      "Diagnostics",
      "Classification"
    ]
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-thresh",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-thresh",
    "title": "Classification",
    "section": "Threshold Analysis",
    "text": "Threshold Analysis\n\nPackages\n\n{runway}\n\n{pROC}\n{ROCR} - Interesting metrics about various threshold points\n\nNet Benefit and Decision Curves\n\nSee also\n\nDecison Intelligence\nA simple, step-by-step guide to interpreting decision curve analysis\n\n\\(\\text{net benefit} = \\text{sensitivity} \\times \\text{prevalence} – (1 – \\text{specificity)} \\times (1 – \\text{prevalence}) × w\\)\n\nWhere w is the odds at the threshold probability.\nFor a prediction model that gives predicted probability, \\(\\hat p\\), of disease, sensitivity and specificity at a given threshold probability \\(p_t\\) is calculated by defining test positive as \\(\\hat p \\geq p_t\\).\n\n\nDetermine the Probability Threshold Using Expected Utility (article)\n\\[\n\\begin{align}\n\\text{expected utility} &= p \\cdot TPR(t) \\cdot \\nu_{TP} \\\\\n&+ p \\cdot (1-TPR(t)) \\cdot \\nu_{FN} \\\\\n&+ (1-p) \\cdot FPR(t) \\cdot \\nu_{FP} \\\\\n&+ (1-p) \\cdot (1-FPR(t)) \\cdot \\nu_{TN}\n\\end{align}\n\\]\n\nWhere\n\n\\(t\\) is the probability threshold for classifying a 1/0 outcome\n\\(\\nu\\) is the utility assigned to a TP, FN, FP, TN prediction\n\nCan be positive or negative values\nCould be dollar amounts or dollar amounts normalized into weights\n\n\\(p\\) is observed proportion of events (i.e. 1s) in the sample.\n\nSee also\n\nEconometrics, Discrete Choice Models\nDecison Intelligence\n\nAlgebraically equivalent expected utility equation\n\\[\nc = TPR(t) \\cdot p \\cdot (\\nu_{TP} - \\nu_{FN}) + FPR(t) \\cdot (1-p) \\cdot (\\nu_{FP} - \\nu_{TN})\n\\]\n\n\\(c\\) is the expected utility\nLinear equation in the form, \\(c = yb + xa\\) where slope, \\(s = -a/b\\)\nSlope\n\\[\ns = \\frac{1-p}{p} \\cdot \\frac{\\nu_{TN} - \\nu_{FP}}{\\nu_{TP} - \\nu_{FN}}\n\\]\n\n\\(s &gt; 1\\): negative classes outweigh positive ones, or correctly classifying a negative class outweighs correctly classifying a positive class in terms of utility or both\n\\(s &lt; 1\\): positive classes outweigh negative ones, or correctly classifying a positive class outweighs correctly classifying a negative class in terms of utility or both\n\n\nOptimizing utility for different values of \\(t\\)\n\nDetermine the blue baseline in the bottom charts for different ranges of \\(s\\)\n\nGiven a range of \\(s\\) and setting \\(t\\) to 0 or 1, we can infer \\(FPR\\) and \\(TPR\\). Then, we use these values to calculate the baseline expected utility\n\nFor \\(s &gt; 1\\): Set \\(t = 1\\), meaning we always reject, resulting in \\(FPR = 0\\) and \\(TPR = 0\\)\nFor \\(s &lt; 1\\): Set \\(t = 0\\), meaning we always accept, resulting in \\(FPR = 1\\) and \\(TPR = 1\\)\n\n\n\nUtility vs Threshold is all you really need. I think the top charts are just there to show how this type of baseline is more difficult (and realistic) bar to pass when compared to the typical 45 degree line used in ROC charts.",
    "crumbs": [
      "Diagnostics",
      "Classification"
    ]
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-calib",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-calib",
    "title": "Classification",
    "section": "Calibration",
    "text": "Calibration\n\nMisc\n\nAlso see\n\nClassification &gt;&gt; Calibration\nTerms section\nA tutorial on calibration measurements and calibration models for clinical prediction models (paper)\n\nrms::val.prob(predicted, observed) (Harrell’s function) outputs most of the calibration metrics mentioned\n\n\npackages\n\n{runway}\n{rtichoke}\n{probably} - tidymodels calibration package\n\nCalibrating Binary Probabilities - Nice tutorial\n\n{CalibrationCurves}\n{{binclass-tools}}\n\nStratification and calibration metrics (e.g. low risk, medium risk, and high risk)\n\nUseful if you want to choose the model that performs best on a specific risk group.\nYou can add a categorical variable to the calibration set and group observations based on the predicted probabilities\nAfter grouping the predicted probabilites, calculate calibration metrics for each group\nSee ICI paper for more details\n\n\n\n\nBasic Workflow\n\nCompute Tests for Miscalibration using 2 vectors\n\nClassifier’s predicted probabilities\nNumeric (0/1) observed outcome\n\nCompute Mean, Weak, and Moderate levels of Calibration\nCreate Calibration Plots\nInterpret\n\nIf model isn’t well-calibrated, then\n\nWhich region of the probability range is being over/under-predicted?\n\nAre these regions important to the use case?\n\nIs the model useful with this level of calibration\nCalibrate predictions (see Classification &gt;&gt; Calibration &gt;&gt; Methods &gt;&gt; Platt Scaling/Isotonic Regression)\n\nIf model wasn’t well-calibrated, but now the predictions have been calibrated,\n\nIs model/predictions still not sufficiently calibrated?\nAre there any regions still being over/under-predicted?\n\nAre these regions important to the use case?\n\nIs the model useful with this level of calibration\n\n\n\n\n\nTests for Miscalibration\n\nMisc\n\nThe ROC curve value might not show diminished performance.\n\nHowever, the J index would be lower for models with pathological distributions for the class probabilities.\n\nThe Hosmer-Lemeshow (H-L) test is popular but has major shortcomings. Not a good test to trust by itself\n\nBased on artificially grouping patients into risk strata, p-value that is uninformative with respect to the type and extent of miscalibration, and low statistical power\nSee Spiegelhalter z statistic &gt;&gt; Manually &gt;&gt; paper for more details\nAlso see RMS Ch 10.5 (end of section, listen to audio)\n\n\n\n\nSpiegelhalter Z Statistic\n\\[\nz_s = \\frac{\\sum_{i=1}^N (O_i - E_i)(1-2E_i)}{\\sqrt{\\sum_{i=1}^N (1-2E_i)^2(1-E_i)E_i}}\n\\]\n\nWhere\n\n\\(E\\) is the predicted probability\n\\(O\\) is the observed outcome (0/1)\n\\(N\\) is the number of observations\n\nStata: This tests “whether an individual Brier score is extreme”\np-value &lt; 0.05 suggests an improperly calibrated model (lower z-stats = more calibrated)\nNull hypothesis is that the estimated probabilities are equal to the true class probabilities\nrms::val.prob(predicted, observed)\n\nGives “the Spiegelhalter Z-test for calibration accuracy, and its two-tailed P-value”\nz-score is under “S:z” and it’s p-value is under “S:p”\n\nManually (paper, github)\nSpiegelhalter_z = function(y, prob){\n  alpha = 0.05\n  z_score = sum((y-prob)*(1-2*prob))/sqrt(sum(((1-2*prob)^2)*prob*(1-prob)))\n  print(z_score)\n  if (abs(z_score) &gt; qnorm(1-alpha/2)){\n    print('reject null. NOT calibrated')\n  } else{\n    print('fail to reject. calibrated')\n  }\n  cat('z score: ', z_score, '\\n')\n  cat('p value: ', 1-pnorm(abs(z_score)), '\\n')\n  return(z_score)\n}\n\ny: observed outcome (1/0)\nprob: predicted probabilities from your classifier\nUses upper-tail p-value, but I think a two-sided p-value is probably more correct (See below)\n\nHypothesis tests\n\nHarrell uses a two-sided p-value but a couple other papers I read just use the upper-tail. I asked Harrell and he tagged Spiegelhalter in a tweet to see which is correct (tweet). Haven’t heard back yet.\nSAS squares it to get a ChiSq distribution, which is equivalent to a 2-tail Z-test, and tests it that way: pchisq(z_stat^2), lower.tail = F)\nStata uses the upper tail probability\n\n\n\n\nIntegrated Calibration Index (ICI, Eavg)\n\npaper\nMisc\n\nInterpreted as weighted mean absolute difference between observed and predicted probabilities, in which observations are weighted by the empirical density function (i.e. empirical distribution) of the predicted probabilities.\nMotivated by Harrell’s Emax index (see below), which is the maximum absolute difference between a smooth calibration curve and the diagonal line of perfect calibration\nLower is better for each metric (ICI, E50, E90, and Emax)\n\nThese metrics are used for model comparison. There aren’t guidelines for metric values that delineate well-calibrated models for poorly calibrated ones.\n\nSee below for ICI bootstrap CIs\nIn general, the larger the sample size the lower the values for each metric\n\ni.e. don’t compare models that were fit with different sized datasets.\n\nEmax is the least stable out of the four metrics\n\nProcess\n\nFit a LOESS model for observed_outcome ~ predicted probabilities\nTake the average of the abs difference between the loess predicted probabilities and your model’s predicted probabilities\n\nLoess predicted probabilities are predicted using your model’s predicted probabilities as new data.\n\n\nManually (paper, github)\nici = function(Y, P){\n\n  loess.calibrate &lt;- loess(Y ~ P) \n\n  # Estimate loess‐based smoothed calibration curve\n  P.calibrate &lt;- predict(loess.calibrate, newdata = P)\n\n  # This is the point on the loess calibration curve corresponding to a given predicted probability.\n  ICI &lt;- mean(abs(P.calibrate - P))\n  return(ICI)\n\n  # plot\n  plot(P, P.calibrate)\n  lines(P.calibrate, x=P)\n}\n\n\n\nE50, E90, and Emax\n\nCode\n# Let Y denote a vector of observed binary outcomes.\n# Let P denote a vector of predicted probabilities.\nloess.calibrate &lt;‐ loess(Y ∼ P)\nP.calibrate &lt;‐ predict(loess.calibrate, newdata = P)\n\nE50 &lt;‐ median(abs(P.calibrate – P))\nE90 &lt;‐ quantile(abs(P.calibrate – P), probs = 0.9)\nEmax &lt;‐ max(abs(P.calibrate – P))\nNotes from ICI paper above\nLower is better for each metric\nEmax - maximal absolute difference between observed and predicted probabilities of the outcome (Harrell’s)\n\ni.e. The maximal absolute vertical distance between the calibration curve and the diagonal line denoting perfect calibration\nIssue: The greatest distance between observed and predicted probabilities may occur at a point at which the distribution of predicted probabilities is sparse.\n\nWhen comparing two competing prediction methods, it is possible that one method, despite having a greater value of Emax, actually displays greater calibration in the region of predicted probabilities in which most predicted probabilities lie.\n\n\nE50 - denotes the median absolute difference between observed and predicted probabilities\n\ni.e. same as ICI (aka Eavg) except uses the median instead of the mean\nLess influenced by a small minority of subjects/observations for whom there is a large discrepancy between observed and predicted probabilities (i.e. outliers)\n\nE90 - denotes the 90th percentile of the absolute difference between observed and predicted probabilities.\n\nSummarizes the limit to the absolute discrepancy for the large majority of subjects in the sample\n\nBootstrap CIs for ICI, E50, E90, Emax\n\n{CalibrationCurves}\n\nHarrell’s val.prob function but only requires the numeric observed outcome and predicted probabilities, so you don’t need to enter Harrellverse\nHas a ton of extra features, but the documentation is kind of poor.\nval.prob.ci.2(preds, observed_y)\n\nExample: Using rms::val.prob with a glm model (article)\n\nAlso code for viz of the bootstrap distributions of each metric\n\nProcedure\n\nDraw bootstrap sample from the training dataset\nDraw bootstrap sample from the calibration dataset\nFit classifier to the training bootstrap sample\nGet predictions from classifier on calibration sample\nCalculate calibration metrics using calibration set’s observed outcomes and classifier’s predictions on the calibration set.\n\nPaper used 2000 bootstrap replicates (B = 2000).\nPercentile‐based bootstrap confidence intervals were then constructed using these 2000 bootstrap replicates.\n\ne.g. the endpoints of the estimated 95% confidence interval were the empirical 2.5th and 97.5th percentiles of the distribution of the given calibration metric across the 2000 bootstrap replicates.\n\nTo calculate CIs for the difference in calibration metrics between models, I think each type of classifier would participate for each replicate and the differences would be calculated in step 5. (glossed over in the paper)\n\nIf the difference CI contains 0, then it can be said that the calibration for both models is comparable.\n\n\n\n\n\nExpected Calibration Error (ECE)\n\nSee Evaluation of Calibration Levels &gt;&gt; Binning Method for details on what’s happening here. Almost all the calculations mentioned in that section are included in this metric.\nAlso described in Expected Calibration Error: A Visual Explanation With Python\nBinary\n\\[\n\\begin{aligned}\n&ECE = \\sum_{m=1}^M \\frac{|B_m|}{n}|\\mbox{acc}(B_m) -  \\mbox{conf}(B_m)|\n\\\\\n&\\begin{aligned}\n\\mbox{where}\\;\\; &\\mbox{acc}(B_m) = \\frac{1}{|B_m|} \\sum_{i\\in B_m} \\mathbb{1} (\\hat y_i = y_i)   \\;\\; \\mbox{and} \\\\\n&\\mbox{conf}(B_m) = \\frac{1}{|B_m|} \\sum_{i \\in B_m} \\hat p_i\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\\(|B_m|\\) - The number of observations in bin \\(m\\)\n\\(\\mbox{acc}(B_m)\\) - The average accuracy of the predictions in bin \\(m\\)\n\nA prediction is the label determined by the predicted probability and the threshold.\n\n\\(\\mbox{conf}(B_m)\\) - The average predicted probability of the predictions in bin \\(m\\)\n\nPolytomous\n\nThere will be a predicted probability for each label.\nFor \\(\\hat p_i\\), the maximum probability will be selected since that probability will determine the predicted label. Then, the calculation for \\(\\mbox{conf}(B_m)\\) will be the same as the binary case.\nFor \\(\\mbox{acc}(B_m)\\), the maximum probability for the observation determines the label. Then, the calculation will be the same as the binary case — correct label = 1, incorrect label = 0.\n\n\n\n\n\nEvaluation of Calibration Levels\n\nLevels\n\nMean - See Calibration-in-the-Large, (Y vs Yhat)\n\naverage predicted risk compared with the overall event rate\n\nWeak - means that, on average, the model does not over or underestimate risk and does not give overly extreme (too close to 0 and 1) or modest (too close to disease prevalence or incidence) risk estimates.\n\nintercept - a target value of 0 (Calibration-in-the-Large)\n\nA logistic regression model, Y ~ offset(Yhat), is fit to get the calibration intercept\n&lt; 0 suggests overestimation\n&gt; 0 suggests underestimation\n\nslope - target value of 1\n\nA logistic regression model, Y ~ Yhat, is fit to get the calibration slope (paper, see supplemental material)\n&lt; 1 suggests that estimated risks are too extreme\n\ni.e., too high for patients who are at high risk and too low for patients who are at low risk\n\n &gt; 1 suggests the opposite\n\ni.e., that risk estimates are too moderate\n\n\nA calibration intercept close to 0 and a calibration slope close to 1 do not guarantee that the flexible calibration curve (Calibration-in-the-Small) is close to the diagonal\nAcceptable to stop evaluation at weak calibration for small datasets\n\nModerate - implies that estimated risks correspond to observed proportions\n\ne.g., among patients with an estimated risk of 10%, 10 in 100 have or develop the event\nSee Calibration-in-the-Small\n\nStrong - the predicted risk corresponds to the observed proportion for every possible combination of predictor values; this implies that calibration is perfect and is a utopic goal\n\n\n\nCalibration Plots (Reliability Diagrams)\n\nMisc\n\nSee Misc &gt;&gt; packages for calibration curve functions\nCalibration curves for nested cv (post)\n\nA loess curve is estimated for each split in the outer loop. Those curves are then “averaged” by fitting another loess curve onto those curves.\n\nHarrell re calibration curves showing miscalibration at the tails: “you can ignore extreme tails where there are almost no data, with the exception that you’d need to mention that if future predictions ever appear in such tails such predictions are of unknown accuracy.\n{probably}\n\nCalibration Curve:\n\npreds &lt;- tibble(\n            truth = data$class,\n            truth_int = as.integer(data$class) - 1,\n            estimate = predict_prob(unbalanced_model, data)\n         )\ncal_plot_breaks(preds, truth = truth_int, estimate = estimate)\n\n\n\n\nLOESS\n\nHarrell uses a loess curve, loess(y ~ yhat, iter = 0)\nBecause binning subjects into strata based on predicted risk may result in a loss of information\nWith rms::calibrate, he also bootstraps the process to create a bias-corrected curve\n\nSee RMS Ch 10.11 pgs 269-270 for details\ncalibrate code on github\n\nBe aware there is more that one “calibrate” function in the repo\n\n\nEvaluating a logistic regression based prediction tool in R\n\nCode for a simple loess smoothed calibration plot and a binned one, both in ggplot2\nThe binned one uses an “lm” smoother. The intercept (and slope?) should measure Calibration-in-the-Large (see Terms)\nThe loess curve vs the 45 degress ab-line should be a visualization of Calibration-in-the-Small (see Terms)\n\n{CalibrationCurves}\n\nHarrell’s val.prob function but only requires the numeric observed outcome and predicted probabilities, so you don’t need to enter Harrellverse\nHas a ton of extra features, but the documentation is kind of poor.\nval.prob.ci.2(preds, observed_y)\n\n\n\n\nBinning Method\n\nx-axis\n\nThe predicted probabilities are divided up into a fixed number of bins along the x-axis.\n\nToo few bins and there won’t be enough points on the curve. Too many bins and there will be too few observations in each bin leading to more noise. It is common to select 10 bins.\nFreedman-Diaconis rule - a statistical rule designed for finding the number of bins that makes the histogram as close as possible to the theoretical probability distribution\n\nFor each bin, calculate the average predicted probability. This will be your x-axis value.\n\ny-axis\n\nObserved proportion of positive events (1) for each x-axis bin\n\nCode\nGetCalibrationCurve &lt;- function(y, y_pred, bins = 10) {\n    data.frame(y = y, y_pred = y_pred) %&gt;%\n        arrange(y_pred) %&gt;%\n        mutate(pos = row_number() / n(),\n               bin = ceiling(pos * bins)) %&gt;%\n        group_by(bin) %&gt;%\n        summarize(pred_prob = mean(y_pred),\n                  obs_prob = mean(y))\n}\n\ndf &lt;- GetCalibrationCurve(y, y_pred, bins = 10)\n\nggplot(df, aes(x = pred_prob, y = obs_prob)) +\n  geom_point() +\n  geom_line() +\n  geom_abline(slope = 1, intercept = 0, linetype = 2) +\n  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +\n  theme_bw() +\n  labs(title = \"Calibration Curve\", x = \"Predicted Probability\",\n       y = \"Observed Probability\")\nAlso see Evaluating a logistic regression based prediction tool in R\n\n\n\nInterpretation\n\nThe better calibrated or more reliable a forecast, the closer the points will appear along the main diagonal from the bottom left to the top right of the plot.\nBelow the diagonal: The model has over-forecast; the probabilities are too large.\nAbove the diagonal: The model has under-forecast; the probabilities are too small.\nExample 1\n\n\nSystematic overestimation (red): Compared to the true distribution, the distribution of predicted probabilities is pushed towards the right. This is common when you train a model on an unbalanced dataset with very few positives.\nSystematic underestimation (blue): Compared to the true distribution, the distribution of predicted probabilities is pushed leftward.\nCenter of the distribution is too heavy (green): This happens when “algorithms such as support vector machines and boosted trees tend to push predicted probabilities away from 0 and 1” (quote from Predicting good probabilities with supervised learning).\nTails of the distribution are too heavy (black): For instance, “Other methods such as naive bayes have the opposite bias and tend to push predictions closer to 0 and 1” (quote from Predicting good probabilities with supervised learning).\n\nExample 2 (paper)\n\n\nBased on an outcome with a 25% event rate and a model with an area under the ROC curve (AUC or c-statistic) of 0.71.\na: General over- or underestimation of predicted risks.\nb: Predicted risks that are too extreme or not extreme enough",
    "crumbs": [
      "Diagnostics",
      "Classification"
    ]
  },
  {
    "objectID": "qmd/diagnostics-clustering.html",
    "href": "qmd/diagnostics-clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Diagnostics",
      "Clustering"
    ]
  },
  {
    "objectID": "qmd/diagnostics-clustering.html#sec-diag-clust-misc",
    "href": "qmd/diagnostics-clustering.html#sec-diag-clust-misc",
    "title": "Clustering",
    "section": "",
    "text": "Also see Notebook, pg 57\nFor K-Means, elbow method (i.e. WSS) is awful. Recommended: Calinski-Harabasz Index and BIC then Silhouette Coefficient or Davies-Bouldin Index\n{{sklearn.metrics.cluster}}",
    "crumbs": [
      "Diagnostics",
      "Clustering"
    ]
  },
  {
    "objectID": "qmd/diagnostics-clustering.html#sec-diag-clust-sphcent",
    "href": "qmd/diagnostics-clustering.html#sec-diag-clust-sphcent",
    "title": "Clustering",
    "section": "Spherical/Centroid Based",
    "text": "Spherical/Centroid Based\n\nWithin-Cluster Sum of Squares (WSS) (aka Inertia)\n\nMeasures the variability of the observations within each cluster. In general, a cluster that has a small sum of squares is more compact than a cluster that has a large sum of squares.\nTo calculate WCSS, you first find the Euclidean distance between a given point and the centroid to which it is assigned. You then iterate this process for all points in the cluster, and then sum the values for the cluster and divide by the number of points\nInfluenced by the number of observations. As the number of observations increases, the sum of squares becomes larger. Therefore, the within-cluster sum of squares is often not directly comparable across clusters with different numbers of observations.\n\nTo compare the within-cluster variability of different clusters, use the average distance from centroid instead.\n\nTypically used in the elbow method for kmeans for choosing the number of clusters\n\nAverage Distance from Centroid\n\nThe average of the distances from observations to the centroid of each cluster.\nThe average distance from observations to the cluster centroid is a measure of the variability of the observations within each cluster. In general, a cluster that has a smaller average distance is more compact than a cluster that has a larger average distance. Clusters that have higher values exhibit greater variability of the observations within the cluster.\n\nSilhouette Coefficient\n\nComputationally expensive\nFormula\n\\[\ns(i) = \\frac{b(i) - a(i)}{\\max \\{a(i), b(i)\\}},\\; \\text{if}\\; |C_I| &gt; 1\n\\]\n\nIntra-Cluster Distance\n\\[\na(i) = \\frac{1}{|C_I| - 1} \\sum_{j \\in C_I, i \\neq j} d(i,j)\n\\]\n\nThe mean distance between i and all the other data points within C.\n\\(|C_I|\\) is the number of points belonging to cluster i, and d(i , j) is the distance between data points i and j in the cluster CI\n\nInter-Cluster Distance\n\\[\nb(i) = \\min_{J \\neq I} \\frac{1}{C_J} \\sum_{j \\in C_J} d(i, j)\n\\]\n\nThe mean distance between i to all the points of its nearest neighbor cluster\n\n\nRange: -1 to 1\nGood: 1\nBad: -1\n\nSays inter-cluster distances are not comparable to the intra-cluster distances\n\n\nCalinski-Harabasz Index (aka Variance Ratio Criterion)\n\nCompares the variance between-clusters to the variance within each cluster\nNOT to be used to the density based methods, such as mean-shift clustering, DBSCAN, OPTICS, etc.\n\nClusters in density based methods are unlikely to be spherical and therefore centroids-based distances will not be that informative to tell the quality of the clustering algorithm\nMuch faster than Silhouette score calculation\n\nRatio of the squared inter-cluster distance sum and the squared intra-cluster distance sum for all clusters\nFormula\n\\[\nCH = \\frac{\\sum_{k=1}^K n_k \\lVert c_k - c \\rVert^2}{K-1} \\cdot \\frac{N-K}{\\sum_{k=1}^K \\sum_{i=1}^{n_k} \\lVert d_i - c_k \\rVert^2}\n\\]\n\n\\(n_k\\) is the size of the kth cluster\n\\(c_k\\) is the feature vector of the centroid of the kth cluster\n\\(c\\) is the feature vector of the global centroid of the entire dataset\n\\(d_i\\) is the feature vector of data point i\n\\(N\\) is the total number of data points\n\nRange: no upper bound\nHigher is better and means the clusters are separated from each other\n\nDavies-Bouldin Index\n\nAverage similarity between each cluster and its most similar one.\nNOT to be used to the density based methods, such as mean-shift clustering, DBSCAN, OPTICS, etc.\n\nClusters in density based methods are unlikely to be spherical and therefore centroids-based distances will not be that informative to tell the quality of the clustering algorithm\n\nMuch faster than Silhouette score calculation\nRange [0,1] and lower is better\nThe type of norm used in the formula should probably match the distance type used in the clustering algorithm. See the wiki for details.\nFormula\n\\[\nDB = \\frac{1}{N} \\sum_{i=1}^N D_i\n\\]\n\nAn averaged similarity score across all clusters with its nearest neighbor cluster\n\\(D_i\\) is the ith cluster’s worst (i.e. largest) similarity score, \\(R_{i,j}\\) across all other clusters\n\\[\nD_i = \\max_{j\\neq i} R_{i,j}\n\\]\nSmaller similarity score indicates a better cluster separation\n\\[\n\\begin{aligned}\n&R_{i,j} = \\frac{S_i + S_j}{M_{i,j}}\\\\\n&\\begin{aligned}\n\\text{where} \\;\\; &S_i = \\left(\\frac{1}{T_i} \\sum_{j=1}^{T_i} \\lVert X_j - A_i \\rVert_{p}^q \\right)^{\\frac{1}{q}} \\;\\; \\text{and} \\\\\n&M_{i,j} = \\lVert A_i - A_j \\rVert_p = \\left(\\sum_{k=1}^n |a_{k,i} - a_{k,j}|^p\\right)^{\\frac{1}{p}}\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\\(X_j\\) : n-dimensional feature vector assigned to Cluster \\(C_i\\)\n\\(T_i\\): The size of cluster \\(C_i\\)\n\\(A_i\\): Centroid of \\(C_i\\) and \\(a_{k,i}\\) is an element of the kth feature vector of \\(A_i\\) which has \\(n\\) elements\n\\(P\\): the degree of the norm; typically 2 for the euclidean norm (i.e. distance)\n\\(q\\): Not sure what this is (something to do with a statistical moment), but wiki only describes the parameter when it’s equal to 1 which makes S the average distance between the feature vectors and the centroid. So that’s probably the typical value.",
    "crumbs": [
      "Diagnostics",
      "Clustering"
    ]
  },
  {
    "objectID": "qmd/diagnostics-clustering.html#sec-diag-clust-featred",
    "href": "qmd/diagnostics-clustering.html#sec-diag-clust-featred",
    "title": "Clustering",
    "section": "Feature Reduction",
    "text": "Feature Reduction\n\nBayesian Information Criteria (BIC)\n\nHigher is better\nGoF metric in this situation that’s typically used for GMMs\nNot on the same scale as WSS\nPy Code\ndef bic_score(X, labels):\n  \"\"\"\n  BIC score for the goodness of fit of clusters.\n  This Python function is directly translated from the GoLang code made by the author of the paper. \n  The original code is available here: https://github.com/bobhancock/goxmeans/blob/a78e909e374c6f97ddd04a239658c7c5b7365e5c/km.go#L778\n  \"\"\"\n\n  n_points = len(labels)\n  n_clusters = len(set(labels))\n  n_dimensions = X.shape[1]\n  n_parameters = (n_clusters - 1) + (n_dimensions * n_clusters) + 1\n  loglikelihood = 0\n  for label_name in set(labels):\n    X_cluster = X[labels == label_name]\n    n_points_cluster = len(X_cluster)\n    centroid = np.mean(X_cluster, axis=0)\n    variance = np.sum((X_cluster - centroid) ** 2) / (len(X_cluster) - 1)\n    loglikelihood += \\\n      n_points_cluster * np.log(n_points_cluster) \\\n      - n_points_cluster * np.log(n_points) \\\n      - n_points_cluster * n_dimensions / 2 * np.log(2 * math.pi * variance) \\\n      - (n_points_cluster - 1) / 2\n\n  bic = loglikelihood - (n_parameters / 2) * np.log(n_points)\n\n  return bic",
    "crumbs": [
      "Diagnostics",
      "Clustering"
    ]
  },
  {
    "objectID": "qmd/diagnostics-cv.html",
    "href": "qmd/diagnostics-cv.html",
    "title": "CV",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Diagnostics",
      "CV"
    ]
  },
  {
    "objectID": "qmd/diagnostics-cv.html#sec-diag-cv-misc",
    "href": "qmd/diagnostics-cv.html#sec-diag-cv-misc",
    "title": "CV",
    "section": "",
    "text": "A better validation set score than the training set score (Notes from link):\n\nYou don’t have that much data and it’s luck.\n\nCan be diagnosed by changing the seed (random_state in py) in data split function\n\n\nGap between them shrinks over time\n\nMay be do to regularization (if it’s being used).\nDuring validation and testing, your loss function only comprises prediction error\n\nGap between them stays the same and training loss has fluctuations\n\nDL: dropout is only applicable during the training process, so it only affects training loss\n\nValidation loss lower than training loss at first but has similar or higher values later on\n\nDL: Training loss is calculated during each epoch, but validation loss is calculated at the end of each epoch\n\n\nCompare Training vs Test\n\nExample: {gt} table, {yardstick} forecast metrics\nbind_rows(\n  yardstick::mape(rf_preds_train, Sale_Price, .pred),\n  yardstick::mape(rf_preds_test, Sale_Price, .pred)\n) %&gt;% \n  mutate(dataset = c(\"training\", \"holdout\")) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt_number(\".estimate\", decimals = 1)",
    "crumbs": [
      "Diagnostics",
      "CV"
    ]
  },
  {
    "objectID": "qmd/diagnostics-cv.html#sec-diag-cv-reg",
    "href": "qmd/diagnostics-cv.html#sec-diag-cv-reg",
    "title": "CV",
    "section": "Regression",
    "text": "Regression\n\nFor prediction, if coefficients vary significantly across the test folds their robustness is not guaranteed (see coefficient boxplot below), and they should probably be interpreted with caution.\n\n\nBoxplots show the variance of the coefficient across the folds of a repeated 5-fold cv.\nThe “Coefficient importance” in the example is just the coefficient value of the standardized variable in a ridge regression\nNote outliers beyond the whiskers for Age and Experience\n\nIn this case, the variance is caused by the fact that experience and age are strongly collinear.\n\nVariability in coefficients can also be explained by collinearity between predictors\n\nPerform sensitivity analysis by removing one of the collinear predictors and re-running the CV. Check if the variance of the variable that was kept has stabilized (e.g. fewer outliers past the whiskers of a boxplot).",
    "crumbs": [
      "Diagnostics",
      "CV"
    ]
  },
  {
    "objectID": "qmd/diagnostics-dl.html",
    "href": "qmd/diagnostics-dl.html",
    "title": "DL",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Diagnostics",
      "DL"
    ]
  },
  {
    "objectID": "qmd/diagnostics-dl.html#sec-diag-dl-misc",
    "href": "qmd/diagnostics-dl.html#sec-diag-dl-misc",
    "title": "DL",
    "section": "",
    "text": "Packages\n\n{{weightwatcher}}\n\nRaschka (thread)\n\nGeneral\n\nMake sure training loss converged\n\nWant to see a plateau in the loss (y-axis)\n\nLeft: bad; Right: better\n\n\nCheck for overfitting\n\nDon’t want the gap between training and validation accuracy to be too large\n\nLeft: bad; Right: better\n\n\nCompare accuracy to a zero-rule baseline\n\nCheck that the validation accuracy is substantially better than a baseline based on always predicting the majority class (aka zero-rule classifier)\n\nTop chunk of code is just to determine which class is the majority class, which is class 1 with 1135 observations (aka examples)\nBottom chunk calculates the accuracy if a model just choose to classify each observation as class 1\n\n\nLook at failure cases\n\nAlways useful to check what cases the model gets wrong.\nAnalysis of these cases might detect things like mislabeled data\n\nPlot at a confusion matrix\n\nExample: PyTorch digit classifier\nimport matplotlib\n\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom torchmetrics import ConfusionMatrix\n\ncmat = ConfusionMatrix(num_classes=len(class_dict))\n\nfor x, y in dm.test_dataloader():\n\n  with torch.inference.mode():\n    pred = lightning_model(x)\n  cmat(pred, y)\n\ncmat_tensor = cmat.compute()\ncmat = cmat_tensor.numpy()\n\nfig, ax = plot_confusion_matrix(\n  conf_mat=cmat,\n  class_names=class_dict.values(),\n  norm_colormap=matplotlib.colors.LogNorm()\n)\nplt.xticks(rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\nplt.savefig('cm.pdf')\nplt.show()\n\n\n\nNew Architecture\n\nCheck that you can overfit 1000 data points, by using the same training and validation.\n\nPyTorch Lightning has this flag\nThe loss should be near zero (because the network should be able to memorize it); if not, there’s a bug in your code.\n\n\nRun {{weightwatcher}}}} and check that the layers have converged individually to a good alpha, and exhibit no rank collapse or correlation traps.",
    "crumbs": [
      "Diagnostics",
      "DL"
    ]
  },
  {
    "objectID": "qmd/diagnostics-forecasting.html",
    "href": "qmd/diagnostics-forecasting.html",
    "title": "Forecasting",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Diagnostics",
      "Forecasting"
    ]
  },
  {
    "objectID": "qmd/diagnostics-forecasting.html#sec-diag-fcast-misc",
    "href": "qmd/diagnostics-forecasting.html#sec-diag-fcast-misc",
    "title": "Forecasting",
    "section": "",
    "text": "Also see\n\nLogistics &gt;&gt; Decision Impact Metrics\nLoss Functions\n\nLoss Functions/Error Metrics\n\nIf you care about errors measured in percent (i.e. “relative”) and your data are strictly positive, then “relative” metrics such as the MALE and RMSLE or MAPE and sMAPE are also in this class of metrics but have issues (See Loss Functions)\nIf you care about errors measured in real units (e.g. number of apples), or your data can be zero or negative, then “raw” metrics such as MAE or MSE are more appropriate.\n\nMAE and RMSE are scale-dependent, so they can only be used to compare models on 1 series or on multiple series if they have the same units. MAE will lead to forecasts of the median, while minimizing the RMSE will lead to forecasts of the mean\nAlso see Loss Functions &gt;&gt; Misc &gt;&gt; re Stochastic Gradient Descent in ML/DL models\n\nIf you want to compare or aggregate performance metrics across time series, then you might want to use scaled metrics such as MASE, MASLE\n\nUsing MASLE will require your data are strictly positive\n\n\nHow does the amount of data affect prediction\n\nCould be useful for choosing a training window for production\nExample: Relative Absolute Error vs number of rows in the training set\n\n\nInterpretation: No pattern?\n\nMight’ve been useful to only look at the values from 0.5. Looks like a lot more points a cluster at data sizes between 1000 and 1200 rows.\n\n\nExample: MAE vs prediction horizon (colored by the number of weeks of data in training set)\n\n\nInterpretation for this data and model: For prediction horizons greater than a couple weeks, having mored data in the training set leads to worse performance\n\n\nDoes the model predict values close to zero\n\n\n“Labels” are the observed values\nBad performance with values close to zero can be the result of the loss function used where lower losses are not penalized as much as higher losses",
    "crumbs": [
      "Diagnostics",
      "Forecasting"
    ]
  },
  {
    "objectID": "qmd/diagnostics-forecasting.html#sec-diag-fcast-prob",
    "href": "qmd/diagnostics-forecasting.html#sec-diag-fcast-prob",
    "title": "Forecasting",
    "section": "Probabilistic",
    "text": "Probabilistic\n\nContinuous Ranked Probability Score (CRPS)\n\nfabletools::accuracy\nMeasures forecast distribution accuracy\nCombines a MAE score with the spread of simulated point forecasts\nSee notebook (pg 172)\n\nWinkler Score\n\nfabletools::accuracy\nMeasures how well a forecast is covered by the prediction intervals (PI)\nSee notebook (pg 172)",
    "crumbs": [
      "Diagnostics",
      "Forecasting"
    ]
  },
  {
    "objectID": "qmd/diagnostics-glm.html",
    "href": "qmd/diagnostics-glm.html",
    "title": "GLM",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Diagnostics",
      "GLM"
    ]
  },
  {
    "objectID": "qmd/diagnostics-glm.html#sec-diag-glm-misc",
    "href": "qmd/diagnostics-glm.html#sec-diag-glm-misc",
    "title": "GLM",
    "section": "",
    "text": "The degrees of freedom are related to the number of observations, and how many predictors you have used. If you look at the mean value in the prostate dataset for recurrence, it is 0.1708861, which means that 17% of the participants experienced a recurrence of prostate cancer. If you are calculating the mean of 315 of the 316 observations, and you know the overall mean of all 315, you (mathematically) know the value of the last observation - recurrence or not - it has no degrees of freedom. So for 316 observations, you have n-1 or 315, degrees of freedom. For each predictor in your model you ‘use up’ one degree of freedom. The degrees of freedom affect the significance of the test statistic (T, or chi-squared, or F statistic).\n\nShould be in the summary of the model\n\nChi Square test for the deviance only works for nested models\n** The formulas for the deviances for a logistic regression model are slightly different since the deviance for the saturated logistic regression model is 0 **",
    "crumbs": [
      "Diagnostics",
      "GLM"
    ]
  },
  {
    "objectID": "qmd/diagnostics-glm.html#sec-diag-glm-dev",
    "href": "qmd/diagnostics-glm.html#sec-diag-glm-dev",
    "title": "GLM",
    "section": "Deviance Metrics",
    "text": "Deviance Metrics\n\nMisc\n\nNotes from Saturated Models and Deviance video\nDeviance is 2 * Log Likelihood\n\nLog likelihood can usually be extracted from the model object (e.g. ll_proposed &lt;- mod$logLik)\n\nSaturated Model also called the Full model (Also see Regression, Discrete &gt;&gt; Misc)\n\nThe full model has a parameter for each observation and describes the data perfectly while the reduced model provides a more concise description of the data with fewer parameters.\nUsually calculated from the data themselves\ndata(wine, package = \"ordinal\")\ntab &lt;- with(wine, table(temp:contact, rating))\n## Get full log-likelihood (aka saturated model log-likelihood)\npi.hat &lt;- tab / rowSums(tab)\n(ll.full &lt;- sum(tab * ifelse(pi.hat &gt; 0, log(pi.hat), 0))) ## -84.01558\n\nGOF: as a rule of thumb, if the deviance about the same size as the difference in the number of parameters (i.e. pfull - pproposed), there is NOT evidence of lack of fit. ({ordinal} vignette, pg 14)\n\nExample (have doubts this is correct)\n\nLooking at the number of params (“no.par”) for fm1 in Example: {ordinal}, model selection with LR tests below and the model summary in Proportional Odds (PO) &gt;&gt; Example: {ordinal}, response = wine rating (1 to 5 = most bitter), the number of parameters for the reduced model is the number of regression parameters (2) + number of thresholds (4)\nFor the full model (aka saturated), the number of thresholds should be the same, and there should be one more regression parameter, an interaction between “temp” and “contact”. So, 7 should be the number of parameters for the full model\nTherefore, for a good-fitting model, the deviance should be close to pfull - preduced = 7 - 6 = 1\nThis example uses “number of parameters” which is the phrase in the vignette but I think it’s possible he might mean degrees of freedom (dof) which he immediatedly discusses afterwards. In the LR Test example below, under LR.Stat, which is essentially what deviance is, the number is around 11 which is quite aways from 1. Not exactly an apples to apples comparison, but the size after adding 1 parameter just makes me wonder if dof would match this scale of numbers for deviances better.\n\n\n\nResidual Deviance (G2): Dresid = Dsaturated - Dproposed\n\n2*log_likelihood between a saturated model and the proposed model\n\n2 *(LL(sat_mod) - LL(proposed_mod))\n-2 * (LL(proposed_mod) - LL(sat_mod))\n\nSee example 7, pg 13 ({ordinal} vignette) for (manual) code\nYour residual deviance should be lower than the null deviance. You can even measure whether your model is significantly better than the null model by calculating the difference between the Null Deviance and the Residual Deviance. This difference [281.9 - 246.8 = 35.1] has a chi-square distribution. You can look up the value for chi-square with 2 degrees (because you had 2 predictors) of freedom. Or you can calculate this in R with pchisq(q = 35.1, df=2, lower.tail = TRUE) which gives you a p value of 1.\n\nNull Deviance: Dnull = Dsaturated - Dnull\n\nAs a GOF for a single model, a model can be compared to the Null model (aka intercept-only model)\n2*log_likelihood between a saturated model and the intercept-only model (aka Null model)\n\n2 *(LL(sat_mod) - LL(null_mod))\n-2 * (LL(null_mod) - LL(sat_mod))\n\n\nMcFadden’s Pseudo R2 = (LL(null_mod) - LL(proposed_mod)) / (LL(null_mod) - LL(saturated_mod))\n\nThe p-value for this R2 is the same as the p-value for:\n\n2 * (LL(proposed_mod) - LL(null_mod))\nNull Deviance - Residual Deviance\n\nFor the dof, use proposed_dof - null_dof\n\ndof for the null model is 1\n\n\n\nExample: Getting the p-value\nm1 &lt;- glm(outcome ~ treat)\nm2 &lt;- glm(outcome ~ 1)\n(ll_diff &lt;- logLik(m1) - logLik(m2))\n## 'log Lik.' 3.724533 (df=3)\n1 - pchisq(2*ll_diff, 3)\n\nLikelihood Ratio Test (LR Test) - For a pair of nested models, the difference in −2ln L values has a χ2 distribution, with degrees of freedom equal to the difference in number of parameters estimated in the models being compared.\n\nRequirement: Deviance tests are fine if the expected frequencies under the proposed model are not too small and as a general rule they should all be at least five.\n\nAlso see Discrete Analysis notebook\n\nExample\n\nχ2 = (-2)*log(model1_likelihood) - (-2)*log(model2_likelihood) = 4239.49 – 4234.02 = 5.47\n\n-2*log can probably be factored out\n\ndegrees of freedom = model1_dof - model2_dof = 12 – 8 = 4\npval &gt; 0.05 therefore the likelihoods of these models are not signficantly different\n\n\nCompare nested models\n\nExample: LR Test Manually\n\nModels\nmodel1 &lt;- glm(TenYearCHD ~ ageCent + currentSmoker + totChol, \n              data = heart_data, family = binomial)\nmodel2 &lt;- glm(TenYearCHD ~ ageCent + currentSmoker + totChol + \n                as.factor(education), \n              data = heart_data, family = binomial)\n\nAdd Education or not?\n\nExtract Deviances\n# Deviances\n(dev_model1 &lt;- glance(model1)$deviance)\n## [1] 2894.989\n(dev_model2 &lt;- glance(model2)$deviance)\n## [1] 2887.206\nCalculate difference and test significance\n# Drop-in-deviance test statistic\n(test_stat &lt;- dev_model1 - dev_model2)\n## [1] 7.783615\n\n# p-value\n1 - pchisq(test_stat, 3)  # 3 = number of new model terms in model2 (i.e. 3(?) levels of education)\n## [1] 0.05070196\n\n\nExample: LR test with anova\nanova(fm2, fm1)\n\nLikelihood ratio tests of cumulative link models:\n    formula:                link:  threshold:\nfm2 rating ~ temp          logit  flexible\nfm1 rating ~ temp + contact logit  flexible\n    no.par AIC    logLik  LR.stat df Pr(&gt;Chisq)\nfm2 5      194.03 -92.013\nfm1 6      184.98 -86.492  11.043  1  0.0008902 ***",
    "crumbs": [
      "Diagnostics",
      "GLM"
    ]
  },
  {
    "objectID": "qmd/diagnostics-glm.html#sec-diag-glm-resid",
    "href": "qmd/diagnostics-glm.html#sec-diag-glm-resid",
    "title": "GLM",
    "section": "Residuals",
    "text": "Residuals\n\nNotes from Deviance Residuals video\nThe Deviance Residuals should have a Median near zero, and be roughly symmetric around zero.\n\nIf the median is close to zero, the model is not biased in one direction (the outcome is not over- nor under-estimated).\n\nDeviance residuals are like the values from computing the residual deviance at each data point\n\n\nTop line: “3.3” is the likelihood for a data point in the saturated model and “1.8” is the likelihood for that same data point in the proposed model\nTherefore, squaring each residual and summing them would give you the Residual Deviance for the model.",
    "crumbs": [
      "Diagnostics",
      "GLM"
    ]
  },
  {
    "objectID": "qmd/diagnostics-mixed-effects.html",
    "href": "qmd/diagnostics-mixed-effects.html",
    "title": "Mixed Effects",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Diagnostics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/diagnostics-mixed-effects.html#sec-diag-me-misc",
    "href": "qmd/diagnostics-mixed-effects.html#sec-diag-me-misc",
    "title": "Mixed Effects",
    "section": "",
    "text": "Test all optimizers\n\nTo assess whether convergence warnings render the results invalid, or to the contrary, the results can be deemed valid in spite of the warnings, Bates et al. (2023) suggest refitting models affected by convergence warnings with a variety of optimizers. (article)\n\nExample: {lme4}\nlibrary(lme4)\nlibrary(dfoptim)\nlibrary(optimx)\n\nfit &lt;- lmer(fatigue ~ spin * reg + (1|ID),\n           data = testdata, REML = TRUE)\n\n# Refit model using all available algorithms\nmulti_fit &lt;- allFit(fit)\n#&gt; bobyqa : [OK]\n#&gt; Nelder_Mead : [OK]\n#&gt; nlminbwrap : [OK]\n#&gt; nmkbw : [OK]\n#&gt; optimx.L-BFGS-B : [OK]\n#&gt; nloptwrap.NLOPT_LN_NELDERMEAD : [OK]\n#&gt; nloptwrap.NLOPT_LN_BOBYQA : [OK]\nsummary(multi_fit)$fixef\n#&gt;                               (Intercept)      spin       reg  spin:reg\n#&gt; bobyqa                          -2.975678 0.5926561 0.1437204 0.1834016\n#&gt; Nelder_Mead                     -2.975675 0.5926559 0.1437202 0.1834016\n#&gt; nlminbwrap                      -2.975677 0.5926560 0.1437203 0.1834016\n#&gt; nmkbw                           -2.975678 0.5926561 0.1437204 0.1834016\n#&gt; optimx.L-BFGS-B                 -2.975680 0.5926562 0.1437205 0.1834016\n#&gt; nloptwrap.NLOPT_LN_NELDERMEAD   -2.975666 0.5926552 0.1437196 0.1834017\n#&gt; nloptwrap.NLOPT_LN_BOBYQA       -2.975678 0.5926561 0.1437204 0.1834016\n\nArticle also has a custom plotting function to visually compare the results",
    "crumbs": [
      "Diagnostics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/diagnostics-mixed-effects.html#sec-diag-me-resid",
    "href": "qmd/diagnostics-mixed-effects.html#sec-diag-me-resid",
    "title": "Mixed Effects",
    "section": "Residuals",
    "text": "Residuals\n\nMisc\n\n{DHARMa} - Built for Mixed Effects Models for count distributions but also handles lm, glm (poisson) and MASS::glm.nb (neg.bin)\n\nBinned Residuals\n\nIt is not useful to plot the raw residuals, so examine binned residual plots\nMisc\n\n{arm} will mask some {tidyverse} functions, so don’t load whole package\n\nLook for :\n\nPatterns\nNonlinear trend may be indication that squared term or log transformation of predictor variable required\nIf bins have average residuals with large magnitude\nLook at averages of other predictor variables across bins\nInteraction may be required if large magnitude residuals correspond to certain combinations of predictor variables\n\nProcess\n\nExtract raw residuals\n\nInclude type.residuals = \"response\" in the broom::augment function to get the raw residuals\n\nOrder observations either by the values of the predicted probabilities (or by numeric predictor variable)\nUse the ordered data to create g bins of approximately equal size.\n\nDefault value: g = sqrt(n)\n\nCalculate average residual value in each bin\nPlot average residuals vs. average predicted probability (or average predictor value)\n\nExample: vs Predicted Values\n\narm::binnedplot(x = risk_m_aug$.fitted, y = risk_m_aug$.resid,\n                xlab = \"Predicted Probabilities\",\n                main = \"Binned Residual vs. Predicted Values\",\n                col.int = FALSE)\nExample: vs Predictor\n\narm::binnedplot(x = risk_m_aug$ageCent,\n                y = risk_m_aug$.resid,\n                col.int = FALSE,\n                xlab = \"Age (Mean-Centered)\",\n                main = \"Binned Residual vs. Age\")\n\nCheck that residuals have mean zero: mean(resid(mod))\nCheck that residuals for each level of categorical have mean zero\nrisk_m_aug %&gt;%\n  group_by(currentSmoker) %&gt;%\n  summarise(mean_resid = mean(.resid))\nCheck for normality.\n# Normal Q-Q plot\nqqnorm(resid(mod))\nqqline(resid(mod))\nCheck normality per categorical variable level\n\nExample: 3 levels\n## by level\npar(mfrow=c(1,3))\n\nqqnorm(resid(mod)[1:6])\nqqline(resid(mod)[1:6])\n\nqqnorm(resid(mod)[7:12])\nqqline(resid(mod)[7:12])\n\nqqnorm(resid(mod)[13:18])\nqqline(resid(mod)[13:18])\n\nData should be sorted by random variable level before modeling. Otherwise you could column bind the residuals to the original data. Then, group by random variable and make q-q plots for each group",
    "crumbs": [
      "Diagnostics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/diagnostics-model-agnostic.html",
    "href": "qmd/diagnostics-model-agnostic.html",
    "title": "Model Agnostic",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Diagnostics",
      "Model Agnostic"
    ]
  },
  {
    "objectID": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-misc",
    "href": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-misc",
    "title": "Model Agnostic",
    "section": "",
    "text": "Other packages\n\n{pdp} - has ggplot option\n\nIndividual prediction interpretation uses:\n\nLook at extreme prediction values and see what predictor variable values are driving those predictions\nExamine distribution of prediction values (on observed or new data)\n\nMulti-Modal? Which variables are driving the different mode’s predicitions\nBreak predictions down by cat variable. If differences between levels are apparent, which predictor variable values are driving those differences in predictions\n\nML model predicts customer in observed data has high probability of conversion yet customer hasn’t converted. Develop strategy around predictor variables (increase or decrease, do or stop doing something) that contributed to that prediction to hopefully nudge that customer into converting",
    "crumbs": [
      "Diagnostics",
      "Model Agnostic"
    ]
  },
  {
    "objectID": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-dalex",
    "href": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-dalex",
    "title": "Model Agnostic",
    "section": "DALEX",
    "text": "DALEX\n\nMisc\n\nNotes from Explanatory Model Analysis\n\nBook shows code snippets for R and Python\n\nAlso available for Python models\nModels from packages handled by {DALEXtra}: scikit-learn, keras, H2O, tidymodels, xgboost, mlr or mlr3\nThe first step is always going to be creating an “explain” object\ntitanic_glm_model &lt;- \n  glm(survived~., \n      data = titanic_imputed, \n      family = \"binomial\")\nexplainer_glm_titanic &lt;- \n  explain(titanic_glm_model, \n          data = titanic_imputed[,-8],\n          y = titanic_imputed$survived)\n\nCustom explainers can be constructed for models not supported by the package. You need to specify directly the model-object, the data frame used for fitting the model, the function that should be used to compute predictions, and the model label.\n{DALEXtra} has helper functions set-up to support {{sklearn}}, {{keras}}, {h2o}, {mlr3}, {tidymodels}, and {xgboost} models.\n\n\n\n\nExplanatory Levels\n\n\nInstance Level\n\nThe model exploration for an individual instance starts with a single number — a prediction. This is the top level of the pyramid.\nTo this prediction we want to assign particular variables, to understand which are important and how strongly they influence this particular prediction. One can use methods as SHAP, LIME, Break Down, Break Down with interactions. This is the second from the top level of the pyramid.\nMoving down, the next level is related to the sensitivity of the model to change of one or more variables’ values. Ceteris Paribus profiles allow to explore the conditional behaviour of the model.\nGoing further, we can investigate how good is the local fit of the model. It may happen, that the model is very good on average, but for the selected observation the local fit is very low, errors/residuals are larger than on average. The above pyramid can be further extended, i.e. by adding interactions of variable pairs.\n\nDataset Level\n\nThe exploration for the whole model starts with an assessment of the quality of the model, either with F1, MSE, AUC or LIFT/ROC curves. Such information tells us how good the model is in general.\nThe next level helps to understand which variables are important and which ones make the model work or not. A common technique is permutation importance of variables.\nMoving down, methods on the next level help us to understand what the response profile of the model looks like as a function of certain variables. Here you can use such techniques as Partial Dependence Profiles or Accumulated Local Dependence.\nGoing further we have more and more detailed analysis related to the diagnosis of the errors/residuals.\n\n\n\n\nInstance Level\n\nUse Case:\n\nInvestigate extreme response values\nInvestigate observations not predicted well by your model\n\n\n\nBreak-Down (BD)\n\nAllows you to see how predictor variables contribute to prediction.\nBD Charts shows additive contributions of variables when they are sequentially added to the model\nBreak-down (BD) plots and Shapley values are most suitable for models with a small or moderate number of explanatory variables. Neither of those approaches is well-suited for models with a very large number of explanatory variables, because they usually determine non-zero attributions for all variables in the model.\n\nFor data with many predictors, use LIME.\n\nInterpretation\n\n\nModel predicts survival probability of a person on the Titanic\nintercept is the mean of all the predictions from of the model on the training data\nFixing age = 8 adds 0.27 probability points to the mean overall prediction\nFixing age = 8 and class = 1st adds \\((0.27 + 0.086 = 0.356)\\) to the mean overall prediction\nprediction is the point prediction for an observation that has all of these predictors fixed at these values\n\nWhen interactions, whether explicit like in linear regression models or implicit like in Random Forest models, are present in the model, the order that the predictors are presented to the DALEX function affects the estimated contribution.\n\n\nWhen interactions aren’t present in the model, then contributions should be the same regardless of ordering of the predictors.\nHere class and age have been presented to the DALEX function is different orders and have different contributions which shows that the RF model is using an interaction of the two predictors.\n\nExample: Assume No Interactions\nlibrary(\"randomForest\")\nlibrary(\"DALEX\")\nexplain_rf &lt;- \n  DALEX::explain(model = titanic_rf,  \n                 data = titanic_imputed[, -9],\n                 y = titanic_imputed$survived == \"yes\", \n                 label = \"Random Forest\")\nbd_rf &lt;- \n  predict_parts(explainer = explain_rf,\n                new_observation = henry,\n                type = \"break_down\")\nbd_rf\n##                                     contribution\n## Random Forest: intercept                   0.235\n## Random Forest: class = 1st                 0.185\n## Random Forest: gender = male              -0.124\n## Random Forest: embarked = Cherbourg        0.105\n## Random Forest: age = 47                   -0.092\n## Random Forest: fare = 25                  -0.030\n## Random Forest: sibsp = 0                  -0.032\n## Random Forest: parch = 0                  -0.001\n## Random Forest: prediction                  0.246\n\nplot(bd_rf) will plot the BD chart\ntype - The method for calculation of variable attribution; the possible methods are “break_down” (the default), “shap”, “oscillations”, and “break_down_interactions”\norder - A vector of characters (column names) or integers (column indexes) that specify the order of explanatory variables to be used for computing the variable-importance measures; if not specified (default), then a one-step heuristic is used to determine the order\nkeep_distributions - A logical value (FALSE by default); if TRUE, then additional diagnostic information about conditional distributions of predictions is stored in the resulting object and can be plotted with the generic plot() function.\n\nWhen this is TRUE, plot will output the BD plot, but instead of point estimates of the contributions for each predictor, violin plots visualize the distribution of contribution values.\nThere are also lines that connect predictions from one distribution to the next distribution which indicates how each model prediction changed wthen the next predictor was added.\nNot very aesthetically pleasing. If you want to use these distributions in a presentation and aren’t concerned with the lines, I’d recommend getting the values and making a raincloud plot with {{ggdist}} or some other package.\n\n\nExample: Assume Interactions\n\n##                                             contribution\n## Random Forest: intercept                           0.235\n## Random Forest: class = 1st                         0.185\n## Random Forest: gender = male                      -0.124\n## Random Forest: embarked:fare = Cherbourg:25        0.107\n## Random Forest: age = 47                           -0.125\n## Random Forest: sibsp = 0                          -0.032\n## Random Forest: parch = 0                          -0.001\n## Random Forest: prediction                          0.246\n\nSame code as before except in predict_parts, type = “break_down_interactions”\n\nFinds an ordering of the predictors in which the most important predictors are placed at the beginning\n\niBD detects an important interaction between fare and class\nCan be time consuming as the running time is quadratic depending on the number of predictors, \\(O(p^2)\\). \\(\\frac{p(p+1)}{2}\\) net contributions for single variables and pairs of variables have to be calculated.\nFor datasets with a small number of observations, the calculations of the net contributions will be subject to a larger variability and, therefore, larger randomness in the ranking of the contributions.\n\n\n\n\nLime\n\nArtificial data is generated around the neighborhood of instance of interest. Predictions are generated by the black-box model. A glass-box model (e.g. lasso, decision-tree, etc.) is trained on the artificial data and predictions. Then, the glass-box model is used as a proxy model for the black-box model for interpreting that instance.\nWidely adopted in the text and image analysis but also suitable for tabular data with many predictors\n\nFor models with large numbers of variables, sparse explanations with a small number of variables offer a useful alternative to BD and SHAP.\n\nIssues\n\nRequires numeric predictors, so different LIME methods will have different transformations of the categorical variables and will provide different results.\nThe glass-box model is selected to approximate the black-box model, and not the data themselves, the method does not control the quality of the local fit of the glass-box model to the data. Thus, the latter model may be misleading.\nSometimes even slight changes in the neighborhood where the artificial data is generated can strongly affect the obtained explanations.\n\nSee Ch. 9.6 for discussion of some of the differences in LIME implementation between {lime}, {localModel}, and {iml}.\nExample: {localModel}\nlibrary(\"localModel\")\nlocMod_johnny &lt;- \n  DALEXtra::predict_surrogate(explainer = explain_rf, \n                              new_observation = johnny_d, \n                              size = 1000, \n                              seed = 1,\n                              type = \"localModel\")\nlocMod_johnny[,1:3]\n##     estimated                        variable original_variable\n## 1  0.23530947                    (Model mean)                  \n## 2  0.30331646                     (Intercept)                  \n## 3  0.06004988                   gender = male            gender\n## 4 -0.05222505                    age &lt;= 15.36               age\n## 5  0.20988506     class = 1st, 2nd, deck crew             class\n## 6  0.00000000 embarked = Belfast, Southampton          embarked\n\ntype = “localModel” says which package to use\n\n{lime} and {iml} available\n\nestimated shows the coefficients of a LASSO glass-box model\nNote the dichotomization of the continuous variable, age\n\nCutpoint chosen using ceteris-paribus profiles. The point at which the largest change in the response is chosens as the cutpoint.\n\nCP Profile: localModel::plot_interpretable_feature(locMod_johnny, \"age\")\n\n\n{iml} doesn’t transform continuous variables and {lime} uses quartile cutpoints\n\nVisualizaton: plot(locMod_johnny)\n\n\n\n\n\nCeteris Paribus (CP)\n\nA CP profile shows how a model’s prediction would change if the value of a single exploratory variable changed\n\nA pdp is the average of cp profiles for all observations.\n\nIf calculated for multiple observations, CP profiles are a useful tool for sensitivity analysis.\nThe larger influence of an explanatory variable on prediction for a particular instance, the larger the fluctuations of the corresponding CP profile. For a variable that exercises little or no influence on a model’s prediction, the profile will be flat or will barely change.\nFor categoricals or discrete variables, bar graphs are used to visualize the CP values.\nIssues\n\nCorrelated explanatory variables may lead to misleading results , as it is not possible to keep one variable fixed while varying the other one.\nIf an interaction is present, whether explicit like in linear regression models or implicit like in Random Forest models, results can be misleading.\n\nPairwise interactions require the use of two-dimensional CP profiles that are more complex than one-dimensional ones.\n\n\nProcess\n\nSupply a single observation with values of your choice and a predictor.\nThat predictor varies while the others in the model are held constant, and predictions are calculated.\nResponses are plotted vs the predictor.\n\nExample: Compare 2 observations\n\nvariable_splits = list(age = seq(0, 70, 0.1), \n                       fare = seq(0, 100, 0.1))                      )\ncp_titanic_rf2 &lt;- \n  predict_profile(explainer = explain_rf, \n                  new_observation = rbind(henry, johnny_d),\n                  variable_splits = variable_splits)\nlibrary(ingredients)\nplot(cp_titanic_rf2, \n     color = \"_ids_\", \n     variables = c(\"age\", \"fare\")) + \n  scale_color_manual(name = \"Passenger:\", \n                     breaks = 1:2, \n                     values = c(\"#4378bf\", \"#8bdcbe\"), \n                     labels = c(\"henry\" , \"johny_d\")) \n\npredict_profile\n\nvariables - (Not used in this example) Names of explanatory variables, for which CP profiles are to be calculated. By default, variables = NULL and the profiles are constructed for all variables, which may be time consuming.\nvariable_splits is an optional argument for providing a custom range of predictor values.\n\nBy default the function uses the range of values in the training data which is obtained throught the explainer object.\nAlso, limits the computations to the variables specified in the data.frame\n\n\nplot\n\ncolor = “_ids_” specifies that more than one observation is being compared\n\n\nExample: Compare Multiple Models\n\nplot(cp_titanic_rf, cp_titanic_lmr, color = \"_label_\",  \n     variables = c(\"age\", \"fare\")) +\n     ggtitle(\"Ceteris-paribus profiles for Henry\", \"\") \n\nage has a non-linear shape because a spline transform was used\n\nProfile Oscillations\n\nMethod to assign importance to individual CP Profiles. Useful when there are a large number of variables used in your model.\n\nFor models with hundreds of variables or discrete variables like zip codes, visual examination can be daunting.\n\nThe method estimates the area under the CP curve by summing the differences between the CP values and the prediction at that instance.\n\nExample: Basic\n\noscillations_uniform &lt;- \n  predict_parts(explainer = explain_rf, \n                new_observation = henry, \n                type = \"oscillations_uni\")\n\noscillations_uniform\n##    _vname_ _ids_ oscillations\n## 2   gender     1   0.33700000\n## 4    sibsp     1   0.16859406\n## 3      age     1   0.16744554\n## 1    class     1   0.14257143\n## 6     fare     1   0.09942574\n## 7 embarked     1   0.02400000\n## 5    parch     1   0.01031683\n\noscillations_uniform$`_ids_` &lt;- \"Henry\"\nplot(oscillations_uniform) +\n    ggtitle(\"Ceteris-paribus Oscillations\", \n            \"Expectation over uniform distribution (unique values)\")\n\nGender is a CP Profile that you should definitely look at. Followed by sibsp, age, and probably class\nvariable_splits is optional. Available if you want to use a custom grid of variables and values.\ntype\n\ntype = “oscillations” is used when variable_splits is used. An average residual is what’s being calculated, and one main difference in the types is the divisor (e.g. number of unique values, sample size) that’s used. Here, I’m guessing this argument value sets the divisor to the number of grid values in variable_splits.\ntype = “oscillations_uni” assumes a uniform distribution of the “residuals” which are differences between the CP value and prediction for that instance.\n\nFilters variables to only unique values\nQuicker to compute\n\ntype = “oscillations_emp” assumes an empirical distribution of the “residuals”\n\nPreferred when there are enough data to accurately estimate the empirical distribution and when the distribution is not uniform.\n\n\n\n\n\n\nLocal Diagnostics\n\nCheck how the model behaves locally for observations similar to the instance of interest.\nNeighbors of the instance of interest are chosen using a distance metric that includes all variables — default being the gower distance which handles categorical and continuous variables.\nLocal Fidelity\n\nCompares the distribution of residuals for the neighbors with the distribution of residuals for the entire training dataset.\nA histogram with both sets of residuals is created.\n\nBoth distributions should be similar. But, if the residuals for the neighbors are shifted towards positive or negative values, then the model has issues predicting instances with variable values in this space.\n\nNon-parametric tests such as Wilcoxon or Kolmogorov-Smirnov test can also be used to test for differences.\n\nLocal Stability\n\nChecks whether small changes in the explanatory variables, as represented by the changes within the set of neighbors, have got much influence on the predictions.\nCP profiles are calculated for each of the selected neighbors\n\nAdding residuals to the plot allows for evaluation of the local model-fit.\n\nInterpretation\n\nParallel lines suggests the relationship between the variable and the response is additive.\nA small distance between the lines suggests model predictionss are stable for values of that variable around the supplied value.\n\n\nExample\n\nLocal-Fidelity\n\nid_rf &lt;- \n  predict_diagnostics(explainer = explain_rf,\n                      new_observation = henry,\n                      neighbours = 100)\nid_rf\n##  Two-sample Kolmogorov-Smirnov test\n## \n## data:  residuals_other and residuals_sel\n## D = 0.47767, p-value = 4.132e-10\n## alternative hypothesis: two-sided\nplot(id_rf)\n\nKS-Test indicates a statistically significant difference between the two distributions\nThe plot suggests that the distribution of the residuals for Henry’s neighbours might be slightly shifted towards positive values, as compared to the overall distribution.\nI don’t think the y-axis values are informative. This is a split axis histogram where neighbor residuals are on top and the rest of the observations are on the bottom.\nThere is a nbinsargument which has a default of twenty. This chart many need a few more bins\n\nLocal-Stability\n\nid_rf_age &lt;- \n  predict_diagnostics(explainer = explain_rf,\n                      new_observation = henry,\n                      neighbours = 10,\n                      variables = \"age\")\nplot(id_rf_age)\n\nThe vertical segments correspond to residuals\n\nShorter the segment, the smaller the residual and the more accurate prediction of the model.\nGreen segments correspond to positive residuals, red segments to negative residuals.\n\nThe profiles are relatively close to each other, suggesting the stability of predictions.\nThere are more negative than positive residuals, which may be seen as a signal of a (local) positive bias of the predictions.\n\n\n\n\n\n\nDataset Level\n\nFitted vs Observed\n\ndiag_ranger &lt;- model_diagnostics(explainer_ranger)\nplot(diag_ranger, variable = \"y\", yvariable = \"y_hat\") +\n  geom_abline(colour = \"red\", intercept = 0, slope = 1)\n\nRed line is the perfect fitting model\nShows that, for large observed values of the dependent variable, the predictions are smaller than the observed values, with an opposite trend for the small observed values of the dependent variable. (Also see Residuals vs Observed)\n\n\n\nResidual Plots\n\nMisc\n\nFor ML models, can help in detecting groups of observations for which a model’s predictions are biased.\nFrom Chapter 19\n\nHistogram Plot\n\nmr_lm &lt;- model_performance(explain_apart_lm)\nmr_rf &lt;- model_performance(explain_apart_rf)\nplot(mr_lm, mr_rf, geom = \"histogram\") \n\nThe split into two separate, normal-like parts, which may suggest omission of a binary explanatory variable in the model.\n\nBoxplot\n\nplot(mr_lm, mr_rf, geom = \"boxplot\")\n\nRed dot represents RMSE (i.e. mean of the residuals)\nThe residuals for the random forest model are more frequently smaller than the residuals for the linear-regression model. However, a small fraction of the random forest-model residuals is very large, and it is due to them that the RMSE is comparable for the two models.\nThe separation of the red dot (mean) and line (median) indicate that the residual distribution is skewed to the right.\n\nResiduals vs Observation ID\n\nmd_rf &lt;- model_diagnostics(explainer_ranger)\nplot(md_rf, variable = \"ids\", yvariable = \"residuals\")\n\nShows an asymmetric distribution of residuals around zero, as there is an excess of large positive (larger than 500) residuals without a corresponding fraction of negative values (i.e. right-skewed distribution)\n\nResiduals vs Fitted - plot(md_rf, variable = \"y_hat\", yvariable = \"residuals\")\n\n\nShould be symmetric around the horizontal at zero\nSuggests that the predictions are shifted (biased) towards the average.\n\nResiduals vs Observed - plot(md_rf, variable = \"y\", yvariable = \"residuals\")\n\n\nShould be symmetric around the horizontal at zero\nShows that, for the large observed values of the dependent variable, the residuals are positive, while for small values they are negative. This trend is clearly captured by the smoothed curve included in the graph. Thus, the plot suggests that the predictions are shifted (biased) towards the average. (See Fitted vs Observed)\n\nAbs Residuals vs Fitted - plot(md_rf, variable = \"y_hat\", yvariable = \"abs_residuals\")\n\n\nVariation of the Scale-Location plot\nShould be symmetric scatter around a horizontal line to indicate homoskedastic variance of the residuals\nLarge concern for linear regression models, but potentially not a concern for tree models like RF\n\nFor tree models, you have to decide whether the bias is acceptable\n\n\n\n\n\nFeature Importance\n\nFrom Chapter 16\nModel-agnostic method used allows comparing an explanatory-variable’s importance between models with different structures to check for agreement.\n\nIf variables are ranked in importance differently in different models, then compare in pdp, factor, or ale plots. See if one of the models that has the variable ranked higher captures a different (e.g. non-linear) relationship with response better than the other models\n\nThe permutation step means there some randomness involved, so it should be repeated many times. This will give you a distribution of importance values for each variable. Then, you can calculate a an interval based on a chosen quantile to represent the uncertainty.\nOverview\n\nTakes a variable, randomizes its rows, measures change in loss function from full model.\nVars that have largest changes are of greater importance (longer bars).\n\nSome of the Args\n\nN = 1000 (default), = NULL to use whole dataset (slower).\nB = 10 (default) - Number of iterations (i.e. number of times you go through the permutation process for each variable)\nType = “raw” (default) is just the value of loss function. You can use “difference” or “ratio” (shown in Steps section), but the ranking isn’t affected\n\nSteps for any given loss function\n\nCompute the value for the loss function for original model\nFor variable i in {1,…,p}\n\nPermute values of explanatory variable\nApply given ML model\nCalculate value of loss function\nCompute feature importance (permuted loss / original loss)\n\nSort variables by descending feature importance\n\nExample\n\nvars &lt;- c(\"surface\",\"floor\",\"construction.year\",\"no.rooms\",\"district\")\nmodel_parts(explainer = explainer_rf, \n        loss_function = loss_root_mean_square,\n                    B = 50,\n            variables = vars)\nlibrary(\"ggplot2\")\nplot(vip.50) +\n  ggtitle(\"Mean variable-importance over 50 permutations\", \"\") \n\nBox-plot represents the distribution of importance values for that variable\n\n\n\n\nPartial Dependence Profiles (PDPs)\n\nMisc\n\nFrom Chapter 17\nThe PDP curve represents the average prediction across all observations while holding a predictor, x, at a constant value\n\nKeep a new data pt constant and calculate a prediction for each observed value of the other predictors then take the average of the predictions\nPartial Dependence profiles are averages of Ceteris-Paribus profiles\n\nWhen comparing PDPs between Regression and Tree models, expect to see flatter areas for tree models as they tend to shrink predictions towards the average and they are not very good for extrapolation outside the range of values observed in the training dataset.\nFor additive models, CP profiles are parallel. For models with interactions, CP profiles may not be parallel.\n\nUse Cases\n\nAgreement between profiles for different models is reassuring. Some models are more flexible than others. If PD profiles for models, which differ with respect to flexibility, are similar, we can treat it as a piece of evidence that the more flexible model is not overfitting and that the models capture the same relationship.\nDisagreement between profiles may suggest a way to improve a model. If a PD profile of a simpler, more interpretable model disagrees with a profile of a flexible model, this may suggest a variable transformation that can be used to improve the interpretable model. For example, if a random forest model indicates a non-linear relationship between the dependent variable and an explanatory variable, then a suitable transformation of the explanatory variable may improve the fit or performance of a linear-regression model.\n\nExample:\n\npdp_lmr &lt;- \n  model_profile(explainer = explainer_lmr, \n                         variables = \"age\")\npdp_rf &lt;- \n  model_profile(explainer = explainer_rf, \n                        variables = \"age\")\nplot(pdp_rf, pdp_lmr) +\n    ggtitle(\"Partial-dependence profiles for age for two models\") \n\nLeft: Indicates that the linear regression isn’t capturing u-shape relationship\nRight: Indicates that the RF may be underestimating the effect\n\n\nEvaluation of model performance at boundaries. Models are known to have different behaviour at the boundaries of the possible range of a dependent variable, i.e., for the largest or the lowest values. For instance, random forest models are known to shrink predictions towards the average, whereas support-vector machines are known for a larger variance at edges. Comparison of PD profiles may help to understand the differences in models’ behaviour at boundaries.\n\nChecks\n\nHighly Correlated Variables\n\nPD profiles inherit the limitations of the CP profiles. In particular, as CP profiles are problematic for correlated explanatory variables may offer a crude and potentially misleading summarization\nIf highly correlated, use Accumulated Local Profies\n\nCheck CP profile for consistent behavior\n\nPD profiles are estimated by the mean of the CP profiles for all instances (observations) from a dataset (i.e. CP profiles are instance-level PD profiles).\nThe mean (i.e. PD) may not be representative of the relationship of the variable and the response\nIf the CP prole lines are parallel, then the PD profile is representative\nSolutions (If not parallel):\n\nCluster CP profiles - Cluster the CP profiles (e.g. k-means) and the entroids will be the PD lines\n\nExample\n\npdp_rf_clust &lt;- \n  model_profile(explainer = explainer_rf, \n                variables = \"age\", \n                k = 3)\nplot(pdp_rf_clust, \n     geom = \"profiles\") + \n    ggtitle(\"Clustered partial-dependence profiles for age\") \n\nGrouped-by CP profiles - Group CP profiles by a moderator variable\n\nDistinctive PD lines can indicate an interaction\nExample\n\npdp_rf_gender &lt;- \n  model_profile(explainer = explainer_rf, \n                variables = \"age\", \n                groups = \"gender\")\nplot(pdp_rf_gender, \n     geom = \"profiles\") + \n    ggtitle(\"Partial-dependence profiles for age, grouped by gender\") \n\nGender looks like a good candidate for an interaction\n\n\n\nExample: RF survival model\n\n\nThe shape of the PD profile does not capture, for instance, the shape of the group of five CP profiles shown at the top of the panel.\nIt does seem to reflect the fact that the majority of CP profiles (predicted probabilities below 0.75) suggest a substantial drop in the predicted probability of survival for the ages between 2 and 18.\n\n\n\nSteps\n\nDetermine grid space of j evenly spaced values across distribution of selected predictor, x\nFor value i in {1,…,j} of grid space\n\nset x == i for all n observations (x is a constant variable)\napply given ML model\nestimate n predicted values\ncalculate the average predicted value\n\ngraph y_hat vs x\n\nCode\npdp_rf &lt;- model_profile(explainer = explainer_rf, variables = \"age\")\nlibrary(\"ggplot2\")\nplot(pdp_rf) +  ggtitle(\"Partial-dependence profile for age\") \n\nOther Args\n\nN - The number of (randomly sampled) observations that are to be used for the calculation of the PD profiles (N = 100 by default); N = NULL implies the use of the entire dataset included in the explainer-object.\nvariable_type - A character string indicating whether calculations should be performed only for “numerical” (continuous) explanatory variables (default) or only for “categorical” variables.\ngroups - The name of the explanatory variable that will be used to group profiles, with groups = NULL by default (in which case no grouping of profiles is applied).\nk - The number of clusters to be created with the help of the hclust() function, with k = NULL used by default and implying no clustering.\ngeom = “profiles” in the plot function, we add the CP profiles to the plot of the PD profile.\n\n\n\n\n\nFunnel Plot\n\nFrom {DALEXtra}, lets us find subsets of data where one of models is significantly better than other ones.\nVery useful in situations where we have models that have similiar overall performance, but where predictive performance of units with certain characteristics are more important to our business than others.\nCan be used to create a sort of ensemble model where predictions from different models are used depending on the values of the predictors.\nExample\n\nexplainer_lm &lt;- \n  explain_mlr(model_lm, \n              apartmentsTest, \n              apartmentsTest$m2.price, \n              label = \"LM\", \n              verbose = FALSE, \n              precalculate = FALSE)\nexplainer_rf &lt;- \n  explain_mlr(model_rf, \n              apartmentsTest, \n              apartmentsTest$m2.price, \n              label = \"RF\",\n              verbose = FALSE, \n              precalculate = FALSE)\n\n plot_data &lt;- \n   funnel_measure(explainer_lm, \n                  explainer_rf,\n                  partition_data = cbind(apartmentsTest,\n                                         \"m2.per.room\" = apartmentsTest$surface/apartmentsTest$no.rooms),\n                  nbins = 5, \n                  measure_function = DALEX::loss_root_mean_square, \n                  show_info = FALSE)\n\nnbins is the number of bins to partition continuous variables into.\nmeasure_function is the metric used to judge each model. Here RMSE is used.\nLength of segment is the difference in prediction performance. The longer the segments are, the better that model is at predicting for those subsets of data.\nFor the most expensive and the cheapest (i.e. first and last bin of m2.price), the linear regression model performs best, while the random forest model performs best average priced (i.e. middle 3 bins of m2.price) apartments.\nThe linear regression is extremely better at predicting prices for apartments in the Srodmiescie district.",
    "crumbs": [
      "Diagnostics",
      "Model Agnostic"
    ]
  },
  {
    "objectID": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-triplot",
    "href": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-triplot",
    "title": "Model Agnostic",
    "section": "triplot",
    "text": "triplot\n\nMisc\n\nGithub, Docs\nTakes into account the correlation structure when assessing variable importance; global and local explanations\nTriplot Features:\n\nThe importance of every single feature,\nHierarchical aspects importance,\nOrder of grouping features into aspects.\n\n\nGlobal Triplot\n\nmod_explainer &lt;- \n  DALEX::explain(model_obj, \n                 data = dat_without_target, \n                 y = target_var, \n                 verbose = FALSE)\ntriplot_global &lt;- \n  triplot::model_triplot(mod_explainer, \n                         B = num_permutations, \n                         N = num_rows_sampled, \n                         corr_method)\nplot(triplot_global)\n\nCurrently only correlation methods for numeric features supported\nUsing small numbers of rows for permutations (N arg) will cause unstable results\nLeft Panel\n\nThe global importance of every single feature\nPermutation feature importance used\n\nCenter Panel\n\nThe importance of groups of variables determined by the hierarchical clustering\nImportance calc’d by permutation feature importance\nNumbers to left of the split point is the group importance\n\nRight Panel\n\nCorrelation structure visualized by hierarchical clustering\nGuess this is the same as the middle panel but a correlation method is used instead of feature importance\n\nInterpretation: Use the middle panel to see if adding correlated features increases group importance.\n\nAdding too many for little gain in importance may not be worth it, depending on sample size.\nMight be useful for deciding whether or not to create a combined feature\n\nUnclear on the technical details on how or what exactly is being clustered.\n\npredict_triplot (local)\n\nLike breakdown or shapley in that it’s goal is to assess the feature contribution to the prediction\n\nvariables can have negative or positive contributions to the prediction value\n\nCombines the approach to explanations used by LIME methods and visual techniques introduced in a global triplot\n# slice a row of original dataset that you want an explanation of the model prediction\ntarget_row &lt;- df %&lt;% slice(1)\ntriplot_local &lt;- triplot::predict_triplot(mod_explainer, target_row, N = num_rows_sampled, corr_method)\nplot(triplot_local)\nsame interpretation as model_triplot but for explaining the prediction of a target observation\nleft panel\n\nthe contribution to the prediction of every single feature\n\nmiddle panel\n\nthe contribution of aspects, that are built in the order determined by the hierarchical clustering\n\nright panel\n\ncorrelation structure of features visualized by hierarchical clustering\n\n\npredict_aspects (local)\n\naspects are groups of variables that can be thought of as latent variables\n\ncan use the middle or right panel from predict_triplot to get ideas on how to group your variables\nthere’s also a group_variables helper function that can group vars by a correlation cutoff value\n\n# Example\n# group variables\nfifa_aspects &lt;- list(\n  \"age\" = \"age\",\n  \"body\" = c(\"height_cm\", \"weight_kg\"),\n  \"attacking\" = c(\"attacking_crossing\",\n                  \"attacking_finishing\", \n                  \"attacking_heading_accuracy\",\n                  \"attacking_short_passing\", \n                  \"attacking_volleys\"))\n# Compare aspect importances from different models\npa_rf &lt;- predict_aspects(rf_explainer,\n                        new_observation = target_row,\n                        variable_groups = fifa_aspects)\npa_gbm &lt;- predict_aspects(gbm_explainer,\n                          new_observation = top_player,\n                          variable_groups = fifa_aspects)\nplot(pa_rf, pa_gbm)\n\noutput is two, side-by-side aspect importance plots\n\nbut they’re more like contribution plots, where aspects can have positive or negative contributions to the predicted value\n\nCan be used to compare models\n\nexamples\n\nif one model underpredicts a target observation more than another model, how do the feature contributions to that prediction differ between the two models?\nare there different aspects more important in one model than the other?\nif they’re the same aspects at the top, does one aspect stand out in one model while in the other model the importance values are more evenly spread?",
    "crumbs": [
      "Diagnostics",
      "Model Agnostic"
    ]
  },
  {
    "objectID": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-auditor",
    "href": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-auditor",
    "title": "Model Agnostic",
    "section": "auditor",
    "text": "auditor\n\nFor GOF measure, model similarity comparison using residuals. Also, uses residual plots and scores to check for asymmetry (around zero) in the distribution, trends, and heteroskedacity.\nAuditor objects only require a predict function and response var in order to be created.\nUsually scores use score(auditor_obj, type = “?”, …), plots use plot(auditor_obj, type = “?”, … )\nPlot function can facet multiple plots, e.g. plot(mod1, mod2, mod3, type = c(“ModelRanking”, “Prediction”), variable = “Observed response”, smooth = TRUE, split = “model”). smooth = TRUE adds a trend line. Split arg splits prediction vs observed plot into plots for each model.\nRegression Error Characteristic (REC) plot- GOF measure - type = “REC” - The y-axis is the percentage of residuals less than a certain tolerance (i.e. size of the residual) with that tolerance on the x-axis. The shape of the curve illustrates the behavior of errors. The quality of the model can be evaluated and compared for different tolerance levels. The stable growth of the accuracy does not indicate any problems with the model. A small increase of accuracy near 0 and the areas where the growth is fast signalize bias of the model predictions (jagged curve).\nArea Over the REC Curve (AOC) score - GOF measure - type = “REC” - is a biased estimate of the expected error for a regression model. Provides a measure of the overall performance of regression model. Smaller is better I’d think.\nRegression Receiver Operating Characteristic (RROC) plot - type = “RROC” - for regression to show model asymmetry. The RROC is a plot where on the x-axis we depict total over-estimation and on the y-axis total under-estimation.\nArea Over the RROC Curve score - GOF measure - type = “RROC” - equivalent to the error variance. Smaller is better I’d think\nModel Ranking plot and table - Multi-GOF measure - type = “ModelRanking” - radar plot of potentially five scores: MAE, MSE, RMSE, and the AOC scores for REC and RROC. They’ve been scaled in relation to the model with the best score for that particular metric. Best model for a particular metric will be farthest away from the center of the plot. In the table, larger scaled score is better while lower is better in the plain score column. You can also add a custom score function but you need to make sure that a lower value = best model.\nResiduals Boxplot - asymmetry measure - type = “ResidualBoxplot” - Pretty much the same as a regular boxplot except there’s a red dot which stands for the value of Root Mean Square Error (RMSE). Values are the absolute value of the residuals. Best models will have medians around zero and small spreads (i.e. short whiskers). Example in the documentation has a good example showing how a long whisker pulls the RMSE which is sensitive to outliers towards the edge of the box.\nResidual Density Plot - asymmetry measure - type = “ResidualDensity” - detects the incorrect behavior of residuals. For linear regressions, residuals should be normally distributed around zero. For other models, non-zero centered residuals can indicate bias. Plot has a rug which makes it possible to ascertain whether there are individual observations or groups of observations with residuals significantly larger than others. Can specify a cat predictor variable and see the shape of the density of residuals w.r.t. the levels of that variable.\nTwo-sided ECDF Plot - type = “TwoSidedECDF” - stands for Empirical Cumulative Distribution Functions. There’s an cumulative distribution curve for each positive and negative residuals. The plot shows the distribution of residuals divided into groups with positive and negative values. It helps to identify the asymmetry of the residuals. Points represent individual error values, what makes it possible to identify ‘outliers’\nResiduals vs Fitted and autocorrelation plots - type = “Residual”, type = “Autocorrelation” - same thing as base R plot. Any sort of grouping, trend, or pattern suggests an issue. Looking for randomness around zero. Can specify a cat predictor variable and see the trend of residuals w.r.t. the levels of that variable.\nAutocorrelation Function Plot (ACF) - autocorrelation check - type = “ACF” - same evaluation as a ACF plot for time series\nScale-Location Plot - type = “ScaleLocation” - The y-axis is the sqrt(abs(std_resids)) and x-axis is fitted values. Different from Resid vs Fitted since that plot uses raw residuals instead of scaled. Equation shows the resids are divided by their std.dev, so that should fix the variance at 1. The presence of any trend suggests that the variance depends on ﬁtted values, which is against the assumption of homoscedasticity. Not every model has an explicit assumption of homogeneous variance, however, the heteroscedasticity may indicates potential problems with the goodness-of-ﬁt. Residuals formed into separate groups suggest a problem with model structure (specification?)\nPeak Test - - type = “Peak” - tests for heteroskedacity. Score’s range is (0, 1]. Close to 1 means heteroskedacity present.\nHalf-Normal Plot - GOF measure - type = “HalfNormal” - graphical method for comparing two probability distributions by plotting their quantiles against each other. Method takes info from the model and simulates response values. Your model is then fitted with the simulated response variable and residuals created. A dotted line 95% CI envelope is created from these simulated residuals. If your residuals come from the normal distribution, they are close to a straight dotted line. However, even if there is no certain assumption of a speciﬁc distribution, points still show a certain trend. Simulated envelopes help to verify the correctness of this trend. For a good-ﬁtted model, diagnostic values should lay within the envelope.\nHalf-Normal Score - GOF measure - scoreHalfNormal(auditor_obj)\n\nCount the number of simulated residuals for observation_i that are greater or equal than resid_i. If the number is around 0.5 the number of simulated residuals, m, then the model residual doesn’t differ that much from the simulated residuals which is a good thing.\nThat calc is repeated for each residual.\nScore = sum(abs(count_i - (m/2)))\nScore’s range = [0, (nm)/2], where n is the number of observations\nLower value indicates better model fit.\n\nModel PCA Plot - Similarity of models comparison - type = “ModelPCA” - vector arrows represent the models and the gray dots are the model residuals. The smaller the angle between the models, the closer their residual structure. Arrows perpendicular the residual dots mean that model’s residuals likely represent that structure. Parallel means not likely to represent that structure.\nModel Correlation Plot - Similarity of models comparison - type = “ModelCorrelation” - Densities in the diagonal are each models fitted response values. Correlations in upper right triangle are between models and between each model and the observed response.\nPredicted Response Plot (aka predicted vs observed) - GOF measure - type = “Prediction” - Should randomly envelop the diagonal line. Trends can indicate values of the response where the model over/under predicts. Groupings of residuals suggest problems in model specification.\nReceiver Operating Characteristic (ROC) curve - classification GOF measure - type = “ROC” - True Positive Rate (TPR) (y-axis) vs False Positive Rate (FPR) (x-axis) on a threshold t (probability required to classify something as an event happening) where t has the range, [0,1]. Each point on the ROC curve represents values of TPR and FPR at different thresholds. The closer the curve is to the the left border and top border of plot, the more accurate the classiﬁer is.\nAUC score - GOF measure - type = “ROC” - guideline is &gt; 0.80 is good.\nLIFT charts - classification GOF measure - type = “LIFT” - Rate of Positive Prediction (RPP) plotted (y-axis) vs number of True Positives (TP) (x-axis) on a threshold t.\n where P is the total positive classifications (TP + FP) predicted for that threshold and N is the total negative (TN + FN). The ideal model is represented with a orange/yellow curve. The model closer to the ideal curve is considered the better classifier.\nCook’s Distances Plot - influential observations - type = “CooksDistance” - a tool for identifying observations that may negatively affect the model. They can be also used for indicating regions of the design space where it would be good to obtain more observations. Data points indicated by Cook’s distances are worth checking for validity. Cook’s Distances are calculated by removing the i-th observation from the data and recalculating the model. It shows an inﬂuence of i-th observation on the model. The 3 observations (default value) with the highest values of the Cook’s distance are marked with the row number of the observation. The guideline seems to be a Cook’s Distance (y-axis) &gt; 0.5 warrants further investigation.",
    "crumbs": [
      "Diagnostics",
      "Model Agnostic"
    ]
  },
  {
    "objectID": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-shap",
    "href": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-shap",
    "title": "Model Agnostic",
    "section": "SHAP",
    "text": "SHAP\n\nDescription\n\nDecomposes predictions into additive contributions of the features\nBuilds model explanations by asking the same question for every prediction and feature: “How does prediction i change when feature j is removed from the model?”\nQuantifies the magnitude and direction (positive or negative) of a feature’s effect on a prediction.\nTheoretical foundation in game theory\n\nIssues\n\nCannot be used for causal inference\nHighly correlated features\n\nMay be indictators for a latent feature.\nThese correlated features will have lower shap values than they would if only 1 were in the feature space.\nSince shap values are additive, we can add the shap values of highly correlated variables to get an estimate of the importance of this potential latent feature. (seems like the sum might be an overestimation unless the variables are Very highly correlated.)\n**See triplot section for handling models with highly correlated features\n\n\nPackages\n\n{shapper} - a wrapper for the Python library\n{fastshap} - uses Monte-Carlo sampling\n{treeshap}\n\nfast\nHandles correlated features by explicitly modeling the conditional expected prediction\n\nDisadvantage of this method is that features that have no influence on the prediction can get a TreeSHAP value different from zero\n\nable to compute interaction values\n\nComputing “residuals” might indicate how well shapley is capturing the contributions if the features are independent (py article)\n\n\n{kernelshap}\n\npermute feature values and make predictions on those permutations. Once we have enough permutations, the Shapley values are estimated using linear regression\nSlow\nDoesn’t handle feature correlation. Leads to putting too much weight on unlikely data points.\n\n\nArticles\n\nExplaining Machine Learning Models: A Non-Technical Guide to Interpreting SHAP Analyses\n\nUltimate explainer\n\nInterpretable Machine Learning (ebook)\n\nmath, compares packages\n\n\nSteps for the approximate Shapley estimation method (used in IML package below):\n\nChoose single observation of interest\nFor variables j in {1,…,p}\n\nm = random sample from data set\nt = rbind(m, ob)\nf(all) = compute predictions for t\nf(!j) = compute predictions for t with feature j values randomized\ndiff = sum(f(all) - f(!j))\nphi = mean(diff)\n\nsort phi in decreasing order\n\nInteractions\n\nSHAP values for two-feature interactions\nResource\nExample: Years on hormonal contraceptives (continuous) interacts with STDs (binary)\n\nInterpretation\n\nIn cases close to 0 years, the occurence of a STD increases the predicted cancer risk.\nFor more years on contraceptives, the occurence of a STD reduces the predicted risk.\nNOT a causal model. Effects might be due to confounding (e.g. STDs and lower cancer risk could be correlated with more doctor visits).\n\n\nCluster by shap value\n\n\nSee ebook chapter for more details\nPlot is all the force plots ordered by similarity score\n\nI think the “similarity score” might come from the cluster model (i.e. a distance measure from from a hierarchical cluster)\n\nInterpretation: group of force plots on the far right shows that the features similarily contributed to that group of predictions\n\nWaterfall plots\n\n\nAn example waterfall plot for the individual case in the Boston Housing Price dataset that corresponds to the median predicted house\nInterpretation example\n\n\nforce plot - Examines influences behind one predicted value that you input to the function.\n\n\nRed arrows represent feature effects (SHAP values) that drives the prediction value higher while blue arrows are those effects that drive the prediction value lower.\nEach arrow’s size represents the magnitude of the corresponding feature’s effect.\nThe “base value” (see the grey print towards the upper-left of the image) marks the model’s average prediction over the training set. The “output value” is the model’s prediction.\nThe feature values for the largest effects are printed at the bottom of the plot.\n\ndecision plot - Examines influences behind one predicted value that you input to the function. Pretty much the same exact thing as the breakdown plot in DALEX.\n\nThe straight vertical line marks the model’s base value. The colored line is the prediction.\nStarting at the bottom of the plot, the prediction line shows how the SHAP values (i.e., the feature effects) accumulate from the base value to arrive at the model’s predicted output at the top of the plot.\nThe feature name on the y-axis is bordered by 2 horzontal grid lines. In the graph, the way the prediction line behaves in between these horizontal grid lines of the feature visualizes the SHAP value, e.g. a negative slope would be equal to the blue arrow in the force plot.\nThe feature values for the largest effects are in parentheses.\nFor multi-variate classification\n\nUsing lines instead of bars (like in breakdown plots) allows it to visualize the influences behind a multi-class outcome prediction where each line is for a category level probability. But the x-axis actually represents the raw (SHAP?) score, not a probability. (I think there’s an option for x-axis as probabilities)\nThe magnitude of score shows strength of confidence in the prediction. A negative score says the model thinks that category level is not the case. For example, if the category level is “no disease”, then a large negative score means the model says there’s strong evidence that disease is present (score was -2.12 so I guess that’s strong).\n\nCan vary one predictor between a range or set of specific values and keep the rest of predictors constant. For classification, you could ask which predictors dominate influence for high probability predictions? Then use the plot to compare predictor behavior.\nCan compare how influences are different for an observation by multiple models.",
    "crumbs": [
      "Diagnostics",
      "Model Agnostic"
    ]
  },
  {
    "objectID": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-iml",
    "href": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-iml",
    "title": "Model Agnostic",
    "section": "IML",
    "text": "IML\n\nUses ggplot, so layers and customizations can be added. Package also has pdp, feature importance, lime, and shapley. Note from https://www.brodrigues.co/blog/2020-03-10-exp_tidymodels/, Bruno wrapped his predict function when he created a IML Predictor object. Also see article if working with workflow and cat vars, he an issue and workaround.\npredict_wrapper2 &lt;- function(model, newdata){\n  predict(object = model, new_data = newdata)\n}\n\npredictor2 &lt;- Predictor$new(\n                          model = best_model_fit,\n                          data = pra_test_bake_features,\n                          y = target,\n                          predict.fun = predict_wrapper2\n                        )\nAcumulated Local Effect (ALE) - shows how the prediction changes locally, when the feature is varied\n\naka Partial Dependence Plot\nChart has a distribution rug which shows how relevant a region is for interpretation (little or no points mean that we should not over-interpret this region)\n\nIndividual Conditional Expectation (ICE)\n\nThese are curves for a chosen feature that illustrate the predicted value for each observation when we force each observation to take on the unique values of that feature.\nSteps for a selected predictor, x:\n\nDetermine grid space of j evenly spaced values across distribution of x\nFor value i in {1,…,j} of grid space\n\nset x == i for all n observations (x is a constant variable)\napply given ML model\nestimate n predicted values\n\n\nThe only thing that’s different here from pdp is that the predicted values, y_hats,  for a particular x value aren’t averaged in order to produce a smooth line across all the x values. It allows us to see the distribution of predicted values for each value of x.\n\nFor a categorical predictor: each category has boxplot and the boxplots are connected at their medians\nFor a numerical predictor: Its a multi-line graph with each line representing row in the training set.\nThe pdp line is added and highlighted with arg, method = “pdp+ice”.\n\n\nInteraction PDP\n\nVisualizes the pdp for an interaction. Shows the how the effect of a predictor on the outcome varies when conditioned on another variable.\n\nExample:\n\noutcome (binary) = probability of getting overtime\npredictor (numeric) = monthly_income\ninteraction_variable (character) = gender\npdp shows a multi-line chart with a distinct gap between male and female\n\n\nCan be used in conjunction using the H-statistic below to find a strong interaction to examine with this plot\n\nH-statistic\n\nMeasures how much of the variation of the predicted outcome depends on the interaction of the features.\nTwo approaches:\n\nSort of an overall measure of a variable’s interaction strength\n\nSteps:\n\nfor variable i in {1,…,m}\n\nf(x) = estimate predicted values with original model\n\nThink this is a y ~ i model only\n\npd(x) = partial dependence of variable i\npd(!x) = partial dependence of all features excluding i\n\nGuessing each non-i variable gets it’s own pd(!x)\n\nupper = sum(f(x) - pd(x) - pd(!x))\nlower = variance(f(x))\nρ = upper / lower\n\nSort variables by descending ρ (interaction strength)\n\nρ = 0 means none of variation in the predictions is dependent on the interactions involving the predictor\nρ = 1 means all of the variation in the predictions is dependent on the interactions involving the predictor\n\nMeasures the 2-way interaction strength of feature\n\nBreaks down the overall measure into strength measures between a variable and all the other variables\nTop variables in the first approach or variables of domain interest are usually chosen to be further examined with this method. This method will show if there are strong co-dependency relationships in the model\nsteps:\n\ni = a selected variable of interest\nfor remaining variables j in {1,…,p}\n\npd(ij) = interaction partial dependence of variables i and j\npd(i) = partial dependence of variable i\npd(j) = partial dependence of variable j\nupper = sum(pd(ij) - pd(i) - pd(j))\nlower = variance(pd(ij))\nρ = upper / lower\n\nSort interaction relationship by descending ρ (interaction strength)\n\n\n\nComputationally intensive as feature set grows\n\n30 predictors for 3 different models = a few minutes\n80 predictors for 3 different models = around an hour\n\n\nSurrogate model\n\nSteps:\n\nApply original model and get predictions\nChoose an interpretable “white box” model (linear model, decision tree)\nTrain the interpretable model on the original dataset with the predictions of the black box model as the outcome variable\nMeasure how well the surrogate model replicates the prediction of the black box model\nInterpret / visualize the surrogate model\n\n\nFeature Importance\n\nthe ratio of the model error after permutation to the original model error\n\nYou can specify the loss function\n\n“rmse” “mse” “mae”\n\n Anything 1 or less is interpreted as not important\nHas error bars\n\nCan also used difference instead of ratio",
    "crumbs": [
      "Diagnostics",
      "Model Agnostic"
    ]
  },
  {
    "objectID": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-easalluv",
    "href": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-easalluv",
    "title": "Model Agnostic",
    "section": "easyalluvial",
    "text": "easyalluvial\n\npdp plots using alluvial flows to display more than 2 dimensions at a time\nhelps to get an intuitive understanding how predictions of a certain ranges can be generated by the model\n\nConstricted alluvial paths indicate specific (or maybe a small range) predictor variable values are responsible for a certain range of outcome variable values\nSpread-out alluvial paths indicate the value of the predictor probably isn’t that important in determining that range of values of the outcome variable\n\nThere are built-in functions for caret and parsnip models, but there are methods that allow you to use any model\npackages\n\n{easyalluvial}\n{parcats} - converts easyalluvial charts into interactive htmlwidget\n\ntooltip also shows probability and counts\n\n\nSteps\n\nBuild model\nCalculate pdp prediction values - p = alluvial_model_response_parsnip(m, df, degree = 4, method = \"pdp\")\n\narguments\n\nm = model\ndf = data\ndegree = number of top importance variables to use\nmethod =\n\n“median” is default which sets variables that are not displayed to median mode, use with regular predictions.\n“pdp” uses pdp method\n\nbins = number of flows for numeric predictors (not intervals, so not sure why they used “bins”)\nparams_bin_numeric_pred = list, Default: list(bins = 5, center = T, transform = T, scale = T)\n\n“pred” = predictions so these are binned predictions of the outcome variable\ntransform = apply Yeo Johnson Transformation to predictions\nThese are params to another function, and I think you can adjust the binning function there.\n\nbin_labels: labels for the bins from low to high, Default: c(“LL”, “ML”, “M”, “MH”, “HH”)\n\nHigh, High (HH)\nMedium, High (MH)\nMedium (M)\nMedium, Low (ML)\nLow, Low (LL)\n\n\nFor numerics, 5 values (bins arg) in the range of each predictor is chosen\n\nThese aren’t IQR values or evenly spaced so not sure what the process is\nFor categoricals, I guess each level is used. Not sure how a cat var with many levels is treated.\n\npredictions are calculated using the method in the method arg\npredictions are transformed and binned into 5 range intervals (params_bin_numeric_pred arg)\n\nPlot p (just have to call the above function)\n\ncan also add importance and marginal histograms\n\np_grid &lt;- p %&gt;%\n  add_marginal_histograms(data_input = df, plot = FALSE) %&gt;%\n  add_imp_plot(p = p, data_input = df)\n\nExtreme values\n\nCheck if extreme values of the sample distribution are covered by the alluvial\n\npred_train = predict(m) \nplot_hist('pred', p, df,\n          pred_train = pred_train, # pred_train can also be passed to add_marginal_histograms()\n          scale = 50)\n\nDistributions\n\nlstat is the sample distribution for the outcome variable\npred_train is the model predictions of sample data\npred is the predictions by the alluvial method using top importance variables\n\nMost of the extreme values are covered by the model predictions (pred_train), but the not the alluvial method.\nIf you want an alluvial with preds for the extreme values, see What feature combinations are needed to obtain predictions in the lower and higher ranges in the docs.\nExample - mlbench::BostonHousing; lstat(outcome var) percentage of lower status of the population; rf parsnip model\n\npredictor variable labels: \n\ntop number: value of the variable used in predictions\nrest: fraction of the flows of that color (predictions bin range) pass through that stratum\n\ne.g. 49% of the medium-high lstat predictions involve medv = 5\n\n\nmethod = “median”, note bottom where it shows that predictions are calculated using the median/mode values of variable\nExample: mtcars; disp (outcome var)\n\nhistograms/density charts\n\n“predictions”: density shows the shape of the predictions distribution and sample distribtution; areas for HH, MH, M, ML, LL\npredictors: shows sample density/histogram and location of the variable values used in the pdp predictions\n\npercent importance: variable importance for the model; “total_alluvial” is the total importance of the predictor variables used in the pdp alluvial.\nWhen comparing the distribution of the predictions against the original distribution of disp we see that the range of the predictions in response to the artificial dataspace do not cover all of the range of disp. Which most likely means that all possible combinations of the 4 plotted variables in combination with moderate values for all other predictors will not give any extreme values",
    "crumbs": [
      "Diagnostics",
      "Model Agnostic"
    ]
  },
  {
    "objectID": "qmd/diagnostics-probabilistic.html",
    "href": "qmd/diagnostics-probabilistic.html",
    "title": "Probabilistic",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Diagnostics",
      "Probabilistic"
    ]
  },
  {
    "objectID": "qmd/diagnostics-probabilistic.html#sec-diag-prob-misc",
    "href": "qmd/diagnostics-probabilistic.html#sec-diag-prob-misc",
    "title": "Probabilistic",
    "section": "",
    "text": "AIC vs BIC (paper)\n\n\nAIC\n\nPenalizes parameters by 2 points per parameter\nIdeal AIC scenario\n\nNumerous hypotheses are considered\nYou have a conviction that all of them are to differing degrees wrong\n\n\nBIC\n\nPenalizes parameters by ln(sample size) points per parameter and ln(20) = 2.996\nAlmost always a stronger penalty in practice\nIdeal BIC scenario\n\nOnly a few potential hypotheses are considered\nOne of the hypotheses is (essentially) correct",
    "crumbs": [
      "Diagnostics",
      "Probabilistic"
    ]
  },
  {
    "objectID": "qmd/diagnostics-probabilistic.html#sec-diag-prob-scor",
    "href": "qmd/diagnostics-probabilistic.html#sec-diag-prob-scor",
    "title": "Probabilistic",
    "section": "Scores",
    "text": "Scores\n\nContinuous Ranked Probability Score (CRPS)\n\nfabletools::accuracy\n{loo} - crps(), scrps(), loo_crps(), and loo_scrps() for computing the (Scaled) Continuously Ranked Probability Score\nManual calculation (article)\nMeasures forecast distribution accuracy\nCombines a MAE score with the spread of simulated point forecasts\nSee notebook (pg 172)\n\nWinkler Score\n\nfabletools::accuracy\nMeasures how well a forecast is covered by the prediction intervals (PI)\nSee notebook (pg 172)",
    "crumbs": [
      "Diagnostics",
      "Probabilistic"
    ]
  },
  {
    "objectID": "qmd/diagnostics-probabilistic.html#sec-diag-prob-vizinsp",
    "href": "qmd/diagnostics-probabilistic.html#sec-diag-prob-vizinsp",
    "title": "Probabilistic",
    "section": "Visual Inspection",
    "text": "Visual Inspection\n\nCheck how well the predicted distribution matches the observed distribution\n{topmodels} currently supported models:\n\nlm, glm, glm.nb, hurdle, zeroinfl, zerotrunc, crch, disttree, and models from {disttree}, {crch}\nAlso see video, Probability Distribution Forecasts: Learning from Random Forests and Graphical Assessment\n\nautoplot produces a ggplot object that can be used for further customization\n(Randomized) quantile-quantile residuals plot\n\nqqrplot(distr_forest_fit)\n\nQuantiles of the standard normal distribution vs quantile residuals (regular ole q-q plot)\nInterpretation\n\nPretty good fit as the points stick pretty close to the line (red dot is the laser pointer from the dude giving the talk)\nLeft and right tails show deviation.\nThe left tail also shows increased uncertainty due the censored distribution that was used to fit the model\n\nCompare with a bad model\n\nc(qqrplot(distr_forest_fit, plot = FALSE), qqrplot(lm_fit, plot = FALSE)) |&gt; autoplot(legend = TRUE, single_graph = TRUE, col = 1:2)\n\n(Randomized) quantile-quantile residuals plot\n\npithist(distr_forest_fit)\n\nCompares the value that the predictive CDF attains at the observation with the uniform distribution\nThe flatter the histogram, the better the model.\nInterpretation: As with the q-q, this model shows some deviations at the tails but is more or less pretty flat\nCompare with a bad model\n\nc(pithist(distr_forest_fit, plot = FALSE), pithist(lm_fit, plot = FALSE) |&gt; autoplot(legend = TRUE, style = \"lines\", single_graph = TRUE, col = 1:2)\n\n(Hanging) Rootogram\n\nrootogram(distr_forest_fit)\n\nCompares whether the observed frequencies match the expected frequencies\nObserved frequencies (bars) are hanging off the expected frequencies (model predictions, red line)\nrobs is the outcome values\nInterpretation: Near perfect prediction for 0 precipitation (outcome variable), underfitting values of “1” precipitation\nCompare with a bad model\n\nc(rootogram(distr_forest_fit, breaks = -9:14), rootogram(lm_fit,\nbreaks = -9:14) |&gt; autoplot(legend = TRUE)\n\nlm model shows overfitting of outcome variable values 1-5 and underfitting the zeros.\nThe lm model doesn’t use a censored distribution so there’s an expectation of negative values\n\n\nReliability Diagram\n\nreliagram(fit)\n\nForecasted probabilities of an event vs observed frequencies\n\nBasically a fitted vs observed plot\nForecast probabilites are binned (points on the line), 10 in this example, and averaged\n\nClose to the dotted line indicates a good model\n\nWorm plot\n\nwormplot(fit)\n\n? ( he didn’t describe this chart)\nGuessing the dots on the zero line indicates a perfect model and dots inside the dashed lines indicates a good model\n\nHe said this model fit was reasonable but doesn’t look that great to me.",
    "crumbs": [
      "Diagnostics",
      "Probabilistic"
    ]
  },
  {
    "objectID": "qmd/diagnostics-regression.html",
    "href": "qmd/diagnostics-regression.html",
    "title": "Regression",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Diagnostics",
      "Regression"
    ]
  },
  {
    "objectID": "qmd/diagnostics-regression.html#sec-diag-reg-misc",
    "href": "qmd/diagnostics-regression.html#sec-diag-reg-misc",
    "title": "Regression",
    "section": "",
    "text": "Collinearity - Umbrella term encompassing any linear association between variables.\nMulticollinearity - Specific type of collinearity where the linear relationship is particularly strong and involves multiple variables.",
    "crumbs": [
      "Diagnostics",
      "Regression"
    ]
  },
  {
    "objectID": "qmd/diagnostics-regression.html#sec-diag-reg-gof",
    "href": "qmd/diagnostics-regression.html#sec-diag-reg-gof",
    "title": "Regression",
    "section": "GOF",
    "text": "GOF\n\n{performance}\n\nHandles all kinds of models including mixed models, bayesian models, econometric models\nperformance::check_model\n\n\nCan take a tidymodel as input.\n\nperformance::model_performance\n\nScores model using AIC, AICc, BIC, R2, R2adj, RMSE, Sigma\n\n“Sigma” is the standard deviation of the residuals (aka Residual standard error, see below)\n\n\nperformance::compare_performance\n\nOutputs table with scores for each model\n\n\n\n\nsummary\n\nStandard Errors: An estimate of how much estimates would ‘bounce around’ from sample to sample, if we were to repeat this process over and over and over again.\n\nMore specifically, it is an estimate of the standard deviation of the sampling distribution of the estimate.\n\nt-score: Ratio of parameter estimate and its SE\n\nUsed for hypothesis testing, specifically to test whether the parameter estimate is ‘significantly’ different from 0.\n\np-value: The probability of finding an estimated value that is as far or further from 0, if the null hypothesis were true.\n\nNote that if the null hypothesis is not true, it is not clear that this value is telling us anything meaningful at all.\n\nF-statistic: This “simultaneous” test checks to see if the model as a whole predicts the response variable better than that of the intercept only (a.k.a. mean) model.\n\ni.e. Whether or not all the coefficient estimates should jointly be considered unable to be differentiated from 0\nAssumes homoskedastic variance\nIf this statistic’s p-value &lt; 0.05, then this suggests that at least some of the parameter estimates are not equal to 0\nMany authors recommend ignoring the P values for individual regression coefficients if the overall F ratio is not statistically significant. This is because of the multiple testing problem. In other words, your p-value and f-value should both be statistically significant in order to correctly interpret the results.\nUnless you only have an intercept model, you have multiple tests (e.g. variables + intercept p-values). There is no protection from the problem of multiple comparisons without this.\n\nBear in mind that because p-values are random variables–whether something is significant would vary from experiment to experiment, if the experiment were re-run–it is possible for these to be inconsistent with each other.\n\n\nResidual standard error - Variation around the regression line.\n\\[\n\\text{Residual Standard Error} = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i-\\hat Y_i)^2}{dof}}\n\\]\n\n\\(\\text{dof} = n − k^*\\) is the model’s degrees of freedom\n\n\\(k^*\\) = The numbers of parameters you’re estimating including the intercept\n\nAside from model comparison, can also be compared to the sd of the observed outcome variable\n\n\n\n\nKolmogorov–Smirnov test (KS)\n\nGuessing this can be used for GOF to compare predictions to observed\nMisc\n\nSee Distributions &gt;&gt; Tests for more details\nVectors may need to be standardized (e.g. normality test) first unless comparing two samples\n\nPackages\n\n{KSgeneral} has tests to use for contiuous, mixed, and discrete distributions; written in C++\n{stats} and {dgof} also have functions, ks.test\nAll functions take a numeric vector and a base R density function (e.g. pnorm, pexp, etc.) as args\n{KSgeneral} docs don’t say you can supply your own comparison sample (2nd arg) only the density function but with stats and dgof, you can.\n\nAlthough they have function to compute the CDFs, so if you need speed, it might be possible to use their functions and do it man\n\n\n\n\n\ng-index\n\nFrom Harrell’s rms (doc)\n\nNote: Harrell often recommends using Gini’s mean difference as a robust substitute for the s.d.\nFor Gini’s mean difference, see Feature Engineering, General &gt;&gt; Continuous &gt;&gt; Transformations &gt;&gt; Standardization\n\nI think the g-index for a model is the total of all the partial g-indexes\n\nEach independent variable would have a partial g-index\nHe also supplies 3 different ways of combining the partial indexes I think\nHarrell has a pretty thorough example in the function doc that might shed light\n\nPartial g-Index\n\nExample: A regression model having independent variables, age + sex + age*sex, with corresponding regression coefficients \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\)\n\ng-indexage = Gini’s mean difference (age * (\\(\\beta_1\\) + \\(\\beta_3\\)*w))\n\nWhere w is an indicator set to one for observations with sex not equal to the reference value.\nWhen there are nonlinear terms associated with a predictor, these terms will also be combined.",
    "crumbs": [
      "Diagnostics",
      "Regression"
    ]
  },
  {
    "objectID": "qmd/diagnostics-regression.html#sec-diag-reg-res",
    "href": "qmd/diagnostics-regression.html#sec-diag-reg-res",
    "title": "Regression",
    "section": "Residuals",
    "text": "Residuals\n\nMisc\n\nPackages\n\n{gglm} - Diagnostic plots for residuals\n\ninfluence.measures(mod) wll calculate DFBETAS for each model variable, DFFITS, covariance ratios, Cook’s distances and the diagonal elements of the hat matrix. Cases which are influential with respect to any of these measures are marked with an asterisk.\n\nStandardized Residuals:\n\\[\nR_i = \\frac{r_i}{SD(r)}\n\\]\n\nWhere r is the residuals\nFollows a Chi-Square distribution\nAny \\(|R|\\) &gt; 2 or 3 is an indication that the point may be an outlier\nIn R: rstandard(mod)\nIt may be a good idea to run the regression twice — with and without the outliers to see how much they have an effect on the results.\nInflation of the MSE due to outliers will affect the width of CIs and PIs but not predicted values, hypothesis test results, or effect point estimates\n\nStudentized Residuals\n\\[\nt_i = r_i \\left(\\frac{n-k-2}{n-k-1-r_i^2}\\right)^{\\frac{1}{2}}\n\\]\n\nWhere \\(r_i\\) is the ith standardized residual, \\(n\\) = the number of observations, and \\(k\\) = the number of predictors.\nSome outliers won’t be flagged as outlierrs because they drag the regression line towards them. These residuals are for detecting them.\n\nTherefore, generally better at detecting outliers than standardized residuals.\n\nThe studentized residuals, \\(t\\), follow a student t distribution with dof = n–k–2\n\nRule of thumb is any \\(|t_i|\\) &gt; 3 is considered an outlier but you can check the residual against the critical value to be sure.\n\nIn R, rstudent(mod)\nIt may be a good idea to run the regression twice — with and without the outliers to see how much they have an effect on the results.\nInflation of the MSE due to outliers will affect the width of CIs and PIs but not predicted values, hypothesis test results, or effect point estimates\n\nCheck for Normality\n\nResiduals vs Fitted scatter\n\nLooking for data to centered around 0\nHelpful to have a horizontal and vertical line at the zero markers on the X & Y axis.\n\nResiduals historgram\n\nLook for symmetry\nHelpful to have a gaussian overlay\n\n\nCheck for heteroskedacity or Non-Linear Patterns\n\nResiduals vs Fitted scatter\n\nHeteroskedastic\n\n\n\nSubtler but present (reverse megaphone shape)\n\nSolutions:\n\nLog transformations of a predictior, outcome or both\nHeteroskedastic robust standard errors (See Econometrics, General &gt;&gt; Standard Errors)\nGeneralized Least Squares (see Regression, Other &gt;&gt; Misc)\nWeighted Least Squares (see Regression, Other &gt;&gt; Weighted Least Squares)\n\nAlso see Feasible Generalized Least Squares (FGLS) in the same note\nExample: Real Estate &gt;&gt; Appraisal Methods &gt;&gt; CMA &gt;&gt; Market Price &gt;&gt; Case-Shiller method\n\nScale model (greybox::sm ) models the variance of the residuals or greybox::alm will call sm and fit a model with estimated residual variance\n\nSee article for an example\nCan be used with other distributions besides gaussian\nThe unknown factor or function to describe the residual variance is a problem w/WLS\n\n{gamlss} also models location, scale, and shape\n\nAlso can be used with other distributions besides gaussian\n\n\n\nNonlinear\n\nBreusch Pagan test (lmtest::bptest or car::ncvtest )\n\nH0: No heteroskedacity present\nbptest takes data + formula or lm model; ncvtest only takes a lm model\n\n\n\nCheck for Autocorrelation\n\nRun Durbin-Watson, Breusch-Godfrey tests: forecast::checkresiduals(fit)  to look for autocorrelation between residuals.\n\nRange: 1.5 to 2.5\nClose to 2 which means you’re in the clear.\n\n\nCheck for potential variable transformations\n\nResidual vs Predictor\n\nRun every predictor in the model and every predictor that wasn’t used.\nShould look random.\nNonlinear patterns suggest non linear model should be used that variable (square, splines, gam, etc.). Linear patterns in predictors that weren’t use suggest they should be used.\n\ncar::residualPlots - Plots predictors vs residuals and performs curvature test\n\np &lt; 0.05 –&gt; Curvature present and need a quadratic version of the variable\nOr car::crPlots(model) for just the partial residual plots\n\n{ggeffects}\n\nIntroduction: Adding Partial Residuals to Marginal Effects Plots\n\nShows how to detect not-so obvious non-linear relationships and potential interactions through visualizing partial residuals\n\n\nHarrell’s {rms}\n\nLogistic regression example (link)\n\nEnd of section 10.5; listen to audio",
    "crumbs": [
      "Diagnostics",
      "Regression"
    ]
  },
  {
    "objectID": "qmd/diagnostics-regression.html#sec-diag-eg-plot",
    "href": "qmd/diagnostics-regression.html#sec-diag-eg-plot",
    "title": "Regression",
    "section": "plot(fit)",
    "text": "plot(fit)\n\nFor each fitted model object look at residual plots searching for non-linearity, heteroskedacity, normality, and outliers. Correlation matrix (see bkmk in stats) for correlation and VIF score for collinearity among predictors.\nIf non-linearity is present in a variable run poly function to determine which polynomial produces the least cv error\nIs heteroscedasticity is present use a square root or log transform on the variable. Not sure if this is valid with multiple regression. If one variable is transformed the others might have to also be transformed in order to maintain interpretability. *try sandwich estimator in regtools, car, or sandwich pkgs*\nOutliers can be investigated further with domain knowledge or other statistical methods\nExample: Diamonds dataset from {ggplot2}\n\nprice ~ carat + cut\n\n\nBad fit\nResiduals vs Fitted: Clear structure in the residuals, not white noise. They curve up at the left (so some non-linearity going on), plus they fan out to the right (heteroskedasticity)\nScale-Location: The absolute scale of the residuals definitely increases as the expected value increases — a definite indicator of heteroskedasticity\nNormal Q-Q: Strongly indicates the residuals aren’t normal, but has fat tails (e.g. when they theoretically would be about 3 on the standardised scale, they are about 5 - much higher)\n\nlog(price) ~ log(carat) + cut\n\n\nGood fit\nResiduals vs Fitted: Curved shape and the fanning has gone and we’re left with something looking much more like white noise\nScale-Location: Looks like solid homoskedasticity\nNormal Q-Q: A lot more “normal” (i.e. straight line) and apart from a few outliers the values of the standardized residuals are what you’d expect them to be for a normal distribution",
    "crumbs": [
      "Diagnostics",
      "Regression"
    ]
  },
  {
    "objectID": "qmd/diagnostics-regression.html#sec-diag-reg-othdiag",
    "href": "qmd/diagnostics-regression.html#sec-diag-reg-othdiag",
    "title": "Regression",
    "section": "Other Diagnostics",
    "text": "Other Diagnostics\n\nMisc\n\nTutorial for modeling with Harrell’s {Hmisc}\n{kernelshap} and {fastshap} can handle complex lms, glms (article)\n\nAlso see Diagnostics, Model Agnostic &gt;&gt; SHAP\n\n\nCheck for influential observations - outliers that are in the extreme x-direction\nCheck VIF score for collinearity \nFor prediction, if coefficients vary significantly across the test folds their robustness is not guaranteed (see coefficient boxplot below), and they should probably be interpreted with caution.\n\n\nBoxplots show the variance of the coefficient across the folds of a repeated 5-fold cv.\nThe “Coefficient importance” in the example is just the coefficient value of the standardized variable in a ridge regression\nNote outliers beyond the whiskers for Age and Experience\n\nIn this case, the variance is caused by the fact that experience and age are strongly collinear.\n\nVariability in coefficients can also be explained by collinearity between predictors\n\nPerform sensitivity analysis by removing one of the collinear predictors and re-running the CV. Check if the variance of the variable that was kept has stabilized (e.g. fewer outliers past the whiskers of a boxplot).\n\nstep_lincomb - Finds exact linear combinations between two or more variables and recommends which column(s) should be removed to resolve the issue. These linear combinations can create multicollinearity in your model.\nexample_data &lt;- tibble(\n  a = c(1, 2, 3, 4),\n  b = c(6, 5, 4, 3),\n  c = c(7, 7, 7, 7)\n)\n\nrecipe(~ ., data = example_data) |&gt;\n  step_lincomb(all_numeric_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 4 × 2\n      a     b\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     6\n2     2     5\n3     3     4\n4     4     3",
    "crumbs": [
      "Diagnostics",
      "Regression"
    ]
  },
  {
    "objectID": "qmd/diagnostics-regression.html#sec-diag-reg-mlpred",
    "href": "qmd/diagnostics-regression.html#sec-diag-reg-mlpred",
    "title": "Regression",
    "section": "ML Prediction",
    "text": "ML Prediction\n\nMisc\n\nGet test predictions from tidymodels workflow fit obj\npreds_tbl &lt;- wflw_fit_obj %&gt;%\n    predict(testing(splits)) %&gt;%\n    bind_cols(testing(splits), .)\n\nGOF\n\nRMSE of model vs naive\npreds_tbl %&gt;%\n    yardstick::rmse(outcome_var, .pred)\npreds_tbl %&gt;%\n    mutate(.naive = mean(outcome_var)) %&gt;%\n    yardstick::rmse(outcome_var, .naive\n\nR2\npreds_tbl %&gt;%\n    yardstick::rsq(outcome_var, .pred)\n\nSquared correlation between truth and estimate to guarantee a value between 0 and 1\n\nObserved vs Predicted plot\npreds_tbl %&gt;%\n    ggplot(aes(outcome_var, .pred)) +\n    # geom_jitter(alpha = 0.5, size = 5, width = 0.1, height = 0) + # if your outcome var is discrete\n    geom_smooth()\nFeature Importance\n\nExample: tidymodel xgboost\nxgb_feature_imp_tbl &lt;- workflow_obj_xgb %&gt;%\n    extract_fit_parsnip() %&gt;%\n    pluck(\"fit\") %&gt;%\n    xgboost::xgb.importance(model = .) %&gt;%\n    as_tibble() %&gt;%\n    slice(1:20)\n\nxgb_feature_imp_tbl %&gt;%\n    ggplot(aes(Gain, fct_rev(as_factor(Feature)))) %&gt;%\n    geom_point()",
    "crumbs": [
      "Diagnostics",
      "Regression"
    ]
  },
  {
    "objectID": "qmd/distributions.html",
    "href": "qmd/distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "Terms",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-terms",
    "href": "qmd/distributions.html#sec-distr-terms",
    "title": "Distributions",
    "section": "",
    "text": "Conditional Probability Distributions\n\nNotes from https://www.causact.com/joint-distributions-tell-you-everything.html#joint-distributions-tell-you-everything\nNotation: \\(P(Y | X) = P(Y \\text{and} X) / P(X) = P(Y, X) / P(X)\\)\n\ni.e. ratio of 2 marginal distributions\n\nExample: two tests for cancer are conducted to determine whether a biopsy should be performed\n\nConditional approach: Biopsy everyone at determined to be high risk from test 1; measure the genetic marker (aka test 2) for patients at intermediate risk and biopsy those with a probability of cancer past a certain level based on the marker\n\n\nEmpirical CDF\n\\[\nF_n (x) = \\frac {1}{n} \\sum_{i = 1}^n I(X_i \\leq x)\n\\]\n\nWhere \\(X_1, X_2,\\ldots,X_n\\) are from a population with CDF, \\(F_n (x)\\)\nProcess\n\nTake n samples from an unknown distribution. The more samples you take, the closer the empirical distribution will resemble the true distribution.\nSort these samples, and place them on the x-axis.\nStart plotting a ‘step-function’ style line — each time you encounter a datapoint on the x-axis, increase the step by 1/N.\n\nExample\n\n\nThe CDF of a normal distribution (green) and its empirical CDF (blue)\n\n\nJoint Probability Distribution - assigns a probability value to all possible combinations of values for a set of random variables.\n\nNotation: \\(P(x_1, x_2, ... ,x_n)\\)\nPlugging in a value for each random variable returns a probability for that combination of values\nExample: Two tests for cancer are conducted to determine whether a biopsy should be performed\n\nJoint approach: biopsy anyone who is either at high risk of cancer (test 1) or who was determined to have a probability of cancer past a certain level, based on the marker from the genetic test (test 2)\nCompare with example in Conditional Probability Distributions\n\n\nLocation - Distribution parameter determines the shift of the distribution\n\ne.g. mean, mu, of the normal distribution.\n\nMarginal Probability Distribution - assigns a probability value to all possible combinations of values for a subset of random variables\n\nNotation: \\(P(x_1)\\)\n\n\\(P(x_1,x_2)\\) is sometimes called the Joint Marginal Probability Distribution\n\nThe marginal distribution, \\(P(Y)\\) where \\(Y\\) is a subset of random variables, is calculated from the joint distribution, \\(P(Y = y, Z = z)\\) where \\(Z\\) is the subset of random variables not in \\(Y\\) .\n\n\\(P(Y) = \\sum_{Z=z} P(Y = y, Z = z)\\)\n\nIf \\(Y\\) is just one variable\n\nSays sum all the joint probabilities for all the combinations of values for the variables in \\(Z\\) while holding \\(Y\\) constant\nRepeat for each value of \\(Y\\) to get this summed probability value\nThe marginal distribution is made up of all these values, one for each value of \\(Y\\) (or combination of values if \\(Y\\) is a subset of variables)\n\n\nWhen the joint probability distribution is in tabular form, one just sums up the probabilities in each row where \\(Y = y\\).\n\n\nScale - Distribution parameter; the larger the scale parameter, the more spread out the distribution\n\ne.g. s.d., sigma, \\(\\sigma\\) of the normal distribtution\nRate Parameter: the inverse of the scale parameter (see Gamma distribution)\n\nShape - Distribution parameter that affects the shape of a distribution rather than simply shifting it (as a location parameter does) or stretching/shrinking it (as a scale parameter does).\n\ne.g. “Peakedness” refers to how round the main peak is",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-tests",
    "href": "qmd/distributions.html#sec-distr-tests",
    "title": "Distributions",
    "section": "Tests",
    "text": "Tests\n\nWhy normality tests are great… as a teaching example and should be avoided in research\n\ntl;dr; KS test has very low power as a Normality test as compared to Shapiro-Wilk, and Shapiro-Wilk isn’t very good for n &lt; 100\nFor detecting moderate skew, you want at least n &gt; 75 to get 80% power for Shapiro-Wilk\nShapiro-Wilk can detect very fat tails at n &lt; 100, but would require larger sample sizes to detect more moderately thick tails.\nKS is worthless in detecting fat tails and near-worthless at detecting skew\nWhen n gets large (e.g. 1000s), these types of tests will almost always reject the null even when the practical deviation from normality is not practically significant.\n\nKolmogorov–Smirnov test (KS)\n\nUsed to compare distributions\n\nCan be used as a Normality test or any distribution test\nCan compare two samples\n\nMisc\n\nVectors may need to be standardized (e.g. normality test) first unless comparing two samples H0: Both distributions are from the same distribution\n\nPackages\n\n{KSgeneral} has tests to use for contiuous, mixed, and discrete distributions written in C++\n{stats} and {dgof} also have functions, ks.test\n\nBoth handle continuous and discrete distributions\n\nAll functions take a numeric vector and a base R density function (e.g. pnorm, pexp, etc.) as args\n\nKSgeneral docs don’t say you can supply your own comparison sample (2nd arg) only the density function but with stats and dgof, you can.\nAlthough they have function to compute the CDFs, so if you need speed, it might be possible to use their functions and do it manually\n\n\n2-sample test as the greatest distance between the CDFs (Cumulative Distribution Function) of each sample\n\nSpecifically, this test determines the distribution of your unknown data sample by constructing and comparing the sample’s empirical CDF  (see Terms) with the CDF you hypothesized. If the two CDFs are close, your unknown data sample likely follows the hypothesized distribution.\n\nKS statistic, \\(D_{n,m} = \\max|\\text{CDF}_1 - \\text{CDF}_2|\\) where \\(n\\) as the number of observations on Sample 1 and \\(m\\) as the number of observations in Sample 2\nCompare the KS statistic with the respective KS distribution based on parameter “en” to obtain the p-value of the test\n\n\\(en = (m \\times n) / (m + n)\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-beta",
    "href": "qmd/distributions.html#sec-distr-beta",
    "title": "Distributions",
    "section": "Beta",
    "text": "Beta\n\n\nDefined on the interval [0,1]\nThe key difference between the Binomial and Beta distributions is that for the Beta distribution the probability, x, is a random variable, however for the Binomial distribution the probability, x, is a fixed parameter.\nShape parameters are \\(\\alpha\\) and \\(\\beta\\), usually.\n\n\\(\\alpha\\) and \\(\\beta\\) are two positive parameters that appear as exponents of the random variable\n\npdf\n\\[\nf(x) = \\frac {x^{\\alpha - 1} (1-x)^{\\beta - 1}} {B(\\alpha, \\beta)}\n\\]\n\\(\\mathbb{E}(X) = \\frac {\\alpha} {\\alpha + \\beta}\\)\n\\(\\text{Var}(X) = \\frac {\\alpha \\cdot \\beta} {(\\alpha + \\beta)^2 \\cdot (\\alpha + \\beta + 1)}\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-betbin",
    "href": "qmd/distributions.html#sec-distr-betbin",
    "title": "Distributions",
    "section": "Beta-Binomial",
    "text": "Beta-Binomial\n\n\n\n\n\n\n\n\nWhere k is the number of events in n trials\n\n\n\n\n\n\n\nWhere \\(\\theta\\) is the probability of an event\n\n\n\n\n\n\n\nUsed when the probability of success, p, in a fixed number of Bernoulli trials is unknown or random and can change from trial to trial.\nShape parameters α and β define the probability of success (i.e. the success parameter is modeled by the Beta Distribution).\n\nFor large values of α and β, the distribution approaches a binomial distribution.\nWhen α and β both equal 1, the distribution equals a discrete uniform distribution from 0 to n\n\nAccuracy analysis data from psychology follow beta-binomial distributions (Jaeger, 2008; Kruschke, 2014)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-dirichlet",
    "href": "qmd/distributions.html#sec-distr-dirichlet",
    "title": "Distributions",
    "section": "Dirichlet",
    "text": "Dirichlet\n\nA family of continuous multivariate probability distributions parameterized by a vector α of positive reals",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-exp",
    "href": "qmd/distributions.html#sec-distr-exp",
    "title": "Distributions",
    "section": "Exponential",
    "text": "Exponential\n\n\nNotes from\n\nStatistical Rethinking &gt;&gt; Chapter 10\n\nConstrained to be zero or positive\nFundamental distribution of distance and duration, kinds of measurements that represent displacement from some point of reference, either in time or space.\nIf the probability of an event is constant in time or across space, then the distribution of events tends towards exponential.\nIts shape is described by a single parameter, the rate of events \\(\\lambda\\), or the average displacement \\(\\lambda −1\\) .\nThis distribution is the core of survival and event history analysis",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gamma",
    "href": "qmd/distributions.html#sec-distr-gamma",
    "title": "Distributions",
    "section": "Gamma",
    "text": "Gamma\n\n\nNotes from\n\nStatistical Rethinking &gt;&gt; Chapter 10\n\nConstrained to be zero or positive\nLike Exponential but can have a peak above zero\nIf an event can only happen after two or more exponentially distributed events happen, the resulting waiting times will be gamma distributed.\n\ne.g. age of cancer onset is approximately gamma distributed, since multiple events are necessary for onset.\n\nThe gamma can be viewed as the sum of iid n exponential random variables. Exponential random variables have a rate parameter, so it makes sense for the Gamma to inherit a rate parameterization. The rate parameter also happens to be related to a scale parameter, so it makes sense for the Gamma to have a scale parameterization.\nShape parameter \\(k\\) and a scale parameter \\(\\theta\\)\n\\(\\mathbb{E}[X] = k\\theta = \\frac{\\alpha}{\\beta}\\)\n\nShape parameter \\(\\alpha = k\\) and an\nInverse Scale parameter (aka Rate Parameter) \\(\\beta = \\frac {1}{\\theta}\\)\nTherefore if you want a gamma distributions with a certain “mean” and “standard deviation,” you’d:\n\nSet your mean to \\(\\mathbb{E}[X]\\), your standard deviation to \\(\\theta\\) (probably but maybe it’s \\(\\beta\\))\nCalculate \\(\\beta\\)\nCalculate \\(\\alpha\\)\nprior(gamma(alpha, beta))\n\n\nExample: Gamma distribution as the sums of random exponential variables\n\nn &lt;- 12\nbeta &lt;- 1.2\n\nrvs &lt;- replicate(1000, {\n  sum(rexp(n, beta))\n})\n\nhist(rvs, freq = F)\ncurve(dgamma(x, shape = n, rate = beta), col='red', add=T)\n\nGamma distribution density overlayed with a histogram of exponential variable sums\n\nUsed in Survival Regression",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gauss",
    "href": "qmd/distributions.html#sec-distr-gauss",
    "title": "Distributions",
    "section": "Gaussian",
    "text": "Gaussian\n\nSpecial case of Student’s t-distribution with the \\(\\nu\\) parameter (i.e. degree of freedom) set to infinity.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gumb",
    "href": "qmd/distributions.html#sec-distr-gumb",
    "title": "Distributions",
    "section": "Gumbel",
    "text": "Gumbel\n\n\nKnown as the type-I generalized extreme value distribution\n\nEVT says it is likely to be useful if the distribution of the underlying sample data is of the normal or exponential type.\n\nUsed to model the distribution of the maximum (or the minimum) of a number of samples of various distributions.\n\nTo model minimums, use the negative of the original data.\n\nUse Cases\n\nRepresent the distribution of the maximum level of a river in a particular year if there was a list of maximum values for the past ten years.\nPredicting the chance that an extreme earthquake, flood or other natural disaster will occur.\nDistribution of the residuals in Multinomial Logit and Nested Logit models\n\nParameters\n\nGumbel(\\(\\mu, \\beta\\)) (location, scale)\nMean: \\(\\mu + \\beta\\gamma\\) where \\(\\gamma\\) is Euler’s constant (\\(\\approx\\) 0.5772)\nMedian: \\(\\mu - \\beta \\ln(\\ln(2))\\)\nMode: \\(\\mu\\)\nVariance: \\(\\frac{\\pi^2}{6}\\beta^2\\)\nStandard Gumbel: When \\(\\mu = 0\\), mean = \\(\\gamma\\), median = \\(-\\ln(\\ln(2)) \\approx 0.3665\\) and the standard deviation = \\(\\pi/\\sqrt{6}\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-multgauss",
    "href": "qmd/distributions.html#sec-distr-multgauss",
    "title": "Distributions",
    "section": "Multivariate Gaussian",
    "text": "Multivariate Gaussian\n\nIf the random variable components in the vector are not normally distributed themselves, the result is not multivariate normally distributed.\nVariance-Covariance matrix must be semi-definite and therefore symmetric\n\nExample of not symmetric for two random variables",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-pareto",
    "href": "qmd/distributions.html#sec-distr-pareto",
    "title": "Distributions",
    "section": "Pareto",
    "text": "Pareto\n\nAlso see Extreme Value Theory &gt;&gt; Distribution Tail Classification\n“Gaussian distributions tend to prevail when events are completely independent of each other. As soon as you introduce the assumption of interdependence across events, Paretian distributions tend to surface because positive feedback loops tend to amplify small initial events.”\nPareto has similar relationship with the exponential distribution as lognormal does with normal \\[\nY_{exp} = \\log \\frac {X_{pareto}} {x_m}\n\\]\n\nWhere \\(X_{pareto} = x_m e^{Y_{\\text{exp}}}\\)\n\n\\(x_m\\) is the (positive) minimum of the randomly distributed pareto variable, X that has index α\n\\(Y_{exp}\\) is exponentially distributed with rate \\(\\alpha\\)\n\n\nSome theoretical statistical moments may not exist\n\nIf the theoretical moments do not exist, then calculating the sample moments is useless\nExample: Pareto (\\(\\alpha\\) = 1.5) has a finite mean and an infinite variance\n\nNeed \\(\\alpha &gt; 2\\) for a finite variance\nNeed \\(\\alpha &gt; 1\\) for a finite mean\nIn general you need \\(\\alpha &gt; p\\) for the pth moment to exist\nIf the nth moment is not finite, then the (n+1)th moment is not finite.\n\n\nFat Tails \\[\n\\bar{F} = x^{-\\alpha} L(x)\n\\]\n\n\\(L(x)\\) is just characterized as slowly varying function that gets dominated by the decaying inverse power law element, \\(x-\\alpha\\). as \\(x\\) goes to infinity\n\n\\(\\alpha\\) is a shape parameter, aka “tail index” aka “Pareto index”",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-poisson",
    "href": "qmd/distributions.html#sec-distr-poisson",
    "title": "Distributions",
    "section": "Poisson",
    "text": "Poisson\n\nObtained as the limit of the binomial distribution when the number of attempts is high and the success probability low. Or the Poisson distribution can be approximated by a normal distribution when λ is large\nProbability Mass Function \\[\n\\text{Pr}(Y = y) = f(y; \\lambda) = \\frac {e^{-\\lambda} \\cdot \\lambda^y} {y!}\n\\]\n\n\\(\\mathbb{E}[Y] = \\text{Var}(Y) = \\lambda\\)\n\n{distributions3}\n\nStats\nY &lt;- Poisson(lambda = 1.5) \nprint(Y) \n## [1] \"Poisson distribution (lambda = 1.5)\"\n\nmean(Y) \n## [1] 1.5 \nvariance(Y) \n## [1] 1.5 \npdf(Y, 0:5) \n## [1] 0.22313 0.33470 0.25102 0.12551 0.04707 0.01412 \ncdf(Y, 0:5) \n## [1] 0.2231 0.5578 0.8088 0.9344 0.9814 0.9955 \nquantile(Y, c(0.1, 0.5, 0.9)) \n## [1] 0 1 3 \nset.seed(0) \nrandom(Y, 5) \n## [1] 3 1 1 2 3\n\nVisualize\n\nplot(Poisson(0.5), main = expression(lambda == 0.5), xlim = c(0, 15)) \nplot(Poisson(2),   main = expression(lambda == 2),   xlim = c(0, 15)) \nplot(Poisson(5),   main = expression(lambda == 5),   xlim = c(0, 15)) \nplot(Poisson(10),  main = expression(lambda == 10),  xlim = c(0, 15))",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-studt",
    "href": "qmd/distributions.html#sec-distr-studt",
    "title": "Distributions",
    "section": "Student’s t-distribution",
    "text": "Student’s t-distribution\n\nStandard Deviation\n\\[\n\\text{sd} = \\sqrt {\\frac {\\nu} {\\nu - 2}}\n\\]\n\n\\(\\nu\\) = degrees of freedom\n\nWhen ν is small, the Student’s t-distribution is more robust to multivariate outliers\nThe smaller the degree of freedom, the more “heavy-tailed” it is\n\n\n-3 on the y-axis says that the probability of being in the tail is 1 in 103\n\nDon’t pay attention to the x-axis. Just note how much the probability of being in the tail gets larger as the dof get smaller\n\nAs the degrees of freedom goes to 1, the t distribution goes to the Cauchy distribution\nAs the degrees of freedom goes to infinity, it goes to the Normal distribution.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-tri",
    "href": "qmd/distributions.html#sec-distr-tri",
    "title": "Distributions",
    "section": "Triangular",
    "text": "Triangular\n\nTriangle shaped distribution\nUseful when you have a known min and max value\nextraDistr::rtriang(n, a, b, c) %\\&gt;% hist()\n\n# Discrete distribution\nextraDistr::rtriang(n, a, b, c) %\\&gt;% round() \\`\\`\\`\n\nn is the number of random values you wish to draw\na is the min value\nb is the max value\nc is the mode\n\nCan use to adjust the skew of the distribution\n\n\n\n\n\n\nWhere k is the number of events in n trials\nWhere \\(\\theta\\) is the probability of an event",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/dl-general.html",
    "href": "qmd/dl-general.html",
    "title": "9  General",
    "section": "",
    "text": "9.1 Misc",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/dl-general.html#sec-dl-gen-misc",
    "href": "qmd/dl-general.html#sec-dl-gen-misc",
    "title": "9  General",
    "section": "",
    "text": "Guide for suitable baseline models: link\nDL model cost calculator (github) (article)\nUse Adam and AdamW optimizers\nKeras also provides out-of-the-box preprocessing layers. This way, when the model is saved, the preprocessing steps will automatically be part of the model.\n\ni.e. the same preprocessing steps applied in training are applied in production\nBut preprocessing steps will be wastefully repeated on each iteration through the training dataset. The more expensive the computation, the more this adds up.\n\nDL architectures",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/dl-general.html#sec-dl-gen-terms",
    "href": "qmd/dl-general.html#sec-dl-gen-terms",
    "title": "9  General",
    "section": "9.2 Terms",
    "text": "9.2 Terms\n\nActivation Function: after the node calculates the weighted sum of the input, the activation function transforms the output which is fed to the next layer.\nFully-Connected Layers (aka Linear Layers) - connect every input neuron to every output neuron. Each neuron applies a linear transformation to the input vector through a weights matrix. As a result, all possible connections layer-to-layer are present, meaning every input of the input vector influences every output of the output vector. Three parameters define a fully-connected layer: batch size, number of inputs, and number of outputs. (see article)\n\nModalities\n\nUnimodal Models: text-only, image-only, etc.\nMultimodal Models: text, image, continuous sensor data, etc.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/dl-general.html#sec-dl-gen-actfun",
    "href": "qmd/dl-general.html#sec-dl-gen-actfun",
    "title": "9  General",
    "section": "9.3 Activation functions",
    "text": "9.3 Activation functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMisc\n\nThe choice of activation function has a large impact on the capability and performance of the neural network, and\nDifferent activation functions may be used in different layers of the model.\n\nCommonly the same activation function is used for the hidden layers and a different one for the outer layer that makes the prediction (e.g. softmax)\n\nMost activation functions add non-linearity to the neural network\nBoth the sigmoid and Tanh functions can make the model more susceptible to problems during training, via the so-called vanishing gradients problem.\nArchitectures\n\nHidden layers\n\nMultilayer Perceptron (MLP): ReLU activation function.\nConvolutional Neural Network (CNN): ReLU activation function.\nRecurrent Neural Network (RNN): Tanh and/or Sigmoid activation function\n\nOuter layer\n\nRegression: One node, linear activation\nBinary Classification: One node, sigmoid activation.\nMulticlass Classification: One node per class, softmax activation.\nMultilabel Classification: One node per class, sigmoid activation.\n\n\n\nBase Types\n\nReLU: if the input value (x) is negative, then a value 0.0 is returned, otherwise, the value is returned\nSigmoid: Logistic function; output is 0 to 1\n\nA perceptron is called the logistic regression model if the activation function is sigmoid.\nGood practice to use a “Xavier Normal” or “Xavier Uniform” weight initialization (aka Glorot initialization)) and scale input data to the range 0-1 (e.g. the range of the activation function) prior to training.\n\ntanh: takes any real value as input and outputs values in the range -1 to 1\n\nThe larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0.\nGood practice to use a “Xavier Normal” or “Xavier Uniform” weight initialization (aka Glorot initialization) and scale input data to the range -1 to 1 (e.g. the range of the activation function) prior to training.\n\n\nComprehensive Survey and Performance Analysis of Activation Functions in Deep Learning, paper",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/dl-general.html#sec-dl-gen-reg",
    "href": "qmd/dl-general.html#sec-dl-gen-reg",
    "title": "9  General",
    "section": "9.4 Regularization",
    "text": "9.4 Regularization\n\nMisc\n\nOther methods\n\nData Augmentation\n\nMore data reduces variance\nComputer vision: Gain data by flipping, zooming, and translating the original images\nDigits Recognition: Gain data by mposing distortion on the images\n\nEarly Stopping\n\nStopping the training phase before a defined number of iterations\nFor an overfitting model, if we plot the cost function on both the training set and the validation set as a function of the iterations.\n\nThe training error always keeps decreasing but the validation error might start to increase after a certain number of iterations.\nWhen the validation error stops decreasing, that is exactly the time to stop the training process.\nBy stopping the training process earlier, we force the model to be simpler, thus reducing overfitting.\n\n\n\n\nL1 and L2 regularization\n\nShrinks model weights\nRegularization process\n\nThe weights of some hidden units become closer (or equal) to 0. As a consequence, their effect is weakened and the resulting network is simpler because it’s closer to a smaller network. As stated in the introduction, a simpler network is less prone to overfitting.\nFor smaller weights, also the input z of the activation function of a hidden neuron becomes smaller. For values close to 0, many activation functions behave linearly.\n\nL1\n\nCost Function, J\n\n\\[\nJ(w^{[1]}, b^{[1]}, \\ldots, w^{[L]}, b^{[L]}) = \\frac{1}{m}\\sum_{i=1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\sum_{l=1}^L \\lVert w^{[l]} \\rVert_1\n\\]\n\n\\(\\mathcal{L}\\): Loss Function\n\\(m\\): Number of Training Observations (aka Examples)\n\\(w\\) and \\(b\\): The weight and bias terms in the output of the node respectively\n\nRegularization term\n\n\\[\n\\frac{\\lambda}{2m} \\sum_{l=1}^L \\lVert w^{[l]} \\rVert_1\n\\]\n\n\\(L\\): The number of layers\n\\(\\lambda\\): Regularization Factor\n\\(w\\): The norm of the weights\n\\[\n\\lVert w^{[l]} \\rVert_1 = \\sum_i \\sum_j |w_{i,j}^{[l]}|\n\\]\n\n\nL2\n\nSame as L1 except for the regularization term\nRegularization term\n\\[\n\\frac{\\lambda}{2m} \\sum_{l=1}^L \\lVert w^{[l]} \\rVert_2^2\n\\]\n\n\\(w\\): The norm of the weights\n\\[\n\\lVert w^{[l]} \\rVert_2^2 = \\sum_i \\sum_j (w_{i,j}^{[l]})^2\n\\]\n\n\n\nDropout\n\n\nRandomly remove some nodes in the network\n\nPerformed separately for each training example.\nTherefore, each training example might be trained on a different network.\n\nRegularization Process\n\nHas the effect of temporarily transforming the network into a smaller one, and we know that smaller networks are less complex and less prone to overfitting.\nBecause some of its inputs may be temporarily shut down due to dropout, the unit can’t always rely on them during the training phase. As a consequence, the hidden unit is encouraged to spread its weights across its inputs. Spreading the weights has the effect of decreasing the squared norm of the weight matrix, resulting in a sort of L2 regularization.\n\nProcess\n\nSet a probability, p, for each node of the network.\n\nTypically, the keeping probability is set separately for each layer of the neural network\nFor layers with a large weight matrix, a smaller keeping probability because, at each step, we want to conserve proportionally fewer weights with respect to smaller layers\n\nDuring the training phase, each node has a p probability to be turned off.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/dl-general.html#sec-dl-gen-ablat",
    "href": "qmd/dl-general.html#sec-dl-gen-ablat",
    "title": "9  General",
    "section": "9.5 Ablation Testing",
    "text": "9.5 Ablation Testing\n\nTests to evaluate how robust DL models are to different kinds of disruption.\nMisc\n\nNotes from Ablation Testing Neural Networks: The Compensatory Masquerade\nSimilar to hyperparamter tuning with the goal of optimization except Ablation Testing is more about changing the architecture of the ANN (e.g. neurons, layers, etc), where as hyperparameter tuning refers to changing structural parameters of the model.\n\nUse Cases\n\nIdentifying Critical Parts of a Neural Network\n\nSome parts of a neural network may do more important work than other parts of a neural network. In order to optimize the resource usage and the training time of the network, we can selectively ablate “weaker learners”\n\nReducing Complexity of the Neural Network\n\nSometimes neural networks can get quite large, especially in the case of Deep MLPs (multi layer perceptrons). This can make it difficult to map their behavior from input to output. By selectively shutting of parts of the network, we can potentially identify regions of excessive complexity and remove redundancy — simplifying our architecture.\n\nFault Tolerance\n\nIn a realtime system, parts of a system can fail. The same applies for parts of a neural network, and thus the systems that depend on their output. Ablation studies can determine if destroying certain parts of the neural network, will cause the predictive or generative power of the system to suffer.\n\n\nTypes\n\nNeuronal Ablation: Remove varying percentages of neurons from our neural network (i.e. dropout rate)\n\nExample:\n\n\nEven at a rate of 80%, it doesn’t effect the accuracy a great deal, which means that removing excess neurons is certainly an optimization to be considered.\n\n\nFunctional Ablation: Change the activation functions of the neurons to different curves, with different amounts of non-linearity. If a more linear activation is acceptable, then a simpler and cheaper model (e.g. linear/logistic regression) may be feasible.\n\nExample:\n\n\nNon-linearity of some kind is important to this classification task, with “ReLU” and hyperbolic tangent non-linearity being the most effective.\n\n\nInput Ablation: Remove or alter features and see how it effects the accuracy.\n\nIncludes adding noise (e.g. distorting images) or rotating images, removing a predictor or columns of pixels, removing a transformation.\nExample: Removing columns of pixels for each image\n\n\nA slight dip in accuracy when we remove columns 8 to to 12, and a rise again after that. That suggests that on average, the more “sensitive” character geometry lies in those center columns, but the other columns especially close to the beginning and end could potentially be removed for an optimization effect.\n\nExample: Adding Gaussian noise to distort images\n\n\nDistortion has a substantial effect on this model’s accuracy.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/dl-general.html#sec-dl-gen-relearn",
    "href": "qmd/dl-general.html#sec-dl-gen-relearn",
    "title": "9  General",
    "section": "9.6 Reinforcement Learning",
    "text": "9.6 Reinforcement Learning\n\nInvolves training a smart agent that can learn to perform a goal through trial & error in an environment and at the end of the training we have an agent that can perform the goal in real life independently\nA type of machine learning problem where, rather than making a single decision, you have to make multiple sequential decisions as part of a strategy\nRL does not require any explicit labels to be provided unlike in supervised learning techniques\nUse Cases\n\nEmail\n\nWhat’s the most optimal time for each user as to when they’ll want to read it.\nHow many is too many emails per user?\n\nDynamic paywall metering\n\nHelp make the decision about a tradeoff between making revenue through serving ads by allowing users to read articles for free and making revenue through subscriptions by blocking free access with a digital paywall (after a certain number of free articles), inducing the user to subscribe\n\n\nDeep Q Network (DQN)\n\nA combination of the principles of deep learning and Q-learning\n\nQ-learning is an algorithm of a class of RL solutions called tabular solutions which aims to learn the q-values for each state.\n\nQ-value of a state is the cumulative (discounted) reward from all the states that the agent could go in the future.\nAn elegant solution for problems that have a finite state spaces such as frozen lake problem.\n\n\nFor larger state spaces, this Q-learning gets unwieldy and needs to adopt an approximate way of estimating state value and this class of solution is called ‘approximate methods’.\n\nDQN is the most popular algorithm among the approximate methods.\n\nIn DQN, the deep learning network serves as a function approximation that estimates the value for a given state/action\nThe solution design, the algorithm and the setup would be the same for all the use cases, but the configuration of the MDP (Markov Decision Process)— the states spaces, rewards, actions to be taken would vary for each use case.\n\nExample\n\nStates — The NL opens/clicks pattern for the last (1/2) months.\nAction — 1–24 hours of the day. This could further be reduced to 12 action values with each action representing a 2 hour period when the email could be send.\nReward — +2 for a NL click, +1 for a NL open, 0 otherwise",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/dl-image.html",
    "href": "qmd/dl-image.html",
    "title": "11  Image",
    "section": "",
    "text": "11.1 Misc",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Image</span>"
    ]
  },
  {
    "objectID": "qmd/dl-image.html#sec-dl-image-misc",
    "href": "qmd/dl-image.html#sec-dl-image-misc",
    "title": "11  Image",
    "section": "",
    "text": "Strong Baseline Model: ResNet/EffNet\n\nResNet18 and EffNet-B0 are small, quick models that are effective for nearly any type of image data.\nOnce you’ve squeezed all the juice out of those, you can scale up to their bigger versions and almost always get better accuracy.\nAlso see this guide for suitable baseline models: link",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Image</span>"
    ]
  },
  {
    "objectID": "qmd/dl-image.html#sec-dl-image-terms",
    "href": "qmd/dl-image.html#sec-dl-image-terms",
    "title": "11  Image",
    "section": "11.2 Terms",
    "text": "11.2 Terms\n\nTransfer Learning - Shortens time and resources required for training by using a feature representation technique over pre-trained models. These pre-trained models are generally trained using high-end computational resources and on massive datasets.\n\nMethods\n\nUsing the pre-trained weights and directly making predictions on the test data\nUsing the pre-trained weights for initialization and training the model using the custom dataset\nUsing only the architecture of the pre-trained network, and training it from scratch on the custom dataset\n\nPre-trained CNN Models\n\nVGG\nXception\nResNet\nInceptionV3\nInceptionResNet\nMobileNet\nDenseNet\nNasNet\nEfficientNet\nConvNEXT\n\nAll these pre-trained models can be loaded as keras models using the keras.application API\n\nSpecifications of these models and instantiation code are in this article",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Image</span>"
    ]
  },
  {
    "objectID": "qmd/dl-image.html#sec-dl-image-datcen",
    "href": "qmd/dl-image.html#sec-dl-image-datcen",
    "title": "11  Image",
    "section": "11.3 Data-Centric",
    "text": "11.3 Data-Centric\n\nImproving your data will likely improve your model more than testing different algorithms or tuning more hyperparameters\n\nGood Data:\n\nis defined consistently (definition of labels, y, is unambiguous)\n\ne.g. consistent object labeling procedures\n\nhas coverage of important cases (good coverage of inputs, x)\n\ne.g. samples of every combination of features the model will encounter\n\nhas timely feedback from production data (distribution covers data drift/concept drift)\nis sized appropriately\n\nExamples\n\nImage\n\nImprove consistency of labeling procedures\n\n\n(top-left) clear separation of boxes; (top-right)tail of left iguana included; (bottom) include all iguanas in one bounding box\n\nfyi 1 is probably best\n\nsolution: Improve procedure by making sure only 1 of these ways is used by labellers given this type of picture\n\n\nAudio\n\nYou discover you speech recognition model performs poorly when there’s car noise in the background\n\nsolution: get more training data with car noise in the background\n\n\n\nSteps\n\nTrain a model\nError analysis: investigate which types of data you model performs poorly on. The points the model performs really poorly on will be edge cases or errors in data collection, measurement, entry, etc.\nImprove these data\n\nFor edge cases, collect more data, augment your data, generate more data (simulation?) (i.e. change inputs of x)\nFor errors, make labeling procedure more consistent if some are found to be ambiguous (i.e. change the labels of y)\n\n\n\nImage (or Data) Boosting Notes from How I Won Andrew Ng’s First Data-Centric AI Competition\n\nAugmented training images are used to get embeddings. Apply nearest neighbors to misclassified images with embeddings to get new data. Add to training data\nSteps\n\nGenerate a very large set of randomly augmented images from the training data (treat these as “candidates” to source from).\nTrain an initial model and predict on the validation set.\nUse another pre-trained model to extract features (aka embeddings) from the validation images and augmented images.\nFor each misclassified validation image, retrieve the nearest neighbors (based on cosine similarity) from the set of augmented images using the extracted features. Add these nearest neighbor augmented images to the training set. I will call this procedure “Data Boosting”.\nRetrain model with the added augmented images and predict on validation set.\nRepeat steps 4–6 until we’ve reached the limit of 10K images.\n\nNotes\n\nAlthough augmented images were used for this competition, in practice any large set of images as candidates to source from.\nI generated ~1M randomly augmented images from the training set as candidates to source from\nThe data evaluation spreadsheet is used to keep track of inaccuracies (misclassified images) and to annotate the data. Alternatively, I also spun up an instance of Label Studio with a PostgreSQL backend but I decided not to use it for this competition due to the unnecessary overhead.\nFor the pre-trained model, I used ResNet50 trained on ImageNet.\nI used the Annoy package to perform approximate nearest neighbor search.\nThe number of nearest neighbors to retrieve per misclassified validation image is a hyper-parameter.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Image</span>"
    ]
  },
  {
    "objectID": "qmd/dl-image.html#sec-dl-image-cnn",
    "href": "qmd/dl-image.html#sec-dl-image-cnn",
    "title": "11  Image",
    "section": "11.4 Convolutional Neural Network (CNN)",
    "text": "11.4 Convolutional Neural Network (CNN)\n\n\nMisc\n\nNotes from Convolutional Neural Network (CNN) Architecture Explained in Plain English Using Simple Diagrams\n\nEspecially designed to work with images. They are widely used in the domain of computer vision\n\nTo use MLPs with images, we need to flatten the image. If we do so, spatial information (relationships between the nearby pixels) will be lost. So, accuracy will be reduced significantly. CNNs can retain spatial information as they take the images in the original format.\nCNNs can reduce the number of parameters in the network significantly. So, CNNs are parameter efficient.\n\nImages\n\n\nGrayscale image is represented as (height, width, 1) or simply (height, width) since the 3rd dim is 1\nRGB image is represented as (height, width, 3), where 3 is the number of color channels in the image (3D array)\n\nLayers\n\nConvolutional layers\n\nThe first convolutional layer takes the images as the input and begins to process\n\nThere can be multiple convolutional layers\nThe ReLU activation is used in each convolutional layer\n\nExtracts a set of features from the image while maintaining relationships between the nearby pixels\nComponents and Operations (fig: Gray scale, 1 Filter)\n\n\nConvolutional Operation is a partial matrix multiplication: 1st row (image) * 1st col (kernel) + 2nd row (image) * 2nd col (kernel) +…\n\ne.g. (0*0 + 3*0 + 0*1) + (2*1 + 0*1 + 1*0) + (0*1 + 1*0 + 3*0) = 3\nExample is row*col but evidently it can also be row*row or col*col. Probably doesn’t matter as long as it’s consistent.\n\nKernel (aka Filter or Feature Detector)\n\nThere can be multiple filters in a single convolutional layer\n\nMultiple filters are used to identify a different set of features in the image\n\nThe number of filters increases in each convolutional layer\n\ne.g If we use 16 filters in the first convolutional layer, we usually use 32 filters in the next convolutional layer, and so on\n\nThe size of the filter and the number of filters should be specified by the user as hyperparameters.\nThe size should be smaller than the size of the input image.\nThe elements inside the filter define the filter configuration.\n\nImage section\n\nSection of the input image matrix that is multiply-summed by the kernel\nThe size of the image section should be equal to the size of the filter(s) we choose.\nWe can move the filter(s) vertically and horizontally on the input image to create different image sections.\nThe number of image sections depends on the stride we use.\n\nStride: The number of steps (pixels) that we shift the filter over the input image\n\nStride = 1 is moving the filter on the image horizontally by one step to the right (see 2nd row of images)\n\n\n\nFeature map\n\nOutput matrix from the Convolutional Operations\n\nStores the outputs of different convolution operations between different image sections and the filter(s)\nSize depends on the stride\n\nThe number of elements in the feature map is equal to the number of different image sections that we obtained by moving the filter(s) on the image.\nInput for the next pooling layer.\n\nImage Input is the input matrix\n\nPadding adds additional pixels with zero values to each side of the image which helps to keep the feature map of the same size as the input\n\n\nIf there are several convolutional layers in the CNN, the size of the feature map will be further reduced at the end so that we cannot do other operations on the feature map\nExample: new size of the input image is (8,8)\n\nIf we do the convolution operation now with Stride=1, we get a feature map of size (6x6) that is equal to the size of the original image before applying Padding\n\n\n\nExample: Gray Scale, Multiple Filters\n\n\nFeature map is now an array\n\nExample: RGB image\n\n\nBecause a RGB image has 3 color channels, 3-channel filters are needed to do the calculations\nfinal result (i.e. for one cell in the feature map) is obtained by adding all outputs of each channel’s convolution operation calculations\n\nExample: RGB, mulitple filters\n\n\nFeature map is now an array\n\n\n\nPooling layers\n\nConvolution and pooling layers are used together as pairs (i.e. one is followed by the other)\nObjectives\n\nExtract the most important (relevant) features by getting the maximum number or averaging the numbers.\nReduce the dimensionality (number of pixels) of the output returned from previous convolutional layers.\nReduce the number of parameters in the network.\nRemove any noise present in the features extracted by previous convolutional layers.\nIncrease the accuracy of CNNs.\n\nComponents and Operations (max pooling, stride = 2)\n\n\nThe Pooling Operation happens between a section of the feature map and the filter. It outputs the pooled feature map\n\nFor RGB, pooling operations will be done on each channel independently.\nTypes\n\nMax pooling: Get the maximum value in the feature map section where the filter is applied.\nAverage pooling: Get the average of the values in the feature map section where the filter is applied.\nGlobal-Average-Pooling: similar to average-pooling but instead of using N×N patches of the feature maps, it uses all feature maps area at once\n\n\n\nFilter: empty so no transformation, it only sets the size of the image section where the pooling operation takes place\n\nSize of the filter should be specified by the user as a hyperparameter.\nSize should be smaller than the size of the feature map\nIf the feature map has multiple channels, we should use a filter with the same number of channels\n\nFeature Map: Input to the pooling layer\n\nSize of the feature map sections should be equal to the size of the filter we choose\nThe number of sections depends on the Stride\n\nPooled Feature Map: Output of the pooling layer\n\nInput for the next convolution layer (if any) or for the flatten operation.\nFor RGB, there are same number of channels in the feature map and the pooled feature map (see examples in Covolutional layers section)\n\nOther hyperparameters\n\nStride: The Stride is usually equal to the size of the filter. If the filter size is (2x2), we use Stride=2. (also see Convolutional layers section)\nPadding: Padding is applied to the feature map to adjust the size of the pooled feature map (also see Convolutional layers section)\n\nThe Flatten Operation flattens the pooled feature map output to a single column so that it can be fed the Multiayer Perceptron (MLP) and the image can be classified\n\nOccurs after the final pooling layer is completed\nUnlike flattening the original image (i.e if a MLP was used to classify an image), important pixel dependencies are retained when pooled maps are flattened.\nExample: Grayscale\n\nExample: RGB\n\n\n\n\nFully connected (dense) layers\n\nLayers of the MLP that’s used to classify the image\nInput is the flattened, pooled feature map\nThe ReLU activation is used in each fully connected layer except in the final layer in which we use the Softmax activation for multiclass classification.\n\n\nArchitectures\n\nAlexNet\n\n\n5 convolution layers with non-increasing kernel sizes\n\nLRN is applied on the first and second convolution layers after applying ReLU\n\n3 fully connected layers\n\nReLU activation in the hidden layers\nsoftmax activation for the output layer",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Image</span>"
    ]
  },
  {
    "objectID": "qmd/dl-tabular.html",
    "href": "qmd/dl-tabular.html",
    "title": "12  Tabular",
    "section": "",
    "text": "12.1 Misc",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tabular</span>"
    ]
  },
  {
    "objectID": "qmd/dl-tabular.html#sec-dl-tab-misc",
    "href": "qmd/dl-tabular.html#sec-dl-tab-misc",
    "title": "12  Tabular",
    "section": "",
    "text": "Resources\n\nRaschka’s list of DL tabular papers\n\nSummaries, links to code if available\n\n\nGuide for suitable baseline models: link\nQuestions that I should be able to answer\n\nHow to manage the convergence mechanism in the training process?\nHow to apply transfer learning on a pre-trained network?\nHow to minimize redundant computation?\nHow to reduce the sensitivity of a deep learning technique?\n\nNumerics\n\nneural networks treat numerical inputs as continuous variables. Meaning:\n\nhigher numbers are “greater than” lower numbers\nnumbers that are similar are treated as being similar items.\n\nOkay for a variable like “age” but is nonsensical when the numbers represent a categorical variable (embeddings solve the categorical encoding problem)",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tabular</span>"
    ]
  },
  {
    "objectID": "qmd/dl-tabular.html#sec-dl-tab-preproc",
    "href": "qmd/dl-tabular.html#sec-dl-tab-preproc",
    "title": "12  Tabular",
    "section": "12.2 Preprocessing",
    "text": "12.2 Preprocessing\n\nContinuous\n\nStandardize\nBin\n\na network with discretized features may have an advantage because it doesn’t have to spend any of its parameter budget learning to partition the input space\n“Gradient Boosted Decision Tree Neural Network” (paper) and Uber (See Uber ETA below) found quantile buckets provided better accuracy than equal-width buckets\n\nPotential reason: “maximized entropy: for any fixed number of buckets, quantile buckets convey the most information (in bits) about the original feature value compared to any other bucketing scheme”\n\n\nLogging can create more compact ranges, which then enables more efficient neural network training",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tabular</span>"
    ]
  },
  {
    "objectID": "qmd/dl-tabular.html#sec-dl-tab-papsum",
    "href": "qmd/dl-tabular.html#sec-dl-tab-papsum",
    "title": "12  Tabular",
    "section": "12.3 Paper Summaries",
    "text": "12.3 Paper Summaries\n\nFrom pytorch-widedeep, deep learning for tabular data IV: Deep Learning vs LightGBM\n\nExperiment compared LightGBM performance vs a bunch of tabular data designed DL algorithms from pytorch-widedeep LIB (including TabNet)\n\nLightGBM destroyed them\n\n“the DL algorithm that achieves similar performance to that of LightGBM is a simple Multilayer Perceptron (MLP)”\n“In my experience, DL models on tabular data perform best on sizeable dataset that involve many categorical features and these have many categories themselves.”\nCases where DL models can complement ML models for tabular data\n\nUsing the categorical feature embeddings from DL models as features in other models\n\nI don’t get this. This article makes it sound like they’re an artefact of the DL model itself. I just always thought this was a preprocessing step.\n\nI wonder how these are produced and extracted from the model.\n\n“the embeddings acquire a more significant value, i.e. we learn representations of those categorical features that encode relationships with all other features and also the target for a specific dataset. Note that this does not happen when using GBMs. Even if one used target encoding, in reality there is not much of a learning element there (still useful of course).”\n“Assume that you have a dataset with metadata for thousands of brands and prices for their corresponding products. Your task is to predict how the price changes over time (i.e. forecasting price). The embeddings for the categorical feature brand will give you information about how a particular brand relates to the rest of the columns in the dataset and the target (price). In other words, if given a brand you find the closest brands as defined by embeddings proximity you would be”naturally” and directly finding competitors within a given space (assuming that the dataset is representative of the market).”\n\nImprove performance on a small dataset by “transferring”(?) whats learned from using a DL model on a similar much larger dataset\n\nThe transferring comes from Transfer learning which I have no idea how it works.\n“Assume you have a large dataset for a given problem in one country but a much smaller dataset for the exact same problem in another country. Let’s also assume that the datasets are, column-wise, rather similar. One could train a DL model using the large dataset and”transfer the learnings” to the second, much smaller dataset with the hope of obtaining a much higher performance than just using that small dataset alone.”\n\n\n\n(Raschka summary) On Embeddings for Numerical Features in Tabular Deep Learning (paper, code)\n\nInstead of designing new architectures for end-to-end learning, the authors focus on embedding methods for tabular data: (1) a piecewise linear encoding of scalar values and (2) periodic activation-based embeddings. Experiments show that the embeddings are not only beneficial for transformers but other methods as well – multilayer perceptrons are competitive to transformers when trained on the proposed embeddings.\n\nUsing the proposed embeddings, ResNet, multilayer perceptrons, and transformers outperform CatBoost and XGBoost on several (but not all) datasets.\nSmall caveat: I would have liked to see a control experiment where the authors trained CatBoost and XGboost on the proposed embeddings.\n\n\n(Raschka summary) Why do tree-based models still outperform deep learning on tabular data? (paper)\n\nThe main takeaway is that tree-based models (random forests and XGBoost) outperform deep learning methods for tabular data on medium-sized datasets (10k training examples). The gap between tree-based models and deep learning becomes narrower as the dataset size increases (here: 10k -&gt; 50k).\n\nSolid experiments and thorough investigation into the role of uninformative features: uninformative features harm deep learning methods more than tree-based methods.\nSmall caveats: some of the recent tabular methods for deep learning were not considered; “large” datasets are only 50k training examples (small in many industry domains.)\nExperiments based on 45 tabular datasets; numerical and mixed numerical-categorical; classification and regression datasets; 10k training examples with balanced classes for main experiments; 50k datasets for “large” dataset experiments.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tabular</span>"
    ]
  },
  {
    "objectID": "qmd/dl-tabular.html#sec-dl-tab-arch",
    "href": "qmd/dl-tabular.html#sec-dl-tab-arch",
    "title": "12  Tabular",
    "section": "12.4 Architectures",
    "text": "12.4 Architectures\n\nDeepETA: Uber’s ETA model \n\nModel for residual calculation\n\nEncoder-Decoder architecture with self-attention (article)\n\nTransformer type of architecture (Also see NLP, Transformers)\n\nProcessing\n\nContinuous features were quantile binned. Then both binned numerics and categoricals are embedded.\nLatitude and longitude was binned and multi-feature hashed (See Feature Engineering, Geospatial)\n\nSelf-attention in transformers is a sequence-to-sequence operation that takes in a sequence of vectors and produces a reweighted sequence of vectors\n\ny is the outcome, x is the predictor, K is the number of features\nAttention matrix calculation has quadratic complexity, O(K2d) (I think d is the number of rows).\n\nFaster alternatives that linearize the self-attention calculation: linear transformer, linformer, performer\n\nlinear transformer’s time complexity is O(Kd2) and uses kernel trick to bypass attention matrix calculation\n\nIf K &gt; d, then the linear transformer is faster\n\n\n\n\nUtilizes feature sparsity for speed (any one prediction touches only about 0.25% of the model’s parameters)\n\n“Handful” of layers\n\nMost parameters are in embedding lookup tables\n\nDiscretizing numerics\nMulti-feature hashing\n\nDeepETA simply quantizes the coordinates and performs a hash lookup, which takes O(1) time.\n\nby precomputing partial answers in the form of large embedding tables learned during training, we reduce the amount of computation needed at serving time.\n\nIn comparison, storing embeddings in a tree data structure would require O(log N) lookup time, while using fully-connected layers to learn the same mapping would require O(N2) lookup time\n\n\nDecoder is a fully connected neural network with a segment bias adjustment layer\n\nBias adjustment layers improve raw predictions when there’s a lot of variance in the outcome between groups\n\ne.g. distribution of absolute errors varies by a lot across delivery trips vs rides trips, long vs short trips, pick-up vs drop-off trips, and also across global mega-regions\nOther approaches\n\nAdding group features - outperformed by the bias adj layer\nmulti-task decoder - didn’t meet latency requirements\n\n\nReLU at output to force predicted ETA to be positive;\nClamping to reduce the effect of extreme values",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tabular</span>"
    ]
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html",
    "href": "qmd/econometrics-discrete-choice-models.html",
    "title": "Discrete Choice Models",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Econometrics",
      "Discrete Choice Models"
    ]
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-misc",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-misc",
    "title": "Discrete Choice Models",
    "section": "",
    "text": "AKA Random Utility or Choice Models\ntl;dr:\n\nThe Utility equation is the model equation.\n\nUtility (\\(U\\)) is made up of Observed Utility (\\(V\\)) and Unobserved Utility (\\(\\epsilon\\)).\n(Total) Utility is the LHS of the model equation\nLogit models are typically used to estimate utility as predicted logits.\n\nThe predicted probabilities are called Choice Probabilities\nIf the data is aggregated to the Market Level instead of the individual level, then the predicted logits are now called Market Shares.\n\nObserved Utility is modeled using our data, and Unobserved Utility is the residual of that model.\nThe coefficients (\\(\\beta\\)s) (log odds ratios) of the model are called Marginal Utilities.\n\nThe observed response is typically polytomous and the categories are called Alternatives which are the potential choices of the Decision Maker.\n\nNotes from ResEcon 703 Video Course\n\nWeeks 3, 4, 9, 10, 12, and 13\nVideo lectures talk through mathematics (interpretations, motivations, benefits, limitations, etc.) of the approaches. Each week ends with a coding session illustrating the approaches that’s not included in the videos but is included in the slides.\n\nThe slides are updated with new material while the videos have not.\n\ngithub with slides and problem sets/solutions with R code. Slides provide a deeper introduction to the application the algorithms and the econometrics. Problem set solutions have similar material but written out more clearly. {mlogit} used throughout.\n\nAlso see:\n\nDecision Intelligence\nRegression, Multinomial\nDiagnostics, Classification &gt;&gt; Multinomial\nClassification &gt;&gt; Discriminant Analysis &gt;&gt; Linear Discriminant Analysis (LDA)\nModel Building, brms &gt;&gt; Logistic Regression\n\nUse Cases:\n\nRespondents to a social survey are classified by their highest completed level of education, taking on the values (1) less than highschool, (2) highschool graduate, (3) some post-secondary, or (4) post-secondary degree.\nWomen’s labor-force participation is classified as (1) not working outside the home, (2) working part-time, or (3) working full-time.\nVoters in Quebec in a Canadian national election choose one of the (1) Liberal Party, (2) Conservative Party, (3) New Democratic Party, or (4) Bloc Quebecois.",
    "crumbs": [
      "Econometrics",
      "Discrete Choice Models"
    ]
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-terms",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-terms",
    "title": "Discrete Choice Models",
    "section": "Terms",
    "text": "Terms\n\nAlternative - The levels of a polytomous response in Random Utility Models.\nChoice Probability - The probability that a decision-maker will chose a particular alternative. The predicted response for a Random Utility Model.\nChoice Settings - Can describe in general the choice set itself, or can also be used to describe the context in which the choice by the decision-maker is taking place.\n\ne.g. binary, ordered multinomial, unordered multinomial, and count data frameworks\ne.g. educational setting, marketing setting\ne.g. costs associated with each alternative for each individual\n\nConsumer Surplus - Monetary gain to a consumer from “purchasing” a good for less than the value the consumer places on the good\nCost Trade-Offs - How changes in one in a choice attribute can be offset by another.\n\ne.g. If the operating cost were to increase by $1, what reduction the purchase price would result in the same utility/choice probability for the consumer.\n\nDiscounted Utility - The utility of some future event, such as consuming a certain amount of a good, as perceived at the present time as opposed to at the time of its occurrence. It is calculated as the present discounted value of future utility, and for people with time preference for sooner rather than later gratification, it is less than the future utility.\nMarginal Utility - Coefficients in the logit model (log odds ratios). The added satisfaction that a consumer gets from having one more unit of a good or service. The concept of marginal utility is used by economists to determine how much of an item consumers are willing to purchase.\nMarket Level - Environment or category where a class or brand of products share the same attributes\n\ne.g. All Cokes should cost the same in Indianapolis but that price may be different from the price of Cokes in Nashville. Therefore, Indianapolis and Nashville are separate markets.\n\nMarket Share - The percentage of total sales in an industry or product category that belong to a particular company during a discrete period of time. For a Random Utility Model, when the data is at the market level instead of the individual level, the predicted response is the Market Share.\nOutside Product (aka Outside Option) - Typically a “purchase nothing” indicator variable/variable category\n\nCan just be a vector of 0s\n\nRandom Utility Models - These models rely on the hypothesis that the decision maker is able to rank the different alternatives by an order of preference represented by a utility function, the chosen alternative being the one which is associated with the highest level of utility. They are called random utility models because part of the utility is unobserved and is modeled as the realization of a random deviate.",
    "crumbs": [
      "Econometrics",
      "Discrete Choice Models"
    ]
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-randutmod",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-randutmod",
    "title": "Discrete Choice Models",
    "section": "Random Utility Models",
    "text": "Random Utility Models\n\nMisc\nRandom Utility - utility theory can readily be understood as the idea that people behave in line with self-interset where self-interest reflects peoples’ needs to save time and economize on effort.\n\nThese models rely on the hypothesis that the decision maker is able to rank the different alternatives by an order of preference represented by a utility function, the chosen alternative being the one which is associated with the highest level of utility. They are called random utility models because part of the utility is unobserved and is modeled as the realization of a random deviate.\n\nData sets used to estimate random utility models have therefore a specific structure that can be characterized by three indexes: the alternative, the choice situation, and the individual. The distinction between choice situation and individual indexes is only relevant if we have repeated observations for the same individual.\n\nExamples of variable types\n\nData Descriptions\n\nEach individual has responded to several (up to 16) scenarios.\nFor every scenario, two train tickets A and B are proposed to the user, with different combinations of four attributes: price (the price in cents of guilders), time (travel time in minutes), change (the number of changes) and comfort (the class of comfort, 0, 1 or 2, 0 being the most comfortable class).\n\nChoice Situation Specific\n\ndata1: Length of the vacation and the Season\ndata2: values of dist, income and urban are repeated four times.\n\ndist (the distance of the trip)\nincome (household income)\nurban (a dummy for trips which have a large city at the origin or the destination)\nnoalt the number of available alternatives\n\n\nIndividual Specific\n\ndata1: Income and Family Size\ndata2: None\n\nAlternative Specific\n\ndata1: Distance to Destination and Cost\ndata2:\n\ntransport modes (air, train, bus and car)\ncost for monetary cost\nivt for in vehicle time\novt for out of vehicle time\nfreq for frequency\n\n\n\nThe unit of observation is typically the choice situation, and it is also the individual if there is only one choice situation per individual observed, which is often the case\n\nAgent/Decison-Maker gets some amount of utility from each of the alternatives (set of actions the user can choose from)\nThe amount of utility for each alternative and each decision maker can depend on:\n\nobserved characteristics of the alternative themselves\nobserved characteristics of the decision maker\nunobserved characteristics\n\nUtility is not observed but can be inferred by factors that are observed (and how each factor affects Utility):\n\nChosen alternative, i, by each decision maker, n\nSome attributes about each alternative\nSome attributes about each decision maker.\n\nTotal Utility: \\(U_{nj} = V_{nj} + \\epsilon_{nj}\\)\n\nWhere Representative (or Observed) Utility, \\(V_{nj} = V(x_{nj}, s_n)\\)\n\n\\(x_{nj}\\) is the vector of attributes for alternative j specific to decison maker n\n\\(s_n\\) is the vector of attributes for decision maker n\\\n\n\\(\\epsilon_{nj}\\) is called the “Random Utility” which is unobserved (i.e. the stuff we didn’t adjust for).\n\nUtility Model: \\(U_{nj} = \\hat{\\beta}x_{nj} + \\epsilon_{nj}\\)\n\n\\(V_j = \\hat{\\beta} \\boldsymbol{x}\\)\nSince \\(U_{nj}\\) is unobserved, decision maker choice from alternatives is used as the response variable to proxy Utility.\n\\(\\beta\\) is called a “structural parameter”\n\nChoice Probabilities\n\\[\nP_{ni} = \\int_{\\epsilon} \\mathbb{1} (\\epsilon_{nj} - \\epsilon_{ni} &lt; V_{ni} - V_{nj} \\;\\; \\forall j \\neq i) f(\\boldsymbol{\\epsilon}_n) d \\boldsymbol{\\epsilon}_n\n\\]\n\nAssuming the decision maker makes choices that maximize utility allows us to use choice probabilities to model which alternative maximizes utility.\nSee video for the derivation from \\(\\text{PR}(U_{ni} &gt; U_{nj} \\;\\; \\forall j \\neq i)\\)\nThis is the cumulative distribution for \\(\\epsilon_{nj} - \\epsilon_{ni}\\) which is a multidimensional integral over the density of unobserved utility, \\(f(\\epsilon_n)\\).\nAssumptions about \\(f(\\epsilon_n)\\) are what yields different discrete choice models",
    "crumbs": [
      "Econometrics",
      "Discrete Choice Models"
    ]
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-binlog",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-binlog",
    "title": "Discrete Choice Models",
    "section": "Binary Logit",
    "text": "Binary Logit\n\nMisc\nChoice Probability\n\\[\n\\begin {align}\nP_{n1} &= \\frac {1}{1 + e^{-(V_{n1} - V_{n0})}} \\\\\n&= \\frac{1}{1 + e^{-(\\hat \\beta \\boldsymbol{x})}}\n\\end {align}\n\\]\n\nThe probability of decision maker, \\(n\\), selecting choice, \\(1\\).\n\ni.e. predicted probability\n\nThe estimated utility of decision maker, \\(n\\), selecting choice, \\(1\\) is the predicted logit not the predicted probability\n\nCost Trade Offs\n\nSee Terms\nExample: An increase in “operating cost” would need to be offset in how much of a reduction in purchasing price in order for the decision maker’s utility not to change?\n\\[\n\\begin {align}\nU_{n1} &= \\beta_0 + \\beta_1 P_n + \\beta_2 C_n + \\epsilon_{n1} \\\\\ndU_{n1} &= \\beta_1 dP_n + \\beta_2 dC_n \\\\\ndU_{n1} &= 0 \\implies \\frac{dP_n}{dC_n} = -\\frac{\\beta_2}{\\beta_1}\n\\end {align}\n\\]\nAlso see Example: Preferences for Air Conditioning\n\nImplied Discount Rate\n\nSee Terms\nExample:\n\\[\n\\begin {align}\n(1) \\;\\; U_{n1} &= \\alpha_0 + \\alpha_1(P_n + \\frac{1}{\\gamma}C_n) + \\omega_n \\\\\n(2) \\;\\; U_{n1} &= \\beta_0 + \\beta_1P_n + \\beta_2C_n + \\epsilon_{n1} \\\\\n&\\therefore \\; \\alpha_1 = \\beta_1 \\;\\&\\; \\frac{\\alpha_1}{\\gamma} = \\beta_2 \\\\\n&\\implies \\gamma = \\frac{\\beta_1}{\\beta_2}\n\\end {align}\n\\]\n\n\\(P_n\\) is product price and \\(C_n\\) is the product’s operating cost\n\\(\\gamma\\) is the discount rate\n\\(\\alpha_1\\) is supposed to be the marginal utility for household income but it doesn’t play into the discount rate calculation\n(1) is the general formula for a household’s expected utility after purchasing the product which assumes infinite time horizon for operating cost\n(2) binary logistic model equation for utility\n\n\nExample: Student preferences for either driving their car or riding the bus to campus\n\nModel\nmodel_1a &lt;- \n  glm(formula = car ~ cost.car + time.car + time.bus,\n      family = 'binomial',\n      data = data_binary)\n\ncar - TRUE/FALSE binary variable that indicates whether the decision maker chose to drive (TRUE) or take the bus (FALSE)\ncost.car - continuous, cost (in dollars) to drive to campus\ntime.car - continuous, time (in minutes) to drive to campus\ntime.bus - time (in minutes) to ride the bus to campus\n\nsummary Results\n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)  2.23327    0.34662   6.443 1.17e-10 ***\n#&gt; cost.car    -2.07716    0.73245  -2.836  0.00457 ** \n#&gt; time.car    -0.33222    0.03534  -9.400  &lt; 2e-16 ***\n#&gt; time.bus     0.13257    0.03240   4.092 4.28e-05 ***\n\nCoefficents (log ORs) are marginal utilities\nThe cost of driving and the time spent driving both decrease the utility of driving, and the time spent riding the bus increases theutility of driving relative to riding the bus.\nIntercept: Driving a car generates 2.23 “utils” of utility\ncost.car: Each additional dollar of driving cost reduces utility of driving by 2.08\ntime.car: Each additional minute of driving reduces utility of driving by 0.33\ntime.bus: Each additional minute riding on the bus increases utility of driving by 0.13\n\nThe dollar value that a student places on each hour spent driving and on each hour spent on the bus.\n## Calculate hourly time-value for each commute mode\nabs(coef(model_1a)[3:4] / coef(model_1a)[2]) * 60\n#&gt; time.car time.bus\n#&gt; 9.596257 3.829494\n\nEach hour of driving has a dollar value of $9.60 and each hour of bus riding has a dollar value of $3.83.\n\nIn other words, a student would be willing to pay $9.60 to spend one less hour commuting by car but only $3.83 to spend one less hour commuting by bus.\n\nSince the time variables are in minutes, I would’ve guessed that you would divide by 60 to get hours. Dividing by 60 gives you values of only 1000ths of a dollar though, so I guess not.\n\nAllowing cost to vary inversely by income (aka Heterogeneous Parameter)\nmodel_1b &lt;- \n  glm(formula = car ~ I(cost.car / income) + time.car + time.bus,\n      family = 'binomial',\n      data = data_binary)\n\nsummary(model_1b)\n#&gt; Coefficients:\n#&gt;                     Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)          2.26541    0.33110   6.842 7.81e-12 ***\n#&gt; I(cost.car/income) -53.63314   14.54884  -3.686 0.000227 ***\n#&gt; time.car            -0.33521    0.03484  -9.622  &lt; 2e-16 ***\n#&gt; time.bus             0.13589    0.02880   4.719 2.37e-06 ***\n\nDividing cost by income takes into account that students with different incomes might have different sensitivities to cost\n\nincome - student annual income (in 1000 dollars)\n\nA higher level of income yields a lower marginal utility of cost.\nIntercept: Driving a car generates 2.27 “utils” of utility\ncost.car/income: An additional 0.1 percentage point of cost.car (i.e. driving cost) as a share of income reduces utility by -53.63\n\nI think a “share” means that proportion of car.cost to income increases by 0.1 of a percentage point\n\nCalculate marginal utility of car cost at different incomes\n-coef(model_1b)[2] / c(15, 25, 35)\n#&gt; [1] 3.575543 2.145326 1.532375\n\nHe doesn’t give units when stating these results, but did so when taking the ratio of 2 marginal utilities.\nHe takes the negative here, but I’m not sure if that’s because you always take the negative or he’s just looking for the magnitude (i.e. abs).\n\nI feel like it’s the latter. If that’s the case though, then it will always be the case that a larger income means less absolute marginal utility no matter the value of the coefficient.\nSo this seems that your making that assumption beforehand by choosing to divide by income instead of multiplying by it, and the goal is to just calculate the amounts by which the effect of cost is affected by larger incomes.\n\n\nCalculate hourly time-value for each commute mode at different incomes\nrep(abs(coef(model_1b)[3:4] / coef(model_1b)[2]), 3) * \n  c(rep(15, 2), rep(25, 2), rep(35, 2)) * 60\n#&gt; time.car  time.bus  time.car  time.bus  time.car  time.bus \n#&gt; 5.625096  2.280260  9.375160  3.800433 13.125224  5.320607 \n\nAt each of these incomes, each hour of driving has a dollar value of $5.63, $9.38, and $13.13, respectively; and each hour of bus riding has a dollar value of $2.28, $3.80, and $5.32, respectively.\n\n\n\nExample: Visualization\n\nKernel Density of Estimated Utilities\n\n## Plot density of utilities\nac_data %&gt;%\nggplot(aes(x = utility_ac_logit)) +\ngeom_density() +\nxlab('Utility of air conditioning') +\nylab('Kernel Density')\n\nWhere utility_ac_logit = predict(binary_logit_mod)\n\nCan also do this with choice probabilities\n\n\nFraction of Choice (Adoption) vs Estimated Utility\n\nac_data %&gt;%\n  mutate(bin = cut(utility_ac_logit,\n                   breaks = seq(-3, 2, 0.25),\n                   labels = 1:20)) %&gt;%\n  group_by(bin) %&gt;%\n  summarize(fraction_ac = mean(air_conditioning), .groups = 'drop') %&gt;%\n  mutate(bin = as.numeric(bin),\n         bin_mid = 0.25 * (bin - 1) - 2.875) %&gt;%\n  ggplot(aes(x = bin_mid, y = fraction_ac)) +\n    geom_point() +\n    xlab('Utility of air conditioning') +\n    ylab('Fraction with air conditioining')\n\nFrom Week 4 slides at around slide 59\nShows the fraction of adoption of air conditioning (choice 1) within each bin of estimated utility.\n\nCan also do this with choice probabilities\n\nIn general, we expect decision makers with greater estimated utility to choose air conditioning more often which is what is seen.\n\nThere are a couple of exception around bin -1.5, but maybe those are due to there not being enough individuals in those bins to be representative. Otherwise, the model is missing something for these individuals.\n\nI’m not sure if this should be linear or maybe sigmoid — it’s a proportion vs a logit. Think I’m leaning towards sigmoid.\n\nHe did do this with choice probabilities and that one looked linear which is what you’d expect since it’s like a observed vs predicted plot.\n\n\n\nExample: Marginal Effects and Elasticities\n\nThese formulas are the same as the ones shown in the Multiomial Logit section\nThese values are on the observation level so one useful thing to look at would be summary statistics and density plots\nMarginal Effects\nac_data &lt;- ac_data %&gt;%\nmutate(marg_eff_system = coef(binary_logit_mod)[2] *\n          probability_ac_logit * (1 - probability_ac_logit),\n       marg_eff_operating = coef(binary_logit_mod)[3] *\n          probability_ac_logit * (1 - probability_ac_logit))\nElasticities\nac_data &lt;- ac_data %&gt;%\nmutate(elasticity_system = coef(binary_logit_mod)[2] *\n          cost_system * (1 - probability_ac_logit),\n       elasticity_operating = coef(binary_logit_mod)[3] *\n          cost_operating * (1 - probability_ac_logit))\n\nExample: Preferences for Air Conditioning\n\nModel\nbinary_logit &lt;- \n  glm(formula = air_conditioning ~ cost_system + cost_operating,\n      family = 'binomial',\n      data = ac_data)\n\nair_conditioning: Binary TRUE/FALSE, the choice between buying an AC (TRUE) or not (FALSE)\ncost_system: Cost of buying the AC\ncost_operating: Cost of operating the AC\n\nCost Trade-Offs\n-coef(binary_logit)[3] / coef(binary_logit)[2]\n## cost_operating\n## -5.186972\n\nSays a dollar increase in AC purchasing cost would need to be offset by a decrease of $5.19 in operating cost in order for the customer utility for buying an AC to remain the same.\n\nImplied Discount Rate\ncoef(binary_logit)[2] / coef(binary_logit)[3]\n## cost_system\n## 0.1927907",
    "crumbs": [
      "Econometrics",
      "Discrete Choice Models"
    ]
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-mnl",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-mnl",
    "title": "Discrete Choice Models",
    "section": "Multinomial Logit (MNL)",
    "text": "Multinomial Logit (MNL)\n\nSee Regression, Multinomial &gt;&gt; Multinomial Logit for examples\nAKA Generalized Logit\nWhen the polytomous response has m levels (aka Alternatives), the multinomial logit model comprises m−1 log-odds comparisons with a reference level, typically the first or last.\n\nThe likelihood under the model and the fitted response probabilities that it produces are unaffected by choice of reference level, much as choice of reference level for dummy regressors created from a factor predictor doesn’t affect the fit of a regression model.\n\nChoice Probability for alternative, i, and decision-maker, n:\n\\[\nP_{ni} = \\frac {e^{V_{ni}}}{\\sum_j e^{V_{nj}}} = \\frac {e^{\\hat{\\beta}x_{ni}}}{\\sum_j e^{{\\hat{\\beta}x_{nj}}}}\n\\]\n\nThe probability that decision-maker, \\(n\\), chooses alternative, \\(i\\)\nPredicted probability output from the model\n\nMarket Shares\n\nMarket Share for alternative, i\n\\[\nS_i = \\frac {e^{V_i}}{\\sum_j e^{V_j}}\n\\]\n\nWhat distinguishes this from a choice probability is that unit is at the market level and not the individual level (see \\(\\vec{x}\\) in the next section), hence no “n” in the equation\n\nThe difference in market share of products i and j\n\\[\n\\ln (S_i) - \\ln (S_j) = \\hat{\\beta} (x_i - x_j)\n\\]\n\n\\(x_i\\) is a set of variables that describe product i.\n\nTherefore these will be the same for every decision-maker.\n\n\\(j\\) is typicially a reference level (aka outside option)\n\nIf the products were a product such as cereal, then the outside option would typically be “not buying cereal” where the utility is 0.\n\n\n\nConsumer Surplus\n\nThe expected consumer surplus that decision maker n obtains when faced with a choice setting is\n\\[\n\\nabla \\mathbb{E}(CS_n) = \\frac{1}{\\alpha_n} \\left[\\ln \\left(\\sum_{j=1}^{J^1} e^{V^1_{ng}} \\right) - \\ln \\left(\\sum_{j=0}^{J^1} e^{V^0_{ng}} \\right) \\right]\n\\]\n\nAssumptions\n\nExogenity of the alternative attribute variables of the alternatives: \\(\\mathbb{E}[\\epsilon \\; | \\; \\vec {x}] = 0\\) (i.e mean of the residuals is 0)\n\nExample of Endogenity\n\nTaking a car to work or taking the bus to work are the alternatives.\nIf a commuter likes to drive, they won’t care about living close to a bus stop\nIf a commuter likes to take the bus, they are more likely to live close to a bus stop\nTherefore your logit model will be biased because any attribute variable that is related to the distance a decision-maker is from a bus stop will be endogenous.\n\ne.g. Time each mode of transportation takes to get to work will be endogenous because distance from the bus stop affects time and distance to bus stop is correlated with a decision maker already biased towards buses and not with a decision maker who would choose bus or car.\n\ni.e. distance isn’t randomly distributed among all decision makers.\n\n\n\nSolution: TBD in a later lecture\n\niid residuals following a Gumbel distribution\n\nResiduals are the unobserved utility (i.e. random utility).\nFor panel data, it’s unlikely that each unobserved alternative attribute for a decision maker is independent across a time component which makes this a bad model for such data.\n\ne.g. If a person chooses Count Chocula one day and Fruit Loops the next, it’s unlikely that the unobserved components of utility that led to the Count Chocula choice aren’t correlated with the unobserved components of utility that led to the Fruit Loops choice.\n\n\n\nSubstitution Patterns\n\nIndependence of Irrelavent Alternatives (IIA) Property of Logit models\n\nExample: car, blue bus , red bus\n\nIn the beginning, the decision maker only has 2 choices: drive car or ride blue bus.\nThe probability of choosing the alternative car is the same for a decision maker as choosing the alternative blue bus: \\(P_c = P_{bb} = 1/2\\)\nThen a red bus is added to the choices, but the decision maker doesn’t care whether the bus is red or blue. Therefore the blue bus and the red bus have the same choice probability: \\(P_{bb} = P_{rb}\\)\nBut from the IIA property, the ratio of choice probability between car and blue bus is not changed after introduction of the red bus. Therefore, \\(P_c = P_{bb} = P_{rb} = 1/3\\)\nIn reality, the choice probability for car has NOT decreased, since for decision maker, the choice is actually between car and bus — color isn’t a relative factor\n\nExample: Hummer, Escalade, Prius\n\nHummer lowers price.\nWill Hummer attract more Escalade drivers than Prius?\nThe logit model says that “proportional substitution to the Hummer will be equal for the Escalade and the Prius.\n\ni.e. The lowering of the Hummer price will have an equal effect on Escalade sales and Prius sales which is unrealistic\n\n\n\nProportional Substitution\n\nNote the logit cross-elasticity equation for alternative, i, given a change in alternative, j (See below, Elasticity &gt;&gt; Cross Elasticity). The change in choice probability for alternative i only depends on the change to the attribute for alternative, j.\nThis means that we can substitute any alternative for i (other than j) and the cross-elasticity will be the same and therefore the change in choice probability will be the same.\n\ni.e. We change the value of an attribute for alternative j. Then, all choice proabilities for the other alternatives (other than j) change by the same proportion. Doesn’t matter if alternative j is correlated more strongly with another alternative or not.\n\nShows a lack of flexibility in the logit model — there is no structure to model the correlation between alternatives. (See Mixed Logit)\n\n\nMarginal Effects\n\\[\n\\text{ME} = \\frac {\\partial P_{ni}} {\\partial z_{ni}} = \\beta_z P_{ni}(1-P_{ni})\n\\]\n\nThe change in the probability of choosing alternative, i, after a change in the attribute of alternative, i.\n\\(P_{ni}\\) is the probability of decision maker, n, choosing alternative i.\nCross Marginal Effect \\[\n\\text{CME} = \\frac {\\partial P_{ni}} {\\partial z_{nj}} = -\\beta_z P_{ni}P_{nj}\n\\]\n\nThe change in the probability of choosing alternative, \\(i\\), after a change in the attribute of alternative, \\(j\\).\n\n\nElasticity\n\\[\nE_{iz_{ni}} = \\beta_z z_{ni} (1-P_{ni})\n\\]\n\nMarginal Effects and Elasticities are similar except elasticities are percent change.\n\ne.g. a percentage change in a regressor results in this much of a percentage change in the response level probability\n\n\\(E_{iz_{ni}}\\) is the elasticity of the probability of alternative, i, with respect to \\(z_{ni}\\), an observed attribute of alternative, i\n\\(P_{ni}\\) is the predicted probability of alternative, i, for decision maker, n.\nCross Elasticity \\[\nE_{iz_{ni}} = -\\beta_z z_{nj} P_{nj}\n\\]",
    "crumbs": [
      "Econometrics",
      "Discrete Choice Models"
    ]
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-nestlog",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-nestlog",
    "title": "Discrete Choice Models",
    "section": "Nested Logit",
    "text": "Nested Logit\n\nFits separate models for each of a hierarchically nested set of binary comparisons among the response categories. The set of m−1 models comprises a complete model for the polytomous response, just as the multinomial logit model does.\nMisc\n\nIIA still holds for two alternatives in the same dichotomy, but doesn’t hold for alternatives of different different dichotomies\nBoth MNL and Nested Logit methods have have p×(m−1) parameters. The models are not equivalent, however, in that they generally produce different sets of fitted category probabilities and hence different likelihoods.\n\nMultinomial logit model treats the response categories symmetrically\n\nExtensions\n\nPaired Combinatorial Logit\n\nAllows alternatives to be in multiple dichotomies and multiple hierarchies and for them to have more complex correlation structures\n\nIn nested logits, only alternatives within the same hierarchy are modeled as being correlated with each other.\n\nCreates pairwise dichotomies for each combination of alternatives (i.e. each alternative appears in J-1 nests)\nBest for data with fewer alternatives since the parameter space can explode fairly quickly\n\nGeneralized Nested Logit\n\nAllow alternatives to be in any dichotomy in any hierarchy, but assign a weight to each alternative in each dichotomy.\nEstimate Parameters: \\(\\lambda_k\\) (See Choice Probability below), \\(\\alpha_{jk}\\) : “weight” or proportion of alternative, \\(j\\), in dichotomy, \\(k\\)\nNeed to be careful about how many dichotomies that you place each alternative, since the parameter space can explode fairly quickly\n\nHeteroskedastic Logit\n\nHeteroskedacity in this sense means that the variance of the unobserved utility (aka residuals) can be different for each alternative\n\\[\n\\mbox{Var}(\\epsilon_{nj}) = \\frac {(\\theta_j \\pi)^2}{6}\n\\]\nSince there’s no closed form solution, simulation methods must be usded to get the choice probabilities and model parameters.\n\n\n\nConstruction of Dichotomies\n\nBy the construction of nested dichotomies, the submodels are statistically independent (because the likelihood for the polytomous response is the product of the likelihoods for the dichotomies), so test statistics, such as likelihood ratio (G2) and Wald chi-square tests for regression coefficients can be summed to give overall tests for the full polytomy.\nNested dichotomies are not unique and alternative sets of nested dichotomies are not equivalent: Different choices have different interpretations. Moreover, and more fundamentally, fitted probabilities and hence the likelihood for the nested-dichotomies model depend on how the nested dichotomies are defined.\nExample: 2 methods of splitting a 4-level response into dichotomies\n\n\nLeft: Y = {1, 2, 3, 4} → {1,2} vs {3,4} → {1} vs {2} and {3} vs {4}\nRight: (Continuous Logit) Y = {1, 2, 3, 4} → {1} vs {2, 3, 4} → {2} vs {3, 4} → {3} vs {4}\n\n{1} vs. {2,3,4} could represent highschool graduation\n{2} vs. {3,4} could represesnt enrollment in post-secondary education\n{3} vs. {4} could represent completion of a post-secondary degree.\n\n\n\nAssumptions\n\nAssumes Gumbel distribution of errors but relax the i.i.d. hypothesis\n\nAllows for the unobserved (and random) components of utility, \\(\\epsilon_{nj}\\) , to be correlated for the same decision maker\nExample: {drive} vs {carpool} and {bus} vs {train}\n\nModel allows from unobserved components of utility (i.e. residuals of the model) of drive and carpool to correlated for the same decision maker but not the unobserved components of utility for carpool and train.\n\n\nAssumes Exogenity (See Multinomial Logit &gt;&gt; Assumptions)\n\nThis model is also bad for panel data\n\n\nChoice Probability for alternative, i, and decision-maker, n\n\\[\nP_{ni} = \\frac {e^{V_{ni}/\\lambda_k}(\\sum_{j\\in B_k}e^{V_{nj}/\\lambda_k})^{\\lambda_k - 1}}{\\sum_{\\ell=1}^K (\\sum_{j\\in B_\\ell}e^{V_{nj}/\\lambda_\\ell})^{\\lambda_\\ell}}\n\\]\n\nThe probability that decision-maker, \\(n\\), chooses alternative, \\(i\\)\n\nPredicted probability output from the model\nWhen data is at the market level, the \\(n\\) index (i.e. individual) is removed from the equation above and the LHS becomes Market Share (See Terms). The difference in market share between alternatives (e.g. brands) is:\n\\[\n\\ln(S_i) - \\ln(S_m) = \\hat{\\beta}(x_i - x_m) + (1-\\lambda_k)\\ln S_{i|B_k} - (1-\\lambda_\\ell)S_{m|B_\\ell}\n\\]\n\nThe index, \\(m\\), is typically a reference level called the “outside option” (See Terms) which is alternative of NOT doing an action where the utility is 0.\n\nIf the products were a product such as cereal, then the outside option would typically be “not buying cereal” which would have a utility is 0.\n\n\n\n\\(\\lambda_k\\) is a measuer of independence in dichotomy, \\(k\\) that gets estimated\n\\(1-\\lambda_k\\) is a measure of the correlation within dichotomy, \\(k\\)\nThe nested logit is consistent with the random utility model when \\(\\lambda_k \\in (0,1] \\; \\forall k\\)\n\nElasticity\n\\[\nE_{iz{ni}} = \\beta_z z_{ni} \\left(\\frac{1}{\\lambda_k} - \\frac{1-\\lambda_k}{\\lambda_k}P_{ni|B_k} - P_{ni}\\right)\n\\]\n\nMarginal Effects and Elasticities are similar except elasticities are percent change.\n\ne.g. a percentage change in a regressor results in this much of a percentage change in the response level probability\n\nElasticity of alternate, \\(i\\), in dichotomy, \\(k\\), for decision-maker, \\(n\\), with respect to its attribute, \\(z_{ni}\\)\n\nCross-Elasticity\n\\[\nE_{iz_{nm}} =\n\\left\\{ \\begin{array}{lcl}\n-\\beta_z z_{zm}P_{nm} \\left(1+ \\frac{1-\\lambda_k}{\\lambda_k} \\frac {1}{P_{nB_k}} \\right) & \\mbox{if} \\; m \\in \\beta_k \\\\\n-\\beta_z z_{zm}P_{nm} & \\mbox{if} \\; m \\notin \\beta_k\n\\end{array}\\right.\n\\]\n\nWhere\n\\[\nP_{nB_k} = \\sum \\limits_{j \\in B_k} P_{nj} \\;\\; \\text{and} \\;\\; P_{ni|B_k} = \\frac {P_{ni}}{P_{nB_k}} = \\frac {P_{ni}}{\\sum_{j \\in B_k} P_{nj}}\n\\]\nThe percent change in the probability of choosing alternative, \\(i\\), after a percent change in the attribute of alternative, \\(m\\).\nCross elasticity depends on whether both alternatives are in the same dichotomy.",
    "crumbs": [
      "Econometrics",
      "Discrete Choice Models"
    ]
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-mixlog",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-mixlog",
    "title": "Discrete Choice Models",
    "section": "Mixed Logit",
    "text": "Mixed Logit\n\nIndividual heterogeneity can be introduced in the parameters associated with the covariates entering the observable part of the utility or in the variance of the errors.\n\nDoesn’t exhibit IIA (See Multinomial Logit &gt;&gt; Substitution Patterns) since correlations between alternatives are modeled.\nModel creates a distribution of \\(\\beta\\)s, so \\(\\beta\\) is allowed to vary throughout a population.\n\nMisc\n\nMarket Level Data\n\nObserve price, market share, and characteristics of a product\n\n\nChoice Probability\n\\[\n\\begin{align}\n&P_{ni} = \\int L_{ni}(\\beta) \\; f(\\beta\\;|\\;\\boldsymbol{\\theta}) \\; d\\beta\\\\\n&\\mbox{where} \\; L_{ni}(\\beta) = \\frac {e^{V_{ni}(\\beta)}}{\\sum_{j=1}^J e^{V_{ni}(\\beta)}}\n\\end{align}\n\\]\n\n\\(\\boldsymbol{\\theta}\\) is a vector of distributional parameters (e.g. \\(\\mu, \\sigma^2\\) for a Normal Distribution)\n\nThe \\(\\theta\\) parameters are what get estimated by the model in order to calculate a distribution of \\(\\beta\\)s (i.e. Bayesian posterior)\n\nIntegrates over a density so there is not a closed form solution. Therefore, numerical simulation must be used.\n\nElasticities don’t have a closed form solution either (Assume Marginal Effects are the same way)(See lecture videos for the math)\n\nThink of it as a weighted average of logit choice probabilities\n\nThe logit choice probabilities are evaluated a different values of \\(\\beta\\), and each logit choice probability is weighted by the density \\(f(\\beta\\;|\\;\\boldsymbol{\\theta})\\)\n\ni.e. Based on the density, more likely \\(\\beta\\)s will be more heavily weighted.\n\n\nIn statistical language, it’s a mixed function of logit choice probabilities, \\(L_{ni}(\\beta)\\), and the mixing distribution, \\(f(\\beta\\;|\\;\\boldsymbol{\\theta})\\)\n\nRandom Coefficients\n\\[\nU_{nj} = \\hat{\\alpha}\\boldsymbol{x}_{nj} + \\hat{\\mu}\\boldsymbol{z}_{nj} + \\epsilon_{nj}\n\\]\n\n\\(x_{nj}\\), \\(z_{nj}\\) - Data for alternative j and decision maker n\n\\(\\hat{\\alpha}\\) - Vector of fixed coefficients (i.e. same for all decision makers)\n\\(\\hat{\\mu}_n\\) - Vector of random coefficients (i.e. a coefficient for each decision maker)\n\\(\\epsilon_{nj}\\) - Residual from an extreme value distribution (e.g. Gumbel)\nCorrelated Random Utility\n\nLet \\(\\nu_{nj} = \\boldsymbol{\\hat{\\mu}}_n \\boldsymbol{z}_{nj} + \\epsilon_{nj}\\) be the random (unobserved) component of utility and \\(\\mbox{Cov}(\\nu_{ni}, \\nu_{nj}) = \\boldsymbol{z}_{ni} \\Sigma \\boldsymbol{z}_{nj}\\) the covariance between random utilities of different alternatives where \\(\\Sigma\\) is the variance/covariance matrix for \\(\\boldsymbol{\\hat{\\mu}}\\)\nThis is a structure to model the correlation between alternatives\n\n\nPanel Data\n\nData where each decision maker makes multiple choices over time periods\nModel allows for unobserved preference variation through random coefficients, which yields correlations in utility over time for the same decision maker\nDecision maker, n, chooses from a vector of alternatives over T time periods\nUtility\n\\[\nU_{njt} = \\hat{\\beta}_n x_{njt} + \\epsilon_{njt}\n\\]\n\nThe utility, U, for decision maker, n, in choosing alternative, j, at time period, t.\n\\(\\hat{\\beta}\\) varies for each decision maker but is constant across time.\n\nLag or lead predictors can be included\nLagged responses can be included\n\nUse cases:\n\nHabit Formation\nVariety-Seeking Behavior\nSwitching Costs\nBrand Loyalty\n\n\nOnly models a sequence of static choices\n\nLagged responses account for past choices affecting current choices, but not how current choices affect future choices\nA fully dynamic discrete choice model models how every choice affects subsequent choices.\n\n\nIndividual-level Coefficients\n\nConditional Distribution of Coefficients: \\(h(\\beta \\;|\\; \\boldsymbol{y}_n, \\boldsymbol{x}, \\boldsymbol{\\theta})\\)\n\nDistribution of \\(\\beta\\) among the group, from a popuation with an unconditional distribution defined by \\(\\boldsymbol{\\theta}\\), who choose a sequence of alternatives \\(y_n\\) when faced with choice setting, \\(\\boldsymbol{x}\\)\n\\(y_n\\) is the sequence of choices made by decision maker, n (i.e. panel data)\n\\(\\boldsymbol{\\theta}\\) is the vector of \\(\\beta\\) distribution parameters\n\\(\\boldsymbol{x}\\) is the vector of choice attributes\nProportional to the product (i.e. joint distribution) of\n\\[\nP(\\boldsymbol{y}_n \\;|\\; \\boldsymbol{x}_n, \\beta) \\;\\times\\; f(\\beta \\;|\\; \\boldsymbol{\\theta})\n\\]\n\n(Left) The probability that an individual with coefficients \\(\\beta\\) would choose \\(y_n\\)\n(Right) The likelihood of observing \\(\\beta\\) in the population\n\n\nMean of the Conditional Distribution (aka Conditional Mean Coefficients)\n\nMore practical to calculate than the conditional distribution itself.\nIt’s the weighted average of vectors of \\(\\beta\\) drawn randomly from the \\(\\beta\\) distribution, \\(\\text f(\\beta|\\boldsymbol{\\theta})\\), \\(R\\) times.\n\\[\n\\begin {align}\n\\breve{\\beta}_n &= \\sum_{r=1}^R w_r \\beta_r \\\\\n\\mbox{where} \\;\\; w_r &= \\frac{P(\\boldsymbol{y}_n|x_n, \\boldsymbol{\\beta}_r)}{\\sum_{r=1}^R P(\\boldsymbol{y}_n|x_n, \\boldsymbol{\\beta}_r)}\n\\end {align}\n\\]\n\n\\(w\\) is the weight which is the proportion choice probability for draw, \\(r\\)\n\nA decision maker must make many choices for the conditional mean coefficient to approach the true individual-level coefficient.\n\nA Monte Carlo simulation in the textbook for these lectures said even after 50 choices there was substantial difference between the two values. Although, in the era of big data, it is conceivable to see a person make hundreds of choices.\n\n\nFuture Choice Probabilities\n\nUse past choices to define a conditional distribution of coefficients for the decision-maker\nThen use this conditional distribution, instead of the unconditional distribution, to calculate the mixed logit choice probabilities.\n\nNo closed form solution so must use simulation. See lecture video for the procedure, but it’s similar to the procedure of the conditional mean coefficient above.",
    "crumbs": [
      "Econometrics",
      "Discrete Choice Models"
    ]
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-reg-multin-ddcm",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-reg-multin-ddcm",
    "title": "Discrete Choice Models",
    "section": "Dynamic Discrete Choice Models",
    "text": "Dynamic Discrete Choice Models\n\nUnlike the previous static models , this model will explicitly represent how 1 choice affects future choice sets and therefore future utilities.\n\nStatic models assume that you make a choice based on the utility of that choice alone at that current time and not on some future utility associated with that first choice.\n\nUtility\n\nTwo Examples for “Go to college or get a job”\n\nTotal Utility for:\n\nTwo Periods: The utility gained during 4 years of college or work + the utility gain by choosing a job from a job set that depends on the completion of college or working for 4yrs\nThree Periods: Same as two but add in utility gained after choosing a retirement plan from a set of plans that were determined by the job you chose in period 2.\n\nDecision maker attends college \\(\\; iff \\;\\; TU_C &gt; TU_W\\)\n2 Period Setting: During College/Work and Available Jobs After College/Work\n\\[\n\\begin {align}\nTU_C &= U_{1C} + \\lambda \\max_j (U_{2j}^C) \\\\\nTU_W &= U_{1W} + \\lambda \\max_j (U_{2j}^W)\n\\end {align}\n\\]\n\nCollege (\\(C\\)) or Work (\\(W\\)) for four years\n\\(U_1C\\) - Utility in period 1 from four years of college\n\\[\nU_{1C} = V_{1C} + \\epsilon_{1C}\n\\]\n\\(U_1W\\) - Utility in period 1 from four years of working (Similar equation to \\(U_{1C}\\) )\n\\(U_{2j}^C\\) - Utility in period 2 from job \\(j\\) after attending college\n\\[\nU_{2j}^C = V_{2j}^C + \\epsilon_{2j}^C\n\\]\n\\(U_{2j}^W\\) - Utility in period 2 from job \\(j\\) after working (Similar equation to \\(U_{2j}^C\\) )\n\\(\\max\\) says the person will take whichever job that gives the most utility\n\nJob choice set is different for decision makers who went to college than those that chose to work in the 1st period (even though both indexed by js).\n\n\\(\\lambda\\) reflects relative weighting of the two periods. Factor that\n“discounts” a future utility. Typically people assign more utility for current things than things in the future, so in that case, this factor will lessen utility.\n\n3 Period Setting: Before College, After College, and Start of Retirement\n\\[\n\\begin {align}\nTU_C &= U_{1C} + \\lambda \\max_j [U_{2j}^C + \\theta \\max_s (U_{3s}^{Cj})] \\\\\nTU_W &= U_{1W} + \\lambda \\max_j [U_{2j}^W + \\theta \\max_s (U_{3s}^{Wj})]\n\\end {align}\n\\]\n\n\\(J\\) possible jobs for a career over many future years\n\\(S\\) possible retirement plans\n\n\\(U_{3s}^{Cj}\\) - Utility in period 3 from retirement plan \\(s\\) after attending college in period 1 and working job \\(j\\) in period 2\n\nSimilar utility model equations as \\(U_{1C}\\) and \\(U_{2j}^C\\)\n\n\\(U_{3s}^{Wj}\\) - Utility in period 3 from retirement plan \\(s\\) after working in period 1 and working job \\(j\\) in period 2\n\n\\(\\max\\) is maximizing both job choice (period 2) and its associated retirement plan (period 3)\n\\(\\theta\\) - Reflects relative weighting of three periods. Discount factor that’s the similar to \\(\\lambda\\).\n\n\n\nNotation\n\n\\(\\{i_1, i_2, \\ldots, i_t\\}\\) - Sequence of choices up to and including period \\(t\\)\n\\(U_{tj}(i_1, i_2, \\ldots, i_{t-1})\\) - Utility obtained in period \\(t\\) from alternative \\(j\\), which depends on all previous choices\n\\(TU_{tj}(i_1, i_2, \\ldots, i_{t-1})\\) - Total Utility (current and all future time periods) obtained from choosing alternative \\(j\\) in period \\(t\\), assuming the optimal choice is made in all future periods. (aka Conditional Value Function) (i.e. conditional on the alternative)\n\nAll possible values of \\(TU_{tj}(i_1, i_2, \\ldots, i_{t-1})\\) need to be calculated in order to express the optimal choice in each time period\n\n\\(TU_t(i_1, i_2, \\ldots, i_{t-1})\\) - Total Utility obtained from the optimal choice in period \\(t\\), assuming the optimal choice is made in all future periods (aka Value Function or Valuation Function at time \\(t\\) )\n\n\\(TU_t(i_1, i_2, \\ldots, i_{t-1}) = \\max_j TU_{tj}(i_1, i_2, \\ldots, i_{t-1})\\)\n\n\nBellman Equation\n\nIncorporates Discounted Utility (See Terms) into the Value Functions\n\nGeneralization of the 2 “college or work” examples above and gives a procedure for how one would go about calculating the utilities for each period.\n\nValue Functions\n\nConditional Value Function\n\\[\nTU_{tj}(i_1, \\ldots, i_{t-1}) = U_{tj}(i_1, \\ldots, i_{t-1}) + \\delta\\max_k[TU_{t+1,k}(i_1, \\ldots, i_t = j)]\n\\]\n\nRegarding \\(TU_{t+1}\\), since \\((i_1, \\ldots, i_{t-1})\\) says “given the sequence of choices prior to time, \\(t\\),” the appending of \\(i_t = j\\) is saying “given all the sequence of choices prior to time \\(t\\) and includeing the current period, \\(t\\), where choice \\(j\\) is made.”\nNot sure what \\(k\\) is and he didn’t say in the video.\n\nValue Function\n\\[\nTU_{t}(i_1, \\ldots, i_{t-1}) = \\max_j[U_{tj}(i_1, \\ldots, i_{t-1}) + \\delta\\;TU_{t+1}(i_1, \\ldots, i_t = j)]\n\\]\n\nSays that the total utility for time period \\(t\\) i\n\\(\\delta\\) - discount rate on utility in future periods (See Terms)\n\n\nProcedure: Work backwards from the last period to the first period to calculate the overall Total Utility. At each period (except the last period), you are using the utility calculated for the future period directly after it.\nAssumptions\n\nPerfect information about:\n\nUtility of each alternative in each future time period\nHow every possible sequence of choices affects this future utility\n\n\nWith J alternatives in each of T time periods, you have to calculate \\(J^T \\times T\\) utilities.\n\nTherefore, only model only broad time periods (e.g. [college, graduation, retirement]) in order to reduce the computational burden of calculating all these utilities.",
    "crumbs": [
      "Econometrics",
      "Discrete Choice Models"
    ]
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-endo",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-endo",
    "title": "Discrete Choice Models",
    "section": "Endogeneity",
    "text": "Endogeneity\n\nNeed exogenous variation in our explanatory variables in order to give our parameter estimates a “causal” inerpretation\n\nIf data are endogenous, our parameters can be interpreted as a kind of correlation between the data and choices\nTrue exogenous variable are tough to come by\n\nExamples of Endogeneity\n\nHousing choice and commute choice are correlated\n\nPeople who like public transit live closer to transit stations which makes their travel time lower\nTherefore, the coefficient on travel time will be biased upwards\n\nPrice and unobserved quality are correlated\n\nProducts with higher unobserved quality cost more and are preferrred by customers\nTherefore, the coefficient on price with be biased downward and may even have the wrong sign\n\nPrice and unobserved marketing are correlated\n\nLarge marketing campaigns may be accompanied by sales or increased prices (increased demand \\(\\rightarrow\\) decreased supply \\(\\rightarrow\\) increased price)\nTherefore, the coefficient on price will be biased, but the direction is uncertain.\n\n\nSolutions\n\nBLP estimation\nControl Function estimation\n\nWhen to use Control Functions instead of BLP\n\nIf any market shares are zero\n\nConstant terms in BLP are not identified for zero market shares\n\nIf you need to control for individual-specific endogeneity rather than market-level endogeneity (not feasible in BLP)\nIf you don’t want to use the Contraction Mapping algorithm.\n\nMore complex to code, computationally intensive\n\n\n\nBoth methods use instruments\n\nGood Instruments:\n\nCorrelated with explanatory variables\nExogenous (i.e. uncorrelated with residuals aka random utility aka unobserved utility)\n\nInstruments are very context specific. So in order to find good instruments for your context, you need to know the details about the process you’re trying to model.\n\n\n\n\nBLP (Berry-Levinsoln-Pakes)\n\nUses instruments to isolate exogenous variation in explanatory variables\nMixed logit model of demand for a differentiated product using market-level data.\n\ni.e. Estimate how the attributes of a product affect consumer demand\n\nPrice is one of the most important attributes, but is almost certainly correlated with other unobserved attributes\nModel includes instruments in a nonlinear model\nUtility Model\n\nIf consumers also get utility from unobserved attributes, then Price is correlated with the composite error term (\\(\\xi_{jm} + \\epsilon_{njm}\\))\nTherefore observed utility, \\(V\\), gets separated into 2 componets, Consumer and Market.\n\\[\n\\begin {align}\nU_{njm} &= \\delta_{jm} + \\tilde V (p_{jm}, \\boldsymbol{x}_{jm}, \\boldsymbol{s}_n, \\boldsymbol{\\tilde \\beta}_n) + \\epsilon_{njm} \\\\\n\\mbox where \\;\\; \\delta_{njm} &= \\bar V (p_{jm}, \\boldsymbol{x}_{jm}, \\boldsymbol{\\bar \\beta}_n) + \\xi_{jm} = \\bar{\\hat{\\boldsymbol{\\beta}}}(p_{jm}, \\boldsymbol{x}_{jm}) + \\xi_{jm}\n\\end {align}\n\\]\n\n\\(\\tilde V (p_{jm}, \\boldsymbol{x}_{jm}, \\boldsymbol{s}_n, \\boldsymbol{\\tilde \\beta}_n)\\) - Component that varies by consumer\n\\(\\bar V (p_{jm}, \\boldsymbol{x}_{jm}, \\boldsymbol{\\bar \\beta}_n)\\) - Component that varies by products and markets\n\\(\\delta_{jm}\\) - Effectively becomes a product-market constant term (i.e. intercept that varies by product and market for a Mixed Logit model) that represents the average utility obtained by product \\(j\\) in market \\(m\\)\n\nThis term gets estimated in the top equation. Then, this \\(\\hat{\\delta}\\) is used as the LHS to estimate \\(\\bar{\\hat{\\boldsymbol{\\beta}}}\\)\n\n\\(p_{jm}\\) - Price of product \\(j\\) in market \\(m\\)\n\n1 product can be the “outside product” or purchase nothing category (See Terms) which could be a vector of 0s\n\n\\(\\boldsymbol{x}_{jm}\\) - Vector of non-price attribtuies of product \\(j\\) in market \\(m\\)\n\\(\\boldsymbol{s}_n\\) - Vector of demographice characteristics of consumer \\(n\\)\n\\(\\boldsymbol{\\beta}_n\\) - Vector of coefficients for consumer \\(n\\)\n\\(\\xi_{jm}\\) - Utility (average) of unobserved attributes of product \\(j\\) in market \\(m\\)\n\\(\\epsilon_{njm}\\) - idiosyncratic (i.e. different per consumer) unobserved utility\nThe Choice Probability equation for this model looks similar to the Mixed Logit choice probability equation (See lecture video for the math)\n\n\nProcedure\n\nEstimate the average utility for product \\(j\\) in market \\(m\\), including observable and unobservable attributes (top equation)\nRegress this average utilitiy value, \\(\\hat{\\delta}\\), on price, \\(p_{jm}\\), and other observable attributes, \\(x_{jm}\\), while instrumenting for price. (i.e. IV model)\n\nIssue\n\n\\(\\hat{\\delta}_{jm}\\) must be estimated for each product in each market. This can result in 100s or even 1000s of terms to estimate.\n\nBLP insights\n\nA set of unique \\(\\delta_{jm}\\) terms equates predicted market shares with observed market shares for a given set of \\(\\boldsymbol{\\theta}\\) parameters\n\nHigher \\(\\delta_{jm}\\) means there will be higher market shares for product \\(j\\) in market \\(m\\)\n\nTheir “Contraction Mapping” algorithm effectively finds these unique \\(\\delta_{jm}\\) terms\n\nContraction Mapping Algorithm\n\nStart with initial value, \\(\\delta^0\\)\nPredict the market share for the current constant value, \\(\\hat{S}_{jm}(\\boldsymbol{\\delta}^s)\\), for each product and each market\n\n\\(\\hat{S}_{jm}(\\boldsymbol{\\delta})\\) is the predicted market share (which is a function of delta)\n\nI think these should be the predicted values for the Utility model above except the choice probabilities are now market shares since we’re predicting with market-level explanatory variables\n\nThe \\(s\\) in the \\(\\delta^s\\) term seems to be a counter for which iteration of this algorithm that you’re on.\n\nAdjust each product-market constant term by comparing predicted and observed market share\n\\[\n\\delta_{jm}^{s+1} = \\delta_{jm}^s + \\ln \\left(\\frac{S_{jm}}{\\hat{S}_{jm}(\\boldsymbol{\\delta}^s)}\\right)\n\\]\n\ni.e. We’re adjusting our \\(\\delta\\) by the difference of log predicted and log observed market shares\n\nRepeat steps 2 and 3 until the algorithm converges to the unique set of \\(\\hat\\delta\\) (product-market) constants\n\nSince this is a unique set, I think algorithm has converged with the observed market shares equal the predicted market shares exactly.\n\n\nOverall Procedure\n\nOuter-Loop: Search over \\(\\boldsymbol{\\theta}\\) to optimize the estimation objective function\n\nInner-Loop: Use contraction mapping to find \\(\\boldsymbol{\\delta(\\theta)}\\), the vector of product-market constant terms conditional on \\(\\theta\\)\nUse \\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{\\delta(\\theta)}\\) to simulate choice probabilities, \\(P_{njm}(\\boldsymbol{\\delta(\\theta)}, \\boldsymbol{\\theta})\\)\nUse the choice probabilities to calculate the estimation objective function\n\nEstimate \\(\\boldsymbol{\\bar\\beta}\\) by regressing \\(\\boldsymbol{\\delta_{jm}}\\) on \\((p_{jm}, \\boldsymbol{x}_{jm})\\) with price instruments, \\(\\boldsymbol{z}_{jm}\\)\n\nEstimator Options (See Week 13, video 5 lecture for details)\n\nMaximized Simulated Likelihood (MSL) for step 1 and Two-Stage Least Squares (2SLS) for step 2\nMSM - Some kind of method of moments algorithm\n\n\n\n\nControl Function\n\nUse instruments to control for endogeneity in explanatory variables\nUtility Model\n\\[\n\\begin {align}\nU_{nj} &= V(y_{nj}, \\boldsymbol{x}_{nj}, \\boldsymbol{\\beta}_n) + \\epsilon_{nj} \\\\\n\\\\\n\\mbox{where} \\;\\; y_{nj} &= W(\\boldsymbol{z}_{nj}, \\boldsymbol{\\gamma}) + \\mu_{nj} \\\\\n\\epsilon_{nj} &= CF(\\mu_{nj}, \\boldsymbol{\\lambda}) + \\tilde \\epsilon_{nj} \\\\\nCF(\\mu_{nj}, \\boldsymbol{\\lambda}) &= \\mathbb{E}[\\epsilon_{nj} \\;|\\; \\mu_{nj}]\n\\end {align}\n\\]\n\n\\(y_{nj}\\) - Endogenous explanatory variable (e.g price) for consumer \\(n\\) and product \\(j\\)\n\\(\\boldsymbol{x}_{nj}\\) - Vector of non-price attributes for consumer \\(n\\) and product \\(j\\)\n\\(\\boldsymbol{\\beta}_n\\) - Vector of coefficients for consumer \\(n\\)\n\nHas density, \\(f(\\boldsymbol{\\beta}_n,\\boldsymbol{\\theta})\\)\n\n\\(\\epsilon_{nj}\\) - Unobserved utility for consumer \\(n\\) and product \\(j\\) (i.e. residuals)\n\\(\\tilde \\epsilon_{nj}\\) - The leftover unobserved utility that is not correlated with \\(\\mu_{nj}\\)\n\nHas the conditional density \\(g(\\tilde \\epsilon_n, \\boldsymbol{\\mu_n})\\)\nThis term controls for the source of endogeneity and therefore \\(y_{nj}\\) is no longer endogenous.\n\n\\(\\boldsymbol{z}_{nj}\\) - Vector of exogenous instruments for \\(y_{nj}\\)\n\\(\\boldsymbol{\\gamma}\\) - Parameters that relate \\(y_{nj}\\) to \\(\\boldsymbol{z}_{nj}\\)\n\\(\\mu_{nj}\\) - Unobserved factors that affect \\(y_{nj}\\) (i.e. residuals)\n\\(CF(\\mu_{nj}, \\boldsymbol{\\lambda})\\) - Control Function that contains all the endogenous variation between \\(\\epsilon_{nj}\\) and \\(\\mu_{nj}\\)\n\nTypically modeled as linear, \\(CF(\\mu_{nj}, \\boldsymbol{\\lambda}) = \\lambda \\mu_{nj}\\), but it’s important the this term is specified correctly else \\(y_{nj}\\) will remain endogenous.\n\n\nAssumptions\n\n\\(\\epsilon_{nj}\\) and \\(\\mu_{n}\\) are correlated. Therefore,\n\n\\(y_{nj}\\) and \\(\\epsilon_{nj}\\), so \\(y_{nj}\\) is endogenous\n\\(\\mathbb{E}[\\epsilon_{nj} \\;|\\; \\mu_{nj}] \\neq 0\\) hence the use of the control function\n\n\\(\\epsilon_{nj}\\) and \\(\\mu_{n}\\) are independent of \\(\\boldsymbol{z}_{nj}\\). Therefore,\n\n\\(\\boldsymbol{z}_{nj}\\) are good instruments for \\(y_{nj}\\)\n\n\nProcedure\n\nEstimate \\(\\hat\\mu_{nj}\\) by regressing \\(y_{nj}\\) on \\(\\boldsymbol{z}_{nj}\\)\n\nTherefore,\\(\\hat\\mu_{nj}\\) will be the residuals of this regression\n\nEstimate \\((\\boldsymbol{\\hat\\theta}, \\boldsymbol{\\hat\\lambda})\\) by Maximized Simulated Likelihood (MSL) using the simulated choice probabilities to construct a simulated log-likelihood function",
    "crumbs": [
      "Econometrics",
      "Discrete Choice Models"
    ]
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#multinomial-probit",
    "href": "qmd/econometrics-discrete-choice-models.html#multinomial-probit",
    "title": "Discrete Choice Models",
    "section": "Multinomial Probit",
    "text": "Multinomial Probit\n\nAssumes a Normal distribution of errors which can deal with heteroscedasticity and correlation of the errors.",
    "crumbs": [
      "Econometrics",
      "Discrete Choice Models"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html",
    "href": "qmd/econometrics-fixed-effects.html",
    "title": "Fixed Effects",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-misc",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-misc",
    "title": "Fixed Effects",
    "section": "",
    "text": "Model with independent intercepts for each time point and/or case, which are called “fixed effects”\n\nThe effects the omitted variables have on the subject at one time, they will also have the same effect at a later time; hence their effects will be constant, or “fixed.”\nA “fixed effect” in statistics is a non-random regression term, while a “fixed effect” in econometrics means that the coefficients in a regression model are time-invariant\n\nNotes from\n\nhttps://www.econometrics-with-r.org/10-rwpd.html\nhttps://www.robertkubinec.com/post/fixed_effects/\n\nPackages\n\n{plm}\n\nFunctions for model estimation, testing, robust covariance matrix estimation, panel data manipulation and information.\n\n{fixest}\n\nFast estimation, has parallel option, glm option and many other features\n\n{estimatr}\n\nProviding a range of commonly-used linear estimators, designed for speed and for ease-of-use. Users can easily recover robust, cluster-robust, and other design appropriate estimates.\nUsers can choose an estimator to reflect cluster-randomized, block-randomized, and block-and-cluster-randomized designs.\n\n\nIf you used {plm} + {coeftest} and want stata errors, then vcov = vcovCL",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-terms",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-terms",
    "title": "Fixed Effects",
    "section": "Terms",
    "text": "Terms\n\nCoarse Clustering - Grouping the data into larger clusters or units. Each cluster represents a broader and more aggregated subset of observations (as compared to Fine Clustering).\n\nCan lead to lower variance in the estimated standard errors because it captures less of the within-cluster variation.\nMay be used when there is less within-cluster heteroscedasticity or correlation, or when computational efficiency is a concern.\n\nFine Clustering - Grouping the data into small clusters or units. Each cluster represents a relatively small and specific subset of observations in the dataset.\n\nCan lead to higher variance in the estimated standard errors because it captures more of the within-cluster variation.\nAppropriate when there is a substantial degree of heteroscedasticity or correlation within these small clusters.\n\nFixed Panel - When the same set of units/people/cases is tracked throughout the study\nHomogeneous (or Pooled) - Panel data models that assume the model parameters are common across individuals.\nHeterogeneous - Panel models allow for any or all of the model parameters to vary across individuals.\n\nFixed effects and random effects models are both examples of heterogeneous panel data models.\n\nRotating Panel - When the units/people/cases change during the study",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-consid",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-consid",
    "title": "Fixed Effects",
    "section": "Considerations",
    "text": "Considerations\n\nFixed Effects or Random Effects (aka mixed effects model)?\n\nIf there’s likely correlation between unobserved group/cases variables (e.g. individual talent) and treatment variable (i.e. E(α|x) != 0) AND there’s substantial variance between group units, then FE is a better choice (see 1-way assumptions or Econometrics, Mixed Effects, Frequentist &gt;&gt; Assumptions for more details)\nIf cases units change little, or not at all, across time, a fixed effects model may not work very well or even at all (SEs for a FE model will be large)\n\nThe FE model is for analyzing within-units variance\n\nDo we wish to estimate the effects of variables whose values do not change across time, or do we merely wish to control for them?\n\nFE: these effects aren’t estimated but adjusted for by explicitly including a separate intercept term for each individual (αi) in the regression equation\nRE: estimates these effects (might be biased if RE assumptions violated)\nThe RE model is for analyzing between-units variance\n\nThe amount of within-unit variation relative to between-unit variation has important implications for these two approaches\n\nArticle with simulated data showed that within variation around sd &lt; 0.5 didn’t detect the effect of explanatory variable but ymmv (depends on # of units, observations per unit, N)\n\nDurbin–Wu–Hausman test (plm::phtest)\n\nIf H0 is not rejected, then both FE and RE are consistent but only RE is efficient. –&gt; use RE but if you have a lot of data, then FE is also fine.\nIf H0 is rejected, then only FE is consistent –&gt; use FE\n\n\nValid research questions for using a fixed effect for:\n\nCases/Units (e.g. State, school, individuals, stores) - “How much does a case unit change relative to other case units?”\nTime (e.g. Year) - “How much does a case change in relation to itself over time?”\n\nHow much each case varies around its average. The larger this coefficient the more cases fluctuate in their outcomes\nExample: Survey data with individual incomes over time\n\nHow the measure is different in a particular year compared to the individual average (e.g., do they have a lower or higher income compared to their normal income).\n\n\nExamples\n\nWhether obtaining more education leads to higher earnings.\nWhether wealthier countries tend to be more democratic than poorer countries\n\n\nFixed Effects or First Difference Estimator (FD)?\n\nTaking the first difference is an alternative to the demeaning step in the FE model\nIf the error terms are homoskedastic with no serial correlation, the fixed effects estimator is more efficient than the first difference estimator.\nIf the error follows a random walk, however, the first difference estimator is more efficient. If T=2, then they are numerically equivalent, and for T &gt; 2, they are not.\n\nIs the panel data balanced?\n\nplm::is.pbalanced(&lt;data&gt;, index = c(\"&lt;id_var&gt;\", \"&lt;time_var&gt;\"))\nBalanced - Has the same number of observations for all groups/units at each time point\nUnbalanced - At least one group/unit is not observed every period\n\ne.g. Have missing values at some time observations for some of the groups/units.\n\nCertain panel data models are only valid for balanced datasets.\n\nFor such models, data will need to be condensed to include only the consecutive periods for which there are observations for all individuals in the cross section.\n\n\nOmitted variable bias\n\nMultiple regression can correct for observable omitted variable bias, however, this cannot account for omitted unobservable factors that differ (e.g. from state to state) \n\nThis refers to doing two multivariable regression models - one for each time period\n\nFE models control for any omitted variables that are constant over time but vary between individuals by explicitly including a separate intercept term for each individual (\\(\\alpha_i\\)) in the regression equation\nYou can difference the outcome and difference predictor variables from period 1 to period 2 in order to remove the effects of unobserved omitted variables that are constant between the time periods\nFrom Kubinec differs regarding omitted variables\n\nAny statistical model should have, as its first requirement, that it match the researcher’s question. Problems of omitted variables are important, but necessarily secondary.\nFixed effects models do not control for omitted variables. What fixed effect models do is isolate one dimension of variance in the model. As a result, any variables that don’t vary on that dimension are by definition removed from the model. This side-effect is trumpeted as the great inferential benefit of fixed effect models, but it has nothing to do with inference. Fixed effects (or their cousin, random effects/hierarchical models) are simply about selecting which part of the panel dataset is most germane to the analysis.",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-pit",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-pit",
    "title": "Fixed Effects",
    "section": "Pitfalls",
    "text": "Pitfalls\n\nBickel: If you performed matching on your sample, don’t condition on any of the matching variables\n\nCan result in collider bias and opening up a previously closed backdoor\nMe: Matching makes sense because FE model has “Common Trends” assumption\n\nKubinec says\n\n2-way fixed models have big problems\n\nSlope Interpretation\n\nCases and time points are nested and we end up making comparisons across both dimensions simultaneously. There is no clear research question that matches this model.\nThe one known use of the model is for difference-in-difference estimation, but only with two time points. Says to read his paper for more details.\n\nIs this what the eor book is describing for unobserved omitted variables? (see above)\n\n\nSlope Value Unreliable\n\nOnly identifiable if there’s a different effect of x on y for each time point/case\n\nI think he’s saying if there is no variation in one of your fixed effects and you fit a two-way model anyways, the calculated effect is unreliable. He says the data looks normal and you wouldn’t recognize what happened necessarily.\n\nWhen this model is unidentifiable, R fixes the problem by deleting the last dummy variable (created by factor(fixed_effect_var)) and spits out the estimate.\n\nThe coefficient estimate for the removed dummy variable shows-up as an NA in the summary\n\n\n\nIt’s best to choose whether within-case or between-case effect is more important and fit the 1-way model.\n\ni.e. It is important to think about which dimension is more relevant, and then go with that dimension.\nAssumptions for a model with just an cases fixed effect\n\nResiduals have mean = 0 (i.e. errors uncorrelated with X)\n\nif violated, then omitted variable bias\n\nX (variable of interest) is i.i.d\n\nwithin-cases, autocorrelation is allowed (e.g. states)\n\nlarge outliers unlikely\nno perfect multicollinearity between variables\n\n\n\nPotential danger of biased effects when treatment is assigned during different periods for each group\n\nExample: group 1 is untreated at periods 1 and 2 and treated at period 3, while group 2 is untreated at period 1 and treated both at periods 2 and 3\nWhen the treatment effect is constant across groups and over time, FE regressions estimate that effect under the standard “common trends” assumption.\n\nRequires that the expectation of the outcome without treatment follow the same evolution over time in every group\n\nEstimates can be severely biased – and may even be incorrectly signed – when treatment effects change over time within treated units (aka hetergeneous treatment effects)\nFundamentally, the main reason TWFE estimates get weird and biased with differently-timed treatments is because of issues with weights—in TWFE settings, treated observations often get negative weights and vice versa\nAccording to Jakiela (2021, 5), negative weights in treated observations are more likely in (1) early adopter countries, since the country-level treatment mean is high, and (2) later years, since the year-level treatment mean is higher.\n\n\nSo, in general, the bias comes from entity variable categories that received the treatment early and the biased weight estimates occur on observations with later time values. This is because of the extreme treatment imbalance during these ranges/intervals, and its effect on the outcome variable.\n\nHaving negative weights on treated observations isn’t necessarily bad! It’s often just a mathematical artefact, and if you have (1) enough never-treated observations and (2) enough pre-treatment data, and if (3) the treatment effects are homogenous across all countries, it won’t be a problem. But if you don’t have enough data, your results will be biased and distorted for later years and for early adopters.\nDiagnostics\n\nDo any treated units get negative weight when calculating βTWFE? Check this by looking at the weights\nCan we reject the hypothesis that the treatment effects are homogenous? Check this by looking at the relationship between Yit and Dit. The slope shouldn’t be different.\n\ntreatment effect homogeneity implies a linear relationship between residualized outcomes and residualized treatment after removing the fixed effects\n\n\nComments\n\nShe states that she’s only looking for linearity between the two sets of residuals, but actually breaks it down further by checking whether the relationship varies by treatment. This whole procedure is computing a partial correlation except instead of the last step of measuring the correlation between the two sets of residuals (e.g. cor.test(treatment_resid, out_resid) and getting the p-value, she looks at an interaction.\nI don’t understand the homogeneity check in 3.2 though. She says that if the linearity relationship varies by treatment then this breaks assumptions for TWFE models. I’ve only looked at her paper and the Chaisemartin paper, and the only assumptions I saw for TWFE models in general was the “common trends” and the “strong exogeneity” assumption. I think this is more likely to be about the “common trends” assumption, and my understanding of that one is that it pertains to the effect across time for a particular group. I’m guessing there’s a connection between those two concepts, but I’m not seeing it.",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-clus",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-clus",
    "title": "Fixed Effects",
    "section": "Clusters",
    "text": "Clusters\n\nMisc\n\nNotes from Cluster-robust inference: A guide to empirical practice (Paper)\n\nSections: 4.2 (level of clustering), 4.3 (level of clustering), 4.3.2 (influential clusters), 8.1 (infl clusters), 8.2 (placebo regression)\n\nAlso see Econometrics, General &gt;&gt; Standard Errors &gt;&gt; HC and HAC vcov estimators\n\nCluster-Robust Variance Estimators (CRVE)\n\n“Random-effects model is the only model within the class of factor models for which including cluster fixed effects can remove all intra-cluster dependence”\n\nThink this says that HC or HAC ({sandwich}) should be used for 2FE but not RE models\n\n“Even very small intra-cluster correlations can have a large effect on standard errors when the clusters are large”\n“It has become quite standard in modern empirical practice both to include cluster fixed effects (and perhaps other fixed effects as well) and also to employ cluster-robust inference.”\n\nLevel of Clustering\n\n“one or more fine clusters nested within each of the coarse clusters”\n“Clustering at too fine a level generally leads to serious over-rejection, which becomes worse as the sample size increases with the numbers of clusters at all levels held constant”\n“Clustering at too coarse a level also leads to both some over-rejection and some loss of power, especially when the number of clusters is small.”\nIssues for Certain Rules of Thumb\n\nJust cluster at the coarsest feasible level\n\nMay be attractive when the number of coarse clusters G is reasonably large, but it can be dangerous when G is small, or when the clusters are heterogeneous in size or other features\n\nCluster at whatever level yields the largest standard error(s) for the coefficient(s) of interest\n\nWill often lead to the same outcome as the first one, but not always. When the number of clusters, G, is small, cluster-robust standard errors tend to be too small, sometimes much too small. Hence, the second rule of thumb is considerably less likely to lead to severe over-rejection than the first one. However, because it is conservative, it can lead to loss of power (or, equivalently, confidence intervals that are unnecessarily long).\n\n\nRecommended: Cluster at the treatment level\n\ne.g. If the treatment is assigned by classroom then cluster by classroom\nBut if there’s concern of significant spillover effects, then cluster at a coarser level than the treatment level (e.g. schools)\n\n\nDiagnostics\n\nStatistical testing for the correct level of clustering\n\nHard to tell but I don’t think any of the tests were recommended in the paper\n\nChecking for influential clusters\n\nInfluential Cluster - Estimates change a lot when it’s deleted.\n“In a few extreme cases, there may be a cluster \\(h\\) for which it is impossible to compute \\(β_j^{(h)}\\). If so, then the original estimates should probably not be believed.\n\nThis will happen, for example, when cluster \\(h\\) is the only treated one. Inference is extremely unreliable in that case.”\n\n\nPlacebo Regressions\n\nProcess\n\nAdd a random dummy variable to the model\nfit model check if dummy variable is significant\nrepeat many times\n\nBecause a placebo regressor is artificial, we would expect valid significance tests at level α to reject the null close to α% of the time when the experiment is repeated many times.\nExample:\n\nClustering at levels below state-level leads to rejection rates far greater than α\nUsing a state-level CRVE is important for survey data that samples individuals from multiple states. If we fail to do so, we will find, with probability much higher than α, that nonsense regressors apparently belong in the model.\n\ni.e. placebo regressors are significant &gt; 5% of the time\n\n\nA placebo-regressor experiment should lead to over-rejection whenever both the regressor and the residuals display intra-cluster correlation at a coarser level than the one at which the standard errors are clustered. (e.g. &lt; 5%)\nIf the placebo regressor is clustered at the coarse level, we would expect significance tests based on heteroskedasticity-robust standard errors to over-reject whenever the residuals are clustered at either level. Similarly, we would expect significance tests based on finely-clustered standard errors to over-reject whenever the residuals are clustered at the coarse level. Table 4 in Section 8.2 displays both of these phenomena.",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-owfe",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-owfe",
    "title": "Fixed Effects",
    "section": "One-Way Fixed Effects",
    "text": "One-Way Fixed Effects\n\nOnly compares different periods within the same cases category and discards the between-cases variance Steps\nSteps\n\nRemove endogeneity (resulting from omitted variable bias)\n\nFirst, the error is broken into 2 parts\n\\[\n\\begin{align}\n&y_{it} = \\beta x_{it}+ \\nu_{it} \\\\\n&\\text{where}\\: \\nu_{it} = \\alpha_i + \\epsilon_{it} = 0\n\\end{align}\n\\]\n\n\\(\\alpha\\) is the cases/units-specific or between part of the error\n\nunit-specific heterogeneity, the error component that is constant over time\nIt’s the unit fixed effect, the unit-specific intercept.\n\n\\(\\epsilon\\) is time-varying or within part of the error\n\nIdiosyncratic, varying both over units and over time\n\n\nThen, each group (aka cases) is centered by each group’s mean\n\\[\n\\begin{align}\ny_{it}-\\bar y_i &= \\beta(x_{it} - \\bar x_i) + (\\alpha_i - \\alpha_i) + (\\epsilon_{it} - \\bar \\epsilon_i) \\\\\n\\tilde y_{it} &= \\beta \\tilde x_{it} + \\tilde \\epsilon_{it}\n\\end{align}\n\\]\n\nThe centering eliminates all between-group variance, including the person-specific part of the error term (\\(\\alpha_i\\)), and leaves only the within-group variability to analyze\n\n\\(\\alpha_i\\) is a constant so it’s mean is equal to itself\n\n\n\nOLS is performed after the endogeneity is removed.\n\nAssumptions\n\nFunctional Form\n\nAdditive fixed effect\nConstant and contemporaneous treatment effect (aka homogeneous treatment effects)\nLinearity in covariates\n\nTime-constant unobserved heterogeneity is allowed (not the case for Mixed Effects models)\n\ni.e. \\(\\mathbb{E}(\\alpha|x) \\neq 0\\) or correlation between unobserved unit variables that are constant across time and \\(x\\) is allowed\n\nThis correlation is seen in the figure at the top of section\n\nEach group’s \\(x\\) values get larger from left to right as each group’s \\(\\alpha\\) (aka \\(y\\)-intercepts) for each unit get larger time-constant, unobserved variablesexplain variation between cases units\n\n\n\nStrong (strict) Exogeneity\n\n\\(\\mathbb{E}(\\epsilon|x,\\alpha)=0\\)\nTime-varying unobserved heterogeneity biases the estimator\nAlso see Pitfalls &gt;&gt; Kubinec\n\n\nExample\ne2 &lt;- plm(wage ~ marriage, data = df2,\n          index = c(\"id\", \"wave\"),\n          effect = \"individual\", model = \"within\")\n\nWhere marriage is the variable of interest, id is the cases variable and wave is the time variable\nUsing effect = “individual”, model = “within” specifies a one-way fixed effects model",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-twfe",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-twfe",
    "title": "Fixed Effects",
    "section": "Two-Way Fixed Effects (TWFE)",
    "text": "Two-Way Fixed Effects (TWFE)\n\nAdds a time variable that is constant (fixed effect) across cases but varies over time\n\\[\ny_{it} = \\beta x_{it} + \\alpha_i + \\zeta_t + \\epsilon_{it}\n\\]\n\nWhere \\(\\zeta\\) is the time fixed effect\n\nSteps\n\nRemove endogeneity (resulting from omitted variable bias)\n\nFirst, the error is broken into 2 parts: \\(\\nu_{it} = \\alpha_i + \\epsilon_{it} = 0\\)\n\nWhere \\(\\alpha\\) is the cases-specific or between part of the error and \\(\\epsilon\\) is time-varying or within part of the error\n\nThen,\n\\[\n(y_{it} - \\bar y_i -\\bar y_t + \\bar y) = \\beta(x_{it} - \\bar x_i - \\bar x_t + \\bar x) +(\\epsilon_{it} = \\bar \\epsilon_i - \\bar \\epsilon_t + \\bar \\epsilon)\n\\]\n\nFor each group/case, variables are centered by that group’s mean\nFor each period, variables are centered by that time period’s mean\nThe grand mean is added back\n\n\nOLS is performed after the endogeneity is removed.\n\nAssumptions\n\nTime-constant unobserved heterogeneity is allowed (See 1-way FE assumptions)\nFunctional Form\n\nAdditive fixed effect\nConstant and contemporaneous treatment effect\nLinearity in covariates\n\nStrong (strict) Exogeneity (also see 1-way FE assumptions)\n\n\\(ε \\perp D_{is}, X_{i}, \\alpha_i, \\zeta_t\\)\n\nThis implies the below statement\n\nTreatment assignment, \\(D_i\\), for a given unit, \\(i\\), in time, \\(s\\), is independent of the potential outcomes for that unit in that time period\n\\[\n{Y_{it}(0), Y_{it}(1)} \\perp D_{is}\\;|\\; \\boldsymbol{X}_i^{1:T}, \\alpha_i, \\boldsymbol{f}^{1:T} \\quad \\quad \\forall\\; i, t, s\n\\]\n\ne.g A policy (i.e. treatment) doesn’t get enacted in region because it experiences negative economic shocks and we’re measuring some economic metric\nAs a result, if we only had observed outcomes (which of course is all we have), we can substitute either \\(Y_{it}(0)\\) or \\(Y_{it}(1)\\) depending on whether we observe \\(D_{is}= 1\\) or \\(D_{is}= 0\\) and we can still, at least theoretically, get an unbiased estimate of the treatment effect.\n\n\\(D\\) is the treatment variable so it’s \\(x_{it}\\) in the other equations above and here, \\(X\\) is probably other adjustment variables\n\\(f\\) is the time fixed effect\n\n\nImplies treatment status is assigned randomly or at one shot, not sequentially\n\n\nCommon Trends\n\nSee Fixed Effects with Individual Slopes (FEIS) section for models that relax this assumption\nFor \\(t \\geq 2, \\mathbb{E}(Y_{g,t}(0) − Y_{g,t−1}(0))\\) does not vary across group, \\(g\\)\n\n\\(Y_{g,t}(0)\\) denotes average potential outcomes without treatment in group \\(g\\) at period \\(t\\).\n\\(Y_{g,t}(1)\\) would denote average potential outcomes with treatment in group \\(g\\) at period \\(t\\).\ni.e. For each period after the first period, the expected change in outcome doesn’t vary across group \\(g\\)\n\nExample\n\n\nBefore treatment (getting married), wages for the treatment group (top 2 lines) were growing at a substantially faster rate than the control group (bottom two lines). This violates the Common Trends assumption\n\n\n\nExample:\nfe3 &lt;- \n  plm(wage ~ marriage, data = df2,\n      index = c(\"id\", \"wave\"),\n      effect = \"twoways\", \n      model = \"within\")\n\nWhere marriage is the variable of interest, id is the cases variable and wave is the time variable\nUsing effect = “twoways”, model = “within” specifies a two-way effects model\n\nExample:\n\nModel\n\\[\n\\begin{align}\nY_{it} &= \\beta_0 + \\beta_1 X_{it} + \\gamma_2 D2_i + \\cdots + \\gamma_n DT_i + \\delta_2B2_t + \\cdots + \\delta_n BT_t + u_{it} \\\\\n\\text{FatilityRate}_{it} &= \\beta_1 \\text{BeerTax}_{it} + \\text{StateEffects} + \\text{TimeFixedEffect} + u_{it}\n\\end{align}\n\\]\n\nIncluding the intercept would allow for a change in the mean fatality rate in the time between the years 1982 and 1988 in the absence of a change in the beer tax.\nThe variable of interest is Beer Tax and it’s effect on Fatality Rate\n\nBeer Tax is a continuous variable with a value for each unit and for each year\n\nThe state and time fixed effects are the dummy variables in the formal equation\nTheir coefficients start at 2 because the intercept coefficient is considered the first coefficient\n\nCode\n# Two ways to fit the model\n\nlm(fatal_rate ~ beertax + state + year - 1, data = Fatalities)\n\nfatal_tefe_mod &lt;- \n  plm::plm(fatal_rate ~ beertax,\n           data = Fatalities,\n           index = c(\"state\", \"year\"),\n           # fixed effects estimator is also called the 'within' estimator\n           model = \"within\",\n           effect = \"twoways\") # twoways required for \"entities\" and \"time\" fixed effects\n\n# only calcs for variable of interest\n# if needed, dof = nrow(dat) - 1\ncoeftest(fatal_tefe_mod, vcov = vcovHC, type = \"HC1\")\n#&gt; t test of coefficients:\n#&gt; \n#&gt;        Estimate Std. Error t value Pr(&gt;|t|) \n#&gt; beertax -0.63998    0.35015 -1.8277  0.06865 .\n\n# moar adjustment vars\nfatalities_mod6 &lt;- \n  plm::plm(fatal_rate ~ beertax + year + drinkage\n                        + punish + miles + unemp + log(income),\n           index = c(\"state\", \"year\"),\n           model = \"within\",\n           effect = \"twoways\",\n           data = Fatalities)\n\nstate and year variables need to be factors\nIntercept removed because it has no meaning in this context\n\n\nExample: {estimatr}\nmodel_lm_robust &lt;- \n    estimatr::lm_robust(primary ~ treatment,\n                        fixed_effects = ~ country + year,\n                        data = fpe_primary,\n                        clusters = country, se_type = \"stata\")\n\ntidy(model_lm_robust)\n##        term estimate std.error statistic p.value conf.low conf.high df outcome\n## 1 treatment    20.4      9.12      2.24  0.0418    0.867        40 14 primary\n\nglance(model_lm_robust)\n##  r.squared adj.r.squared statistic p.value df.residual nobs se_type\n## 1    0.768        0.742        NA      NA          14  490  stata\nExample: {fixest}\nmodel_feols &lt;- \n  fixest::feols(primary ~ treatment | country + year,\n                data = fpe_primary,\n                cluster = ~ country,\n                dof = dof(fixef.K = \"full\"))\n\ntidy(model_feols)\n## # A tibble: 1 × 5\n##  term      estimate std.error statistic p.value\n##  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n## 1 treatment    20.4      9.12      2.24  0.0418\n\nglance(model_feols)\n## # A tibble: 1 × 9\n##  r.squared adj.r.squared within.r.squared pseudo.r.squared sigma  nobs  AIC  BIC logLik\n##      &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1    0.768        0.742            0.111              NA  14.7  490 4071. 4280. -1985.\n\n# Standard print,summary output from a fixest model (from vignette)\nprint(fixest_pois_mod)\n#&gt; Poisson estimation, Dep. Var.: Euros\n#&gt; Observations: 38,325 \n#&gt; Fixed-effects: Origin: 15,  Destination: 15,  Product: 20,  Year: 10\n#&gt; Standard-errors: Clustered (Origin) \n#&gt;              Estimate Std. Error t value  Pr(&gt;|t|)   \n#&gt; log(dist_km) -1.52787  0.115678 -13.208 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; Log-Likelihood: -7.025e+11  Adj. Pseudo R2: 0.764032\n#&gt;            BIC:  1.405e+12    Squared Cor.: 0.612021\n\n# With clustered SEs\nsummary(fixest_pois_mod, vcov = \"twoway\")\n#&gt; Poisson estimation, Dep. Var.: Euros\n#&gt; Observations: 38,325 \n#&gt; Fixed-effects: Origin: 15,  Destination: 15,  Product: 20,  Year: 10\n#&gt; Standard-errors: Clustered (Origin & Destination) \n#&gt;              Estimate Std. Error  t value  Pr(&gt;|t|)   \n#&gt; log(dist_km) -1.52787  0.130734 -11.6869 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; Log-Likelihood: -7.025e+11  Adj. Pseudo R2: 0.764032\n#&gt;            BIC:  1.405e+12    Squared Cor.: 0.612021",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-feis",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-feis",
    "title": "Fixed Effects",
    "section": "Fixed Effects with Individual Slopes (FEIS)",
    "text": "Fixed Effects with Individual Slopes (FEIS)\n\nFixed effects model that relaxes the Common Trends assumption (see 2-way FE assumptions above)\n\nGives each case (e.g. State, school, individual, store) it’s own intercept and slope\nData are not cases “demeaned” like with a FE estimator, but “detrended” by the predicted individual slope of each cases unit\n\nMisc\n\nNotes from https://ruettenauer.github.io/Panel-Data-Analysis/Panel_part2.html#Fixed_Effects_Individual_Slopes\n{feisr}\n** Each additional slope variable requires more observations per cases category **\n\nEach cases unit needs at least q+1 observations to contribute to the model. If not, they are dropped.\n\nWhere q number of slope parameters (including a constant)\n\nMost likely this refers to the number of slope variables + constant\nExample: Slope variables are exp + I(exp^2)\n\nq = number_of_slope_vars + constant = 2 + 1 = 3 observations for each unit are required.\n\n\n(Probably not this) Example (Based on the feisr vignette): Slope variables are exp + I(exp^2)\n\nq = number_of_cases_units * (number_of_slope_vars + constant)\nq = number_of_ids * 3\nThis is the actual number of slope parameters estimated but this could be huge, so I doubt it’s this.\n\n\n\n\nModel Equation: \\(y_i = \\beta X_i + \\alpha_i W_i + \\epsilon_i\\)\n\n\\(W\\) is a matrix of slope variables\n\\(\\alpha\\) is a vector of estimated parameters for the slope variables\n\nProcess\n\nIt’s equivalent to a typical lm model except with dummies of your cases variable (e.g. “id” below) and 2-way interaction terms for all combinations of dummies \\(\\times\\) each slope variable\nActual process (more efficient) (see article for more mathematical detail)\n\nEstimate the individual-specific predicted values for the dependent variable and each covariate based on an individual intercept and the additional slope variables of \\(W_i\\)\nDetrend the original data by these individual-specific predicted values\nRun an OLS model on the residual (‘detrended’) data\n\n\nExample: Does marrying increase (log) wages\nwages.feis &lt;- \n  feis(lnw ~ marry + enrol + yeduc + as.factor(yeargr)\n       | exp + I(exp^2), \n       data = mwp, \n       id = \"id\",\n       robust = TRUE)\n\nsummary(wages.feis)\n## Coefficients:\n##                      Estimate Std. Error t-value  Pr(&gt;|t|)   \n## marry              0.0134582  0.0292771  0.4597  0.64579   \n## enrol              -0.1181725  0.0235003 -5.0286 5.325e-07 ***\n## yeduc              -0.0020607  0.0175059 -0.1177  0.90630   \n## as.factor(yeargr)2 -0.0464504  0.0378675 -1.2267  0.22008   \n## as.factor(yeargr)3 -0.0189333  0.0524265 -0.3611  0.71803   \n## as.factor(yeargr)4 -0.1361305  0.0615033 -2.2134  0.02697 * \n## as.factor(yeargr)5 -0.1868589  0.0742904 -2.5152  0.01196 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Cluster robust standard errors\n## Slope parameters:  exp, I(exp^2)\n\nExperience (exp) is used for the slope variables.\nTo estimate the slope parameters, the relationship with wage (lnw) is assumed to be non-linear (exp + I(exp^2))\nInterpretation: Marrying doesn’t reliably affect wages (p-value = 0.64579)",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-general.html",
    "href": "qmd/econometrics-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Econometrics",
      "General"
    ]
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-misc",
    "href": "qmd/econometrics-general.html#sec-econ-gen-misc",
    "title": "General",
    "section": "",
    "text": "Packages\n\nCRAN Task View\n\nATE or LATE?\n\nIf proposed policy is to give everyone the treatment, then ATE\nIf proposed policy only affects a subset, then maybe LATE is more appropriate",
    "crumbs": [
      "Econometrics",
      "General"
    ]
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-terms",
    "href": "qmd/econometrics-general.html#sec-econ-gen-terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nDisturbances - the error term in an econometric models (aka residuals in a regression)\nEconomic Shock - refers to any change to fundamental macroeconomic variables or relationships that has a substantial effect on macroeconomic outcomes and measures of economic performance,\n\nExamples: unemployment, consumption, and inflation.\n\nEndogenous Variable - variables that are correlated with the population error term. “Determined inside the model”.\n\nAn observed endogenous variable is affected by other variables in the system (it is roughly equivalent to a dependent variable in an experiment). It’s the variable that show differences we wish to explain.\nWhen the causality between X and Y goes both directions, both are endogenous.\nAlso see Glossary: DS terms\n\nExogenous Variable - Variables that are NOT correlated with the population error term. “Determined outside the model”\n\nAn observed exogenous variable is not controlled by other variables in the system (it is roughly equivalent to an independent variable in an experiment). It’s the variable used to explain the differences in the endogenous variable.\n\nLimited Dependent Variable (LDV) - a variable whose range of possible values is “restricted in some important way.”\n\ni.e. censoring, truncating, discrete\ne.g. probabilities, or is constrained to be positive, as in the case of wages or hours worked",
    "crumbs": [
      "Econometrics",
      "General"
    ]
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-se",
    "href": "qmd/econometrics-general.html#sec-econ-gen-se",
    "title": "General",
    "section": "Standard Errors",
    "text": "Standard Errors\n\nHeteroskedastic Robust Standard Errors\n\nDiscussed by Zeileis in his econometrics book and in White’s paper\nTypes\n\nH0 is the original\nH1 follows H0 but corrects for degrees of freedom. Only unbiased when experiment is balanced.\n\nBalance example: data for vehicle accidents has state and year variables. Balanced is each state has accidents data for each year\nExample\nlmtest::coeftest(model, vcov. = vcovHC, type = \"HC1\") # vcovHC is part of the sandwich package\n\nH2 is unbiased when errors are homeoskedastic\n\ni.e. Don’t just blindly use without checking for homeoskedastity\n\nH3 is derived from jackknife procedure\nAlso HC4, HC5, and modified HC4m\n\nHC4 corrects for high-leverage points\nCribari-Neto F., Da Silva W.B. (2011). “A New Heteroskedasticity-Consistent Covariance Matrix Estimator for the Linear Regression Model.” Advances in Statistical Analysis, 95(2), 129–146\n\n\nGuidelines\n\ntl;dr - Use HC3 (default method for vcovHC) for small to moderately sized data sets and jackknife, vcovBS(..., type = \"jackknife\") or vcovJK, for large datasets (HC3 computation will fail).\nIf sample size is small and heteroskedasticity present, then H0, H1, or H2 shouldn’t be used. H3 isn’t quite as reliable as regular OLS standard errors.\nIf heteroskedasticity present, then H3 is superior, otherwise H2 better than H1 which is better than H0\nNo real guidance on HC4, HC5, and modified HC4m. See paper (above)\n\n\n\n\nHeteroskedastic and Autocorrelation (HAC) Consistent\n\nDiscussed in Hanck book\nSimilar as for heteroskedasticity, autocorrelation invalidates the usual standard error formulas as well as heteroskedasticity-robust standard errors since these are derived under the assumption that there is no autocorrelation.\nR2 and F test not affected (Wald test preferred when heterskadasticity and autocorrelation present)\n\nnot sure where I got this. Wasn’t from the Hanck book\n\nNewey-West is suboptimal; the QS kernel is optimal\n\n2021 tweet  or paper\nuse sandwich::kernHAC(kernel=\"Quadratic Spectral\") for vcov arg in lmtest::coeftest()\n\nfyi Newey-West is a special case of these kernel estimators\n\nI don’t think these are for clustered data\n\n\n\n\nClustered Standard Errors\n\nBelong to HAC type of standard errors. They allow for heteroskedasticity and autocorrelated errors within an entity but not correlation across entities.\nFrom https://datascience.blog.wzb.eu/2021/05/18/clustered-standard-errors-with-r/\nIn ordinary least squares (OLS) regression, we assume that the regression model errors are independent. This is not the case here: Each subject may be surveyed several times so within each subject’s repeated measures, the errors will be correlated. Although that is not a problem for our regression estimates (they are still unbiased [Roberts 2013], it is a problem for for the precision of our estimates — the precision will typically be overestimated, i.e. the standard errors (SEs) will be lower than they should be [Cameron and Miller 2013]. The intuition behind this regarding our example is that within our clusters we usually have lower variance since the answers come from the same subject and are correlated. This lowers our estimates’ SEs.\nvcovCL() will give STATA clustered standard errors.\nvcovCL()may be biased downwards\n\nCircumstances where it may be biased downwards (i.e. CIs too small)\n\nImprecise calculations arise when there is a low number of clusters (e.g. classrooms, schools)\n\nless than 50 clusters\n\nMulti-way (i.e. more than 1 fixed effect in panel data)\nIf the cluster sizes are wildly different.\nIf the intra-cluster correlations varies across clusters.\n\nSolutions:\n\nvcovJK -Not downward biased and yield better coverage rates for confidence intervals compared to other “robust” covariance estimates\n\nBased on leave-one-out estimates of the coefficients/parameters of a model. This means that the model is reestimated after dropping each observational unit once, i.e., each individual observation in independent observations or each cluster in dependent data\nHC3 seems to be an estimate of the Jackknife. To obtain HC3 covariances that exactly match the jackknife covariances, the jackknife has to be centered with the full-sample estimates (arg center = “estimate”) and the right finite-sample adjustment (?) has to be selected for the HC3.\n\nSatterthwaite corrected cluster robust sandwich estimator (?)\nWild Cluster Bootstrap {clubSandwich} {fwildclusterboot}\n\nComputationally expensive\n\nfwildclusterboot is VERY fast though\n\n\nFor small cluster sizes, choose wild cluster bootstrap over Satterthwaite corrected cluster robust sandwich estimator when: (article)\n\nExtreme treatment proportions (e.g. 80% obs treated, 10% control)\nExtreme differences in cluster sizes (i.e. extreme imbalance)\n\n{ceser} - Cluster-estimated standard errors\n\nMore conservative than the CRSE method, sandwich::vcovCL\nLess sensitive to the number of clusters and to the heterogeneity of the clusters, which can be a problem for both CRSE and bootstrap methods\nAlso has heteroskedacity corrections: HC0, HC1, HC2, HC3, or HC4",
    "crumbs": [
      "Econometrics",
      "General"
    ]
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-iv",
    "href": "qmd/econometrics-general.html#sec-econ-gen-iv",
    "title": "General",
    "section": "Instrumental Variable (IV)",
    "text": "Instrumental Variable (IV)\n\nModel\n\\[\n\\begin{align}\nY_i &= \\beta_0 + \\beta_1 X_i + \\beta_2 W_i + u_i \\\\\nX_i &= \\pi_0 + \\pi_1 Z_i + \\pi_2 W_i + v_i \\quad \\text{where}\\; i = 1, \\ldots, n\n\\end{align}\n\\]\n\n\\(u\\) and \\(v\\) are error terms, \\(X\\) is an endogenous variable, \\(Z\\) is an exogenous, instrumental variable, \\(W\\) is another predictor of the outcome variable except it’s exogenous.\n\nMisc\n\nResources\n\nVideo series covering all the concepts\n\nIf X and u are correlated (endogenity) then OLS is inconsistent. So IV modeling uses the Z to isolate the part of X that isn’t correlated with u. Potential causes for this correlation between X and u are:\n\nUnobservable omitted variable(s)\n\nUsing an IV allows us to use part of X than isn’t associated with the omitted variable (i.e. confounder) but is still associated with Y\n\nMeasurement error\nsimultaneous causality\n\n\nTerms\n\nEndogenous Variable - Variables that are correlated with u, the population error term. “Determined inside the model”.\n\nWhen the causality between X and Y goes both directions, both are endogenous.\n\nExogenous Variable - Variables that are NOT correlated with u. “Determined outside the model”\n\nConditions for Valid Instruments\n\nInstrument Relevance: \\(Corr (X , Z) \\neq 0\\) (Predictive of X)\n\nChecks (1st stage)\n\nInstrument should have a significant p-val\nF-Test stat &gt; 10; t-test stat &gt; 3.16 (rules of thumb)\n\n\nInstrument Exogeneity: \\(Corr (Z, u) = 0\\), \\(Corr(Z, v) = 0\\)\n\nCheck: “balancing test (t-test); results should be insignificant” (?)\n\nWould’ve thought you could do some kind of check on the residuals\n\n\nExclusion restiction: no impact on the dependent variable directly. It only impacts the dependent variable through its impact on the treatment variable\n\nCheck\n\nCorrelation? or a Partial Correlation?\n\n\n\nGood Instruments\n\nMay not have a strong causal relationship with x and therefore overlooked in the subject matter literature. Domain “field work” into the data generating process can help identify new instruments.\nThe effect of the instrument on the population is somewhat random (Instruments perform a quasi-randomization)\n\ne.g. A policy that may or may not have an effect on a population should make it exogenous. Something outside the control of the individual that influences her likelihood of participating in a program, but is otherwise not associated with her characteristics.\n\nExamples\n\nOutcome: log(wage),  predictor: woman’s education, instrument: mother’s education\n\nThis might not follow condition #2. If the daughter’s “ability” is an omitted variable which is in the error term (u), and “mother’s education” are correlated, then #2 is violated.\n\nOutcome: #_of_kids (fertility), predictor: years_of_education, instrument: pre/post government policy that increases mandatory years of education\n\n\nSteps\n\nRegress X on Z where the π-terms and Z are the parts uncorrelated with u.\nDrop the error term, v, which is the part of X that’s correlated with u\nRegress the Y on the modified X to estimate the βs\n\\[\n\\begin{align}\nY_i &= \\beta_0 + \\beta_1 \\tilde X_i + \\beta_2 W_i + u_i \\\\\n\\tilde X_i &= \\pi_0 + \\pi_1 Z_i + \\pi_2 W_i\n\\end{align}\n\\]\n\nWhen OLS is used to calculate the modified X, this process is called Two-Stage Least Squares (2SLS)\nEffects (Also see LATE in Effects, Calculating LATE and Compliance in Experiments, Analysis)\n\nUsing an instrumental variable allows us to identify the impact of the treatment on compliers. This is known as the local average treatment effect or LATE.\n\nThe LATE is the impact that the treatment has on the people that comply with the instrument.\n\n\\(\\hat\\beta_{IV}\\) only captures the causal effect of X on Y for compliers whose X vary by Z\n\\(\\hat\\beta_{IV}\\) is a weighted average of the treatment effect for compliers, with more weight given to more compliant groups\n\nExample: it is the impact of additional years of schooling (treatment) on fertility of women (outcome) affected by the school reform policy (instrument) only because they live in municipalities that had implemented it.\n**Requires an extra restriction on the instrumental variable**\n\nMonotonocity (no defiers): There is no one in the sample that does not receive the treatment because they received the instrument. This is usually a reasonable assumption to make but it can only be made based on intuition.\n\n(Mathematically) it’s the number of people assigned and received treatment is always greater than or equal to the number of people not assigned yet received treatment.\n\n\nLATE = ATE if any of the following is true\n\nNo heterogeneity in treatment effects\n\n\\(\\beta_{1,i} = \\beta_1 \\quad \\forall i\\)\n\nNo heterogeneity in first-stage responses to the instrument Z\n\n\\(\\pi_{1,i} = \\pi_1 \\quad \\forall i\\)\n\nNo correlation between response to instrument Z and response to treatment X\n\n\\(\\text{Cov}(\\beta_{1,i} , \\pi_{1,i}) = 0\\)\n\n\n\nAlso see Complier Average Causal Effects (CACE) https://www.rdatagen.net/post/cace-explored/\n\nCaveats\n\nThe IV model is not an unbiased estimator, and in small samples its bias may be substantial\nA weak correlation between the instrument and endogenous variable may provide misleading inferences about parameter estimates and standard errors.\n\\(\\beta_1\\), the average treatment effect, assumes that all subgroups experience the roughly the same effect. If there are different subgroups of the population that are substantially affected differently, then a “weighted average of subsets” approach can be used.\n\nExample: Y = lung cancer, X = cigarettes, Z = cigarette tax. Perhaps people whose smoking behavior is sensitive to a tax may have a different β1 than other people",
    "crumbs": [
      "Econometrics",
      "General"
    ]
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-did",
    "href": "qmd/econometrics-general.html#sec-econ-gen-did",
    "title": "General",
    "section": "Difference-in-Differences Estimator",
    "text": "Difference-in-Differences Estimator\n\n\nWithout random samples as data, the selection into one of the two groups is by choice, thus introducing a selection bias\nSome treatments we wish to apply cannot be applied at the individual level but necessarily effect entire groups. Instead of comparing treatment and control groups within the same population at the same time, we can compare the relative change across treatment and control populations across time.\nWhen you have group-level treatments or data available, use random variation across populations to compare their overall trends over time\nPackages\n\n{did}\n\nManually\ndf_did &lt;- df %&gt;%\n  mutate(after = year &gt;= 2014) %&gt;%\n  mutate(treatafter = after*treat)\nreg &lt;- lm(murder ~ treat + treatafter + after, data = DiD)\nBasically a standard lm with an interaction between the treatment indicator and time period indicator demarking before/after treatment.\n\nPredictions\n\n\nThe DiD effect works out to be the interaction effect, \\(\\beta_3\\) = (2nd - 1st eq) - (4th - 3rd eq)\n\nExample\n\nWe want to estimate the effect of a store remodel on visits.\nA remodel affects all potential customers, so this “treatment” cannot be applied at the individual level; in theory, it could be randomized to individual stores, but we do not have the budget for or interest in randomly remodel many stores before there is evidence of a positive effect.\n\nApproach\n\nIn two separate populations, one receives the treatment and one does not. We believe but-for the treatment the two populations would have similar trends in outcome\nWe can estimate the treatment effect by taking the difference between the (post-treatment difference between populations)(solid lines after treatment) and (the pre-treatment difference between populations) (solid lines before treatment)\n\nDiD’s control (dotted line) is an extrapolation of the treatment case that must be parallel to the mean post-treatment outcome (green line post-treatment) of the non-treated case\nThe effect is the difference between DiD’s control (blue dotted line) and the post-treatment outcome (blue line post-treatment) of the treated case.\n\nIn effect, this is the same as extrapolating the counterfactual for the treated population in the post-treatment period if it had not received treatment (the dashed line in the image above)\nTechnically, this is implemented as a fixed-effects regression model\n\nKey Assumptions\n\nThe decision to treat the treatment group was not influenced by the outcome (no anticipation to treat)\n\ne.g. poverty rate spikes and community expects a policy to be enacted soon, so it acts (spends money, etc.) in anticipation of that help coming\n\nIf not for the treatment, the two groups being compared would have parallel trends in the outcome. Note that groups are allowed to have different levels but must have similar trends over time\n\nPretesting parallel trends assumption:\n\n{did} vignette\n{HonestDiD} vignette\nThese aren’t particularly liked. Tests are considered low-powered\n\nOptions if this assumption is violated\n\nUse pre-treatment variables to filter data to create similar groups (Treatment/Control) so they are more likely to have similar trends (pre-cursor to Synthetic Controls method)\n\nEestimate the propensity score based on observed covariates; compute the fitted value\nRun a weighted DiD model\n\nExtrapolate the difference in pre-treatment trends to post-treatment (paper) (Also see {HonestDiD})\nUse a “differential trends” method (explainer, says code available on request)\n\nincludes each post-intervention time period as a dummy variable in your model, and average these to obtain an average treatment effect\n\nCombination of DiD and IV (paper)\n\n\nThere is no spill-over effect such that treating the treatment group has an effect on the control group\n\nApplication\n\nWe can estimate the effect of a store remodel on visits by comparing store traffic before and after the remodel with traffic at a store that did not remodel.\nNote how sensitive this method is to our assumptions:\n\nIf the remodel is an expansion and caused by a foreseen increase in traffic, our first assumption is violated and our effect will be overestimated\nIf the control we chose is another nearby store in the same town, we could experience spillover effects where more people who would have otherwise gone to the control store decide to go to the treatment store instead. This again would overestimate the effect\nAnother counter-example that violates the assumption would be measuring the effect of placing a certain product brand near a store’s check-out on sales and using sales of a different brand of the same product as the control. Why? Since these products are substitutes, the product placement of the treatment group could “spillover” to negatively effect sales of the control\n\n\nRelated Methods\n\nVariants exist that relax different assumptions. For example, we may consider cases in which different units receive the treatment at different times, different units have different (heterogenous) treatment effects, the parallel trend assumption only holds after conditioning on covariates, and many more scenarios\nSynthetic control methods can be thought of as an extension of difference-in-differences where the control is a weighted average of a number of different possible controls\nBayesian structural time-series methods relax the “parallel trends” asumptions of difference-in-differences by modeling the relationship between time series (including trend and seasonal components)\n\nAbadie, Alberto (2005). “Semiparametric Difference-in-Differences Estimators,” Review of Economic Studies (2005) 72, 1–19\n\nAssumption: non-parallel outcome dynamics between treated and controls caused by observed characteristics\nTwo-step strategy:\n\nEstimate the propensity score based on observed covariates; compute the fitted value\nRun a weighted DiD model\n\nThe idea of using pre-treatment variables to adjust trends is a precursor to synthetic control\n\nStrezhnev (2018) extends this approach to incorporate pre-treatment outcomes\nOther considerations (article)\n\nLevels are important.\n\nAlways look at differences in levels between treatment and control, and not just trends. If there are large differences, than think about why they are so different.\nCould these differences affect future trends in our outcome of differences?\n\nFunctional forms matter.\n\nWhen comparing our treatment and control trends, do we think that they evolve similarly in terms of absolute or relative terms? Do we want to use levels or logs?\n\nPre-treatment parallel tests are problematic.\n\nOnly because we reject an unequal parallel trend does not mean that we confirmed its validity, and often, these rejection tests are underpowered.\n\n\nStepped Design (Athey)\n\nAssumptions\n\nAdoption date is conditional on the potential outcomes and possibly pretreatment variables. Guaranteed by design\n\nYou can relax (troublesome) random assignment assumption by requiring only that the adoption date is completely random within subpopulations with the same values for the pre-treatment variables (e.g. units are clusters of individuals like states)\n\nPotential outcomes which rules out the presence of certain treatment effects\n\nNo anticipation - outcome at present is not affected by anticipation of a future treatment date.\nInvariance to history - duration of treatment prior to a given period doesn’t affect the outcome variable value for that period\n\nMore plausible when units are clusters of individuals (e.g. states)\n\n\n\n“Auxillary” Assumptions (i.e. Sometimes needed for particular analyses)\n\nConstant treatment effect across time\nConstant treatment effect across units",
    "crumbs": [
      "Econometrics",
      "General"
    ]
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-scm",
    "href": "qmd/econometrics-general.html#sec-econ-gen-scm",
    "title": "General",
    "section": "Synthetic Control Method (SCM)",
    "text": "Synthetic Control Method (SCM)\n\nCreates a synthetic control based on the pre-treatment features of the treatment unit and non-treated units. A control that’s based on comparison units (i.e. non-treatment units) often provides a better control than a control solely based on the treated unit (like in DiD). After treatment, you take the difference between this synthetic control and your treatment unit to estimate the effect of the treatment. Similar to DiD, except on how the control is formulated.\nMisc\n\nNotes from: Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects\nExtensions\n\nGeneralized synthetic control by Xu (2017)\nSynthetic difference-in-differences by Doudchenko and Imbens (2017)\nPenalized synthetic control of Abadie e L’Hour (2020)\nMatrix completion methods of Athey et al. (2021)\n\n\nTerms\n\nDonor Pool or Donors: The group of units that are untreated which are used to calculate the synthetic control\nUnit, Cell, Case: Interchangeable names for the population level you’re testing in the experiement (e.g. herd, school, store, city, state, precinct)\n\nRecommended Use Cases\n\nWhen events take place at the aggregated level, e.g. county, state, province.\nYou only have one treated unit and a few control units.\n\nAdvantages\n\nBetter apples to apples comparison that DiD since the control should be a better estimate.\nThe weights (which sum to 1) from the calculation of the synthetic control add to the interpretability of the method by giving us information about the “importance” of each non-treated unit in the formulation of the synthetic control\n\nThe donor weights are sparse due to the optimization process. Only a few donors contribute to the synthetic control.\n\nSCM provides transparency about how different the non-treatment units are from the treatment unit. This difference can be calculated.\nThe choice of a synthetic control does not rely on the post-intervention outcomes, which makes it impossible to cherrypick the study design that may affect the conclusions.\n\nChoosing Units for the Donor Pool\n\nThe risk of over-fitting may also increase with the size of the donor pool, especially when T0 (pre-treatment period) is small\nEach of the units in the donor pool have to be chosen judiciously to provide a reasonable control for the treated unit. Including in the donor pool units that are regarded by the analyst to be unsuitable controls (because of large discrepancies in the values of their observed attributes Zj or because of suspected large differences in the values of the unobserved attributes μj relative to the treated unit) is a recipe for bias.\nDonor units with similar values of the observed predictors as the treated unit should be chosen. If it’s believed that a unit has a large unobserved difference with the treated unit, it shouldn’t be included.\nAs a rule of thumb, Abadie, Diamond, and Hainmueller (2010) suggest excluding units for which the prediction MSE is larger than twice the MSE of the treated unit.\n\nPredictors\n\nPredictors are often time series reported by government agencies, multilateral organizations, and private entities (e.g. GDP, crime statistics, cigarette usage, census survey micro-data\nThe predictors of the outcome variable, which are used to calculate the synthetic control, are not affected by the treatment\n\nData\n\nThe larger the pre-treatment period the smaller the bias of the synthetic control estimator (assuming the synthetic control closely tracks the outcome variable during the pre-treatment period.\n\nA trade-off of obtaining more pre-treatment data may be that the predictors are better short term than long term.\n\nIf this is the case, adding weights that favor more recent predictor data can help\n\nIf the amount of pretreatment data is relatively small, then you need very good predictors of the post-treatment outcome such that residual variance will be small which will reduce the chance of overfitting.\n\n\nRobustness Checks\n\nin-time placebo test (backdating): move the treatment date backwards in the data. If the synthetic control still closely tracks the outcome variable until the actual treatment date, then this is evidence of a reliable synthetic control\n\n\nActual date of the treatment (i.e. German reunification) is 1990. Here the re-calculated synthetic control (dashed line) using 1980 as the treatment still tracks GDP until the actual treatment date then they split. Therefore this is evidence of a credible synthetic control.\n\nRobustness with alternate design\n\nMethods\n\nRemove a donor from the donor pool and refit the model and see if the results hold. Repeat with each donor.\n\n\nAll synthetic checks closely track pretreatment GDP and are centered around the synthetic control that used all the donors. Effect for all the synthetic checks are still negative. Evidence of robustness.\nIf the exclusion of a unit from the donor pool has a large effect on results without a discernible change in pre-intervention fit, this may warrant investigating if the change in the magnitude of the estimate is caused by the effects of other interventions or by particularly large idiosyncratic shocks on the outcome of the excluded untreated unit (see Potential Issues and Solutions below)\n\nThe choice of predictors of the outcome variable (no example given)\n\n\nPre-Post Error Ratio\n\\[\n\\lambda = \\frac{\\text{MSE}_{\\text{post}}}{\\text{MSE}_{\\text{pre}}} = \\frac{\\frac{1}{n}\\sum_{t\\in \\text{post}}(Y_t - \\hat Y_t)^2}{\\frac{1}{n}\\sum_{t\\in \\text{pre}}(Y_t - \\hat Y_t)^2}\n\\]\n\nAbadie, Diamond, and Hainmueller (2010) suggest to perform a randomization test is the ratio between pre-treatment MSE and post-treatment MSE.\nP-Value (article)\nlambdas = {}\nfor city in cities:\n    mse_pre = synth_predict(df, SyntheticControl(), city, treatment_year).mse\n    mse_tot = np.mean((df[f'Synthetic [{city}]{style='color: #990000'}'] - df[city])**2)\n    lambdas[city] = (mse_tot - mse_pre) / mse_pre\n\nprint(f\"p-value: {np.mean(np.fromiter(lambdas.values(), dtype='float') &gt; lambdas[treated_city]):.4}\")\n\n\nPotential Issues and Solutions\n\nVolatility of the outcome variable is low. Small or even large effects are difficult to detect if the outcome experiences a lot of shocks that are larger or comparable to the size of the effect.\n\nIn units where substantial volatility is present in the outcome of interest it is advisable to remove it via filtering, in both the treatment unit as well as in the non-treatment units, before applying synthetic control techniques\n\nIdiosyncratic shocks in donor units\n\nimportant to eliminate from the donor pool any units that may have suffered large idiosyncratic shocks to the outcome variable during the treatment period, if it is judged that such shocks would not have affected the outcome of the treatment unit in the absence of the intervention.\n\nI guess the shocks indicate a substantial difference between the treatment unit and the donor\n\n\nAnticipation: if any agents jumped the gun in anticipation of a policy/treatment and engaged in behavior that affects a predictor or outcome variable, the SCM results may be biased.\n\nIf this happens, the treatment date in the dataset should be moved back to just before the agent began it’s behavior or the change in the variable occurred in reaction to agent’s behavior.\n\nSpillover: Donor units experience effects of the treatment even though they weren’t treated. Common if donor units are in close geographical proximity to the treatment unit.\n\nDonor units affected by spillover should be removed from the dataset.\nIf you do include the donor, make note of the direction of the bias. Then, if the bias has a “negative” effect on the treatment effect, you can say the synthetic control estimate provides a lower bound on the magnitude of the causal effect of the treatment\n\nExtreme values in the treatment unit\n\nIf the extreme values are in a predictor variable, but the synthetic control tracks the observed outcome in the pretreatment period, then all is well.\nIf the synthetic control doesn’t track, then the outcome variable should be transformed to differences or growth rates\n\nLong time horizons: Some treatments effects take a long time to emerge.\n\nIn these cases, you either have to just continue to wait, use surrogate outcomes, or use leading indicators\n\nI think “surrogate outcomes” means indirect or proxy measures of the outcome of interest\nAnd leading indicators isn’t referring to normal usage as a predictor but to use as the outcome.",
    "crumbs": [
      "Econometrics",
      "General"
    ]
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-its",
    "href": "qmd/econometrics-general.html#sec-econ-gen-its",
    "title": "General",
    "section": "Interrupted Time Series (ITS)",
    "text": "Interrupted Time Series (ITS)\n\n\nAnalysis of a single time-series data before and after the intervention\n\nExamine whether the outcome variable returns to the baseline after taking away the treatment condition\n\nDoing this multiple times increases data and adds power to the analysis (see Netflix articles in bkmks)\n\nUses Segmented Regression to examine the effects of the intervention\n\nEach segment has its own slope and intercept, and we compare the two segmented regression models to derive the effects\n\n\nMisc\n\nNotes from: A Practitioner’s Guide To Interrupted Time Series\n\nStrengths\n\nTo control for long-term time trends in the data. ITS presents a long-term analytical framework with more extended periods, which better explain any data trends.\nTo account for individual-level bias and to evaluate the outcome variable at the population level. Individual-level data may introduce bias, but not with population data. Honestly, this is both a blessing and a curse. We will elaborate more on the latter aspect in the following part.\nTo evaluate both intended and unintended consequences of interventions. We can easily enlarge analysis and incorporate more outcome variables with minimum or no adaptations.\nTo conduct stratified analyses of subpopulations of individuals and to derive different causal effects. This is critical. We can divide the total population into different sub-groups according to various criteria and examine how each sub-group may behave differently. Social groups are different, and grouping them together may dilute or hide critical information, as positive and negative effects mix together and cancel out (see Harper and Bruckner for examples).\nTo provide clear and interpretable visual results. Visual inspections are always welcome and should be treated seriously (See my other post for more explanations).\n\nLimitations\n\nMultiple rounds of data entries. A minimum of 8 periods before and 8 after an intervention to evaluate the changes. So, we need a total of 16 data entries, which may not be possible all the time. I think Penfold and Zhang (2013) are being cautious about the number of data entries. It’s still possible to apply ITS with few rounds of data entry. Just the causal power may not as robust as the one with multiple rounds.\nTime lag. It takes some unknown time for a program to achieve intended results, which makes it difficult to pinpoint the causal effects of several events that coincide. Let’s say the transportation department in the U.S. adopt three policies within a two-year timespan to curb highway speeding. Playing God, we somehow know it would take 1 yr for Policy A to have any effect, 1.5 ys for Policy B, and 3 yrs for Policy C. In the meantime, it becomes impossible to separate the intertwined effects using ITS.\nInference Level. It’s population-level data, so we can’t make inferences about each individual.\n\nPower and Sample Size Considerations\n\nNumber of time points in each before- and after- segment\n\nRecommendations range from 3 time points per segment to 50 time points per segment\n\nAverage sample size per time point\nFrequency of time points (e.g. weekly, monthly, yearly, etc.)\nLocation of intervention (e.g. midway, 1/3, 2/3, etc.)\n\nAs long as there are sufficient time points per segment and each time point is supported by a large enough sample size, there is not much difference in the study power of an early or late intervention\n\nExpected effect size\n\nSlope change: a gradual change in gradient (or slope) of trend\nLevel change: an instant change in level (i.e. mean)",
    "crumbs": [
      "Econometrics",
      "General"
    ]
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-rdd",
    "href": "qmd/econometrics-general.html#sec-econ-gen-rdd",
    "title": "General",
    "section": "Regression Discontinuity Design (RDD)",
    "text": "Regression Discontinuity Design (RDD)\n\n\nRDDs generate asymptotically unbiased estimates of the effect of an intervention if:\n\nThe relationship between the outcome and running variable is modeled appropriately\n\nDon’t use a particular curve to justify the discontinuity. Gelman prefers reasonable nonlinear curves but don’t go crazy with it so that it maximizes the effect.\n\nThe forcing variable was not manipulated (either behaviorally or mechanically) to influence assignment to the intervention group.\n\ne.g. if the running variable is a test score and the threshold is a particular test score, is there evidence of some sort of cheating to where assignment of students around the threshold isn’t random? (see bullet under step 1 below)\n\n\nMisc\n\nGelman: The big mistakes seem to come from:\n\nUnregularized regression on the forcing variable which randomly give you wild jumpy curves that pollute the estimate of the discontinuity\nNot adjusting for other important pre-treatment predictors\nTaking statistically significant estimates and treating them as meaningful, without looking at the model that’s been fit.\n\nUsecases\n\nLee study of the incumbency effect\n\nWe want to know if a party holding a House seat gives that party an advantage in the next election. But candidates who win (the incumbent) tend to better than challengers from the same party. To overcome this, Lee used an RDD with the Democratic share of the two-party vote in the last election as the forcing variable for Democratic incumbency in the current election. Thee key idea is that, in close elections, seats where a Democratic candidate won will have similar characteristics to districts where a Democratic candidate lost.\n\n\nLakeland recommends using Bayesian estimation and Chebyshev Polynomials\n\nTypes\n\nSharp RDD:\n\nThe threshold separates the treatment and control group exactly\n\nFuzzy RDD:\n\nThe threshold influences the probability of being treated\nThis is in fact an instrumental variable approach (estimating a LATE)\n\n\nTerms\n\nForcing or Assignment or Running Variable:\n\nUsed to assign units to the intervention group and comparison group on either side of a fixed threshold (Cutoff Score).\nMay or may not be related to the potential outcomes but we assume that relationship is smooth, so that changes in the outcome around the threshold can be interpreted as a causal effect.\n\nBandwidth - The number of points selected on each side of the cutoff\n\nShould be wide enough to include a sufficient number of observations and obtain precise estimates. It should also be narrow enough to compare similar units and reduce selection bias.\nCurrent best practice for defining the “neighborhood” of the threshold is to use weights based on a triangular kernel and an “optimal” bandwidth proposed by Imbens and Kalyanaraman (2012). The optimal bandwidth is derived for the simple RDD model with no covariates, though the authors comment that inclusion of additional covariates should not greatly affect the result unless the covariates are strongly correlated with the outcome, conditional on the running variable.\n\n\nSteps\n\nFind and include adjustment variables for differences between the treatment and control groups. Avoid only adjusting for one pre-treatment variable.\n\nThose individuals on both sides of the cut-off point, should be very similar (i.e. it’s more or less random that they’re on one side of the cutoff and not the other). Therefore have something close to a random allocation into treatment and control group\n\ni.e if the cutoff is a test score of 71, then, characteristically, students scoring a 70 should be very similar to students scoring a 72.\n\nIncluding covariates shouldn’t affect the LATE very much but should help lower the std errors some.\n\nIf there is a large effect then the function is probably creating interaction terms with treatment and the covariates. (see bkmk)\n\n\nFit a regression line (or curve) for the intervention group and similarly for the comparison group,\nThe difference in these regression lines at the threshold value of the forcing variable is the estimate of the effect of the intervention (i.e. Local Average Treatment Effect (LATE)).\n\nExample: Corruption\n\nFrom\n\nArticle: Quasi-Experimental Design: Regression Discontinuity Design\nPaper: Businesspeople in Elected Office: Identifying Private Benefits from Firm-Level Returns\n\nDo businesspeople who win elected office in Russia use their positions to help their former firms?\nVariables:\n\nOutcome:\n\nLog total revenue of the candidate’s former firm\nProfit margin: net profit/total revenue during last year of term if member won election or if the member lost, the last year of the hypothetical term if they had won.\n\nTreatment: electoral victory or not (1/0)\nRunning: Vote margin (difference between former firm member/current candidate and their opponent\n\nnegative if former firm member lost, positive if they won\nCutoff = 0\n\n\nAssumptions\n\nCheck for manipulation of the running variable\n\nExamine the balance along a range of covariates between winning and losing candidates in close elections (i.e. around the threshold).\n\nIf no significant imbalance is detected, then there’s no evidence that electoral manipulation favors a specific type of candidate or firm\nIs there any coordination among firms and their candidates?\n\nIf so, we’d expect to see a sharing of the spoils after the election. Therefore, some conspicuous number of firm’s revenue or profit margin should increase even though they lost.\n\n\nThe splits looks pretty even around the cutoff between treated (winners) and control (losers)\n\n\n\nDensity Test: assess the validity of the assumption of continuity around the threshold.\nlibrary(rddensity)\nsummary(rddensity(X = cons$margin, vce=\"jackknife\"))\n\n#&gt; Manipulation testing using local polynomial density estimation.\n#&gt; Number of obs =       2806\n#&gt; Model =               unrestricted\n#&gt; Kernel =              triangular\n#&gt; BW method =           estimated\n#&gt; VCE method =          jackknife\n#&gt; c = 0                 Left of c          Right of c         \n#&gt; Number of obs         1332               1474               \n#&gt; Eff. Number of obs    448                409                 \n#&gt; Order est. (p)        2                  2                   \n#&gt; Order bias (q)        3                  3                   \n#&gt; BW est. (h)           0.157              0.172               \n#&gt; Method                T                  P &gt; |T|             \n#&gt; Robust                -1.7975            0.0723\n\nHave to check out docs + referenced papers to get a detailed idea of whats happening, but the p-value is what’s important\nX is the running variable\npval &gt; 0.05 says not enough evidence to reject null where H0: there’s continuity around the cutoff (i.e. no manipulation)\n\n\n\nFit the RDD\nlibrary(rdrobust)\nfit &lt;- rdrobust(cons$fullturnover.e.l.d, cons$margin, c = 0, all=TRUE)\nsummary(fit)\n\nBW est. (h)                  0.138      0.138\nBW bias (b)                  0.260      0.260\n=============================================================================\n        Method    Coef. Std. Err.        z    P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    0.548    0.197    2.777    0.005    [0.161 , 0.934]     \nBias-Corrected    0.619    0.197    3.136    0.002    [0.232 , 1.005]     \n        Robust    0.619    0.225    2.746    0.006    [0.177 , 1.060]     \n=============================================================================\n\nOutcome: cons$fullturnover.e.l.d\nRunning: cons$margin\nc is the cutoff (default = 0)\nall = TRUE says to report three different methods for std.errors\n\nConventional RD estimates with conventional standard errors.\nBias-corrected estimates with conventional standard errors.\nBias-corrected estimates with robust standard errors.\n\nBW est (for Conventional estimate), BW bias (for bias-corrected estimate) are the bandwidths used\n\nrdbwselect can be used to calculate diffferent bandwidths and then specified in rdrobust with h and b args.\n\nIf 2 numbers are provided for an arg, then it specifies different bandwidths for before and after the cutoff\nThere are a quite a few different methods available (see manual for details)\ndefault “mserd”: one common MSE-optimal bandwidth selector for the RD treatment effect estimator\n\n\np and q args specify the order of polynomial to be used to fit the Conventional model and Bias-corrected model respectively (default = 2 , quadratic)\nInterpretation\n\nThe LATE is 0.548 with a pval = 0.005.\nThere is enough evidence to reject the claim that when a businessperson from a company barely wins an election to a state legislature, there is no effect to the firm’s revenue.\nThe revenue of the firm in the next year will be 0.548 larger than if the businessperson didn’t win the election\n\n\nPotential covariates in this dataset: dummy for foreign ownership, a dummy for state ownership, and logged total fixed assets in the year prior to taking office (baseline feature), categorical financial sector of the firm.\nSensitivity Checks\n\nAdjust the bandwidth and polynomial orders\nif your effect is no longer significant or looks substantially different, then your result is too sensitive and not very credible.\n\nRobustness Checks\n\nTest other values of the cutoff variable.\nThere shouldn’t be a significant effect or one that is similar in strength to the effect when the original cutoff was used.\n\n\nExample: Sometimes a rdd isn’t the answer\n\nMight be worth following this precedure and use the results as a check on the rdd or as a alternative after an rdd doesn’t show convincing results\nFrom Gelman critique, “Air Filters, Pollution, and Student Achievement”:\nDescription:\n\nAliso Canyon gas leak leads many schools to install air filters. RDD study shows test scores went up after the filters were installed. What follows is how Gelman would have conducted the study.\n\n\nSteps\n\nCompare outcomes in schools in the area with and without air filters\n\nFit a regression\n\ndata has one row per school\noutcome being average post-test score per school\npredictors: average pre-test score per school\nindicator: air filters installed\n\n\nMake a scatterplot of post-test vs. pre-test with one point per school, displaying treated schools as open circles and control schools as dots.\nMake a separate estimate and graph for each grade level if you’d like, but I’m guessing that averages will give you all the information you need.\nMake plots of pre-test scores, post-test scores, and regression residuals on a map, using color intensities. I don’t know that this will reveal much either, but who knows. I’d also include the schools in the neighborhood that were not part of the agreement\n(Optional) fit a multilevel model using data from individual students (random effect)—why not, it’s easy enough to do—but I don’t think it will really get you much of anything beyond the analysis of school-level averages.",
    "crumbs": [
      "Econometrics",
      "General"
    ]
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-psm",
    "href": "qmd/econometrics-general.html#sec-econ-gen-psm",
    "title": "General",
    "section": "Propensity Score Matching",
    "text": "Propensity Score Matching\n\nA Propensity Score is the probability of being assigned to a certain treatment, conditional on pre-treatment (or baseline) characteristics\n\n\nMisc\n\nAlso see Survey, Analysis &gt;&gt; Weights &gt;&gt; Types &gt;&gt; Inverse Probability Weights\nPackages\n\n{MatchIt}: propensity score methods\n\nAlso non-parametric: nearest neighbor matching, optimal pair matching, optimal full matching, genetic matching, exact matching, coarsened exact matching, cardinality matching, and subclassification\n\n{MatchThem}\n\nProvides essential tools for the pre-processing techniques of matching and weighting multiply imputed datasets.\nVignette\n\nOthers\n\n{twang}, {Matching}, {optmatch}, {CBPS}, {ebal}, {WeightIt}, {designmatch}, {sbw}, and {cem}\n\nViz\n\n{cobalt}: balance tables and plots using output from above packages\n\n\nNotes from\n\nTwitter thread\n\nblog post\n\nslack::kris used “coarsened exact matching” in his project. No idea what this is. Need to check it out.\nPaper: Choosing the Estimand When Matching or Weighting in Observational Studies\n\nHow to choose an estimand based on your question (and how that maps to particular weighting / matching choices)\n\n\n\n\n\n\nBBR Chapter 17.2 to 17.8\n\nBiostatistics for Biomedical Research (Harrell) Ch.17.2 - 17.8: Modeling for Observational Treatment Comparisons\nAdjusting for Confounders\n\nUse of the Propensity Score (PS) allows one to aggressively adjust for measured potential confounders\nDoing an adjusted analysis where the adjustment variable is the PS simultaneously adjusts for all the variables in the score insofar as confounding is concerned (but not with regard to outcome heterogeneity)\nStratifying for PS does not remove all the measured confounding\n\nBut adjusting only for PS is inadequate.\n\nTo get proper conditioning so that the treatment effect can generalize to a population with a different covariate mix, one must condition on important prognostic factors\nNon-collapsibility of hazard and odds ratios is not addressed by PS adjustment\nAdjusting only for PS can hide interactions with treatment\n\nPS is not necessary if the effective sample size (e.g. number of outcome events) &gt; 5p where p is the number of measured covariates\nWhen judging covariate balance (as after PS matching) it is not sufficient to examine the mean covariate value in the treatment groups\nTreatment Effects\n\nEliminate units in intervals of PS where there is no overlap between treatment A and treatment B, or include an interaction between treatment and a baseline characteristic\n\nExample: Including an interaction between age and treatment and there were no units greater than 70 years old receiving treatment B\n\nThen, the B:A difference for age greater than 70 would have an extremely wide confidence interval as it depends on extrapolation. So the estimates that are based on extrapolation are not misleading; they are just not informative.\n\n\n\nTypes\n\nPairs Matching\n\nThrows away data –&gt; low power\n\nUnits get discarded that have characteristics which are the same as another unit and has already been matched (i.e. units that have the same information)\n\n\nInverse Probability Weighting\n\na high variance/low power approach like matching\nAlso see Survey, Analysis &gt;&gt; Weights &gt;&gt; Types &gt;&gt; Inverse Probability Weights\n\n\nModeling\n\\[\n\\begin{align}\nY =\\:\\: &\\operatorname{treat} + \\log \\frac{\\text{PS}}{1-\\text{PS}} \\\\\n&+ \\text{nonlinear functions of}\\: \\log \\frac{\\text{PS}}{1-\\text{PS}} \\\\\n&+ \\text{important prognostic variables}\n\\end{align}\n\\]\n\nIn biostatistics, a prognostic factor or variable is a patient characteristic that can predict that patient’s eventual response to an intervention\nPrognostic variables need to be in model even though they are also in the PS, to account for subject outcome heterogeneity (susceptibility bias)\nIf outcome is binary and you can afford to ignore prognostic variables, use nonparametric regression, Y ~ PS, and fit a model to each treatment group’s data\n\nNonparametric Regression - does not assume linearity; only assumes smoothness, Y ~ X where X is continuous\n\ne.g. moving avg, loess, other smoothers, etc.\nSee BBR Ch 8.7 for details, examples (no binary outcome examples)\nChecking functional form in logistic regression using loess plots\n\nShows a binary outcome used in a loess model\n\n\nPlotting these two curves with PS on x-axis and looking at vertical distances between curves is an excellent way to adjust for PS continuously without assuming a model\n\nGuessing the average distance between the curves is the treatment effect (?)\n\n\n\n\n\n\nPsuedo-Distributions After Weighting According to the Type of Estimand\n\n{cobalt} (not on CRAN) can be used to produce the balance plots below using output from various propensity scoring packages (see above)\nShows how weights derived from propensity scores makes treatment and control groups comparable\nLight green and light blue show psuedo-counts that are added to the groups after applying weights\nNo Estimand\n\nMirrored histogram of propensity scores for treatment (top) and control (bottom) groups\nNo groups are upweighted (or equivalently, for both groups, weights = 1)\n\n\nx-axis is the propensity score\ny-axis is the count of people with that score\nMore mass on the *right* in the treatment group (top) means that more people in that group had a higher probability of receiving treatment (duh)\nMore mass in the treatment group than the control group means more people received the treatment than control\n\n\nAverage Treatment Effect (ATE)\n\nTarget: whole population\nTreated and Control groups are upweighted\n\n\nLight green and light blue show psuedo-counts that are added to the groups after applying weights\nBoth groups now similar (i.e. comparable)\nindividual \\(\\text{unit}_i\\) weights\n\n\\(\\text{treatment\\_weight}_i = \\frac{1}{\\text{propensity\\_score}_i}\\)\n\\(\\text{control\\_weight}_i = \\frac{1}{1 - \\text{propensity\\_score}_i}\\)\n\n\nPotential Issues\n\nWeights are unbounded\n\nReally small propensity scores for the Treatment group (or really large ones for control) could have an oversized effect on the analysis.\nCan lead to finite sample bias // variance issues\n\n\n\nAverage Treatment Effect on the Treated (ATT)\n\nTarget: treatment group\nControl group is upweighted\n\n\nIndividual \\(\\text{unit}_i\\) weights\n\n\\(\\text{treatment\\_weight}_i = 1\\)\n\\({\\text{control\\_weight}_i} = \\frac{\\text{propensity\\_score}_i}{1 - \\text{propensity\\_score}_i}\\)\n\n\nPotential Issues\n\nExtremely unbalanced groups\n\nIn this example, there are much more treated units than control units \\(\\rightarrow\\) control group must be substantially upweighted to become comparable\nCan lead to instability\n\n\n\nAverage Treatment Among Overlap Population (ATO)\n\nTarget: Clinical equipoise\n\nThe assumption that there is not one ‘better’ intervention present (for either the control or experimental group) during the design of a randomized controlled trial (RCT). A true state of equipoise exists when one has no good basis for a choice between two or more care options.\nSee Notes from &gt;&gt; Paper for more details\n\nTreated is downweighted\n\n\nIndividual \\(\\text{unit}_i\\) weights\n\n\\(\\text{treatment\\_weight}_i = 1 - \\text{propensity\\_score}_i\\)\n\\(\\text{control\\_weight}_i = \\text{propensity\\_score}_i\\)\n\n\nWeights are bounded by 0 and 1, so they have nice variance properties",
    "crumbs": [
      "Econometrics",
      "General"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html",
    "href": "qmd/econometrics-mixed-effects-frequentist.html",
    "title": "Mixed Effects",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-misc",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-misc",
    "title": "Mixed Effects",
    "section": "",
    "text": "Random Effects model = Multi-level model with random intercepts = Hierarchical model\nBayesian\n\nResources\n\nBayesian Generalized Linear Mixed Effects Models for Deception Detection Analyses\n\nPaper that’s an in-depth tutorial. Uses {brms}, {emmeans}, {parameters}\n\n\n\nAdvantages of a mixed model (y ~ x + (x | g)) vs a linear model with an interaction (y ~ x * g)\n\nFrom T.J. Mahr tweet\nConceptual: Assumes participant means are drawn from the same latent population\nComputational: partial pooling / smoothing\nBoth will have very similar parameter estimates when the data is balanced with few to no outliers\n\nlinear least squares regression can overstate precision, producing t-statistics for each fixed effect that tend to be larger than they should be; the number of significant results in LLSR are then too great and not reflective of the true structure of the data\nStandard errors are underestimated in the interaction model though.\n\nDoesn’t account for dependence in the repeated measure for each subject\n\nFor unbalanced data w/some group categories having few data points or with outliers, the mixed effects model regularizes/shrinks/smooths the estimates to the overall group (i.e. population) mean\n\nExample of model formula interpretation\n\n\nPackages\n\n{lme4} - linear and generalized linear mixed-effects models; implemented using the ‘Eigen’ C++ library for numerical linear algebra and ‘RcppEigen’ “glue”.\n\nUsing {lmerTest} will produce a summary of lme4 models with pvals for coefficients\n\n{multilevelmod} - tidymodels wrapper for many mixed model packages.\n{plm} - linear models for panel data; including within/fixed effects, random effects, between, first-difference, nested random effects\n{glmmTMB} - for fitting generalized linear mixed models (GLMMs) and extensions\n\nWide range of statistical distributions (Gaussian, Poisson, binomial, negative binomial, Beta …) and zero-inflation.\nFixed and random effects models can be specified for the conditional and zero-inflated components of the model, as well as fixed effects for the dispersion parameter.\n\n{spaMM} - Inference based on models with or without spatially-correlated random effects, multivariate responses, or non-Gaussian random effects (e.g., Beta).\n\nMixed Effects and repeated measures (aka longitudinal)\n\nMixed Effects Models = Fixed Effects and Random Effects\n\ni.e. variation within the unit and between the units\n\nHarrell (article)\n\nSays that Mixed Models models can capture within-subject correlation of repeated measures over very short time intervals but not over extended time intervals where autocorrelation comes into play\n\nExample of a short interval is a series of tests on a subject over minutes when the subject does not fatigue\nExample of a long interval is a typical longitudinal clinical trial where patient responses are assessed weekly or monthly\n\nHe recommends a Markov model for longitudinal RCT data (see bkmks)\n\nBartlett\n\nMixed model repeated measures (MMRM) in Stata, SAS and R\n\nTutorial; uses {nlme::gls}\n\n\n\n{tidymodels} workflows (optional outputs: lmer, glmer, stan_glmer objects)",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-consid",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-consid",
    "title": "Mixed Effects",
    "section": "Considerations",
    "text": "Considerations\n\nMotivation for using a Random Effects model\n\nYou think the little subgroups are part of some bigger group with a common mean effect\n\ne.g. multiple observations of a single person, or multiple people in a school, or multiple schools in a district, or multiple varieties of a single kind of fruit, or multiple kinds of vegetable from the same harvest, or multiple harvests of the same kind of vegetable, etc.\n\nThese subgroup means significantly deviate from the big group mean.\nThese deviations follow a distribution, typically Gaussian.\n\nThat’s where the “random” in random effects comes in: we’re assuming the deviations of subgroups from a parent follow the distribution of a random (Gaussian) variable\n\nThe variation between subgroups is assumed to have a normal distribution with a 0 mean and constant variance (variance estimated by the model).\n\nGEEs are a semi-parametric option for panel data (See Regression, Other &gt;&gt; Generalized Estimating Equations (GEE))\n\n\nFixed Effects or Random Effects?\n\nIf there’s likely correlation between unobserved group/cases variables (e.g. individual talent) and treatment variable (i.e. E(α|x) != 0) AND there’s substantial variance between group units, then FE is a better choice (See Econometrics, Fixed Effects &gt;&gt; One-Way Fixed Effects &gt;&gt; Assumptions)\nIf cases units change little, or not at all, across time, a fixed effects model may not work very well or even at all (SEs for a FE model will be large)\n\nThe FE model is for analyzing within-units variance\n\nDo we wish to estimate the effects of variables whose values do not change across time, or do we merely wish to control for them?\n\nFE: these effects aren’t estimated but adjusted for by explicitly including a separate intercept term for each individual (αi) in the regression equation\nRE: estimates these effects (might be biased if RE assumptions violated)\n\nThe RE model is for analyzing between-units variance\n\n\nThe amount of within-unit variation relative to between-unit variation has important implications for these two approaches\n\nArticle with simulated data showed that within variation around sd &lt; 0.5 didn’t detect the effect of explanatory variable but ymmv (depends on # of units, observations per unit, N)\n\nDurbin–Wu–Hausman test ({plm::phtest})\n\nIf H0 is not rejected, then both FE and RE are consistent but only RE is efficient. \\(\\rightarrow\\) use RE but if you have a lot of data, then FE is also fine.\nIf H0 is rejected, then only FE is consistent \\(\\rightarrow\\) use FE\n\nICC &gt; 0.1 is generally accepted as the minimal threshold for justifying the use of Mixed Effects Model (See Diagnostics &gt;&gt; ICC)\n\nPooling\n\nComplete pooling - Each unit is assumed to have the same effect\n\nExample: County is the grouping variable and radon level is the outcome\n\nAll counties are alike.\n\ni.e. all characteristics of counties that affect radon levels in houses have the statistically same effect across counties. Therefore, the variable has no information.\n\nRun a single regression to estimate the average radon level in the whole state.\n\nlm(radon_level ~ predictors)\nNote that “county” is NOT a predictor in this model\n\n\n\nNo pooling - All units are assumed to have independent effects\n\nExample: County is the grouping variable (although not a predictor in this case) and radon level is the outcome\n\nAll counties are different from each other.\n\ni.e. there are no common characteristics of counties that affect radon levels in houses. Any characteristic a county has that affects radon levels is unique to that county.\n\nRun a regression for each county to estimate the average radon level for each county.\n\nlm(radon_level ~ 0 + county + predictors\nUsing the “0 +” formula removes the common intercept which means each county will get it’s own intercept\n\n\n\nPartial pooling - Each unit is assumed to have a different effect, but the data for all of the observed units informs the estimates for each unit\n\nExample: County is the grouping variable (random effect) and radon level is the outcome\n\nAll counties are similar each other.\n\ni.e. all charcteristics of counties that affect radon levels in house have statistically varying effects sizes depending on the particular county\n\nRun a multi-level regression to share information across counties.\n\nlmer(radon_level ~ predictors + (1 + predictor | county))\n\n\nThis can be a nice compromise between estimating an effect by completely pooling all groups, which masks group-level variation, and estimating an effect for all groups completely separately, which could give poor estimates for low-sample groups.\nIf you have few data points in a group, the group’s effect estimate will be based partially on the more abundant data from other groups. (2 min video)\nPartial pooling is typically accomplished through hierarchical models. Hierarchical models directly model the population of units. From a population model perspective, no pooling corresponds to infinite population variance, whereas complete pooling corresponds to zero population variance.\n\n\nVariable Assignment\n\nQuestions (article has examples)\n\nCan the groups we see be considered the full population of possible groups we could see, or are they more like a random sample of a larger set of groups?\n\nFull Population: Fixed\nRandom Sample: Random\n\nDo we want to estimate a coefficient for this grouping, or do we just want to account for the anticipated structure that the groups may impose on our observations?\n\nY: Fixed\nN: Random\n\n\nFixed Effects provide estimates of mean-differences or slopes.\n\n“Fixed” because they are effects that are constant for each subject/unit\nLevel One: variables measured at the most frequently occurring observational unit\n\ni.e. vary for each repeated measure of a subject and vary between subjects\n\nIn the dataset, these variables that (for the most part) have different values for each row\nTime-dependent if you have longitudinal data\n\nFor a RE model, these are usually the adjustment variables\n\ne.g. conditioning on a confounder\n\n\nLevel Two: variables measured at the observational unit level\n\ni.e. constant for each repeated measure of a subject but vary between each subject\nFor a RE model, these are usually the treatment variables or variables of interest\n\nThey should contain the information about the between-subject variation\n\nIf a factor variable, it has levels which would not change in replications of the study\n\n\nRandom Effects estimate of variation between and within subgroups\n\nClustering variable\nQuantifies how much of the overall variation can be attributed to that particular variable.\n\nExample: the variation in beetle DAMAGE was attributable to the FARM at which the damage took place, so you’d cluster by FARM (1|FARM)\n\nIf you want slopes to vary according to a variable, the variation of slopes between-units will be a random effect\n\nUsually a level 2 fixed effect variable\n\nIf you want intercepts to vary according to a variable, the variation of intercepts between-units will be a random effect\n\nThis will the unit/subject variable (e.g student id, store id) that has the repeated observations\n\nTypically categorical variables that we are not interested in measuring their effects on the outcome variable, but we do want to adjust for. This variation might capture effects of latent variables.\n\nThis factor variable has levels which can be thought of as a sample from a larger population of factor levels (e.g. hockey players)\nExample: 2 hockey players both averaged around 20 minutes per game last year (fixed variable). Predictions of the amount of points scored by just accounting for this fixed variable would produce similar results. But using PLAYER as a random variable will capture the impact of persistent characteristics that might not be observable elsewhere in the explanatory data. PLAYER can be thought of as a proxy for “offensive talent” in a way.\n\nIf the values of the variable were chosen at random, then you should cluster by that variable (i.e. choose as the random variable)\n\nExample: If you can rerun the study using different specific farms (i.e .different values of the FARM factor, see above) and still be able to draw the same conclusions, then FARM should be a random effect.\n\nHowever, if you had wanted to compare or control for these particular farms, then Farm would be “fixed.”\nSay that there is nothing about comparing these specific fields that is of interest to the researcher. Rather, the researcher wants to generalize the results of this experiment to all fields. Then, FIELD would be “random.”\n\n\nIf the random effects are correlated with variables of interest (fixed effects), leaving them out could lead to biased fixed effects. Including them can help more reliably isolate the influence of the fixed effects of interest and more accurately model a clustered system.\nTo see individual random effects: lme4::ranef(lme_mod) or lme4::ranef(tidy_mod$fit)",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-assum",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-assum",
    "title": "Mixed Effects",
    "section": "Assumptions",
    "text": "Assumptions\n\nNo Time-Constant Unobserved Heterogeneity: \\(\\mathbb{E}(\\alpha_i\\;|\\;x_{it}) = 0\\)\n\ni.e. No correlation between time-invariant valued, unobserved variables (aka random effect variable) and the explanatory variable of interest (e.g. treatment)\n\nTime-Invariant Valued, Unobserved Variables: Variables that are constant across time for each unit and explain variation between-units\n\ne.g. If random effect variable (aka clustering variable) is the individual, then the “time-invariant, unobserved” variable could be something latent like talent or something measureable like intelligence or socio-economic status. Whatever is likely to explain the variance between-units in relation to the response.\n\n\nThe effect that these variables have must also be time-invariant\n\ne.g. The effect of gender on the outcome at time 1 is the same as the effect of gender at time 5\nIf effects are time-varying, then an interaction of gender with time could be included\n\nIt’s not reasonable for this to be exactly zero in order to use a mixed effected model, but for situations where there’s high correlation, this model should be avoided\nExample: Violation\n\n\nEach group’s x values get larger from left to right as each group’s α (aka y-intercepts) for each unit get larger\n\ni.e. Mixed-Effects models fail in cases where there’s very high correlation between group intercepts and x, together with large between-group variability compared to the within-group variability\n\n**FE model would be better in this case\n\nGelman has a paper that describes how a mixed effect model can be fit in this situation though\n\n\n\nNo Time-Varying Unobserved Heterogeneity: \\(\\mathbb{E}(\\epsilon_{it}|x_{it}) = 0\\)\n\ni.e No endogeneity (no correlation between the residuals and explanatory variable of interest)\nIf violated, there needs to be explicit measurements of omitted time-invariant variables (see 1st assumption for definition) if they are thought to interact with other variables in the model.",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-specnot",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-specnot",
    "title": "Mixed Effects",
    "section": "Specifications and Notation",
    "text": "Specifications and Notation\n\nVarying Intercepts: (1 | v1)\n\nAKA Random Intercepts, where each level of the random variable (our random effect) had an adjustment to the overall intercept\nExample: Each department (random variable) has a different starting (intercepts) salary (outcome variable) for their faculty members (observations), while the annual rate (slope) at which salaries increase is consistent across the university (i.e. effect is constant between-departments)\n\n\\[\n\\widehat{\\text{salary}}_i = \\beta_{0 j[i]} + \\beta_1 \\cdot \\text{experience}_i\n\\]\n\nWhere j[i] is the index for department\nThis strategy allows us to capture variation in the starting salary (intercepts) of our faculty\n\n\nVarying Slopes: (0 + v2| v1)\n\nAKA Random Slopes, which would allow the effect of the v2 to vary by v1\n\ni.e. v2 is a predictor whose effect varies according to the level of the grouping variable, v1.\n\ni.e. A slope for each level of the random effects variable\nExample: faculty salary (outcome) increase at different rates (slopes) depending on the department (random variable).\n\n\\[\n\\widehat{\\text{salary}}_i = \\beta_0 + \\beta_{1 j[i]} \\cdot \\text{experience}_i\n\\]\n\nWhere j[i] is the index for department\nThis strategy allows us to capture variation in the change (slopes) in salary\n\n\nVarying Slopes and Intercepts: (1 + v2 | v1) or just (v2 | v1) (See examples)\n\nSee above for descriptions of each\nExample: Each department (random variable) has a different starting (intercepts) salary (outcome variable) for their faculty members (observations), while the annual rate (slope) at which salaries increase varies depending on department\n\n\\[\n\\widehat{\\text{salary}}_i = \\beta_{0 j[i]} + \\beta_{1 j[i]} \\cdot \\text{experience}_i\n\\]\n\nWhere j[i] is the index for department\nSee above for descriptions of each type of variation this strategy captures\n\n\nNested effects\n\ne.g. Studying test scores (outcome variable) within schools (random variable) that are within districts (random variable)\nNotation: (1 | v1 / v2) says intercepts varying among v1 and v2 within v1.\n\ne.g. schools is v2 and districts is v1\n\nUnit IDs may be repeated within groups.\n\nExample: lme4::lmer(score ~ time  + (time | school/student_id))\n\nThe random effect at this level is the combination of school and student_id, which is unique.\nYou can verify this by calling ranef on the fitted model. It’ll show you the unique combinations of student_ids within schools used in the model.\n\n\nIf you take the fixed effects values shown in summary(model) then add the ranef(model) values to them, that’s what coef(model) gives.",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-strat",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-strat",
    "title": "Mixed Effects",
    "section": "Strategy",
    "text": "Strategy\n\nMisc\n\nAlso see\n\nBMLR Ch 8 &gt;&gt; Model Building Workflow: Simple to Complex\nModel Building, Concepts &gt;&gt; Misc\n\nBegins with a saturated fixed effects model, determines variance components based on that, and then simplifies the fixed part of the model after fixing the random part.\n\nOverall:\n\nEDA\nFit some simple, preliminary models, in part to establish a baseline for evaluating larger models.\nThen, build toward a final model for description and inference by attempting to add important covariates, centering certain variables, and checking model assumptions.\n\nProcess\n\nEDA at each level (See EDA, Multilevel, Longitudinal)\nExamine models with no predictors to assess variability at each level\nCreate Level One models: starting simple and adding terms as necessary (See Considerations &gt;&gt; Variable Assignment &gt;&gt; Fixed Effects )\nCreate Level Two models: starting simple and adding terms as necessary (See Considerations &gt;&gt; Variable Assignment &gt;&gt; Fixed Effects)\n\nBeginning with the equation for the intercept term.\n\nExamine the random effects and variance components (See Considerations &gt;&gt; Variable Assignment &gt;&gt; Random Effects)\n\nBeginning with a full set of error terms and then removing covariance terms and variance terms where advisable\n\ne.g. When parameter estimates are failing to converge or producing impossible or unlikely values\n\nSee Specifications and Notation for different RE modeling strategies.",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-diag",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-diag",
    "title": "Mixed Effects",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nAlso see\n\nDiagnostics, Mixed Effects\nBMLR Ch.8 &gt;&gt; Model Building Workflow &gt;&gt; Unconditional Means for an ICC example\nExamples &gt;&gt; Random Intercept-Only model for an ICC example\n\nInterClass Coefficient (ICC): The proportion of variation that is between-cases\n\n\\[\n\\rho = \\frac{\\sigma_0}{\\sigma_0 + \\sigma_\\epsilon}\n\\]\n\nWhere \\(\\sigma_0\\) is the between-case variance and \\(\\sigma_\\epsilon\\) is the within-case variance.\n1-ICC is the proportion of variation within cases\nStatistical power is a function of ICC (article)\n\n\nBoth higher ICCs and cluster size variability lead to reduced power\nThe dispersion parameter is a parameter used in the data simulation\n\nGuideline: ICC &gt; 0.1 is generally accepted as the minimal threshold for justifying the use of Mixed Effects Model\nExample: {sjPlot}\nlibrary(sjPlot)\ntab_model(lme_fit)\n\nMight need {lmerTest} loaded to get coefficient pvals\nAlso calculates two R2 values\n\nMarginal: proportion of variance explained , by the fixed effects only\nConditional: proportion of variance explained by the fixed effects and random effects",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-examp",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-examp",
    "title": "Mixed Effects",
    "section": "Examples",
    "text": "Examples\n\nExample: One-Way Random Effects\nre1 &lt;- plm(happiness ~ age, data = df,\n          index = c(\"id\", \"wave\"),\n          effect = \"individual\", model = \"random\")\n\nsummary(re1)\n## Effects:\n##                  var std.dev share\n## idiosyncratic 0.03641 0.19081 0.096\n## individual    0.34396 0.58648 0.904\n## theta: 0.8394\n## Coefficients:\n##              Estimate Std. Error z-value  Pr(&gt;|z|)   \n## (Intercept)  6.933947  1.534054  4.5200 6.183e-06 ***\n## age         -0.015621  0.031277 -0.4994    0.6175\n\nIn Fixed effects, “id” is the cases variable and “wave” is the time variable, but I’m not sure if that’s how they’re referred to in a Mixed Effects model\n\nMaybe indexes are always required for panel data even if it isn’t a FE model\n\neffect = “individual”, model = “random” specifies the model as a mixed effects model\nidiosyncratic is the time-variant variance (within-cases, ε) variance\nindividual is the time-constant (between-cases, α) variance\n\nExample: {lme4}\n\nFrom https://www.alexcernat.com/etimating-multilevel-models-for-change-in-r\nusl is UK sociological survey data\n\nlogincome is a logged income variable\npidp is the person’s id\nwave0 is the time variable that’s indexed at 0\n\n\n\nRandom InterceptsRandom InterceptsRandom Slopes and Intercepts\n\n\nlibrary(lme4)\n# unconditional means model (a.k.a. random effects model)\nm0 &lt;- lmer(data = usl, logincome ~ 1 + (1 | pidp))\n\n# check results\nsummary(m0)\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: logincome ~ 1 + (1 | pidp)\n##    Data: usl\n## \n## REML criterion at convergence: 101669.8\n## \n## Scaled residuals: \n##    Min      1Q  Median      3Q    Max \n## -7.2616 -0.2546  0.0627  0.3681  5.8845 \n## \n## Random effects:\n##  Groups  Name        Variance Std.Dev.\n##  pidp    (Intercept) 0.5203  0.7213 \n##  Residual            0.2655  0.5152 \n## Number of obs: 52512, groups:  pidp, 8752\n## \n## Fixed effects:\n##            Estimate Std. Error t value\n## (Intercept) 7.162798  0.008031  891.8\n\n0 covariates\nInterpretation\n\nFixed effects:\n\n(Intercept) = 7.162798, which is the grand mean\n\nSays that over all the time points and individuals, the estimated average log income is 7.162798\n\n\nRandom effects:\n\nEverything that varies by pidp\nBetween-case (or person, group, cluster, etc.) variance: (Intercept) = 0.5203\nWithin-case (or person, group, cluster, etc.) variance: Residual = 0.2655\n\n\nICC\n\n\\(\\frac{0.520}{0.520 + 0.265} = 0.662\\)\nSays about 66% of the variation in log income comes between people while the remaining (~ 34%) is within people.\n\n\n\n\nlibrary(lme4)\n# unconditional change model (a.k.a. MLMC)\nm1 &lt;- lmer(data = usl, logincome ~ 1 + wave0 + (1 | pidp))\n\nsummary(m1)\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: logincome ~ 1 + wave0 + (1 | pidp)\n##    Data: usl\n## \n## REML criterion at convergence: 100654.8\n## \n## Scaled residuals: \n##    Min      1Q  Median      3Q    Max \n## -7.1469 -0.2463  0.0556  0.3602  5.7533 \n## \n## Random effects:\n##  Groups  Name        Variance Std.Dev.\n##  pidp    (Intercept) 0.5213  0.7220 \n##  Residual            0.2593  0.5092 \n## Number of obs: 52512, groups:  pidp, 8752\n## \n## Fixed effects:\n##            Estimate Std. Error t value\n## (Intercept) 7.057963  0.008665  814.51\n## wave0       0.041934  0.001301  32.23\n## \n## Correlation of Fixed Effects:\n##      (Intr)\n## wave0 -0.375\n\n1 covariate\nRandom Effect: Cases; Fixed Effect: Time\nInterpretation\n\nFixed effects:\n\n(Intercept) = 7.057; the expected log income at the beginning of the study (wave 0).\nwave0 = 0.0419\n\nThe average rate of change with the passing of a wave.\n\nI don’t think this is a percentage. It’s the same as a standard OLS regression interpretation.\n\nSo after each wave, individual log income is expected to slowly increase by 0.041 on average.\n\n\nRandom Effects\n\nEverything that varies by pidp\nBetween-Case (or person, group, cluster, etc.) variance in log income: (Intercept) = 0.5213\nWithin-Case (or person, group, cluster, etc.) variance in log income: Residual = 0.2593\nSo this stuff is still very similar numbers as the random intercept-only model\n\n\nVisualize Effect\n\nusl$pred_m1 &lt;- predict(m1)\nusl %&gt;% \n  filter(pidp %in% 1:5) %&gt;% # select just five individuals\n  ggplot(aes(wave, pred_m1, color = pidp)) +\n  geom_point(aes(wave, logincome)) + # points for observed log income\n  geom_smooth(method = lm, se = FALSE) + # linear line showing wave0 slope\n  theme_bw() +\n  labs(x = \"Wave\", y = \"Logincome\") + \n  theme(legend.position = \"none\")\n\nwave was indexed to 0 for the model but now wave starts at 1. He might’ve reverted wave to have a starting value of 1 for graphing purposes\nLines show the small, positive, fixed effect slope for wave0\nParallel lines means we assume the change in log income over time is the same for all the individuals\n\ni.e. We assume there is no between-case variation in the rate of change.\n\n\n\n\n\nlibrary(lme4)\n# unconditional change model (a.k.a. MLMC) with re for change\nm2 &lt;- lmer(data = usl, logincome ~ 1 + wave0 + (1 + wave0 | pidp))\n\nsummary(m2)\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: logincome ~ 1 + wave0 + (1 + wave0 | pidp)\n##    Data: usl\n## \n## REML criterion at convergence: 98116.1\n## \n## Scaled residuals: \n##    Min      1Q  Median      3Q    Max \n## -7.8825 -0.2306  0.0464  0.3206  5.8611 \n## \n## Random effects:\n##  Groups  Name        Variance Std.Dev. Corr \n##  pidp    (Intercept) 0.69590  0.8342       \n##          wave0       0.01394  0.1181  -0.51\n##  Residual            0.21052  0.4588       \n## Number of obs: 52512, groups:  pidp, 8752\n## \n## Fixed effects:\n##            Estimate Std. Error t value\n## (Intercept) 7.057963  0.009598  735.39\n## wave0       0.041934  0.001723  24.34\n## \n## Correlation of Fixed Effects:\n##      (Intr)\n## wave0 -0.558\n\nRandom Effect: outcome (i.e. intercept) by cases\nTime Effect by cases\nFixed Effect: time\n(1 + wave0 | pidp) - Says let both the intercept (“1”) and wave0 vary by pidp - Which means that average log income varies by person and the rate of change (slope) in log income over time (wave0 fixed effect) varies by person.\nInterpretation\n\nFixed effects:\n\n(Intercept) = 7.057963: The expected log income at the beginning of the study (wave 0).\nwave0 = 0.0419\n\nThe average rate of change with the passing of a wave.\n\nI don’t think this is a percentage. It’s the same as a standard OLS regression interpretation.\n\nSo after each wave, individual log income is expected to slowly increase by 0.041 on average.\n\n\nRandom effects:\n\nEverything that varies by pidp\nBetween-Case (or person, group, cluster, etc.) variance of log income:\n\n(Intercept) = 0.69590\nHow much average log income varies by person at the start of the study (wave = 0)\n\nWithin-Case (or person, group, cluster, etc.) variance of log income:\n\nResidual = 0.21052\nHow much each person’s log income varies in relation to her average log income\n\nBetween-Case variance in the rates of change of log income over time\n\nwave0 = 0.01394\ni.e. How much the fixed effect, wave0, slope varies by person\n\n\n\nVisualize Effect\n\nusl$pred_m2 &lt;- predict(m2) \nusl %&gt;%  \n  filter(pidp %in% 1:5) %&gt;% # select just two individuals \n  ggplot(aes(wave, pred_m2, color = pidp)) + \n  geom_point(aes(wave, logincome)) + # points for observed logincome \n  geom_smooth(method = lm, se = FALSE) + # linear line based on prediction \n  theme_bw() + # nice theme \n  labs(x = \"Wave\", y = \"Logincome\") + # nice labels \n  theme(legend.position = \"none\")\n\nDifferent slopes for each person\n\n\n\n\n\nExample: Varying Slopes and Intercepts\nm_slope &lt;- lmer(pp60 ~ position + toi + \n                (1 + age | player),\n                data = df)\n\nWe use this model if we wanted to incorporate an age variable into our model and we wanted the influence of that variable to vary by player, it would be incorporated like so, before the | player:\n\nExample: {ggeffects} Error Bar Plot\n\nlibrary(ggeffects)\nlibrary(ggplot2)\n# create plot dataframe\n# Has 95% CIs for fixed effects and lists random effects\nplot_data &lt;- ggpredict(fit, terms = c(\"Season\"))\n\n#create plot\nplot_data %&gt;%\n  #reorder factor levels for plotting\n  mutate(x = ordered(x, levels = c(\"Preseason\", \"Inseason\", \"Postseason\"))) %&gt;%\n  #use plot function with ggpredict objects\n  plot() + \n  #add ggplot2 as needed\n  theme_blank() + ylim(c(3000,7000)) + ggtitle(\"Session Distance by Season Phase\")\n\nDescription:\n\nOutcome: Distance\nFixed Effect: Season\nRandom Effect: Athlete\n\n\nExample: {tidymodels} Varying Intercepts\nlmer_spec &lt;- \n  linear_reg() %&gt;% \n  set_engine(\"lmer\")\n\ntidy_mod &lt;- \n  lmer_spec %&gt;% \n  fit(pp60 ~ position + toi + (1 | player),\n      data = df)\n\ntidy_mod\n## parsnip model object\n## \n## Linear mixed model fit by REML ['lmerMod']\n## Formula: pp60 ~ position + toi + (1 | player)\n##    Data: data\n## REML criterion at convergence: 115.8825\n## Random effects:\n##  Groups  Name        Std.Dev.\n##  player  (Intercept) 0.6423 \n##  Residual            0.3452 \n## Number of obs: 80, groups:  player, 20\n## Fixed Effects:\n## (Intercept)    positionF          toi \n##    -0.16546      1.48931      0.06254",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-bmlrch8",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-bmlrch8",
    "title": "Mixed Effects",
    "section": "BMLR Chapter 8: Multilevel Models",
    "text": "BMLR Chapter 8: Multilevel Models\n\nBeyond Multiple Linear Regression, Chapter 8: Introduction to Multilevel Models\nVariable Types\n\nLevel One: variables measured at the most frequently occurring observational unit\n\ni.e. Variables that (for the most part) have different values for each row\ni.e. Vary for each repeated measure of a subject and vary between subjects\n\nLevel Two: variables measured on larger observational units\n\ni.e. Constant for each repeated measure of a subject but vary between each subject\n\n\nData Description in the Examples\n\nna (outcome) - Negative Affect (i.e. Performance Anxiety)\nlarge - binary\n\nlarge = 1 means the performance type is a Large Ensemble\nlarge = 0 means the performance type is either a Solo or Small Ensemble\n\norch - binary\n\norch = 1 means the instrument is Orchestral\norch = 0 means the instrument is either a Keyboard or it’s Vocal\n\n\nTwo-Stage Model\n\nThe 2-stage model shows a clearer depiction of how a multi-level model is fit in principle\n\n2-stage model uses Ordinary Least Squares to fit a system of regression equations in stages\nThe multi-level model fits a composite of that system of equations using Maximum Likelihood Estimation\n\nProcess\n\nLevel 1 models are fitted for each subject/unit\nUsing the estimated level 1 intercepts and slopes as outcome variables, Level 2 intercept and slope models respectively are fit\nThe errors from the Level 2 models are the random effects\n\nIssues\n\nWeights every subject the same regardless of the number of repeated observations\nResponds to missing individual slopes (i.e. subjects never exposed to treatment) by simply dropping those subjects\nDoes not share strength effectively across subjects\n\nSpecification\n\nLevel 1\n\\[\nY_{ij} = a_i + b_i \\text{large}_{ij} + \\epsilon_{ij}\n\\]\n\n\\(b_i\\) is the fixed effect for subject/unit \\(i\\) and observation \\(j\\) (i.e. each subject has repeated observations)\n\nLevel 2\n\\[\na_i = \\alpha_0 + \\alpha_1 \\text{orch}_i + u_i \\\\\nb_i = \\beta_0 + \\beta_1 \\text{orch}_i + v_i\n\\]\n\n\\(u_i\\) and \\(v_i\\) are the random effects for subject/unit \\(i\\)\n\\(a_i\\) is the true (i.e. not estimated since no hat) mean of performance anxiety (outcome) when subject plays solos or small ensembles (large = 0)\n\\(b_i\\) is the true mean difference in performance anxiety (outcome) for subjecti between large ensembles and other performance types (large contrast)\n\n\n\n\n\nMulti-Level\n\nProcess\n\nLevel 1 and Level 2 equations have been combined through substitution then reduced into a composite model\nParameters estimated through Maximum Likelihood Estimation\n\nMisc\n\nError Distribution\n\nCorrelation between parameters is accounted for using a multivariate normal distribution estimated through a variance-covariance matrix of the error terms (aka random effects) of each Level (see link for details)\n\nAdding Level 1 variables to a model formula should change within-person variability (\\(\\hat\\sigma^2\\) and \\(\\hat\\sigma\\))\n\nEven without adding a Level 1 variable, small changes could occur due to numerical estimation procedures used in likelihood-based parameter estimates\n\nOptimization methods\n\nUsually very little difference between ML and REML parameter estimates\nRestricted Maximum Likelihood (REML) is preferable when the number of parameters is large or the primary interest is obtaining estimates of model parameters, either fixed effects or variance components associated with random effects\nMaximum Likelihood (MLE) should be used if nested fixed effects models are being compared using a likelihood ratio test, although REML is fine for nested models of random effects that have the same fixed effects.\n\nP-Values\n\nCan’t be calculated because the exact distribution of the test statistics under the null hypothesis (no fixed effect) is unknown, primarily because the exact degrees of freedom is not known\nRule of Thumb: t-values (ratios of parameter estimates to estimated standard errors) with absolute value above 2 indicate significant evidence that a particular model parameter is different than 0\nPackages that do report p-values of fixed effects typically using conservative assumptions, large-sample results, or approximate degrees of freedom for a t-distribution\n\nUsing {lmerTest} will produce a summary of {lme4} with pvals for coefficients\n\nParametric Bootstrap can be used to approximate the distribution of the likelihood test statistic and produce more accurate p-values by simulating data under the null hypothesis\n\nModel Comparison\n\nPseudo-R2\n\nSee below in Simple to Complex Model Building Workflow &gt;&gt;\n\nRandom Slopes and Intercepts Model (with 1 covariate)\nRandom Slopes and Intercepts Model (with 2 covariates)\n\n\nAIC, BIC\n\nSee Random Intercepts (with 2 covariates)\n\nLikelihood Ratio Test (LR-Test)\n\nSee Random Slopes and Intercepts Model (with 3 covariates)\n\n\n\nComposite Specification\n\\[\nY_{ij} = [\\alpha_0 + \\alpha_1 \\text{orch}_i + \\beta_0 \\text{large}_{ij} + \\beta_1 \\text{orch}_i \\text{large}_{ij}] + [u_i + v_i \\text{large}_{ij} + \\epsilon_{ij}]\n\\]\n\nComposite model of level 1 and level 2 equations\nRandom Slopes and Intercepts Model (with 2 covariates)\nFixed Effects: \\(\\alpha_0\\), \\(\\alpha_1\\), \\(\\beta_0\\) and \\(\\beta_1\\)\n\\(u_i + v_i \\cdot \\text{large}_{ij} + \\epsilon_{ij}\\) is the interesting part.\n\nThis part has all the error terms and any variables in the Level 2 equations\nThe first part is a typical regression with interaction terms.\n\n\nInterpretation\n\nSee Two-stage model for definitions for \\(a_i\\) and \\(b_i\\)\n\nKeyboardists and Vocalists (orchi = 0)\n\\[\n\\begin{align}\na_i &= \\alpha_0 + u_i \\\\\nb_i &= \\beta_0 + v_i\n\\end{align}\n\\]\nOrchestral Instrumentalists (orchi = 1)\n\\[\n\\begin{align}\na_i = (\\alpha_0 + \\alpha_1) + u_i \\\\\nb_i = (\\beta_0 + \\beta_1) + v_i\n\\end{align}\n\\]\n\nMean Performance Anxiety (outcome) when:\n\nKeyboardists or Vocalists (orch = 0) play solos or small ensembles (large = 0): \\(\\alpha_0\\)\nKeyboardists or vocalists (orch = 0) play large ensembles (large = 1): \\(\\alpha_0 + \\beta_0\\)\nOrchestral instrumentalists (orch = 1) play solos or small ensembles (large = 0): \\(\\alpha_0 + \\alpha_1\\alpha_0 + \\alpha_1\\)\nOrchestral instrumentalists (orch = 1) play large ensembles (large = 1): \\(\\alpha_0 + \\alpha_1 + \\beta_0 + \\beta_1\\)\n\n\nModel Summary (using lmer4::lmer())\n#&gt;     Linear mixed model fit by REML ['lmerMod'] \n#&gt; A)  Formula: na ~ orch + large + orch:large + (large | id) \n#&gt;         Data: music \n#&gt; B)  REML criterion at convergence: 2987 \n\n#&gt; B2)      AIC      BIC  logLik deviance df.resid \n#&gt;         3007    3041    -1496    2991      489 \n\n#&gt;     Random effects: \n#&gt;       Groups  Name        Variance Std.Dev. Corr \n#&gt; C)  id      (Intercept)    5.655   2.378         \n#&gt; D)            large        0.452   0.672   -0.63 \n#&gt; E)  Residual              21.807   4.670         \n#&gt; F)  Number of obs: 497, groups:  id, 37 \n\n#&gt;     Fixed effects: \n#&gt;                 Estimate Std. Error t value \n#&gt; G)  (Intercept)  15.930      0.641  24.83 \n#&gt; H)  orch          1.693      0.945   1.79 \n#&gt; I)  large        -0.911      0.845  -1.08 \n#&gt; J)  orch:large   -1.424      1.099  -1.30\n\nDefinitions\n\nA: How our multilevel model is written in R, based on the composite model formulation.\nB: Measures of model performance. Since this model was fit using REML, this line only contains the REML criterion.\nB2: If the model is fit with ML instead of REML, the measures of performance will contain AIC, BIC, deviance, and the log-likelihood.\nC: Estimated variance components (\\(\\hat \\sigma^2_u\\) and \\(\\hat \\sigma_u\\)) associated with the intercept equation in Level Two. (between-unit variability)\nD: Estimated variance components (\\(\\hat \\sigma^2_v\\) and \\(\\hat \\sigma_v\\)) associated with the large ensemble (large = 1) effect equation in Level Two. (Also between-unit variability but just for the slope)\n\nAlso, in the “Corr” column, the estimated correlation (\\(\\hat \\rho_{uv}\\)) between the two Level Two error terms.\n\nE: Estimated variance components (\\(\\hat \\sigma^2\\) and \\(\\hat \\sigma\\) associated with the Level One equation. (within-unit variability)\nF: Total number of performances where data was collected (Level One observations = 497) and total number of subjects (Level Two observations = 37).\nG: Estimated fixed effect (\\(\\hat \\alpha_0\\)) for the intercept term, along with its standard error and t-value (which is the ratio of the estimated coefficient to its standard error).\nH: Estimated fixed effect (\\(\\hat \\alpha_1\\)) for the orchestral instrument (orch = 1) effect, along with its standard error and t-value.\nI: Estimated fixed effect (\\(\\hat \\beta_0\\)) for the large ensemble (large = 1) effect, along with its standard error and t-value.\nJ: Estimated fixed effect (\\(\\hat \\beta_1\\)) for the interaction between orchestral instruments (orch = 1) and large ensembles (large = 1), along with its standard error and t-value.\n\nInterpretations\n\nFixed effects:\n\n\\(\\hat \\alpha_0 = 15.9\\) — The estimated mean performance anxiety (outcome) for solos and small ensembles (large = 0) for keyboard players and vocalists (orch = 0) is 15.9.\n\\(\\hat \\alpha_1 = 1.7\\) — Orchestral instrumentalists (orch = 1) have an estimated mean performance anxiety (outcome) for solos and small ensembles (large = 0) which is 1.7 points higher than keyboard players and vocalists (orch = 0).\n\\(\\hat \\beta_0 = −0.9\\) — Keyboard players and vocalists (orch = 0) have an estimated mean decrease in performance anxiety (outcome) of 0.9 points when playing in large ensembles (large = 1) instead of solos or small ensembles (large = 0).\n\\(\\hat \\beta_1 = −1.4\\) — Orchestral instrumentalists (orch = 1) have an estimated mean decrease in performance anxiety (outcome) of 2.3 points when playing in large ensembles (large = 1) instead of solos or small ensembles (large = 0), 1.4 points greater than the mean decrease among keyboard players and vocalists (orch = 0).\n\nHe’s calculating simple slope/marginal effect (See Regression, Interactions) and choosing to use that as the interpretation for the interaction term\n\nFrom Ch. 1 (link): “We interpret the coefficient for the interaction term by comparing slopes under fast and non-fast conditions; this produces a much more understandable interpretation for a reader than attempting to interpret the -0.011 (interaction coef) directly”\n\nFast and non-fast are levels of a main effect and a term in the interaction.\n\n\nCalculation: The interaction is the difference in slopes for one interaction variable (e.g. large) at different values of the other interaction variable (e.g. orch)\n\nRegression eq (ignoring the random effects part):\n\n\\(\\hat Y_i = 15.93 + 1.693\\text{orch}_i - 0.911\\text{large} − 1.424\\text{orch}_i \\times \\text{large}_i\\)\n\nExamining the large slope so only dealing with terms that contain “large”\n\nlarge = 1 is reference category in the interaction that the coefficient is associated with so it remains constant\n\nWhen orch = 0 and large = 1, the slope for large is -0.911 = \\(\\hat \\beta_0\\)\n\n\\(- 0.911\\text{large} − 1.424\\text{orch}_i \\times \\text{large}_i = -0.911 \\times 1 - 1.424 \\times \\boldsymbol{0} \\times 1 = -0.911 = \\hat \\beta_0\\)\n\nWhen orch = 1 and large = 1, the slope for large is -2.335 (uses this one for his interpretation)\n\n\\(- 0.911\\text{large} − 1.424\\text{orch}_i \\times \\text{large}_i = -0.911 \\times 1 - 1.424 \\times \\boldsymbol{1} \\times 1 = -0.911 - 1.424 = -2.335\\) (i.e. “decrease… of 2.3 points”)\n\nThe difference in these slopes is the interaction coefficient: \\(-2.335 - (-0.911) = -1.424\\)\n\n\n\nVariance components\n\n\\(\\hat \\sigma_u = 2.4\\) — The estimated standard deviation of performance anxiety (outcome) for solos and small ensembles (large = 0) is 2.4 points, after controlling for instrument (orch) played.\n\\(\\hat \\sigma_v = 0.7\\) — The estimated standard deviation of differences in performance anxiety (outcome) between large ensembles (large = 1) and other performance types (large = 0) is 0.7 points, after controlling for instrument (orch) played.\n\\(\\hat \\rho_{uv} = −0.64\\) — The estimated correlation between performance anxiety scores (outcome) for solos and small ensembles (large = 0) and increases in performance anxiety (outcome) for large ensembles (large = 1) is -0.64, after controlling for instrument (orch) played.\n\nThose subjects with higher performance anxiety scores for solos and small ensembles tend to have greater decreases in performance anxiety for large ensemble performances.\n\n\\(\\hat \\sigma = 4.7\\) — The estimated standard deviation in residuals for the individual regression models is 4.7 points.\n\n\n\n\n\n\nModel Building Workflow: Simple to Complex\n\nWorkflow should include:\n\nFixed effects that allow one to address primary research questions\nFixed effects that control for important covariates at all levels\nInvestigation of potential interactions\nVariables that are centered where interpretations can be enhanced\nImportant variance components\nRemoval of unnecessary terms\nA final model that tells a “persuasive story parsimoniously”\n\nModel Comparison (Also see multi-level &gt;&gt; misc &gt;&gt; p-values & model comparison)\n\nLR-Test for nested models\nAIC, BIC for unnested models\nConsider models that involve removing terms that have t-stats &lt; |2|\n\nModels\n\nUnconditional Means (aka Random Intercepts)\nRandom Slopes and Intercepts Model (with 1 covariate)\nRandom Slopes and Intercepts Model (with 2 covariates)\nRandom Intercepts (with 2 covariates)\nRandom Slopes and Intercepts Model (with 3 covariates)\nFinal Model\n\n\n\nUnconditional Means (aka Random Intercepts)\n\nSpecification\n\nLevel 1: \\(Y_{ij} = a_i + \\epsilon_{ij} \\quad \\text{where}\\: \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\)\n\n\\(a_i\\) is the true mean response of all observations for subjecti\n\nLevel 2: \\(a_i = \\alpha_0 + u_i \\quad \\text{where} \\: u_i \\sim \\mathcal{N}(0, \\sigma^2_u)\\)\n\nEach subject’s intercept is assumed to be a random value from a normal distribution centered at \\(\\alpha_0\\) with variance \\(\\sigma^2_u\\).\n\nComposite: \\(Y_{ij} = \\alpha_0 + u_i + \\epsilon_{ij}\\)\n\nModel\n#Model A (Unconditional means model)\nmodel.a &lt;- lmer(na ~ 1 + (1 | id), REML = T, data = music)\n##  Groups  Name        Variance Std.Dev.\n##  id      (Intercept)  4.95    2.22   \n##  Residual            22.46    4.74\n##  Number of Level Two groups =  37\n##            Estimate Std. Error t value\n## (Intercept)    16.24    0.4279  37.94\nInterpretation\n\n\\(\\hat \\alpha_0 = 16.2\\) — The estimated mean performance anxiety score across all performances and all subjects.\n\\(\\hat \\sigma^2 = 22.5\\) — The estimated variance in within-person deviations.\n\\(\\hat \\sigma^2_u = 5.0\\) (rounded from 4.95) — The estimated variance in between-person deviations.\nICC\n\\[\n\\hat \\rho = \\frac{\\text{between-person variability}}{\\text{total variability}} = \\frac{\\hat \\sigma^2_u}{\\hat \\sigma^2_u + \\hat \\sigma ^2} = \\frac{5.0}{5.0 + 22.5} = 0.182\n\\]\n\n18.2% of the total variability in performance anxiety scores are attributable to differences among subjects.\n\n*For plain random intercepts models only*, you can also say this same number says that the average correlation for any pair of responses from the same individual is a moderately low 0.182.\n\nAs \\(\\rho\\) approaches 0: responses from an individual are essentially independent and accounting for the multilevel structure of the data becomes less crucial.\nAs \\(\\rho\\) approaches 1: repeated observations from the same individual essentially provide no additional information and accounting for the multilevel structure becomes very important.\nEffective Sample Size (ESS): The number of independent pieces of information we have for modeling\n\nWith \\(\\rho\\) near 0: ESS approaches the total number of observations.\nWith \\(\\rho\\) near 1: ESS approaches the number of subjects in the study.\n\n\n\n\n\n\nRandom Slopes and Intercepts Model (with 1 covariate)\n\n1 - Level 1\nSpecification\n\nLevel 1: \\(Y_{ij} = a_i + b_i \\text{large}_{ij} + \\epsilon_{ij}\\)\nLevel 2\n\\[\na_i = \\alpha_0 + u_i \\\\\nb_i = \\beta_0 +v_i\n\\]\nComposite\n\\[\n\\begin{aligned}\n&Y_{ij} = [\\alpha_0 + \\beta_0 \\text{large}_{ij}] + [u_i + v_i \\text{large}_{ij} + \\epsilon_{ij}] \\\\\n&\\begin{aligned}\n\\text{where}\\quad \\epsilon &\\sim \\mathcal{N}(0, \\sigma^2) \\: \\text{and}\\\\\n\\left[ \\begin{array}{cc} u_i \\\\ v_i \\end{array} \\right] &\\sim \\mathcal{N} \\left(\\left[\\begin{array}{cc} 0\\\\0 \\end{array}\\right], \\left[\\begin{array}{cc} \\sigma^2_u\\\\\\rho\\sigma_u \\sigma_v \\quad \\sigma^2_v \\end{array}\\right]\\right)\n\\end{aligned}\n\\end{aligned}\n\\]\n\nModel\nmodel.b &lt;- lmer(na ~ large + (large | id), data = music)\n##  Groups  Name        Variance Std.Dev. Corr \n##  id      (Intercept)  6.333  2.517         \n##          large        0.743  0.862    -0.76\n##  Residual            21.771  4.666\n##  Number of Level Two groups =  37\n##            Estimate Std. Error t value\n## (Intercept)  16.730    0.4908  34.09\n## large        -1.676    0.5425  -3.09\nInterpretation\n\n\\(\\hat \\alpha_0 = 16.7\\) — The mean performance anxiety level (outcome) before solos and small ensemble performances (large = 0).\n\\(\\hat \\beta_0 = −1.7\\) — The mean decrease in performance anxiety (outcome) before large ensemble performances (large = 1).\n\nSubjects had a performance anxiety level of 16.7 before solos and small ensembles, and their anxiety levels were 1.7 points lower, on average, before large ensembles, producing an average performance anxiety level before large ensembles of 15.0\nStatistically significant since |t-value| &gt; 2\n\n\\(\\hat \\sigma^2 = 21.8\\) — The variance in within-person deviations.\n\\(\\hat \\sigma^2_u = 6.3\\) — The variance in between-person deviations in performance anxiety scores (outcome) before solos and small ensembles (large = 0).\n\\(\\hat \\sigma^2_v = 0.7\\) — The variance in between-person deviations in increases (or decreases) (slope) in performance anxiety scores (outcome) before large ensembles (large = 1).\n\\(\\hat \\rho_{uv} = −0.76\\) (Corr column)\n\nSlopes and intercepts are negatively correlated\nA strong negative relationship between a subject’s performance anxiety (outcome) before solos and small ensembles (large = 0) (Intercept) and their (typical) decrease in performance anxiety (Slope) before large ensembles (large = 1) .\n\n\nPseudo-R2 (Not always a reliable performance measure)\n\nUnconditional Means vs Random Intercepts and Slopes\n\\[\n\\text{Psuedo R}^2_{L1} = \\frac{\\hat \\sigma^2 (\\text{Model A}) - \\hat\\sigma^2(\\text{Model B})}{\\hat \\sigma^2(\\text{Model A})} = \\frac{22.5 - 21.8}{22.5} = 0.031\n\\]\n\nWhere Model A: Unconditional Means; Model B: Random intercepts and slopes\nFYI\n\n\\(\\text{percent decrease} = \\frac{\\text{original value} - \\text{new value}}{\\text{orginal value}}\\)\n\npseudo-R2 uses this one\nIf negative, it’s a percent increase.\n\n\\(\\text{percent increase} = \\frac{\\text{new value} - \\text{original value}}{\\text{original value}}\\)\n\nIf negative, it’s a percent decrease.\n\n\n\nPositive value means Model B is an improvement over Model A\n\nMeans the variance in Model B’s error terms is smaller than Model A’s, and therefore better fits the data. (I think)\n\nThe estimated within-person variance \\(\\hat \\sigma^2\\) decreased by 3.1% (from 22.5 to 21.8) from the unconditional means model\nImplies that only 3.1% of within-person variability in performance anxiety scores can be explained by performance type\nValues of \\(\\hat \\sigma^2_u\\) and \\(\\hat \\sigma^2_v\\) from Model B cannot be compared to between-person variability from Model A using pseudo-R2, since the inclusion of performance type has changed the interpretation of these values\nIssues\n\n“Because of the complexity of estimating fixed effects and variance components at various levels of a multilevel model, it is not unusual to encounter situations in which covariates in a Level Two equation for the intercept (for example) remain constant (while other aspects of the model change), yet the associated pseudo R-squared values differ or are negative.” (?)\n\n\n\n\n\nRandom Slopes and Intercepts Model (with 2 covariates)\n\n1 - Level 1; 1 - Level 2\nSpecification, Model, Interpretation (see Multi-Level section above)\n\nSays the large effect (slope) varies across subjects/units\n\nPseudo-R2\n\\[\n\\begin{align}\n\\text{Psuedo R}^2_{L2_u} &= \\frac{\\hat \\sigma^2_u (\\text{Model B}) - \\hat\\sigma^2_u(\\text{Model C})}{\\hat \\sigma^2_u(\\text{Model B})} = \\frac{6.33 - 5.66}{6.33} = 0.106 \\\\\n\\text{Psuedo R}^2_{L2_v} &= \\frac{\\hat \\sigma^2_v (\\text{Model A}) - \\hat\\sigma^2_v(\\text{Model B})}{\\hat \\sigma^2_v(\\text{Model A})} = \\frac{0.74 - 0.45}{0.74} = 0.392\n\\end{align}\n\\]\n\nModel B is Random Slopes and Intercepts Model (with 1 covariate)\nModel C is Random Slopes and Intercepts Model (with 2 covariates)\nThe addition of Level 2 variable, orch, in Model C:\n\n(Top) Decreased the between-person variability in mean (intercept) performance anxiety (outcome) before solos and small ensembles (large = 0) by 10.6%\n(Bottom) Decreased the between-person variability in the effect (slope) of large ensembles (large = 1) on performance anxiety (outcome) by 39.2%\n\n\n\n\n\nRandom Intercepts (with 2 covariates)\n\n1 - Level 1; 1 - Level 2\nInstead of assuming that the large ensemble (large) effects, after accounting for instrument played (orch), vary by individual, we are assuming that large ensemble effect is fixed across subjects.\nSpecification\n\nLevel 1: \\(Y_{ij} = a_i + b_i \\text{large}_{ij} + \\epsilon_{ij}\\)\nLevel 2\n\\[\n\\begin{align}\n&a_i = \\alpha_0 + \\alpha_1 \\text{orch}_i + u_i\\\\\n&b_i = \\beta_0 + \\beta_1 \\text{orch}_i \\\\\n&\\text{where}\\: \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\: \\text{and} \\: u_i \\sim \\mathcal{N}(0, \\sigma^2_u)\n\\end{align}\n\\]\n\nOnly difference is the random effect/error term, \\(v_i\\), isn’t specified\n\nComposite\n\nNot given, but probably very similar to the previous model just without \\(v_i\\)\n\n\nModel\nmodel.c2 &lt;- lmer(na ~ orch + large + orch:large + (1|id), data = music)\n##  Groups  Name        Variance Std.Dev.\n##  id      (Intercept)  5.13    2.27   \n##  Residual            21.88    4.68\n##  Number of Level Two groups =  37\n##            Estimate Std. Error t value\n## (Intercept)  15.9026    0.6187  25.703\n## orch          1.7100    0.9131   1.873\n## large        -0.8918    0.8415  -1.060\n## orch:large   -1.4650    1.0880  -1.347\nInterpretation\n\nSame as previous model except there’s no between-units variation estimate for the slope (see previous model and multi-level model section)\nEstimates are similar to the previous model’s. Largest difference seems to be the between-unit variation for the Intercept (5.66 vs 5.13)\n\nComparison\n        df  AIC\nmodel.c  8 3003\nmodel.c2 6 2999\n        df  BIC\nmodel.c  8 3037\nmodel.c2 6 3025\n\nModel C is the previous model and Model C2 is this model\nBoth criterion pick this model to predict best\nThe more complex model (e.g. varying intercepts and varying slopes) doesn’t always perform best\n\n\n\n\nRandom Slopes and Intercepts Model (with 3 covariates)\n\n1 - Level 1; 2 - Level 2\nMPQnem has been centered (mean = 31.63)\n\nOtherwise some interpretations would involve MPQnem = 0 when the minimum score is 11 which would make the interpretation nonsensical\n\nSpecification\n\nLevel 1: \\(Y_{ij} = a_i + b_i \\text{large}_{ij} + \\epsilon_{ij}\\)\nLevel 2\n\\[\n\\begin{align}\na_i = \\alpha_0 + \\alpha_1 \\text{orch}_i + \\alpha_2 \\text{MPQnem}_i + u_i \\\\\nb_i = \\beta_0 + \\beta_1 \\text{orch}_i + \\beta_2 \\text{MPQnem}_i + v_i\n\\end{align}\n\\]\nComposite\n\\[\nY_{ij} = [\\alpha_0 + \\alpha_1 \\text{orch}_i + \\alpha_2 \\text{mpqnem}_i + \\beta_0 \\text{large}_{ij} + \\beta_1 \\text{orch}_i \\text{large}_{ij} + \\beta_2 \\text{mpqnem}_i \\text{large}_{ij}] + [u_i + v_i \\text{large}_{ij} + \\epsilon_{ij}]\n\\]\n\nModel\nmodel.d &lt;- lmer(na ~ orch + mpqnem + large +\n                    orch:large + mpqnem:large +\n                    (large | id),\n                    data = music)\n##  Groups  Name        Variance Std.Dev. Corr \n##  id      (Intercept)  3.286  1.813         \n##          large        0.557  0.746    -0.38\n##  Residual            21.811  4.670\n##  Number of Level Two groups =  37\n##               Estimate Std. Error t value\n## (Intercept)   16.25679    0.54756 29.6893\n## orch           1.00069    0.81713  1.2246\n## cmpqnem        0.14823    0.03808  3.8925\n## large         -1.23484    0.84320 -1.4645\n## orch:large    -0.94927    1.10620 -0.8581\n## cmpqnem:large -0.03018    0.05246 -0.5753\nInterpretation\n\nCompared to Random Slopes and Intercepts Model (with 2 covariates):\n\nDirections of the effects of instrument (orch) and performance type (large) are consistent\nEffect sizes and levels of significance are reduced because of the relative importance of the negative emotionality (mpqnem) term\n\n\\(\\alpha_0 = 16.26\\) — The estimated mean performance anxiety (outcome) for solos and small ensembles (large = 0) is 16.26 for keyboard players and vocalists (orch=0) with an average level of negative emotionality at baseline (mpqnem = 31.63)\n\\(\\hat \\alpha_1 = 1.00\\) — Orchestral instrument (orch = 1) players have an estimated mean performance anxiety (outcome) level before solos and small ensembles (large = 0) which is 1.00 point higher than keyboardists and vocalists (orch = 0), controlling for the effects of baseline negative emotionality.\n\\(\\hat \\sigma^2 = 0.15\\) — A one point increase in baseline negative emotionality (mpqnem = 0) is associated with an estimated 0.15 mean increase in performance anxiety (outcome) levels before solos and small ensembles (large = 0), after controlling for instrument (orch).\n\\(\\hat \\beta_0 = −1.23\\) — Keyboard players and vocalists (orch = 0) with an average level of baseline negative emotionality levels (mpqnem = 31.63) have an estimated mean decrease in performance anxiety level (outcome) of 1.23 points before large ensemble performances (large = 1) compared to other performance types (large = 0).\n\nSee multi-level&gt;&gt; interpretation for explainer on the interaction slope interpretations\n\n\\(\\hat \\beta_1 = −0.95\\) — After accounting for baseline negative emotionality (mpqnem = 0), orchestral instrument players (orch = 1) have an estimated mean performance anxiety level (outcome) before solos and small ensembles (large = 0) which is 1.00 point higher than keyboardists and vocalists (orch = 0), while the mean performance anxiety (outcome) of orchestral players (orch = 1) is only .05 points higher before large ensembles (large = 1) (a difference of .95 points).\n\nSee multi-level &gt;&gt; interpretation for explainer on the interaction slope interpretations\n\n\\(\\hat \\beta_2 = −0.03\\) — After accounting for instrument, a one-point increase in baseline negative emotionality is associated with an estimated 0.15 mean increase in performance anxiety levels before solos and small ensembles, but only an estimated 0.12 increase before large ensembles (a difference of .03 points).\n\nLR-Test for comparing nested models\ndrop_in_dev &lt;- anova(model.d, model.c, test = \"Chisq\")\n#&gt;         npar  AIC  BIC logLik  dev Chisq Df      pval\n#&gt; model.c    8 3007 3041  -1496 2991    NA NA        NA\n#&gt; model.d   10 2996 3039  -1488 2976 14.73  2 0.0006319\n\nMLE must be used instead of REML to estimate the parameters of each model\nmodel.d is the current and more complex model, Random Slopes and Intercepts Model (with 3 covariates)\nmodel.c is the less complex model, Random Slopes and Intercepts Model (with 2 covariates)\nProcess\n\nThe likelihood is larger (and the log-likelihood is less negative) under the larger model (Model D);\n\\(\\text{Chisq} = 14.734 = -2 \\cdot (-1488 - (-1496))\\)\nUsing dof = 2, signifying the number of additional terms in Model D, we obtain a p-value of .0006.\n\nA p-value &lt; 0.05 indicates the difference in log-likelihoods is significantly different from 0, and therefore the more complex model, model.d, fits the data better.\nNote: dropping the mpqnem:large term which has a t-stat &lt; |2| (-0.5753) produces a better model than model.d according the LR-test\n\n\n\n\nFinal Model\n\nDescription\n\nOne of other valid final models\nPerformance type categorical is no longer collapsed into the binary “large” ensemble/not large and is has now been collapsed into the binary, solo, not solo\nAudience categorical has been transformed to dummies: students, juried, public with instructor as the reference category.\nVarying/random slopes for previous, students, juried, public, and solo\nmpqnem is in the solo level 2 equation, so the combined model has an interaction between the two in the fixed effects.\n\nSpecification\n\nLevel 1: \\(Y_{ij} = a_i + b_i \\text{previous}_{ij} + c_i \\text{students}_{ij} + d_i \\text{juried}_{ij} + e_i \\text{public}_{ij} + f_i \\text{solo}_{ij} + \\epsilon_{ij}\\)\nLevel 2\n\\[\n\\begin{align}\na_i &= \\alpha_0 + \\alpha_1 \\text{mpqpem}_i + \\alpha_2 \\text{mpqab}_i + \\alpha_3 \\text{orch}_i + \\alpha_4 \\text{mpqnem}_i + u_i \\\\\nb_i &= \\beta_0 + v_i \\\\\nc_i &= \\gamma_0 + w_i \\\\\nd_i &= \\delta_0 + x_i \\\\\ne_i &= \\epsilon_0 + y_i \\\\\nf_i &= \\zeta_0 + \\zeta_1 \\text{mpqnem}_i + z_i\n\\end{align}\n\\]\n\nVariance-Covariance matrix\n\\[\n\\left[\\begin{array}{cc} u_i\\\\v_i\\\\w_i\\\\x_i\\\\y_i\\\\z_i \\end{array} \\right]\n\\sim \\mathcal{N} \\left(\n\\left[\\begin{array}{cc} 0\\\\0\\\\0\\\\0\\\\0\\\\0\\end{array} \\right],\n\\begin{bmatrix}\n\\sigma_u^2 \\\\\n\\sigma_{uv} & \\sigma_v^2 \\\\\n\\sigma_{uw} & \\sigma_{vw} & \\sigma^2_w \\\\\n\\sigma_{ux} & \\sigma_{vx} & \\sigma_{wx} & \\sigma_x^2 \\\\\n\\sigma_{uy} & \\sigma_{vy} & \\sigma_{wy} & \\sigma_{xy} & \\sigma_{y}^2 \\\\\n\\sigma_{uz} & \\sigma_{vz} & \\sigma_{wz} & \\sigma_{xz} & \\sigma_{yz} & \\sigma_z^2\n\\end{bmatrix}\n\\right)  \n\\]\n6 variance terms and 15 correlation terms at Level Two, along with 1 variance term at Level One.\n\nThe number of correlation terms is equal to the number of unique pairs among Level Two random effects\n\n\n\nModel\n# Model F (One - of many - reasonable final models)\nmodel.f &lt;- lmer(na ~ previous + students + juried + \n    public + solo + mpqpem + mpqab + orch + mpqnem + \n    mpqnem:solo + (previous + students + juried + \n    public + solo | id), REML = T, data = music)\n\n##  Groups  Name        Variance Std.Dev. Corr       \n##  id      (Intercept) 14.4802  3.805               \n##          previous     0.0707  0.266    -0.65     \n##          students     8.2151  2.866    -0.63  0.00\n##          juried      18.3177  4.280    -0.64 -0.12\n##          public      12.8094  3.579    -0.83  0.33\n##          solo         0.7665  0.876    -0.67  0.47\n##  Residual            15.2844  3.910     \n##             \n##  0.84           \n##  0.66  0.58     \n##  0.49  0.21  0.90\n## \n##  Number of Level Two groups =  37\n##            Estimate Std. Error t value\n## (Intercept)  8.36883    1.91369  4.3731\n## previous    -0.14303    0.06247 -2.2895\n## students     3.61115    0.76796  4.7022\n## juried       4.07332    1.03130  3.9497\n## public       3.06453    0.89274  3.4327\n## solo         0.51647    1.39635  0.3699\n## mpqpem      -0.08312    0.02408 -3.4524\n## mpqab        0.20377    0.04740  4.2986\n## orch         1.53138    0.58384  2.6230\n## mpqnem       0.11465    0.03591  3.1930\n## solo:mpqnem  0.08296    0.04158  1.9951\nInterpretation\n\nIn general\n\nPerformance anxiety (outcome) is higher when a musician is performing in front of students, a jury, or the general public rather than their instructor\n\nstudents, juried, and public are indicator variables created from the audience categorical variable (so that “Instructor” is the reference level in this model)\n\nPerformance anxiety (outcome) is lower for each additional diary the musician previously filled out\n\nprevious is the number of previous diary entries filled out by that individual\n\nMusicians with lower levels of positive emotionality (mpqpem) and higher levels of absorption (mpqab) tend to experience greater performance anxiety (outcome)\nThose who play orchestral instruments experience (orch = 1) more performance anxiety than those who play keyboards or sing (orch = 0)\n\nKey terms\n\n\\(\\hat \\alpha_4 = 0.11\\) (mpqnem, fixed) — A one-point increase in baseline level of negative emotionality (mpqnem) is associated with an estimated 0.11 mean increase in performance anxiety (outcome) for musicians performing in an ensemble group (solo=0), after controlling for previous diary entries (previous), audience (dummy vars), positive emotionality (mpqpem), absorption (mpqab), and instrument (orch).\n\\(\\hat \\zeta_1 = 0.08\\) (solo:mpqnem, fixed) — When musicians play solos (solo = 1), a one-point increase in baseline level of negative emotionality (mpqnem) is associated with an estimated 0.19 mean increase in performance anxiety (outcome), 0.08 points (73%) higher than musicians playing in ensemble groups (solo = 0), controlling for the effects of previous diary entries (previous), audience (dummy vars), positive emotionality (mpqpem), absorption (mpqab), and instrument (orch).\n\nSee multi-level &gt;&gt; interpretation for explainer on the interaction slope interpretations\n\n\nAddressing the researchers’ primary hypothesis:\n\nAfter controlling for all these factors, we have significant evidence that musicians with higher levels of negative emotionality (mpqpem) experience higher levels of performance anxiety (outcome), and that this association is even more pronounced (interaction) when musicians are performing solos (solo = 1) rather than as part of an ensemble group (solo = 0).",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-bmlrch9",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-bmlrch9",
    "title": "Mixed Effects",
    "section": "BMLR Chapter 9: Longitudinal Data",
    "text": "BMLR Chapter 9: Longitudinal Data\n\nBeyond Multiple Linear Regression, Chapter 9: Two-Level Longitudinal Data\nResearch Questions\n\nWhich factors most influence a school’s performance in Minnesota Comprehensive Assessment (MCA) testing?\nHow do the average math MCA-II scores for 6th graders enrolled in charter schools differ from scores for students who attend non-charter public schools? Do these differences persist after accounting for differences in student populations?\nAre there differences in yearly improvement between charter and non-charter public schools?\n\nData Answers\n\nWithin school—changes over time\nBetween schools—effects of school-specific covariates (charter or non-charter, urban or rural, percent free and reduced lunch, percent special education, and percent non-white) on 2008 math scores and rate of change between 2008 and 2010.\n\nMisc\n\nMissing data is a common phenomenon in longitudinal studies",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-psa.html",
    "href": "qmd/econometrics-psa.html",
    "title": "Propensity Score Analysis",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Econometrics",
      "Propensity Score Analysis"
    ]
  },
  {
    "objectID": "qmd/econometrics-psa.html#misc",
    "href": "qmd/econometrics-psa.html#misc",
    "title": "Propensity Score Analysis",
    "section": "",
    "text": "Notes from:\n\nnyhackr meet-up Video\nVignette: PSAgraphics: An R Package to Support Propensity Score Analysis\nBook: Applied Propensity Score Analysis in R (unfinished)\n\nAny classification model should be able to produce propensity scores.\n\nExamples (code): Logistic Regression, Bayesian Logistic Regression, Probit BART, Conditional Inference Trees (partykit), Decision Trees, Random Forest.\n\nIf two people have similar propensity scores, then they will be similar in all the values of the covariates that were used to create that score\npsa::psa_simulation_shiny - App that simulates and visualizes balance of groups according effect size, sample size, estimand (e.g ATE, ATT, etc.), and PSA method\nProcess\n\n\nPhase 1\n\nSelect covariates\n\nTypically whatever variables are available if it’s a secondhand dataset\nIf running and experiment, collect data that will demonstrate baseline equivalence (?), which can later be used in a PSA\n\nChoose PSA method\nCheck Balance (i.e. do observations in treatment and control look equivalent)\nRepeat if necessary until sufficient balance is achieved\n\nPhase 2\n\nEstimate Treatment Effects\n\nPhase 3\n\nSensitivity Analysis\n\nTest how sensitive the results are to an unobserved confounder\n\ni.e. How much variance would a variable need to have before it changes the results (sign change or just significance?)\n\n\nGo back to Phase I and choose a different method\n\nTests how sensitive the results are to the PSA method chosen\n\n\n\nMethods:\n\nStratification - Treatment and Comparison (aka Control) units are divided into strata (or subclasses) based upon a propensity score (e.g. quantiles), so that treated and comparison units are similar within each strata.\n\nLogistic Regression (treatment/control ~ covariates) can be used to estimate scores (i.e. predicted logits)\n\nlr.out &lt;- \n  glm(\n  treatment ~ x1 + x2 + x3,\n  data = dat,\n  family = binomial(link = logit)\n  )\ndat$ps &lt;- fitted(lr.out)\n\n5 stratum removes &gt; 90% of the bias in estimated treatment effect (Cochran 1968)\nbreaks5 &lt;- psa::get_strata_breaks(dat$ps)\ndat$strata5 &lt;-\n  cut(\n    x = dat$ps,\n    breaks = breaks5$breaks,\n    include.lowest = TRUE,\n    labels = breaks5$labels$strata\n  )\nWith Regression, each strata has very similar numbers of observations.\nTrees, strata are determined by the leaf nodes and each strata generally differs in the numbers of observations.\n\nIn a decision tree example, strata had some funky proportions in relation to the categorical and continuous predictors (see Paper)\n\nWithin each stratum, independent sample t-tests are conducted and then pooled to provide an overall estimate\n\nMatching - Each treatment unit is paired with a comparison unit base upon the pre-treatment covariates\n\nAlgorithms\n\nPropensity Score Matching\nLimited Exact Matching\nFull Matching\nNearest Neighbor Matching\nOptimal/Generic Matching\nMahalanobis distance matching (quantitative covariates only)\nMatching with and without replacement\nOne-to-One or One-to-Many Matching\n\nChoice of algorithm is trial and error — whichever one gives the best balance.\nDependent sample tests (e.g. repeated mearsures, t-test w/paired = TRUE) are conducted using the match pairs.\nExample:\ndata(lalonde, package = \"Matching\")\nrr &lt;- \n  Matching::Match(\n    Y = lalonde$re78, \n    Tr = lalonde$treat, \n    X = lalonde$ps, \n    M = 1,\n    estimand = 'ATT',\n    ties = FALSE)\nsummary(rr)\n#&gt; \n#&gt; Estimate...  2579.8 \n#&gt; SE.........  637.69 \n#&gt; T-stat.....  4.0456 \n#&gt; p.val......  5.2189e-05 \n#? \n#&gt; Original number of observations..............  445 \n#&gt; Original number of treated obs...............  185 \n#&gt; Matched number of observations...............  185 \n#&gt; Matched number of observations  (unweighted).  185\n\nX: Variables to match on or PS\nM: how many matches per unit that you want where 1 is a 1:1 match between treatment and control\n\nFor imbalanced data, you’d want set to something higher depending on the level of imbalance between treatment and control groups.\nFor values greater than 1, units get used more than once which may not be accepted in some fields.\n\nties: For units that have multiple matches, TRUE means that the matched dataset will include the multiple control group matches with weights that reflect it (see docs for more details). FALSE means one of the multiple matches is chosen at random.\nestimand: ATE and ATC also available; Value printed in the results as Estimate.\n\nDependent Sample Assessment Plot\n\nmatches &lt;- data.frame(Treat = lalonde[rr$index.treated,'re78'],\n                      Control = lalonde[rr$index.control,'re78'])\ngranovagg::granovagg.ds(\n  matches[,c('Control','Treat')], \n  xlab = 'Treat', \n  ylab = 'Control')\n\n{granovagg} is a ggplot extension of {granova}. The package has been archived on CRAN and the github has broken links and no documentation.\nVery similar characteristics and interpretation as the Propensity Score Assessment Plot except instead of the circles which represented strata, each dot represents a matched pair between treatment and control. (Also a different color scheme)\nThe purple CI bar is difficult to spot since it’s very short, but it stradles the green dashed ATE line and doesn’t include the diagonal (i.e. zero).\n\nBalance Assessment\n\nlalonde_formu &lt;- treat ~ age + I(age^2) + educ + I(educ^2) + black +\n    hisp + married + nodegr + re74  + I(re74^2) + re75 + I(re75^2)\npsa::MatchBalance(df = lalonde, \n                  formu = lalonde_formu,\n                  formu.Y = update.formula(lalonde_formu, \n                                           re78 ~ .),\n                  M = 1, \n                  estimand = 'ATT', \n                  ties = FALSE) |&gt; \n  plot()\n\nCurrently this package doesn’t have a website or CRAN docs. See Matching::Match above for details on some of these arguments or see the R script or ?psa::MatchBalance if you have it installed.\nCharts\n\nTop Left: For 2% of the matched pairs, they only matched on 12% of the covariates.\n\nThe percent annotation was cut-off on his chart but it very likely says 88%. Since the y-axis in “% unmatched”, the percent matched is 12%.\n\nBottom-Left: A breakdown of the unmatched covariates for each individual matched pair. Each column is a matched pair. Each colored segment in that column represents unmatched covariate. Each row is a covariate which is labelled in the bar graph at the bottom-right.\n\nThe first matched pair was unmatched on the covariates: nodegr, black, re74, and hisp\nSome of the segments have a light coloring instead of dark and I’m not sure what that means.\n\nBottom-Right: The aggregated unmatched percentage for each covariate across all matched pairs\nTop-Right: Standardized Estimate for each covariate. A variable that’s CI doesn’t include 0 indicates an imbalance. (e.g. nodegr)\n\nEstimate is the mean difference from a paired t-test function that’s been standardized by dividing the estimate by the sd of the variable over the whole sample (i.e. treatment and control groups).\nUsing paired because the units have been matched.\nSo if the data has been perfectly matched for a variable, the mean difference between the treatment units and control units should be 0.\nA CI of this estimate that includes 0 means that the variable is balanced.\n\n\nYou can apply Partial Exact Matching to try bring any imbalanced variables into balance. This method allows you to be able to specify variables that want to guarantee that units to be matched upon. Doing this affects the matching upon other variables though. So, a variable that was matched before might not still be matched after using this method.\n\npsa::MatchBalance(df = lalonde, \n                  formu = lalonde_formu,\n                  formu.Y = update.formula(lalonde_formu, \n                                           re78 ~ .),\n                  M = 1, \n                  estimand = 'ATT', \n                  ties = FALSE,\n                  exact.covs = c('nodegr')) |&gt; \n  plot()\n\nYou can see nodegr has been fixed in the Top-Right error-bar chart. It also appears this has brought re75 into balance as well while not throwing the rest of the variables out of balance.\nThe x-axis range has narrowed which gives the appearance that the error bars have widened, but I don’t think they have.\n\n\n\nWeighting - Each observation is weighted by the inverse of probability of being in that group\n\nPropensity Scores are used as weights for the regression model that you’ll use to perform your analysis.\n\nThe specific weights will depend on the estimand (e.g ATE, ATT, etc.)\n\nAverage Treatment Effect (ATE): \\(\\text{ATE} = E(Y_1 - Y_0|X) = E(Y_1|X) - E(Y_0|X)\\)\n\nWeight: \\(w_{\\text{ATE}} = \\frac{Z_i}{\\pi_i} + \\frac{1-Z_i}{1-\\pi_i}\\)\n\\(Z_i = 1\\) says the unit is in the treatment group\n\\(\\pi_i\\) is the propensity score for unit i\nExample:\ndat &lt;- \n  dat |&gt; \n  mutate(\n    ate_weights = psa::calculate_ps_weights(treatment, \n                                            ps, \n                                            estimand = \"ATE\"))\n\n# via {psa}\npsa::treatment_effect(\n  treatment = dat$treatment,\n  outcome = dat$outcome,\n  weights = dat$ate_weights\n)\n# via lm\nlm(outcome ~ treatment,\n   data = dat,\n   weights = dat$ate_weights)\n\nATE Among the Treated (ATT): \\(\\text{ATT} = E(Y_1 - Y_0|X,C = 1) = E(Y_1|X,C = 1) - E(Y_)|X,C = 1)\\)\n\nWeight: \\(w_{\\text{ATT}} = \\frac{\\pi_i Z_i}{\\pi_i} + \\frac{\\pi_i(1-Z_i)}{1-\\pi_i}\\)\nExample:\ndat &lt;- \n  dat |&gt; \n  mutate(\n    att_weights = psa::calculate_ps_weights(treatment, \n                                            ps, \n                                            estimand = \"ATT\"))\n\n# via {psa}\npsa::treatment_effect(\n  treatment = dat$treatment,\n  outcome = dat$outcome,\n  weights = dat$att_weights\n)\n# via lm\nlm(outcome ~ treatment,\n   data = dat,\n   weights = dat$att_weights)\n\nATE Among the Control (ATC): \\(\\text{ATC} = E(Y_1- Y_0|X = 0) = E(Y_1|X = 0) - E(Y_0|X = 0)\\)\n\nWeight: \\(w_{\\text{ATC}} = \\frac{(1-\\pi_i)Z_i}{\\pi_i} + \\frac{(1-e_i)(1-Z_i)}{1-\\pi_i}\\)\nNot certain what \\(e_i\\) is, but it might be the residual for the unit from the propensity score model.\nExample: Same as above except with ATC weights\n\nATE Among the Evenly Matched (ATM): \\(\\text{ATM}_d = E(Y_1 - Y_0|M_d = 1)\\)\n\nWeight: \\(w_{\\text{ATM}} = \\frac{\\min\\{\\pi_i, 1-\\pi_i\\}}{Z_i \\pi_i(1-Z_i)(1-\\pi_i)}\\)\nExample: Same as above except with ADM weights\n\nShows mirrored axis histogram with PS on x-axis and count on y-axis; guessing treatment/control are top/bottom histograms?\nTreament Effect for Weighting\n\\[\n\\text{TE} = \\frac{\\sum Y_iZ_i}{\\sum Z_i w_i} - \\frac{Y_i(1-Z_i)w_i}{\\sum(1-Z_i)w_i}\n\\]\n\nThis formula is an alternative to using the weights in a model to estimate the effect.\n\n\n\nVisualization\n\nDistribution of propensity scores\n\nggplot(dat) +\n  geom_histogram(\n    data = dat[dat$treatment == 1, ],\n    aes(x = ps, y = after_stat(count)),\n    bins = 50,\n    fill = cols[2]\n  ) +\n  geom_histogram(\n    data = dat[dat$treatment == 0, ],\n    aes(x = ps, y = -after_stat(count)),\n    bins = 50,\n    fill = cols[3]\n  ) +\n  geom_hline(\n    yintercept = 0,\n    lwd = 0.5\n  ) + \n  scale_y_continuous(lable = abs)\n\nTreatment above and Control below\nTreatment is skewed negative and Control is skewed positive which makes sense, because as the propensity scores get larger (i.e. the probability of being in the treatment increases), we should see the number of treatment observations increase while the number of control observations decrease.\n\nCovariate Balance Plot\n\nPSAgraphics::cv.bal.psa(\n  dat[, 1:3],\n  data$treatment,\n  dat$ps,\n  strata = 5\n)\n\nAbsolute standardized Covariate effect sizes with (blue) and without (red) PS adjustment\nWant blue dots close to zero (0.1 is recommended) which says that after PS adjustment, the covariates have little predictive power in determining whether a unit is in the treatment group or control group. It means the PS are effectively adjusting for the selection bias of the treatment/control “assignment” in the observational data.\n\nBoxplot by Propensity Score Strata by Treatment\n\nPSAgraphics::box.psa(\n  continuous = dat$x2,\n  treatment = dat$treatment,\n  strata = dat$strata5\n)\n\nFor a continuous predictor, it allows you to visually examine the balance produced by the PS strata by comparing the distributions between treatment and control within each strata\nConnected dots are the means within each distribution and numbers below each box are its sample size.\n\nSetting trim = 0.5 will connect medians. (Range of trim is from 0 to 0.5)\n\nWithin strata, the more similar the distributions between treatment and control, the better the balance. Balance should be assessed for each predictor.\nTrends or patterns between strata can hint at potential variation and may help explain effects detected in the later performed analysis\n\nUnits in higher strata (i.e. higher PS) are more likely to be treated than those in lower strata, so looking at the values of predictor, you can say whether those with higher or lower values of the predictor are more likely to be treated.\n\nWith balance = TRUE (default is FALSE),\n\nHistogram\n\nPermuted data are randomly assigned to strata, absolute differences of means calculated within each strata, and the differences summed to produce the statistic.\nRepeated until there’s a distribution which is then visualized as a histogram\nThe sum of the mean differences of the original data is represented by a red dot.\nThe further left the red dot is from the mean (or median with trim = 0.5) of the permuation distribution, the better the balance\n\ni.e. the smaller the sum of mean differences of the original data as compared the permuted distribution of sums of mean differences, the better.\nThe rank shows where the original data statistic ranks in comparison to all of the summed differences in the permuted distribution.\n\nTotal_number_of_ranks = number_of_permutations + 1 (aka original data)\n\n\n\nBoxplot\n\nP-Values for KS-tests on treatment and control distributions are placed above the sample sizes of each strata. A lack of balance for a given strata would be indicated by a p-value &lt; 0.05.\n\n\n\nStacked Bar Chart by Propensity Score Strata by Treatment\n\nPSAgraphics::cat.psa(\n  categorical = dat$x3,\n  treatment = dat$treatment,\n  strata = dat$strat5\n)\n\nFor a categorical predictor, it allows you to visually examine the balance produced by the PS strata by comparing the category proportions between treatment and control within each strata\nWithin each strata, side-by-side segmented bars can be used to compare proportions of cases in each category\nSample sizes are above each bar\nif subsequent analysis indicates large differences in size or direction of effects for different strata, then comparing covariate distributions across strata may give an initial indication of potential causes.\nWith balance = TRUE (default is FALSE), it’s very similar to box.psa.\n\nHistogram\n\nUnits within each category are permuted between strata\nDifferences in proportions is used instead of difference in means\nInterpretation is the same\n\nFisher’s Exact Tests are performed and the p-values are shown at the bottom of the bars for each strata. A lack of balance for a given strata would be indicated by a p-value &lt; 0.05.\n\n\nPropensity Score Assessment Plot\n\nstrata5 &lt;- cut(lalonde$ps, \n               quantile(lalonde$ps, seq(0, 1, 1/5)), \n               include.lowest = TRUE, \n               labels = letters[1:5])\ncirc.psa(lalonde$re78, \n         lalonde$treat, \n         strata5)\n\nWhen differences in outcomes between treatments vary across strata, the investigator may want to learn how these differences are related to changes in covariate distributions within the strata.\nDisplays contributions of individual strata to the overall effect, weighing contributions of individual strata according to the relative sizes of the respective strata. The overall effect is plotted as a heavy dashed diagonal line that runs parallel to the identity diagonal.\nThe x-axis is the response values when treatment = 0, and the y-axis is the response values when treatment = 1. Labels depend on the values of the treatment variable.\nCircles\n\nEach circle represents a stratum, and the sizes of circles vary according to their respective sample sizes.\nThe center of each stratum’s circle corresponds to outcome means for the respective control and treatment groups for that stratum.\n\nThe Crosses represent the difference in response means between treatment and control.\nThe blue dashed line is the mean of the differences between the strata which is the ATE, and the green line is its 95%CI.\n\nCIs that span the diagonal means no significant effect.\nCIs become increasingly unreliable for larger values of the trim arg.\n\nThe vertical and horizontal dashed red lines represent the (weighted) response means for the control and treatment groups respectively.\nInterpretation\n\nCircles/Crosses that fall on the diagonal means that those strata do not have a treatment effect.\nCircles on the lower side of diagonal black line show that the corresponding x-axis (e.g. treatment = 0) mean for that strata is larger than the y-axis (e.g. treatment = 1) mean for that stratum\nCircles close proximity to one another on the same side of the diagonal indicates concordance of outcome values in these strata\nCircles far outside of the cluster of circles should be investigated via the other predictor charts above to see what characteristics make-up that particular strata. If the sizes of each strata aren’t relatively balanced, then a strata with few units may present outside the cluster. It may be useful try a larger amount of strata to see how that affects the outlier circle as it may help narrow the strata profile.\nThe distance of the crosses from the diagonal represents the size of estimated effect. A stratum with a much larger effect would influence the ATE (blue line) more strongly.\n\n\nLOESS Plot\n\npsa::loess_plot(\n  ps = psadf[psadf$Y &lt; 30000,]$ps, \n  outcome = psadf[psadf$Y &lt; 30000,]$Y, \n  treatment = as.logical(psadf[psadf$Y &lt; 30000,]$Tr))\n\nIncludes:\n\nTop: PS density plots for treatment and control. Looking to make sure there’s substantial overlap of the two densities.\nRight: Density plot of the outcome variable by treatment variable. Example shows significant skew and should be logged.\nCenter: LOESS plot\n\nParallel lines would indicate that there is a treatment effect, and it is homogeneous (i.e. same for all units) across all propensity scores which is not often the case.\n\nAn important feature of PSA in detecting heterogeneous, or uneven, treatments based upon different “profiles.”\n\nI think a profile would be kind of like a latent variable description of a cluster, but in this case, clusters are strata determined by the propensity scores. Profiles can be built by looking at the predictor vs propensity score strata charts above.\n\n\nCan be used to find strata boundarys. Potential boundaries are indicated where the treatment and control lines narrow or cross. Spaces between the lines are where you can expect a treatment effect.\n\ne.g. Setting int = c(0.375, 0.55, 0.875, 1) says that 0.357 and 1 are the minimum and maximum propensity scores and 0.55 and 0.875 are scores where the loess lines narrow or cross. This would represent 3 strata: (0.375, 0.55], (0.55, 0.875], and (0.875, 1].\nSeems like this would be highly dependent on the parameters of the LOESS parameters that are set unless you have a lot of data.\n\n\nStratification Plot\n\npsa::stratification_plot(ps = psadf$ps,\n                         treatment = psadf$Tr,\n                         outcome = psadf$Y,\n                         n_strata = 5)\n\nInstead of assuming a continuous vector of propensity scores as with the LOESS, it visualizes the mean differences in the response variable for each strata\nEssentially a visualization of an independent t-test.",
    "crumbs": [
      "Econometrics",
      "Propensity Score Analysis"
    ]
  },
  {
    "objectID": "qmd/econometrics-psa.html#sensitivity-analysis",
    "href": "qmd/econometrics-psa.html#sensitivity-analysis",
    "title": "Propensity Score Analysis",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\n\nObserve how the treatment effect size changes in the presence of unobserved confounders.\n\nIf the observational analysis results only depends on the observed covariates which is unlikely, then the analysis is free of hidden bias (i.e. the p-value is valid if there are no unobserved confounders)\n\nHidden bias exists if two units with the same covariate values have different propensity scores.\nTechnique currently only available for the Matching PSA method\n\nNot sure why this is only for Matching since the formula below is just an odds ratio of propensity scores. Maybe he’s talking about the package used in his example.\n\n\n\nSelection Bias Ratio\n\\[\n\\Gamma = \\frac{O_a}{O_b} = \\frac{\\frac{\\pi_a}{1-\\pi_a}}{\\frac{\\pi_b}{1-\\pi_b}}\n\\]\n\nThe ratio of the odds of the treated unit being in the treated group to the odds of the control unit being in the control group\nConverting Gamma to probability\nExamples\n\nIn smoking studies, the ratio is around 6 which means the results are very robust to unobserved confounders. Anything around 1 is considered sensitive to confounders.\nFor Social Science studies, it’s typically between 1 and 2.\n\nConverting to the probability of 1 unit of the matched pair being treated\n\\[\n\\frac{1}{\\Gamma + 1} \\leq p_a, p_b \\leq \\frac{\\Gamma}{\\Gamma + 1}\n\\]\n\n\\(p_a\\) is the probability of a unit in group \\(a\\) being treated.\n\\(\\Gamma = 1 \\quad \\longrightarrow \\quad p_a =0.5 \\;\\&\\; p_b = 0.5\\) which means each unit is equally likely to get treated\n\\(0.5 \\leq \\Gamma \\leq 2 \\quad \\longrightarrow \\quad 0.33 \\leq p_a, p_b \\leq 0.66\\) which means no unit can be more than twice as likely as its match to get treated\n\\(0.33 \\leq \\Gamma \\leq 3 \\quad \\longrightarrow \\quad 0.25 \\leq p_a,p_b \\leq 0.75\\) which means no unit can be more than three times as likely as its match to get treated.\n\n\n\n\nWilcoxon Signed Rank Test\n\nUsed in the sensitivity test to determine which level of \\(\\Gamma\\) will produce a non-significant treatment effect.\nSteps\n\nDrop matched pairs that have the same outcome value\nCalculate the difference in outcome value between each matched pair\nRank the absolute differences from smallest (1) to largest (N)\nCalculate W\n\\[\nW = \\left|\\; \\sum_1^N \\operatorname{sgn} (x_{T,i} - x_{C,i}) \\cdot R_i \\; \\right|\n\\]\n\n\\(N\\) is the number of ranked pairs\n\\(R_i\\) is the Rank for pair \\(i\\)\n\\(x_{T,i}\\) and \\(x_{C,i}\\) are the outcome values for each unit of pair \\(i\\)\n\n\n\nExample:\nrbounds::psens(x = lalonde$re78[rr$index.treated], \n               y = lalonde$re78[rr$index.control],\n               Gamma = 2, \n               GammaInc = 0.1)\n\n#&gt; \n#&gt;  Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value \n#&gt;  \n#&gt; Unconfounded estimate ....  2e-04 \n#&gt; \n#&gt;  Gamma Lower bound Upper bound\n#&gt;    1.0       2e-04      0.0002\n#&gt;    1.1       0e+00      0.0016\n#&gt;    1.2       0e+00      0.0069\n#&gt;    1.3       0e+00      0.0215\n#&gt;    1.4       0e+00      0.0527\n\n{rbounds} performs Rosenbaum Bounds Sensitivity Tests for Matched and Unmatched Data. Also has functions for IV models and 2x2 contingency tables.\n\nThere is another function for continuous/ordinal outcomes called hlsens which uses the Hodges-Lehman point estimate. After reading the wiki, the output of this function seems to be a median difference or effect size for a given Gamma, but I’m not sure how that’s supposed to be interpreted in this context.\nFor binary outcomes, use binarysens. The x and y args say to use counts of discrepant pairs. This means counts where the treatment group and control groups have different outcomes. So in the 2x2 table, that’s the counts in cells (1,2) and (2,1)\n\nBryer’s slides say to use McNemar’s test, but after looking at the code, I don’t see how Gamma can be incorporated into that test.\n\n\nArguments\n\nx: Treatment group outcome values ordered by matched pairs\ny: Control group outcome values ordered by matched pairs\nGamma: Largest value of Gamma that you want to test\nGammaInc: Increment value of Gamma.\n\nThe Lower bound and Upper bound are a sort of CI for the Wilcoxon test’s p-values\nLook for the value of Gamma that would nullify a significant treatment effect in your analysis.\n\ne.g. \\(\\Gamma = 1.4\\) (since it’s Upper bound value is &gt; 0.05) says that a confounder that adds 40% more variance in explaining the outcome would result in a Wilcoxon Test that doesn’t reject the Null Hypothesis (i.e. no significant treatment effect).\n\n\n\n\n\nBootstrapping\n\nWith bootstrapping, you can check the sensitivity to model choice.\nSampling with replacement is done for the treatment and control observations is done separately\nFor each bootstrap sample balance statistics and treatment effects are estimated using each method (five by default). Overall treatment effect with confidence interval is estimated from the bootstrap samples\nExample\nlalonde_formu &lt;- treat ~ age + I(age^2) + educ + I(educ^2) + black +\n    hisp + married + nodegr + re74  + I(re74^2) + re75 + I(re75^2)\npsaboot &lt;- \n  PSAboot::PSAboot(Tr = lalonde$treat,\n                   Y = lalonde$re78,\n                   X = lalonde,\n                   formu = lalonde_formu)\nsummary(psaboot)\n#&gt; Stratification Results:\n#&gt;    Complete estimate = 1658\n#&gt;    Complete CI = [242, 3074]\n#&gt;    Bootstrap pooled estimate = 1476\n#&gt;    Bootstrap weighted pooled estimate = 1461\n#&gt;    Bootstrap pooled CI = [66.5, 2885]\n#&gt;    59% of bootstrap samples have confidence intervals that do not span zero.\n#&gt;       59% positive.\n#&gt;       0% negative.\n#&gt; ctree Results:\n#&gt;    Complete estimate = 1598\n#&gt;    Complete CI = [-6.62, 3203]\n#&gt;    Bootstrap pooled estimate = 1465\n#&gt;    Bootstrap weighted pooled estimate = 1472\n#&gt;    Bootstrap pooled CI = [172, 2758]\n#&gt;    38.1% of bootstrap samples have confidence intervals that do not span zero.\n#&gt;       38.1% positive.\n#&gt;       0% negative.\n#... etc for the other methods\n\nBy default, PSAboot uses logistic regression (i.e. boot.strata) {partykit::ctree} (See Algorithms, ML &gt;&gt; Trees &gt;&gt; Distributional Trees/Forests), {rpart} (decision trees), {Matching}, and {MatchIt}, but boot.weighting is also available by manually specifying the functions through the methods argument.\ncontrol.ratio and treat.ratio arguments allow for undersampling in the case of imbalanced data\nDefault parallel = TRUE, M = 100. Also has a seed argument. Watch your RAM for large values of M, because I don’t think there’s any garbage collection in his code.\n\nBalance Plot\n\n\nMetric: Average Balance (mean difference/sd(strata) for strata or sd(N) for matching) across all covariates\nUnadjusted (red): No propensity score adjustment\nComplete (blue): Propensity scores used but no bootstrapping\nPooled (black): Propensity scores and bootstrapped\n\nSo the distribution is made-up “blue” measurements calculated from bootstrap resamples. Then, the black line is the mean of that distribution\n\nThe red and blue estimates seem to the same as the ones in the Covariate Balance Plot (PSAgraphics::cv.bal.psa)\nInterpretation: The weighting method is the most balanced.\n\nBox Plot\n\nPSAboot::boxplot(psaboot)\n\nFrom the Bryer video, I assume this is a boxplot version of the balance plot with the same interpretation of each color (black, red, and blue) except that since the input isn’t a balance object, these are unstandardized mean differences? (note x-axis).\n\nI’m confused as to why Weighting is no longer the best method (i.e. closest to zero) though.\n\nThere currently isn’t any sufficient documentation to make certain of estimate or the green lines, but I think the green lines look like averages of the whiskers.\n\nCorrelation Between Boot Distributions\n\nPSAboot::matrixplot(psaboot)\n\n{rpart} shows the least relationship with the other methods.",
    "crumbs": [
      "Econometrics",
      "Propensity Score Analysis"
    ]
  },
  {
    "objectID": "qmd/econometrics-psa.html#multinomial",
    "href": "qmd/econometrics-psa.html#multinomial",
    "title": "Propensity Score Analysis",
    "section": "Multinomial",
    "text": "Multinomial\n\nExample: 2 Treatments and 1 Control\n\nEstimate three separate propensity score models for each pair of groups (i.e. Control-to-Treat1, Control-to-Treat2, Treat1-to-Treat2).\nDetermine the matching order. The default is to start with the treatment with the smallest number of observations, then the other treatment, followed by the control which typically has the most observations.\nFor each unit in the first group (e.g. Treatment 1), find all units from the second group (e.g. Treatment 2) that have PSs within a certain threshold (i.e. difference between PSs is within a specified caliper).\nFor each unit in the second group, find all units from third group (e.g. Control) within a certain threshold.\nCalculate the distance (i.e. difference) between each unit in the third group matched in the previous step and the unit from the first group. Eliminate candidates that exceed the caliper (i.e. threshold).\nCalculate a total distance (sum of the three distances) and retain the smallest unique M group 1 units (by default M=2)\n\nPS estimated from multiple logistic regression models. One for each combination of of categories of the treatment variable (e.g. treatment_1 ~ control, treatment_2 ~ control, treatment_1 ~ treatment_2)\nFinds matched triplets that minimize the total distance (i.e. sum of the standardized distance between propensity scores within the three models). within a caliper.\nProvides multiple methods for determining which matched triplets are retained:\n\nOptimal: which attempts to retain all treatment units.\nFull: which retains all matched triplets within the specified caliper (.25 by default as suggested by Rosenbaum).\nAnalog of the one-to-many for matched triplets. Specify how many times each treat1 and treat2 unit can be matched.\nUnique which allows each unit to be matched once, and only once.\n\nFunctions for conducting repeated measures ANOVA and Freidman Ranksum Tests are provided.",
    "crumbs": [
      "Econometrics",
      "Propensity Score Analysis"
    ]
  },
  {
    "objectID": "qmd/eda-multilevel-longitudinal.html",
    "href": "qmd/eda-multilevel-longitudinal.html",
    "title": "Multilevel, Longitudinal",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "EDA",
      "Multilevel, Longitudinal"
    ]
  },
  {
    "objectID": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-misc",
    "href": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-misc",
    "title": "Multilevel, Longitudinal",
    "section": "",
    "text": "Also see Econometrics, Mixed Effects &gt;&gt; Considerations &gt;&gt; Variable Assignment\nNeed to figure out if\n\nThere’s significant within-unit variation. If so, then FE model will likely be the best model\n\nArticle with simulated data showed that within variation around sd &lt; 0.5 didn’t detect the effect of explanatory variable but ymmv (depends on # of units, observations per unit, N)\n\nThere’s significant between-unit variation. If so, then RE model will likely be the best model",
    "crumbs": [
      "EDA",
      "Multilevel, Longitudinal"
    ]
  },
  {
    "objectID": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-mult",
    "href": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-mult",
    "title": "Multilevel, Longitudinal",
    "section": "Multilevel",
    "text": "Multilevel\n\nMisc\n\nIs my data clustered?\nSeparate variables into levels\n\nLevel One: Variables measured at the most frequently occurring observational unit\n\ni.e. Variables that (for the most part) have different values for each row\ni.e. Vary for each repeated measure of a subject and vary between subjects\n\nLevel Two: Variables measured on larger observational units\n\ni.e. Constant for each repeated measure of a subject but vary between each subject\n\n\n\n\n\nUnivariate\n\nLevel 1 and Level 2\n\nGroup-level correlation or autocorrelation in variables can mislead or obscure patterns\n\nIf level 2 variable categories are pretty well balanced and there’s sufficient data, then plotting means can remove the correlation affect in the plot\n\nContinuous\n\nLooking at the skew, median/mean, bimodal or not\nExample:\n\n\n(Top) Each observation is plotted as if each observation is independent of the other\n\n* Ignores dependency (via repeated measures)\n\n(Bottom) Means for each subject or case or other level of a random variable\n\n* Removes dependency\n\nInterpretation: Right skew remains in both plots but plot 1’s decrease is smoother than plot 2’s\n\n\nCategorical\n\nCalculate proportions of each category and noting trends (ordinal variables) or severe imbalances\n\n\n\n\n\nBivariate\n\nQuestions\n\nIs there is a general trend suggesting that as the covariate increases the response either increases or decreases (trend)\nDo subjects at certain levels of the covariate tend to have similar mean values of the response (low variability)\nIs the variation in the response at different levels of the covariate (unequal variability)\n\nMe: Comparison between plots that take into account dependency and the same plot that doesn’t\n\nTrend in plot that ignores dependency but no trend in plot that removes dependency\n\nMay indicate within-subject variation\n\nNo trend in plot that ignores dependency but trend in plot that removes dependency\n\nMay indicate between-subject variation\n\n\nBoxplots (Categorical)\n\n\nLevel 1 categorical covariates (y-axis) vs continuous outcome (x-axis)\n* Ignores dependency (via repeated measures)\nInterpretation\n\nLeft: ordinal covariate, medians are close and boxes pretty much contained within each other but there might be a trend\nRight: Looks like some decent variation between categories\n\nMean outcome (per subject) vs covariate\n\n\n* Removes dependency\nInterpretation: looks like some decent variation between categories\n\n\nScatter (Continuous)\n\n\nLevel 1 continuous covariate (x-axis) vs continuous outcome (y-axis)\n* Ignores dependency (via repeated measures)\nActually a discrete  covariate being treated as continuous the fact that does seem to be a small trend is what’s important\nMean outcome (per subject) vs covariate\n\n\n* Removes dependency\nInterpretation: PEM not showing much of an correlation if any\n\n\nFacetting previous plots by subject\n\n\n\nLeft\n\nMostly downward trends but some upward trends\nUseful for prior formulation\nGives an idea about the uncertainty of the slope of this variable\n\nRight\n\nScarcity of points for some categories makes boxplots a bad idea\nDifficult to spot any trends\n\n* Removes dependency\n\n\n\n\nTrivariate\n\nScatter, color by random variable\n\n\nVariables\n\n“points per 60 min” (outcome)\n“time on ice” (fixed effect)\nfacetted by “position” (fixed effect)\ncolored by “player” (potential random variable)\n\nInterpretation\n\nTheres does seem to be clustering by “player” therefore a mixed effects model might be a good choice.\n\n\nNull Model (aka random intercept-only model)\nm0 &lt;- \n  lmer(pp60 ~ 1 + (1 | player), \n       data = df)\n\njtools::summ(m0)\nGROUPING VARIABLES\nGROUP # GROUPS ICC\nplayer       20      0.89\nICC &gt; 0.1 is generally accepted as the minimal threshold for justifying the use of Mixed Effects model (See ICC section)",
    "crumbs": [
      "EDA",
      "Multilevel, Longitudinal"
    ]
  },
  {
    "objectID": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-long",
    "href": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-long",
    "title": "Multilevel, Longitudinal",
    "section": "Longitudinal",
    "text": "Longitudinal\n\nMisc\n\nRepeated measures that have a sequential or time component\npackages: {brolgar}\n\n\n\nUnivariate\n\nContinuous Outcome vs. Time\n\nFacetted by Observational Unit (e.g. school)\n\n\n\nLinear Fit and Line Chart\n\nSpaghetti\n\n\n\n\n\nBivariate\n\nBold line is the overall fit with LOESS\nContinuous Outcome vs Time\n\nFacetted by Categorical\n\nFacetted by Binned Continuous\n\n\nTime Endpoints\n\n\n“School Type” is a categorical, level 2 variable and “Math Score” is the numeric outcome\nLooking for change from the initial measurement to the final measurement\n\n\n\n\nLinear parameters\n\nFit a linear regression for each subject/unit with its repeated measurements\n\nSee univariate &gt;&gt; numerical outcome vs. time &gt;&gt; Facetted by observational unit &gt;&gt; (left) linear fit\n\nAdvantages\n\nEach unit’s/subject’s data points can be summarized with two summary statistics—an intercept and a slope\n\nBigger advantage when there are more observations over time per unit/subject\n\nSeems like a good way for using empirical bayes (i.e. use these distributions for prior specifications)\n\nDisadvantages\n\nSlopes cannot be estimated for those units/subjects with just a single observation\nR-squared values cannot be calculated for those units/subjects with no variability in test scores during the time period\nR-squared values must be 1 for those units/subjects with only two test scores.\n\nSummary Statistics\n\nMean and SD for intercepts and slopes\n\nUnivariate\n\n\\(y_t = \\beta_0 + \\beta_1 t + \\epsilon_t\\)\n\n\\(t\\) is the time variable (aka trend)\n\nParameter Distributions\n\nCorrelation\n\n\nLower intitial values (intercepts) show the greatest growth (slopes) over time\nCorrelation = -0.32\n\n\nBivariate\n\nProcess\n\nFilter data by Level 2 variable\nFor each category of the Level 2 variable, fit a regression, yt = β0 + β1t + εt, for each unit/subject.\nAggregate results\n\nParameter Distributions\n\n\n“School Type” is a Level 2, binary variable",
    "crumbs": [
      "EDA",
      "Multilevel, Longitudinal"
    ]
  },
  {
    "objectID": "qmd/eda-text.html",
    "href": "qmd/eda-text.html",
    "title": "Text",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "EDA",
      "Text"
    ]
  },
  {
    "objectID": "qmd/eda-text.html#sec-eda-text-misc",
    "href": "qmd/eda-text.html#sec-eda-text-misc",
    "title": "Text",
    "section": "",
    "text": "Also see\n\nFeature Engineering, Tokenization\nFeature Engineering, Embeddings\nDiagnostics, NLP\nNLP, General\n\nCount most popular words\ndata %&gt;%\n    unnest_tokens(word, text_var) %&gt;%\n    count(word, sort = TRUE)\nAvg value of outcome variable that associated with words\n\ndata %&gt;%\n    unnest_tokens(word, text_var) %&gt;%\n    group_by(word) %&gt;%\n    summarize(avg_outcome = mean(outcome),\n              n = n()) %&gt;%\n    arrange(desc(n)) %&gt;%\n    head(30) %&gt;%\n    mutate(word = fct_reorder(word, avg_outcome)) %&gt;%\n    ggplot(aes(avg_outcome, word, size = n)) +\n    geom_point()\n\nPattern in the example shows that words in an airbnb listing probably have predictive power on price (outcome variable)",
    "crumbs": [
      "EDA",
      "Text"
    ]
  },
  {
    "objectID": "qmd/eda-time-series.html",
    "href": "qmd/eda-time-series.html",
    "title": "Time Series",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "EDA",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/eda-time-series.html#sec-eda-ts-misc",
    "href": "qmd/eda-time-series.html#sec-eda-ts-misc",
    "title": "Time Series",
    "section": "",
    "text": "Resources\n\n{astsa} - Package, code, and docs for the books\n\nTime Series Analysis and Its Applications: With R Examples (graduate-level)\n\nSee R &gt;&gt; Documents &gt;&gt; Time Series\n\nTime Series: A Data Analysis Approach using R (introductory)\n\nSee R &gt;&gt; Documents &gt;&gt; Time Series\n\n\n\nPackages\n\n{timetk} - Various diagnostic, preprocessing, and engineering functions. Also available in Python\n{{diaquiri}}\n\nAggregated values are automatically created for each data field (column) depending on its contents (e.g. min/max/mean values for numeric data, no. of distinct values for categorical data)\nOverviews for missing values, non-conformant values, and duplicated rows.",
    "crumbs": [
      "EDA",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/eda-time-series.html#sec-eda-ts-basic",
    "href": "qmd/eda-time-series.html#sec-eda-ts-basic",
    "title": "Time Series",
    "section": "Basic Characteristics",
    "text": "Basic Characteristics\n\nPlot series\nts_tbl |&gt; \n    timetk::plot_time_series(date, \n                             value, \n                             .smooth = T, \n                             .smooth_span = 0.3, \n                             .interactive = F)\n\nIncludes a LOESS smoothing line. Control the curviness of LOESS using smooth_span\n\nMissingness\n\nNAs affect the number of lags to be calculated for a variable\n\ne.g. exports only recorded quarterly but stock price has a monthly close price you want to predict. So if you’re forecasting monthly oil price then creating a lagged variable for exports is difficult\n\nBizsci (lab 29), minimum_lag = length_of_sequence_of_tail_NAs +1 \n\ntail(ts) are the most recent values\n\n\n\nEven if you don’t want a lag for a predictor var and it has NAs, you need to recipe::step_lag(var, lag = #_of_tail_NAs). So var has no NAs.\nConsider seasonality of series when determining imputation method\n\nOutcome Variable Distribution\n\nFor low volume data, a right-skewed distribution might be needed instead of a Gaussian.\n\nThin tails - Use a Gamma distribution\nHeavier tails - Use a Log Normal or the Inverse Gaussian\n\nExample\n\ngghistogram(lynx, add.kde=TRUE)\n\nadd.normal adds a gaussian density\n\n\nZeros\n\nSee notebook for tests on the number of zeros in Poisson section\nMight be tests in the intermittent forecasting packages, so see bkmks\nIf so, see Logistics, Demand Forecasting &gt;&gt; Intermittent Demand for modeling approaches\nAre there gaps in the time series (e.g. missing a whole day/multiple days, days of shockingly low volume)?\n\nIs data recorded at irregular intervals. If so:\n\n{{BINCOR}}handles cross-correlation between 2 series with irregular intervals and series with regular but different intervals\nI think common forecasting algorithms require regularly spaced data, so look towards ML, Multilevel Model for Change (MLMC), or Latent Growth Models\nMay also try binning points in the series (like BINCOR does) or smoothing them to get a regular interval",
    "crumbs": [
      "EDA",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/eda-time-series.html#sec-eda-ts-seas",
    "href": "qmd/eda-time-series.html#sec-eda-ts-seas",
    "title": "Time Series",
    "section": "Seasonality",
    "text": "Seasonality\n\nSTL Decomposition\n\nAlso see Forecasting, Decomposition\nIf we are interested in short- or long-term movements of the time series, we do not care about the various seasonalities. We want to know the trend component of the time series and if it has reached a turning point\nIs there strong seasonality or trend?\nIf there’s a pattern in the random/remainder component, this could indicate that there are other strong influences present.\nExample: {timetk}\n\nts_tbl %&gt;%\n    plot_stl_diagnostics(\n        date, value,\n        .feature_set = c(\"observed\", \"season\", \"trend\", \"remainder\"),\n        .trend       = 180,\n        .frequency   = 30,\n        .interactive = F\n    )\nDaily Seasonal Adjustment (DSA)\n\nUsed to remove seasonal and calendar effects from time series data with daily observations. This allows you to analyze the underlying trends and patterns in the data without being masked by predictable fluctuations like weekdays, weekends, holidays, or seasonal changes.\nIn a EDA context, the deseasonalized series can be used to locate the time series trend (daily data is very noisy), identify turning points, and compare components with other series.\nPackages: {dsa}\nNotes from Seasonal Adjustment of Daily Time Series\n\nExample shows it outperforming Hyndman’s STR procedure in extracting seasonal components more completely.\n\nDaily data can have multiple seasonalities present\n\nRaw Time Series\n\nDSA Processed Time Series\n\n\nCombines the seasonal trend decomposition procedure using Loess (STL) with a regression model with ARIMA errors\nProcedure\n\nSTL adjusts intra-weekly periodic patterns.\nRegARIMA estimates calendar effects, cross-seasonal effects, and outliers.\nSTL adjusts intra-monthly periodic effects.\nSTL adjusts intra-annual effects\n\n\n\nSeasonality Tests (weekly, monthly, and yearly)\n\n{seastests} QS and Friedman (see bkmk in Time Series &gt;&gt; eda for example)\nQS test’s null hypothesis is no positive autocorrelation in seasonal lags in the time series\nFriedman test’s null hypothesis is no significant differences between the values’ period-specific means present in the time series\nFor QS and Friedman, pval &gt; 0.05 indicates NO seasonality present\n\nAdditive or Multiplicative Structure\n\nIs the variance (mostly) constant (Additive) or not constant (Multiplicative) over time?\nDoes the amplitude of the seasonal or cyclical component increase over time? \n\nThe amplitude of the seasonal component increases over time so this series has a multiplicative structure\nAlso multiplicative, if there’s a changing seasonal amplitude for different times of the year\n\nIf you have a multiplicative structure and zeros in your data (i.e. intermittent data), then they must handled in some way.\n\nSee Logistics &gt;&gt; Demand Forecasting &gt;&gt; Intermittent Data",
    "crumbs": [
      "EDA",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/eda-time-series.html#sec-eda-ts-group",
    "href": "qmd/eda-time-series.html#sec-eda-ts-group",
    "title": "Time Series",
    "section": "Groups",
    "text": "Groups\n\nQuantile values per frequency unit (by group and total)\n\n{timetk::plot_time_series_boxplot}\nAverage closing price for each month, each day of the month, each day of the week\nWhen are dips and peaks?\nWhich groups are similar\nWhat are the potential reasons behind these dips and peaks?\nExample: Daily Power Consumption\n\n\nMedian, the lower quartile, and the upper quartile for Saturdays and Sundays are below the remaining weekdays when inspecting daily power consumption\nSome outliers are present during the week, which could indicate lower power consumption due to moving holidays\n\nMoving holidays are holidays which occur each year, but where the exact timing shifts (e.g. Easter)\n\n\nExample: {timetk} Calendar Effects\n\nts_tbl %&gt;%\n    plot_seasonal_diagnostics(date, value, .interactive = F)\nExample: Monthly Power Consumption\n\nMedian, the lower quartile, and the upper quartile of power consumption are lower during the spring and summer than autumn and winter\n\nExample: Demand per Month and per Category\n\nVariance of value by group\n\nExample: how sales vary between store types over a year\n\nImportant to standardize the value by group\n\ndf %&gt;% group_by(group) %&gt;% mutate(sales = scale(sales))\n\nWhich groups vary wildly and which are more stable\n\n\nRates by group\n\nExample: sales($) per customer\n\ndf %&gt;% group_by(group, month) %&gt;% mutate(sales_per_cust = sum(sales)/sum(customers)",
    "crumbs": [
      "EDA",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/eda-time-series.html#sec-eda-ts-feats",
    "href": "qmd/eda-time-series.html#sec-eda-ts-feats",
    "title": "Time Series",
    "section": "Statistical Features",
    "text": "Statistical Features\n\nOutliers\n\nAlso see\n\n“PCA the Features” below\nAnomaly Detection\n\nExample: {timetk}\n\nwalmart_sales_weekly |&gt;  \n    group_by(id) |&gt; \n    plot_anomaly_diagnostics(Date, Weekly_Sales,\n                             .message = FALSE,\n                             .facet_ncol = 3,\n                             .ribbon_alpha = 0.25,\n                             .interactive = FALSE)\n\nStatistical Features vs Outcome\n\nSee Feature Engineering, Time Series\nFirst Autocorrelation Coefficient vs Categorical vs Binary Outcome\n\n\nThere does seem to be some variance. An interaction between autocorrelation and the cateogorical variable might be predictive of a heart murmur event.\n\n\nTrend Strength and Seasonal Strength plots\n\n{tsfeatures} has seasonality strength metric\n\nShannon Spectral Entropy\n\nfeasts::feat_spectral  will compute the (Shannon) spectral entropy of a time series, which is a measure of how easy the series is to forecast.\nA series which has strong trend and seasonality (and so is easy to forecast) will have entropy close to 0.\nA series that is very noisy (and so is difficult to forecast) will have entropy close to 1.\n\nPCA the Features\n\nIdentify characteristics for high dimensional data (From fpp3)\n\nlibrary(broom)\nlibrary(feasts)\n\ntourism_features &lt;- tourism |&gt;\n  features(Trips, feature_set(pkgs = \"feasts\"))\n\npcs &lt;- tourism_features |&gt;\n  select(-State, -Region, -Purpose) |&gt;\n  prcomp(scale = TRUE) |&gt;\n  augment(tourism_features)\npcs |&gt;\n  ggplot(aes(x = .fittedPC1, y = .fittedPC2, col = Purpose)) +\n  geom_point() +\n  theme(aspect.ratio = 1)\n\nHoliday series behave quite differently from the rest of the series. Almost all of the holiday series appear in the top half of the plot, while almost all of the remaining series appear in the bottom half of the plot.\nClearly, the second principal component is distinguishing between holidays and other types of travel.\nThe four points where PC1 &gt; 10 stand out as outliers.\n\nVisualize Outliers\n\noutliers &lt;- pcs |&gt;\n  filter(.fittedPC1 &gt; 10) |&gt;\n  select(Region, State, Purpose, .fittedPC1, .fittedPC2)\noutliers\n#&gt; # A tibble: 4 × 5\n#&gt;   Region                 State             Purpose  .fittedPC1 .fittedPC2\n#&gt;   &lt;chr&gt;                  &lt;chr&gt;             &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 Australia's North West Western Australia Business       13.4    -11.3  \n#&gt; 2 Australia's South West Western Australia Holiday        10.9      0.880\n#&gt; 3 Melbourne              Victoria          Holiday        12.3    -10.4  \n#&gt; 4 South Coast            New South Wales   Holiday        11.9      9.42\noutliers |&gt;\n  left_join(tourism, \n            by = c(\"State\", \"Region\", \"Purpose\"), \n            multiple = \"all\") |&gt;\n  mutate(Series = glue(\"{State}\", \"{Region}\", \"{Purpose}\", \n                       .sep = \"\\n\\n\")) |&gt;\n  ggplot(aes(x = Quarter, y = Trips)) +\n  geom_line() +\n  facet_grid(Series ~ ., \n             scales = \"free\") +\n  labs(title = \"Outlying time series in PC space\")\nWhy might these series be identified as unusual?\n\nHoliday visits to the south coast of NSW is highly seasonal but has almost no trend, whereas most holiday destinations in Australia show some trend over time.\nMelbourne is an unusual holiday destination because it has almost no seasonality, whereas most holiday destinations in Australia have highly seasonal tourism.\nThe north western corner of Western Australia is unusual because it shows an increase in business tourism in the last few years of data, but little or no seasonality.\nThe south western corner of Western Australia is unusual because it shows both an increase in holiday tourism in the last few years of data and a high level of seasonality.",
    "crumbs": [
      "EDA",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/eda-time-series.html#sec-eda-ts-assoc",
    "href": "qmd/eda-time-series.html#sec-eda-ts-assoc",
    "title": "Time Series",
    "section": "Association",
    "text": "Association\n\nSee Association, Time Series\nLag Scatter Plots\n\nLag scatterplots between target series and lags of the target series (i.e. yt vs yt+h)\n\nastsa::lag1.plot(y, 12) # lags 1-12 of y\nastsa::lag1.plot(soi, 12, col=astsa.col(4, .3), pch=20, cex=2) # prettified\n\nAutocorrelation values in upper right corner\n\nAutocorrelations/Cross-Correlation values only valid if relationships are linear but maybe still useful in determining a positive or negative relationship\n\nLOESS smoothing line added\nNonlinear patterns can indicate that behavior between the two variables is different for high values and low values\n\nLag scatterplots between target series and lags of the predictor Series (i.e. yt vs xt+h)\n\nastsa::lag2.plot(y, x, 8) # y vs lags 0-8 of x\nastsa::lag2.plot(soi, rec, 8, cex=1.1, pch=19, col=5, bgl='transparent', lwl=2, gg=T, box.col=gray(1))  #prettified\n\nIf either series has autocorrelation, then it should be prewhitened before being inputted into the function.\n\nSee Association, Time Series &gt;&gt; CCF\n\nCross-Correlation (CCF) values in upper right corner\n\nAutocorrelations/Cross-Correlation values only valid if relationships are linear but maybe still useful in determining a positive or negative relationship\n\nNonlinear patterns can indicate that behavior between the two variables is different for high values and low values\n\n\nPartial Autocorrelation\n\nRemoves the correlation between \\(y_{t-k}\\) and lags before it (\\(y_{t-(k-1)}, \\ldots, y_{t-1}\\)) to get a more accurate correlation between yt and yt-k. Sort of like a partial correlation but for a univariate time series.\nCan be interpreted as the amount correlation between yt-k and yt thats not explained by the previous lags.\nExample: forecast::Pacf(plot = TRUE) or forecast::ggPacf\n\nExample: {timetk} plots ACF and PACF charts\n\nm4_hourly %&gt;%\n    group_by(id) %&gt;%\n    plot_acf_diagnostics(\n        date, value,               # ACF & PACF\n        .lags = \"7 days\",          # 7-Days of hourly lags\n        .interactive = FALSE\n    )\n{feasts::feat_pacf}\n\nContains several features involving partial autocorrelations including:\n\nThe sum of squares of the first five partial autocorrelations for the original series\nThe first-differenced series and the second-differenced series.\nFor seasonal data, it also includes the partial autocorrelation at the first seasonal lag.",
    "crumbs": [
      "EDA",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/eda-time-series.html#sec-eda-ts-station",
    "href": "qmd/eda-time-series.html#sec-eda-ts-station",
    "title": "Time Series",
    "section": "Stationarity",
    "text": "Stationarity\n\nCCF and most statistical and ML models need or prefer stationary time series.\nACF\n\nFor a stationary series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly.\nFor a non-stationary series:\n\nThe value of r1 (correlation between yt and yt-1) is often large and positive.\nA steady, slow decline towards 0 indicates trend is present\n\nIs the series a trend-stationary or unit root process?\n\nTest all series of interest with ADF and KPSS tests (See Forecasting, Statistical &gt;&gt; Preprocessing &gt;&gt; Detrend or Difference\n\n\nA scalloped pattern indicates seasonality is present\n\n95% CIs are \\(\\pm \\frac{1.96}{\\sqrt{T}}\\) where \\(T\\) is the length of the time series.\nExample: forecast::Acf(plot = TRUE) or forecast::ggAcf\n\n\nThe ACF of the differenced Google stock price (right fig) looks just like that of a white noise series. There are no autocorrelations lying outside the 95% limits, and the Ljung-Box \nQ∗ statistic (Ljung-Box) has a p-value of 0.355 (for h = 10) which implies the ts is stationary. This suggests that the daily change in the Google stock price is essentially a random amount which is uncorrelated with that of previous days.\n\n\nLjeung-Box\n\nstats::Box.test or feasts::ljung_box\n\nx: numeric or univariate ts\nlag: Recommended 10 for non-seasonal, 2m (e.g. m = 12 for monthly series, m = 4 for quarterly), maximum is T/5 where T is the length of the series.\ntype: “Lj”\n\nInterpretation: small Q* or p-value &gt; 0.05 means the time series is stationary.",
    "crumbs": [
      "EDA",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html",
    "href": "qmd/experiments-a_b-testing.html",
    "title": "17  A/B Testing",
    "section": "",
    "text": "17.1 Misc",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-misc",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-misc",
    "title": "17  A/B Testing",
    "section": "",
    "text": "Resources\n\nPractical Guide to Controlled Experiments on the Web (Paper)\n\nGeneral non-parametric A/B testing based on the Random Forest (Paper)\n\nUses the sum of OOB errors for each group as a test statistic\n{hypoRF}\n\nGoogle Analytics switched in 2017 from exact user counts to estimates of user counts by default. Estimates have Stderr and have significant effects on A/B testing. Think this can be switched to exact users in settings, see link\n\nhttps://towardsdatascience.com/the-perils-of-using-google-analytics-user-counts-in-a-b-testing-e50b5dfc5f6c\nWith user estimates and greater than 12K users per test arm, accuracy begins to suffer. Occurrences of significant p-values becomes inflated\nMaybe even worse with Adobe Analytics which uses a similar algorithm to estimate their user counts\n\nRegardless of the test results, there may be business considerations to take into account when deciding whether to add a feature\n\nIt’s believed user experience will be improved\nOne group shows positive results while another doesn’t, but in order to maintain parity you decide to add the feature\nYou were unable to perform the test during the optimal part of the year, and you have reason to believe the results would’ve been positive if the test was conducted during that time period.\n\nOften a good idea to limit outcome measurement to events that happen (or don’t happen) within a reasonable attribution window from exposure or assignment.\n\nBetter to use the first exposure as the attribution window start, as it should be unaffected by the experiment variants\nExample: a simple exposure of seeing “sign up” for the control and “sign up today!” for the treatment\n\nThe attribution window should be a within hours of exposure as we don’t expect the call-to-action text to have long-lasting effects.\n\n\nWhen running experiments on as few users as possible to reach a specified level of statistical significance, experiments may end up being statistically significant but with little precision in terms of the estimate\n\n\nAllows us to have conviction in rolling out the treatment to production\nNo certainty of the expected impact of the treatment\nIssues with this approach (see universal holdout section for solution)\n\nShort-term impact from a product feature may not equal its longer-term impact (e.g. novelty effect)\nOne algorithm’s performance advantage over another may fade over time\nCertain metrics are lagging indicators where we cannot observe any effects unless we extend the time horizon\n\n\nOptimal Stopping\n\nAlso see multi-armed bandit section\nSequential Probability Ratio Test (article)\n\nperforms a rolling likelihood ratio test (LRT) and once the value passes a threshold, the experiment’s p-value has a certain probability of correctly accepting/rejecting the null hypothesis\n\n\nUsed gated dial-up to minimize risk to business\n\nProcess\n\nAssign users that will be in the experiment and not in the experiment\n\nThis population can be just 1% of the population in the beginning, and increase from there.\n\nFor users in the experiment, divide them randomly with equal probability into control and treatment\n\n** Don’t use gradual dial-up over the whole population\n\nExample: start with a 1% treatment, dial up to 5% after a week, then to 10%, 25%, and finally to 50%\nIssues:\n\nCannot use the data from the dial-up period itself in the A/B test because it may be biased by seasonal effects.\nPre-exposure effect: some of the users in the treatment group have already had exposure to the treatment before and this pre-exposure can change their actions during the measured test period.\n\nThe gated dial-up solves these issues because both groups in the experiment are of the same size at all times and no participant in the treatment has been pre-exposed",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-terms",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-terms",
    "title": "17  A/B Testing",
    "section": "17.2 Terms",
    "text": "17.2 Terms\n\nA/A testing - uses A/B testing to test two identical versions of a page against each other. Typically, this is done to check that the tool being used to run the experiment is statistically fair. In an A/A test, the tool should report no difference in conversions between the control and variation, if the test is implemented correctly. Useful for exposing experiments with high false positive rates. (link, article)(Also see Workflow section)\nExploitation - One exploits a single action without knowing the rewards of the other actions.\n\ne.g. A person finds a good restaurant and repeatedly goes to that restaurant instead of trying others\n\nExploration - Always exploring the other actions to acquire as much information as possible to finally choose the best action\n\ne.g. A person repeatedly goes to a new restaurant instead of always going to the same restaurant\n\nMinimum Detectable Effect (MDE) - The smallest improvement over the baseline we are willing to detect in a controlled experiment. How large the difference should be in order to generate statistically significant outcomes (generally provided by the A/B testing platform)\nSample Ratio Mismatch (SRM) - refers to the mismatch between the sample ratio set by the experimenter and the observed sample ratio (example)\n\nExample\n\nSuppose you have an A/B test where you expected a 50/50 split of users in each test group. Instead, the Control and Variation groups are off by around 7%.",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-wkflw",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-wkflw",
    "title": "17  A/B Testing",
    "section": "17.3 Workflow",
    "text": "17.3 Workflow\n\nNotes from Typical 8-Step A/B Test Workflow for Data Scientists in 2022\nPre-Experiment: Think About the Product & Talk with the Product Manager (PM)\n\nExplore the product logic and user journey in detail\nFeasibility and Impact Questions\n\nWhat are the hypotheses?\nCan we find calculable metrics to test that?\nWhat are the risks for our product and users?\nIs there a network effect?\nIs it possible for users to demonstrate a strong novelty effect or change aversion\nIs it fair to keep some users from our new feature treatment?\nWhy do we need this feature?\nWhat is our expected gain from this feature, is it short-term or long-term?\n\n\nDesign (also see Design, Metrics sections)\n\nEvaluation Metrics: What are our north star metrics, directly-influenced metrics, guardrail metrics, and adoption metrics in the treatment group?\n\nApart from the business interpretation, it would be better for the evaluation metrics to have lower variance and be sensitive to changes.\nExample: Add Top 10 movies/shows feature to Netflix homepage\n\nHypothesis: “Showing members the Top 10 ‘experience’ will help them find something to watch, increasing member joy and satisfaction.”\nPrimary Metric: Engagement with the Netflix app\n\nMetric answers, “Are the ideas we are testing helping our members to choose Netflix as their entertainment destination on any given night?”\nResearch shows that this metric is correlated, in the long term, with the probability that members will retain their subscriptions.\n\nSecondary Metrics:\n\nTitle-level viewing of those titles that appear in the Top 10 list\n\ni.e. viewing of titles that appear in the Top 10 list\n\nFraction of viewing that originates from that Top 10 row of movies vs other parts of the UI\n\nGuardrail Metric: Compare customer service contacts for the control and treatment groups\n\nAn increase may indicate member confusion or dissatisfaction\n\n\n\nArticulate a causal chain\n\nShow how user behavior will change in response to the new product experience (aka feature) to the change in our primary decision metric\nMonitor secondary metrics along this chain\n\nHelps build confidence that any movement in our primary metric is the result of the causal chain we are hypothesizing, and not the result of some unintended consequence of the new feature (or a false positive)\nConfident in positive effect of primary metric IF secondary metrics have also shown an increase\nSkeptical of positive effect of primary metric IF secondary metrics have shown a decrease.\n\n\nUnit of Diversion: Shall we randomize the test based on user_id or session_id. (see Assignment)\nSet the False Positive Rate (typically 5%) (see Experiments, General &gt;&gt; Misc)\nPostulate an effect size and calculate the sample size\n\nFor different evaluation metrics and various minimum detectable effects (MDE), what are the necessary sample sizes for the result to be statistically significant?\n\nVery often we have to retrieve historical data and calculate on our own when the metrics of interest are too specific and are not supported by the A/B test platforms.\n\nChoice of effect size that you want to be able to detect should be meaningful in terms of business value\n\nExperiment Duration & Traffic Portion: This usually depends on the required samples size and the eligible traffic size that could potentially see the new feature.\n\nAlso take into consideration the risks of the experiment.\n\nExperiment Layer & Experiment Type: Determine which experiment layer to deploy our A/B tests.\n\nCommon that most of the experiment layers have been populated with other tests.\n\nOptions: Choose another layer or consider orthogonal experiments, if our feature is not correlated with others.\n\n\n\nCheck A/A variation\n\nMost platforms have automated this process\nCheck the A/A variation output and make sure everything is as expected — the p-values out of A/A simulations are uniformly distributed (Also see links in Terms section)\n\nA/A variation is the difference in key metrics between the control group (group A) and another control group (~group A). Since there is no actual difference between the two, the A/A variation is not expected to deviate from 0 in a statistically significant (p-value &lt; 0.0005 recommended) manner. (i.e. any detections are false positives)\n\nPre-treatment A/A (e.g. 60 days): A statistically significant result from the pre-assignment test indicates bias in the A/B test\n\nLikely due to user assignment.\n\ne.g. (user characteristics) A pre-assignment test would uncover that treatment users already had a higher number of purchases even before the experiment.\n\n\nTest under different scenarios: e.g. multiple steps in the conversion funnel (e.g., product impression, click, adding to the shopping cart, and purchase), client-side and server-side experiments, logged-out and logged-in experiments, etc.\nKeep past A/A tests and use them as a baseline Should be performed after any improvement to your experimentation platform\n\nDuring Experiment\n\nCheck the Invariant Metrics & Experiment Set-up\n\nMake sure users in the treatment group see all the treated features as expected\n\nCan be very bad if they aren’t and you’re using the traffic bought by your advertisers for A/B testing but end up sabotaging the experiments due to wrong parameters\n\nCheck that the diversion of traffic is random (also see Assignment section &gt;&gt; SRM Check)\n\nTypically use the Chi-squared Test to check the population across control and treatment groups\n\nCheck that the distribution of user profiles (e.g. gender, age) is homogeneous across treatment/control groups\n\nMost platforms will compute these\n\n\nMonitor Key Evaluation Metrics\n\nIf the key metrics and guardrail metrics drop significantly and continuously, consider taking a step back and re-evaluating the experiment for the benefit of our product and users, as the negative significant results are likely to be just a matter of time\n\nStart doing checks as soon as the experiment launches\n\nCheck regularly for at least the first week for new experiments\n\nNew experiments should be treated like intensive care patients\n**Be aware that the more you test, the greater probability of getting a false positive**\n\nShould apply a multiple-testing correction to the p-value (e.g. bonferoni)\n\nSee Romano-Wolf\n\nSee Statistical Concepts &gt;&gt; Null Hypothesis Significance Testing (NHST) &gt;&gt; Romano and Wolf’s correction\nSimilar to Westfall-Young but less restrictive\n\n\n\n\n\n\nPost-Experiment - Analyze the Results\n\nCollect the descriptive differences as well as the p-value between the treatment groups and control groups\n\nAlso see\n\nPost-Hoc Analysis, general\nExperiments, Analysis &gt;&gt; A/B Post-Hoc Analysis\nWilcoxon test in notebook and bkmks\n\n\nDo the results align with the hypothesis?\n\nIf they do, delve deeper to understand the reasons by breaking it down into key business dimensions (e.g. new/old users, channels, geographic regions).\n\nIf observed effect is large, it’s more likely that it’s a false positive or something went wrong with the execution of the experiment.\n\nInsignificant results can because of the sample size or high variance in the metrics being measured\n\nFor reducing variance, see Experiments, General &gt;&gt; Decreasing the sampling variance of the treatment effect\n\n\nLook at changes in these secondary metrics to assess if any changes in the primary metric follow the hypothesized causal chain\nIs there additional supporting or refuting evidence?\n\ne.g. consistent patterns across similar variants of the same feature\nIf you test 20 variants and only one yields a significant movement in the primary decision metric, you should be skeptical of that one variant’s positive effect.\n\nWith that 5% false positive rate, we expect on average one significant (1 in 20) result from random chance alone\n\n\nCompare the empirical Minimum Detectable Effect (MDE) to the actual difference in key metrics\n\nOften reveals how volatile the metrics are.\nGreat reference when studying user behaviors or designing similar experiments in the future.\n\n\nIf results are mixed/suggestive but not conclusive, run a second A/B experiement based on learnings from the first test\n\nExample: Half of the feature variants had a positive effect, but the other half did not.\n\nRefine these most promising variants, and run a new test\n\nWith fewer variants to test, you can also increase the allocation size to gain more power.\n\nFormulate Product Recommendations\n\nSummarize the analysis of the experiment results\nDerive a product recommendation on whether we should gradually expose more traffic to the treatment or select a more conservative strategy.\n\nWrite the Report\n\nContents\n\nproduct backgrounds and feature hypotheses\nexperiment resign\nresult analysis\nproduct recommendations",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-dsn",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-dsn",
    "title": "17  A/B Testing",
    "section": "17.4 Design",
    "text": "17.4 Design\n\nSimple\n\nfind sample size\n\nStandard way: use equation with power, significance level, and effect size\n“Rule of Thumb” way: sample size = (16* σ2) / δ2\n\nσ is variance of the data\nδ is the effect size (stake holders or literature) (e.g. 1% increase of revenue would be practically significant)\n\n\nCalculate run time of the experiment\n\nDivide sample size the number of users per group (i.e. experiment and control groups)\nRound up to the next week to capture weekly seasonality, calculate weekly metrics, etc.\n\nSubmit recommendation based on the results of the experiment\n\nLink results goals and business impact (see step 1) (how much does a 1% increase in click rate relate to revenue?)\nDiscuss any conflicting results (e.g. rise in daily active users and bounce rate)\n\nTranslate how each affects the user and the company\n\nShort term vs long term impact\n\nDo the benefits (e.g. dau) outweigh the drawbacks (e.g. bounce back) over the long term?",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-ass",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-ass",
    "title": "17  A/B Testing",
    "section": "17.5 Assignment",
    "text": "17.5 Assignment\n\nMisc\n\nSpookyHash hash function meets all 4 characteristics of a good assignment algorithm + it’s fast (see below for characteristics)\n\nFNV hash function fails #3\nMD5 hash function is ~50% slower than SpookyHash\n\nUse a population split (e.g. userid, sessionid, cookieid). Never do a time split because it violates the identity assumption (i.e. the statistical properties of the control and treatment groups are identical), and the test results are therefore inconclusive\nCookies have a limited life-time which makes it difficult to measure the long-term user impact\n\nSome browsers like Safari’s Intelligent Tracking Prevention (ITP) delete some cookies after 7 days, so if a test runs for more than a week, then users will be re-assigned after each week.\nEU’s General Data Protection Regulation (GDPR) states that website owners must receive explicit user consent before they use any cookies except those that are “strictly necessary” (and it’s hard to argue that A/B testing is strictly necessary to run a service)\n\n\nPotential Issues\n\nAssignment of ineligible users - These may be bots or users that already have an account.\n\nIf we include many ineligible users in our analysis, we may underestimate the effect size even if their distribution across groups is uniform.\n\nCrossovers - These are users that manage to experience both variants.\n\nExample:  Users may come across our site on mobile with the “sign up today!” (exposure) text, and then switch to desktop and see the “sign up” (control) message.\nDepending on the instrumentation we have in place, we may not be able to detect such users, or we may only detect them if they sign up on one device and then log in on the other device.\n\nAssignment without exposure - Due to implementation constraints, we may not be guaranteed that assigned users are actually exposed to the treatment and control.\n\nExample: it may be that the assignment is done on the backend while exposure happens conditionally and asynchronously on the frontend – some users may bounce in the gap between assignment and exposure, and never see the exposure (e.g. call-to-action text, sign up today!“ ).\n\nMultiple exposures - Once a user has been assigned, they may get exposed to the treatment and control multiple times (without crossing over).\n\nExample: Users may visit the landing page repeatedly and see the “sign up” (control) or “sign up today!” (exposure) text multiple times before deciding to sign up.\n\n\nCharacteristics of a good assignment algorithm (Paper)\n\nUsers must be equally likely to see each variant of an experiment (assuming a 50–50 split). There should be no bias toward any particular variant. (i.e. uniform distribution)\n\nCan run a Chi-Square or KS test to compare the random numbers generated (see R in example) to the uniform distribution.\n\nRepeat assignments of a single user must be consistent; the user should be assigned to the same variant on each successive visit to the site.\nWhen multiple experiments are run concurrently, there must be no correlation between experiments. A user’s assignment to a variant in one experiment must have no effect on the probability of being assigned to a variant in any other experiment.\nThe algorithm should support monotonic ramp-up, meaning that the percentage of users who see a Treatment can be slowly increased without changing the assignments of users who were already previously assigned to that Treatment.\n\nExample: Wish AI (article)\n\n\nSteps\n\nConcatenate salt and user ID to get string, S.\n\nSalt is just a string of letters & numbers (e.g. E1F53135E559C253) that gets concantenated to an object to provide an extra layer of security\nUser ID can also be cookie ID, session ID, etc.\n\nApply a Hash function to map the concatenated string, S, to a hash value H. Note, H follows a uniform distribution due to the uniformity property of hash values.\nAssuming the hash function is 64 bit, H is then divided by float(0xFFFFFFFFFFFFFFFF) and multiplied by 10,000 to get a uniform random number integer R ranging from 0 to 9,999.\nDivide R by 100. If R/100 &gt;= exposure_rate (e.g., 10%) times 100, we assign ignore to this user, and the user will be excluded in any calculations for this experiment.\nR modulo 100. Assuming there are two experiment buckets: control, treatment. If the remainder is &lt; control bucket percentage (e.g. 50%) time 100, assign control. Otherwise, assign treatment.\n\nOne random number R is generated. The first two digits of R are used for determining exposure, and the last two digits are used for assigning treatment/control buckets. The first two digits and the last two digits of R are independent\n\nSample Ratio Mismatch (SRM) Check\n\n\nMisc\n\nMake sure to use “users” and not “visits” or “sessions” for the check\nDon’t be concerned if early in the experiment, some mismatch occurs\n\nCheck for glaringly large or small ratios\n\ne.g. If you see 1,000 users in one group and 100 in the other, you know there’s a problem\n\nCalculate the sample ratios to check ratios closer to one that might still be a problem\n\ncontrol = users_in_control / total_users_in_test\ntreatment = users_in_treatment / total_users_in_test\n\nTest counts with Chi-Square Test\ngroup_counts &lt;- c(170471, 171662)\np &lt;- c(0.50, 0.50)\nchisq.test(x = group_counts, p = p)\n#&gt; Chi-squared test for given probabilities\n#&gt; data: group_counts\n#&gt; X-squared = 4.146, df = 1, p-value = 0.04173\n\nP-values &lt; 0.01 indicate a SRM\n\nFor this use case, using 0.05 results in too many false positives (see article)\nWe expect the type I error rate (FPR) to be less than 1%\n\n**Be aware that the more you test, the greater probability of getting a false positive**\n\nThe probability of obtaining a false positive using a Chi-squared test configured at the 0.05 level can increase up to 0.14 with as few as five usages\nShould apply a multiple-testing correction to the p-value (e.g. bonferoni)\n\nSee Romano-Wolf correction\n\nSee Statistical Concepts &gt;&gt; Null Hypothesis Significance Testing (NHST) &gt;&gt; Romano and Wolf’s correction\nSimilar to Westfall-Young but less restrictive\n\n\n\nTest counts with the sequential SRM test\n\nBayesian method\nSee github for code, blog and paper links",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-pb",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-pb",
    "title": "17  A/B Testing",
    "section": "17.6 Potential Biases",
    "text": "17.6 Potential Biases\n\nAlso see\n\nExperiements, RCT &gt;&gt; Sources of Bias\nExperiments, Planning &gt;&gt; Misc\n\nThese biases can cause violation in assumptions for difference-in-means post-hoc tests (e.g. Wilcoxon signed-rank test, Welch’s t-test)\n\nIndependence of sampling between groups (e.g. Interference between variants, see below)\n\nassigning samples to groups (e.g. treatment/control) at the user level, rather than the visit level, can ensure there is no cross-pollution of effects between groups if the user can see both versions.\n\nIndependence and identical distribution (iid) of sampling within groups (e.g. treatment/control)\n\nCondorcet Voting Paradox (article)\n\nWhen testing more than 1 treatment sequencially in binary testing scenarios, order of testing may determine the outcome\nCustomer segments or groups may not have identical transitive preferences\n\nExample: Customer Segment preferences for treatments X, Y, Z are different (all segments have equal sample sizes)\n\nSegment 1: X &gt; Y &gt; Z\nSegment 2: Y &gt; Z &gt; X\nSegment 3: Z &gt; X &gt; Y\n\nScenario 1: Test X against Y in an A/B test, then test the winner against Z\n\n1st rd A/B testing: Segments 1 and 3 prefer X to Y, so X wins the first round of testing.\n2nd rd A/B testing: Segments 2 and 3 prefer Z to X, so Z wins is the overall winner\n\nScenario 2: Test Y against Z in an A/B test, then test the winner against X\n\n1st rd A/B testing: Segments 1 and 2 prefer Y to Z\n2nd rd A/B testing: Segments 1 and 3 prefer X to Y, so X is the overall winner.\n\n\nSuppose the three segments represent 45%, 35%, and 20% of the market respectively. We can still have any option be the final winner, depending on the order of testing.\n\nBut now some tests are better than others. If we tested all three options at once in an A/B/C test, we’d learn that a plurality of the market prefers X, and we’d learn that there is no option that the market as a whole prefers.\n\n\nPrimacy or Novelty effect\n\nAfter a short time period after implementation, the effect measured in the A/B experiement sharply degrades or grows.\nPrimacy Effect: A new feature is implemented and subgroup of users are reluctant to change. This subgroup may initially use the product less but eventually return to normal usage or ramp up their usage or potentially churn\nNovelty Effect: A new feature is implemented and a subgroup of users who are excited by change begin to use the product more often. This subgroup may initially use the product more but eventually ramp down their usage.\nSince these affects shouldn’t effect first time users, either:\n\nConduct the A/B experiment on first time users, or\nAfter the experiment, investigate by comparing effects of first time users to returning users\n\nA long-run holdout (keeping a small control group for a long period of time) is one way to help capture this over a longer period of time\n\nInterference between variants\n\nCan happen when multiple variants of a feature are being test at the same time\nAssumption of user independence is violated\nCase 1: Social Media\n\nUsers within the same social network influence each other\nIf one user posts in their social media about using a product, then that influences the usage of another user in that person’s social network. If one person is in the experimental group and the other is in the control group, then the experiment is biased\nThis bias dampens the measured effect: actual effect &gt; measured effect\nSolutions:\n\nCreate network clusters and then randomize by cluster\nEgo-network randomization (linkedin, paper)\n\nmore scalable than randomizing by network clusters\nfocal node known as the Ego, and the nodes to whom ego is directly connected to, called Alters, with edges showing links between ego to altars or between altars\nA network effect is said to take place when a new feature not only impacts the people who receive it, but also other users of the platform, like their connections or the people who follow them\n\n\n\nCase 2: Two-sided markets\n\nCompany resources (e.g. people) are finite. Therefore if one variant is popular, then it will consume more company resources which steals resources away from other variant groups.\nUber has a finite amount of drivers. If a feature variant becomes popular before other variants, then more users in that variant group will request more rides which means more drivers will be occupied by that variant group. This reduces driver availability for other variant groups which inflates the effect for first variant to acquire an outsized proportion of the company’s resources.\nThis bias inflates the measured effect for some variants and dampens them in others.\nSolutions:\n\nGeo-based randomization\n\nCompany resources are usually allocated by region. So by splitting variant groups according to region, resources from one region can’t be leached by another region and therefore one feature variant can’t leach resources from another variant\n\nTime-based randomization\n\nSelect a day of the week for each variant to be ran. Since Tuesday’s resources can’t be used on Wednesday, a variant can’t leach resources from another variant\nBest used when treatment effect is short-lived (e.g. surge pricing, not referral programs)\n\n\n\n\nType I error (aka false positive): the models perform equally well, but the A/B test still produces a statistically significant result. As a consequence, you may roll out a new model that doesn’t really perform better. It’s a false positive. You can control the prevalence of this type of error with the p-value threshold. If your p-value threshold is 0.05, then you can expect a Type I error in about 1 in 20 experiments, but if it’s 0.01, then you only expect a Type I error in only about 1 in 100 experiments. The lower your p-value threshold, the fewer Type I errors you can expect.\nType II error (aka false negative): the new model is in fact better, but the A/B test result is not statistically significant. In statistical terms, your test is underpowered, and you should either collect more data, choose a more sensitive metric, or test on a population that’s more sensitive to the change.\nType S error (sign error): the A/B test shows that the new model is significantly better than the existing model, but in fact the new model is worse, and the test result is just a statistical fluke. This is the worst kind of error, as you may roll out a worse model into production which may hurt the business metrics.\nType M error: (magnitude error): the A/B test shows a much bigger performance boost from the new model than it can really provide, so you’ll over-estimate the impact that your new model will have on your business metrics.\nRandom assignment may fail to distribute power users equally: If your user population contains a few users that create a large amount of user activity, then a random A/B assignment is not guaranteed to distribute these users equally. This may violate the identity assumption and make the test results mode difficult to interpret.\nTreatment self-selection: If users themselves can opt into a treatment group, the A/B test violates the identity assumption: the two groups are not identical, but instead the treatment group consists of a particular subset of users, namely those more willing to take part in experiments. In that case we will not be able to tell whether the difference between the groups is due to the treatment or due to the sample differences.",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-met",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-met",
    "title": "17  A/B Testing",
    "section": "17.7 Metrics",
    "text": "17.7 Metrics\n\nAlso see\n\nGoogle, Analytics, Reports\nMarketing\nExperiments, Analysis &gt;&gt; A/B Post-Hoc Analysis\n\nMost A/B tests should be based on user-based metrics, ideally Average Revenue per User. Having user-based instead of session-based metrics means that:\n\nThe results are much less ambiguous in interpretation and can have a much greater impact on decision-making.\nUnlike metrics based on sessions or page views, user-based metrics are as close as possible to independent observations which is a crucial assumption in many statistical tests, thus promoting statistical validity.\n\nFocus on ‘fast-twitch’ metrics, such as those measuring conversion rates or user engagement: this might be a difference between the two groups you can meaningfully measure (and that you know in the long-run is correlated with driving more revenue or more users)\n\nrevenue or the number of users are likely long-run metrics and due to the experiment (e.g. changing the color scheme of your website), it may not possible to produce a measurable effect for one experiment using those metrics\n\nLook at the historical movements of any metric to understand how noisy it is, if it is impacted by seasonality etc.\nUtilize guard-rail metrics to make sure that if your experiment metric is positive that it isn’t similtaneously driving down another important metric.\n\nExample: Recommendation module change cannabalizes usage of other tools\n\nA typical A/B test of a recommendation module commonly involves the change in the underlying machine learning algorithm, its user interface, or both.  The recommendation change significantly increased users’ clicks on the recommendation while significantly decreasing users’ clicks on organic search results.\n\n\nGross Merchandise Sales (GMS)\n\nThere is an intuitive explanation to the drop in search clicks: users might not need to search as much as usual because they could find what they were looking for through recommendations.  In other words, improved recommendations effectively diverted users’ attention away from search and thus cannibalized the user engagement in search.\n\n\nBe careful of divisor metrics\n\nExample: You might see the average daily engagements per active user go up in the variant group, but if the number of users who are actually active in the variant group drops over the testing period because they don’t like the feature, you have a bias that you are only measuring the improvement from the users who continue to remain active Don’t test too many metrics (multiple-testing problem), be selective\nUsing a correction (e.g. Bonferroni) also reduces the power of your experiment\n\nSee Romano-Wolf Correction\n\nSee Statistical Concepts &gt;&gt; Null Hypothesis Significance Testing (NHST) &gt;&gt; Romano and Wolf’s correction\nSimilar to Westfall-Young but less restrictive\n\n\nThe correction method should be based on the False Discover Rate (article)",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-interlv",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-interlv",
    "title": "17  A/B Testing",
    "section": "17.8 Interleaving Experiments",
    "text": "17.8 Interleaving Experiments\n\nExperiments where you present to each user both the control and the treatment, and see which version they prefer\nBecause each user gets to directly select from the control and the treatment, we should get test results sooner compared to traditional A/B testing with two populations\n\nNetflix report that they need 100X fewer users to achieve 95% experimental power (the equivalent of recall in an A/B experiment) compared to traditional, population-based A/B testing.\n\nExample: Team-Draft Interleaving for recommender models\n\n\nThe recommendations shown to the user are a mix of the results from model A and model B:\n\nThe two models simply take turns contributing their highest ranked recommendatiion that is not yet in the interleaved list\nThe model that gets to pick first is selected by a coin flip",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-univhold",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-univhold",
    "title": "17  A/B Testing",
    "section": "17.9 Universal Hold-Out",
    "text": "17.9 Universal Hold-Out\n\nRandomly sampled group that isn’t exposed to any treatments during a period where multiple experiments are taking place\nNotes from: Universal Holdout Groups at Disney Streaming\nBenefits\n\nDetermine accurate lifts of cumulative product change efforts (i.e. total treatment effect from all experiments)\nVerify if changes made have lasting impact, or are ephemeral (e.g. novelty effect).\nObserve potential long-term changes in metrics that the core product typically can’t influence with a single change, such as retention.\nInnovate faster by running more experiments simultaneously on fewer users; leave it to the universal holdout to evaluate the lift of the winners.\n\nHow long should we run each holdout for?\n\nIf too long:\n\nIncreased engineering costs of maintaining two separate experiences for every experimental change\nPotential negative impact for non-exposed customers\n\nIf too short:\n\nCannot assess the long-term impact of product changes\n\nExample: Disney Streaming/Hulu resets the universal holdout group every 3 months (i.e. 1 quarter)\n\nPower analysis\n\nDecide on the size of the effect size you want to be able to detect\n\nExample: A 1% change is large enough to drive a financially meaningful ad revenue impact for Hulu.\n\n\nHoldout then evaluate\n\n\nSteps\n\nFor the first three months — the “enrollment period” — we actively sample a fixed percentage of visiting users into the experiment.\n\n“into the experiment” - Is the experiment stage?\n\nFor the fourth month — the “evaluation period” — we stop sampling users into the experiment, and assess the impact of changes over the course of one month\n\nRun a standard A/B experiment for 1–3 weeks. (dunno if this belongs here or not)\nRelease the product change to all users who are not in the universal holdout group (if the results from stage one are positive).\n\n\nThis doesn’t make sense to me (could be the chart)\n\nSooo, does this mean that they aren’t actually collecting any data for the first 3 months? It’s just picking people?\nSupposedly Disney resets their holdout group every 3 months, but in the chart it’s extended to 4 months\n\nThey would analyze treatment effects during the 4th month of the holdout vs treatment\nThis makes more sense to me, but I don’t think this is what’s happening\n\nIf the chart is wrong and the holdout is for 3 months then reset, then is the chart saying there’s no control group for that particular experiment during the evaluation period?\nThe evaluation procedure would clear some things up.",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-tandr",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-tandr",
    "title": "17  A/B Testing",
    "section": "17.10 Test and Roll",
    "text": "17.10 Test and Roll\n\nFrom Test & Roll: Profit-Maximizing A/B Tests\nBayesian, small sample test design\n\nThink this is an extension/improvement upon Thompson Sampling (see below, Multi-Armed Bandit Algorithms &gt;&gt; Thompson Sampling)\n\nWhile these tests have traditionally been analyzed using hypothesis testing, we re-frame them as an explicit trade-off between the opportunity cost of the test (where some customers receive a sub-optimal treatment) and the potential losses associated with deploying a sub-optimal treatment to the remainder of the population.\na closed-form expression is derived for the profit-maximizing test size and show that it is substantially smaller than typically recommended for a hypothesis test, particularly when the response is noisy or when the total population is small.\nThe common practice of using small holdout groups can be rationalized by asymmetric priors. The proposed test design achieves nearly the same expected regret as the flexible, yet harder-to-implement multi-armed bandit under a wide range of conditions.\nWe [Feit and Berman] demonstrate the benefits of the method in three different marketing contexts—website design, display advertising and catalog tests—in which we estimate priors from past data. In all three cases, the optimal sample sizes are substantially smaller than for a traditional hypothesis test, resulting in higher profit.",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-mbandit",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-mbandit",
    "title": "17  A/B Testing",
    "section": "17.11 Multi-Armed Bandit (MAB) Algorithms",
    "text": "17.11 Multi-Armed Bandit (MAB) Algorithms\n\nCompeting ad designs are viewed as different slot machines each with its own rate of success (conversion). We want to find the slot machine with the best rate and then keep pulling its arm.\nMisc\n\nNotes from\n\nhttps://www.inwt-statistics.com/read-blog/multi-armed-bandits-as-an-a-b-testing-solution.html\n\nPackages\n\n{contextual}\n\n\nA/B Experiment Issues\n\nLarge experimentation costs. Because all the competing treatments in the A/B test are guaranteed a fixed portion of the sample size, even a “bad” treatment can be exposed to a significant amount of users and it could be hurtful to the user experience. A longer experiment means even larger experimentation costs.\nProne to erroneous decisions if not analyzed correctly. The A/B tests are designed to be analyzed only when the targeted sample size is reached. But inexperienced and impatient experimenters are often inclined to peek at results and make decisions before the experiments, which could lead to erroneous conclusions.\n\nMAB Benefits A MAB algorithm will provide a principled way to iteratively adjust the assignment ratio throughout the experiment until the best treatment receives the majority of the sample.\n\nMAB has the advantage of reducing the opportunity costs from the experimentation and is immune to peeking.\n\nEpsilon Greedy (ε-greedy)\n\nAfter an initial number of purely exploratory trials, a random lever is pulled a fraction ε of the time. The rest of the time (1 - ε), the lever with the highest known payoff is pulled.\n\nExample: if we set ε to 0.10, 10% of the time the algorithm will explore random alternatives, and 90% of the time it will exploit the variant that is performing the best.\n\nWe can expand on the ε-greedy algorithm by having it reduce the value of ε over time, thus limiting the possibility of continuing to explore once we’re aware of each lever’s payoff. This is referred to as Decayed Epsilon Greedy.\nThe disadvantage of exploring alternatives randomly is it is possible to run into bad actions that you have already run into before. (solution: UCB)\n\nUpper Confidence Bounds (UCB)\n\nAssumes that the unknown payoff of each lever is as high as the observable data indicate is possible. UCB initially explores randomly to try to get an understanding of the true probability of a lever being successful. Then, the algorithm adds an “exploration bonus” to levers it is unsure about, prioritizing those until it becomes more sure about the lever’s performance. This exploration bonus decreases over each round, similar to the Decayed Epsilon Greedy in that respect.\n\nExample: A person using this algorithm would likely choose one of the restaurants where his reviews have been very different between his different visits. If he has already been to a restaurant four times and the ratings have always been relatively the same non-optimal result, it is very unlikely to be chosen since going to this restaurant for a fifth time would likely produce the same results.\n\nEquations\n\\[\n\\begin{align}\n&a_t^{\\text{UCB}} = \\text{argmax}_{a \\in A} \\hat Q_t(a) + U_t(a)\\\\\n&\\text{where}\\;\\; U_t(a) = \\sqrt{\\frac{2\\log t}{N_t(a)}}\n\\end{align}\n\\]\n\n\\(a\\) - Action\n\\(N_t(a)\\) - The number of times a has been performed at time, t\n\\(\\hat Q_t(a)\\) - ?\n\\(U_t(a)\\) - Exploration bonus at time, t?\nThe more the same action is performed, the more \\(N_t(a)\\) increases and therefore \\(U_t(a)\\) decreases which results in at being less likely to be chosen.\n\n\nThompson Sampling\n\nIt is also possible that a winning lever may initially appear weak, and thus not be explored by a “greedy” algorithm sufficiently enough to determine its true payoff. Thompson Sampling is a Bayesian, non-greedy alternative. In this algorithm, a probability distribution of the true success rate is built for each variant based on results that have already been observed. For each new trial, the algorithm chooses the variant based on a sample of its posterior probability of having the best payout. The algorithm learns from this, ultimately bringing the sampled success rates closer to the true rate.",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-analysis.html",
    "href": "qmd/experiments-analysis.html",
    "title": "18  Analysis",
    "section": "",
    "text": "18.1 Misc",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-analysis.html#sec-exp-anal-misc",
    "href": "qmd/experiments-analysis.html#sec-exp-anal-misc",
    "title": "18  Analysis",
    "section": "",
    "text": "Also see Post-Hoc Analysis, ANOVA &gt;&gt; ANCOVA\nRecommended metrics to be reported for medical studies (Harrell). Some of this is perhaps generalizable to any RCT with a binary outcome.\n\nThe distribution of Risk Difference (RD)\ncovariate-adjusted OR\nadjusted marginal RD (mean personalized predicted risk as if all patients were on treatment A minus mean predicted risk as if all patients were on treatment B) (emmeans?)\nmedian RD\n\nAnalysis of a two armed trial comparing a treatment to placebo:\n\nQuestions\n\nWas there a treatment effect in this trial?\nWhat was the ATE of this trial?\nWas the treatment effect identical for all patients in the trial?\nWhat was the treatment effect for the different subgroups of the trial?\nWhat will the treatment effect be when used more generally (outside the trial)?\n\nStrategies\n\n(Predictive) adjustment variables (e.g. Age) are good to help answer Q1 & Q2\nInteractions (group_cat ⨯ treatment) could help answer Q3 and Q4.\nQ5 is about external validity (see Diagnostics, Classification &gt;&gt; Terms)\n\n\nAnalysis shouldn’t only include Intent-to-Treat effects\n\nTo adequately guide decision making by all stakeholders, report estimates of both the intention-to-treat effect and the per-protocol effect, as well as methods and key conditions underlying the estimation procedures.\n“Intent-to-treat analysis makes sense from a public health point of view if it closely reflects the actual medical practice. But from a patient point of view of making a decision regarding treatment, the actual treatment is more meaningful than intent-to-treat. So, when the two estimates differ considerably, it seems to me that they should both be reported – or, at least, the data should be provided that would allow both analyses to be done.” Lehman from Gelman blog post\n“The correct model models the 4 groups and the conditional probability to be in the 4 groups as a function of pre-treatment covariates, including both psychological covariates, cultural covariates, as well as symptomatic and disease progression knowledge.” Lakeland\n\nAnalysis of substudies must be separate\n\nExample of Study Effects: 2 Studies with the same treatment (Lumiracoxib) but different controls (ibuprofen, naproxen) (article)\n\nCells are p-values\nBottom table shows when a “Sub-study” indicator is used as a model term (“Treatment given Sub-study”) vs not used (“Treatment”), the p-values for all the “Demographic Characteristic” variables lose significance.",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-analysis.html#sec-exp-anal-comp",
    "href": "qmd/experiments-analysis.html#sec-exp-anal-comp",
    "title": "18  Analysis",
    "section": "18.2 Compliance",
    "text": "18.2 Compliance\n\nRefers to how observations respond to treatment.\nFull compliance with treatment means that all units to whom a program (i.e. treatment) has been offered actually enroll, and none of the control (aka comparison) units receive the program\nExclusion of some randomized subjects can be justified (such as patients who were deemed ineligible after randomization or certain patients who never started treatment) when analyzing the results of an experiment.\nTypes\n\nAlways-Takers: A person who receives the treatment regardless of the assignment (or instrument in IV).\nNever-Takers: A person who never receives the treatment regardless of the assignment (or instrument in IV).\nCompliers: A person who receives the treatment only when assigned to the treatment group (or when the instrument compels them to).\nDefiers: A person who receives the treatment only when they are NOT assigned to the treament group (or the instrument does not compel them to). They always do the opposite of what the assignment mechanism instructs them to do (assholes). Usually a reasonable assumption to make that there are none in your (quasi-) experiment, but it can only be made based on intuition\n\nExample: receiving additional years of Education is the treatment; whether an educational Reform was implemented in their region is the instrument\n\nA person who decides to stay in school regardless of whether there’s a government policy to do so would be an Always-Taker (EDUCATION=1, regardless of REFORM )\nCompliance Modeling (Lakeland thread)\n\nModel how people make the decision to adhere to the randomization or not (“Person makes decision about actual treatment” step)\n\nRealistic Model: Enroll person \\(\\rightarrow\\) Randomize to treatment group \\(\\rightarrow\\) Person makes decision about actual treatment \\(\\rightarrow\\) Actual treatment occurs \\(\\rightarrow\\) Outcomes observed\n\nITT effect by itself is misleading\n\nSuppose you did a mouse experiment and randomized mice to different surgeries and then told the surgeons to do the surgery they were randomized to unless they see conditions A,B,C in which case do a different surgery, and then analyzed via intention to treat. That would just be disingenuous. The fact that we don’t know a precise rule for adherence doesn’t mean adherence isn’t part of the question. So build a probabilistic rule for adherence\n\nHighlights a source of uncertainty which should then increase the uncertainty in the final analysis, but would not be included in the ITT or any other frequentist analysis\nExample: Prostate Cancer\n\nPretreatment Survey\n\nYou’re trying to understand the patient’s state-of-mind.\nQuestions:\n\nWhat is your age\nWhat is your biological sex\nWhat race do you identify as (set of choices)\nWhat religious affiliation do you affiliate most with … (set of choices)\nOn the following scale rate the discomfort that your condition causes… 0,1,2,3,4,5,6 with some text descriptors from “none” to “severe discomfort”\nWhat is your education level (from some grade school to PhD, and a check mark for whether it’s a biology/medicine related degree)\nCheck all that apply: what are the primary motivations for entering the study (things like “to get free treatment” and “to improve the state of knowledge about the disease” and various other things)\nWhat is your income level?\nDo you have insurance that would cover this treatment outside the study? What is the level of coverage?\nWhat is your level of concern about having surgery? (from zero to “I am very nervous about having surgery”)\nWhat is your level of concern about potential side effects (similar scale)\nWhich side effects are you concerned about (check boxes)\nWhat are your existing thoughts about watchful waiting (0 I believe it is not right for me, up to 6 watchful waiting is my currently preferred treatment)\nWhat is your current belief about the severity of your condition: similar\nWhat is your current belief about the aggressiveness of your condition: similar\nWhat is your current belief about metastatic tumors: 0… I have no reason to believe… 6 … I have medical biopsy proving metastatic\n\n\nBayesian Procedure\n\nNow, hypothesize at least the *direction* that each of these affects the probability of compliance with surgery assignment, and with watchful waiting assignment.\nPlace a prior over coefficients of each one with bias towards the direction hypothesized in a logistic regression with nonlinear response on any variables that may seem appropriate.\nNow, hypothesize the direction with which some subset of these predicts actual aggressiveness of the underlying condition, for example using the patients own beliefs, using the reported level of symptoms, using info about metastatic condition, etc.\nPlace prior over coefficients in a hidden variable model for severity… again with nonlinear response if necessary.\nCollect dat\nPosterior distribution of parameters…\n\nModel: adherence ~ Binomial(inv_logit(k * symptom_severity * assigned_monitoring + random_individual_factor))\n\nWith random_individual_factor having a group level prior distribution gets an estimate of the probability of adherence\nSo you can fit probability of good outcome based on adherence with symptom_severity informing, telling you something about whether outcomes are due to selection or severity.",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-analysis.html#sec-exp-anal-effs",
    "href": "qmd/experiments-analysis.html#sec-exp-anal-effs",
    "title": "18  Analysis",
    "section": "18.3 Effects",
    "text": "18.3 Effects\n\nComparison\n\n\nATE is for the whole population (he should’ve had the arrow coming out of the word, population, or the edge of the circle)\nCircle is split into half: treatment (upper left, yellow), control/comparison (bottom right, pink)\n\nATT is for treated (treated compliers, always takers, treated defiers)\nATU is for control/comparison (non-treated compliers, never takers, non-treated defiers)\n\nLATE is for treated compliers\nCATE is for a subset of the population (e.g. men)\n\nHeterogeneous Treatment Effect (HTE) - Also called differential treatment effect, includes difference of means, odds ratios, and Hazard ratios for time-to-event outcome vars\n\nAscertaining subpopulations for which a treatment is most beneficial (or harmful) is an important goal of many clinical trials.\nOutcome heterogeneity is due to wide distributions of baseline prognostic factors. When strong risk factors exist, there is hetergeneity in the outcome variable.\n\nSolution: add baseline predictors to your model that account for these strong risk factors.\n\nHeterogeneity of Treatment Effects - The degree to which different treatments have differential causal effects on each unit.\nExamples\n\nThe effect of attending college on earnings differs across students\nThe effect of a state-wide smoking ban on smoking rates varies across states\n\n\nIntention-to-Treat (ITT) - estimates the difference in outcomes between the units assigned to the treatment group and the units assigned to the control (aka comparison group in quasi-experiments) group, irrespective of whether the units assigned to the treatment group actually receive the treatment. An intention-to-treat analysis is not feasible if trial participants are lost to follow-up\n\nPotential solution: weighted average of the outcomes of participants and non-participants in the treatment group compared with the average outcome of the control group\nExample: Doctor tells everyone in a treatment group to go home and exercise for an hour per day and tell the control group nothing.\n\nAfter a month, if you just compare the difference in mean blood pressures between the two groups, you get the intention to treat estimator\nDoesn’t tell you the causal effect of exercise on blood pressure, but the causal effect of telling people to exercise on blood pressure.\n\nThis estimate would be smaller than the treatment effect of exercise per se, as only a (small!) fraction of people in the treatment group would completely follow the treatment\n\n\n\nModified Intention-to-Treat (mITT) - No ineligible users. This applies to cases where we detect the ineligibility after assignment, but the eligibility criteria are based on factors that could have been known before the experiment. Hence, it should be safe to exclude the ineligible users after the fact\n\ne.g. bots and existing users should increase the observed effect size, but not change the preferred variant.\n\nModified Intention-to-Treat No Crossovers (mITTnc). If we have a mechanism to detect some crossovers, excluding them and comparing the results to the intention-to-treat analysis may uncover implementation bugs.\n\nCrossovers are users that experience both the treatment and control exposures or (unintentionally) more than one treatment\nIt’s worth noting that crossovers shouldn’t occur in cases where we can uniquely identify users at all stages of the experiment – it is a problem that is more likely to occur when dealing with anonymous users.\nAs such, and given the inability to detect all crossovers, A/B experiments should be avoided when users are highly motivated to cross over.\n\nExample: displaying different price levels based on anonymous and transient identifiers like cookies is often a bad idea.\n\n\nAverage Treatement Effect (ATE) - expected causal effect of the treatment across all individuals in the population\n\nOLS estimate, Yi = β0 + β1Xi + ui\n\nβ1 = ATE = E[Y |X = 1] − E[Y |X = 0] = E[β1,i ] = Average effect of a unit change in X\n\nConditional Average Treatment Effect (CATE) - ATE for a subgroup\n\nCoefficient for an interaction (e.g. explanatory*treatment)\n\nAlso see Generalized Additive Models (GAM) &gt;&gt; Interactions\n\n\nAverage Treatment Effect on the Treated (ATT) - expected causal effect of the treatment for individuals in the treatment group ATT = E[δ | D = 1] = E[Y1 − Y0 | D = 1] = E[Y1 | D = 1] − E[Y0 | D = 1]\n\nwhere δ: individual-level causal effect of the treatment and D is the treatment\nIn the ideal scenario of a randomized control trial (RCT) (commonly violated in observational studies), ATE equals ATT because we assume that:\n\nthe baseline of the treatment group equals the baseline of the control group (layman terms: people in the treatment group would do as bad as the control group if they were not treated) and\nthe treatment effect on the treated group equals the treatment effect on the control group (layman terms: people in the control group would do as good as the treatment group if they were treated).\n\nATT should be used instead of ATE when there’s extreme imbalance between covariate criteria of treated vs control/comparison groups (e.g. quasi-experiment)\n\n\nAlso see Econometrics, General &gt;&gt; Propensity Scoring\nOverlap plot or balance plot from video\n\n{cobalt} may provide a way generate these\ny-axis: count, x-axis: covariate, color: treatment\n\nThe range of x covered by blue (treatment) is much smaller than the range of x covered by red (control), therefore ATT might be a better choice of estimated effect\n\n\n\nLocal Average Treatment Effect (LATE) - applies when there is noncompliance in the treatment group, comparison group, or both simultaneously.\n\nIf there is noncompliance in both the treatment and comparison group, then the LATE estimate is valid only for those in the treatment group (who enrolled in the program; i.e. treated) and (who would have not enrolled had they been assigned to the control/comparison group).\n“who would have not enrolled had they been assigned to the comparison group” is a weird counterfactual\n“Local” indicates that LATE is the average effect for the group known as compliers\nTreatment and Instrument are binary variables\n\nIV models still valid for treatments and instruments with more than 2 levels, but effect calculation is more complicated\n\nCalculation (always-takers and defiers are assumed not to exist)\n\nLATE = (avg potential outcome of compliers who do receive treatment) - (avg potential outcome of compliers who don’t receive treatment)\nThe (avg potential outcome of compliers who don’t receive treatment) has to be solved for.\n\nGiven that we know the proportions and outcomes for the compliers and never-takers in our treatment group, you can solve a simple equation for this quantity.\nSee video for details\n\nThink this is the primary estimate of an IV model as well (see Econometrics, General &gt;&gt; Instrumental Variables)\n\nTreatment-on-the-treated (ToT) is simply a LATE in the more specific case when there is noncompliance only in the treatment group. Estimates the difference in outcomes between the units that actually receive the treatment and the comparison group (Seems similar to ATT)\n\nPer-Protocol Effect (PPE) - the effect of receiving the assigned treatment strategies throughout the follow-up as specified in the study protocol\n\ni.e. the effect that would have been observed if all patients had adhered to the protocol of the RCT\nAlternative to the intention-to-treat effect that is not affected by the study-specific adherence to treatment\nValid estimation of the per-protocol effect in the presence of imperfect adherence generally requires untestable assumptions\nApproaches below are generally invalid to estimate the per-protocol effect. (G-estimation and instrumental variable methods can sometimes be used to estimate some form of per-protocol effects even in the presence of unmeasured confounders)\n\n(biased) Approaches:\n\nAs-Treated: Compare the outcomes of those who took treatment (A=1) and didn’t take the the treatment (A=0) regardless of their assignment\n\nPr[Y=1|A=1] − Pr[Y=1|A=0]\n\nPer-Protocol: Compare the outcomes of those who took treatment (A=1) among those assigned to Treatment (Z=1) to those who didn’t take the treatment (A=0) among those assigned to Control (Z=0)\n\nPr[Y=1|A=1, Z=1] − Pr[Y=1|A=0, Z=0].)",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-analysis.html#sec-exp-anal-late",
    "href": "qmd/experiments-analysis.html#sec-exp-anal-late",
    "title": "18  Analysis",
    "section": "18.4 Calculating LATE",
    "text": "18.4 Calculating LATE\n\nMisc\n\nNotes from https://chris-said.io/2021/03/28/youre-measuring-wrong/\n\nExperiment\n\n\nz is the treatment availability assignment\ncp is shorthand for complier, referring to people who complied with the instructions.\nYcp,z = 1 therefore represents the average outcome of the group that actually received treatment, since they were compliers (cp) who were randomly assigned to treatment availability (z=1).\nYz = 0 represents the average outcome of the control group (z=0).\n\nLocal Average Treatment Effect (LATE)\n\n\nThe LATE tells you how much the treatment affects the people who actually got treated\nδcp = Ycp,z = 1 - Y0cp,z=1\nδcp is the Local Average Treatment Effect (LATE), since it reflects the impact of the treatment on a particular subpopulation (subpopulation being the compliers who were treated)\nYcp,z = 1 is the average outcome for compliers who were treated\nY0cp,z=1 is the average outcome for compliers if they hypothetically weren’t treated (counterfactual)\n\nUsing substitution and some algebra (see article above for details), the counterfactual part can be avoided and this equation becomes\n\n\nπcp is the fraction of compliers\n\nBias within complier group\n\nGroup’s counterfactual outcomes might be different from other groups.\n\nLATE accounts for that by correctly reporting the impact of the treatment relative to the counterfactual.\n\nThe treatment might be more effective in the complier group than in the never-taker group.\n\nThat bias is unescapable and is known as a heterogenous treatment effect. The way to deal with this bias is to acknowledge it transparently.\n\n\nIf treatment involves more than a single dose, and people can withdraw midway through the program.\n\nReport the Intention To Treat (ITT) metric, which is the impact of being assigned to treatment (Yz=1 − Yz=0) rather than the impact of being treated.",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-analysis.html#sec-exp-anal-csm",
    "href": "qmd/experiments-analysis.html#sec-exp-anal-csm",
    "title": "18  Analysis",
    "section": "18.5 Change Score Models",
    "text": "18.5 Change Score Models\n\nChange Scores - subtract the baseline value of the outcome from the value measured at the end of the study and use that difference for your statistical tests or models.\nMisc\n\nSee tutorial, https://github.com/CRFCSDAU/EH6126_data_analysis_tutorials/blob/master/Unit_1_Review/Change_scores/Change_scores.md\nReason: Randomization of participants will result in groups (e.g. treated/control) that are comparable “on average” over many hypothetical trials, at the end of the day, we just have the one trial that we actually ran. And for that one trial there really could be important differences between the groups at baseline that could lead to errors of inference (e.g. concluding the treatment is beneficial when it isn’t).\nExample: a trial for a blood pressure medication that we hope will lower patients’ SBP values. So we set up the trial, recruit some patients and randomize them into two groups. Then we give one group the new medication we are testing, and the other gets standard-of-care. At the end of the study we compare the mean blood pressure of the two groups and find that the active group had a SBP that was 3 mmHg lower, on average, than the values seen in the control group. We might thus conclude that the treatment worked. However, what if it just so happened that the active group also had a similarly lower mean blood pressure (vs the other group) measured at baseline, before the intervention?\n\nExample: Change Score Model\n\nw1 &lt;- \n  glm(data = dw,\n      family = gaussian,\n      (post - pre) ~ 1 + tx)\nSpecification\n\nWhere\n\npost, pre are pre-treatment, post-treatment measurements of the outcome variable\ntx is the treatment indicator variable\nβ0: population mean for the change in the control group\n\neasier to interpret if “pre” is mean centered\n\nβ1: parameter is the population level difference in pre/post change in the treatment group, compared to the control group.\n\nAlso a causal estimate for the average treatment effect (ATE) in the population, τ",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-analysis.html#sec-exp-anal-exmpls",
    "href": "qmd/experiments-analysis.html#sec-exp-anal-exmpls",
    "title": "18  Analysis",
    "section": "18.6 Examples",
    "text": "18.6 Examples\n\nExample: pretest-posttest between-person factorial design to compare competing theories on depression and suicidal behaviors. (link)\nExample: Gelman (general approach)\n\nNotes from: Article\nEDA\n\n\nVariables\n\nOutcome:\n\n(top) Absolute EEG (brainwaves) power; X-axis is frequency\n(bottom) log(Absolute EEG (brainwaves) power); X-axis is frequency\n\nBlue: Each control group patient\nRed: Each treatment group patient\n\nInterpretations\n\nRaw, z-scores, Relative to mean:\n\nNot logged: substantial overlap between Control and Treatment groups\nlog: Control and Treatment groups almost completely overlap\n\nGroup averages relative to the mean at each frequency:\n\nSo grouped by group, frequency–&gt; mutate(mean)?\nSubstantial differences\n\n\nCompare sample data with random-chance data\n\nKeep the same observations but randomly permute the treatment assignment variable and see what happens\nRepeat (e.g. 9 times)\n\n\n\n\nGroup averages: patterns in these random permutations don’t look so different, either qualitatively or quantitatively, from what we saw from the actual comparison\n\nThe red line is on top most of the time and substantially separated from the blue line 3 out fo the 9 times.\n\n\n\n\nlog response if:\n\nall the measurements are positive\nSeems reasonable to start with proportional effect\n\ninclude pre-test brain activity as a predictor (baseline)\nfit y ~ z + x\n\ny = outcome (log brain activity),\nz = treatment indicator\nx = pre-treatment measure\n\nTry including interaction of x and z\nPlot y vs. x with blue dots for the controls (z=0) and red dots for the treated kids (z=1).\n\nExample: Soloman, RCT, 2 groups, unequal treatment schedule\n\n\nNotes from: Thread\nfirst three time points are baseline, mid-treatment, and immediately post-treatment. The last two are follow-ups\nNonlinear means indicate a GAM would be a good option\nSolution\n\nGAM: mgcv::gam(data, y ~ 1 + group + s(weeks, by = group, k = 4) + s(week, subject, bs = \"fs\", k = 4))\n\nrandom smooth for subjects not just a random intercept\n“subject” is a factor variable\n“fs” is a factor smooth spline\n\npredict(fit, exclude = \"s(weeks,subject)\")\n\ncomputes the population fitted lines\ngratia::smooths(model) will return the labels for all smooths in the model so you know what you need for the exclude call without having to call summary()\n\n\n\nExample: Randomized Complete Block Design (RCBD)\n\nnotes from https://www.r-bloggers.com/2020/12/accounting-for-the-experimental-design-in-linear-nonlinear-regression-analyses/\nData\nObs block trt\n1   2     B\n2   2     C\n3   2     A\n4   2     D\n5   2     E\n6   2     F\n7   1     B\n8   1     C\n9   1     E\n10  1     A\n11  1     F\n12  1     D\n13  3     D\n14  3     A\n15  3     C\n16  3     F\n17  3     B\n18  3     E\n19  4     A\n20  4     F\n21  4     B\n22  4     C\n23  4     D\n24  4     E\nFitting a linear model (eda: check scatterplot of outcome vs treatment)\ndo not model with block as a fixed effect\n\n\nmod.reg &lt;- lm(yield ~ block + density, data=dataset)\n\nassumes that the blocks produce an effect only on the intercept of the regression line, while the slope is unaffected\ndo model with block as a random effect (i.e. block effect may produce random fluctuations for both model parameters, intercept and slope)\n\nmodMix.1 &lt;- lme(yield ~ density, random = ~ density|block, data=dataset)\n# or equivalently\nmodMix.1 &lt;- lme(yield ~ density, random = list(block = pdSymm(~density)), data=dataset)\n## Linear mixed-effects model fit by REML\n##  Data: dataset \n##        AIC      BIC    logLik\n##  340.9166 355.0569 -164.4583\n## \n## Random effects:\n##  Formula: ~density | block\n##  Structure: General positive-definite, Log-Cholesky parametrization\n##            StdDev    Corr \n## (Intercept) 3.16871858 (Intr)\n## density    0.02255249 0.09 \n## Residual    1.38891957       \n## \n## Fixed effects: yield ~ density \n##                Value Std.Error DF  t-value p-value\n## (Intercept) 31.78987 1.0370844 69  30.65311      0\n## density    -0.26744 0.0096629 69 -27.67704      0\n##  Correlation: \n##        (Intr)\n## density -0.078\n## \n## Standardized Within-Group Residuals:\n##        Min        Q1        Med        Q3        Max \n## -1.9923722 -0.5657555 -0.1997103  0.4961675  2.6699060 \n## \n## Number of Observations: 80\n## Number of Groups: 10\n\nIf there is NOT a strong correlation between the slope (e.g. listed above as corr = 0.09 for density) and intercept (i.e. correlated random effects) in the Random Effects section of summary(modMix.1), try modeling with the random effects as independent\n\nmodMix.2 &lt;- lme(yield ~ density, random = list(block = pdDiag(~density)), data=dataset)\n\n‘pdDiag’ specifies a var-covar diagonal matrix, where covariances (off-diagonal terms) are constrained to 0\ncheck if the change made a significant difference (i.e. pval &lt; 0.05)\n\nanova(modMix.1, modMix.2)\n\nOther options include: either random intercept or random slope\n\n# Model with only random intercept\nmodMix.3 &lt;- lme(yield ~ density, random = list(block = ~1), data=dataset)\n# Alternative notation\n# random = ~ 1|block\n\n# Model with only random slope\nmodMix.4 &lt;- lme(yield ~ density, random = list(block = ~ density - 1), data=dataset)\n# Alternative notation\n# random = ~density - 1 | block\n\nFitting a nonlinear model\n\nlibrary(aomisc)\ndatasetG &lt;- groupedData(yieldLoss ~ 1|block, dataset)\nnlin.mix &lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, \n                        fixed = list(i ~ 1, A ~ 1),\n            random = i + A ~ 1|block)\n# or equivalently\nnlin.mix2 &lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, \n                              fixed = list(i ~ 1, A ~ 1),\n                  random = pdSymm(list(i ~ 1, A ~ 1)))\n\n## Nonlinear mixed-effects model fit by maximum likelihood\n##  Model: yieldLoss ~ NLS.YL(density, i, A) \n##  Data: datasetG \n##        AIC      BIC    logLik\n##  474.8225 491.5475 -231.4113\n## \n## Random effects:\n##  Formula: list(i ~ 1, A ~ 1)\n##  Level: block\n##  Structure: General positive-definite\n##          StdDev    Corr \n## i        0.1112839 i   \n## A        4.0466971 0.194\n## Residual 1.4142009     \n## \n## Fixed effects: list(i ~ 1, A ~ 1) \n##      Value Std.Error  DF  t-value p-value\n## i  1.23242  0.038225 104 32.24107      0\n## A 68.52068  1.945173 104 35.22600      0\n##  Correlation: \n##  i     \n## A -0.409\n## \n## Standardized Within-Group Residuals:\n##        Min        Q1        Med        Q3        Max \n## -2.4414051 -0.7049356 -0.1805322  0.3385275  2.8787362 \n## \n## Number of Observations: 120\n## Number of Groups: 15\n\nExclude correlation between random effects (0.194 above) if not substantial for a simpler model\n\nnlin.mix3 &lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, \n                              fixed = list(i ~ 1, A ~ 1),\n                  random = pdDiag(list(i ~ 1, A ~ 1)))",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-designs.html",
    "href": "qmd/experiments-designs.html",
    "title": "19  Designs",
    "section": "",
    "text": "19.1 Misc",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Designs</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-designs.html#sec-exp-des-misc",
    "href": "qmd/experiments-designs.html#sec-exp-des-misc",
    "title": "19  Designs",
    "section": "",
    "text": "Causal Hierarchy\n\n\nPreference of experiment in measuring causality where RCT is the most desirable",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Designs</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-designs.html#sec-exp-des-types",
    "href": "qmd/experiments-designs.html#sec-exp-des-types",
    "title": "19  Designs",
    "section": "19.2 Types",
    "text": "19.2 Types\n\nRCT\n\nCannot return valid causal estimates of the treatment effect at the participant level, but it can return a valid causal estimate of the average treatment effect (ATE), in the population\n\nApproaches for estimating  the ATE\n\nChange-Score model (see Experiments, RCT &gt;&gt; Change Score Model)\nANCOVA model (see ANOVA &gt;&gt; ANCOVA)\n\n\nTypical procedure\n\nrecruit participants from the target population,\nmeasure the outcome variable during a pre-treatment assessment,\nrandomly assign participants into\n\na control condition or\nan experimental treatment condition,\n\ntreat the participants in the treatment condition, and\nmeasure the outcome variable again at the conclusion of treatment.\n\nReasons for not running a RCT\n\nIt’s just not technically feasible to have individual-level randomization of users as we would in a classical A/B test\n\ne.g. randomizing which individuals see a billboard ad is not possible\n\nWe can randomize but expect interference between users assigned to different experiences, either through word-of-mouth, mass media, or even our own ranking systems; in short, the stable unit treatment value assumption (SUTVA) would be violated, biasing the results\n\n\nQuasi-Experiemental\n\nDue to the lack of a random assignment, the treatment and control groups are not equivalent before the intervention. So, any differences from these two groups could be caused by the pre-existing differences.\nExample\n\nRandomly choose some cities within which to show billboards and other cities to leave without.\nWe can look for changes in the test regions at-specific-times as compared to the control regions at-specific-times.\nSince random changes happen all the time, we need to look historically to figure out what kinds of changes are normal so we can identify the impact of our test.\nBecause groups of individuals are assigned based on location rather than assigning each individual at random, and without the individual randomization there is a much larger chance for imbalance due to skewness and heterogeneous differences.\n\nTypes\n\nDifference-in-Differences, Regression Discontinuity Design, Synthetic Control Method, Interrupted Time Series\n\n\nObservational\n\nTypes\n\nMatching, Propensity Score Matching, Propensity Score Stratification, Inverse Probability of Treatment Weighting, and Covariate Adjustment",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Designs</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-designs.html#sec-exp-des-factdes",
    "href": "qmd/experiments-designs.html#sec-exp-des-factdes",
    "title": "19  Designs",
    "section": "19.3 Factorial Designs (aka Multifactorial Designs)",
    "text": "19.3 Factorial Designs (aka Multifactorial Designs)\n\nTwo or more independent variables that are qualitatively different\n\nEach has two or more levels\n\nNotation\n\nDescribed in terms of number of IVs and number of levels of each IV\nExample 2 X 2 X 3\n\n3 IVs\n\n2 with 2 levels and 1 with 3 levels\n\nResults in 12 conditions\n\n\nFlavors\n\nBetween-subjects: different subjects participating in each cell of the matrix\nWithin-subjects: the same subjects participating in each cell of the matrix\nMixed: a combination where one (or more) factor(s) is manipulated between subjects and another factor(s) is manipulated within subjects\n\nCombined/Expericorr\n\n\nIn this example both depressed and non-depressed categories are between-subjects & non-experimental\n\nI think experimental/non-experimental terminology is the same as manipulated/measured\n\nBelieve the no\nAn experimental design that includes one or more manipulated independent variables and one or more preexisting participant variables that are measured rather than manupulated\nSometimes participant continuous variables are dicotomized to keep a strict factorial design but this may bias the results by missing effects that are actually present or obtaining effects that are statistical artifacts. (Should just use multivariable regression instead)\n\nMedian-split procedure – participants who score below the median on the participant variable are classified as low, and participants scoring above the median are classified as high\nExtreme groups procedure – use only participants who score very high or low on the participant variable (such as lowest and highest 25%)\n\nUse cases\n\nDetermine whether effects of the independent variable generalize only to participants with particular characteristics\nExamine how personal characteristics relate to behavior under different experimental conditions\nReduce error variance by accounting for individual differences among participants",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Designs</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-designs.html#sec-exp-des-obs",
    "href": "qmd/experiments-designs.html#sec-exp-des-obs",
    "title": "19  Designs",
    "section": "19.4 Observational",
    "text": "19.4 Observational\n\nMatching and Propensity Score Matching\nPropensity Score Stratification\nInverse Probability of Treatment Weighting\nCovariate Adjustment",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Designs</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-designs.html#sec-exp-des-quasexp",
    "href": "qmd/experiments-designs.html#sec-exp-des-quasexp",
    "title": "19  Designs",
    "section": "19.5 Quasi-Experimental",
    "text": "19.5 Quasi-Experimental\n\nTypical Preconditions\n\nThe treated group looks like the control group (similarity for comparability);\nA sufficiently large number of observations within each group (a large n)\n\nRandomizing at the lowest level possible Notes from: Key Challenges with Quasi Experiments at Netflix\n\nDescription: RCTs require you to randomize similar units (e.g. individual people) into treatment and control groups. If this isn’t possible at the individual level, then randomizing at the lowest level possible is the closest, next best thing.\nExample\n\nNetflix: Measure the impact of TV or billboard advertising on member engagement. It is impossible to have identical treatment and control groups at the member level as we cannot hold back individuals from such forms of advertising. Randomize our member base at the smallest possible level. For instance, TV advertising can be bought at TV media market level only in most countries. This usually involves groups of cities in closer geographic proximity.\n\nProblems\n\nsmall sample sizes\n\ne.g. If randomizing by geographical units, there are probably not too many of these\n\nhigh variation and uneven distributions in treatment and control groups due to heterogeneity across units\n\ne.g. London with its high population is randomly assigned to the treatment cell, and people in London love sci-fi much more than other cities. London’s love for sci-fi would result in an overestimated effect.\n\n\nSolutions\n\nrepeated randomizations (aka re-randomization)\n\nkeep randomizing until we find a randomization that gives us the maximum desired level of balance on key variables across treatment cells\nSome problems still remain\n\nCan only simultaneously balance on a limited number of observed variables, and it is very difficult to find identical geographic units on all dimensions\nCan still face noisy results with large confidence intervals due to small sample size\n\n\nImplement designs involving multiple interventions in each treatment cell over an extended period of time whenever possible (i.e. instead of a typical experiment with single intervention period).\n\nThis can help us gather enough evidence to run a well-powered experiment even with a very small sample size. Large amounts of data per treatment cell increases the power of the experiment.\n\nUse a Bayesian Dynamic Linear Model (DLM) to estimate the treatment effect\n\nuses a multivariate structure to analyze more than a single point-in-time intervention in a single region.\ndlm PKG (see bkmks)",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Designs</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-designs.html#sec-exp-des-rcbd",
    "href": "qmd/experiments-designs.html#sec-exp-des-rcbd",
    "title": "19  Designs",
    "section": "19.6 Randomized Complete Block Design (RCBD)",
    "text": "19.6 Randomized Complete Block Design (RCBD)\n\nNotes from https://www.r-bloggers.com/2020/12/accounting-for-the-experimental-design-in-linear-nonlinear-regression-analyses/\nAlso see\n\nExperiments, Analysis &gt;&gt; Examples for an analysis example of RCBD\n\nThe defining feature is that each block sees each treatment exactly once\nRunning a linear regression analysis without taking into account the correlation within blocks\n\nAny block-to-block variability goes into the residual error term, which is, therefore, inflated.\nTaking the mea\n\nAdvantages\n\nGenerally more precise than the completely randomized design (CRD).\nNo restriction on the number of treatments or replicates.\nSome treatments may be replicated more times than others.\nMissing plots are easily estimated.\n\nDisadvantages\n\nError degrees of freedom is smaller than that for the CRD (problem with a small number of treatments).\nLarge variation between experimental units within a block may result in a large error term\nIf there are missing data, a RCBD experiment may be less efficient than a CRD\n\nSteps\n\nChoose the number of blocks (minimum 2) – e.g. 4\n\nThe number of blocks is the number of “replications”\n\nChoose treatments (assign numbers or letters for each) – e.g. 6 trt – A,B, C, D, E, F\n\nTreatments are assigned at random within blocks of adjacent subjects, each treatment once per block.\nAny treatment can be adjacent to any other treatment, but not to the same treatment within the block\n\nRandomize the treatments and blocks\n\nExample\nObs block trt\n1   2     B\n2   2     C\n3   2     A\n4   2     D\n5   2     E\n6   2     F\n7   1     B\n8   1     C\n9   1     E\n10  1     A\n11  1     F\n12  1     D\n13  3     D\n14  3     A\n15  3     C\n16  3     F\n17  3     B\n18  3     E\n19  4     A\n20  4     F\n21  4     B\n22  4     C\n23  4     D\n24  4     E",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Designs</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-designs.html#conjoint-analysis",
    "href": "qmd/experiments-designs.html#conjoint-analysis",
    "title": "19  Designs",
    "section": "19.7 Conjoint Analysis",
    "text": "19.7 Conjoint Analysis\n\nConjoint experiments are a special kind of randomized experiment where study participants are asked questions that have experimental manipulations. (wiki)\n\nThe objective of conjoint analysis is to determine what combination of a limited number of attributes is most influential on respondent choice or decision making.\nUnlike a standard randomized experiment where one feature of interest is manipulated (like in an A/B test), conjoint experiments are choose-your-own-adventure randomized experiments.\nParticipants are presented with 2+ possible options that have a variety of features with different levels in those features, and then they’re asked to choose one (for a binary outcome) or rate them on some sort of scale (for a continuous outcome).\n\nMisc\n\nNotes from\n\nThe ultimate practical guide to conjoint analysis with R\n\n\nExample: What effect do different political candidate characteristics have on the probability that a respondent would select that candidate (or on candidate favorability)?\n\nSurvey Question\n\n\n\n\nCandidate 1\nCandidate 2\n\n\n\n\nMilitary service\nDid not serve\nServed\n\n\nReligion\nNone\nMormon\n\n\nCollege\nState university\nIvy League university\n\n\nProfession\nLawyer\nBusiness owner\n\n\nGender\nFemale\nFemale\n\n\nIncome\n$54,000\n$92,000\n\n\nRace/Ethnicity\nWhite\nAsian American\n\n\nAge\n45\n68\n\n\n\nIf you had to choose between them, which of these two candidates would you vote for?\n\nCandidate 1\nCandidate 2\n\nChoice Attributes\n\n\n\n\n\n\n\nFeatures/Attributes\nLevels\n\n\n\n\nMilitary service\nServed, Did not serve\n\n\nReligion\nNone, Jewish, Catholic, Mainline protestant, Evangelical protestant, Mormon\n\n\nCollege\nNo BA, Baptist college, Community college, State university, Small college, Ivy League university\n\n\nProfession\nBusiness owner, Lawyer, Doctor, High school teacher, Farmer, Car dealer\n\n\nGender\nMale, Female\n\n\nIncome\n$32,000; $54,000; $65,000; $92,000; $210,000; $5,100,000\n\n\nRace/Ethnicity\nWhite, Native American, Black, Hispanic, Caucasian, Asian American\n\n\nAge\n36, 45, 52, 60, 68, 75\n\n\n\n\nEach of the eight attributes had different levels within them that respondents could possibly see.\n2 × 6 × 6 × 6 × 2 × 6 × 6 × 6, or 186,624 different attribute combinations that very likely won’t all be used in a one setting.",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Designs</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-planning.html",
    "href": "qmd/experiments-planning.html",
    "title": "20  Planning",
    "section": "",
    "text": "20.1 Misc",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-planning.html#sec-exp-plan-misc",
    "href": "qmd/experiments-planning.html#sec-exp-plan-misc",
    "title": "20  Planning",
    "section": "",
    "text": "If you’re going to analyzing the results of a test, ask to be involved in the planning stages. This well help insure that the test has usable results.\nSources of Bias\n\nAlso see\n\nExperiments, A/B Testing &gt;&gt; Potential Biases\nExperiements, RCT &gt;&gt; Sources of Bias\n\nSampling Bias - The probability distribution in the collected dataset deviates from its true natural distribution one would actually observe in the wilderness.\nSpectrum Bias - Whenever a distribution which a model has been trained with changes, e.g. due to spatial or temporal effects, the validity of this model expires. (model drift?)\n\nCheck randomization procedure by testing for pairwise associations between the treatment variable and the adjustment variables. If independence is rejected (pval &lt; 0.05), then randomization failed. (also see Experiments, A/B Testing &gt;&gt; Terms &gt;&gt; A/A Testing)\n\ntreatment vs continuous - 2 sample t-tests\ntreatment vs categorical - chisq test\n\n\nError\n\n\nThe false positive rate is closely associated with the “statistical significance” of the observed difference in metric values between the treatment and control groups, which we measure using the p-value.\n\nFPR typically set to 5% (i.e. falsely conclude that there is a “statistically significant” difference 5% of the time)\n\nFalse negatives are closely related to the statistical concept of power, which gives the probability of a true positive given the experimental design and a true effect of a specific size\n\nPower = 1 - FNR",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-planning.html#sec-exp-plan-cons",
    "href": "qmd/experiments-planning.html#sec-exp-plan-cons",
    "title": "20  Planning",
    "section": "20.2 Considerations",
    "text": "20.2 Considerations\n\nMetrics\n\nIf using multiple metrics/KPIs, make sure that you and the product manager agree on which metric/KPI should be primary and which should be secondary.\n\nWhere do users get randomized? Can depend on the KPI you’re measuring.\n\nApp or website login - appropriate for product purchasing\nA click on the first screen of the signup flow - appropriate for app subscriptions\n\nWill you only be testing a subset of your customers?\n\nExample: testing changes in one country or platform and apply the learnings from the test before releasing them to our remaining users\nMay affect the baseline KPI used to calculate the sample size\n\nExample: if a new feature is only going to be tested for English users on iOS the conversion rate may be different than the rate for all users on iOS. This also affects the number of users expected to enter the test because more users logged into iOS versus just English users.\n\n\nCalculate sample size\n\nMay take months to reach the sample size needed to determine statistical significance of a measured effect\n(approx) Sample Size\n\nSee Sample Size/Power/MDE\n\nIssues\n\ngetting more samples or running an experiment for a longer time to increase the sample size might not always be easy or feasible\n\nIf your sample size is large and therefore test duration is too long, you may need to change the metric/KPI you’re measuring\n\nExample\n\nKPI: test whether new feature increased the percentage of new users that returned to the app 30 days after signup.\nThis meant the test needed to run an additional 30 days to ensure new users in the control didn’t get exposed to the new feature within the 30-day engagement window we wanted to measure.\n\n\n\nDoes the time of year matter?\n\nIs there a seasonality aspect to your KPI, customer engagement, etc.?\n\nIf so, the treatment effect may differ depending on when the test is conducted\n\n\nMonitoring\n\nConfirm group/cohort proportions\n\nExample: If you have 3 treatments (aka variants) and 1 control, make sure each group has 25% of the test participants\nUnbalanced groups can result in violations of assumptions for the statistical tests used on the results\n\nTrack KPIs\n\nVery bad treatments could substantially affect KPIs negatively. So you need to pull the plug if your business starts to tank.",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-planning.html#sec-exp-plan-smppow",
    "href": "qmd/experiments-planning.html#sec-exp-plan-smppow",
    "title": "20  Planning",
    "section": "20.3 Sample Size/Power/MDE",
    "text": "20.3 Sample Size/Power/MDE\n\n20.3.1 MIsc\n\nUnderpowered Experiments\n\n“In particular, if your data are noisy relative to the size of the effects you can reasonably expect to find, then it’s a big mistake to use any sort of certainty thresholding (whether that be p-values, confidence intervals, posterior intervals, Bayes factors, or whatever) in your summary and reporting. That would be a disaster—type M and S errors will kill you.\nSo, if you expect ahead of time that the study will be summarized by statistical significance or some similar thresholding, then I think it’s a bad idea to do the underpowered study. But if you expect ahead of time that the raw data will be reported and that any summaries will be presented without selection, then the underpowered study is fine.” Gelman\n\n\n\n\n20.3.2 Approximate Sample Size\n\n80% Power\n\nn = 8 / (effect size^2)\n\nYou can substitute correlation (?) for effect size\n\nDifference between means of two groups\n\nn = 32 / (effect size^2)\n\nUsing variance\n\nn = (16* σ2) / δ2\n\nσ is variance of the data (outcome?)\nδ is the effect size\n\n\n\n90% Power\n\nn = 11 / (effect size^2)\n\nBayesian\n\nFrom https://www.rdatagen.net/post/2021-06-01-bayesian-power-analysis/\nBayesian inference is agnostic to any pre-specified sample size and is not really affected by how frequently you look at the data along the way\nA bayesian power analysis to calculate a desired sample size entails using the posterior distribution probability threshold (or another criteria such as the variance of the posterior distribution or the length of the 95% credible interval)\n\nMinimum Detectable Effect (MDE) is proportional to 1/sqrt(sample_size)\nExample: Gelman (Confirming sample size of 126 has 80% power)\n\nAssumption: drug (binary treatment) increased survival rate by 25 percentage points (i.e. treatment effect)\n\nEvidently for a survival model, but Gelman uses standard z-test gaussian power calculation. So, I guess the survival model part doesn’t matter.\n\n“With 126 people divided evenly in two groups, the standard error of the difference in proportions is bounded above by √(0.5*0.5/63 + 0.5*0.5/63) = 0.089, so an effect of 0.25 is at least 2.8 standard errors from zero, which is the condition for 80% power for the z-test.”\n\nSE for the difference in 2 proportions\n\n\nIn the example, the experiment is balanced so both the treatment and control groups have an equal number of participants (i.e. 63 in each group which is a 0.5 proportion of the total sample size)\n\n0.25 / 0.089 = 2.8 s.d. from 0\n\nGelman’s Explanation: “If you have 80% power, then the underlying effect size for the main effect is 2.8 standard errors from zero. That is, the z-score has a mean of 2.8 and standard deviation of 1, and there’s an 80% chance that the z-score exceeds 1.96 (in R, pnorm(2.8, 1.96, 1, lower.tail = F) = 0.8).”\n\nExplanation of the Explanation: “A two-tail hypothesis with a significance level of 0.05 are assumed. The right-tail critical value is 1.96. The power is the mass of the sampling distribution under the alternative to the right of this decision boundary. Then we want to find a Gaussian with a standard deviation of 1 so that 80% of its mass is to the right of 1.96. Then a mean of 2.8 gives the desired outcome.”\nAlso see Notebook pg 95\n\n\n\n\n\n20.3.3 Increasing Power\n\nIncrease the expected magnitude of the effect size by:\n\nBeing bold vs incremental with the hypotheses you test.\nTesting in new areas of the product\n\nLikely more room for larger improvements in member satisfaction\n\n\nIncrease sample size\n\nAllocate more members (or other units) to the test\nReduce the number of test groups\n\nthere is a tradeoff between the sample size in each test and the number of non-overlapping tests that can be run at the same time.\n\n\nTest in groups where the effect is homogenous\n\nincreases power by effectively lowering the variability of the effect in the test population\nNetflix paper\nExample: Testing a feature that improves latency\n\ne.g. the delay between a member pressing play and video playback commencing\nLatency effects are likely to substantially differ across devices and types of internet connections\nSolution: run the test on a set of members that used similar devices with similar web connections\n\n\n\n\n\n20.3.4 {PUMP}\n\nFrequentist Multilevel Model Power/Sample Size/MDE Calculation\nMisc\n\ngithub, paper\n\nalso has vignettes and shiny app\n\nNotes from\n\nVideo useR conference 2022\n\nAssumes multi-test correction procedure (MTP) will occur\nBayesian calculation for this specification would be different\n\nFactors affecting power\n\nWith at least 1 outcome:\n\ndesign of the study; assumed model (type of regression)\nnbar, J, K: number of levels (e.g. students, schools)\n\nUnless block size differences are extreme, these should not affect power that much\n\nT: proportion of units treated\nnumber of covariates\n\nand R2, the proportion of variance that they explain\n\nICC: ratio of variance at a particular level (e.g. student, school) to overall variance\n\nUnique to multiple outcomes\n\nDefinitions of power\n\nChoose depends on how we define success\nTypes\n\nIndividual: probability of rejecting a particular H0\n\nthe one you learn in stats classes\n\n1-Minimal: probability of rejecting at least 1 H0\nD-Minimal: probability of rejecting at least D H0s\nComplete (Strictest): probability of rejecting all H0s\n\nNote: in the video, the presenter wasn’t aware of any guidelines (e.g. 80% for Individual) for the different types of power definitions\n\nM: number of outcomes, tests\nrho: correlation between test statistics\nproportion of outcomes for which there truly are effects\nMultiple Testing Procedure (MTP)\n\n\nUses a simulation approach\n\nCalculate test statistics under alternative hypothesis\nUse these test stats to calculate p-values\nCalculate power using the distribution of p-values\n\nPUMP::pump_power\n\noptions\n\nExperiment\n\nLevels: 1, 2, or 3\nRandomization level: 1st , 2nd, or 3rd\n\nModel\n\nIntercepts: fixed or random\nTreatment Effects: constant, fixed, or random\n\nMTP\n\nBonferroni: simple, conservative\nHolm: less conservative for larger p-values than Bonferroni\nBenjamini-Hochberg: controls for the false discovery rate (less conservative)\nWestfall-Young\n\npermutation-based approach\ntakes into account correlation structure of outcomes\ncomputationally intensive\nNot overly conservative\n\nRomano-Wolf\n\nSee Statistical Concepts &gt;&gt; Null Hypothesis Significance Testing (NHST) &gt;&gt; Romano and Wolf’s correction\nSimilar to Westfall-Young but less restrictive\n\n\n\nExample\n\nDescription\n\nOutcome: 3 level categorical\n2-level Block Design\n\n“2-level”:  students within schools\n“Block Design”: treatment/control randomization of students occurs within each school\n\n\nPower calculation\n\n\nd_m is the code for the experimental design (assume these are listed in the documentation)\nMDES is a vector of the treatment effects for each of the 3 levels of the outcome\nSee “Factors affecting power” (above) for descriptions of some of these args.\n\nResults\n\n\nSee above for descriptions of the types of power (Factors affecting power &gt;&gt; Unique to multiple outcomes &gt;&gt; Definitions of Power)\nNone: w/o multi-test correction: 81% power\nBF: w/ Bonferroni (multiply p-values by number of outcomes): 67%\nD 1,2,3 are individual power for each of the 3 levels of the outcome\nmin 1, 2 = at least 1, 2 levels  of the outcome\ncomplete is for all 3 levels of the outcome (will always be lowest)\n\n\npump_mdes() calculates minimal detectable effect size (MDES)\npump_sample() calculates the sample size given target power (e.g. 0.80) and MDES\n\nSample Size Types\n\nK: number of level 3 units (e.g. school districts)\nJ: number of level 2 units (e.g. schools)\nnbar: number of level 1 units (e.g. students)\n\nExample\n\n\nResults\n\n\n\nObserve the sensitivity of power for different design parameter values\n\nExample\npgrid &lt;- update_grid(\n    pow,\n    # vary parameter values\n    rho = seq(0, 0.9, by = 0.1)\n    # compare multiple MTPs\n    MTP = c(\"BF\", \"HO\", \"WY-SS\", \"BH\")\n)\nplot(pgrid, var.vary = \"rho\")\n\n\nOutputs facetted multi-line plots with\n\ny = rho, y = power\nmultiple lines by MTP\nfacetted by power definition",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-planning.html#sec-exp-plan-olctn",
    "href": "qmd/experiments-planning.html#sec-exp-plan-olctn",
    "title": "20  Planning",
    "section": "20.4 Collection",
    "text": "20.4 Collection\n\nRecord data; don’t calculate or transform it\n\nIf possible, store data as text or in text compatible format. (i.e. .csv, .tsv, or some other delimited file)\n\nSome other formats add trailing spaces, etc.\n\n\nBack up data\n\nMultiple places is recommended\n\nCurate Data Organization\n\nClean data with simple organization fosters its use and a shared understanding of procedures and analysis.\nObservations, cases, or units, etc. appear in rows\nvariables appear in columns\nvalues for observations on variables appear in the matrix of cells between them\nNesting structure (i.e. grouping variables) should appear in columns, not rows.\nBeware complicated row, column, or value labels.\n\nRow, column, or value labels with case sensitive characters, special characters, or whitespace cause problems in analytical software beyond the spreadsheet (they can be a problem within the spreadsheet as well)\nUse lower cases that fully denote the observation, variable, or label, unless data is used as-is.\nAvoid spaces.\nUse underscores rather than periods to indicate white space.\nAvoid special characters — “percent” or “pct” is better than “%.”\n\n\nAll calculations should occur outside the data repository\n\n** keep an original, un-adulterated copy of the data in a separate sheet or file **\nCarrying calculations, summaries, and analysis within the data structure gets in the way of efficient updating.\nUpdating an analysis means merely updating the data set (again in the native form) called by the procedure if scripts and functions are well-documented.\nAutomating reporting and analysis is a big deal in both the public and private sectors.\n\nDo not summarize data during collection (unless the need is pressing)",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-planning.html#sec-exp-plan-peec",
    "href": "qmd/experiments-planning.html#sec-exp-plan-peec",
    "title": "20  Planning",
    "section": "20.5 Post-Experiment Evaluation Checklist",
    "text": "20.5 Post-Experiment Evaluation Checklist\n\nDid the test run long enough so that the sample size reached?\nAre treatment variants proportioned correctly?\nDid users get exposed to multiple treatment variants and how many?",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/experiments-planning.html#sec-exp-plan-dsvte",
    "href": "qmd/experiments-planning.html#sec-exp-plan-dsvte",
    "title": "20  Planning",
    "section": "20.6 Decreasing the Sampling Variance of the Treatment Effect",
    "text": "20.6 Decreasing the Sampling Variance of the Treatment Effect\n\n20.6.1 Misc\n\nNotes from Online Experiments Tricks — Variance Reduction\nAlternative to increasing power\nWinsorize ({DescTools::Winsorize}), dichotomizing, etc. metrics will help decrease the variance significantly, but introduce more bias\nCUPED is widely used and productionalized in tech companies and ML-based methods are often used to incorporate multiple covariates. (see below)\n\n\n\n20.6.2 Stratified Sampling\n\n\nSee Surveys, Sampling Methods &gt;&gt; Probabilistic Sampling Methods &gt;&gt; Stratified Sampling\nPro - Provides an unbiased estimate of the treatment effect and effectively removes the between-strata variance\nCon - Very hard to implement stratified sampling before experiments\n\n\n\n20.6.3 Post-Stratification\n\n\nPost-stratification randomly samples the population first and then places individuals into strata.\nThe Effect is measured as a difference in means between treated and untreated\nSteps\n\nRandomly sample population then allocate individuals into strata\nRandomly assign treatment to all individuals all together\n\nShe didn’t do the assignment per strata which I’m not sure is correct.  You could get a long run of 1s for one strata and a long run of zeros for another strata.\n\nRun experiment\nFor each strata\n\nCalculate mean outcome for treated and mean outcome for untreated\nCalculate the difference in mean outcomes\n\nTake the mean of the differences for the average treatment effect (ATE)\n\nDenominator is the number of strata\n\n\nIn the example, the procedure was simulated multiple times to get an ATE distribution\n\nI guess you could bootstrap or use {emmeans} to CIs, pvals, etc.\n\n\n\n\n20.6.4 CUPED\n\n\nControlled-Experiment Using Pre-Experiment Data\nY is the outcome variable\nX is pre-experiment values of the outcome variable\n\nSo, you’d need as many pre-experiment values as observed values during the experiment.\n\n…and potentially the same individuals? Probably not necessary but desirable.\n\nWhen no pre-experiment values of the outcome variable exist, a variable highly correlated to the outcome variable that’s NOT RELATED TO THE EXPERIMENT can be used.\n\nLike an instrument from an IV model.\nCan use ML to construct the control variate. (see CUPAC below)\n\nThis blog post goes through the algebra extending CUPED from one covariate, X,  to multiple covariates.\n\nAlso see Understanding CUPED\nSteps\n\nRandomly assign treatment to individuals\nPerform experiment\nCalculate θ (eq.3)\nCalculate Ycuped (eq.1)\nCalculate the effect size by taking the difference between the treated Ycuped mean and the untreated Ycuped mean\n\n\n\n\n20.6.5 Variance-Weighted Estimators\n\nVariance is reduced by calculating a weighted variance based on the variance of an individual’s pre-experiment data\n\n\nY is the outcome variable\nZ is the treatment indicator\nδ is the treatment effect\nσi2 is the pre-experiment variance of individual i’s data\n\nalternative ways of estimating the variance include ML models and using Empirical Bayes estimators (Paper)\n\n\nSteps\n\nCalculate individual variances, σi2\nBucket individuals into k strata based on their variances\nCalculate the mean of each strata’s variance, stratak_mean_variance\nRandomly assign treatment to individuals\nPerform experiment\nFor each strata\n\nCalculate the effect for each strata by taking the difference between the treated mean Y and untreated mean Y\nCalculate strata weight, wk = 1 / stratak_mean_variance\nCalculate weighted effect for strata k, δw,k = δk x wk\n\nCalculate variance weighted treatment effect by adding all the weighted effects and dividing it by the sum of the weights\n\nδw = sum(δw,k) / sum(wk)\n\n\nPros and Cons\n\nThe variance-weighted estimator models individual pre-experiment variance as weight and it can be used as a nice extension to other methods such as CUPED.\n\nI guess you just calculate k Ycuped and then do the weighting procedure. θ and X shouldn’t be affected — just some grouped calculations.\n\nIt works well when there is a highly skewed variance between users and when the pre-treatment variance is a good indicator of the post-treatment variance.\n\nNot sure what exactly is meant by “highly skewed variance between users.” Most users have high or most users have low variance for the pre-experiment data?\n\nWhen the variance of the pre-treatment variance is low or when the pre- and post-experiment variances are not consistent, the variance-weighted estimator might not work.\nThe variance-weighted estimator is not unbiased. Managing bias is important for this method.\n\n\n\n\n20.6.6 CUPAC\n\nControl Using Predictions As Covariates\nML extension of CUPED (Paper)\nAssuming we have pre-experiment metrics, X1, X2, X3, and X4. Essentially, what this method does is to use some machine learning model to predict Y using X1, X2, X3, and X4. And then, we can use the predicted value as the control covariate in CUPED.\n\n\n\n20.6.7 MLRATE\n\nMachine Learning Regression-Adjusted Treatment Effect Estimator\nAlso see\n\nPaper\nUpgrade Variance Reduction Beyond CUPED: Introducing MLRATE\nDeep dive into MLRATE - machine learning regression-adjusted treatment effect estimator and comparing it to other methods\n\nDoes the same thing as CUPAC to get the control covariate, but instead using the CUPED equation with θ to get Ycuped, it estimates Ycuped using OLS regression.\n\nSee Introducing MLRATE article for more details",
    "crumbs": [
      "Experiments",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/extreme-value-theory-(evt).html",
    "href": "qmd/extreme-value-theory-(evt).html",
    "title": "23  Extreme Value Theory",
    "section": "",
    "text": "23.1 Misc",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Extreme Value Theory</span>"
    ]
  },
  {
    "objectID": "qmd/extreme-value-theory-(evt).html#sec-evt-misc",
    "href": "qmd/extreme-value-theory-(evt).html#sec-evt-misc",
    "title": "23  Extreme Value Theory",
    "section": "",
    "text": "Packages\n\nCRAN Task View\n{erf} - able to extrapolate extimates beyond the training data since erf is based on EVT and is also flexible since it uses a RF\n\nvideo: from the 33min mark to 55:19\nQ(τ) is the desired quantile you want to estimate\n\nQ(τ0) is an intermediate quantile (e.g. 0.80) that can be estimated using a quantile RF (package uses {grf})\n\nDepends on thickness of tail (i.e whether the shape parameter is negative, 0, or positive)\n0.80 tends to work reasonable well\nThe higher the threshold you use, the less variance but higher bias\n\n\nξ(x) and σ(x) says the shape and scale parameters depend on the predictors. They’re estimated by minimizing a probability distribution’s log-likelihood which are multiplied by weights extracted from quantile RF.\ntune minimum node size, penalty term on the variability of the shape parameter\ncv using deviance metric for model selection\n\n{gbex} - no docs, only paper, gradient boosting for extreme quantile regression; able to extrapolate since they’re based on EVT\n{evgam} - Extreme Value GAM; able to extrapolate since they’re based on EVT\n\n{erf} and {gbex} peform better than regular quantile rf model types for quantiles &gt; 0.80 (video: from the 33min mark to 55:19, results towards the end)\n\nNon-ML methods like {evgam} perform poorly for data with highh dim\n\nWhy using Random Forest models that do NOT incorporate EVT usually don’t produce good results.\n\nTypical RF weighs every data point equally while a grf (see Regression, Quantile), depending on the quantile estimate, will weigh data points closer to the quantile more heavily\nQuantile Regression Forests work fine on moderate quantiles (e.g. 0.80) but even those like grfs struggle with more extreme quantiles because no matter how large the quantile you choose, the predicted quantile will be no larger than the most extreme data point. They use empirical methods and have no way to extrapolate.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Extreme Value Theory</span>"
    ]
  },
  {
    "objectID": "qmd/extreme-value-theory-(evt).html#sec-evt-distrtail",
    "href": "qmd/extreme-value-theory-(evt).html#sec-evt-distrtail",
    "title": "23  Extreme Value Theory",
    "section": "23.2 Distribution Tail Classification",
    "text": "23.2 Distribution Tail Classification\n\nMisc\n\nNotes from quantitative risk management lectures QRM 4-3, 4-4, https://www.youtube.com/watch?v=O0fdBwBRGU4\n\nDifference between tail events and outliers:\n\nOutliers tend to be extreme values that occur very infrequently. Typically they are less than 1% of the data.\nTail events are less extreme values compared to outliers but occur with greater frequency.\n\nTail events can be difficult to predict because\n\nAlthough not as rare as outliers, it’s still difficult to get enough to data to model these events with any sufficient precision.\nDifficult to obtain leading indicators which are correlated with the likelihood of a tail event occurring\n\nPrediction tips\n\nConsider binning numerics to help the model learn sparse patterns.\nUse realtime features\n\nExample: Predicting delivery time tail events\n\nunexpected rainstorm (weather data)\nroad construction (traffic data)\n\n\nUtilize a quadratic or L2 loss function.\n\nMean Squared Error (MSE) is perhaps the most commonly used example. Because the loss function is calculated based on the squared errors, it is more sensitive to the larger deviations associated with tail events\n\n\nHeavy tails\n\nYour random variable distribution is heavy tailed if:\n\n\n\nwhere the exponential survival function,  \nSays if you take the ratio of your most extreme positive values (i.e. your survival function) at the tail (i.e. supremum)(numerator) and those of the positive tail of exponential survival function (denominator), then that ratio will go to positive infinity as x goes to infinity\nOr in other words, the probability mass of the pdf of your random variable in the tail is greater than the probability mass that of the exponential pdf\nAlso means that the moment generating function is equal to infinity which means that it can’t be used to calculate distribution parameters\n\n\nSubsets of heavy tails\n\nAlong with survival function ratio (see above), these tails have additional conditions \nlong tails\n\ncommon in finance\nYour random variable distribution is long tailed if:\n\nit follows the explosion principle\n\nIf an extreme event manifests itself, then the probability of an even more extreme event approaches 1\n\nno time prediction on the next more extreme event but extreme value theory + timeseries + conditions say extreme events tend to cluster\n\n\nSays, for example, if you take a huge loss in your portfolio, it’s a mistake to think that that value is an upper bound on losses or that the probability of an even larger loss is negligible\n\nNot practical to determine from data\n\nsubexponential tails\n\nsubset of long tail\nYour random variable distribution is subexponential tailed if:\n\nit follows the one-shot aka catastrophe principle aka “winner takes all”\n\n\n\nwhere Sn is a partial sum of values of your random variable; Mn is a partial maximum; x is a large value\nsays at some point the partial sum, Sn , will be dominated by one large value, Mn\nExample: if your portfolio follows this principle, then your total loss can be mostly attributed to one large loss\n\n\ntools available to practically test\n\nExamples:\n\nlog-normal\n\ncan get normal parameters from lognormal parameters by formula that involves exponentiation (see notebook) or vice versa with logs\nall statistical moments always exist\n\n\nfat tails\n\n\n\nL(x) is just characterized as slowly varying function that gets dominated by the decaying inverse power law element, x-α. as x goes to infinity\nα is a shape parameter, aka “tail index” aka “Pareto index”\n\nExamples\n\npareto\n\npareto has similar relationship with the exponential distribution as lognormal does with normal\n\n\nxm is the (positive) minimum of the randomly distributed pareto variable, X that has iindex α\nYexp is exponentially distributed with rate α\n\nsome theoretical statistical moments may not exist\n\nIf the theoretical moments do not exist, then calculating the sample moments is useless\nExample: Pareto (α = 1.5) has a finite mean and an infinite variance\n\nNeed α &gt; 2 for a finite variance\nNeed α &gt; 1 for a finite mean\nIn general you need α &gt; p for the pth moment to exist\nIf the nth moment is not finite, then the (n+1)th moment is not finite.\n\n\n\n\n\n\n\n\n\nLight tails\n\nOpposite of heavy\nInstead of larger than pdf or survival function of the exponential version, it’s equal to or smaller than.\n\ni.e. your function decays as fast or faster as x goes to infinity as an exponential\n\nExamples\n\nexponential, normal\n\n\nBoth\n\nclass depends on parameter values\nExamples\n\nWeibull\n\n\n\n\n\nTests\n\nNotes\n\nAll the plots below should be used and considered when diagnosing tails\nCan use the zipf and me plots to find the thresholds in the data where it would be useful to start modeling the data as pareto or lognormal\n\nAsk these questions\n\nDoes the subject matter you’re modeling lead you to expect a certain type of tail?\n\nExample: Does the explosion principle hold or not?\n\nIs there an upper bound to your data (theoretical or actual)?\n\nExample: Is the upper bound due to the quality of the data\n\nDo I have over 10,000 observations?\n\nIn the various plots below, it can be difficult to distinguish between Pareto (fat tail) and Lognormal (long tail) distributions. As a rule-of-thumb, usually takes 10K observations to really be able to tell the two apart in order to get enough data points in the tail.\nUsually get at least 10K observations in a market risk portfolio, but not in credit risk or operational risk portfolios\n\n\nQ-Q plot\n\nexponential quartiles on the y-axis and ordered data on the x-axis\n\nSee EDA &gt;&gt; Numeric &gt;&gt; Q-Q plot for code\n\nif data hugs the diagonal line –&gt; exponential –&gt; light tails\nif data is concave –&gt; potentially heavy tails\nif data is convex –&gt; potentially tails that are lighter than an exponential\n\nZipf plot\n\nlog-log plot of the empirical survival function of the data\n\nlog of the pareto survival function makes it linear where the slope of the line is -α\n\nindicates if there’s a power law decay in the tails of the data (i.e. fat tails)\n\nresults of this plot is “necessary” but not “sufficient” for confirmation of fat tails (pareto)\nIt is sufficient to say it’s not a pareto if there’s curvature\n\n\n\n\nThe real data shows linearity at the very end, so even though it’s not linear from the beginning, it is still potentially fat tailed\n\nReal data often show mixed, complex behaviors.\n\nAlso not that even in the simulated dataset, the data points at the end have some randomness to them and don’t fall directly on the line.\n\nthe randomness is called small sample bias; usually not much data in the tails\n\n\n\n\nlog-normal can look like a pareto if its sigma parameter is large (small data). It will look linear and curve down at the very end.\nExample above shows lognormal with sd = 1, so sd doesn’t have to be very large to be tricky to discern from a Pareto.\nIf the data has a smallish range (x-axis), then that is a signal to wary about deeming the distribution to having fat tails\n\nThis one goes from 0 to 100 while the one above it goes from 0 to a million\n“Large” or “small” depends on the type of data your looking at though. In another subject matter, maybe 100 is considered large, so context matters\n\n\n\n\nCompare slopes between your original data (red) and aggregations of your data in a zipf plot; If you have fat tails, the line will be shifted because of aggregation but the slope, α, will remain the same\n\nExamples of aggregation methods (halves the sample size)\n\nOrder data from largest to smallest; add a1 + an, a2 + an-1, …; plot alongside original data (green)\nOrder data from largest to smallest; add a1 + a2, a3 + a4, …; plot alongside original data (blue)\n\n\n\n\nMean Excess (ME) plot\n\nCalculating the empirical mean excess variable - Order the data, calc mean2, remove the 1 data point, calc mean2, remove data points 1 and 2, calc mean3, and so on. Then plot the means\n\n\nlognormal is similar to pareto in this plot as well. The more data you have the easier it will be to distinguish the two.\nThe left equation is for the lognormal curve (with Normal parameters) and the right equation is the pareto\n\nNeed α &gt; 1, so that the mean is finite\n\n\n\n\nDisregard last few points (small sample bias)\nPoints in green circles (only a few points in tails, so difficult to be confident about)\n\nleft: shows a straight line\nright: concave down\n\nRight plot: curvature at the beginning common in the wild, since you’re not likely dealing with pure distributions but some kind of noisy mixture\n\n\nMaximum to sum plot (MS Plot)\n\n\nS is the partial sum, M is the partial maximum, p is the order of the moment that you want to see if it exists or not\n\nFor lognormal, all moments always exist\nFor pareto, you usually only need to check up to p = 4 or p = 5\n\nFor higher levels of p (and hence α) the pareto distribution begins to act like a normal\nUsually in credit, market, or operational risk markets you’re dealing with pareto 0 &lt; α &lt;= 3\n\n\nProcedure\n\nchoose a p that you want to check\nfor each n, calculate the sum, maximum, and ratio\ny-axis is the ratio, x-axis is the n value\n\n\n\nA lognormal will always converge to 0 for every p you check (black line)\nWhen a moment doesn’t exist (i.e. infinite), it just oscillates and never converges (orange line)\nMS plots always start at 1\nPotentially with fewer than 100 observations, you could start to see a convergence if one is going to happen. Of course hundreds of observations is better. Point is that it doesn’t take thousands.\n\nLeft - credit data (real estate losses), Right - operational loss data\n\n\n\nLeft\n\np = 1 definitely exists; p = 2 is iffy; p = 3,4 don’t exist\nInterpretation: either α is between 1 and 2 or there aren’t enough observations to show a convergence\n\nAlthough n is pretty large in this case\n\n\nRight\n\np = 1 is iffy, the rest don’t exist\nInterpretation: α might be less than 1\n\n\n\n\nConcentration Profile\n\nRequirements\n\ndata &gt;= 0 and mean is finite\n\nSimilar to the Mean Excess plot, except the gini index is computed instead of the mean\n\n\nIn the wild you can expect mixtures, so there will likely be noisy behavior in the beginning and when the fat tail is reached, a flat line is formed",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Extreme Value Theory</span>"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-embeddings.html",
    "href": "qmd/feature-engineering-embeddings.html",
    "title": "Embeddings",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Engineering",
      "Embeddings"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-misc",
    "href": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-misc",
    "title": "Embeddings",
    "section": "",
    "text": "Also see\n\nFeature Engineering, Tokenization\nDatabases, Vector Databases\nDiagnostics, NLP\nEDA, Text\nNLP, General\n\nEmbeddings can be aggregated (e.g. averaged) to make larger groupings\n\nExample: Averaging the embeddings of food items to create an embedding for a meal\n\nJina AI model, jina-embeddings-v2, has 8K (8192 tokens) context length, which puts it on par with OpenAI’s proprietary model, text-embedding-ada-002, in terms of both capabilities and performance\n\nWillison llm plugin for this model",
    "crumbs": [
      "Feature Engineering",
      "Embeddings"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-conc",
    "href": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-conc",
    "title": "Embeddings",
    "section": "Concepts",
    "text": "Concepts\n\nEmbeddings are numerical representations of words or sentences\nMisc\n\nNotes from a video\n\nCapture analogies well\n\nHorizontally: Puppy is to Dog as Calf is to Cow\n\nX-Axis could represent a latent variable like Age\n\nVertically: Pupy is to Calf as Dog is to Cow\n\nY-Axis could represent a latent variable like Size\n\n\nEach latent variable is a dimension in the embedding\nWord and Sentence embedding matrices can have lengths in the 1000s\n\n\nIssue with one-hot encoding is that it does not place similar entities closer to one another in vector space.\nThe embeddings form the parameters — weights — of the network which are adjusted to minimize loss on the task.\nWhich categories get placed closer to each other in the embedding depends on the outcome variable during the training\n\nFiguring out how to create the supervised task to produce relevant representations is the toughest part of making embeddings.\nExample: Categorical to be embedded is book titles\n\n“Whether or not a book was written by Leo Tolstoy”  as the outcome variable will result in embeddings would place books written by Tolstoy closer to each other.\n\n\nThe categorical variable with 100s of levels can be reduced to something like 50 vectors (node weights in the embedding layer of the network)\n\n\nSale Price is the outcome (observed values represented by “Sale Price” in orange box)\nSparse Vector Encoding (I think this is one-hot) for the categorical levels you want embedded\nOther features are included in the embedding model but they only connect to other hidden layers (pink) We can use the same group of predictors and outcome variable in the embedding DL model that we want to use in the tree (or whatever) algorithm",
    "crumbs": [
      "Feature Engineering",
      "Embeddings"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-eng",
    "href": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-eng",
    "title": "Embeddings",
    "section": "Embeddings",
    "text": "Embeddings\n\nHyperparameter: Dimension of the embedding layer\n\nHigher dimension embeddings are a more accurate representation of the relationship\n\nDownside is a greater risk of overfitting and longer training times\n\nShould be tuned\n\nStarting point: dimensions ≈ (possible values)0.25\n\n“possible values” would be the vocabulary for a text variable embedding\n\n\n\n{text}\n\nprovides access to hugginface transformers\n\n{embed}\n\nExample\nembed::step_embed(cat_var,\n    num_terms = 4, hidden_units = 16, outcome = vars(binary_outcome_var),\n    options = embed_control(loss = \"binary_crossentropy\",\n                            epochs = 10))\n\nnum_terms is the dimension of the embedding layer (i.e. number of output variables)\nhidden_units is the layer between the embedding layer and output layer\nShould probably try totune() num_terms at least.\n\nExample: Likelihood (aka Effect) Encoding\nmuseum_rec &lt;- \n  recipe(Accreditation ~ ., data = museum_train) %&gt;%\n  update_role(museum_id, new_role = \"id\") %&gt;%\n  step_lencode_glm(Subject_Matter, outcome = vars(Accreditation)) %&gt;%\n  step_dummy(all_nominal_predictors())\n\n“Subject_Matter” is the high cardinality cat var\nstep_lencode_glm fits a glm for each(?) level of the cat var uses its estimated effect as the encoded value\nmixed linear model (step_lencode_mixed) and bayesian model (step_lencode_bayes) are also available instead of a glm\nThese type of embeddings use the average estimated effect as a value for any new levels that show-up in future data\ntidy(grants_glm, number = 1) %&gt;%\n  dplyr::filter(level == \"..new\") %&gt;%\n  select(-id)\n\nView embedding values\nprep(museum_rec) %&gt;%\n  tidy(number = 1)\n\nNot sure if “number = 1” is the step in the recipe or what",
    "crumbs": [
      "Feature Engineering",
      "Embeddings"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-embeddings.html#engineering",
    "href": "qmd/feature-engineering-embeddings.html#engineering",
    "title": "Embeddings",
    "section": "Engineering",
    "text": "Engineering\n\n{embed}\n\nstep_collapse_stringdist- Collapse factor levels that have a low stringdist between them\nexample_data &lt;- tibble(\n  x = c(\"hello\", \"helloo\", \"helloo\", \"helloooo\", \n        \"boy\", \"boi\", \"dude!\")\n)\n\nrecipe(~., data = example_data) |&gt;\n  step_collapse_stringdist(all_predictors(), distance = 1) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n#&gt; # A tibble: 7 × 1\n#&gt;   x       \n#&gt;   &lt;fct&gt;   \n#&gt; 1 hello   \n#&gt; 2 hello   \n#&gt; 3 hello   \n#&gt; 4 helloooo\n#&gt; 5 boi     \n#&gt; 6 boi     \n#&gt; 7 dude!\n\ndistance needs to be an integer\nWith distance = 1, the”helloo” factor level becomes “hello” and the “boy” factor level becomes “boi”\n\nNot sure how the “osa” metric works but maybe “hello” is chosen because it has less characters than “helloo” and “boi” is chosen because “i” comes before the “y” in “boy.”\n\nDistance Metrics (method = “osa” is default)\n\n\n\n\n\n\n\nosa\nOptimal string aligment, (restricted Damerau-Levenshtein distance).\n\n\nlv\nLevenshtein distance (as in R’s native adist).\n\n\ndl\nFull Damerau-Levenshtein distance.\n\n\nhamming\nHamming distance (a and b must have same nr of characters).\n\n\nlcs\nLongest common substring distance.\n\n\nqgram\nq-gram distance.\n\n\ncosine\ncosine distance between q-gram profiles\n\n\njaccard\nJaccard distance between q-gram profiles\n\n\njw\nJaro, or Jaro-Winkler distance.\n\n\nsoundex\nDistance based on soundex encoding (see below)\n\n\n\n\nDocs gives further details on each distance measure",
    "crumbs": [
      "Feature Engineering",
      "Embeddings"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-aug",
    "href": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-aug",
    "title": "Embeddings",
    "section": "Augmentation",
    "text": "Augmentation\n\nUseful for imbalanced outcomes with text predictors\n\nBetter performance than subsampling\n\nWords are randomly swapped, deleted, as well as replaced or inserted with synonyms using pretrained word embeddings\n\nAdversarial Text Attack\n\nDiagnostic to test how a model performs on a test set where data augmentation techniques have been applied\n{{textattack}}, article, ipynb",
    "crumbs": [
      "Feature Engineering",
      "Embeddings"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-mon",
    "href": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-mon",
    "title": "Embeddings",
    "section": "Monitoring",
    "text": "Monitoring\n\nTracking embeddings drift\nMisc\n\nNotes from\n\nMeasuring Embedding Drift\n\nRecommended Euclidean\n\nHow to Measure Drift in ML Embeddings\n\n\nRecommended Model-based first and Share of Drifted Components second\nAlso has overview and link to paper for Maximum Mean Discrepancy (MMD) which is a more complex method I didn’t want to get into at the time.\nCode available; Used {{evidently}}\n\n\n\nReasons for drift\n\nChanging your model’s architecture can change the dimensionality of the embedding vectors. If the layers become larger/smaller, your vectors will too.\n\nUse another extraction method: In the event of the model not changing, you can still try several embedding extraction methods and compare between them.\n\nRetraining your model: Once you retrain the model from which you extract the embeddings, the parameters that define it will change. Hence, the values of the components of the vectors will change as well.\nVision\n\nUnique situations, events, people or objects that are observed in production data but are missing from the training set\n\nText\n\nWhen a word, category or language that does not exist in the training data emerges in production\nAny changes in terminology in the data or changes to the context or meaning of words or phrases over time can contribute to drift.\n\nLow-resource languages and cultural gaps in speech can also compound these difficulties\n\nExample: a sentiment classification model trained on apparel product reviews in English but in production, encounters reviews in Spanish for the first time\n\nOr is asked to predict the sentiment of reviews of specialized medical devices.\n\n\n\nExtraction Methods\n\nExtract embeddings from the current model in production.\n\nExtracting the last fully connected layer before the classification to create an image embedding is advisable.\n\nThe latent structure in this layer will contain information about structure in the image such as objects and actions, in addition to general quality information relative to images in the training set\n\nIn the case of a vision transformer (ViT), it is recommended that you extract the embedding that the multilayer perceptron (MLP) is acting on to make an image-level decision\nExample on how to extract embeddings from a well-known Hugging Face model (article)\n\nUsing a model to extract the embedding\n\ne.g. a foundational model like BERT\nAdvantages\n\nNo modification is needed on the production model\nEasy option for testing and running on internal data.\n\nDisadvantage: the model is only looking at the data itself and is not looking at the internal model decisions.\n\n\nChoose a reference vector or dataset\n\nFormulate an expectation on how stable or volatile your data is and choose the reference data that adequately captures what you expect to be a “typical” distribution of the input data and model responses.\nExamples\n\nValidation data\nPast period that you consider representative\n\ne.g. this week’s data to the previous week and move the reference as you go.\n\n\n\nMetrics\n\nWhichever metric you choose, it will need to be tuned. To tune it, you can model your drift detection framework using historical data or, alternatively, start with some sensible default and then adjust it on the go.\nEuclidean distance\n\nSmaller the distance, the more similar the embeddings\n\nRange: 0 to ∞\n\nMore stable, sensitive and scalable compared to hyperbox IOU, euclidean distance, cosine distance, and clustering-based group purity scores.\nAn absolute measure which makes setting a specific drift alert threshold harder: the definition of “far” will vary based on the use case and the embedding model used. You need to tune the threshold individually for different models you monitor.\nCalculate centroid for embedding vectors in the production model (and baseline model)\n\nNote: vertical dots are missing in the vectors\nThe centroid is calculated by taking the row-wise averages\n\nCompare production centroid to baseline centroid\n\nCosine Distance (1 - cosine similarity)\n\n\nSmaller the distance, the more similar the embeddings\nIf the two distributions are the same, the Cosine similarity will be 1, and the Cosine distance will be 0. The distance can take values from 0 to 2.\nThe threshold might be not very intuitive to tune, since it can take values as low as 0.001 for a change that you already want to detect.\nAnother downside is that it does not work if you apply dimensionality reduction methods like PCA, leading to unpredictable results.\n\nModel-based drift detection\n\nTrain a classification model that tries to identify to which distribution each embedding belongs and use the AUROC as the drift score.\nIf the model can confidently predict from which distribution the specific embedding comes, it is likely that the two datasets are sufficiently different.\n\ni.e. An AUROC score above 0.5 shows at least some predictive power, and an AUROC score of 1 corresponds to “absolute drift” when the model can always identify to which distribution the data belongs.\n\nWorks consistently for different datasets and embedding models we tested, both with and without PCA. It also has an intuitive threshold\n\nShare of drifted components\n\n\nThe individual components of each embedding are treated as “columns” in a structured dataset.\nCompute the drift in each component.\n\ne.g. Wasserstein (Earth-Mover) distance with the 0.1 threshold. The intuition behind this metric is that when you set the threshold to 0.1, you will notice changes in the size of the “0.1 standard deviations” of a given value.\n\nMeasure the overall share of drifting components.\n\ne.g. if your vector length is 400, you can set the threshold to 20%. If over 80 components drift, you will get a drift detection alert.\n\nAllows you to reuse familiar techniques that you might be using to detect drift on tabular data.\nMore tuning parameters than other methods: underlying drift detection method, its threshold, and the share of drifting components to react to.\n\n\nUse the 2-sample Kolmogorov–Smirnov test (KS)\n\n“Multiple samples from the embedding set can be taken calculating the euclidean distance metric for each sample set separately, and the KS test can be used to determine if drift has or hasn’t occurred.”\nSee Distributions &gt;&gt; Tests for more details",
    "crumbs": [
      "Feature Engineering",
      "Embeddings"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html",
    "href": "qmd/feature-engineering-splines.html",
    "title": "Splines",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-misc",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-misc",
    "title": "Splines",
    "section": "",
    "text": "Knots are placed at several places within the data range with (usually) low-order polynomials that are chosen to fit the data between two consecutive knots.\n\nChoices\n\nNumber of knots\nTheir positions\nDegree of polynomial to be used between the knots (a straight line is a polynomial of degree 1)\n\nThe type of polynomial and the number and placement of knots is what defines the type of spline.\n\ne.g. cubic splines are created by using a cubic polynomial in an interval between two successive knots.\n\nIncreasing the number of knots may overfit the data and increase the variance, whilst decreasing the number of knots may result in a rigid and restrictive function that has more bias.\n\nNotes from A review of spline function procedures in R (paper)\nAlso see:\n\nFeature Engineering, General &gt;&gt; Continuous &gt;&gt; Binning &gt;&gt; Harrell on the benefits of using splines vs binning\nFeature Engineering, Time Series &gt;&gt; Engineering &gt;&gt; Calendar features\nStatistical Rethinking &gt;&gt; (end of ) Ch 4\nFeature Engineering, Geospatial &gt;&gt; Cyclic Smoothing Spline\nHarrell’s RMS\nModel Building, tidymodels &gt;&gt; Recipe &gt;&gt; Transformations &gt;&gt; Splines\n\nCommon variables: trend, calendar features, age, cardinal directions (N, S, E, W, etc.)\nPackage Comparison\n\nDefault types: {mgcv} uses thin plate splines (see smoothing splines) as a default for it’s s() which makes it’s spline more flexible (i.e. curvy) than the default splines for {gam}, {VGAM}, and {gamlss} which use cubic smoothing splines.\n\n{gamlss} doesn’t use s but instead has specific functions for specific types of splines\n\nP-Splines: {mgcv} and {gamlss} are very similar, and the differences can be attributed to the different way that two packages optimize the penalty weight, λ.\n\n{mgcv}: option, “ps” within s will create a cubic p-spline basis on a default of 10 knots, with a third order difference penalty.\n\nThe penalty weight, λ, is optimized with generalized cross validation.\n\n{gamlss}: pb defines cubic p-splines functions with 20 interior knots and a second order difference penalty.\n\nThe smoothing parameter is estimated using local maximum likelihood method, but there are also other options based on likelihood methods, AIC, generalized cross validation and more.\nMultiple other functions available for p-splines with various attributes.\n\n\nDependencies: {mgcv} creates its own spline functions while {gam}, {VGAM}, and {gamlss} use the base R package, {splines}.\n\n{gam} and {VGAM} call the base R function smooth.spline (smoothing spline) with four degrees of freedom as default and give identical results",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-terms",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-terms",
    "title": "Splines",
    "section": "Terms",
    "text": "Terms\n\nSmoothly Joined -  Means that for polynomials of degree n, both the spline function and its first n-1 derivatives are continuous at the knots.",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-tune",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-tune",
    "title": "Splines",
    "section": "Tuning Parameters",
    "text": "Tuning Parameters\n\nB: Basis functions (e.g. B-Spline)\nd: The degree of the underlying polynomials in the basis\n\nTypically d = 3 (cubic) is used (&gt;3 usuallly indistinguishable)\n\nK: Number of knots for Regression Splines\n\nUsually k = 3, 4, 5. Often k = 4\n\nHarrell (uses natural splines): “For many datasets, k = 4 offers an adequate fit of the model and is a good compromise between flexibility and loss of precision caused by overfitting”\n\nIf the sample size is small, three knots should be used in order to have enough observations in between the knots to be able to fit each polynomial.\nIf the sample size is large and if there is reason to believe that the relationship being studied changes quickly, more than five knots can be used.\n\n\nThere should be at least 10–20 events per degree of freedom (Harrell, RMS)\nVariables that are thought to be more influential on the outcome or more likely to have non-linear associations are assigned more degrees of freedom (i.e. more knots)\nFlexibility of fit vs. n and variance\n\nLarge n (e.g. n ≥ 100): k = 5\nSmall n (e.g. n &lt; 30): k = 3\n\nCan use Akaike’s information criterion (AIC) to choose k\n\nThis chooses k to maximize model likelihood ratio of χ2 − 2k.\nCross-Validation is also valid\n\nAlso option for knot positions\n\nLocations not important in most situations\nPlace knots where data exist e.g. fixed quantiles of predictor’s marginal distribution (See Regression Splines &gt;&gt; B-Splines for examples)\n\nFrom Harrell’s RMS\n\n\n\n\nλ: Penalty weight for Smoothing Splines\n\nCalculated by generalized cross-validation in {mgcv} which is an approximation of LOO-CV\n\nSee article or Wood’s GAM book or Elements of Statistical Learning (~pg 244) for details",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-interp",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-interp",
    "title": "Splines",
    "section": "Interpretation",
    "text": "Interpretation\n\nA regression fit will result in estimated coefficients for each parameter used in the splines.\nOther than including them in technical appendices, in almost all cases, one does not present these estimated coefficients – their interpretation is essentially meaningless.\nVisual interpretations of the predicted response vs the splined variable are useful in discovering trends or patterns.\nPredicted responses, given representative values, outlier values, or any values of interest of the splined variable, are useful in calculating various contrasts.\nEffective Coefficient\n\nIt shows how the effect of the variable on the response varies over its range\nThink this is only possible for a natural spline\nExample: Age on Survival in Titanic dataset (link)\n\nmodel_02 &lt;- \n  glm(Survived ~ SibSp + ns(Age, df = 3) + Pclass + Parch + Fare,\n      data = titanic,\n      family = binomial)\n#\n# Create a data frame for prediction: only `Age` will vary.\n#\nN &lt;- 101\nx &lt;- titanic[which.max(complete.cases(titanic)), ]\ndf &lt;- do.call(rbind, lapply(1:N, function(i) x))\ndf$Age &lt;- with(titanic, seq(min(Age, na.rm=TRUE), max(Age, na.rm=TRUE), length.out=N))\n#\n# Predict and plot.\n#\ndf$Survived.hat &lt;- predict(model_02, newdata=df) # The predicted *link,* by default\nwith(df, plot(Age, Survived.hat, type=\"l\", lwd=2, ylab=\"\", main=\"Relative spline term\"))\nmtext(\"Spline contribution\\nto the link function\", side=2, line=2)\n#\n# Plot numerical derivatives.\n#\ndAge &lt;- diff(df$Age[1:2])\ndelta &lt;- diff(df$Survived.hat)/dAge\nage &lt;- (df$Age[-N] + df$Age[-1]) / 2\nplot(age, delta, type=\"l\", lwd=2, ylab=\"Change per year\", xlab=\"Age\",\n     main=\"Spline Slope (Effective Coefficient)\")\n\nThe varying coefficient is computed by calculating the first derivatives numerically: divide the successive differences in predicted values by the successive differences in age.\nAt Age near 35 the effective slope is nearly zero, meaning small changes of Age in this range have no effect on the predicted response. Near ages of zero, the effective slope is near −0.15, indicating each additional year of Age reduces the value of the link function by about 0.15. At the oldest ages, the effective slopes are settling down to a value near −0.09, indicating each additional year of age in this age group decreases the link function by −0.09.",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-reg",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-reg",
    "title": "Splines",
    "section": "Regression Splines",
    "text": "Regression Splines\n\nNo penalty function added\n\nSplined variable is just added to the regression model like any other predictor\n\nTypes\n\nTruncated Power Basis\n\nIssue: Basis functions are not supported locally but over the whole range of the data\n\nCould lead to high correlations between some basis splines, implying numerical instabilities in spline estimation\n\nExample: d = 3 (cubic) with 5 equidistant knots\n\nExample: d = 3 with 3 knots (τ1, τ2, τ3)\n\n\\[\nf(X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\beta_4 (X - \\tau_1)^3 + \\beta_5 (X - \\tau_2)^3 + \\beta_5 (X - \\tau_3)^3\n\\]\n\n7 dof\n\n\nB-splines\n\n\nBased on a special parameterization of a cubic spline\nSee Statistical Rethinking Notebook &gt;&gt; (end of) Chapter 4\nBasis functions supported locally which leads to high numerical stability, and also in an efficient algorithm for the construction of the basis functions.\nIssue: can be erratic at the boundaries of the data (boundary knots)\nDegrees of freedom (dof) = d + K\nbs(x) will create a cubic B-spline basis with two boundary knots and one interior knot placed at the median of the observed data values\n\nBounded by the range of the data\nlm(y ~ bs(x))\n\nExample: bs(x, degree=2, knots=c(0,.5,1))\n\ndegree specifies d\nknots specifies the number of knots and their locations\n\nExample: bs(x, knots = median(x))\n\n1 interior knot created at the median\n4 dof since d + K = 3 + 1\n\nd = 3 (default)\n\n\nExample: bs(x, knots = c(min(x), median(x), max(x)))\n\n1 interior knot specified at the median and 2 boundary knots at the min and max.\n6 dof since d + K = 3 + 3\n\nd = 3 (default)\n\n\n\nNatural Cubic and Cardinal Splines\n\n\nStable at boundaries of data because of additional constraints that they are linear in the tails of the boundary knots\nDegrees of freedom (dof) = K + 1\nns(x) returns a straight line within the boundary knots\n\nlm(y ~ ns(x))\n\nExample: ns(x,df=3)\n\n“df” specifies degrees of freedom\n“knots”: alternatively to specifying df, you can specify the knots (# and positions) like in bs\n\nCardinal splines\n\nHave an additional constraint that leads to the interpretation that each coefficient \\(\\beta_k\\) is equal to the value of the spline function at the knot \\(\\tau_k\\)",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-smth",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-smth",
    "title": "Splines",
    "section": "Smoothing Splines (aka Penalized Splines)",
    "text": "Smoothing Splines (aka Penalized Splines)\n\nAutomatically handles the number of knots and knot positions by using a large number of knots and letting λ control the amount of smoothness\n\nDifferent packages usually produce similar results. Penalties are very powerful in controlling the fit, given that enough knots are supplied into the function\n\nRequires modification of the fitting routine in order to accommodate it\n\nProbably need a GAM package to use.\n\nA special case of the more general class of thin plate splines\nFunction\n\\[\n\\hat{\\beta} = \\arg\\max_{\\beta} [l_\\beta (x_1, y_1, \\ldots, x_n, y_n) - \\lambda J_\\beta]\n\\]\n\nThe maximization of this function implies a trade-off between smoothness and model fit that is controlled by the tuning parameter λ\nTerms\n\nlβ is the likelihood\nJβ (penalty function) is the roughness penalty (expresses the smoothness of the spline function)\n\nFor a gaussian regression this is the integrated second derivative of the spline function (see paper for more details)\n\nExample:\n\\[\n||y-f||^2 + \\lambda \\int \\left(\\frac {\\partial^2 f(\\text{log[baseline profit]})}{\\partial \\; \\text{log[baseline profit]}^2}\\right)^2 \\partial x\n\\]\n\n\nλ is a tuning parameter that’s ≥0\n\n\nB-Spline basis is typically used\nNot easy to specify the degrees of freedom, since they will vary depending on the size of the penalty\n\nUsually can be restricted to a maximum number of degrees of freedom or desired degrees of freedom\n\nPenalized Regression Splines\n\nApproximation of a smoothing spline\nBest used when n is large and the variable range is covered densely by the observed data\nP-Spline\n\nBased on the cubic B-spline basis and on a ‘large’ set of equidistant knots (usually, 10–40)\nSimplifies the calculation of Jβ (see paper for more details)\nPackages: {mgcv}, {gamlss} (See above, Misc &gt;&gt; Package Comparison)",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-inter",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-inter",
    "title": "Splines",
    "section": "Interactions",
    "text": "Interactions\n\nNumeric spline varying by indicator\ns(log_profit_rug_business_b, by = treatment)\n\nCoefficient is a conditional average treatment effect (CATE)\nCreates the main effect and the interaction",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-tokenization.html",
    "href": "qmd/feature-engineering-tokenization.html",
    "title": "Tokenization",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Engineering",
      "Tokenization"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-misc",
    "href": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-misc",
    "title": "Tokenization",
    "section": "",
    "text": "Also see\n\nFeature Engineering, Embeddings\nDiagnostics, NLP\nEDA, Text\nNLP, General\n\nText augmentation\n\nUseful for imbalanced outcomes with text predictors\n\nBetter performance than subsampling\n\nWords are randomly swapped, deleted, as well as replaced or inserted with synonyms using pretrained word embeddings\n\n{{textattack}}, article, ipynb\nAdversarial Text Attack\n\nDiagnostic to test how a model performs on a test set where data augmentation techniques have been applied\n{{textattack}}, article, ipynb",
    "crumbs": [
      "Feature Engineering",
      "Tokenization"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-terms",
    "href": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-terms",
    "title": "Tokenization",
    "section": "Terms",
    "text": "Terms\n\nOut-of-Vocabulary (OOV) - Sometimes vocabularies a trimmed to the only the most common tokens. Words that aren’t in the vocabulary get assigned OOV tokens.\nTokenization - process of splitting a phrase, sentence, paragraph, one or multiple text documents into smaller units. Different algorithms follow different processes in performing tokenization\nToken - output from tokenization — a word, a subword (e.g. prefix, suffix, or root), or even a character.\nVocabulary - a set of unique tokens in a corpus (a dataset in NLP) that is then converted into numbers (IDs) which helps us in modeling",
    "crumbs": [
      "Feature Engineering",
      "Tokenization"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-preproc",
    "href": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-preproc",
    "title": "Tokenization",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nMisc\n\nPrimer on Cleaning Text Data - py code for cleaning\nThe Ultimate Preprocessing Pipeline for Your NLP Models - py code for cleaning\n\nClusters embeddings to remove “boilerplate language” (i.e. noise) from data\nParts of Speech (POS) tagging\n\n** Cleaning needs to happen before any clustering, POS tagging, Lemmatization or Stemming takes place. **\n\nSimilarly, clustering should happen before POS tagging and lemmatizing, since using the entire text will make the compute extremely costly and and less effective.\n\n\nReplace NA values with empty spaces\nLowercase all text\nReplace digits with words for the numbers\nRemove punctuation\n\n#$%&\\’()*+,-./:;?@[\\\\]^_{|}`~\n\nRemove emojis\n\nIf you are trying to sentiment analysis, trying to transform emojis into some text format instead of outright removing them may be beneficial\n\nSpell out contractions\nStrip HTML Tags\nRemove stopwords\nRemove accented characters\nRemove URLs, Mentions (@), hastags (#) and special characters\nRemove whitespace\nRemove boilerplate language (see article)\n\n\nText embeddings are created. The embeddings are clustered to find repeatedly occurring sentences and words and removes them, with an assumption that something that is repeated more than a threshold number of times, is probably “noise”.\nThe resultant text is a cleaner, more meaningful, summarized form of the input text. By removing noise, we are pointing our algorithm to concentrate on the important stuff only.\nArticle reduces size of text by 88%\n\nParts of Speech (POS) tagging\n\nLabels each word in a sentence as a noun, verb, adjective, pronoun, preposition, adverb, conjunction, or interjection.\nContext can largely affect the natural language understanding (NLU) processes of algorithms.\n\nLemmatization and Stemming\n\nStemming removes suffixes from words to bring them to their base form.\nLemmatization uses a vocabulary and a form of morphological analysis to bring the words to their base form.\nBoth reduce the dimensionality of the input feature space.\nLemmatization is generally more accurate than stemming but is computationally expensive\nLemmatization preserves the semantics of the input text.\n\nAlgorithms that are meant to work on sentiment analysis, might work well if the tense of words is needed for the model. Something that has happened in the past might have a different sentiment than the same thing happening in the present.\n\nStemming is fast, but less accurate.\n\nIn instances where you are trying to achieve text classification, where there are thousands of words that need to be put into categories, stemming might work better than lemmatization purely because of the speed.\n\nSome deep-learning models have the ability to automatically learn word representations which makes using either of these techniques, moot.",
    "crumbs": [
      "Feature Engineering",
      "Tokenization"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-texrec",
    "href": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-texrec",
    "title": "Tokenization",
    "section": "Text Recipes",
    "text": "Text Recipes\n\nMisc\n\nEvery text preprocessing sequence should probably start with step_tokenize and end withstep_tf or step_tfidf \n\nExample sequence of steps: step_tokenize, step_stopwords, step_ngrams, step_tokenfilter, step_tfidf\n\nModels with large numbers (100s) of features increases the opportunity for feature drift (i.e. use step_tokenfilter)\nIf you have a number of text columns, it’ll be easier to create a char var and use that in the recipe\ntext_cols &lt;- c(\"text_col1, \"text_col2\", \"text_col3\")\nstep_whatever(all_of(text_cols))\n\nExamine Tokens from recipe\ntoken_list_tbl &lt;- \n  text_recipe %&gt;% \n    prep() %&gt;% \n    juice()\n\n# look at tokens created from 1st row of text column (e.g. text_col)\ntoken_list_tbl$text_col[1] %&gt;%\n    attr(\"unique_tokens\")\n\nCan use this after every text recipe step to examine the results\n\nDebugging\n\n“length of ‘dimnames’ [2] not equal to array extent”\n\nExample: from DRob’s Prediction box office performance\n\nUsing the min_times arg with step_tokenfilter and step_stopwords with one of the tokenized variables resulted in this error.\n\n\n\nstep_tokenize\ntextrecipes::step_tokenize(genres, production_countries,\n                           token = 'regex',\n                           options = list(pattern = \";\"))\n\nTake delimited text and create tokens\nGenres has values like “action;horror;comedy” per row\nCreated using function that extract values from json columns\n\nSee script Code &gt;&gt; JSON &gt;&gt; extract-values-from-json-column.R\n\n\nstep_stopwords\ntextrecipes::step_stopwords(tokenized_var1, tokenizedvar2)\n\nRemove words like “the” and “at”\ncustom_stopword_source: Provide a vector of stopwords\nkeep: Provide a vector of words you don’t want filtered out\n\nstep_ngram\n\nCreates ngrams (i.e. combines tokens into n number of words to create phrases)\nstep_ngram(\n    tokenized_var1,\n    num_tokens = 3,\n    min_num_tokens = 1\n)\n\nExample creates uni-gram (aka tokens), bi-grams, and tri-grams (n = 1, 2, and 3)\nTokens combined (with underscores) sequentially as they occur in the original string\n\ne.g. Bigram: 1st word combined with 2nd word, then 2nd word combined with 3rd word, etc.\n\n\n\nstep_tokenfilter\n\nUseful for reducing features\n\nThis step can cause the number of features to explode into the thousands, so if your sample size is in the 100s, you’ll need to reduce the number of features\n\nRemove tokens if they have a count below a specified number\ntextrecipes::step_tokenfilter(genres, production_countries\nmin_times = 50\n\nSee above for example of genre variable\nSo if “action” doesn’t appear in at least 50 rows of the dataset, a token won’t be created for it\nIf you set arg, percentage=T, then min_times can be a percentage\nmax_times also available\n\n\nOnly keep tokens with a count in the top_n\ntextrecipes::step_tokenfilter(genres, production_countries\nmax_tokens = 50\n\nCan use with min/max_times but max_tokens gets applied after min/max times\nIn the example, there are 2 text columns (genres, production countries) and max_tokens = 50, therefore 50*2 = 100 columns get created.\nDefault = 100\n\nstep_tf\ntextrecipes::step_tf(genres, production countries)\n\nTerm Frequency Columns; Creates indicator variables for your tokens\nFormat: “tf_variableName_tokenName”\nExample: “tf_genres_action”\n\nstep_tfidf\n\n\nMixture of tf and idf\n\nTerm frequency (tf) measures how many times each token appears in each observation\n\ni.e. Number of times a word appears in a document, divided by the total number of words in that document\n\nInverse document frequency (idf) is a measure of how informative a word is, e.g., how common or rare the word is across all the observations.\n\ni.e. Logarithm of the number of documents in the corpus divided by the number of documents where the specific term appears\n\nWeighs down the frequent words and scaling up the rare ones\n\n\n\nThe “mixture” seems to be the product of these two values\nstep_tfidf(var_thats_been_tokenized_and_filtered)\n\n**Strongly advised to use step_tokenfilter before using step_tfidf to limit the number of variables created; otherwise you may run into memory issues**\n\n\nstep_dummy_extract - Pull out a pattern and create dummy variables\nexample_data &lt;- tribble(\n  ~ language,\n  \"English, Italian\",\n  \"Spanish, French\",\n  \"English, French, Spanish\"\n)\n\nrecipe(~., data = example_data) |&gt;\n  step_dummy_extract(language, sep = \", \") |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 3 × 5\n#&gt;   language_English language_French language_Italian language_Spanish\n#&gt;              &lt;int&gt;           &lt;int&gt;            &lt;int&gt;            &lt;int&gt;\n#&gt; 1                1               0                1                0\n#&gt; 2                0               1                0                1\n#&gt; 3                1               1                0                1\n#&gt; # ℹ 1 more variable: language_other &lt;int&gt;\nstep_textfeature\nlibrary(textrecipes)\n\ndata(tate_text, package = \"modeldata\")\n\nrecipe(~ medium, data = tate_text) |&gt;\n  step_textfeature(medium) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\n#&gt; Rows: 4,284\n#&gt; Columns: 26\n#&gt; $ textfeature_medium_n_words        &lt;int&gt; 8, 3, 3, 3, 4, 4, 4, 3, 6, 3, 3, 3, …\n#&gt; $ textfeature_medium_n_uq_words     &lt;int&gt; 8, 3, 3, 3, 4, 4, 4, 3, 6, 3, 3, 3, …\n#&gt; $ textfeature_medium_n_charS        &lt;int&gt; 48, 14, 14, 14, 16, 16, 19, 14, 22, …\n#&gt; $ textfeature_medium_n_uq_charS     &lt;int&gt; 19, 12, 12, 12, 11, 11, 12, 11, 14, …\n#&gt; $ textfeature_medium_n_digits       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_n_hashtags     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_n_uq_hashtags  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_n_mentions     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_n_uq_mentions  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_n_commas       &lt;int&gt; 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_n_periods      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_n_exclaims     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_n_extraspaces  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_n_caps         &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#&gt; $ textfeature_medium_n_lowers       &lt;int&gt; 43, 13, 13, 13, 15, 15, 18, 13, 21, …\n#&gt; $ textfeature_medium_n_urls         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_n_uq_urls      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_n_nonasciis    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_n_puncts       &lt;int&gt; 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_first_person   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_first_personp  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_second_person  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_second_personp &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_third_person   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_to_be          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ textfeature_medium_prepositions   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 , 0, …\n\nCreates a set of predictors that count the number of characters, words, periods, emojis, etc.\n\nOther\n\nNot sure what this weight scheme does and how it helps\nstep_tokenize(market_category) %&gt;%\nstep_tokenfilter(market_category, min_times = 0.05, max_times = 1, percentage = TRUE) %&gt;%\nstep_tf(market_category, weight_scheme = \"binary\")",
    "crumbs": [
      "Feature Engineering",
      "Tokenization"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-tokalgs",
    "href": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-tokalgs",
    "title": "Tokenization",
    "section": "Tokenization Algorithms",
    "text": "Tokenization Algorithms\n\nWord-Based\n\nSplits a piece of text into words based on a delimiter\nAdvantages: meaningful tokens\nIssues:\n\nVocabularies can be very large\n\nAssigns different IDs to the words like “boy” and “boys”\nCausing the model to be heavier and requiring more computational resources\nSolution: Restrict size of the vocabulary to only most common tokens. Tradeoff is that you’re losing information\n\nMisspelled words in the corpus get assigned an OOV token\nAll semantic uses for a word are combined into one representation.\n\nExample, the word “play” in “I’m going to see a play” and “I want to play” will have the same embedding, without the ability to distinguish context\n\n\nTypes: space and punctuation, rule-based\nNLP models that use this type:\n\nTransformer XL (vocabulary = 267,735)\nWord2Vec (?)\n\n\nCharacter-Based\n\nSplits the text into individual characters\nLanguage has many different words but has a fixed number of characters. This results in a very small vocabulary.\n\nEnglish Language\n\n256 different characters (letters, numbers, special characters)\n170,000 words in its vocabulary\n\n\nAdvantages:\n\nCan create a representation of the unknown words (words not seen during training) using the representation for each character\nMisspelled words can be spelled correctly rather can marking them as OOV tokens and losing information (?)\nFast and requires less compute resources\n\nIssues:\n\nLess meaningful tokens for some languages:\n\nCharacters have meaning in some languages (e.g. Chinese) but not others (e.g. English).\n\nTokenized sequence is much longer than the initial raw text (e.g. “knowledge” will have 9 different tokens)\n\n\nSubword-Based\n\nSplits based on rules:\n\nDo not split the frequently used words into smaller subwords.\nSplit the rare words into smaller meaningful subwords.\n\nUses a special symbol to indicate which word is the start of the token and which word is the completion of the start of the token.\n\n“tokenization” can be split into “token” and “##ization” which indicates that “token” is the start of the word and “##ization” is the completion of the word.\nDifferent models use different special symbols (Wordpiece uses ##)\n\nAdvantages:\n\nMeaningful tokens but with a more managable vocabulary size\nPossible for a model to process a word which it has never seen before as the decomposition can lead to known subwords\n\nTypes\n\nWordPiece used by BERT and DistilBERT\nUnigram by XLNet and ALBERT\nBye-Pair Encoding by GPT-2 and RoBERTa",
    "crumbs": [
      "Feature Engineering",
      "Tokenization"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-eng",
    "href": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-eng",
    "title": "Tokenization",
    "section": "Engineering",
    "text": "Engineering\n\ne.g. Comments on a social media platform\nSentiment\n\nCategorize each comment as positive, negative, or neutral\n\nNumber of new comments\nCreate consumer profiles\n\nClustering based on consumer characteristics and use a feature\nSocial media characteristics\n\nLocation of comments\n\nLocation tags\n\nLanguage spoken\nBiographical data in profile\n\nShared links",
    "crumbs": [
      "Feature Engineering",
      "Tokenization"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html",
    "href": "qmd/feature-reduction.html",
    "title": "Feature Reduction",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-misc",
    "href": "qmd/feature-reduction.html#sec-feat-red-misc",
    "title": "Feature Reduction",
    "section": "",
    "text": "Curse of Dimensionality\n\nIts when there are more variables than observations.\nCauses the least squares coefficient estimates to lose uniqueness.\nCauses overfitting in ML algorithms\n\nPackages\n\n{intRinsic} - Likelihood-Based Intrinsic Dimension Estimators; implements the ‘TWO-NN’ and ‘Gride’ estimators and the ‘Hidalgo’ Bayesian mixture model\n\nProvides a clustering function for the Hidalgo model\nGraphical outputs built using ggplot2 so they are customizable\nSee section 5 (Summary and discussion) of the vignette for the recommended workflow and examples\n\n{Rdimtools} - feature selection, manifold learning, and intrinsic dimension estimation (IDE) methods\n\nCurrent version delivers 145 Dimension Reduction (DR) algorithms and 17 Intrinsic Dimension Estimator (IDE) methods.\n\n{RDRToolbox} - nonlinear dimension reduction with Isomap and LLE\n\nFor Time Series, see\n\nForecasting, Multivariate &gt;&gt; Dynamic Factor Models\n(Below) Dynamic Mode Decomposition",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-terms",
    "href": "qmd/feature-reduction.html#sec-feat-red-terms",
    "title": "Feature Reduction",
    "section": "Terms",
    "text": "Terms\n\nIntrinsic Dimension (ID) - the minimal number of parameters needed to represent all the information contained in the data without significant information loss. A necessary piece of information to have before attempting to perform any dimensionality reduction, manifold learning, or visualization tasks. An indicator of the complexity of the features of a dataset.\nIsomap (IM) - nonlinear dimension reduction technique presented by Tenenbaum, Silva and Langford in 2000 [3, 4]. In contrast to LLE, it preserves global properties of the data. That means, that geodesic distances between all samples are captured best in the low dimensional embedding\nLocally Linear Embedding (LLE) - introduced in 2000 by Roweis, Saul and Lawrence. It preserves local properties of the data by representing each sample in the data by a linear combination of its k nearest neighbors with each neighbor weighted independently. LLE finally chooses the low dimensional representation that best preserves the weights in the target space.\nProjection Methods - maps the original data to a lower-dimensional space. The projection function can be linear, as in the case of PCA or or nonlinear, as in the case of locally linear embedding, Isomap, and tSNE.\nGeometric Methods - rely on the topology of a dataset, exploiting the properties of the distances between data points",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-pca",
    "href": "qmd/feature-reduction.html#sec-feat-red-pca",
    "title": "Feature Reduction",
    "section": "PCA",
    "text": "PCA\n\nDescription\n\nCreates a subset of variables that maximises the covariance with the initial variable set, in order to store as much information as possible in a lower dimension.\nCompute an orthogonal basis of the space created by the original set of variables. The vectors creating this basis are the eigenvectors of the variance-covariance matrix. Reducing the dimension is then easily done by selecting the eigenvectors that are most representative of the initial data: those that contain most of the covariance. The amount of covariance stored by the vectors is quantified by the eigenvalues: the larger the eigenvalue, the more interesting its associated vectors.\nProjects variables orthogonally which removes correlation between predictor variables. The projection is in the direction of maximum variation such that the variation is distributed unequally among the transformed vectors. This allows the user to reduce the feature space while still being able to capture most of variance in the data.\nThe principal components are equal to linear combinations of the correlated variables and these components are orthogonal to each other.\nWhy? When multiple variables are highly correlated to each other it causes the math used calculate regression models to break down. High dimension datasets also require large amount computational resources. Too many columns compared to the number of rows.\n\n\n\nMisc\n\nAs a multicollinearity detector?\n\n“use principal component analysis, and examine the screeplot, or proportion of variation explained by a subset of principal components. If all (or almost all) of the variation is explained with a small subset of all the variables, it means you have a multicollinearity problem. You will need to drop some variables or do some other dimension reduction to fix it before choosing your final model.”\nI mean what if you have 20 variables and 3 are collinear, would this be detectable with PCA? I don’t think so. Seem more likely that it would take a large portion of your variables being collinear for it to be detectable in this fashion.\n\n\n\n\nPreprocessing\n\nNotes from thread\nCenter variables\n\ncenter = T is default in prcomp( )\nIf variables are NOT on similar scales, then the data need to be scaled, also.\n\nProbably safer to always scale.\n\n\nSqrt any count variables\nLog any variable with a heavy tail\nIf you have too many features, use a sparse matrix to speed the process.\n\n\n\nDiagnostics\n\nTest for localization (repo with R code/docs)\n\nBad: if you make a histogram of a component (or loading) vector and it has really big outliers (aka localization)\n\nMeans this vector is mostly noise\n\nSolution: Regularized spectral clustering (links to resources)\nD_r = Diagonal(1/ sqrt(rs + mean(rs))\nD_c = Diagonal(1/ sqrt(cs + mean(cs))\n# Do SVD on\nD_r %*% A %*% D_c\n\nA is your matrix\nrs is a vector containing the row sums of the matrix\ncs is a vector containing the column sums of the matrix\n\n\n\n\n\nSteps\n\nCenter data in design matrix, A (n x p)\n\nIf data are centered and scaled then the computation in step 2 will result in the correlation matrix instead of the covariance matrix.\n\nCompute \\(n \\times n\\) Covariance Matrix,\n\\[\nC_x = \\frac{1}{n-1}AA^T\n\\]\n\nAlso seen \\(A^T A\\) but I don’t think it matters. The upper triangle and the lower triangle of this product are just reverse covariances of each other and thus equal and I suspect the order just switches flips the triangles. The eigenvectors/eigenvalues get reordered later on anyways.\nThe diagonal of this matrix is the variable variances.\n\nCalculate eigenvectors and eigenvalues: \\(C_x V = D_\\lambda V\\) shows the covariance matrix as a transformation matrix. \\(D\\) is a diagonal matrix (\\(p\\times p\\)) with eigenvalues along the diagonal. \\(V\\) is a matrix (\\(p \\times p\\)) of eigenvectors\n\\[\nD_\\lambda = VC_x V^{-1}\n\\]\nOrder eigenvalues from largest to smallest\nOrder the eigenvectors according to the order of their corresponding eigenvalues\nEquation for the ith value of the PC1 vector: \\(\\text{PC1}_i = V_{(,1)} \\cdot A_{(i,)}\\)\n\n\\(\\text{PC2}\\) is similar except \\(V_{(,2)}\\) is used\nWhere all the variables in \\(A\\) have been standardized and \\(V\\) contains the loadings (see below)\n\n\n\n\nNotes\n\n\\(AA^T\\) is positive definite\n\nWhich means it’s symmetric\nWhich means it has real eigenvalues and orthogonal eigenvectors\nWhich means the eigenvectors have covariances = 0\nWhich means the eigenvectors aren’t correlated.\n\nThe eigenvalues are eigenvector’s standard deviations which determines how much variance is explained by that PC.\nThe variance of a variable is the dot-product of itself and it’s transpose, \\(x_i \\cdot x^t_i\\)\nThe covariance between two variables, \\(x_i \\cdot x^t_j\\)\nIn step 3 equation, eigenvalues give the magnitude (length of vector) and eigenvectors the direction after being transformed by the covariance matrix.\nElements in a PC vector are called scores and elements in the V eigenvector are called loadings.\n\nThe loadings are the coefficients in the linear combination of variables that equals the PC vector\nLoadings range from -1 to 1\nVariables with high loadings (usually defined as .4 in absolute value or higher because this suggests at least 16% of the measured variable variance overlaps with the variance of the component) are most representative of the component\nThe sign of a loading (+ or -) indicates whether a variable and a principal component are positively or negatively correlated.\n\nScaling your design matrix variables just means your using a correlation matrix instead of a covariance matrix.\nPCA is sensitive to outliers. Variance explained will be inflated in the direction of the outlier\n\nGuessing this means components strongly influenced by variables with outlier values will have their variance-explained value inflated\n\nRow order of data matters as to which interpretation (latent) of component is valid from Principle Components and Penguins\n\nUsed data from palmerpenguins to create a “penguin size” variable from performing PCA on the data.\nIn one row order, high values of pc1 were associated with high body mass, but after scrambling the rows, high values of pc1 were associated with low body mass.\nHave to be careful when adding new data to the PCA-created feature. It might arbitrarily change the sign of the component and change the meaning of the feature.\n\nPCA doesn’t take the response variable into account (unsupervised). Therefore, the directions (eigenvectors) obtained may be well-suited for the predictor variables, but not necessarily optimal for predicting the response. It does often produce pretty good results though.\n\nAn alternative would be Partial Least Squares (PLS) which does take the response into account (supervised).\n\nIn practice, PLS reduces bias while potentially increasing the variance so the benefit vs PCA regression is usually a wash.\nCapable of handling multivariate regression\nPopular in chemometrics for analyzing spectra.\n\n\n\n\n\nPlots\n\nMisc\n\nNotes from: https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/multivariate/how-to/principal-components/interpret-the-results/all-statistics-and-graphs/\n(See bkmk) use broom::augment to add original data to pca output. Coloring the points by categorical variables can help with interpreting the components\nAlso see pkgs in notebook for visualization options\n\nScore\n\n\nClusters\n\nIf data follow a multivariate normal distribution then scores should be randomly distributed around zero\nIf there are clusters, then there may be multiple distributions present\n\nExtreme points (e.g. point in bottom right) might be outliers and it might be worthwhile to investigate them further\n\nLoadings\n\n\nNeed to imagine an axis at (0,0). Don’t know why they don’t plot them with those axes.\nFinding the largest variable influences on a PC can used to interpret it’s meaning (think latent variable)\nArrows\n\nA (near) horizontal arrow (along the x-axis) describes that the feature contributes strongly toward PC1.\nA (near) vertical arrow (along the y-axis) describes that a feature contributes strongly towards PC2.\n\nValues\n\nLoadings range from -1 to 1\nThe termination coordinate of the line gives the loading values for that variable for both PCs\nLoadings (absolute magnitude) close to 0 indicate that the variable has little influence on that PC\nLarger the absolute value of the loading the greater the influence on that PC\nNegative values have negative influence on the latent variable that the PC represents (vice versa for positive values)\n\nAngle\n\nacute angles represent a positive correlation between those variables\nobtuse angles represent a negative correlation between those variables\n90 degree angles represent independence between those variables\n\nExample\n\nAge, Residence, and Employ have large influences on PC1 (interpretation: financial stability)\nCredit cards, Debt, and Education have large influences on PC2 (interpretation: credit history)\nSays, “As the number credit cards increases, credit history (PC2 interpretation) becomes more negative.”\n\n\nBi-Plot\n\n\nCombination plot of the score and loading plot\nCan augment pca output (see top of section) with original data and color the scores by different categorical variables\n\nIf a categorical variable level is clustered around Education, you could say as Education rises, the more likely that that person is &lt;categorical level&gt;.\nIn turn, that categorical level would be either positively or negatively (depending on the loading sign) associated with that PC.\n\n\nInterpretation\n\nExample: Bluejays\n\nLoadings\n\n\nPC2 represents the difference between bill size and skull size\n\nLoadings together with components plot\n\n\nMale birds larger than female birds\n\nIf you look at the loadings plot, negative pc1 corresponds to larger size and the components plot shows males with negative PC1 values\n\nBoth sexes have large and short bills relative to their overall size\n\nMales and females both show values above and below 0 in PC2\nLarger bills but smaller bodies (+PC2) and larger bodies but smaller bills (-PC2)\n\n\nVariance Explained\n\n\nOverall bird size explains &gt; 50% of the variation in measurements\n\n\n\nExample: How much variation in a principal component can be explained by a categorical variable\n# Penguins dataset\n# pca_values is a prcomp() object\npca_points &lt;- \n  # first convert the pca results to a tibble\n  as_tibble(pca_values$x) %&gt;% \n  # now we'll add the penguins data\n  bind_cols(penguins)\n## # A tibble: 6 x 12\n##    PC1    PC2    PC3    PC4 species island bill_length_mm bill_depth_mm\n##  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n## 1 -1.85 -0.0320  0.235  0.528 Adelie  Torge…          39.1          18.7\n## 2 -1.31  0.443  0.0274  0.401 Adelie  Torge…          39.5          17.4\n## 3 -1.37  0.161  -0.189  -0.528 Adelie  Torge…          40.3          18 \n## 4 -1.88  0.0123  0.628  -0.472 Adelie  Torge…          36.7          19.3\n## 5 -1.92 -0.816  0.700  -0.196 Adelie  Torge…          39.3          20.6\n## 6 -1.77  0.366  -0.0284  0.505 Adelie  Torge…          38.9          17.8\n## # … with 4 more variables: flipper_length_mm &lt;int&gt;, body_mass_g &lt;int&gt;,\n## #  sex &lt;fct&gt;, year &lt;int&gt;\n\npc1_mod &lt;- \n  lm(PC1 ~ species, pca_points)\nsummary(pc1_mod)\n## Call:\n## lm(formula = PC1 ~ species, data = pca_points)\n## \n## Residuals:\n##    Min      1Q  Median      3Q    Max \n## -1.3011 -0.4011 -0.1096  0.4624  1.7714 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)      -1.45753    0.04785  -30.46  &lt;2e-16 ***\n## speciesChinstrap  1.06951    0.08488  12.60  &lt;2e-16 ***\n## speciesGentoo    3.46748    0.07140  48.56  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5782 on 330 degrees of freedom\n## Multiple R-squared:  0.879,  Adjusted R-squared:  0.8782 \n## F-statistic:  1198 on 2 and 330 DF,  p-value: &lt; 2.2e-16\n\nFrom https://bayesbaes.github.io/2021/01/28/PCA-tutorial.html\nAdjusted R-squared: 0.8782\n\n\nCan be seen visually in this chart by looking at the points in relation to the x-axis where species is segregated pretty nicely.\n\n\n\n\n\nOutliers\n\nMahalanobis Distance (MD)\n\nThis method might be problematic. Supposedly outliers affect the covariance matrix which affects the PCA, which affects the scores, which affects theMahalanobis distance (MD). So the MD might be biased and not be accurate in determining outliers\n\nRobust forms of PCA (see section below) would be recommended if you suspect outliers in your data.\n\nDisplays the Mahalanobis distance (MD) for each observation and a reference line to identify outliers. The Mahalanobis distance is the distance between each data point and the centroid of multivariate space (the overall mean).\n\nOutliers determined by whether the Mahalanobis Distance is greater than the square root of the Chi-Square statistic where m is the number of variables and α = 0.05\n\nNo outliers in the chart above as all MDs lower than the threshold at 4.4\n\n\nLeverage Points and Orthogonal Outliers\n\nNotes from https://towardsdatascience.com/multivariate-outlier-detection-in-high-dimensional-spectral-data-45878fd0ccb8\nTypes\n\nLeverage Points\n\ncharacterized by a high score distance\ngood leverage points also have short orthogonal distance and bad leverage points have long orthogonal distances\ngood leverage points have a positive effect\n\nOrthogonal Outliers\n\ncharacterized by a high orthogonal distance\n\n\nType determined by (see article for the math)\n\nScore DIstance (SD) - the distance an observation is from center of K-dimensional PCA subspace\nOrthogonal Distance (OD) - the deviation — i.e. lack of fit — of an observation from the k-dimensional PCA subspace\nOutliers are determined by Chi-Square test very similar to the Mahalanobis Distance method (see above).\n\n\n\nIn the example shown, the red dots are data known to be measurement errors. Most of the red dots are captured in the orthogonal and bad sections but quite a few normal observation (blue) points too. So this method needs to be used as a guide and followed up upon when it flags points.\n\n\n\nHotelling’s T2 and SPE/DmodX (Complementary Tests)\n\n{{pca}}\nHotelling’s T2 works by computing the chi-square tests across the top n_components for which the p-values are returned that describe the likeliness of an outlier. This allows for ranking the outliers from strongest to weak.\nSPE/DmodX (distance to model) based on the mean and covariance of the first 2 PCs\n\n\n\n\nExtensions\n\nRobust PCA\n\nData with outliers and high dimensional data (p &gt;&gt; n) are not suitable for regular PCA where p is the number of variables.\nLow Dim methods (only valid when n &gt; 2p) that find robust (against outliers) estimates of the covariance matrix\n\nS-estimator, MM-estimator, (Fast)MCD-estimator, re-weighted MCD- (RMCD) estimator\n\nHigh Dim Methods\n\nRobust PCA by projection-pursuit (PP-PCA)\n\nfinds directions for eigenvectors that maximize a “projection index” instead of directions that maximize variance\n\nMAD or Qn-estimator is used a projection index\n\n\nSpherical PCA (SPCA)\n\nhandles outliers by projecting points onto a sphere instead of a line or plane\n\nalso uses MAD or Qn-estimator\n\n\nRobust PCA (ROBPCA)\n\ncombines projection index approach with low dim robust covariance estimation methods somehow\n\nRobust Sparse PCA (ROSPCA)\n\nsame but uses sparse pca\napplicable to both symmetrically distributed data and skewed data\n\n\n\nKernel PCA\n\nPackages: {kernlab}\nNonlinear data (notebook)\nPCA in a hypothetical (kernel trick), higher dimensional space\nWith more dimensions, data points become more separable.\nResults depend on type of kernel\n\nGaussian Kernel\n\nTuning parameter: sigma\n\n\n\n\n\n\nTidymodels\n\nRecipe step\n# if only using dummy vars, no sure if normalization is necessary\n# step_normalize(&lt;pca variables&gt;)\nstep_pca(starts_with(\"tf_\"), num_comp = tune())\n# don't forget to include num_comp in your tuning grid\nTaking a tidymodel’s recipe object and performing PCA\ntf_mat &lt;- recipe_obj %&gt;%\n    # normalizing tokenized indicators (?)\n    # since these are all dummy vars, not sure if a normalization step is necessary)\n    step_normalize(starts_with(\"tf_\")) %&gt;%\n    prep() %&gt;%\n    bake() %&gt;%\n    # only want to pca text features\n    select(starts_with(\"tf_\") %&gt;%\n    as.matrix()\n\ns &lt;- svd(tf_mat)\n# scree plot\ntidy(s, matrix = \"d\") %&gt;%\n    filter(PC &lt;= 50) %&gt;%\n    ggplot(aes(x = PC, y = percent)) +\n    geom_point()\n\nmatrix (tidy arg):\n\n“u”, “samples”, “scores”, or “x”: Returns info about the map from the original space to the pc space\n“v”, “rotation”, “loadings”, or “variables”: Returns information about the map from the pc space to the original space\n“d”, “eigenvalues”, or “pcs”: Returns information about the eigenvalues\n\n\nExample\nlibrary(tidymodels) \nlibrary(workflowsets) \nlibrary(tidyposterior) \ndata(meats, package= \"modeldata\") \n# Keep only the water outcome \nmeats &lt;- select(meats, -fat, -protein) \nset.seed(1) \nmeat_split &lt;- initial_split(meats) \nmeat_train &lt;- training(meat_split) \nmeat_test &lt;- testing(meat_split) \nset.seed(2) \nmeat_folds &lt;- vfold_cv(meat_train, repeats = 3)\nbase_recipe &lt;- \n  recipe(water ~ ., data = meat_train) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_YeoJohnson(all_predictors()) %&gt;% \n  step_normalize(all_predictors()) \npca_recipe &lt;- \n  base_recipe %&gt;% \n  step_pca(all_predictors(), num_comp = tune()) \npca_kernel_recipe &lt;- \n  base_recipe %&gt;% \n  step_kpca_rbf(all_predictors(), num_comp = tune(), sigma = tune())",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-efa",
    "href": "qmd/feature-reduction.html#sec-feat-red-efa",
    "title": "Feature Reduction",
    "section": "Exploratory Factor Analysis (EFA)",
    "text": "Exploratory Factor Analysis (EFA)\n\nIdentifies a number of latent factors that explain correlations between observed variables\n\nFrequently employed in social sciences where the main interest lies in measuring and relating unobserved constructs such as emotions, attitudes, beliefs and behaviour.\nLatent variables, referred to also as factors, account for the dependencies among the observed variables, referred to also as items or indicators, in the sense that if the factors are held fixed, the observed variables would be independent.\nIn exploratory factor analysis the goal is the following: for a given set of observed variables x1, . . . , xp one wants to find a set of latent factors ξ1, . . . , ξk, fewer in number than the observed variables (k &lt; p), that contain essentially the same information.\nIn confirmatory factor analysis, the objective is to verify a social theory. Hence, a factor model is specifed in advance and its fit to the empirical data is tested.\n\nMisc\n\nPackages\n\n{psych} - factor analysis, item response theory, reliability analysis\n{factominer} - Multiple Factor Analysis (MFA}\n{fspe} - Model selection method for choosing number of factors\n\nUses the connection between model-implied correlation matrices and standardized regression coefficients to do model selection based on out-of-sample prediction errors\n\n\nTwo main approaches for analysing ordinal variables with factor models:\n\nUnderlying Response Variable (URV)\n\nThe ordinal variables are generated by underlying continuous variables partially observed through their ordinal counterparts. (also see Regression, Ordinal &gt;&gt; Cumulative Link Models (CLM))\n\nItem Response Theory (IRT)\n\nOrdinal indicators are treated as they are.\n\n\n\nMethods for selecting the right number of factors\n\nMisc\n\nIssue: more factors always improve the fit of the model\n\nParallel Analysis: analyze the patterns of eigenvalues of the correlation matrix\nModel Selection: likelihood ratio tests or information criteria\n\nComparison with PCA\n\nPCA is a technique for reducing the dimensionality of one’s data, whereas EFA is a technique for identifying and measuring variables that cannot be measured directly (i.e. latent factor)\nWhen variables don’t have anything in common, EFA won’t find a well-defined underlying factor, but PCA will find a well-defined principal component that explains the maximal amount of variance in the data.\nDifferences in the results between PCA and EFA don’t tend to be obvious in practice. As the number of variables (&gt;40 variables) involved in the analysis grows, results from PCA and EFA become more and more similar.\nSimilarly calculated method to PCA, but FA is an analysis on a reduced correlation matrix, for which the ones in the diagonal have been replaced by squared multiple correlations (SMC)\n\nA SMC is the estimate of the variance that the underlying factor(s) explains in a given variable (aka communality).\n\nThe variability in measured variables in PCA causes the variance in the principal component. This is in contrast to EFA, where the latent factor is seen as causing the variability and pattern of correlations among measured variables\nAn eigenvalue decomposition of the full correlation matrix is done in PCA, yet for EFA, the eigenvalue decomposition is done on the reduced correlation matrix\nFactor Analysis is a latent variable measurement model\n\nThe causal relationship is flipped in FA as compared to PCA.\n\n\nF is the latent variable (instead of component in PCA), b is a weight (like loadings in PCA), Y is a predictor variable, and u is an error\n\nHere, b estimates how much F contributes to Y",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-autoenc",
    "href": "qmd/feature-reduction.html#sec-feat-red-autoenc",
    "title": "Feature Reduction",
    "section": "Autoencoders",
    "text": "Autoencoders\n\nUnsupervised neural networks that learn efficient coding from the input unlabelled data. They try to reconstruct the input data by minimizing the reconstruction loss\nMisc\n\nUndercomplete Autoencoder (AE) — the most basic and widely used type, frequently referred to as an Autoencoder\nSparse Autoencoder (SAE) — uses sparsity to create an information bottleneck\nDenoising Autoencoder (DAE) — designed to remove noise from data or images\nVariational Autoencoder (VAE) — encodes information onto a distribution, enabling us to use it for new data generation\n\nLayers\n\nEncoder: Mapping from Input space to lower dimension space\nDecoder: Reconstructing from lower dimension space to Output space\n\nProcess\n\n\nEncodes the input data (X) into another dimension (Z), and then reconstructs the output data (X’) using a decoder network\nThe encoded embedding (Z) is preferably lower in dimension compared to the input layer and contains all the efficient coding of the input layer\nOnce the reconstruction loss is minimized, the learned weights or embeddings, in the Encoder layer can be used as features in ML models and the Encoder layer can be used to generate embeddings on future data.\n\nSparse Autoencoder (SE)\n\n\nUses regularization\nDimension reduction in the center is achieved through deactivating neurons\nExample\n\nThe model consists of 5 layers: one input, three hidden and one output.\nInput and output layers contain 784 neurons each (the shape of our data, i.e number of columns), with the size of hidden layers reduced to 16 neurons each.\nWe will train the model over 50 epochs and plot a loss chart (see below).\nWe will separate the encoder part of the model and save it to our project directory. Note, if you are not planning to reuse the same model afterwards, you don’t need to keep a copy of it.",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-dmd",
    "href": "qmd/feature-reduction.html#sec-feat-red-dmd",
    "title": "Feature Reduction",
    "section": "Dynamic Mode Decomposition (DMD)",
    "text": "Dynamic Mode Decomposition (DMD)\n\nCombines PCA and fourier transform\nSupposed to handle time series better than PCA\n{{pydmd}}",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/forecasting-decomposition.html",
    "href": "qmd/forecasting-decomposition.html",
    "title": "Decomposition",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Forecasting",
      "Decomposition"
    ]
  },
  {
    "objectID": "qmd/forecasting-decomposition.html#sec-decomp-misc",
    "href": "qmd/forecasting-decomposition.html#sec-decomp-misc",
    "title": "Decomposition",
    "section": "",
    "text": "Also see EDA, Time Series &gt;&gt; Seasonality\n\nPresents details about Daily Seasonal Adjustment (DSA)\n\nThere are two approaches to noise reduction: filtering algorithms and smoothing algorithms. In filtering algorithms, signal points are fed sequentially, therefore only the current and the previous points are used to get rid of noise at the current point. Smoothing algorithms assume that the entire signal has been received, and all signal points, both previous and subsequent, are used to get rid of noise at the current point.\nLow Pass FIlter\n\nDampens higher frequencies in the data and allows lower frequencies to “pass” through.\n\n\nA smoother version of the original data\n\n\nHigh Pass FIlter\n\nDampens low frequencies and allows high frequencies to pass\n\n\nLooks like a series of residuals with the trend removed\n\n\nMatching Filter\n\nOriginal series with extreme changepoints\n\nMatching filter indicates the changepoints with peaks in the filtered series",
    "crumbs": [
      "Forecasting",
      "Decomposition"
    ]
  },
  {
    "objectID": "qmd/forecasting-decomposition.html#sec-decomp-saf",
    "href": "qmd/forecasting-decomposition.html#sec-decomp-saf",
    "title": "Decomposition",
    "section": "Seasonal Adjusted Forecasting",
    "text": "Seasonal Adjusted Forecasting\n\nThe seasonality is extracted from the time series using STL. The extracted seasonal time series and the seasonally adjusted time series are forecast separately. Both forecasts are then added back together to produce the final forecast.\n\nA seasonal naive method is commonly used to forecast the seasonal component.\nNot sure how you would combine the uncertainty (i.e PIs)\n\nExample\nfrom statsmodels.tsa.api import STL\nfrom sktime.forecasting.naive import NaiveForecaster\n\n# fitting the seasonal decomposition method\nseries_decomp = STL(yt, period=period).fit()\n\n# adjusting the data\nseas_adj = yt - series_decomp.seasonal\n\n# forecasting the non-seasonal part\nforecaster = make_reduction(estimator=RidgeCV(),\n                            strategy='recursive',\n                            window_length=3)\n\nforecaster.fit(seas_adj)\n\nseas_adj_pred = forecaster.predict(fh=list(range(1, 13)))\n\n# forecasting the seasonal part\nseas_forecaster = NaiveForecaster(strategy='last', sp=12)\nseas_forecaster.fit(series_decomp.seasonal)\nseas_preds = seas_forecaster.predict(fh=list(range(1, 13)))\n\n# combining the forecasts\npreds = seas_adj_pred + seas_preds",
    "crumbs": [
      "Forecasting",
      "Decomposition"
    ]
  },
  {
    "objectID": "qmd/forecasting-decomposition.html#sec-decomp-stl",
    "href": "qmd/forecasting-decomposition.html#sec-decomp-stl",
    "title": "Decomposition",
    "section": "Seasonal Trend Decomposition using LOESS (STL)",
    "text": "Seasonal Trend Decomposition using LOESS (STL)\n\nTime series get decomposed into trend-cycle (T), seasonal (S), and remainder (R) components\n\nYt = Tt + St + Rt, where t = 1,2,…,N\n\nLOESS smoothes a time series by:\n\nWeights are applied to the neighborhood of each point which depend on the distance from the point\nA polynomial regression is fit at each data point with points in the neighborhood as explanatory variables\n\nComponents are additive\nEntails two recursive procedures: inner loop and outer loop\n\nEach iteration updates the trend-cycle and seasonal components\nInner Loop is iterated until there’s a robust estimate of the trend and seasonal component\nThe outer loop is only iterated if outliers exist among the data points\nInner Loop Procedure\n\nDetrending\n\nInitially occurs by subtracting an initial trend component (?) from the original series\n\nSubseries smoothing\n\n12 monthly subseries are separated and collected\nEach is smoothed with LOESS\nRe-combined to create the initial seasonal component\n\nLow-Pass filtering of smoothed seasonal component\n\nSeasonal component passed through a 3x12x12 moving average.\nResult is again smoothed by LOESS (length = 13) in order to detect any trend-cycle in it.\n\nDetrending of smoothed subseries\n\nResult of low-pass filtering is subtracted from seasonal component in step 2 to get the final seasonal component\n\nDe-seasonalization\n\nFinal seasonal component is subtracted from the original series\n\nTrend smoothing\n\nLOESS is applied to deseasonalized series to get the final trend component\n\n\nOuter Loop procedure\n\nFinal trend and seasonal components are subtracted from the original series to get the remainder/residual series\nThe final trend and seasonal components are tested for outlier points\nA weight is calculated and used in the next iteration of the Inner Loop to down-weight the outlier points.",
    "crumbs": [
      "Forecasting",
      "Decomposition"
    ]
  },
  {
    "objectID": "qmd/forecasting-dl.html",
    "href": "qmd/forecasting-dl.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Forecasting",
      "Deep Learning"
    ]
  },
  {
    "objectID": "qmd/forecasting-dl.html#sec-fcast-dl-misc",
    "href": "qmd/forecasting-dl.html#sec-fcast-dl-misc",
    "title": "Deep Learning",
    "section": "",
    "text": "A callback is a function that performs some action during the training process\n\ne.g. saving a model after each training epoch; early stopping when a threshold is reached\nList of Keras callbacks",
    "crumbs": [
      "Forecasting",
      "Deep Learning"
    ]
  },
  {
    "objectID": "qmd/forecasting-dl.html#sec-fcast-dl-preproc",
    "href": "qmd/forecasting-dl.html#sec-fcast-dl-preproc",
    "title": "Deep Learning",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nMisc\n\nNotes from\n\nEXAMPLE py, global, keras LSTM, preprocessing - Deep Learning for Forecasting: Preprocessing and Training | by Vitor Cerqueira | Mar, 2023 | Towards Data Science\n\n\nScale by the mean\n\nFor global forecasting, this brings all series into a common value range. Therefore, the scale of the values won’t be a factor in model training.\nExample: global forecasting\nfrom sklearn.model_selection import train_test_split\n\n# leaving last 20% of observations for testing\ntrain, test = train_test_split(data, test_size=0.2, shuffle=False)\n\n# computing the average of each series in the training set\nmean_by_series = train.mean()\n\n# mean-scaling: dividing each series by its mean value\ntrain_scaled = train / mean_by_series\ntest_scaled = test / mean_by_series\n\nLogging\n\nLog series after scaling transformation\nHandles heterskedacity\nCan create more compact ranges, which then enables more efficient neural network training\nHelps avoid saturation areas of the neural network.\n\nSaturation occurs when the neural network becomes insensitive to different inputs. This hampers the learning process, leading to a poor model.\n\nExample\nimport numpy as np\n\nclass LogTransformation:\n\n    @staticmethod\n    def transform(x):\n        xt = np.sign(x) * np.log(np.abs(x) + 1)\n\n        return xt\n\n    @staticmethod\n    def inverse_transform(xt):\n        x = np.sign(xt) * (np.exp(np.abs(xt)) - 1)\n\n        return x\n\n# log transformation\ntrain_scaled_log = LogTransformation.transform(train_scaled)\ntest_scaled_log = LogTransformation.transform(test_scaled)\n\nCreate a matrix with lags and leads\n\nExample: Univariate\n\n# src module here: https://github.com/vcerqueira/blog/tree/main/src\nfrom src.tde import time_delay_embedding\n\n\n# using 3 lags as explanatory variables\nN_LAGS = 3\n# forecasting the next 2 values\nHORIZON = 2\n\n# using a sliding window method called time delay embedding\nX, Y = time_delay_embedding(series, n_lags=N_LAGS, horizon=HORIZON, return_Xy=True)\n\n“Target variables” are lead variables\n\nExample: Global\n\n# src module here: https://github.com/vcerqueira/blog/tree/main/src\nfrom src.tde import time_delay_embedding\n\n\nN_FEATURES = 1 # time series is univariate\nN_LAGS = 3 # number of lags\nHORIZON = 2 # forecasting horizon\n\n\n# transforming time series for supervised learning\ntrain_by_series, test_by_series = {}, {}\n\n# iterating over each time series\nfor col in data:\n    train_series = train_scaled_log[col]\n    test_series = test_scaled_log[col]\n\n    train_series.name = 'Series'\n    test_series.name = 'Series'\n\n    # creating observations using a sliding window method\n    train_df = time_delay_embedding(train_series, n_lags=N_LAGS, horizon=HORIZON)\n    test_df = time_delay_embedding(test_series, n_lags=N_LAGS, horizon=HORIZON)\n\n    train_by_series[col] = train_df\n    test_by_series[col] = test_df\n\ntrain_df = pd.concat(train_by_series, axis=0) # combine data row-wise",
    "crumbs": [
      "Forecasting",
      "Deep Learning"
    ]
  },
  {
    "objectID": "qmd/forecasting-dl.html#sec-fcast-dl-ff",
    "href": "qmd/forecasting-dl.html#sec-fcast-dl-ff",
    "title": "Deep Learning",
    "section": "Feed-Forward",
    "text": "Feed-Forward\n\nFrom Hyndman paper on Local vs Global modeling, Principles and Algorithms for Forecasting Groups of Time Series:Locality and Globality\n\nDeep Network Autoregressive (Keras):\n\nReLu MLP with 5 layers, each of 32 units width\nLinear activation in the final layer\nAdam optimizer with default learning rate.\nEarly stopping on a cross-validation set at 15% of the dataset\nBatch size is set to 1024 for speed\nLoss function is the mean absolute error (MAE).",
    "crumbs": [
      "Forecasting",
      "Deep Learning"
    ]
  },
  {
    "objectID": "qmd/forecasting-dl.html#sec-fcast-dl-lstm",
    "href": "qmd/forecasting-dl.html#sec-fcast-dl-lstm",
    "title": "Deep Learning",
    "section": "LSTM",
    "text": "LSTM\n\nExtensions\n\nCNN-LSTM - utilizes the CNN layers to improve the feature extraction before sequence prediction by the LSTM\nAutoregressive LSTM (AR-LSTM) -takes in n timesteps worth of data for a product and then makes a prediction for the n+1 week. The prediction for the n+1 week is then used to generate the features as input for the n+2 th week’s prediction GRU - bi-directional model - In NLP, it uses the preceding value and a succeeding value to predict the middle value. In forecasting, the preceding value is used as a substitute for the succeeding value \n\nExample: You have a sequence 15,20,22,24 and you want to predict the next value\n\nOne GRU which takes the input 15,20,22,24 often called the forward GRU.\n\nThis input sequence of the forward model is often called the forward context\n\nThen you use another representation of the same sequence in reverse order i.e. 24,22,20 and 15 which is used by another GRU called the backward GRU.\n\nThis input sequence of the forward model is often called the backward context\n\nThe final prediction is a function of the prediction of both the GRUs.\n\nGRU Extension: “bi-directional model of forecasting with truly bi-directional features” (BD-BLSTM) (article)\n\nBasically adds seasonality to the GRU\n\nExample: Forecasting the value for May 24th 8:00am\n\nForward GRU: 07:00 AM, 07:15 AM, and 07:45 AM values from 24th May AND 07:00 AM, 07:15 AM, 07:45 AM values from 23rd May\nBackward GRU: 08:15 AM, 08:30 AM and 08:45 AM values from 23rd May",
    "crumbs": [
      "Forecasting",
      "Deep Learning"
    ]
  },
  {
    "objectID": "qmd/forecasting-dl.html#sec-fcast-dl-rnn",
    "href": "qmd/forecasting-dl.html#sec-fcast-dl-rnn",
    "title": "Deep Learning",
    "section": "RNN",
    "text": "RNN\n\nEssentially a bunch of neural nets stacked on top of each other\noverly simplistic in their assumptions about what should be passed to the next hidden layer\n\nLong Short Term Memory (LSTM) and Gate Recurring Units (GRU) layers provide filters for what information get’s passed down the chain\n\nExample\n\n\nx’s in blue are predictor variables\nh’s in yellow are hidden layers\ny’s in green are predicted values\noutput of the model at h1 feeds into the next model at h2, etc.\n\nNot sure if he output is y1 or some kind of embedding from h1 that feeds into h2\n\nI think each predictor variable",
    "crumbs": [
      "Forecasting",
      "Deep Learning"
    ]
  },
  {
    "objectID": "qmd/forecasting-dl.html#transformers",
    "href": "qmd/forecasting-dl.html#transformers",
    "title": "Deep Learning",
    "section": "Transformers",
    "text": "Transformers\n\nMisc\n\nI think most of the research is in using these as forecast models themselves and not necessarily as a categorical/discrete feature transformation\nNotes from\n\nTransformers in Time Series: A Survey\n\nAttention heads -  enable the Transformer to learn relationships between a time step and every other time step in the input sequence\n\nArchitecture Comparisons\n\nRNNs implement sequential processing: The input (let’s say sentences) is processed word by word.\nTransformers use non-sequential processing: Sentences are processed as a whole, rather than word by word\nThe LSTM requires 8 time-steps to process the sentences, while BERT requires only 2\n\nBERT is better able to take advantage of parallelism, provided by modern GPU acceleration\n\nRNNs are forced to compress their learned representation of the input sequence into a single state vector before moving to future tokens.\nLSTMs solved the vanishing gradient issue that vanilla RNNs suffer from, but they are still prone to exploding gradients. Thus, they are struggling with longer dependencies\nTransformers, on the other hand, have much higher bandwidth. For example, in the Encoder-Decoder Transformer model, the Decoder can directly attend to every token in the input sequence, including the already decoded.\nTransformers use a special case called Self-Attention: This mechanism allows each word in the input to reference every other word in the input.\nTransformers can use large Attention windows (e.g. 512, 1048). Hence, they are very effective at capturing contextual information in sequential data over long ranges.\n\nIssues\n\nThe initial BERT model has a limit of 512 tokens. The naive approach to addressing this issue is to truncate the input sentences.\n\nAlternatively, we can create Transformer Models that surpass that limit, making it up to 4096 tokens. However, the cost of self-attention is quadratic with respect to the sentence length.\n\n\nRobustness and Model Size\n\n\nRobustness: As the time series gets longer (aka Input Len), Autoformer holds it’s performance best\n\nThe rest start to crumble after the length gets past 192 time points\n\nModel Size: Autoformer is again the best and holds its performance pretty much up through 24 layers\nHyper-parameters:\n\nembedding dimension\nnumber of heads\nnumber of layers\n\nIn general, 3-6 layers yields the best performance\nMore layers typically adds more accuracy. In NLP and Computer Vision (CV), their models are usually 12 to 128 layers, so this will be an area of improvement.\n\n\nAutoformer Wu et al., 2021 has a moving average trend decomposition component that can be added to other transformer architectures to greatly enhance performance\n\n\nPatchTST (paper, article)\n\nMisc\n\nPackages\n\n{{neuralforecast}} - NIXLA collection of neural forecasting models\n\n\nPatched Attention: Their attention takes in large parts of the time series as tokens instead of a point-wise attention\n\nPrevious Architectures\n\nSelf-Attention treats each timestamp treated as a token\n\nIssues\n\nPermutation-Invariance — where the same attention values would be observed if you flipped the points around (?)\nEach timestamp doesn’t have a lot of information in it and gets its importance from the timestamps around it. So treating each timestamp as a token is like tokenizing a character instead of a word.\n\nResults\n\nOverfitting: adding noise didn’t significantly decrease transformer performance\nLonger lookback periods didn’t increase accuracy. Meaning significant temporal patterns weren’t recognized\n\n\n\nPatchTST\n\nSplit each input time series up into fixed-length “patches” (i.e. windows).\nThese patches are then passed through dedicated channels as the input tokens to the main model (the length of the patch is the token size).\nThe model then adds a positional encoding to each patch and runs it through a vanilla transformer encoder.\nBenefits\n\nTakes advantage of local semantic information (i.e. window of timestamps)\nFewer input tokens needed allowing the model to capture info from longer sequences and dramatically reducing the memory required to train and predict\nMakes it viable to do Representational Learning (?)\n\n\n\nChannel Independence: different target series in a time series are processed independently of each other with different attention weights.\n\nPrevious Architectures\n\nAll target time series would be concatenated together into a matrix where each row of the matrix is a single series and the columns are the input tokens (one for each timestamp).\nThese input tokens would then be projected into the embedding space, and these embeddings were passed into a single attention layer\n\nPatchTST\n\nEach target series is passed independently into the transformer backbone\nTherefore, every series has its own set of attention weights, allowing the model to specialize better.\n\nSeems like the opposite rationalization of global model forecasting (See Forecasting, Hierarchical/Grouped &gt;&gt; Global\n\n\n\nBenchmarks\n\n\nTwo variants: 64 “patches” and 42 “patches”\n\n42 patch variant has the same lookback window as the other models\nBoth variants have a patch length of 16 and a stride of 8 were used to construct the input tokens\n\nOn average, PatchTST/64 achieved a 21% reduction in MSE and a 16.7% reduction in MAE. PatchTST/42 achieved a 20.2% reduction in MSE and a 16.4% reduction in MAE.",
    "crumbs": [
      "Forecasting",
      "Deep Learning"
    ]
  },
  {
    "objectID": "qmd/forecasting-ensembling.html",
    "href": "qmd/forecasting-ensembling.html",
    "title": "Ensembling",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Forecasting",
      "Ensembling"
    ]
  },
  {
    "objectID": "qmd/forecasting-ensembling.html#sec-fcast-ensemb-misc",
    "href": "qmd/forecasting-ensembling.html#sec-fcast-ensemb-misc",
    "title": "Ensembling",
    "section": "",
    "text": "Statistical ensemble nearly as good as a DL ensemble and was much faster and cheaper. (Thread)\n\n4 models used in the ensemble\n\nAutoARIMA and Exponential Smoothing ({forecast})\nComplex Exponential Smoothing ({smooth})\nDynamically Optimized Theta method ({{StatsForecast}})",
    "crumbs": [
      "Forecasting",
      "Ensembling"
    ]
  },
  {
    "objectID": "qmd/forecasting-ensembling.html#sec-fcast-ensemb-diag",
    "href": "qmd/forecasting-ensembling.html#sec-fcast-ensemb-diag",
    "title": "Ensembling",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nResidual Testing\n\nHyndman suggests taking average degrees of freedom (dof) of the models included in the ensemble to use for residual tests (?)",
    "crumbs": [
      "Forecasting",
      "Ensembling"
    ]
  },
  {
    "objectID": "qmd/forecasting-ensembling.html#sec-fcast-ensemb-avg",
    "href": "qmd/forecasting-ensembling.html#sec-fcast-ensemb-avg",
    "title": "Ensembling",
    "section": "Averaging (Equal Weighting)",
    "text": "Averaging (Equal Weighting)\n\nCDC ensemble: https://github.com/reichlab/covid19-forecast-hub#ensemble-model\n\nEach model has a forecast distribution for each horizon\n\n“23 quantiles to be submitted for each of the one through four week ahead values for forecasts of deaths, and a full set of 7 quantiles for the one through four week ahead values for forecasts of cases (see technical README for details), and that the 10th quantile of the predictive distribution for a 1 week ahead forecast of cumulative deaths is not below the most recently observed data.”\n\nUsed the median prediction across all eligible models at each quantile level",
    "crumbs": [
      "Forecasting",
      "Ensembling"
    ]
  },
  {
    "objectID": "qmd/forecasting-ensembling.html#sec-fcast-ensemb-adensmb",
    "href": "qmd/forecasting-ensembling.html#sec-fcast-ensemb-adensmb",
    "title": "Ensembling",
    "section": "Adaptive Ensembling",
    "text": "Adaptive Ensembling\n\nMisc\n\nVideo, Github (Amaal Saadallah)\n\nShe also has paper that uses Reinforcement Learning with actor-critic approach to compute weights, EADRL\n\n\nOverview\n\nSteps\n\nDrift Detection\n\nDrift from stationarity detected which triggers a re-selection of base models and ensemble\nCalculating drift\n\nSelect a testing window with most recent data\nCalculate predictions using each base model from the ensemble for this testing window\nCalculate scaled root correlation (SRC) between each model’s predictions and the testing window observations\n\\[\n\\text{SRC}(\\hat Y_{W,t}^{M_i}, Y_{M,t}) = \\sqrt{\\frac{1-\\operatorname{corr}(\\hat Y_{W,t}^{M_i}, Y_{W,t})}{2}}\n\\]\n\n\\(\\hat Y\\) is the predictions for model, \\(M_i\\), and testing window \\(W,t\\).\n\\(Y\\) is testing window observations for testing window \\(W,t\\)\n\\(\\text{corr}\\) is the pearson correlation\n\nHoeffding Bound\n\\[\n\\epsilon_F = \\sqrt{\\frac{R^2 \\log(1/\\delta)}{2W}}\n\\]\n\n\\(R\\) is the range of the random variable\n\\(W\\) is the number of observations of the testing window used to calculate the \\(\\text{SRC}\\)\n\\(\\delta\\) is a user-defined hyperparameter\n\n\n\nPrune base models\n\nCV\nRemove the awful ones\n\nCluster pruned set of models based on predictions\n\nBy selecting a model from each cluster, you get a diversified set of models for the ensemble\n\nBy reducing covariance between models in your ensemble, it reduces the variance in the ensemble’s residuals (my notes)\n\nThe representative model from each cluster is selected for the ensemble\nSuggests using an Improper Maximum Likelihood Estimator Clustering (IMLEC) method\n\n{otrimle}\n\nHyperparameters automatically tuned; Outliers removed\nSeems to be a robust gaussian mixture clustering algorithm\nHas links to paper, Coretto and Hennig, 2016\n\nSays Euclidean distance is bad. This method uses covariance to form clusters which fits with trying to get a set of time series with maximum variance for the ensemble\nSince this is a GMM you could have models belonging to more than 1 cluster I assume.\n\nIf 1 model is chosen as representative for more than 1 cluster than probably wouldn’t matter too much. Maybe less redundancy.\nif 1 model is in more than 1 cluster but not chosen as representative in both, it might be interesting to know that. Maybe means there’s some redundancy in your ensemble but not too much. Might want to score an ensemble with only 1 of cluster representatives and see if it’s better.\n\n\nShe has a DTW option but seems to prefer this IMLEC method\n\nShowed some RMSE stats where the dtw method had a 50 RMSE and IMLEC had a 43\nUsed partitioning for a clustering method but no tuning was involved\n\n\nCombine models predictions\n\nsliding window average, averaging, stacking, metalearner",
    "crumbs": [
      "Forecasting",
      "Ensembling"
    ]
  },
  {
    "objectID": "qmd/forecasting-hierarchical_grouped.html",
    "href": "qmd/forecasting-hierarchical_grouped.html",
    "title": "Hierarchical/Grouped",
    "section": "",
    "text": "Modeltime",
    "crumbs": [
      "Forecasting",
      "Hierarchical/Grouped"
    ]
  },
  {
    "objectID": "qmd/forecasting-hierarchical_grouped.html#sec-fcast-group-mdltm",
    "href": "qmd/forecasting-hierarchical_grouped.html#sec-fcast-group-mdltm",
    "title": "Hierarchical/Grouped",
    "section": "",
    "text": "Misc\n\nNotes from Learning Lab 50\nHierarchical\n\nData in long format\nAll hierarchical levels were forecasted at once\nFeature engineering\n\nSeparate column for hierarchical categories  and they were dummied\nSeparate column for the levels of the hierarchical  categories and they were dummied\nNo real explicit sequencing variable like a date column used\n\nrow_id and date were designated as ids in recipe step\n\ne.g. update_role(row_id, date, new_roll = “id”)\n\n\n\ndate features were added\n\nrolling means\nmonths, days of week, etc\n\n\nGrouped (local modeling)\n\nA group variable is used to group_nest the datasets and modeling is done on each group’s dataset.\n\n\nExample: Local Modeling\n\nData preparation\nlibrary(modeltime)\nnested_data_tbl &lt;- data_tbl %&gt;%\n\n# 1. Extending: We'll predict 52 weeks into the future.\nextend_timeseries(\n    .id_var        = id,\n    .date_var      = date,\n    .length_future = 52\n) %&gt;%\n\n# 2. Nesting: We'll group by id, and create a future dataset\n#    that forecasts 52 weeks of extended data and\n#    an actual dataset that contains 104 weeks (2-years of data)\n    nest_timeseries(\n        .id_var        = id,\n        .length_future = 52,\n        .length_actual = 52*2\n    ) %&gt;%\n\n# 3. Splitting: We'll take the actual data and create splits\n  #    for accuracy and confidence interval estimation of 52 weeks (test)\n  #    and the rest is training data\n    split_nested_timeseries(\n        .length_test = 52\n    )\n\nCreate NA values for 52 weeks into the future to later fill with forecasts\nCreate the nested (i.e. grouped) data structure\nCreate train/test splits of the nested (i.e. grouped) structure\n\nid - grouping var\n.actual_data - all data\n.future_data - NAs to filled with forecasts\n.splits - train/test splits\n\n\nSpecify workflow objects\n# prophet\nrec_prophet &lt;- recipe(value ~ date, extract_nested_train_split(nested_data_tbl, 1)) \nwflw_prophet &lt;- workflow() %&gt;%\n  add_model(\n    prophet_reg(\"regression\", seasonality_yearly = TRUE) %&gt;% \n      set_engine(\"prophet\")\n  ) %&gt;%\n  add_recipe(rec_prophet)\n\n# xgb           \nrec_xgb &lt;- recipe(value ~ ., extract_nested_train_split(nested_data_tbl, 1)) %&gt;%\n    step_timeseries_signature(date) %&gt;%\n        step_rm(date) %&gt;%\n        step_zv(all_predictors()) %&gt;%\n        step_dummy(all_nominal_predictors(), one_hot = TRUE)\n    wflw_xgb &lt;- workflow() %&gt;%\n        add_model(\n            boost_tree(\"regression\") %&gt;%\n            set_engine(\"xgboost\")\n        ) %&gt;%\n        add_recipe(rec_xgb)\n\nrecipe: notice .splits[[1]] (training split) is used in the recipe step\n\nTrain, Test, and Forecast\n\nTrain and test\nnested_modeltime_tbl &lt;- \n    modeltime_nested_fit(\n\n        # Nested data \n        nested_data = nested_data_tbl,   \n\n        # Add workflows\n        wflw_prophet,\n        wflw_xgb,\n        control = control_nested_fit(allow_par = TRUE)\n    )\nAssess performance\nnested_modeltime_tbl %&gt;% \n    extract_nested_test_accuracy() %&gt;%\n    table_modeltime_accuracy(.interactive = F)\nDisplays table of forecasting metrics\nnested_modeltime_tbl %&gt;% \n    extract_nested_test_forecast() %&gt;%\n    group_by(id) %&gt;%\n    plot_modeltime_forecast(\n        .facet_ncol  = 2,\n        .interactive = FALSE\n      )\n\nfacetted line charts for each group showing forecasts vs the test set.\n\nShow any errors that occurred during modeling\nnested_modeltime_tbl %&gt;% \n      extract_nested_error_report()\nSelect model\nbest_nested_modeltime_tbl &lt;- \n    nested_modeltime_tbl %&gt;%\n        modeltime_nested_select_best(\n            metric                = \"rmse\", \n            minimize              = TRUE, \n            filter_test_forecasts = TRUE\n        )\n\nmetrics: rmse (default), mae, mape, mase, smape, rsq\nminimize: If TRUE, says select model with the lowest score\nfilter_test_forecasts: If TRUE, says only keep the best test forecasts for each group logged\n\nYou can access these test set forecasts using extract_nested_test_forecast()\n\n\nPerformance metrics of best models\nbest_nested_modeltime_tbl %&gt;%\n    extract_nested_best_model_report()\n\nmetrics for the best model of each group’s test set\n\nVisualize test set forecasts\nbest_nested_modeltime_tbl %&gt;%\n    extract_nested_test_forecast() %&gt;%\n        group_by(id) %&gt;%\n        plot_modeltime_forecast(\n        .facet_ncol  = 2,\n        .interactive = FALSE\n        )\n\nfacetted line charts showing test set forecasts for each group\n\nForecast\nnested_modeltime_refit_tbl &lt;- \n    best_nested_modeltime_tbl %&gt;%\n        modeltime_nested_refit(\n            control = control_nested_refit(verbose = TRUE, allow_par = TRUE)\n        )\n\nFits selected models on each group’s whole dataset and forecasts\n\nAccess Forecasts and plot them\n\nnested_modeltime_refit_tbl %&gt;%\n    extract_nested_future_forecast() %&gt;%\n        group_by(id) %&gt;%\n        plot_modeltime_forecast(\n            .interactive = FALSE,\n            .facet_ncol  = 2\n        )",
    "crumbs": [
      "Forecasting",
      "Hierarchical/Grouped"
    ]
  },
  {
    "objectID": "qmd/forecasting-hierarchical_grouped.html#sec-fcast-group-global",
    "href": "qmd/forecasting-hierarchical_grouped.html#sec-fcast-group-global",
    "title": "Hierarchical/Grouped",
    "section": "Global Modeling",
    "text": "Global Modeling\n\nMisc\n\nfrom Why aren’t you getting the most out of your Marketing AI\n\n“Consider the consumer goods company whose data scientists proudly announced that they’d increased the accuracy of a new sales-volume forecasting system, reducing the error rate from 25% to 17%. Unfortunately, in improving the system’s overall accuracy, they increased its precision with low-margin products while reducing its accuracy with high-margin products. Because the cost of underestimating demand for the high-margin offerings substantially outweighed the value of correctly forecasting demand for the low-margin ones, profits fell when the company implemented the new,”more accurate” system.”\nThis should be something to be aware of and monitored with global forecasting model. Does the optimization of single function unintentionally optimize the error of a series with less financial importance over a more financially important series in order to minimize global error? Where in classification models, error metrics and loss functions are chosen by the cost/benefit of TP, FP, TN, FN.  Can a loss function be developed that tunes a global model and weights errors by group series?\n\n\nFrom paper, Principles and Algorithms for Forecasting Groups of Time Series:Locality and Globality More detailed (raw) notes, Principles and Algorithms for Forecasting Groups of Time Series: Locality and Globality\n\nPaper tested global vs local methods on groups of series across many datasets (thousands of series).\n\nglobal models almost always had better generalization error. (ge = training set score - test set score)\n\nWhile local models may have better in-sample performance, the low generalization error of global models makes its CV score a truer representation of out-of-sample than with local methods. This should result in better production models since our algorithm selection will be better.\n\nglobal models almost always outperformed local methods on most homogeneous and heterogenous groups of series\n\nFinds that for global methods, the more complex the model (e.g. OLS –&gt; ML –&gt; DL), the better the forecasting\n\nUsing a global method –&gt; larger dataset –&gt; more degrees of freedom –&gt; suitable conditions for more complex models to perform well\n\nDancho, global example using modeltime, makes point that global models perform better on group dataset as a whole but not necessarily for every individual series in the dataset.\n\nmodeltime_accuracy(acc_by_id = T) will measure the accuracy per time series.\nOptions\n\nMight be better to remove worse performing series and just forecast them using a local method\nMore likely it’s better to keep those series in the global method since they probably adds predictive value to the other series in the dataset and additionaly forecast those worse performing series with local method\n\n\nIf we want to estimate a complex pattern in a series and that series has sufficient sample size, then we are better off with local models.\nTerms\n\nlocal method - fitting each series separately with a different model\nglobal method - one model for the entire set of series\n\nAssumptions\n\nGroups of independent time series\n\nEach series in the group should evolve by similar patterns but independently of each other\n\nThinking this assumption can be somewhat relaxed and still achieve superior results vs local models. (see pedestrian data example below)\nPaper lists many popular datasets that can be tested for independence to see how strict this assumption needs to be.\n\nUnder heavy dependence global models loose their beneficial performance guarantees (though practicioners could still try them)\n\n\nExample\n\nEach time series in the group represents the hourly pedestrian count of a different location across Melbourne’s CBD (city bar district: entertainment area)\n\nEach location in this district is different series.\nThe domain that allows for the “same pattern of evolution” would be that each series is measuring pedestrian activity.\nSince they’re at different locations, the counts are independent of each other.\n\n(I guess… if this is a large area and the same pedestrians are being counted. Maybe one group is goint to a concert, another is going to a show, and another is bar hopping, etc.)\n\n\n\nGlobal Method Steps:\n\nFix an AR order (i.e. number of lags)\nCreate lags according to that AR order for each series\nFor each series, embed the observed series and each lag into a matrix\nStack these matrices on top of each other to create one large matrix\nRun CV, using the large matrix, to determine the best lag order\nForecast using the large matrix.\n\nLags\n\nGlobal models should have large memories (i.e. large number of lags)\n\nLag order (i.e. max lag) should usually be determined by the length of the shortest time series in the dataset.\n\nOptimal lag may be in the 100s\n\nNotable exception to the pattern of “longer memory, better accuracy” is intermittent data where smaller lag orders work better for global models.\n\nMake sure the lag order is NOT a multiple of seasonal/cyclical frequency of each series\n\nPreferrably the lag order should be greater than the largest seasonal/cyclical frequency of the group as there is usually an improvement in error.\n\nThis effect is explained as an over-fitting phenomena related to the heterogeneity of seasonal strengths in the datasets. In those datasets that contain both strongly seasonal and non seasonal series (or different seasonal periods), a global model tends to favor the seasonality at memory levels that are multiples of the frequency because it produces good results in-sample. When the memory is over the seasonal period, the global model can use its coefficients to fit both seasonal and non seasonal patterns\n\n\nPolynomial Autoregression (AR) models exponentiate the lags. So the design matrix includes the lags and the exponentiated series.\n\nIf the polynomial is order 3, then order 2 is also included. So, now, the design matrix would be the lags, the square of each lag, and the cube of each lag\n\n\nPreprocessing\n\nProbably should standardize or at least scale\n\nFor ML/DL models, standardization of the series had a positive, minor effect (lm models are scale invariant)\nIf any of the series has a meaningful maximum or minimums at 1 or 0 respectively, then you maybe just want to scale.\nAlso possible to add a scale variable to the matrix so that even if you standardize or scale, that info is still available to the model.\nWithout standardization and depending on the error measure that you use, prediction errors on series that are on larger scales may have an outsized effect on the overall error calculation\nScaling can help deal with outlier series (e.g. one series has much larger scale than the rest, dominating the fitting) and in the case of deep networks, it makes them faster to train\n\nScaling by MASE and using MASE as the loss function is equivalent to to minimizing the MAE in the preprocessed time series.\n\n“scaling done by the MASE can be applied to data as a preprocessing normalizing step, which we apply to each series. Then minimizing the MASE is equivalent to minimizing the mean absolute error in the preprocessed time series. Compared to SMAPE, MASE is easier to fit with off-the-shelf optimizers (it is convex), and it therefore isolates better the effect of the model class rather than the optimization procedure used to fit it”\n\n\n\nIndex variable not needed.\n\ntidymodels has a recipe step to where you can include an index variable but give it role = id. It won’t be used as a predictor but could help with separating out predictons later on.\n\n\nSeasonality\n\nDon’t remove the seasonality from the data\nAutomatically discovered, even by a global AR model, and even though it’s not part of the model specification.\nFor DL/ML models\n\nadding seasonality features may help but is not required for seasonal time series\nadding tsfeatures (ML/DL) and using larger input window sizes (DL) or sufficiently large AR orders (ML) should be enough to account for seasonality\n\n\nPartitioning heterogeneous time series\n\nheterogeneous: different domains (measured in different units) and with different frequencies (from hourly to yearly measurements), patterns, trends, etc.\nFor heterogeneous series, potentially more accuracy with partitioning the series by frequency and/or domains before fitting sub(?)-global models\n\nPartioning (or clustering) reduces sample size though. So something to keep an eye on.\n\nWithout partitioning, DL global models with tsfeatures + lags + frequency variables (fourier or splines?) outperformed partitioned models in the paper\n\nDeep Network Autoregressive (Keras):\n\nReLu MLP with 5 layers, each of 32 units width\nLinear activation in the final layer\nAdam optimizer with default learning rate.\nEarly stopping on a cross-validation set at 15% of the dataset\nBatch size is set to 1024 for speed\nLoss function is the mean absolute error (MAE).\n\n\nPartitioning and clustering\n\nBoth can potentially be beneficial\nUsing clustering adds a potentially expensive processing step, plus a sufficiently complex model (e.g. the DL model w/additional features above) should detect whatever additional information that clustering is likely to provide.\n\n\nPolynomial Regression\n\nBad to use for a local model but pretty good as a global model\n\nLocal Model: Recursive forecasting makes polynomials numerically unstable (e.g. an 18-step-ahead forecast for this dataset is equivalent to a 54 degree polynomial) and are not recommended in the literature for automatic time series forecasting.\nGlobal Model: 2nd order polynomial fit would’ve placed 2nd in M4 (yearly ts) and the DL global model would’ve placed 3rd for the Quarterly data.\n\nAccomplished by squaring the lags of the series and adding them as extra variables to the large matrix.\n\n\nPolynomial Autoregression (AR) models exponentiate the lags. So the design matrix includes the lags and the exponentiated series.\n\nIf the polynomial is order 3, then order 2 is also included. So, now, the design matrix would be the lags, the square of each lag, and the cube of each lag\nlibrary(dplyr); library(timetk)\n# tbl w/polynomial design matrix of order 3 (along with original series, group variable, and date variable)\n# value is the ts values (original series)\npoly_tbl &lt;- group_tbl %&gt;% \n  tk_augment_lags(.value = value, .lags = 1:4) %&gt;%   \n  mutate(across(contains(\"lag\"), .fns = list(~.x^2, ~.x^3), .names = \"{.col}_{ifelse(.fn==1, 'quad','cube')\"))\n\n.fn  is the item number in the .fns list.\nsquared lag 2 will have the name “value_lag2_quad”\n\n\n\nMultivariate Time Series (see section in Forecasting, Multivariate)\n\nInterdependent series (think VAR models)\nExample: Energy consumption per household and income per household time series (i.e. energy consumption and income are interdependent) represent a multivariate process\n\nDataset has households sampled all around the planet. The pair of time series for each household does not necessary affect any of the other households, but we could assume that they follow the same pattern.\n\n“Sampled from around the planet” allows us to reasonably assume these pairs of time series are independent.",
    "crumbs": [
      "Forecasting",
      "Hierarchical/Grouped"
    ]
  },
  {
    "objectID": "qmd/forecasting-ml.html",
    "href": "qmd/forecasting-ml.html",
    "title": "ML",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Forecasting",
      "ML"
    ]
  },
  {
    "objectID": "qmd/forecasting-ml.html#sec-fcast-ml-misc",
    "href": "qmd/forecasting-ml.html#sec-fcast-ml-misc",
    "title": "ML",
    "section": "",
    "text": "Notes from\n\nLearnings from Kaggle Forecasting Competitions: The Walmart Competition\n\nDeep networks suffer from the problem of instability for recursive forecasting, and it’s recommended to use direct forecasting\nTree based models can only predict within the range of training data.\nModels seen used\n\nEnsemble combination: KNN, random forest and principal components regression together with ARIMA, Unobserved Components Model and linear regression\n\nConditions in which these models perform poorly\n\nUnder 500 obs (need to reread the paper to get this more exact). Think at around this number, the ML models start to catch the statistical models\nAdditional explanatory variables have poor predictive power\nTime Series has high seasonality\n\nAlways fit a statistical model for comparison\nAlgorithm Specifications\n\nFrom Hyndman paper on Local vs Global modeling, Principles and Algorithms for Forecasting Groups of Time Series:Locality and Globality\n\nXGboost,\n\nsubsampling = 0.5\ncol_sampling = 0.5\nEarly stopping on a cross-validation set at 15% of the dataset.\nLoss function is RMSE\nValidation error is MAE\n\n\nLightGBM\n\nlinear_tree fits a piecewise linear model for each leaf. Helps to extrapolate linear trends in forecasting\n\nSeems to act like a basic distribution forest",
    "crumbs": [
      "Forecasting",
      "ML"
    ]
  },
  {
    "objectID": "qmd/forecasting-ml.html#sec-fcast-ml-terms",
    "href": "qmd/forecasting-ml.html#sec-fcast-ml-terms",
    "title": "ML",
    "section": "Terms",
    "text": "Terms\n\nShort-Term Lags: Lags &lt; Forecast Horizon.\nDirect Forecasting: Modeling each horizon separately",
    "crumbs": [
      "Forecasting",
      "ML"
    ]
  },
  {
    "objectID": "qmd/forecasting-ml.html#sec-fcast-ml-preproc",
    "href": "qmd/forecasting-ml.html#sec-fcast-ml-preproc",
    "title": "ML",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nDifference until stationary\n\nSince tree models can’t predict outside the range of the their training data, trend must be removed.\nIn addition to the deterministic trend, this approach also removes stochastic trends.\nForecasts will need to be back-transformed\n\\[\n\\hat y_{t+1} = \\hat \\Delta y_{t+1} + y_t\n\\]\n\nForecast = Differenced Forecast + Previous Value\nThen recursively for the next forecasts in the horizon\n\\[\n\\hat y_{t+h} = \\hat \\Delta y_{t+h} + \\hat y_{t+h-1}\n\\]\n\n\nLog transform\n\nForecasts need to be back-transformed\n\nTarget encode categorical features then lag them\nScale series\nRemove time stamp after creating date features\n\nDate features will help keep track of time\n\nThese will need to be one-hot encoded or some other discrete/categorical transformation",
    "crumbs": [
      "Forecasting",
      "ML"
    ]
  },
  {
    "objectID": "qmd/forecasting-ml.html#sec-fcast-ml-mstep",
    "href": "qmd/forecasting-ml.html#sec-fcast-ml-mstep",
    "title": "ML",
    "section": "Multi-Step Forecasting",
    "text": "Multi-Step Forecasting\n\nMisc\n\nNotes from: 6 Methods for Multi-step Forecasting\n\nExample dataset\n\n\nt-3 through t-0 are the predictors\nt+1 through t+4 are potential outcome variables\n\n\n\nRecursive (aka Iterative) - Training a single model for one-step ahead forecasting. Then, the model’s one-step ahead forecast is used as data to get the 2nd-step ahead forecast.\n\nThe one-step ahead forecast is NOT added to the training data and the model refitted. It replaces the 0th lag value in the training data, and the previous 0th lag data becomes the 1st lag data, etc. The final lag (e.g t-3 in the example data) that was used in training the model is discarded. Therefore, the same number of lags that was used in training the model remains the same. This updated dataset becomes “new data” and along with the original model are used as input for predict\nThis process is iterated using previous forecasts to get predictions for multiple steps ahead\nMethod leads to propagation of errors\n\nDirect - Builds one model for each horizon\n\nEach model trains on a lead of the outcome variable that matches the horizon step\n\ne.g. The model forecasting the 2nd step-ahead value will train with an outcome variable that is t+2\n\nNo previous forecast is used to predict a forecast ahead of it.\nAssumes that each horizon is independent which is usually false\nExample\nfrom sklearn.multioutput import MultiOutputRegressor\ndirect = MultiOutputRegressor(LinearRegression())\ndirect.fit(X_tr, Y_tr)\ndirect.predict(X_ts)\n\nY_tr contains lead variables for each step in the forecast horizon\n\n\nExample: Multi-step time series forecasting with XGBoost\n\nCodes a sliding-window (don’t think it’s a cv approach though) and forecasts each window with the model\nNot exactly sure how this works. No helpful figs in the article or the paper if follows, so would need to examine the code\n\nExample: Preprocessing training data for 1 then 2 steps ahead and forecasting (post, Code &gt;&gt; Time Series &gt;&gt; direct-multistep-forecasting-regression.R)\n\nThe FIRST 9 rows and the LAST 4 rows of the training data before preprocessing are shown\ny_observed is the original outcome column\ny_f is y_observed but stepped forward\nx_0 is a copy of y_observed\nx_1 to x_5 are lags of y_observed\n1-Step Ahead Training Data\n\nBefore Preprocessing\n\n\ny_f has been stepped forward 1-step\ny_observed and rows with NAs are deleted\nEven though 6 rows are deleted from the training data, all values are used for training\n\nAfter Preprocessing\n\n\nThe model is fit using this data\n\n\n2-Step Ahead Training Data\n\nBefore Preprocessing\n\n\ny_f has been stepped forward 2-steps\ny_observed and rows with NAs are deleted\nEven though 7 rows are deleted from the training data, all values are used for training\n\nAfter Preprocessing\n\n\nThe model is fit using this data\n\n\nForecast\n\n\nEach step-ahead model predicts on this same row of data which is the final row of the training data (row 110)\n\nNote that this row wasn’t used to train the model.\n\n\n\nDirect-Recursive (aka Chaining) - Combo of Recursive and Direct\n\nRecursive - Previous forecasts are used as data to get the next step-ahead forecast\nDirect - The previous forecasts are added to the training data and the model is retrained for each horizon.\nMore computationally intensive since there’s more data and a model has to trained for each horizon step.\n\nCan’t be parallelized since the process is sequential.\n\n\nData as Demonstrator (DaD) - Error correction method for the Recursive method\n\nUse the training set to correct errors that occurs during multi-step prediction to mitigate the propagation of errors that occurs in recursive forecasting.\nIteratively enriches the training set with these corrections after each recursive forecast.\ngithub\n\nDynamic Factor Machine Learning (DFML) - A feature reduction method is applied to a multivariate or multistep time series and the model trains on the components as outcome variables. Then, the forecasts are backtransformed.\n\nArticle uses the direct method\n\nSee article for code. Not sure how effective it is but the code is pretty simple\n\n\nMulti-Output - A single model which learns all the forecasting horizon jointly\n\nGoal is to capture the dependencies between forecasts better.\nOutput is a multi-step output all at once\n\nExample uses KNN but the article mentions regularized regression, rf, DL algorithms can be used\n\nThink this sounds like a multivariate approach and not a global approach\nPreprocessing should include removing the seasonality.\nSee article for paper reference\n\nHorizon as a Feature\n\nNotes from Forecast Multiple Horizons: an Example with Weather Data\nGlobal approach where design matrices for each horizon are stacked (i.e. bind_rows) on top of each other to create a global design matrix. A horizon feature variable is added (e.g. 1 for the 1-step, 2 for the 2-step, etc.) to index each sub-design matrix.",
    "crumbs": [
      "Forecasting",
      "ML"
    ]
  },
  {
    "objectID": "qmd/forecasting-ml.html#sec-fcast-ml-feateng",
    "href": "qmd/forecasting-ml.html#sec-fcast-ml-feateng",
    "title": "ML",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nAlso see Feature Engineering, Time Series\nDate time Features\n\nMinutes in a Day\nQuarter of the Year\nHour of Day\nWeek of the Year\nSeason of the Year\nWeekend or Not\nDaylight Savings or Not\nPublic Holiday or Not\nBefore or After Business Hours\nExample: py\nts_data['hour'] = [ts_data.index[i].hour for i in range(len(ts_data))]\nts_data['month'] = [ts_data.index[i].month for i in range(len(ts_data))]\nts_data['dayofweek'] = [ts_data.index[i].day for i in range(len(ts_data))]\n\nLags - Lags of the target variable\n\n{{pandas}} has a shift function\n\nWindow Statistics (See Python, Pandas &gt;&gt; Time Series &gt;&gt; Calculations &gt;&gt; Aggregation)\n\nSliding window features - At each time point, a summary of values over a fixed window of prior timesteps\nRolling window statistics - A range of values that includes the time point itself as well as some specified number of data points before and after (?) the time point used\n\n{{pandas}} has a rolling function\n\nExpanding window statistics\n\n{{pandas}} has an expanding function\nExample: minimum, mean, and maximum\n# create expanding window features \nfrom pandas import concat\nload_val = ts_data[['load']] window = load_val.expanding() \nnew_dataframe = concat([window.min(),  window.mean(), window.max(), load_val. shift(-1)], axis=1) new_dataframe.columns = ['min', 'mean', 'max', 'load+1']\nprint(new_dataframe.head(10))\n\n                    min      mean    max      load+1 \n2012-01-01 00:00:00 2,698.00 2,698.00 2,698.00 2,558.00 \n2012-01-01 01:00:00 2,558.00 2,628.00 2,698.00 2,444.00 \n2012-01-01 02:00:00 2,444.00 2,566.67 2,698.00 2,402.00 \n2012-01-01 03:00:00 2,402.00 2,525.50 2,698.00 2,403.00 \n2012-01-01 04:00:00 2,402.00 2,501.00 2,698.00 2,453.00 \n2012-01-01 05:00:00 2,402.00 2,493.00 2,698.00 2,560.00 \n2012-01-01 06:00:00 2,402.00 2,502.57 2,698.00 2,719.00 \n2012-01-01 07:00:00 2,402.00 2,529.62 2,719.00 2,916.00 \n2012-01-01 08:00:00 2,402.00 2,572.56 2,916.00 3,105.00 \n2012-01-01 09:00:00 2,402.00 2,625.80 3,105.00 3,174.00\n\n\nUse of singular value decomposition to learn specific patterns across a grouping variable (e.g. department specific patterns across stores)\n\nPerform holiday adjustments to the data\nGet the training data for a specific department into a matrix with weeks as rows and stores as columns.\nUse SVD to produce a low-rank approximation of the sales matrix. The winner reduced the rank from 45 (# stores) to between 10-15.\nReconstruct the training matrix using the low-rank approximation.\nForecast the reconstructed matrix using an STL decomposition followed by exponential smoothing of the seasonality adjusted series. This was conducted in R using the stlf function of the forecast package.\n\nExternal Features\n\nAverage Weekly Temperature by Region,\nFuel Price by Region,\nConsumer Price Index (CPI)\nUnemployment Rate.\n\nExample: From Hyndman local vs global models paper (See Algorithm Specifications section)\n\nUsed\n\n12 lags (Length of shortest series in M4)\nFrequency Features (fourier series?)\nOther Features ({tsfeatures})\n\ntsfeatures measure a bunch of ts characteristics.\nThese would be just 1 number per series. I’m not sure how you use them as a predictors\n\n\nData was a matrix stack of all the time series in the M4 competition\n\nMaybe since it’s a ts stack, the tsfeatures stuff has enough variance to useful in prediction",
    "crumbs": [
      "Forecasting",
      "ML"
    ]
  },
  {
    "objectID": "qmd/forecasting-multivariate.html",
    "href": "qmd/forecasting-multivariate.html",
    "title": "Multivariate",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Forecasting",
      "Multivariate"
    ]
  },
  {
    "objectID": "qmd/forecasting-multivariate.html#sec-fcast-multivar-misc",
    "href": "qmd/forecasting-multivariate.html#sec-fcast-multivar-misc",
    "title": "Multivariate",
    "section": "",
    "text": "Multivariate time series analysis seeks to analyze several time series jointly. The rationale behind this is the possible presence of interdependences between the different time series.\nExamples\n\nUS GDP data, S&P 500 data, and oil prices\nMortgage applications, interest rate data, and unemployment rates\nOrder flow data, price levels, and volatilities\n\nModels\n\nVector Autoregression (VAR)\n\ntypes\n\nstructural: interdependent time series affect each other contemporaneously\nreduced: interdependent time series do not affect each other contemporaneously (method commonly used)\n\n\nExponentially Weighted Moving Average (EWMA)\nBEKK(p, q)\n\n(after Baba, Engle, Kraft, and Kroner) is a multivariate extension of the GARCH\n\nDynamic Conditional Correlation (DCC)\nMatrix Autoregression (MAR)\n\nNotes from Matrix Autoregressive Model for Multidimensional Time Series Forecasting\nSame as VAR but with some advantages\n\nCan maintain the original data representation in the form of matrix.\nReduces the amount of parameters in autoregressive models.\n\nFor example, if we use VAR to explore such data, we would have (mn)² parameters in the coefficient matrix. But using MAR, we only have _m_²+_n_². This can avoid the over-parameterization issue in VAR for handling high-dimensional data.\n\n\nLoss function\n\n\nMatrices A and B are coefficient matrices than need to be estimated\nThere are closed form solutions for A and B but the solution to A is dependent on the solution to B and vice versa\n\nThe Alternating Least Squares (ALS) Algorithm is used to solve this problem",
    "crumbs": [
      "Forecasting",
      "Multivariate"
    ]
  },
  {
    "objectID": "qmd/forecasting-multivariate.html#sec-fcast-multivar-dynfact",
    "href": "qmd/forecasting-multivariate.html#sec-fcast-multivar-dynfact",
    "title": "Multivariate",
    "section": "Dynamic Factor Models (DFM)",
    "text": "Dynamic Factor Models (DFM)\n\nCommon dynamics of a large number of time series variables stem from a relatively small number of unobserved (or latent) factors, which in turn evolve over time.\nFor some macroeconomic applications it might be interesting to see whether a set of obserable variables depends on common drivers (unobserved factors). The estimation of such common factors can be done using so-called factor analytical models.\n\nFactor Analysis but for time series\nFor large sets of time series, which show, in most cases, strong correlation patterns\n\ntl;dr:\n\nRun PCA on bunch of time series that are relevant to the outcome variable you want to model or forecast.\nDetermine the number of factors (components) from the PCA\nUse VAR to determine the number of lags (or potentially leads) for the factors\nFit a model for the outcome variable using the factors and their lags as predictors.\n\nMisc\n\nNotes from\n\nHandbook of Macroeconomics, Ch.8\n\n111 pgs; SVARs, FAVARs, DFMs\n\n\npackages\n\n{dfms}:\n\nImports {collapse} and {Rcpp} so it should be very fast\n3 methods available\n\nA two-step estimator based on Kalman filtering\n\nfast, and forcasts should be similar to the other 2\n\nA quasi-maximum likelihood approach\nMaximum likelihood estimation of factor models on datasets with arbitrary pattern of missing data\n\n\n{nowcasting}- fit dynamic factor models specific to mixed-frequency nowcasting applications.\n\nThe latter two packages additionally support blocking of variables into different groups for which factors are to be estimated, and EM adjustments for variables at different frequencies\n\n{bvartools}\n\nUse Cases\n\nSensor Data\n\ne.g. IoT devices, telecommunication, industrial manufacturing, electric grid\n\nFinance\nMacroeconomics\n\nExample: (ebook ch8. introduction): a single-factor DFM fit using 58 quarterly US real activity variables (sectoral industrial production (IP), sectoral employment, sales, and National Income and Product Account (NIPA) series) is used to backcast four-quarter growth rates of four measures of aggregate economic activity (real Gross Domestic Product (GDP), total nonfarm, employment, IP, and manufacturing and trade sales)\n\n\nBlack solid line is the the observed; Red dotted line is the fitted value\nFitted line from lm(outcome ~ factor)\n\noutcome is total nonfarm, employment, IP, or manufacturing and trade sales\nfactor is from the dfm\n\nR2s of the fits range from 0.73 for GDP to 0.92 for employment.\n\nChicago Fed fits a factor model called the “cfnai” on these key economic indicators (Thread)\n\n\n\n“Structural DFMs, FAVARs, and SVARs are a unified suite of tools with fundamentally similar structures that differ in whether the factors are treated as observed or unobserved. By using a large number of variables and treating the factors as unobserved, DFMs”average out” the measurement error in individual time series, and thereby improve the ability to span the common macroeconomic structural shocks.”\n\nFactor Analytical Model General Form (Static)\n\nxt = λ*ft + ut where ut ∼ N(0,R)\n\nxt is an M-dimensional vector of observable variables\nft is an N×1 vector of unobserved factors\nλ is an M×N matrix of factor loadings\nut is an error term\nR is a M×M measurement (observation) covariance matrix and assumed to be diagonal\n\nAlso known as the measurement equation, which describes the relationship between the observed variables and the factors\n\nAllowing ft to take autocorrelation into account makes it “dynamic”\n\nft = A1*ft−1 + … + Ap*ft−p + vt where vt∼ N(0,Q)\n\nAi is the N×N coefficient matrix correponding to the ith lag of factor, f\nvt is an error term\nQ is the N×N state covariance matrix\n\nAlso known as the transition equation, which describes the intertemporal relationships between the factors\n\nPreprocessing\n\nAll series must be stationary with no cointegration\nAll series must be scaled and centered\nThe number of factors must be chosen.\nThen, the number of lags can chosen given the number of factors\nData at different frequencies\n\n{dfms} has a couple methods that approximate values\nDuplicate the longer frequency series multiple times. (see article)\n\ne.g. duplicate a quarterly series 3 times in a dataset with monthly time series.\nWhen these series get converted to shorter frequencies before fitting (e.g. quarterly to monthly), the missing values will just be NAs which will cause these series to have less weight in the estimation process. By duplicating them, it gives there values more weight.\nIf you decide to give these series more weight, you should be relatively certain they’re important to the forecast.\n\nEstimate different factor models for monthly and quarterly series, and combine them for the final prediction (e.g. by aggregating or blocking the monthly factor estimates (?)).\n{nowcasting} and {nowcastDFM} provide elaborate models for mixed-frequency nowcasting (but will be slower)\n\n\nForecasts\n\nFactors are dynamically forecasted using the transition equation.\nThe factor forecasts can then be fed to the observation equation to obtain forecasts for the input series\nExample: Introduction to dfms\n\n\n4 factors have been chosen and they’re highlighted\nInput series are in gray\nForecasts (after dotted vertical) converge the mean.\nquarterly variables, such as US GDP, to be forecast using a large set of monthly variables released with different lags\nreduce the information contained in dozens of monthly time series into only two dynamic factors. These two estimated factors, which are initially monthly, are then transformed into quarterly factors and used in a regression against GDP\n\nBackcasting refers to forecasting the value of a yet unpublished variable for a past period, while nowcasting will be with respect to the current period\nThe chosen number of factors, r∗ will correspond the IC with the lowest value. The penalty in equation IC2 is highest when considering finite sample\nGiven a strategy to identify one or more structural shocks, a structural DFM can be used to estimate responses to these structural shocks.\ndescription of the error terms to the equations\n\nmean-zero idiosyncratic component et (measurement eq)\n\nidiosyncratic disturbance can be serially correlated. if so, then model is incompletely specified. Solution is to model it as an autoregression\nIf there is no autocorrelation in the error terms (*unreasonable assumption across most applications), then the model is an exact DFM, and the correlation of one series with another occurs only through the\nlatent factors\n\nηt is the q x 1 vector of (serially uncorrelated) mean-zero innovations to the factors (transition eq)\n\nIf some of the variables are cointegrated, then transforming them to first differences loses potentially important information that would be present in the error correctionterms (that is, the residual from a cointegrating equation, possibly with cointegrating coefficients imposed). Here we discuss two different treatments of cointegrated variables, both of which are used in the empirical application of Sections 6 and 7.\n\nOption 1: Include the first difference of some of the variables and error correction terms for the others. This is appropriate if the error correction term potentially contains important information that would be useful in estimating one or more factors.\nOption 2: include all the variables in first differences and not to include any spreads. Appropriate if the first differences of the factors are informative for the common trend but the cointegrating residuals do not load on common factors\n\nDidn’t understand it lol (pg 427). Think there’s an example in sect 7.2 that will illustrate it.\nExample: While there is empirical evidence that these oil prices, for example Brent and WTI, are cointegrated, there is no a priori reason to believe that the WTI-Brent spread is informative about broad macro factors, and rather that spread reflects details of oil markets, transient transportation and storage disruptions, and so forth.\n\nOption 3: specify the DFM in levels or log levels of some or all of the variables, then to estimate cointegrating relations and common stochastic trends as part of estimating the DFM. This approach goes beyond the coverage of this chapter, which assumes that variables have been transformed to be I(0) and trendless.\n\nBanerjee and Marcellino (2009)and Banerjee et al. (2014, 2016) develop a factor-augmented error correction model (FECM) in which the levels of a subset of the variables are expressed as cointegrated with the common factors. The discussion in this chapter about applications and identification extends to the FECM.\n\n\nThe error in estimation of the factors during PCA can be ignored when they are used as regressors",
    "crumbs": [
      "Forecasting",
      "Multivariate"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html",
    "href": "qmd/forecasting-statistical.html",
    "title": "Statistical",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-misc",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-misc",
    "title": "Statistical",
    "section": "",
    "text": "For intermittent data, see Logistics &gt;&gt; Demand Planning &gt;&gt; Intermittent Demand\nLet the context of the decision making process determine the units of the forecast\n\ni.e. Don’t forecast on a hourly scale just because you can.\n\nWhat can be forecast depends on the predictability of the event:\n\nHow well we understand the factors that contribute to it;\n\nWe have a good idea of the contributing factors: electricity demand is driven largely by temperatures, with smaller effects for calendar variation such as holidays, and economic conditions.\n\nHow much data is available;\n\nThere is usually several years of data on electricity demand available, and many decades of data on weather conditions.\n\nHow similar the future is to the past;\n\nFor short-term forecasting (up to a few weeks), it is safe to assume that demand behaviour will be similar to what has been seen in the past.\n\nWhether the forecasts can affect the thing we are trying to forecast.\n\nFor most residential users, the price of electricity is not dependent on demand, and so the demand forecasts have little or no effect on consumer behaviour.\n\n\nStarting a project\n\nUnderstand the dgp through eda and talking to domain experts\n\nHow are sales generated? (e.g. online, brick and mortar,…)\n\nWhat is the client currently using to forecast?\n\nModel that you need to beat\nWhere does it fail?\n\nBiased? underfitting or overfitting somewhere\nMissing seasonality?\n\n\nWhat is the loss function?\n\nCarrying this many items in inventory results in this cost\nIf we’re out of stock and lose this many sales, how much does this cost\n\nWhat does the client really want?\n\nHow is success measured\n\n\nFable models produce different results with NAs in the time series\n\nIn rolling cfr project, steinmetz’s manually calc’d rolling 7-day means and his lagged vars had NAs, models using data with and without NAs had different score\n\nIt is helpful to keep track of and understand what our forecast bias has historically been.  Even where we are fortunate enough to show a history of bias in both directions.\nForecasting shocks is difficult for an algorithm\n\nIt can better to smooth out (expected) shocks (Christmas) in the training data and then add an adjustment to the predictions during the dates of the shocks.\nThe smoothed out data will help the algorithm produce more accurate predictions for days when there isn’t an expected shock.\nExamples of shocks that may need training data to have manual adjustments and not be smoothed by an algorithm\n\nOne-Time spikes due to abnormal weather conditions\nOne-Off promotions\nA sustained marketing campaign that is indistinguishable from organic growth.\n\n\nIntermittent(or sporadic) time series (lotsa zeros).\n\n{thief} has the latest methods while {tsintermittent} has older methods\n\nBenchmark models\n\nNaive\n28-day moving average (i.e. 4 week MA)",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-terms",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-terms",
    "title": "Statistical",
    "section": "Terms",
    "text": "Terms\n\nWeak stationarity (commonly referred to as just stationarity)(aka covariance stationary) - Implies that the mean and the variance of the time series are finite and do not change with time.\nCointegration - \\(x_t\\) and \\(y_t\\) are cointegrated if \\(x_t\\) and \\(y_t\\) are \\(I(1)\\) series and there exists a \\(\\beta\\) such that \\(z_t = x_t - \\beta y_t\\) is an \\(I(0)\\) series (i.e. stationary).\n\nImportant for understanding stochastic or deterministic trends.\nThe differences in the means of the set of cointegrated series remain constant over time, without offering an indication of directionality\nMight have low correlation, and highly correlated series might not be cointegrated at all.\nCan use Error Correction Model (ECM) with differenced data and inserting a error correction term (residuals from a OLS regression)\n\nStochastic - No value of a variable is known with certainty. Some values may be more likely than others (probabilistic). Variable gets mapped onto a distribution.",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-preproc",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-preproc",
    "title": "Statistical",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nFilling in gaps\n\nBi-directional forecast method from AutoML for time series: advanced approaches with FEDOT framework\n\nSteps\n\nSmooth series prior to the gap\n\nThey used a “Gaussian filter w/sigma = 2” (not sure what that is)\n\nCreate lagged features of the smoothed series\nForecast using ridge regression where h = length of gap\nRepeat in the opposite direction using the series after the gap\nUse the average of the forecasts to fill the gap in the series.\n\n\n\nLog before differencing (SO post)\nDetrend or Difference\n\n(The goal is to get a stationary series, so if one doesn’t work try the other.)\nDifferencing (for unit root processes)(stochastic trend)\n\nif the process requires differencing to be made stationary, then it is called difference stationary and possesses one or more unit roots.\n\nSometimes see charts of roots and a unit circle. I read this in an article about VAR models “process is stationary if all the roots \\(z_1, \\ldots , z_n\\) of the determinant \\(\\det(\\psi(z))\\), or \\(\\det(I − Bz) = 0\\), lie outside of the unit circle.”\n\nOne advantage of differencing over detrending to remove trend is that no parameters are estimated in the differencing operation.\nOne disadvantage, however, is that differencing does not yield an estimate of the stationary process\nIf the goal is to coerce the data to stationarity, then differencing may be more appropriate.\nDifferencing is also a viable tool if the trend is fixed\n\nRandom Walking looking series should be differenced and not detrended.\n\nBackshift operator notation:\n\nIn general: \\(\\nabla^d = (1 − B)^d\\)\n\nWhere \\(d\\) is the order of differencing\nFractional differencing is when \\(0 \\lt d \\lt 1\\)\n\nWhen \\(0 \\lt d \\lt 0.5\\), the series is classified as a long term memory series (often used for environmental time series arising in hydrology)\n\nIf \\(d\\) is negative, then its called forward-shift differencing\n\nExamples:\n\nIdentities:\n\\[\nB y_t = y_{t-1} \\\\\nB^2 y_t = y_{t-2}\n\\]\nSeasonal Difference:\n\\[\n(1 - B)(1 - B^m) y_t = (1 - B - B^m + B^{m + 1})y_t = y_t - y_{t-1} - y_{t-m} + y_{t-m-1}\n\\]\nARIMA : AR(p)I(d) = MA(q)\n\\[\n(1-\\phi_1 B - \\cdots - \\phi_p B^p)(1-B)^d y_t = c+(1+\\theta_1 B + \\cdots + \\theta_q B^q)\\epsilon_t\n\\]\nARIMA(1,1,1)(1,1,1)4 for quarterly data (m = 4)\n\\[\n(1-\\phi_1 B)(1-\\Phi B^4)(1-B)(1-B^4)y_t = (1+\\theta_1 B)(1+\\Theta B^4)\\epsilon_t\n\\]\n\n\n\nDetrending (for trend-stationary processes)(deterministic trend)\n\nIt is possible for a time series to be non-stationary, yet have no unit root and be trend-stationary\n\na trend-stationary process is a stochastic process from which an underlying trend (function solely of time) can be removed (detrended), leaving a stationary process.\n\nIf an estimate of the stationary process is essential, then detrending may be more appropriate.\nHow is this back-transformed after forecasting?\n\nmaybe look at “forecasting with STL” section in fpp2\n\n\nIn both unit root and trend-stationary processes, the mean can be growing or decreasing over time; however, in the presence of a shock, trend-stationary processes are mean-reverting (i.e. transitory, the time series will converge again towards the growing mean, which was not affected by the shock) while unit-root processes have a permanent impact on the mean (i.e. no convergence over time).\nTesting\n\nKPSS test: H0 = Trend-Stationary, Ha = Unit Root.\n\nurca::ur_kpss the H0 is stationarity\ntseries::kpss.test(res, null = \"Trend\") where H0 is “trend-stationarity”\n\nDickey-Fuller tests: H0 = Unit Root, Ha = Stationary or Trend-Stationary depending on version\nKPSS-type tests are intended to complement unit root tests, such as the Dickey–Fuller tests. By testing both the unit root hypothesis and the stationarity hypothesis, one can distinguish series that appear to be stationary, series that appear to have a unit root, and series for which the data (or the tests) are not sufficiently informative to be sure whether they are stationary.\n\nSteps:\n\nADF:\n\nIf H0 rejected. The trend (if any) can be represented by a deterministic linear trend.\nIf H0 is not rejected then we apply the KPSS test.\n\nKPSS :\n\nIf H0 rejected then we conclude that there is a unit root and work with the first differences of the data.\n\nUpon the first differences of the series we can test the significance of other regressors or choose an ARMA model.\n\nIf H0 is not rejected then data doesn’t contain enough information. In this case it may be safer to work with the first differences of the series.\n\n\nSteps when using an ARIMA:\n\nSuppose the series is not trending\n\nIf the ADF test (without trend) rejects, then apply model directly\nIf the ADF test (without trend) does not reject, then model after taking difference (maybe several times)\n\nSuppose the series is trending\n\nIf the ADF test (with trend) rejects, then apply model after detrending the series\nIf the ADF test (with trend) does not reject, then apply model after taking difference (maybe several times)",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-diag",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-diag",
    "title": "Statistical",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nTesting for significant difference between model forecasts\n\nNemenyi test\n\nsutils::nemenyi\n\nMCB\n\ngreybox::rmcb",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-alg",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-alg",
    "title": "Statistical",
    "section": "Algorithms",
    "text": "Algorithms\n\nMisc\n\nAuto Arima, ETS and Theta are general-purpose methods particularly well-suited for monthly, quarterly and annual data\nTBATS and STL will also handle multiple seasonalities such as arise in daily and weekly data.\nWhen to try nonlinear models (see Forecasting, Nonlinear)\n\nLinear prediction methods (e.g. ARIMA) don’t produce adequate predictions\nChaotic nature of the time series is obvious (e.g. frequent, unexplainable shocks that can’t be explained by noise)\n\n\n\n\nRegression (including ARIMA)\n\nMisc\n\nDouble check auto_arima, for the parameters, (p, d, q), one should pick q to be at least p (link)\nSometimes the error terms are called random shocks.\nIf using lm and there are NAs, make sure to use na.action = NULL else they get removed and therefore dates between variables won’t match-up. See lm doc for further details on best practices.\nARIMA models make h-step out predictions by iterating 1-step forward predictions and feeding the intermediate predictions in as if they were actual observations (0 are used for the errors)\nPolynomial Autoregression (AR) models exponentiate the lags. So the design matrix includes the lags and the exponentiated series.\n\nIf the polynomial is order 3, then order 2 is also included. So, now, the design matrix would be the lags, the square of each lag, and the cube of each lag\nExample:\nlibrary(dplyr); library(timetk)\n# tbl w/polynomial design matrix of order 3\n# value is the ts values\npoly_tbl &lt;- group_tbl %&gt;%\n  tk_augment_lags(.value = value, .lags = 1:4) %&gt;%\n  mutate(across(contains(\"lag\"), \n         .fns = list(~.x^2, ~.x^3), \n         .names = \"{.col}_{ifelse(.fn==1, 'quad','cube')}\"))\n\n.fn is the item number in the .fns list.\nSquared lag 2 will have the name “value_lag2_quad”\n\n\n\nTypes\n\nAR: single variable with autoregressive dependent variable terms\nARMA: same as AR but errors models as a moving average\nARIMA: same as ARMA but with differencing the timeseries\nSARIMA: same as ARIMA but also with seasonal P, D, Q terms\nARMAX: same as ARMA but with additional exogenous predictors\nDynamic Regression: OLS regression with modeled (usually arima) errors\n\nOLS vs ARIMA\n\nJohn Mount\n\nThe fear in using standard regression for time series problems is that the error terms are likely correlated.\n\nSo one can no longer appeal to the Gauss Markov Theorem (i.e. OLS is BLUE) to be assured of good out of sample performance (link)\n\n\nRyer and Chan regarding Dynamic Regression vs OLS\n\n“Regression (with arima errors) coefficient estimate on Price is similar to that from the OLS regression fit earlier, but the standard error of the estimate is about 10% lower than that from the simple OLS regression. This illustrates the general result that the simple OLS estimator is consistent but the associated standard error is generally not trustworthy”\n\nHyndman\n\n“The forecasts from a regression model with autocorrelated errors are still unbiased, and so are not “wrong,” but they will usually have larger prediction intervals than they need to. Therefore we should always look at an ACF plot of the residuals.”\nThe estimated coefficients are no longer the best estimates, as some information has been ignored in the calculation;\n\nMeaning modeling the errors to take into account the autocorrelation\n\nAny statistical tests associated with the model (e.g., t-tests on the coefficients) will be incorrect.\n\nAffected by the bloated std errors\n\nThe AICc values of the fitted models are no longer a good guide as to which is the best model for forecasting.\nIn most cases, the p-values associated with the coefficients will be too small, and so some predictor variables will appear to be important when they are not. This is known as “spurious regression.”\n\n\nTrend (\\(\\beta\\) \\(t\\)) is modeled by setting the variable \\(t\\) to just an index variable (i.e. \\(t = 1, \\ldots, T\\)). Modeling quadratic trend would be adding in \\(t^2\\) to the model formula.\n\nHyndman suggests that using splines is a better approach than using t2\nFrom Steinmitz’s CFR article\n\nInstead of trend (like in {forecast}), he’s using poly(date, 2) to include a quadratic trend\n\n\nR2 and Adjusted-R2\n\nAppropriate for time series (i.e. estimate of the population R2), as long as the data are stationary and weakly dependent\n\ni.e. The variances of both the errors and the dependent variable do not change over time.\ni.e. If \\(y_t\\) has a unit root (Integrated of order 1, I(1)) (needs differenced)\n\n\nInterpretation of coefficients\n\\[\ny_t = \\alpha + \\beta_0 x_t + \\beta_1 x_{t-1} + \\cdots + \\beta_s x_{t-s} + \\cdots + \\beta_q x_{t-q}\n\\]\n\nIf \\(x\\) increases by one unit today, the change in \\(y\\) will be \\(\\beta_0+\\beta_1+...+\\beta_s\\) after \\(s\\) periods; This quantity is called the \\(s\\)-period interim multiplier. The total multiplier is equal to the sum of all \\(\\beta\\) s in the model.\n\nResiduals\n\nTypes\n\n“Regression” is for the main model\n\nOriginal data minus the effect of the regression variables\n\n“Innovation” is for the error model\n\nDefault arg\nHyndman uses these for dynamic regression residual tests\n\n\nAutocorrelation tests\n\nFailing the test does not necessarily mean that (a) the model produces poor forecasts; or (b) that the prediction intervals are inaccurate. It suggests that there is a little more information in the data than is captured in the model. But it might not matter much.\nBreusch-Godfrey test designed for pure regression or straight AR model\n\nDoes handle models with lagged dependent vars as predictors\nLM (lagrange multiplier) test\nforecast::checkresiduals can calculate it and display it, but you don’t have access to the values programmatically\n\nDefaults for lag is \\(\\min(10,n/5)\\) for nonseasonal and \\(\\min(2m, n/5)\\) for seasonal where the frequency is seasonality, m\nlag &lt;- ifelse(freq &gt; 1, 2 * freq, 10)\nlag &lt;- min(lag, round(length(residuals)/5))\nlag &lt;- max(df+3, lag)\n\n{lmtest} and {DescTools} (active) packages have the function that forecast uses but only takes lm objects\n\nDurbin-Watson designed for pure regression\n\nError term can’t be correlated with predictor to use this test\n\nSo no lagged dependent variables can be used as predictors\nThere is an durbin alternate test mentioned in stata literature that can do lagged variables but I haven’t seen a R version that specifies that’s the version it is.\n\n{lmtest} and {DescTools} takes a lm object and has a small sample size correction available\n{car::durbinWatsonTest} takes a lm object or residual vector.\n\nOnly lm returns p-value. Residual vector returns DW statistic\n\np-values \\(\\lt 0.05\\) \\(\\rightarrow\\) Autocorrelation present\nDW statistic guide (\\(0 \\lt \\text{DW} \\lt 4\\))\n\nAround 2 \\(\\rightarrow\\) No Autocorrelation\nSignifcantly \\(\\lt 2\\) \\(\\rightarrow\\) Positive Correlation\n\nSaw values \\(\\lt 1\\) have p-values = 0\n\nSignificantly \\(\\gt 2\\) \\(\\rightarrow\\) Negative Correlation\n\n\nLjung-Box\n\nFor dynamic regression, arima, ets, etc.\n\nThere’s a SO post that shows this shouldn’t be used for straight regression\n\nFor straight AR models, the comments show it should be fine as long as lags \\(\\gt\\) model [df]{arg-text} (see below)\n\n\nTest is whether a group of lagged residuals has significant autocorrelation, so an acf of the residuals might show individual spikes but the group as a whole may not have significant autocorrelation\n\nIf you see a spike in the residuals, may be interesting to include that lag number in the group of lags and see if significance of the group changes\n\n{feasts::ljung_box}\n\nRequires numeric residuals vector, model degrees of freedom, number of lags to check\n\nThe model df is number of variables used in the regression + intercept + p + q (of ARIMA error model)\n\ne.g. Model with predictors: trend + cases and an error model: arima (2,1,1) had df = 2 (predictors: trend, cases) + 1 (intercept) + 2 (p) + 1 (q) = 6 d.f.\ndof &lt;- length(fit$coef)\n\nSee Breusch-Godfrey section for number of lags to use\n\n\np-values \\(\\lt 0.05\\) \\(\\rightarrow\\) autocorrelation present\n\n\n\nSpectral analysis takes the approach of specifying a time series as a function of trigonometric components (i.e. Regression with fourier terms)\n\nA smoothed version of the periodogram, called a spectral density, can also be constructed and is generally preferred to the periodogram.\n\n\n\n\nRandom Walk\n\nA process integrated to order 1, (an I(1) process) is one where its rate of change is stationary. Brownian motion is a canonical I(1) process because its rate of change is Gaussian white noise, which is stationary. But the random walk itself is not stationary. So the \\(t+1\\) value of a random walk is just the value at \\(t\\) plus a number sampled from some bell curve.\nCharacteristics\n\nLong periods of apparent trends up or down\nSudden and unpredictable changes in direction\n\nA special case of an autoregressive model\n\\[\ny_t = c + \\phi_1 y_{t-1} + \\cdots + \\phi_p y_{t-p} + \\epsilon_t\n\\]\n\nWhere \\(c=0\\), \\(p=1\\), \\(\\phi = 1\\), and \\(\\epsilon \\sim \\mathcal {N}(0, s)\\)\n\nDrift\n\n\n\n\n\n\n\n\nFeature\nRandom Walk without Drift\nRandom Walk with Drift\n\n\n\n\nSteps\nPurely random, equal probability left/right\nBiased, one direction slightly more likely\n\n\nChange in value\nAverage change is zero\nAverage change includes a constant drift\n\n\nPath\nZig-zag around starting point\nZig-zag with upward/downward trend\n\n\nMean\nStays roughly the same\nIncreases/decreases over time depending on drift\n\n\nVariance\nIncreases with time\nIncreases with time\n\n\nStationarity\nNon-stationary\nNon-stationary\n\n\n\nExamples with and without drift\n\n\n\n\n\n\nProphet\n\nThe basic methodology is an iterative curve-matching routine, where Prophet will then train your data on a bigger period, then predict again and this will repeat until the end point is reached.\nThe development team of Prophet claim that its strengths are:\n\nWorking with high-frequency data (hourly, daily, or weekly) with multi-seasonality, such as hour of day, day of week and time of year;\nSpecial events and bank holidays that are not fixed in the year;\nAllowing for the presence of a reasonable number of missing values or large outliers;\nAccounting for changes in the historical trends and non-linear growth curves in a dataset.\n\nFurther advantages include the ability to train from a moderate sized dataset, without the need for specialist commercial software, and fast start up times for development.\nDisadvantages\n\nNo autoregressive (i.e. lags of target series) features since it’s a curve-fitting algorithm\n\nTime series decomposition by prophet:\n\n\\(g(t)\\): Logistic or linear growth trend with optional linear splines (linear in the exponent for the logistic growth). The library calls the knots “change points.”\n\\(s(t)\\): Sine and cosine (i.e. Fourier series) for seasonal terms.\n\\(h(t)\\): Gaussian functions (bell curves) for holiday effects (instead of dummies, to make the effect smoother).\n\n\n\n\nKalman Filter\n\nMisc\n\nNotes from How a Kalman filter works, in pictures\nIf a dynamic system is linear and with Gaussian noise (inaccurate measurements, etc.), the optimal estimator of the hidden states is the Kalman Filter\n\nFor nonlinear systems, we use the extended Kalman filter, which works by simply linearizing the predictions and measurements about their mean. (I may do a second write-up on the EKF in the future)\nGood for predictions where the measurements of the outcome variable over time can be noisy\n\nAssumptions\n\nGaussian noise\nMarkov property\n\nIf you know \\(x_{t−1}\\), then knowledge of \\(x_{t−2},\\ldots , x_0\\) doesn’t give any more information about xt (i.e. not much autocorrelation if at all)\n\n\ntl;dr\n\nA predicted value from a physically-determined autoregression-type equation with 1 lag that gets adjusted for measurement error\n\nAdvantages\n\nLight on memory (they don’t need to keep any history other than the previous state)\nVery fast, making them well suited for real time problems and embedded systems\n\nUse cases\n\nEngineering: common for reducing noise from sensor signals (i.e. smoothing out measurements)\nDetection-based object tracking (computer vision)\n\n\n\n\nFirst set of equations\n\n\nNotes\n\nThis set of equations deals physical part of the system. It’s kinda how we typically forecast.\n\nThe \\(\\hat x_k\\) equation is pretty much like a typical auto-regression plus explanatory variables except for the F matrix which may require knowledge of system dynamics\n\nWiki shows a term, \\(w_k\\), added to the end of the \\(\\hat x_k\\) equation. \\(w_k\\) is the process noise and is assumed to be drawn from a zero mean multivariate normal distribution,\n\nThe new best estimate is a prediction made from previous best estimate, plus a correction for known external influences.\n\n\\(\\hat x_k\\): The step-ahead predicted “state”; \\(\\hat x_{k-1}\\) is the current “state”\n\\(u_k\\) (“control” vector): An explanatory variable(s)\n\\(F_k\\) (“prediction” matrix) and \\(B_k\\) (“control” matrix) are transformation matrices\n\n\\(F_k\\) was based on one of Galileo’s equations of motion in the example so this might be very context specific\nMight need to based on substantial knowledge of the system to create a system of linear equations (i.e. \\(F_k\\) matrix) that can be used to model the it.\n\n\nAnd the new uncertainty is predicted from the old uncertainty, with some additional uncertainty from the environment.\n\n\\(P_k\\) and \\(P_{k-1}\\) are variance/covariance matrices for the step-ahead predicted state and current state respectively\n\\(Q_k\\) is the uncertainty term for the variance/covariance matrix of the predicted state distribution\n\n\n\n\nSecond Set of Equations\n \n\nNotes\n\nThese equations refine the prediction of the first set of equations by taking into account various sources of measurement error in the observed outcome variable\nThe equations do this by finding the intersection, which is itself a distribution, of the transformed prediction distribution, \\(μ_0\\), and the measurement distribution, \\(μ_1\\).\n\n\nThis mean, \\(\\mu'\\), of this intersection distribution is the predicted value that most likely to be the true value\n\n\n\\(H_k\\) is a transformation matrix that maps the predicted state (result of the first set of equations), \\(\\hat x_k\\) , to the measurement space\n\nWhere \\(H_k \\cdot \\hat x_k\\) is the expected measurement (pink area) (i.e. Mean of the distribution of transformed prediction)\n\n\\(\\vec z_k\\) is the mean of the measurement distribution (green area)\n\\(\\hat x_k'\\) is the intersection of the transformed prediction distribution and the measurement distribution (i.e. the predicted state thats most likely to true)\n\\(R_k\\) is the uncertainty term for variance/covariance matrix for the measurement distribution\n\\(K'\\) is called the Kalman Gain\n\nDidn’t read anything interpretative about the value. Just seems to a mathematical construct that’s part of the derivation.\nIn the derivation, it starts out as the ratio of the measurement covariance matrix to the sum of the measurement variance covariance matrix and the transformed prediction variance covariance matrix\n\n\n\n\n\nProcess\n\n\n\nHyperparameters\n\n\\(Q\\) is the process noise covariance\n\nControls how sensitive the model will be to process noise.\n\n\\(R\\) is the measurement noise variance\n\nControls how quickly the model adapts to changes in the hidden state.\n\n\nGuessing “std” is the default value?\n\n\n\n\n\n\nExponential Smoothing\n\nThe general idea is that future values are a weighted average of past values, with the weights decaying exponentially as we go back in time\nMethods\n\nSimple Exponential Smoothing\ndouble Exponential Smoothing or Holt’s Method (for time series with a trend)\nTriple Exponential Smoothing or Holt-Winter’s method (for time series with a trend and sesaonality)\n\n\n\n\nTBATS\n\nTrigonometric seasonality, Box-Cox transformation, ARMA errors, Trend, and Seasonal components\nCan treat non-linear data, solve the autocorrelation problem in residuals since it uses an ARMA model, and it can take into account multiple seasonal periods\nRepresents each seasonal period as a trigonometric representation based on Fourier series. This allows the model to fit large seasonal periods and non-integer seasonal periods",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-intv",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-intv",
    "title": "Statistical",
    "section": "Interval Forecasting",
    "text": "Interval Forecasting\n\nNotes from Video: ISF 2021 Keynote\nInterval data is commonly analyzed by modeling the range (difference between interval points)\n\nRange data doesn’t provide information about the variation of the mean (aka level) over time.\nRange only provides information about the boundaries, where interval analysis provides information about the boundary and the interior of the interval.\n\nProvides more information than point forecasts.\nData examples:\n\nDaily Temperature, Stock Prices: Each day a high and low values are recorded\nStock Price Volatility, Bid-Ask spread use hi-lo value differences\nIntra-House Inequality: difference between wife and husband earnings\nUrban-Rural income gap\nInterval-Valued Output Growth Rate: China reports it’s targeted growth rate as a range now.\nDiastolic and Systolic blood pressure\n\nOthers: Blood Lipid, White Blood Cell Count, Hemoglobin\n\n\nExamples where (generalized) intervals can be modeled instead of differences:\n\nStock Volatility\n\nGARCH models often used to model volitility but Conditional Autoregressive Range (CARR) gives better forecasts\n\nBecause GARCH model is only based on the closing price but the CARR model uses the range (difference).\n\nDynamic Interval Modeling\n\nUse Autoregressive Interval (ARI) model to estimate the parameters using an interval time series (not the range)\nThen take the forecasted left and right values of the interval to forecast the volatility range in a CARR model\nThe extra information of the interval data over time (instead of a daily range) yields a more efficient estimation of the parameters\n\n\nCapital Asset Pricing Model (CAPM)\n\nAlso see Finance, Valuation &gt;&gt; Cost of Capital &gt;&gt; WACC &gt;&gt; Cost of Equity\nStandard Equation\n\\[\nR_t - R_{ft} = \\alpha + β(R_{mt} - R_{ft}) + \\epsilon_t\n\\]\n\n\\(R_t\\): Return of Certain Portfolio\n\\(R_{ft}\\): Risk-Free Interest Fate\n\\(R_{mt}\\): Return of Market Portfolio\n\\(R_t - R_{ft}\\): Asset Risk Premium\n\nInterval-based version\n\\[\nY_t = (\\alpha_0 + \\beta_0I_0) + \\beta X_t + u_t\n\\]\n\n\\(I_0 = [-0.5, 0.5]\\)\n\\(Y_t = [R_{ft}, R_t]\\)\n\\(X_t = [R_{ft}, R_{mt}]\\)\nThe \\(R_t - R_{ft}\\) can then be calculated by taking the difference of the interval bounds of the interval-based predictions\n\n\n\nModel the center of the interval and the range in a bi-variate VAR model (doesn’t use all points in the interval data)\n\nBi-variate Nonlinear Autoregressive Model for center and range\n\nHas an indicator variable that captures nonlinearity of interval data\n\nSpace-time autoregressive model\n\nAutoregressive Conditional Interval model\n\nThe interval version of an ARMA model\n\ndepends on lags and lagged residuals\n\nACI(p,q):\n\n\\[\nY_t = (\\alpha_0 + \\beta_0 I_0) + \\sum_{j=`}^p \\beta_jY_{t-j} + \\sum_{j=1}^p \\gamma_ju_{t-j} + u_t\n\\]\n\n\\(\\alpha_0\\), \\(\\beta_0\\), \\(\\beta_j\\), \\(\\gamma_j\\) are unknown scalar parameters\n\\(I_0 = [-\\frac{1}{2},\\; \\frac{1}{2}]\\) is a unit interval\n\\(\\alpha_0 + \\beta_0I_0 = [\\frac{\\alpha_0 - \\beta_0}{2},\\: \\frac{\\alpha_0 + \\beta_0}{2}]\\) is a constant interval intercept\n\\(u_t\\) is the interval residuals that satisfies \\(\\mathbb{E}(u_t\\;|\\;I_{t-1}) = [0,0]\\)\n\\(Y_t\\) is a random interval variable\n\nObjective function that gets minimized is called \\(D_k\\) distance\n\n\\(D^2_K [u_t(\\theta),0]\\)\n\n\\(u_t(\\theta)\\) is the interval residuals\n\\(K\\) refers to some kind of kernel function\n\nIt’s a wacky quadratic with constants a,b,c\nMeasures the distance between all pairs of points\n\n\nThe minimization is a two-stage process\n\nFinds the optimal kernel, \\(K\\), then uses it to minimize the residuals to estimate the parameters\n\n\n\nThreshold Autoregressive Interval (TARI)\n\nNonlinear ACI model and interval version of TAR(p) model (¯\\_(ツ)_/¯)\n2-Procedure Model\n\nBasically 2 autoregressive equations with an \\(i_1u_t\\) or \\(i_2u_t\\) added on to the end.\nThe interval series, \\(Y_t\\) ,follows one of the equations based on threshold variable \\(q_t\\) is less than or equal to a threshold parameter, \\(\\gamma\\) or greater than.\n\nEstimation is similar to ACI model\nFor more details, need to research what a TAR model (Terasvirta, Tjostheim, and Granger 2010) is",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/generalized-additive-models-(gam).html",
    "href": "qmd/generalized-additive-models-(gam).html",
    "title": "Generalized Additive Models",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Generalized Additive Models"
    ]
  },
  {
    "objectID": "qmd/generalized-additive-models-(gam).html#sec-gam-misc",
    "href": "qmd/generalized-additive-models-(gam).html#sec-gam-misc",
    "title": "Generalized Additive Models",
    "section": "",
    "text": "Also see Feature Engineering, Splines\nLarge gaps in the values of the predictor variable can be a problem if you are trying to interpolate between those gaps. (See bkmks, method = \"reml\" + s(x, m = 1))",
    "crumbs": [
      "Generalized Additive Models"
    ]
  },
  {
    "objectID": "qmd/generalized-additive-models-(gam).html#sec-gam-diag",
    "href": "qmd/generalized-additive-models-(gam).html#sec-gam-diag",
    "title": "Generalized Additive Models",
    "section": "Diagnostics",
    "text": "Diagnostics\n\n“Deviance explained” is the R2 value for GAMs\nmgcv::gam.check(gam_fit) \n## Method: GCV  Optimizer: magic\n## Smoothing parameter selection converged after 19 iterations.\n## The RMS GCV score gradient at convergence was 5.938335e-08 .\n## The Hessian was positive definite.\n## Model rank =  21 / 22 \n## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may\n## indicate that k is too low, especially if edf is close to k'.\n##                                           k'  edf  k-index p-value   \n## s(id)                                   1.00  0.35    0.82  &lt;2e-16 ***\n## s(log_profit_rug_business_b)            9.00  8.52    1.01    0.69   \n## s(log_profit_rug_business_b):treatment 10.00  1.50    1.01    0.62   \n## ---\n## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nCheck if the size of the basis expansion (k) for each smooth is sufficiently large\n\nk.check can also do this\nIf all your smoothing predictors are not sufficiently large, then this indicates that using a GAM is a bad fit for your data.\nSee SO post from Simpson\n\n\nFormal test for the necessity of a smooth\nm &lt;- \n  gam(y ~ x + s(x, m = c(2, 0), bs = \"tp\"),\n      data = foo,\n      method = \"REML\",\n      family = binomial())\n\nbs = \"tp\" is just the default thin plate basis function\nFit the predictor of interest as a linear term (x) plus a smooth function of x\nModify the basis for the smooth so that it no longer includes linear functions in the span of the basis with m = c(2, 0)\n\nIndicates we want the usual second order derivative penalty but with a 0 size null space (the span of functions that aren’t affected by the penalty because they have 0 second derivative).\n\nsummary will give a test for the necessity of the wiggliness provided by the smooth over the linear effect estimated by the linear term.\n\nFrom Simpson SO post\nAlso see Wood’s “Generalized Additive Models: An Introduction with R”, 2nd Ed, section 6.12.3, “Testing a parametric term against a smooth alternative” p 312-313 (R &gt;&gt; Documents &gt;&gt; Regression &gt;&gt; gam)",
    "crumbs": [
      "Generalized Additive Models"
    ]
  },
  {
    "objectID": "qmd/geospatial-analysis.html",
    "href": "qmd/geospatial-analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Geospatial",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/geospatial-analysis.html#sec-geo-anal-misc",
    "href": "qmd/geospatial-analysis.html#sec-geo-anal-misc",
    "title": "Analysis",
    "section": "",
    "text": "Also see\n\nDomain Knowledge &gt;&gt; Epidemiology &gt;&gt; Disease Mapping",
    "crumbs": [
      "Geospatial",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/geospatial-analysis.html#sec-geo-anal-terms",
    "href": "qmd/geospatial-analysis.html#sec-geo-anal-terms",
    "title": "Analysis",
    "section": "Terms",
    "text": "Terms\n\nBuffer - a zone around a geographic feature containing locations that are within a specified distance of that feature, the buffer zone. A buffer is likely the most commonly used tool within the proximity analysis methods. Buffers are usually used to delineate protected zones around features or to show areas of influence.\nCatchment - The area inside any given polygon is closer to that polygon’s point than any other. Refers to the area of influence from which a retail location, such as a shopping center, or service, such as a hospital, is likely to draw its customers. (also see Retail &gt;&gt; Catchment)",
    "crumbs": [
      "Geospatial",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/geospatial-analysis.html#sec-geo-anal-proxanal",
    "href": "qmd/geospatial-analysis.html#sec-geo-anal-proxanal",
    "title": "Analysis",
    "section": "Proximity Analysis",
    "text": "Proximity Analysis\n\nExample: Basic Workflow\n\nData: Labels, Latitude, and Longitude\n\nCreate Simple Features (sf) Object\ncustomer_sf &lt;- \n  customer_table %&gt;%\n    sf::st_as_sf(coords = c(\"longitude\", \"latitude\"),\n                 crs = 4326)\n\nMerges the longitude and latitude columns into a geometry column and transforms the coordinates in that column according to projection (e.g. crs = 4326)\n\nView points on a map\n\nmapview::mapview(customer_sf)\nCreate Buffer Zones\n\ncustomer_buffers &lt;- \n  customer_sf %&gt;%\n    sf::st_transform(26914) %&gt;%\n    sf::st_buffer(5000)\n\nmapview::mapview(customer_buffers)\n\nMost of projections use meters, and based on the size of the circles as related to the size of Denton, TX, I’m guessing the radius of each circle is 5000m. Although, that still looks a little small.\n\nCreate Isochrones\n\ncustomer_drivetimes &lt;- \n  customer_sf %&gt;%\n    mapboxapi::mb_isochrone(time = 10, \n                            profile = \"driving\", \n                            id_column = \"name\")\n\nmapview::mapview(customer_drivetimes)\n\n10 minutes drive-time from each location\ntime (minutes): The maximum time supported is 60 minutes. Reflects traffic conditions for the date and time at which the function is called.\n\nIf reproducibility of isochrones is required, supply an argument to the depart_at argument.\n\ndepart_at: Specifying a time makes it a time-aware isochrone. Useful for modeling peak business hours or rush hour traffic, etc.\n\ne.g. Adding depart_at = “2024-01-27T17:30” to the isochrone above gives you a 10-minute driving isochrone with predicted traffic at 5:30pm tomorrow\n\n\nAdd Demographic Data\n\ndenton_income &lt;- \n  tidycensus::get_acs(\n    geography = \"tract\",\n    variables = \"B19013_001\",\n    state = \"TX\",\n    county = \"Denton\",\n    geometry = TRUE\n  ) %&gt;%\n    select(tract_income = estimate) %&gt;%\n    sf::st_transform(st_crs(customer_sf))\n\ncustomers_with_income &lt;- customer_sf %&gt;%\n  sf::st_join(denton_income)\n\ncustomers_with_income\n\nAdds median income estimate according to the census tract each person lives in.\nJoins on the geometry variable\n\n\nCircular Buffer Approach\n\nNotes from GIS-based Approaches to Catchment Area Analyses of Mass Transit\nThe simplest and most common used approach to make catchment areas of a location is to consider the Euclidean distance from the location.\nDue to limitations (See below), it’s best suited for overall analyses of catchment areas.\nOften the level of detail in the method has been increased by dividing the catchment area into different rings depending on the distance to the station.\n\nExample: By applying weights for each ring it is possible to take into account that the expected share of potential travelers at a train station will drop when the distance to the stop is increased.\n\n\n\nLimitation: Does not take the geographical surroundings into account.\n\nExample: In most cases, the actual walking distance to/from a location is longer than the Euclidean distance since there are natural barriers like rivers, buildings, rail tracks etc.\n\nThis limitation is often coped with by applying a detour factor that reduces the buffer distance to compensate for the longer walking distance.\nHowever, in cases where the length of the detours varies considerably within the location’s surroundings, this solution is not very precise.\nFurthermore, areas that are separated completely from a location, e.g. by rivers, might still be considered as part of the location’s catchment area\n\n\nUse Case: Ascertain Travel Potential to Determine Potential Station Locations\n\nEvery 50m along the proposed transit line, calculate the travel potential for that buffer area\n\nUsing the travel demand data for that buffer area, calculate travel potential\n\nTravel Potential Graph\n\n\nLeft side represents the transit line.\nRight Side\n\nY-Axis are locations where buffer areas were created.\nX-Axis: Travel Potential\n\nNot sure if that is just smoothed line with a point estimate of Travel Potential at each location or how exactly those values are calculated.\n\n50m isn’t a large distance so maybe all the locations aren’t shown on the Y-Axis and the number of calculations produces an already, mostly, smooth line on it’s own.\n\nPartitioning a buffer zone into rings or some kind of interpolation could provided more granular estimates around the central buffer location.",
    "crumbs": [
      "Geospatial",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/geospatial-remote-sensing.html",
    "href": "qmd/geospatial-remote-sensing.html",
    "title": "Remote Sensing",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Geospatial",
      "Remote Sensing"
    ]
  },
  {
    "objectID": "qmd/geospatial-remote-sensing.html#sec-geo-rs-misc",
    "href": "qmd/geospatial-remote-sensing.html#sec-geo-rs-misc",
    "title": "Remote Sensing",
    "section": "",
    "text": "Resources\n\nGLCM Texture: A Tutorial\n\nTypes of Measures\n\nTexture - Descriptive statistic that measures spatial relationships\n\nValues cannot be transferred from one situation to another\n\ne.g. you can’t say, “forests always have Contrast values between .5 and .7”\n\nPrimarily useful in comparing one part of an image to another part\n\nFor multi-image comparison (e.g. mosaic):\n\nThe images analysed must be equivalent radiometrically, in regards to sun angle, and phenologically with regards to cyclically variable ground phenomena\n\n\n\nSpectral - Descriptive statistics that essentially measure chemical properties of the ground objects\n\nSpectral and spatial are very likely to be independent data and so complement one another\nGrey Level Co-occurrence Matrix (GCLM) - Used for texture measurements. A tabulation of how often different combinations of pixel brightness values (grey levels) occur in an image.\nPCA Issues\n\nEach new dataset requires recalculation of both, landscape metrics and principal components analysis (PCA)\nHighly correlated landscape metrics are used\nPCA results interpretation is not straightforward\n\nInformation Theory (IT) Based Metrics\n\nMarginal entropy [H(x)] - Diversity (composition) of spatial categories - from monothematic patterns to multithematic patterns\nRelative Mutual Information [U] - Clumpiness (configuration) of spatial categories from fragmented patterns to consolidated patterns)\nH(x) and U are uncorrelated\nIssues\n\nRelative mutual information is a result of dividing mutual information by entropy. What to do when the entropy is zero?\nHow to incorporate the meaning of categories into the analysis?",
    "crumbs": [
      "Geospatial",
      "Remote Sensing"
    ]
  },
  {
    "objectID": "qmd/geospatial-remote-sensing.html#sec-geo-rs-terms",
    "href": "qmd/geospatial-remote-sensing.html#sec-geo-rs-terms",
    "title": "Remote Sensing",
    "section": "Terms",
    "text": "Terms\n\nNormalized Difference Vegetation Index (NDVI) - A widely-used metric for quantifying the health and density of vegetation using sensor data. It is calculated from spectrometric data at two specific bands: red and near-infrared. The spectrometric data is usually sourced from remote sensors, such as satellites.\n\nRange: -1 and 1\nInterpretation\n\n0: Area has nothing growing (e.g. Deserts)\n1: Arean has dense, healthy vegetation\n&lt;0: Suggest lack of dry land (e.g. oceans have NDVI = -1)\n\n\nSemantic Segmentation - The process of labelling pixels or regions of the image\n\nEssential in many applications including infrastructure planning, land cover, humanitarian crisis maps and environmental assessments.",
    "crumbs": [
      "Geospatial",
      "Remote Sensing"
    ]
  },
  {
    "objectID": "qmd/geospatial-remote-sensing.html#sec-geo-rs-patt",
    "href": "qmd/geospatial-remote-sensing.html#sec-geo-rs-patt",
    "title": "Remote Sensing",
    "section": "Pattern-based",
    "text": "Pattern-based\n\nEnables spatial analyses of raster data such as searching, change detection, clustering, or segmentation\nMisc\n\nNotes from Analysis of Spatial Patterns: Current State and Future Challenges (Slides)\n\nUse-Cases\n\nFinding similar spatial structures (one-to-many)\n\n\nTake the normalized cove of the mountain (or other structure) and compare other local areas with it. (i.e. which coves are least dissimilar the mountain cove)\n\nQuantitative assessment of changes in spatial structures (one-to-one)\n\n\nThis pic represents the change in land coverage in the Amazon from two different time periods\nTake the normalized coves from the earlier time period and make a one-to-one comparison (i.e. Calculate the difference in JSDs) with the coves of the currrent time period\nAreas with the greatest change have the highest JSD values.\n\nClustering similar spatial structure (many-to-many)\n\n\nCluster the normalized coves\nMetrics\n\nIntra-cluster heterogeneity - determines distances between all landscapes within a group\nInter-cluster isolation - determines distances between a given group and all others\n\n\n\nSteps\n\nDivide data into a large number of smaller areas (local landscapes)\nRepresent each area using a statistical description of the spatial pattern - a spatial signature.\n\nMost landscape metrics are single numbers representing specific features of a local landscape. Spatial signatures, on the other hand, are multi-element representations of landscape composition and configuration.\nThe basic signature is the co-occurrence matrix:\n\n\n\n\nagriculture\nforest\ngrassland\nwater\n\n\n\n\nagriculture\n272\n218\n4\n0\n\n\nforest\n218\n38778\n32\n12\n\n\ngrassland\n4\n32\n16\n0\n\n\nwater\n0\n12\n0\n2\n\n\n\n\nLand Coverage Categories: agriculture, forest, grassland, water, wetland, settlement, shrubland, sparse vegetation, bare area.\nLandform Categories: flat or nearly flat plains, smooth plains with some local relief, irregular plains with moderate relief, irregular plains with low hills, scattered moderate hills, moderate hills, scattered high hills, high hills, scattered low mountains, low mountains, scattered high mountains, high mountains, tablelands with moderate relief, tablelands with considerable relief, tablelands with high relief, tablelands with very high relief, surface water.\nI believe this is a comparison of two local landscapes where, for example, ag vs grass = 4 indicates there are 4 grid cells that coincide to a grassland in one local area and an agricultural area in the other local area.\n\nA spatial signature should allow simplification to the form of a normalized vector\nNormalized Co-Occurence Vector:\n\nCo-Occurence Vector (cove) - c(272, 218, 4, 0, 218, 38778, 32, 12, 4, 32, 16, 0, 0, 12, 0, 2)\n\nNumbers are taken from the co-occurrence matrix (See above) where the rows are combined end-to-end to create a vector.\n\nSimplified Co-Occurence Vector (cove) - c(136 , 218, 19389, 4, 32, 8, 0, 12, 0, 1)\n\nThe process name wasn’t mentioned in the slide, but here, the cove has been simplified by creating a vector with the halved diagonal values and unique values of off-diagonal cells in the original cove (Doubt order matters).\n\nNormalized Co-Occurence Vector\nsimple_cove &lt;- c(136, 218, 19389, 4, 32, 8, 0, 12, 0, 1)\nmoose &lt;- as.matrix(simple_cove)\nround(simple_cove/norm(moose), 4)\n#&gt; [1] 0.0069 0.0110 0.9792 0.0002 0.0016 0.0004 0.0000 0.0006 0.0000 0.0001\n\n\nSpatial signatures can be compared using a large number of existing distance or dissimilarity measures\n\nDissimilarity\n\nMeasuring the distance between two signatures in the form of normalized vectors allows determining dissimilarity between spatial structures.\nExample: Jensen-Shannon Divergence - Lower\n\n\n\n\n\n\n\n\nReference (cove_ref)\n\n\n\n\n\n\n\nArea of Interest (cove_x1)\n\n\n\n\n\n\ncove_ref &lt;- c(0.0069, 0.011, 0.9792, 0.0002, 0.0016, 0.0004, 0, 0.0006, 0, 0.0001)\ncove_x1 &lt;- c(0.1282, 0.0609, 0.8105, 0.0002, 0.0002, 0.0001, 0, 0, 0, 0)\ncove_mat1 &lt;- rbind(cove_ref, cove_x1)\n\nphilentropy::JSD(cove_mat)\n#&gt; jensen-shannon \n#&gt;     0.06826663\n\nA lower JSD means the two images are less dissimilar (i.e more similar)\n\nExample: Jensen-Shannon Divergence - Higher\n\n\n\n\n\n\n\n\nReference (cove_ref_ext)\n\n\n\n\n\n\n\nArea of Interest (cove_x1)\n\n\n\n\n\n\n# zeros added to match length of cov_x2\ncove_ref_ext &lt;- c(0.0069, 0.011, 0.9792, 0.0002, 0.0016, 0.0004, 0, 0.0006, 0, 0.0001, 0, 0, 0, 0, 0)\ncove_x2 &lt;- c(0.2033, 0.1335, 0.2944, 0.1747, 0.0562, 0.1307, 0.0035, 0.0002, 0.0004, 0.0015, 0.0007, 0.0005, 0, 0, 0.0005)\ncove_mat2 &lt;- rbind(cove_ref_ext, cove_x2)\n\nphilentropy::JSD(cove_mat2)\n#&gt; jensen-shannon \n#&gt;      0.4444198 \n\nA higher JSD means the two images are more dissimilar.\n\n\n\n\n\n\n\n\nReference (cove_ref)\nArea of Interest (cove_x1)\nReference (cove_ref_ext)\nArea of Interest (cove_x1)",
    "crumbs": [
      "Geospatial",
      "Remote Sensing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html",
    "href": "qmd/geospatial-processing.html",
    "title": "Processing",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Geospatial",
      "Processing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-misc",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-misc",
    "title": "Processing",
    "section": "",
    "text": "Beware statistical computations of tibbles/sf_tibbles with geometry columns\n\nCould result in an expensive union operation over identical geometries and an R session crash\n\nExample with 100K rows crashed R.\n\nNotes from thread\nOption 1 (slower): Set do_union = FALSE in summarize\ntx_income_groups &lt;- \n  get_acs(\n    geography = \"tract\",\n    table = \"B19001\",\n    state = \"TX\",\n    year = 2020,\n    geometry = TRUE\n  ) |&gt; \n  filter(variable != \"B19001_001\") |&gt; \n  mutate(bracket = case_when(\n    variable &gt; \"B19001_012\" ~ \"Above $100k\",\n    TRUE ~ \"Below $100k\"\n  )) |&gt; \n  group_by(GEOID, bracket) |&gt; \n  summarize(n_households = sum(estimate, na.rm = TRUE),\n            do_union = FALSE)\nOption 2 (faster): Perform calculation without geometries then join\ntx_tracts &lt;- tracts(\"TX\", cb = TRUE, year = 2020) |&gt; \n  select(GEOID)\n\ntx_income_groups &lt;- \n  get_acs(\n    geography = \"tract\",\n    table = \"B19001\",\n    state = \"TX\",\n    year = 2020,\n    geometry = TRUE\n  ) |&gt; \n  filter(variable != \"B19001_001\") |&gt; \n  mutate(bracket = case_when(\n    variable &gt; \"B19001_012\" ~ \"Above $100k\",\n    TRUE ~ \"Below $100k\"\n  )) |&gt; \n  group_by(GEOID, bracket) |&gt; \n  summarize(n_households = sum(estimate, na.rm = TRUE))\n\ntx_income_groups &lt;- tx_tracts |&gt; \n  left_join(tx_income_groups, by = \"GEOID\")\n\n{tidycensus} has an arg to bypass d/ling the geometries, geometry = FALSE and a separate tracts function to get the census tract geometries",
    "crumbs": [
      "Geospatial",
      "Processing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-filtyp",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-filtyp",
    "title": "Processing",
    "section": "File Types",
    "text": "File Types\n\nShape Files\n\nD/L and Load a shapefile\nMay need API key from Census Bureau (see {tigris} docs)\nExample: Counties in California\ntbl &lt;- tigris::counties(state = \"CA\") %&gt;%\n    st_set_crs(4326)\n{tigris} - US data\nlibrary(tigris)\n\nus_states &lt;- states(resolution = \"20m\", year = 2022, cb = TRUE)\n\nlower_48 &lt;- us_states %&gt;%\n  filter(!(NAME %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")))\n{rnaturalearth} - World data\n# Via URL\n# Medium scale data, 1:50m Admin 0 - Countries\n# Download from https://www.naturalearthdata.com/downloads/50m-cultural-vectors/\nworld_map &lt;- read_sf(\"ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp\") %&gt;%\n  filter(iso_a3 != \"ATA\")  # Remove Antarctica\n\n# Via Package\nlibrary(rnaturalearth)\n\n# rerturnclass = \"sf\" makes it so the resulting dataframe has the special\n# sf-enabled geometry column\nworld_map &lt;- ne_countries(scale = 50, returnclass = \"sf\") %&gt;%\n  filter(iso_a3 != \"ATA\")  # Remove Antarctica\n\nGeoJSON\n\nWrite data to geojson\ndata %&gt;%\n    st_write(\"mb_shapes.geojson\")",
    "crumbs": [
      "Geospatial",
      "Processing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-proj",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-proj",
    "title": "Processing",
    "section": "Projections",
    "text": "Projections\n\nWGS 84\n\nGoogle “epsg code” + “your region name” to find a reasonable projection code to use\n\nStandard projection is 4326 aka WGS84 (required by leaflet)\nTransform shapefile\nmb_shapes &lt;- read_sf(download_folder)\nmb_shapes %&gt;%\n  st_transform(4326)\n\n\nTransform latitude and longitude then visualize\nnew_tbl &lt;- old_tbl # contains latitude and longitude variables\n    # convert to simple features object\n    sf::st_as_sf(\n        coords = c(\"&lt;longitude_var&gt;\", \"&lt;latitude_var&gt;\"), # order matters\n        crs = 4326 # standard crs\n    ) %&gt;%\n    mapviw::mapview()\nWGS 84 projection, which is what Google Maps (and all GPS systems) use\nus_states &lt;- us_states %&gt;% # df with geometries\n  sf::st_transform(st_crs(\"EPSG:4326\"))  # WGS 84\nNAD83, Albers, Mercator, Robinson\n\nlibrary(patchwork)\n\np1 &lt;- ggplot() +\n  geom_sf(data = lower_48, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"EPSG:4269\")) +  # NAD83\n  labs(title = \"NAD83 projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np2 &lt;- ggplot() +\n  geom_sf(data = lower_48, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  labs(title = \"Albers projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np3 &lt;- ggplot() +\n  geom_sf(data = world_map, fill = \"#FF4136\", color = \"white\", linewidth = 0.1) +\n  coord_sf(crs = st_crs(\"EPSG:3395\")) +  # Mercator\n  labs(title = \"Mercator projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np4 &lt;- ggplot() +\n  geom_sf(data = world_map, fill = \"#FF4136\", color = \"white\", linewidth = 0.1) +\n  coord_sf(crs = st_crs(\"ESRI:54030\")) +  # Robinson\n  labs(title = \"Robinson projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\n(p1 | p2) / (p3 | p4)",
    "crumbs": [
      "Geospatial",
      "Processing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-py",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-py",
    "title": "Processing",
    "section": "Python",
    "text": "Python\n\nExample: Filter Data based on a polygon using latitude and longitude data\n\nGet California’s polygon\nimport osmnx\nimport geopandas as gpd\n\nplace = \"California, USA\"\ngdf = osmnx.geocode_to_gdf(place)\n# Get the target geometry\ngdf = gdf[[\"geometry\", \"bbox_north\", \"bbox_south\", \"bbox_east\", \"bbox_west\"]]\nFilter data according the polygon geometry\nfrom shapely.geometry import Point\n\n# Convert to a GeoDataFrame with Point geometry\ngeometry = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])]\nearthquake_gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:4326')\n\n# Filter to keep only points within the California bounding box\npoints_within_california = gpd.sjoin(earthquake_gdf, gdf, how='inner', predicate='within')\n\n# Select latitude, longitude etc. columns\ndf = points_within_california[['id', 'Latitude', 'Longitude', 'datetime', 'properties.mag']]\n\nLatitude and longitude are converted to point geometry to match the polygon point geometry\nAn inner join is used on the data and california polygon to get the points that are only in California.",
    "crumbs": [
      "Geospatial",
      "Processing"
    ]
  },
  {
    "objectID": "qmd/glossary-ds-terms.html",
    "href": "qmd/glossary-ds-terms.html",
    "title": "Glossary: DS terms",
    "section": "",
    "text": "200 Status - An API serving an ML model returns a HTTP 200 OK success status response code indicates that the request has succeeded.\nAMI - amazon machine image. Thing that has R and the main packages you need to load onto the cloud server\nAnti-Patterns - certain patterns in software development that are considered bad programming practices.\n\nAs opposed to design patterns which are common approaches to common problems which have been formalized and are generally considered a good development practice, anti-patterns are the opposite and are undesirable.\n\nArm - a group of patients receiving a specific treatment (or no treatment). Trials involving several arms, or randomized trials, treat randomly-selected groups of patients with different therapies in order to compare their medical outcomes. Experimental arms, which receive an experimental drug, are compared with control arms. Single-arm or non-randomized trials, in which everyone enrolled in a trial receives the experimental therapy\nArtifacts - objects that are created as a result of a process. e.g. model objects, cleaned data sets, visuals, etc.\nAsynchronous Programming - code runs (or must run) after something else happens and also not sequentially (e.g. when a function calls a callback function in JS).\nAthena - amazon query service that works with S3. Best for analyses using kubernetes. ODBC drivers are best with interactive app\nB2C, B2B - business-to-consumer, business-to-business, describes a business that’s end-product is being sold to a consumer or a business.\nBalanced Design (aka orthogonal) has an equal number of observations for all possible level combinations. For example in an experiment where gender is an independent variable, an equal number of males receive the treatment as do females receive treatment. If the male/female counts were unequal, then the experiment is unbalanced.\n\nStat tests have greater power for balanced designs\nTest stat less susceptible to to small departures from the assumption of equal variances (homoscedasticity).\n\nBatch - collect a large number of data points, process them periodically and store results somewhere (contrasts with real-time in which a data input leads to an immediate prediction)\nBootstrapping (CS) - usually applies to a situation where a system depends on itself to start, sort of a chicken and egg problem. (e.g. How do you start an OS initialization process if you don’t have the OS running yet?) Typically a simple file that starts a large process.\nBounce, Email - When an email cannot be delivered to an email server.\n\nHard Bounce - indicates a permanent reason an email cannot be delivered (e.g. Recipient email address doesn’t exist; Recipient email server has completely blocked delivery)\nSoft Bounce - indicates a temporary delivery issue (for details on the reasons, see link)\n\nBounce Rate - the percentage of visitors to a particular website who navigate away from the site after viewing only one page. Low bounce rate can indicate the landing page needs improvement\nBPI - Business process improvement is a management exercise in which enterprise leaders use various methodologies to analyze their procedures to identify areas where they can improve accuracy, effectiveness and/or efficiency and then redesign those processes to realize the improvements.\nBLUE - best linear unbiased estimator, e.g. regression line\nCAC - customer acquisition cost - measures how much an organization spends to acquire new customers. The total cost of sales and marketing efforts, as well as property or equipment, needed to convince a customer to buy a product or service.\nCapEx - Capital Expenditure - 1 of 2 main forward budgeting mechanisms for a corporation (also see OpEx). Often used to undertake new projects or investments or large-scale asset acquisitions (buildings and vehicles)\nClinical Trial - research studies (e.g. RCT) performed in people that are aimed at evaluating a medical, surgical, or behavioral intervention\nCDI - Customer Data Infrastructure - built to collect behavioral data from primary or first-party data sources, but some solutions also support a handful of secondary data sources (third-party tools)\nCDP - Customer Data Platform - add-ons from CDI vendors; a layer on top of CDI that offers a set of capabilities to analyze data using a visual interface.\nCDN - content delivery network - a system of distributed servers (network) that deliver pages and other web content to a user, based on the geographic locations of the user, the origin of the webpage and the content delivery server.\nCLV/CLTV - Customer Lifetime Value - how much money a customer will bring your brand throughout their entire time as a paying customer.\nCOGS - Cost of goods sold (aka Cost of Sales) - refers to the direct costs of producing the goods sold by a company. This amount includes the cost of the materials and labor directly used to create the good. It excludes indirect expenses, such as distribution costs and sales force costs.\nComplete Factorial Design - a research study involving two or more independent variables in which every possible combination of the levels of each variable is represented. For instance, in a study of two drug treatments, one (A) having two dosages and the other (B) having three dosages, a complete factorial design would pair the dosages administered to different individuals or groups of participants as follows: A1 with B1, A1 with B2, A1 with B3, A2 with B1, A2 with B2, and A2 with B3.\nCPG - Consumer packaged goods are items used daily by average consumers that require routine replacement or replenishment, such as food, beverages, clothes, tobacco, makeup, and household products.\nCPC - Cost Per Click - refers to the cost an advertiser pays each time an online user clicks on his or her digital ad\nCRM - customer relationship management i.e. customer service. Salesforce tracks this data. Example: what features your salesperson promised, and when? How much revenue you have from each customer? Or which salesperson sold the most in the past year?\ncron- standard tool used on Unix and Unix-like systems to schedule the periodic execution in the background of a command or script (like a batch script)\nCrossed Factors - when every category of one factor co-occurs in the design with every category of the other factor. In other words, there is at least one observation in every combination of categories for the two factors. (in contrast to “nested factors”). As a consequence, interaction terms involving these two factors is allowed.\nCrossover Study - A type of clinical trial in which the study participants receive each treatment in a random order. With this type of study, every patient serves as his or her own control. Crossover studies are often used when researchers feel it would be difficult to recruit participants willing to risk going without a promising new treatment.\nCross-Section Data - randomly sampled data from a population. Like a survey. Aka observational data. See experimental data for comparison.\n\nPooled - differs from panel data in that it is observations of different subjects (instead of the same subjects) in different time periods.\nRolling - both the presence of an individual in the sample and the time at which the individual is included in the sample are determined randomly.\n\nCross-Tabs - section of survey analysis where the aggregated results are broken down by demography, party affiliation, etc.\nCTA - marketing term, call-to-action. any device designed to prompt an immediate response or encourage an immediate sale; words or phrases that can be incorporated into sales scripts, advertising messages or web pages that encourage consumers to take prompt action\nCTR - click through rate: the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement. It is commonly used to measure the success of an online advertising campaign for a particular website as well as the effectiveness of email campaigns.\nCRM - Customer Relationship Management - acquiring new customers but especially about retaining existing ones\nDAU - daily active users, ex: daily avg # of registered users of the site over past 30 days\nDBA - Database Administrator is an admin role that understands the particular database technology and how to get the best out of it. This includes improving performance, backups and recovery.\nDDL - Data definition or description language - Subset of SQL. Used to:\n\nKeep a snapshot of the database structure\nSet up a test system where the database acts like the production system but contains no data\nProduce templates for new objects that you can create based on existing ones. For example, generate the DDL for the Customer table, then edit the DDL to create the table Customer_New with the same schema.\n\nDesparate Impact Analysis - Analysis of the result of the application of a standard, requirement, test or other screening tool used for selection that—though appearing neutral—has an adverse effect on individuals who belong to a legally protected class Differential Dropout**]{style=‘color: #009499’} - Differing dropout rates between treatment arms\nDMA - Designated Market Area; a geographic region where Nielsen, the ratings company, analyzes and quantifies how television is viewed. Residents can receive the same local TV and radio stations\nDNS -  Domain Name System**]{style=‘color: #009499’} -  translates domain names to IP addresses so browsers can load Internet resources.\nDSL - domain-specific language - a computer language specialized to a particular application domain\nEMR - Amazon version of a spark cluster used for big data processing and analysis.\nEndogenous - A model variable is correlated with other variables excluded from the model (omitted variable bias). Determined by measuring the correlation between the variable and residuals of the model. If a predictor variable hasn’t been randomly assigned, it’s likely to be endogenous.\nEquitability - concept that says a dependence measure should give equal importance to linear and nonlinear relationships. Consistent strength measurements across different variable relationships that have similar amounts of noise.\nERP - enterprise resource planning, sort of a catch-all for manufacturing, supply-chain, etc, see the wiki\nETL - extract, transfer, load - usually refers to transferring data from one location to another\nEndpoint (biostats) - Outcome variable measured in a medical study. e.g. Death, stroke, or quality of life are good endpoints. Blood tests and images on scans are not good endpoints.\n\nA composite endpoint is one that consists of two or more events\n\nExample: death due to cardiovascular causes or hospitalization due to heart failure\n\nSo the binary outcome would be a 1 if either of those events took place or a 0 if they did not. Or in a survival model, time until either of those events.\n\n\n\nEOF - End of file - Input from a terminal never really “ends” (unless the device is disconnected), but it is useful to enter more than one “file” into a terminal, so a key sequence is reserved to indicate end of input.\nex ante - based on assumption and prediction and being essentially subjective and estimative\nex post - based on knowledge and retrospection and being essentially objective and factual\nExperimental Data - data from a RCE/RCT. Compare with observational data\nFaaS - Function as a service - type of cloud service for developing, running, and managing apps (e.g. AWS Lambda)\nFactorial Design - Experiment where you’re interested in the effect of two or more independent variables.\nFraud Rules - fraud scores are calculated based on rules, which add or subtract points. The user action may be a transaction, signup or login. Rules look at data points such as an email address, IP address, or social media presence.\nFraud Score - assigned values to how risky a user action is. Scoring determined by fraud rules.\nFuzzy Design - See Sharp Design\nGHA - Github Actions\nGMV - Gross merchandises value - the total value of merchandise sold over a given period of time through a customer-to-customer (C2C) exchange site\nGRP - Gross Rating Point. A standard measure in advertising, it measures advertising impact. You calculate it as a percent of the target market reached multiplied by the exposure frequency. Thus, if you get advertise to 30% of the target market and give them 4 exposures, you would have 120 GRP.\nHTE - Heterogeneous Treatment Effect - Also called differential treatment effect, includes difference of means, odds ratios, and Hazard ratios for time-to-event outcome vars\n\nAscertaining subpopulations for which a treatment is most beneficial (or harmful) is an important goal of many clinical trials.\nOutcome heterogeneity is due to wide distributions of baseline prognostic factors. When strong risk factors exist, there is hetergeneity in the outcome variable.\n\nSolution: add baseline predictors to your model that account for these strong risk factors.\n\nHeterogeneity of Treatment Effects - The degree to which different treatments have differential causal effects on each unit.\n\nHit Ratio - percent of records that were read in order to complete a query in a database. Cloud db providers often charge by the number of records searched\nHomogeneity of Treatment Effects - See Heterogeneity of Treatment Effects\nHPC - High Performance Computing\nHoneypot - data (for example, in a network site) that appears to be a legitimate part of the site, but is actually isolated and monitored, and that seems to contain information or a resource of value to attackers, who are then blocked.\nIaaS - infrastructure-as-a-service ( Hardware is provided by an external provider and managed for you)\nIAM - identity and access management, keys and passwords etc\nIRB - institutional review board, reviews studies ethical and moral issues\nITT - Intent-to-Treat analysis includes all randomized patients in the groups to which they were randomly assigned, regardless of their adherence with the entry criteria, regardless of the treatment they actually received, and regardless of subsequent withdrawal from treatment or deviation from the protocol. Avoids overoptimistic estimates of the efficacy of an intervention resulting from the removal of non-compliers by accepting that noncompliance and protocol deviations are likely to occur in actual clinical practice. So mimics likely situation in the real world, but not good for estimating the causal effect of a treatment.\nKernels - (article) - system kernels - the interface between the operating system, i.e. the software, and the hardware components in a device. It is used in all devices with an operating system, for example, computers, laptops, smartphones, smartwatches, etc.\n\nWhen we use a program on a computer, such as Excel, we handle it on the so-called Graphical User Interface (GUI). The program converts every button click or other action into machine code and sends it to the operating system kernel. If we want to add a new column in an Excel table, this call goes to the system core. This in turn passes the call on to the computer processing unit (CPU), which executes the action.\nJupyter Kernels - an engine that executes notebook code and is specific to a particular programming language (e.g. python kernel)\nKaggle Kernels - a free platform from Kaggle to run Jupyter notebooks in the browser. Advantage is that you don’t have to set-up an environment locally.\n\nKPI- key performance indicator\nKYC - Know-Your-Customer is info a company collects to verify your identity to combat fraud. Used by telecoms and financial services\nLazy Evaluation - ” never pulls data into R unless you explicitly ask for it. It delays doing any work until the last possible moment. It collects together everything you want to do and then sends it to the database in one step.”\nLikelihood - probability of seeing this data given a specific value for a distribution parameter (eg mean, sd). Goal is to search for parameter values until the likelihood is maximized.\nLOB - Line of Business is a general term which refers to a product or a set of related products that serve a particular customer transaction or business need. (i.e. product categories)\n\nExamples\n\nConsumer Banking: credit cards, line of credit or loan program, mortgages, and corporate, small business and personal bank accounts.\nFinancial services and brokerages: mergers and acquisitions or partnerships, real estate investments, and wealth management\nProperty and casualty insurance companies: property and casualty insurance (i.e., homeowners, car, boat, renters, etc.), life insurance, health insurance, and commercial business insurance.\n\nSub-lines of Business would be sub-categories within each LOB\n\nLongitudinal Data - see panel data\nLTV - see CLV/CLTV\nManual Review - A human is reviews the case to determine whether action is needed. In fraud, an model output may trigger a “manual review” to determine whether an event was indeed fraudulent.\nMLlib - Apache Spark machine learning library\nMVC - Minimum Viable Corpus - a data size threshold; such that below this threshold, the data simply isn’t useful/valuable. Used in data products business.\nMVP - minimum viable project, agile term. Version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort\nNamespace - allows you to use two functions with the same name but from different packages, e.g. dplyr::select or in general, package::function. https://stackoverflow.com/questions/3384204/what-are-namespaces/3384384#3384384\nNNH - Numbers Needed to Harm - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a particular adverse outcome. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNNT - Numbers Needed to Treat - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a favorable outcome such as treatment response. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNPS - Net Promoter Score - a measure of customer loyalty. Widely used market research metric that typically takes the form of a single survey question asking respondents to rate the likelihood that they would recommend a company, product, or a service to a friend or colleague.\nNRT - near real-time, aka streaming data\nObservational Data - see cross sectional data\nOEM - original equipment manufacturer\nOKR - Objectives and Key Results is a popular management strategy for goal setting within organizations. A framework for turning strategic intent into measurable outcomes for an organization.\nOnline Machine Learning - A method of machine learning where the model incrementally learns from a stream of data points in real-time. It’s a dynamic process that adapts its predictive algorithm over time, allowing the model to change as new data arrives.\nOn-Prem - on-premises — working with servers in the the building and not in the cloud.\nOOD - out-of-distribution - data which differ from the training data and on which a model might underperform\nOpen Cohort - subjects can leave or be added over time.\nOpEx - Operational Expenditures - 1 of 2 main forward budgeting mechanisms for a corporation (also see CapEx). Relates to day-to-day expenses (such as payroll and software subscriptions). Smaller payouts over time.\nOpportunity Sizing - Quantitative analysis to select a subset of ideas to which to devote resources in product development\nNested Factors - happens when all the levels of one factor only occur in combination with one level of another factor (in contrast to “crossed factors”). As a consequence, your model can’t have an interaction term involving these two variables.\nP&L - Profit and Loss Statement Panel data - cross section data with a time element. Repeated measures of the same subject over time. Synonym for Longitudinal Data\nParcel - a land record that defines the boundary of a piece of land. These boundaries are the basic administrative unit of local government in regards to land and property. Managing ownership and tax records are the primary reason local governments generate these files. So these are boundaries differentiating ownership of properties.\nPEP8 - style guide for python\nPI - principal investigator\nPivot Table - Excel name for a group_by %\\&gt;% summarize calculation\n\ne.g. from a table of individual fruit sales: group_by(fruit_type, country) %\\&gt;% summarize(total_amt = sum(amount))\n\nPLG - Product-led growth is an end user-focused growth model that relies on the product itself as the primary driver of customer acquisition, conversion, and expansion. e.g. open source a product, let the customer go through the documentation and use and experiment with the product on their own time. In contrast to sales pitching a product to a customer and letting them use it for a trial basis.\nPM - product manager\nPoC - Proof of Concept\nPOS - point of sale, The point of sale or point of purchase is the time and place where a retail transaction is completed. It can be in a physical store, where POS terminals and systems are used to process card payments or a virtual sales point such as a computer or mobile electronic device.\nRCE - randomized controlled experiment, subjects randomly assigned to two groups, treatment and control. Double blind means the researcher doesn’t know who is in which group.\nRCT - randomized clinical trial\nRDD - Regression discontinuity design\nRedis - REmote DIctionary Server - is an in-memory, key-value database, commonly referred to as a data structure server. Used when volume of read and write operations exceed the capabilities of traditional databases. With Redis’s capability to easily persist the data to disk, it is a superior alternative to the traditional memcached solution for caching.\nRefactoring - updating or optimizing code\nRegression Testing - checks if changes made to a system negatively impacted or broke any of the existing features. It is often performed right after each update or commit to the code base to identify new bugs and ensure that your system works properly.\nRFI - Request for Information - Used to collect written information about the capabilities of various suppliers. Normally it follows a format that can be used for comparative purposes. An RFI is primarily used to gather information to help make a decision on what steps to take next. RFIs are therefore seldom the final stage and are instead often used in combination with request for proposal (RFP), request for tender (RFT), and request for quotation (RFQ).\nRFM - recency, frequency, monetary value - method of estimating customer value; common in retail\nRFP - Request for Proposal - A document that an organization, often a government agency or large enterprise, posts to elicit a response -- a formal bid -- from potential vendors for a desired solution. The RFP specifies what the customer is looking for and describes each evaluation criterion on which a vendor’s proposal will be assessed.\n\nROAS - return on ad spend\nRUG - Regional User Group\nS3 - Amazon simple storage service, database\nSaaS - Software-as-a-service is a mechanism through which companies offer the functionality of their apps, which remain on their company servers, to other companies or customers.\nSCO - sales cycle optimization, active process of providing content on your site (and beyond) that speaks to each of the key phases\nSEO - Search engine optimization, generating high page rankings for key search terms\nSDK - software development kit\nSharp Design - Each individual or group receives the same “amount” of treatment (e.g. a state law or medication dosage). Opposite being fuzzy design (?)\nSKU - Stock Keeping Unit**]{style=‘color: #009499’} - Usually a bar code that has all the information to distinguish it from another product. These attributes can include manufacturer, description, material, size, color, packaging, and warranty terms. When a business takes inventory of its stock, it counts the quantity it has of each SKU.\nSLA - service level agreement - a contract between a service provider and its internal or external customers that documents what services the provider will furnish and defines the service standards this provider is obligated to meet. service. Important for holding prediction latency of an app to a certain standard or maintaining data reliability with vendors. (see link for more details on SLA, SLO, and SLI)\nSLI - service level indicators - metrics that measure compliance with an SLO (see link for more details on SLA, SLO, and SLI)\nSLO - service level objectives - objectives your team must meet in order to meet the conditions of the SLA (see link for more details on SLA, SLO, and SLI)\nSME - Subject Matter Experts\nSPC - Statistical process control is a method of quality control which employs statistical methods to monitor and control a process\nSpill - missed opportunity metric, measures “lost trading days” on which flights or hotels filled too quickly (the result of pricing too low)\nSpoil - missed opportunity metric, measures empty seats or rooms (often the result of pricing too high)\nSSH - secure shell is a cryptographic Network protocol for operating Network Services securely over an unsecured Network. Typical applications include remote command line login in remote command execution\nstdout - standard output, which is the terminal by default\nTDD - Test-driven development is a style of programming where coding, testing, and design are tightly interwoven\nTF-IDF- stands for term frequency-inverse document frequency, and is often used in information retrieval and text mining.\nThroughput - the amount of material or items passing through a system or process.\ntx - treatment, seen as variable with different treatments as values\nURI - Uniform Resource Identifier - a string of characters that unambiguously identifies a particular resource. e.g. s3//bucket/path/to/folder or http://127.0.0.1:5000or c:\\Users\\me\\path\\to\\folder\nUTM - Urchin Traffic Monitor - used to identify marketing channels\n\ne.g. http://yourwebsite.com/your-post-title/?utm_source=google\n\nutm code = string after “?”\n\nThis person clicked a google ad to get to your site\n\nName comes from Urchin Tracker, a web analytics software that served as the base for Google Analytics.\n\nVPS - virtual private server\nWIP - Work-in-Progress\nWithin Person Study - multiple treatments on each person either all in the same period or different treatments in different periods\nYear-Over-Year - used to make comparisons between one time period and another that is one year earlier.\n\nFormula (percentage): (value_this_year / value_previous_year) - 1\nExample: (sales_Jul_2023 / sales_Jul_2022) - 1",
    "crumbs": [
      "Glossary: DS terms"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html",
    "href": "qmd/google-bigquery.html",
    "title": "BigQuery",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-misc",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-misc",
    "title": "BigQuery",
    "section": "",
    "text": "Also see\n\nFinOps: Four Ways to Reduce Your BigQuery Storage Cost\n\n{bigrquery}\nI think query sizes under a 1TB are free\n\nif you go above that, then it’s cheaper to look at flex spots\n\nData Manipulation Language (DML) - Enables you to update, insert, and delete data from your BigQuery tables. (i.e. transaction operations?) (docs)\nBigQuery vs Cloud SQL\n\nCloud SQL is a service where relational databases can be managed and maintained in Google Cloud Platform. It allows its users to take advantage of the computing power of the Google Cloud Platform instead of setting up their own infrastructure. Cloud SQL supports specific versions of MySQL, PostgreSQL, and SQL Server.\nBigQuery is a cloud data warehouse solution provided by Google. It also comes with a built-in query engine. Bigquery has tools for data analytics and creating dashboards and generating reports. Cloud SQL does not have strong monitoring and metrics logging like BigQuery.\nBigQuery comes with applications within itself, Cloud SQL doesn’t come with any applications.\nCloud SQL also has more database security options than BigQuery.\nThe storage space in Cloud SQL depends on the db engine being used, while that of Bigquery is equivalent to that of Google cloud storage.\nPricing\n\nBoth have free tiers\nCloudSQL has 2 types: Per Use and Packages\n\nIf usage over 450 hours monthly, then packages is a good option\n\n\n\nLeft pic is Per Use pricing; Right pic is Package pricing\n\n\nBigQuery based on Usage",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-sqlfun",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-sqlfun",
    "title": "BigQuery",
    "section": "SQL Functions",
    "text": "SQL Functions\n\nUNNEST - BigQuery - takes an ARRAY and returns a table with a row for each element in the ARRAY (docs)\n\nGoogle, Analytics, Analysis &gt;&gt; Example 17",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-specexp",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-specexp",
    "title": "BigQuery",
    "section": "BQ Specific Expressions",
    "text": "BQ Specific Expressions\n\nNotation Rules\n\nSquare brackets [ ] indicate optional clauses.\nParentheses ( ) indicate literal parentheses.\nThe vertical bar | indicates a logical OR.\nCurly braces { } enclose a set of options.\nA comma followed by an ellipsis within square brackets [, … ] indicates that the preceding item can repeat in a comma-separated list.\n\nUsing EXCEPT within SELECT\n\nPIVOT for pivot tables\n\n\n\nWith more than 1 aggregate\nselect * from (select No_of_Items, Item, City from sale)\n    pivot(sum(No_of_Items) Total_num, AVG(No_of_Items) Avg_num\n    for Item in ('Laptop', 'Mobile'))\n\nUNPIVOT\n\nselect * from sale\nunpivot(Sales_No for Items in (Laptop, TV, Mobile))\n\nIt’s a pivot_longer function that puts columns, Laptop, TV, and Mobile, into Items and their values into Sales_No\nCollapse columns into fewer categories\n\nselect * from sale\n    unpivot(\n        (Category1, Category2)\n        for Series\n        in ((Laptop, TV) as 'S1', (Tablet, Mobile) as 'S2')\n    )\n\nColumns have been collapsed into 2 categories, S1 and S2\n\n2 columns for each category\n\nValues for each category gets its own column\n\n\nGROUP BY + ROLLUP\n\n\ntotal sales (where quarter = null) and subtotals (by quarter) by year\n\nQUALIFY\n\n\nAllows you to apply it like a WHERE condition on a column created in your SELECT statement because it’s evaluated after the GROUP BY, HAVING, and WINDOW statements\n\ni.e. a WHERE function that is executed towards the end of the order of operations instead of at the beginning\n\nUsing a WHERE instead of QUALIFY, the above query looks like this",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-vars",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-vars",
    "title": "BigQuery",
    "section": "Variables (aka Parameters)",
    "text": "Variables (aka Parameters)\n\nWays to create variables\n\nUsing a CTE\n\nBasically just using a CTE and calling it a variable or table of variables\n\nUsing BigQuery procedural language\n\n\n\nStatic values using CTE\n\n1 variable, 1 value\n  -- Input your own value\nWITH\n  variable AS (\n  SELECT 250 AS product_threshold)\n\n  -- Main Query\nSELECT\n  *\nFROM\n  `datastic.variables.base_table`,\n  variable\nWHERE\n  product_revenue &gt;= product_threshold\n\nCTE\n\n“variable” is the name of the CTE that stores the variable\n“product_threshold” is  set to 250\n\nQuery\n\nThe CTE is called in the FROM statement, then the “product_threshold” can be used in the WHERE expression\n\n\n1 variable, multiple values\n-- Multiple values\nWITH\n  variable AS (\n  SELECT\n    *\n  FROM\n    UNNEST([250,45,75]) AS product_threshold)\n\n-- Main Query\nSELECT\n  *,\n  'base_table_2' AS table_name\nFROM\n  `datastic.variables.base_table_2`,\n  variable\nWHERE\n  product_revenue IN (product_threshold);\n\nCTE\n\n“variable” is the name of the CTE that stores the variable\nUses SELECT, FROM syntax\nlist of values is unnested into the variable, “product_threshold”\n\nSee SQL Functions for UNNEST def\nUNNEST essentially coerces the list into a 1 column vector\n\n\nQuery\n\nThe CTE is called in the FROM statement, then the “product_threshold” can be used in the WHERE expression\n\nNot sure why parentheses are around the variable in this case\n\nTable is filtered by values in the variable\n\n\nMultiple variables with multiple values\n  -- Multiple variables as a table\nWITH\n  variable AS (\n  SELECT\n    product_threshold\n  FROM\n    UNNEST([\n        STRUCT(250 AS price,'Satin Black Ballpoint Pen' AS name), \n        STRUCT(45 AS price,'Ballpoint Led Light Pen' AS name), \n        STRUCT(75 AS price,'Ballpoint Led Light Pen' AS name)]\n        ) AS product_threshold)\n  -- Main Query\nSELECT\n  *\nFROM\n  `datastic.variables.base_table`,\n  variable\nWHERE\n  product_revenue = product_threshold.price\n  AND product_name = product_threshold.name\n\nAlso see (Dynamic values using CTE &gt;&gt; Multiple variables, 1 valuej) where “table.variable” syntax isn’t used\nCTE\n\n“variable” is the name of the CTE that stores the variable\nInstead of SELECT *, SELECT  is used\n\nnot sure if that’s necessary or not\n\nUNNEST + STRUCT coerces the array into 2 column table\n\nThe “price” and “name” variables each have multiple values\nEach STRUCT expression is a row in the table\n\n\nQuery\n\nThe CTE is called in the FROM statement, then the “product_threshold” can be used in the WHERE expression\nEach variable is accessed by “table.variable” syntax\nSurprised IN isn’t used and that you can do this with “=” operator\n\n\n\n\n\nDynamic values using CTE\n\nValue is likely to change when performing these queries with new data\n1 variable, 1 value\n\nExample: value is a statistic of a variable in a table\n  -- Calculate twice the average product revenue \nWITH\n  variable AS (\n  SELECT\n    AVG(product_revenue)*3 AS product_average\n  FROM\n    `datastic.variables.base_table`)\n\n-- Main Query\nSELECT\n  *\nFROM\n  `datastic.variables.base_table`,\n  variable\nWHERE\n  product_revenue &gt;= product_average\n\nFor basic structure, see (Static values using CTE &gt;&gt; 1 variable, 1 value)\nValue is calculated in the SELECT statement and stored as “product_average”\n\n\n1 variable, multiple values\n\nExample: current product names\nWITH\n  variable AS (\n  SELECT\n    product_name AS product_threshold\n  FROM\n    `datastic.variables.base_table`\n  WHERE\n    product_name LIKE '%Google%')\n\n-- Main Query\nSELECT\n  *\nFROM\n  `datastic.variables.base_table`,\n  variable\nWHERE\n  product_name IN (product_threshold)\n\nFor basic structure, see (Static values using CTE &gt;&gt; 1 variable, multiple values)\nCTE\n\nProduct names with “Google” are stored in “product_threshold”\n\n\n\nMultiple variables, 1 value\nWITH\n  variable AS (\n  SELECT\n    MIN(order_date) AS first_order,\n    MAX(order_date) AS last_order\n  FROM\n    `datastic.variables.base_table_2`)\n\n  -- Main Query\nSELECT\n  a.*\nFROM\n  `datastic.variables.base_table` a,\n  variable\nWHERE\n  order_date BETWEEN first_order\n  AND last_order\n\nBasically the same as the 1 variable, 1 value example\nCTE\n\nvariable is the name of the CTE where “first_order” and “last_order” are stored\n\nQuery\n\nNot idea why “a.*” is used here\n\n\n\n\n\nProcedural Language\n\nMisc\n\nDocs\nNotes from\n\nBigQuery SQL Procedural Language to Simplify Data Engineering\n\n\nDeclare/Set\n\nDECLARE statement initializes variables\nSET statement will set the value for the variable\nExample: Basic\n\nExample: SET within IF/THEN conditional\n\n\nChecks if a table had the latest data before running the remaining SQL\nProcedure\n\nchecks the row count of the prod_data table where the daily_date field is equal to 2022–11–18 and sets that value to the rowcnt variable\nusing IF-THEN conditional statements\n\nIf rowcnt is equal to 1, meaning if there’s data found for 2022–11–18, then the string FOUND LATEST DATA will be shown.\nElse the latest_date is set to the value of the max date in the prod_data table and DATA DELAYED is displayed along with the value of latest_date.\n\n\nResult: data wasn’t found and the latest_date field shows 2022–11–15.\n\n\nLoop/Leave\n\nExample: Loops until a condition is met before running your SQL statements\n\n\nContinues from 2nd Declare/Set example\nProcedure\n\nA counter variable is added with default = -1\nSubtract days from 2022–11–18 using the date_sub function by the counter variable until the rowcnt variable equals 1.\nOnce rowcnt equals 1 the loop ends using the LEAVE statement\n\n\n\nTable Function\n\na user-defined function that returns a table\nDocs\nCan be used anywhere a table input is authorized\n\ne.g. subqueries, joins, select/from, etc.\n\nExample: Creating\nCREATE OR REPLACE TABLE FUNCTION mydataset.names_by_year(y INT64)\nAS\n  SELECT year, name, SUM(number) AS total\n  FROM `bigquery-public-data.usa_names.usa_1910_current`\n  WHERE year = y\n  GROUP BY year, name\n\ny is the variable and its type is INT64\n\nExample: Usage\nSELECT * FROM mydataset.names_by_year(1950)\n  ORDER BY total DESC\n  LIMIT 5\nExample: Delete\nDROP TABLE FUNCTION mydataset.names_by_year",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-remote",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-remote",
    "title": "BigQuery",
    "section": "Remote Functions",
    "text": "Remote Functions\n\nUser defined functions (UDF)\nNotes from\n\nRemote Functions in BigQuery\nBigQuery UDFs Complete Guide\n\nDocs\nUseful in situations where you need to run code outside of BigQuery, and situations where you want to run code written in other languages\nDon’t want go overboard with remote functions because they have performance and cost disadvantages compared to native SQL UDFs\n\nyou’ll be paying for both BigQuery and Cloud Functions.\n\nUse Cases\n\nInvoke a model on BigQuery data and create a new table with enriched data. This also works for pre-built Google models like Google Translate and Vertex Entity Extraction\n\nNon-ML enrichment use cases include geocoding and entity resolution.\nif your ML model is in TensorFlow, I recommend that you directly load it as a BigQuery ML model. That approach is more efficient than Remote Functions.\n\nLook up real-time information (e.g. stock prices, currency rates) as part of your SQL workflows.\n\nExample: a dashboard or trading application simply calls a SQL query that filters a set of securities and then looks up the real-time price information for stocks that meet the selection criteria\nWITH stocks AS (\nSELECT\n  symbol\nWHERE\n  ...\n)\nSELECT symbol, realtime_price(symbol) AS price\nFROM stocks\n\nWhere realtime_price is a remote function\n\n\nReplace Scheduled ETL with Dynamic ELT\n\nELT as need can result in a significant reduction in storage and compute costs\n\nImplement hybrid cloud workflows.\n\nMake sure that the service you are invoking can handle the concurrency\n\nInvoking legacy code from SQL",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-flex",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-flex",
    "title": "BigQuery",
    "section": "Flex Slots",
    "text": "Flex Slots\n\nDocs\nFlex slots are like spot instances on aws but for running queries\n\nDocs\nA BigQuery slot is a virtual CPU used by BigQuery to execute SQL queries.\nBigQuery automatically calculates how many slots are required by each query, depending on query size and complexity\n\nUsers on Flat Rate commitments no longer pay for queries by bytes scanned and instead pay for reserved compute resources (“slots” and time)\n\nWith on-demand pricing, you pay for the cost of the query and bytes scanned\n\nUsing Flex Slots commitments, users can now cancel the reservation anytime after 60 seconds.\n\nAt $20/500 slot-hours, billed per second, Flex Slots can offer significant cost savings for On-Demand customers whose query sizes exceed 1TiB.\nview reservation assignments on the Capacity Management part of the BigQuery console\n\nAn hour’s worth of queries on a 500 slot reservation for the same price as a single 4TiB on-demand query (currently priced at $5/TiB)\nExperiment\n\n\n\nNot sure why there aren’t lower count on-demand slots. Maybe you have to use 2000 slots for on-demand.\nX-axis is the duration of the query\n\n\nYou’re charged by the minute (I think) with 1 minute being the minimum of Idle time.",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-opt",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-opt",
    "title": "BigQuery",
    "section": "Optimization",
    "text": "Optimization\n\nMisc\n\nAlso see DB, Engineering &gt;&gt; Cost Optimization and SQL &gt;&gt; Best Practices\nNotes from 14 Ways to Optimize BigQuery SQL Performance\nSet-up Query Monitoring:\n\nGoals\n\nspot expensive/heavy queries executed by anyone from the organization. The data warehouse can be shared among the entire organization including people who don’t necessarily understand SQL but still try to look for information. An alert is to warn them about the low-quality of the query and Data Engineers can help them with good SQL practices.\nspot expensive/heavy scheduled queries at the early stage. It’s going to be risky if a scheduled query is very expensive. Having the alerting in place can prevent a high bill at the end of the month.\nunderstand the resource utilization and do a better job on capacity planning.\n\nGuide\n\n“Bytes shuffled” affects query time; “Bytes processed” affects query cost\n\nLIMIT speeds up performance, but doesn’t reduce costs\n\nFor data exploration, consider using BigQuery’s (free) table preview option instead.\nThe row restriction of LIMIT clause is applied after SQL databases scan the full range of data. Here’s the kicker — most distributed database (including BigQuery) charges based on the data scans but not the outputs, which is why LIMIT doesn’t help save a dime.\nTable Preview\n\n\nallows you to navigate the table page by page, up to 200 rows at a time and it’s completely free\n\n\nAvoid using SELECT * . Choose only the relevant columns that you need to avoid unnecessary, costly full table scans\n\nWith row-based dbs, all columns get read anyway, but with columnar dbs, like BigQuery, you don’t have to read every column.\n\nUse EXISTS instead of COUNT when checking if a value is present\n\nIf you don’t need the exact count, use EXISTS() because it exits the processing cycle as soon as the first matching row is found\n\nSELECT EXISTS (\n  SELECT\n    number\n  FROM\n    `bigquery-public-data.crypto_ethereum.blocks`\n  WHERE\n    timestamp BETWEEN \"2018-12-01\" AND \"2019-12-31\"\n    AND number = 6857606\n)\nUse Approximate Aggregate Functions\n\nWhen you have a big dataset and you don’t need the exact count, use approximate aggregate functions instead\nUnlike the usual brute-force approach, approximate aggregate functions use statistics to produce an approximate result instead of an exact result.\n\nExpects the error rate to be ~ 1 to 2%.\n\nAPPROX_COUNT_DISTINCT()\nAPPROX_QUANTILES()\nAPPROX_TOP_COUNT()\nAPPROX_TOP_SUM()\nHYPERLOGLOG++\n\nSELECT\n  APPROX_COUNT_DISTINCT(miner)\nFROM\n  `bigquery-public-data.crypto_ethereum.blocks`\nWHERE\n  timestamp BETWEEN '2019-01-01' AND '2020-01-01'\nReplace Self-Join with Windows Function\n\nSelf-join are always inefficient and should only be used when absolutely necessary. In most cases, we can replace it with a window function.\nA self-join is when a table is joined with itself.\n\nThis is a common join operation when we need a table to reference its own data, usually in a parent-child relationship.\n\nExample\nWITH\n  cte_table AS (\n  SELECT\n    DATE(timestamp) AS date,\n    miner,\n    COUNT(DISTINCT number) AS block_count\n  FROM\n    `bigquery-public-data.crypto_ethereum.blocks`\n  WHERE\n    DATE(timestamp) BETWEEN \"2022-03-01\"\n    AND \"2022-03-31\"\n  GROUP BY\n    1,2\n  )\n\n/* self-join */\nSELECT\n  a.miner,\n  a.date AS today,\n  a.block_count AS today_count,\n  b.date AS tmr,\n  b.block_count AS tmr_count,\n  b.block_count - a.block_count AS diff\nFROM\n  cte_table a\nLEFT JOIN\n  cte_table b\n  ON\n    DATE_ADD(a.date, INTERVAL 1 DAY) = b.date\n    AND a.miner = b.miner\nORDER BY\n  a.miner,\n  a.date\n\n/* optimized */\nSELECT\n  miner,\n  date AS today,\n  block_count AS today_count,\n  LEAD(date, 1) OVER (PARTITION BY miner ORDER BY date) AS tmr,\n  LEAD(block_count, 1) OVER (PARTITION BY miner ORDER BY date) AS tmr_count,\n  LEAD(block_count, 1) OVER (PARTITION BY miner ORDER BY date) - block_count AS diff\nFROM\n  cte_table a\n\nORDER BY or JOIN on INT64 columns if you can\n\nWhen your use case supports it, always prioritize comparing INT64 because it’s cheaper to evaluate INT64 data types than strings.\nIf the join keys belong to certain data types that are difficult to compare, then the query becomes slow and expensive.\ni.e. join on an int instead of a string\n\nInstead of NOT IN , use NOT EXISTS operator to write anti-joins because it triggers a more resource-friendly query execution plan\n\nanti-join  - a JOIN operator with an exclusion clause WHERE NOT IN , WHERE NOT EXISTS, etc) that removes rows if it has a match in the second table.\nSee article for an example\n\nIn any complex query, filter the data as early in the query as possible\n\nApply filtering functions early and often in your query to reduce data shuffling and wasting compute resources on irrelevant data that doesn’t contribute to the final query result\ne.g. SELECT DISTINCT , INNER JOIN , WHERE , GROUP BY\n\nExpressions in your WHERE clauses should be ordered with the most selective expression first\n\nDoesn’t matter except for edge cases (e.g. the example below didn’t result in a faster query) such as:\n\nIf you have a large number of tables in your query (10 or more).\nIf you have several EXISTS, IN, NOT EXISTS, or NOT IN statements in your WHERE clause\nIf you are using nested CTE (common table expressions) or a large number of CTEs.\nIf you have a large number of sub-queries in your FROM clause.\n\nNot optimized\nWHERE\n  miner LIKE '%a%'\n  AND miner LIKE '%b%'\n  AND miner = '0xc3348b43d3881151224b490e4aa39e03d2b1cdea'\n\nThe LIKE expressions are string searches which are expensive so they should be towards the end\nThe expression with the “=” operator is the “most selective” expression since it’s for a particular value of “miner,” so it should be near the beginning\n\nOptimized\nWHERE\n  miner = '0xc3348b43d3881151224b490e4aa39e03d2b1cdea'\n  AND miner LIKE '%a%'\n  AND miner LIKE '%b%'\n\nUtilize PARTITIONS and/or CLUSTERS to significantly reduce amount of data that’s scanned\n\nMisc\n\nAlso see\n\nDB, Engineering &gt;&gt; Cost Optimization &gt;&gt; Partitions and Indexes for CLUSTER\nSQL &gt;&gt; Partitions\nPartitioning Docs\nClustering Docs\n\nNotes from\n\noriginal optimization article\nHow to Use Partitions and Clusters in BigQuery Using SQL\n\nWhen to use Clustering instead of Partitioning:\n\nYou need more granularity than partitioning allows.\nYour queries commonly use filters or aggregation against multiple columns.\nThe cardinality of the number of values in a column or group of columns is large.\nYou don’t need strict cost estimates before query execution.\nPartitioning results in a small amount of data per partition (approximately less than 10 GB). Creating many small partitions increases the table’s metadata, and can affect metadata access times when querying the table.\nPartitioning results in a large number of partitions, exceeding the limits on partitioned tables.\nYour DML operations (See Misc section) frequently modify (for example, every few minutes) most partitions in the table.\n\nUse BOTH partitions and clusters on tables that are bigger than 1 GB to segment and order the data.\n\nFor big tables, it’s beneficial to both partition and cluster.\n\nLimits\n\n4,000 partitions per table\n4 cluster columns per table\n\nInfo about partititoning and cluster located in Details tab of your table\n\n\nPartitioning\n\nMisc\n\nPartition columns should always be picked based on how you expect to use the data, and not depending on which column would evenly split the data based on size.\n\nExample: partition on county because your analysis or transformations will largely be done by county even though since some counties may be much larger than others and will cause the partitions to be substantially imbalanced.\n\n\nTypes of Partition Keys\n\nTime-Unit Column: Tables are partitioned based on a time value such as timestamps or dates.\n\nDATE,TIMESTAMP, or DATETIME types\n\nIngestion Time: Tables are partitioned based on the timestamp when BigQuery ingests the data.\n\nUses a pseudocolumn named _PARTITIONTIME or _PARTITIONDATE with the value of the ingestion time for each row, truncated to the partition boundary (such as hourly or daily) based on UTC time\n\nInteger Range: Tables are partitioned based on a number. (e.g. customer_id)\n\nExample: Partition by categorical\nCREATE TABLE database.zoo_partitioned\nPARTITION BY zoo_name AS\n  (SELECT *\n  FROM database.zoo)\nExample: Partition by date; Truncate to month\nCREATE OR REPLACE TABLE\n  `datastic.stackoverflow.questions_partitioned`\nPARTITION BY\nDATE_TRUNC(creation_date,MONTH) AS (\n  SELECT\n  *\n  FROM\n  `datastic.stackoverflow.questions`)\n\ncreation_date is truncated to a month which reduces the number of partitions needed for this table\n\nDays would exceed the 4000 partition limit\n\n\nPartition Options\n\npartition_expiration_days: BigQuery deletes the data in a partition when it expires. This means that data in partitions older than the number of days specified here will be deleted.\nrequire_partition_filter: Users can’t query without filtering (WHERE clause) on your partition key.\nExample: Set options\nCREATE OR REPLACE TABLE\n  `datastic.stackoverflow.questions_partitioned`\nPARTITION BY\nDATE_TRUNC(creation_date,MONTH) OPTIONS(partition_expiration_days=180,\n    require_partition_filter=TRUE) AS (\n  SELECT\n    *\n  FROM\n    `datastic.stackoverflow.questions`)\n\nALTER TABLE\n  `datastic.stackoverflow.questions_partitioned`\nSET\n  OPTIONS(require_partition_filter=FALSE,partition_expiration_days=10)\n\nQuerying: Don’t add a function on top of a partition key\n\nExample: WHERE\n\nBad: WHERE CAST(date AS STRING) = '2023-12-02'\n\ndate, the partition column and filtering column, is transformed\nSeems like a bad practice in general\n\nGood: WHERE date = CAST('2023-01-01' AS DATE)\n\nThe filtering condition is transformed to match the column type\n\n\n\n\nClustering\n\nClustering divides the table into even smaller chunks than partition\nA Clustered Table sorts the data into blocks based on the column (or columns) that we choose and then keeps track of the data through a clustered index.\nDuring a query, the clustered index points to the blocks that contain the data, therefore allowing BigQuery to skip through irrelevant ones. The process of skipping irrelevant blocks on scanning is known as block pruning.\nBest with values that have high cardinality, which means columns with various possible values such as emails, user ids, names, categories of a product, etc…\nAble cluster on multiple columns and you can cluster different data types (STRING, DATE, NUMERIC, etc…)\nExample: Cluster by categorical\nCREATE TABLE database.zoo_clustered\nCLUSTER BY animal_name AS\n  (SELECT *\n  FROM database.zoo)\nExample: Cluster by tag\nCREATE OR REPLACE TABLE\n  `datastic.stackoverflow.questions_clustered`\nCLUSTER BY tags AS ( \n  SELECT\n    *\n  FROM\n    `datastic.stackoverflow.questions`)\n\nPartition and Cluster\n\nExample\nCREATE OR REPLACE TABLE\n  `datastic.stackoverflow.questions_partitioned_clustered`\nPARTITION BY\n  DATE_TRUNC(creation_date,MONTH)\nCLUSTER BY\n  tags AS (\n  SELECT\n    *\n  FROM\n    `datastic.stackoverflow.questions`)\n\n\nUse ORDER BY only in the outermost query or within window clauses (analytic functions)\n\nOrdering is a resource intensive operation that should be left until the end since tables tend to be larger at the beginning of the query.\nBigQuery’s SQL Optimizer isn’t affected by this because it’s smart enough to recognize and run the order by clauses at the end no matter where they’re written.\n\nStill a good practice though.\n\n\nPush complex operations, such as regular expressions and mathematical functions to the end of the query\n\ne.g. REGEXP_SUBSTR()  and SUM()\n\nUse SEARCH() for nested data\n\nCan search for relevant keywords without having to understand the underlying data schema\n\nTokenizes text data, making it exceptionally easy to find data buried in unstructured text and semi-structured JSON data\n\nTraditionally when dealing with nested structures, we need to understand the table schema in advance, then appropriately flatten any nested data with UNNEST() before running a combination of WHERE and REGEXP clause to search for specific terms. These are all compute-intensive operators.\nExample\n-- old way\nSELECT\n  `hash`,\n  size,\n  outputs\nFROM\n  `bigquery-public-data.crypto_bitcoin.transactions`\nCROSS JOIN\n  UNNEST(outputs)\nCROSS JOIN\n  UNNEST(addresses) AS outputs_address\nWHERE\n  block_timestamp_month BETWEEN \"2009-01-01\" AND \"2010-12-31\"\n  AND REGEXP_CONTAINS(outputs_address, '1LzBzVqEeuQyjD2mRWHes3dgWrT9titxvq')\n\n-- with search()\nSELECT\n  `hash`,\n  size,\n  outputs\nFROM\n  `bigquery-public-data.crypto_bitcoin.transactions`\nWHERE\n  block_timestamp_month BETWEEN \"2009-01-01\" AND \"2010-12-31\"\n  AND SEARCH(outputs, ‘`1LzBzVqEeuQyjD2mRWHes3dgWrT9titxvq`’)\nCreate a search index for the column to enable point-lookup text searches\n# To create the search index over existing BQ table\nCREATE SEARCH INDEX my_logs_index ON my_table (my_columns);\n\nCaching\n\nBigQuery has a cost-free, fully managed caching feature for our queries\nBigQuery automatically caches query results into a temporary table that lasts for up to 24 hours after the query has ran.\n\nCan toggle the feature through Query Settings on the Editor UI\n\nCan verify whether cached results are used by checking “Job Information” after running the query. The Bytes processed should display “0 B (results cached)”.\nNot all queries will be cached. Exceptions include: A query is not cached when it uses non-deterministic functions, such as CURRENT_TIMESTAMP(), because it will return a different value depending on when the query is executed.\n\nWhen the table referenced by the query received streaming inserts because any changes to the table will invalidate the cached results. If you are querying multiple tables using a wildcard.",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-mod",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-mod",
    "title": "BigQuery",
    "section": "Modeling",
    "text": "Modeling\n\nMisc\n\nDocs\nmodel options\n\nTrain/Validation/Test split\n\nCreate or choose a unique column\n\nCreate\n\nUse a random number generator function such as RAND() or UUID()\nCreate a hash of a single already unique field or a hash of a combination of fields that creates a unique row identifier\n\nFARM_FINGERPRINT() is a common function\n\nAlways gives the same results for the same input\nReturns an INT64 value (essentially a number, rather than a combination of numbers and characters) that we can control with other mathematical functions such as MOD() to produce our split ratio.\nOthers don’t have these qualities, e.g. MD5() or SHA()\n\n\n\n\n\nBigQueryML\n\nSyntax\nCREATE MODEL dataset.model_name\n  OPTIONS(model_type=’linear_reg’, input_label_cols=[‘input_label’])\nAS SELECT * FROM input_table;\n\nMake predictions with ML.PREDICT\n\nExample: Logistic Regression\nCREATE MODEL `mydata.adults_log_reg`\nOPTIONS(model_type='logistic_reg') AS\nSELECT *,\n  ad.income AS label\nFROM\n  `mydata.adults_data` ad\n\nOutput\n\n\nModel appears in the sidebar alongside your data table. Click on your model to see an evaluation of the training performance.",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/inkscape.html",
    "href": "qmd/inkscape.html",
    "title": "Inkscape",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Inkscape"
    ]
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-misc",
    "href": "qmd/inkscape.html#sec-ink-misc",
    "title": "Inkscape",
    "section": "",
    "text": "In order to save something, it must be converted a “path”\n\nPath &gt;&gt; Object to Path\n\nThe bottom row of buttons is determined by what tool is selected in the left panel\nRotate object\n\nOption 1\n\nClick select tool (&gt;&gt; select object if necessary)\nSelect object a second time to get the rotation arrows\nHold ctrl (in order to rotate in descrete degree units, e.g. 45, 90) or don’t if want to rotate in continuous degrees\nClick, hold, and drag one of the arrows to rotate the object\n\nOption 2\n\nClick select tool (&gt;&gt; select object if necessary)\nHold ctrl+shift and rotate using mouse wheel\n\n\nConvert object to path\n\nNotes\n\nTo perform certain actions between 2 or greater objects, then need to be paths\n\ne.g. Creating 3d objects by Nick’s union-method or by using Extensions &gt;&gt; generate from path &gt;&gt; Extrude\n\nBottom bar (middle, below color palette) shows what you the type of thing you’ve selected. Should say path or group of paths is that’s what you got.\n\nSteps (object)\n\nSelect object with select tool (S key or left panel, top)\nPath &gt;&gt; Object to Path\nObject &gt;&gt; ungroup\nPath &gt;&gt; union\n\nSteps (stroke or outline)\n\nPath &gt;&gt; Stroke to Path\nPath &gt;&gt; break apart\nPath &gt;&gt; union\n\n\nRemove outline from an object\n\nHold shift and click the boxed X at the far left on the color bar (bottom, far left)",
    "crumbs": [
      "Inkscape"
    ]
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-keysh",
    "href": "qmd/inkscape.html#sec-ink-keysh",
    "title": "Inkscape",
    "section": "Keyboard shortcuts",
    "text": "Keyboard shortcuts\n\nGrab and object and move it to a different location\n\nClick on select tool then just click, hold, and drag object\n\nZoom-in/out\n\nHold ctrl and zoom using mouse-wheel\n\n“1” zooms out to 100%\nPan around canvas\n\nPress and hold mouse wheel and drag mouse\n\nScale up/down object\n\nSelect an object (may need to convert to path first)\nShift+ctrl and scale by dragging arrows around object\n\nSelect different layers\n\nUse the select tool to click the upper most layer of an object\nCheck out the fill color below the color bar (bottom) to tell you which layer you have\nHold alt and click the same spot again. This will select the next layer underneath.\nCheck out the fill color below the color bar (bottom) to tell you which layer you have\nRepeat until you have the desired layer selected.\n\nAlign objects\n\nctrl+shift+a",
    "crumbs": [
      "Inkscape"
    ]
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-setup",
    "href": "qmd/inkscape.html#sec-ink-setup",
    "title": "Inkscape",
    "section": "Set-up",
    "text": "Set-up\n\nRemove or set page border to desired size\n\nctrl+shift+d\nChoose page size or untick “Show page border”\nCan also set the canvas measure to pixels\n\nSet to pixels\n\n3rd row &gt;&gt; towards end (usually set to mm)\n\nView set to custom\n\nView &gt;&gt; tick Custom at the bottom of the menu\n\nZoom to 1:1\n\nView &gt;&gt; Zoom &gt;&gt; Zoom 1:1\n\nOpen Align and Distribute objects\n\nSymbol: horizontal bar graph,\nEnd 2nd row\nSet “Relative to” to Last selected\n\nOpen Edit Objects colors, gradients, etc\n\nSymbol: half triangle + diagonal paintbrush,\nEnd 2nd row",
    "crumbs": [
      "Inkscape"
    ]
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-img",
    "href": "qmd/inkscape.html#sec-ink-img",
    "title": "Inkscape",
    "section": "Images",
    "text": "Images\n\nConvert to a vector object (path/bitmap)\n\nProbably worthwhile to duplicate the image before breaking apart so you have a reference of how everything fits together\nSteps\n\nDrag png into Inkscape\nSelect image (if not already selected)\nPath &gt;&gt; Trace Bitmap\nChoose Single Scan (Black and White) or Multiple Scans (Color)\nChoose Algorithm\n\nAll do a little something different\nAutotrace seems good for B&W images\nMess with settings to see if it improves\nSome algorithms can be very computationally expensive, so monitor resources\n\nWhile selected, Path &gt;&gt; break apart\n\nDefaults for opitions\n\nSpeckles: 2\nSmooth corners: 1\nOptimize: 0.20\n\nAfter conversion and break apart, recommend selecting whole object and lowering opacity to get a sense of the different layers.\n\nClipping\n\nExample taking a square portrait and cutting into an oval\nSteps\n\nDrag image into Inkscape\nSelect the circle tool, (left-panel, top)\nClick and drag circle to an approximate shape\nChange fill color (right panel or bottom color bar) and reduce opacity (right panel)\nClick and drag the center of the circle (there should be an “x”) to the center of the area you want to preserve after clipping\nClick and drag edges of circle by nodes to finalize shape\nSwitch to select tool (left panel, top)\n(Circle should be already selected) Hold shift and click outer edge of photo to also select photo\nObject &gt;&gt; clip &gt;&gt; set",
    "crumbs": [
      "Inkscape"
    ]
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-txt",
    "href": "qmd/inkscape.html#sec-ink-txt",
    "title": "Inkscape",
    "section": "Text",
    "text": "Text\n\nNotes\n\nAfter conversion from object to path, text is no longer editable\n\nI think this means stylistically - like font family, bold, text size, etc.\n\n\nAdd text to canvas\n\nClick Text Icon (left panel, bottom)\nSymbol: AI\nLeft panel &gt;&gt; middle\n\nPaste text from outside source\n\nClick Text icon (left panel, bottom)\nDrag out a box big enough to hold the text\nctrl+v\n\nChange Font\n\nctrl + shift + T or Text menu &gt;&gt; Text and Font\n\nAdd outline color to text\n\nOption 1\n\nSelect tool (arrow, left panel) &gt;&gt; select object\nClick “stroke paint” in right panel &gt;&gt; choose color\n\nHSL (Hue, Saturation, Luminosity) and A (alpha, aka opacity)\nEnter a hex color\n\nClick “stroke style” and adjust width\n\nMay want to use pixels\n\nIn “stroke style” window, you can select select a type of “join” that give smooth edges or pointed, etc.\n\nOption 2\n\nSelect tool (arrow, left panel) &gt;&gt; select object\nHold shift and select color on the palette bar at the bottome of the screen\nAdjust hue, stroke width, join, etc. in the right panel (see option 1)\n\n\nBreak apart text into individual letters\n\nObject to path (see Misc section)\nUngroup\n\nText Portraits\n\nPNGs get a texture of words (or numbers)\nPNGs\n\nIf you have colors, convert png to greyscale (extensions &gt;&gt; color &gt;&gt; grayscale)\nIf pngs has layers, all layers need to have opacity = 1\nPNG background probably needs to be black to get maximal contrast\n\nFor numbers (useful as a logos for data packages), just knit a Rmd and copy/paste the numbers. It’s suprisingly hard to find a document online that’s full on numbers to copy and paste.\n---\ntitle: \"\"\noutput: html_document\n---\n\n```{r, echo=FALSE}\noptions(scipen=999)\n```\n`r stringr::str_remove_all(toString(sample.int(1e4, 1e4)),\",\")`\n\nTo paste text, select text tool (left panel, bottom), place cursor on canvas, ctrl+v\n\nDon’t create a text box to paste text into or paste it, select it and reshape it  (see below)\n\nText &gt;&gt; “flow into frame” is required but it’s picky as hell\n\nIf you use text that isn’t generated by an extension inside Inkscape, you have to paste it directly onto the canvas and NOT into a text box or other shape. Otherwise, “flow into frame” will not work. \n\nThe dashed line is actually a string of text but I’ve zoomed out so much that you can’t tell. You have to work with the text in this form (e.g. font, color, spacing, et.) before you selecting it and using “flow into frame” to place into some shape of container.\n\nIf you try to paste it into a text box and “flow into frame” the text disappears or is placed outside the box like some funky glitch.\n\n\n\nSelecting the text and not the shape\n\nOnce you’ve flowed the text into a shape, you’ll need to center the alignment. First, you have to select the text before selecting the text tool and aligning\nSteps\n\nClick outside the shape to deselect all objects\nZoom into the box, so that the text is pretty large\nPlace selecting arrow on a letter or number of text and click\nCheck bottom status bar to confirm “Text-in-shape” has been selected.",
    "crumbs": [
      "Inkscape"
    ]
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-shp",
    "href": "qmd/inkscape.html#sec-ink-shp",
    "title": "Inkscape",
    "section": "Shapes",
    "text": "Shapes\n\nCircle\n\nSelect circle in left pane,\nMove cursor to location,\nHold ctrl+shift\nDrag mouse outward/inward until desired size\n\nLine\n\nSteps\n\nClick draw bezier curves (left pane, middle)\nClick snap-to-cusp-nodes (top row, middle) if not already selected\nClick location on canvas\nHold ctrl and drag mouse horizontal or vertical\nRelease mouse button then ctrl\nPress enter\n\nColor\n\nSelect object and convert to path using “stroke to path”\nChoose color from bottom color bar or from right panel",
    "crumbs": [
      "Inkscape"
    ]
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-crbkgd",
    "href": "qmd/inkscape.html#sec-ink-crbkgd",
    "title": "Inkscape",
    "section": "Create a Background",
    "text": "Create a Background\n\nClick square shape from left panel\nFrom location on canvas, drag mouse outward to desired size (see size in width/heigth in 3rd row, middle)\n\nOr just drag until square is large enough to encase your object\n\nClick select on left panel and select square\nClick color on color bar (bottom)\n\nOr from right panel &gt;&gt; click fill &gt;&gt; adjust HSL or enter a hex color\n\nIf necessary, remove outline by holding shift and click “X” on color bar (bottom, left-side)\nDrag square in front of object\nClick “lower selection to bottom” (3rd row, left side) and adjust position fo square",
    "crumbs": [
      "Inkscape"
    ]
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-nodes",
    "href": "qmd/inkscape.html#sec-ink-nodes",
    "title": "Inkscape",
    "section": "Nodes",
    "text": "Nodes\n\nAdd nodes to segment\n\nClick “edit paths by nodes” (left panel, top)\nDrag box around desired segment\nClick “insert new nodes into selected segments” (bottom row, far left)\n\nRepeat to insert desired number of nodes into segment(s)\n\n\nManipulate length or angle of segment by node\n\nDiscrete increments –&gt; hold ctrl and drag node\nContinuous increments –&gt; drag node",
    "crumbs": [
      "Inkscape"
    ]
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html",
    "href": "qmd/job-organizational-and-team-development.html",
    "title": "Organizational and Team Development",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Job",
      "Organizational and Team Development"
    ]
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-misc",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-misc",
    "title": "Organizational and Team Development",
    "section": "",
    "text": "Considerations for transitioning to cloud infrastructure\n\nMaximize your existing infrastructure\n\nKeep training on-premise and inference in the Cloud. If you have GPUs on-site, then get every ounce of training out of them. They’re a sunk cost, yes, but already installed and operational. No need to move the most computationally expensive step to the Cloud just yet.\n\nDeploy automation activities by modules and stages, not by projects\n\nThe more you can reuse code across steps, the more you’ll be able to scale on future projects.\n\nBuild your provisioning automation scripts as early as possible\n\nAlthough it seems like it should happen later, this gives your team the confidence to de-provision training and inference instances as soon as possible with no productivity loss.\n\n\nBuying data\n\nA common failure mode is to build a business on top of somebody else’s data. If you depend on a single upstream source for your data inputs, they can simply raise prices until they capture all of the economics of your product. That’s a losing proposition.\n\nSo you should try to build your own primary data asset, or work with multiple upstream providers such that you’re not at the mercy of any single one.\n\nAdd proprietary value of your own\n\nA sufficiently large transformation of your source data is tantamount to creating a new data product of your own\n\nEven merging multiple datasets can add substantial value\nOther ways: quality control, labelling and mapping, deduping, provenancing, and imposing data hygiene",
    "crumbs": [
      "Job",
      "Organizational and Team Development"
    ]
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-startdsd",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-startdsd",
    "title": "Organizational and Team Development",
    "section": "Starting a Data Science Department",
    "text": "Starting a Data Science Department\n\nMisc\n\nNotes from\n\nBuilding a data team at a mid-stage startup: a short story\n\n\n\n\nConsiderations\n\nData\n\nwhere is it stored; how is it stored; is it ready to use?\nIs there a more efficient storage and processing pipeline?\n\nTransfer data to centralized relational db or warehouse (See Databases, Relational &gt;&gt; Transitioning from Spreadsheet to DB)\nCreate wrapper functions to connect to separate dbs (See below, Develop internal packages)\n\n\nStack\n\nWhat’s the current tech stack?\nAre there other tools available that achieve better efficiency, reproducibility, automation, and reporting?\nSee Developing a Data Platform below\n\nPersonnel\n\nDo people need to be hired in order to fill necessary roles?\n\nMinimal team: data scientist, data engineer, (maybe) IT\n\n\n\n\n\nHiring\n\nPredictors of future job performance (thread)\nStart small by favoring “Full Data Stack” capabilities and keeping your data team’s objectives in mind; you can grow the team one member at a time as your necessities evolve.\n1st hire: Data Analyst or a Data Engineer with Analytics skills (Python, SQL, etc.) will be more valuable as a first hire\n\nThis person could work alongside Software Engineers on a first POC, which would help identify the first pipeline needs.\nData Analyst primary responsibilities:\n\nCreating dashboards to communicate data to business teams\nWriting reports to answer business questions\nAnalyzing results of A/B tests and presenting findings\nSupporting business teams in new product launches, marketing campaigns, and accounting\n\n\n2nd hire: Data Engineer or Analytics Engineer in order to proceed with building the platform and making appropriate infrastructure choices\n\nAnalytics engineers move data, similar to data engineers, but are concerned with the actual data itself. They get into the nitty-gritty details such as data types, column names, and the time that the data arrived in the warehouse.\nAnalytics Engineer primary responsibilities:\n\nOwning ingestion to orchestration\nEnsuring data is high quality and investigating when it isn’t\nSafeguarding the data warehouse\nWriting reusable data models\nPushing data models to production\nMonitoring and troubleshooting the data pipeline\n\nThe Data Analyst or Data Scientist will be the Analytic Engineer’s stakeholder. If the analytics engineer is doing their job, the analyst should have to be:\n\nwriting complex, long-running queries directly within a business intelligence platform\nreformatting the same data columns over and over again in different dashboards\nrepeating the same business logic in multiple reports or dashboards\njoining related tables that should already be joined together in one dataset\n\n\nLater hires: base these hires on current level of projects and objectives\n\nExploratory: The company has some data, but you don’t know how to capitalize on it because you don’t know where to find it or whether you should trust it.\nAnalytics: Your leadership is convinced that becoming data-driven is key to making better business decisions. There might already be some attempts to do analytics in Microsoft Excel or other tools, but you want to take data usage to the next level.\nInnovation: When you already have the necessary insights for making decisions, you think AI/ML will help you create your next differentiating edge; therefore, you want to start investing in that direction.\n\n\n\n\nData Team Placement\n\nPush for centralization in the reporting structure, but keeping the work management decentralized.\n\n\nYou want analysts or data scientists embedded in departments to report to you, but to have ad-hoc requests by departments to go through them or members of the main data team.\n\nAvoids bottlenecks where any important but simple analyses don’t have to go through you.\nAlso, strong data people want to report into a manager who understands data, not into a business person\nExample: assignments\n\ndata infrastructure, onboarding product team, supply chain team, checkout team, marketing team, support for the CEO and helping with investor/board decks\n\n\nSend out an email to a large group of people outlining this change, and make it very clear who people should work with for their data needs.\nAs you hire people going forward, you are planning to assign them to different teams throughout the company.\n\nMostly product/engineering teams, but in some cases other teams.\n\n\nWithin Engineering: in some organizations like LinkedIn, the data team is part of engineering. Having seen a similar setup play out in the past, I think that Data and Engineering teams should work as partners and, therefore, with separate reporting lines. Creating a reporting dynamic between the two might jeopardize the efficiency of the collaboration and distance the Data team from the business.\nWithin product: this makes sense when the product is tightly related to data and when the organization relies on data primarily for feature testing and other product analytics use cases.\nWithin a business entity: finance or marketing, e.g., This is usually the case for small data teams where the scope and objectives only pertain to this particular team (not recommended for larger companies).\nAs an Independent Entity: reporting directly to the CEO or CFO. This makes sense for an organization that has: a. reached a good level of data maturity and company-wide data literacy, and b. a wide variety of well-defined use cases across different business functions, and is considering a “Data as a Product” type of approach catering to various business domains.\n\n\n\nDevelop Relationships\n\nDevelop a strong relationship with those who are interested in analytics first. They will be invaluable as your advocates to excutive stakeholders and other more skeptical colleagues.\nDevelop a strong relationship with IT so you get some of the technical obstacles removed quickly.\n\nOpening ports, getting permissions\nRemote access to one of their servers so you can run resource-intensive analytics processes\n\n\n\n\nCreate Developement, Staging, and Production Environments\n\nDevelopment\n\nReproducibility\n\nEnables you to tranfer lockfiles or something similar to colleagues for collaboration\nResources\n\nBuilding Reproducible Pipelines in R\n\nPackages, Libraries\n\n{renv} - See Renv\n{{poetry}},{{pip-compile-multi}}, etc. - Python, General &gt;&gt; Environments, Python, General &gt;&gt; Dependencies\nNix\n\n\nDevelop internal packages\n\nAlso see Package Development\nDeveloping packages for your company’s specific use cases increases efficiency\nSee Building a team of internal R packages | Emily Riederer and VIDEO How to make internal R packages part of your team - RStudio\nFunctions that get/process data commonly used in most projects\nWrapper functions for connecting to separate databases (See Databases, Relational &gt;&gt; Misc &gt;&gt; Wrapper for db connections)\ncon_depA &lt;- localDB::connect_databaseA(username = ..., password = ...)\ncon_depB &lt;- localDB::connect_databaseB(username = ..., password = ...)\ncon_Lex &lt;- localDB::connect_databaseLex(username = ..., password = ...)\n\n\nSecurity\n\nNeed a secure way to download packages (e.g. remember log4j vulnerability)\nPosit Package Manager - repository management server that allows you download packages while being disconnected from the internet\n\nPosit Public Package Manager - a free service that provides complete mirrors of CRAN, Bioconductor, and PyPI, including historic package snapshots. It is based on Posit Package Manager, the professional product for teams who need greater control over how they expose R, Python, and Bioconductor packages within their organizations.\n\nAlso {RDepot} for management might be an option\n\nStaging\n\nStaging environment should be as close as possible the production environment\n\nDeployment\n\nAuthentification system - system where a user or user group with permissions is allowed to access your application using a username and password\nScheduling system - scripts running on a schedule\nMonitoring - alerts you to any errors in the pipeline\n\n\n\n\nEarly Projects\n\nSmall companies have limited budgets, so demonstrating frugality and resourcefulness in your early projects may assist you in obtaining more resources in the future\nSet up a meeting with the CEO later that week.\n\nYour goal is to figure out a few metrics she wants reported on weekly in an automatic email.\n\nSet up weekly 1:1s with a number of key people across the org that need data.\n\nYour goal is to find data gaps and opportunities and dispatch them to the data scientists.\n\nIf data is decentralized, create a data warehouse that houses all data\n\n\nFind analysts or other people in departments that have experience with SQL or are interested in learning it.\n\nAfter the data has been centralized, they will then be able to answer some of the ad-hoc data questions for the managers of those departments\nThese people may go too far at times but giving them access to the data will be a net positive.\n\n\nCreate presentations for departments that aren’t data driven\n\nExample:\n\nSituation:\n\nData team projects for product aren’t being put into production.\n\n“Product managers put it on the backlog, but they keep pushing it off because other things keep coming up.”\n\nProduct puts features into production without A/B testing, because they’re being “bold” and don’t want to wait for experiments that last months\n\nNo idea if the results of their features are significant or not.\n\n\nProduct managers are not thinking about data as a tool for building better features\nThere is a lack of alignment between what product teams want to build versus what data teams have\n\nPresention: Showcase many examples of tests with unexpected outcomes from your previous experience, and you make parts of the presentation a bit interactive where the audience has to guess whether test or control won.\n\nFind analyses or queries that are computationally expensive\n\nBuild pipelines to produce “derived” datasets. Once these datasets are built, costs of performing the analyses will be much lower\n\nSounds like a job for dbt.\n\n\nWork with every department team and make sure they have their own dashboard with the top set of metrics they care about.\n\n\n“white-glove analytics” means give special attention (prioritize) to analytics for excecs and CEO.\n\nProduce something quick initially (establishes respect for your expertise)\n\nGet a list of the most pressing problems that the executives wanted me to solve.\nFind a relatively small but high-impact problem.  (see Project, Planning)\nNail it and make sure that it is recognized as solving a high-priority problem.\n\n\n\n\nCreate a Culture\n\nLimit ad-hoc requests\n\ne.g. creating one-time reports and pulling data\nSee Job, On the Job &gt;&gt; Requests\nMake it clear that automating your pipeline, cleaning up your dashboards, and streamlining your data definitions will have a massive impact in the future\nCreate or purchase a ticketing system to manage all incoming requests from other departments in a company (article)\n\nThis will help in managing the workload, and prevent the pressure of stakeholders sending direct emails or ping-ing team members on the requests.\nIt needs to manage the lifecycle of every individual request, from submission to resolution\nPrioritizes the most important problems and creates a sense of urgency for the team to focus and solve.\nForm should include:\n\nRequestor’s team\nDetails of the request: what are the data needed? In which format (i.e spreadsheet, dashboard, analytics documents, database table, etc)? What/how the data will be used for?\nPriority/time when the data is needed\n\n\nEducate Stakeholders if they’re eager to learn\n\nVideo on navigating the repository (?)\nSimple utilization of the dashboards\n\nutilizing the filters and interactive parameters to get the data their needs.\n\nMore advanced education can be a workshop on data sources and SQL/querying courses, but this is not mandatory.\n\n\nWrite clear documentation of all processes and standards\n\nWithout documentation of team practices, losing team members means losing domain knowledge\nWith documentation comes team reviews of this documentation. Through reviews, everyone learns something new. The rest of your team will be introduced to new concepts and ideas that will only make your team stronger\nData documentation\n\nKnowledge graphs are a paradigm often leveraged by SaaS data solutions that automatically represents data as nodes in a graph, drawing connections via logic and metadata. Knowledge graph-based tools, like lineage, are often automated to help data teams save time and resources when it comes to generating quick documentation about the relationships between and about data across the organization.\nFormalizing service-level agreements (SLAs) and service-level indicators (SLIs), and putting together implicit and explicit contracts between teams can help everyone stay aligned on priorities as you work to meet your goals.\n\n**See Developing a Data Platform &gt;&gt; Data Observability &gt;&gt; Data Reliability\n\n\n\nStart having code reviews\n\nMake code reviews a common practice by enforcing them through Github\nBest way everyone can optimize their code\nTidyteam code review principles\n\nKnowledge-sharing presentations\n\n“lunch and learn”\nAll about building one another up and using each other as learning opportunities\nGet rid of the pressure of having to do everything in a certain domain\nIf one person’s area becomes flooded with tasks, having others with those same skills came lessen the load on one person\nIf a person gets sick or leaves, another team member can pick up the slack",
    "crumbs": [
      "Job",
      "Organizational and Team Development"
    ]
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-datlit",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-datlit",
    "title": "Organizational and Team Development",
    "section": "Data Literacy",
    "text": "Data Literacy\n\nMisc\n\nWhy is it necessary? A recent report from ThoughtSpot and the Harvard Business Review found that successful companies are enabling frontline workers, like customer service representatives and delivery and repair workers, with better data and insights to make good decisions. When workers have that information, the report found, companies have higher rates of customer and employee satisfaction and higher productivity and top-line growth.\nLong process but measureable results can be had within a year\nData Literacy is not the final goal. Others:\n\nData Maturity — easy access throughout the organization to good data\n\nAlso see Data Maturity\n\nData-Driven Leadership — Meaning that leaders demonstrate the skills they require of workers\n\nAlso see Developing a Data Driven Organization\n\nData-Driven Decision-Making\n\nAlso see Developing a Data Driven Organization\n\n\n\n\n\nLevels\n\nFrom Data Literacy Personas\nData Skeptics : They don’t believe in the value and power of analytics. They see analytics and data as a ‘burden’ to their work. They can derail any Data Literacy project unless carefully nurtured into becoming Data Enthusiasts. A good data awareness program is imperative in turning these skeptics into enthusiasts.\nData Enthusiasts : They believe in the power of data. They are eager to learn more about how to use data and interpret it in their work. A good Data Literacy program could usher them and the company to new heights toward being data-driven.\nData Literates : They understand the analytics landscape and can be an active participant in discussions involving data. They are willing to hone up their analytics skills. A good recipe-based analytics program with hands-on practice on the most employed analytics techniques can take them and the company a long way toward being data-driven.\nCitizen Analysts: They are data-driven employees who can solve 80% of their business problems using a structured approach to analytics and, in the process, align the stakeholders as well. They can get to an actionable solution and move the critical metrics for the company. Some Citizen Analysts can also be taught advanced analytics.\nData Scientists: They are well versed in advanced analytics methodologies. They can solve almost 100% of business problems using analytics. They are adept in using cutting-edge tools like R, Python, and SAS to manipulate data and build models. These Data Scientists are capable of aligning stakeholders toward an actionable solution and excelling at the data-driven decision-making process.\nData-Driven Executives: They understand the power of analytics, which is the discovery and interpretation of meaningful patterns in data and their application for effective decision making. Data plays an integral role in the decision-making process. They hold their team accountable for their work and can understand when analytics has been executed in the right manner or not.\n\n\n\nCreate a Data Literacy Plan\n\nNotes from How to Build Data Literacy at Your Company\nDefine data literacy goals, assessing employees’ current skill levels, and laying out appropriate learning paths\nSteps:\n\nDistinguish between data literacy and technical literacy\n\nBeing able to use a tool and understanding how derive insights from it are two different things\nLiteracy ( https://dam-prod.media.mit.edu/x/2016/10/20/Edu_D’Ignazio_52.pdf )\n\nRead with data, which means understanding what data is and the aspects of the world it represents.\nWork with data, including creating, acquiring, cleaning, and managing it.\nAnalyze data, which involves filtering, sorting, aggregating, comparing, and performing other analytic operations on it.\nArgue with data, which means using data to support a larger narrative that is intended to communicate some message or story to a particular audience.\n\n\nStart with a baseline of employee skills\n\nAfter knowing a baseline, you can develop a plan to upskill employees\nExample survey: https://aryng.com/data-literacy-test\n\nIt’s not that good, but it gives an idea of how it should be formulated\n{shinysurvey} could be used to develop something suitable for a company’s particular business model\n\nScore and categorize using something like the “levels” (above)\n\nUse common language\n\nUsing jargon or imprecise terms can create confusion and complicate communication about data.\nLanguage = Culture\n\nBuild a culture of learning and reward curiosity\n\nLeaders should make sure to foster an environment that rewards curiosity instead of punishing lack of data literacy.\nIf there is a culture of fear rather than of continuous learning and improvement, then people would feel ashamed that they’re not data-literate.\nDon’t punish people for negative data. Confront the brutal facts of the negative data and learn from it. If punishment is the first reaction, then people will try to hide the data or manipulate it – vanity metrics.\n\nTake into account different learning styles\n\nNot everyone is suited to a three-hour training classes — some employees learn best with hands-on exercises, while others might like self-led courses\n\nTrack progress and develop metrics\n\nNo real examples of metrics.\n\nLeadership must be involved\n\nChief Data Officers are often the ones in charge of literacy initiatives, but all top executives needs to be on board and modeling the desired results.\nExecutives should be part of the program",
    "crumbs": [
      "Job",
      "Organizational and Team Development"
    ]
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-daddo",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-daddo",
    "title": "Organizational and Team Development",
    "section": "Developing a Data Driven Organization",
    "text": "Developing a Data Driven Organization\n\n\n\nMethods for getting buy-in from stakeholders\n\nUse your tools and skills to confirm something “obvious” (establish trust) in an accessible (probably visual) way, then use same to show something non-obvious and actionable (establish value).\n\nStakeholders need to know what levers to pull that will affect lead indicators before being given metrics that measure those indicators (i.e. the tool or report they’re asking for).\n\nLag indicators - The thing you care about but can only measure in hindsight. (e.g monthly sales). They measure success.\nLead indicators - Things that are predictive of the lag indicators (e.g. site traffic, unit sales, customer webpage behaviour, etc.). Granular aspects of the business.\n\nData teams focus on measuring these. Stakeholders take actions based on these indicators to affect the lag indicators.\nThese indicators need to continue to be refined.\n\nIf they know what decision they want to make (i.e. levers to pull) and that decision can reasonably generate business value, then tool you create to calculate the metric will be used and used correctly.\n\nThe stakeholder/business user needs to provide an action plan that answers 4 questions\n\nWhat result are they influencing with this dataset?\nWhat do they expect the dataset to look like when it arrives?\nHow they will extract answers from this dataset?\nWhat levers will they move to action on the results?\n\nExample: “We want to determine the sweet spot for each ad channel spend to get the best ROI for each channel. Once we have the optimal spends for each channel, we’ll adjust our spends and regularly rerun the model to readjust our targets.”\n2 and 3 (also maybe 1) sound like questions for the data scientist and not necessarily the business user. The dude who wrote this article is a data engineer so I guess he’s writing from that perspective. This list of questions is likely what should be answered by the data scientist and business user before building an expensive pipeline.\n\n\nCompanies launching data driven initiatives should focus on small areas of primary need first, and advance these areas to a high degree before spreading out.\n\nSmaller focused areas that produce high quality will increase confidence of stakeholders.\nStarting wide and producing low quality will litter the decision making landscape with false conclusions and conflicting truths. Therefore, decreasing stakeholder confidence",
    "crumbs": [
      "Job",
      "Organizational and Team Development"
    ]
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-datmat",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-datmat",
    "title": "Organizational and Team Development",
    "section": "Data Maturity",
    "text": "Data Maturity\n\nMisc\n\nNotes from A Maturity Model for Data Modeling and Design\nA few design decisions may have to be revisited in the maturing phase if proven false with real-life data\nIf the end reporting takes a lot longer to create than expected, performance tuning will have to be carried out on an ongoing basis\nA database admin may have requirements to collect statistics on the data\nAn entire operating model would be created around this warehouse to ensure it stays fit for purpose.\n\nTeams using it would need training on how to use it,\nData governance principles would be applied\nData quality and the cleansing process would be formalized.\n\n\n\n\nComponents\n\nAlso see Databases, Warehouses &gt;&gt; Designing a Warehouse\nMetamodeling:\n\nDefines how the conceptual, logical, and physical models are consistently linked together.\nProvides a standardized way of defining and describing models and their components (i.e. grammar, vocabulary), which helps ensure consistency and clarity in the development and use of these models.\nData ownership should be assigned based on a mapping of data domains to the business architecture domains (i.e. market tables to the marketing department?)\n\nConceptual Modeling - Involves creating business-oriented views of data that capture the major entities, relationships, and attributes involved in particular domains such as Customers, Employees, and Products.\nLogical Modeling - Involves refining the conceptual model by adding more detail, such as specifying data types, keys, and relationships between entities, and by breaking conceptual domains out into logical attributes, such as Customer Name, Employee Name, and Product SKU.\nPhysical Data Modeling - Involves translating the logical data model into specific database schemas that can be implemented on a particular technology platform\n\n\n\nDimensions to Assess Data Maturity Within an Organization\n\nStrategy — The organization’s overall data modeling strategy, including the alignment of data modeling efforts with business goals and objectives.\n\nMetamodel - There should be 1 and only 1 meta model in place that is used consistently across the organization\nPhysical Models - Should have well-designed and efficient database schemas in place that meet applicable performance and scalability requirements\n\nPeople/Talent — The articulation of specific roles and their responsibilities, as well as required expertise, skills, and training.\n\nMetamodel - There should be a single person with ultimate authority over the metamodel. He or she can take in feedback and collect change requests to ensure it is and stays fit-for-purpose.\nConceptual & Logical Models - There is a skilled person with core data modeling expertise and the ability to project it onto a real-life business domain to describe it in logical attributes that make sense to the business and technology organization alike.\nPhysical Models- Should have people who can design and implement the schemas\n\nProcesses — The processes and workflows, including the documentation of data modeling methodologies, the development of data modeling templates and standards, and the establishment of quality control and review processes.\n\nMetamodel - There should be description of how the metamodel is to be used in modeling activities which makes work easier as data people have a clear basis to start from.\nConceptual & Logical Models - The process of creating these models should have an owner and there should be a structured process to create new logical attributes, and to then have them reviewed, approved, and published.\nPhysical Models - A data dictionary can also be used to define and standardize the technical details such as data types, constraints, and other database objects.\n\nTechnology — The tools required to support data modeling efforts such data modeling software and tools, and the integration of data modeling tools with other systems and applications.\n\nConceptual & Logical Models - There should tools that can provide a visual representation of the models and can support collaboration, version control, and integration with other systems such as a data catalogue or metadata management system. A business glossary can be used to define and standardize the business concepts and terms that are used in the models.\nPhysical Models - Should have appropriate technology tools to support schema design and implementation. Database design software can be used to create and maintain physical data models. These tools can generate database schemas from the logical data model and can support collaboration, version control, and integration with other systems such as a data catalogue or metadata management system.\n\nAdoption — The adoption and usage of data modeling practices within and across the organization. This may include socialization programs, the resolution of barriers to adoption, and the tracking metrics to measure the effectiveness and impact of data modeling efforts.\n\n\n\nTips and Best Practices\n\nGet the metamodel right first. The metamodel drives reusability and consistency across the entire enterprise. It makes sure that all subsequent modeling efforts incrementally build out the overall model. If you don’t have one in place, you’re up for a gargantuan task of aligning and bridging existing, incompatible models in the future.\nConsider prebaked industry or universal models. Depending on where you are in your journey, you can consider adopting a preexisting data model. This can drive alignment with international best practices and standards, save you time and effort to build a model entirely from scratch, and enable efficient and reliable data exchanges with external parties. For example, BIAN provides a standardized banking services reference model that defines a common language, taxonomy, and business process framework for the banking industry.\nIterate between conceptual, logical, and physical. Data modeling takes time — the job will never be done. It is recommended to prioritize domains — reference domains like Customers and Products are good candidates—and start with 1 or 2, where you first complete the logical model and then guidelines for the physical model, before you move on to the next domain.\nDon’t overdo the physical. Data modeling can be complex, time-consuming, and therefore expensive. Completing a basic conceptual and logical model is almost always definitely worth the effort, but once venturing into the physical domain, you may not need to centrally direct and capture all of the physical models. You may want to prioritize here as well — for example, identify “mission critical” systems and document physical models for those, but for other ones, it may be sufficient to ensure that local application owners abide by specific modeling norms and standards.\nStrategically implement technology. They can be expensive, and you might not need them for the first domain, but eventually your data models will grow exponentially in terms of their size and complexity. Consider a data catalogue, business glossary, and data dictionary, or something that can serve as all of these. Without it, consumption (and hence value creation) will be poor.",
    "crumbs": [
      "Job",
      "Organizational and Team Development"
    ]
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-datstrat",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-datstrat",
    "title": "Organizational and Team Development",
    "section": "Developing a Data Strategy",
    "text": "Developing a Data Strategy\n\nMisc\n\nThere is no one path to developing a data strategy since every situation is unique\n\nRather than looking at what other companies have done, the key is examining your own needs and prioritizing the best data investments for your company\n\nPlanning\n\nPlanning forces a discussion about prioritization that can lead to key realizations about hiring and resourcing.\nSet three OKRs every quarter that will truly move the needle that align with your company’s bottom line\nKeep your primary goal at the forefront and stay flexible by revisiting your plan often\nWorkflow Example: B2C (business-to-consumer) model\n\n\n\n\n\nObjectives and Key Results (OKRs)\n\nAlso see\n\nThread\nProduct Development &gt;&gt; Metrics\nKPIs\n\nOKRs are a framework for turning strategic intent into measurable outcomes for an organization.\n\nObjectives are written as a set of goals that the company or department wants to achieve over a given time horizon, usually a year. Key Results are quantitative measures that might indicate you’ve reached that Objective.\nExample: a CEO will set goals for acquiring customers and the Chief Marketing Officer in turn will develop the objectives of marketing campaign reach and customer acquisitions that are expressed as the Key Results (outcomes) that will show that these goals have been achieved\nProduct outcomes measure a change in customer behavior. Product outcomes help drive business outcomes. Business outcomes measure the health of the business.\n\nExample: Education Platform\n\nBusiness Outcomes:\n\nObjective: Help more teams adopt “continuous discovery” (her business) as a way of working.\nKey result: Reduce the # of unsold seats in our courses.\nKey result: Sell more books\nKey result: Grow membership by x%\n\nProduct Outcomes:\n\nObjective: Help members invest in their “discovery habits.” (what she teaches)\nKey result: Increase % of members who interview every week.\nKey result: Increase % of members who assumption test every week\nKey result: Increase % of members who define outcomes every quarte\n\n\n\n\nAs OKRs are transparent across the company they can help plug the strategic planning gap.\n\nExample: The Data division can see what Marketing are trying to achieve and what their intended outcomes will be. The Marketing Data team can plan and see what activities they can help out with. Importantly the Marketing Data team can add in improvement objectives that might make them a better team and provide a better service.\n\nTest and verify Metrics\n\nIt’s critical to test metrics before using them in OKR, otherwise, metrics may lead to biased investments\nDeveloping the metrics and means of collection that will be used to measure progress against each OKR.\nCollect data and verify hypotheses with visualization or statistical methods.\n\nWhen different stakeholders agree upon metrics, teams can now build a dashboard to monitor how business decisions affect metrics and profits\n\nSocialize and fine tune the OKRs with the data teams that will be responsible for delivery.\nThe data teams will then determine the tasks and activities needed to make the key results happen. How this happens depends on how your company builds products and services. For example in an agile shop this will then lead to story development and sprint planning.\n\nReview OKRs regularly\n\nWhen businesses evolve, so should metrics\n\n\n\n\nComponents\n\nAlignment with customer needs\n\nData team needs to provide support for the product, marketing, customer services, mobile, website engineering\nExample\n\nO: Acquire more mobile app customers\nKR: 75% uplift in mobile app downloads\nData Strategy: Align with functional teams to meet company objectives\nData OKR: all customer journeys that lead to app store purchases must have metrics collection and analytics to measure progress or drop-out.\n\n\nData Platform Technology and Architecture: A plan to build a robust platform of data storage, data feeds, visualisation and modelling tools and a supporting connective infrastructure.\nAnalytics: An ability to apply models and perform deep analysis on the data that you have.\nDemocratization of data:\n\nMaking data available where necessary, cataloguing it, making it discoverable and well understood to encourage staff in the company to make effective use of it.\n\nPeople:\n\nHiring and retaining top talent, developing the staff you already have, fostering a culture of technical excellence and collaboration.\nExample\n\nO: Commit to developing our staff to reduce attrition and skills leaking out\nKR: Staff churn is kept below 10% each quarter\nData Strategy: Keep our teams technically skilled, engaged and current\nData OKR: 75% of staff in our data teams successfully complete 3 online technical courses in a year\n\n\nCompliance/Governance: Remaining compliant with regulatory data requirements and company policies with respect to data collection and usage. Having efficient and transparent processes in place to ensure data teams are applying regulations and policies when developing solutions.\nData Quality and Management: Setting the standards and mechanisms for data to be trusted as it flows through the company.\nSecurity: Keeping in lock step with the enterprise’s broader approach to keeping data and systems safe.\nData Literacy and Culture: Plugging the outputs of models and analytics into the decision fabric of the company. How to take data outcomes and operationalise them, turning them into actions for the business. The promotion of data as a first class concern for the company.\n\nExample\n\nO: Data plays a key part of the input to product development\nKR: Use Lifetime Value (LTV) calculations as an input to the product owners who are developing product features to engage higher value customers\nData Strategy: Improve Data Literacy for Decision Making\nData OKR: The output of LTV calculations are linked to &gt;200 feature development story points in the product team scrums\n\n\n\n\n\nPerforming an Organization Assessment\n\nThis assesment will provide the foundation for a data strategy\n\nThis is in the context of a nonprofit organization but the main parts should generalizable\n\nMission and Theory of Change\n\nA concrete outline which states\n\nThe impact that will be generated\nThe conditions needed to generate the impact\nThe programs in place to create those conditions.\n\nEach piece of the Theory of Change can then be stated in terms of a quantifiable measure of success which will serve as the starting point for developing a data strategy.\nExample\n\n\nStakeholders\n\nAnswer these questions:\n\nWho are your stakeholders?\n\nIdentify subgroups and individuals who fall into these groups\n\nDonors and Volunteers.\nManagement and Employees.\nBeneficiaries.\n\n\nWhat questions do stakeholders have that can be answered through data?\n\nDonors and Volunteers\n\nExample: A data-driven Impact Report which provides a holistic view of how the nonprofit utilizes their resources to achieve its mission.\n\nContains anectdotal stories with data that demonstrate how effectively their resources are being used\n\n\nManagement and Employees\n\nExample: More granular views on how individual programs and initiatives are performing on metrics related to their Theory of Change\n\nBeneficiaries\n\nExample: Data around how projects in different sectors are performing\n\n\nHow will the data affect stakeholder decision making?\n\nDonors and Volunteers\n\nCan influence decisions around donating time and money\n\nManagement and Employees\n\nProvides visibility into how resources are allocated internally and empowers internal decision makers to evaluate how to get the most impact out of the limited resources they have\n\nbeneficiaries\n\nCan be used to garner buy in and allow the nonprofit access to communities that they would otherwise not have\n\n\n\n\nData Gap Analysis\n\nIdentify gaps between current data capabilities and those needed to answer all stakeholder questions\nContents\n\nOutline all data needs in the form of questions derived from your Theory of Change and stakeholder analysis.\nDeep dive into the required data to answer the questions and an estimate of how much that data would cost to obtain. Don’t forget that the same data could answer multiple questions.\nIdentify data that has already been collected and any existing efforts to collect additional data.\nConnect existing data and data efforts to questions and determine gaps between questions and data.\nPropose strategies to bridge data gaps and sustain data assets. Evaluate both the benefits of answering the question and costs of acquiring the data.\nPrioritize data gaps to close.\nCommunicate findings to relevant stakeholders.\n\n\n\n\n\nData Advantage Matrix\n\nNotes from Data Advantage Matrix: A New Way to Think About Data Strategy\nSystematic way of organizing a data strategy around data investments which will help you build sustainable competitive advantages to outperform your competitors\n\nThese investments won’t necessarily be ones that return the greatest ROI at least in the near-term.\n\nMatrix Format\n\n\nTypes (Y-Axis)\n\nOperational: This is about understanding the levers that drive your business, then using them to improve operations. A key aspect is making data available and understandable to those who are making daily decisions.\n\nExamples:\n\nDaily updates about key metrics.\nIs there a drop in conversion rate?\nAre we meeting our KPIs?\n\n\nStrategic: Every company makes a few critical strategic decisions each year. The more data-driven these decisions are, the more likely that they will jumpstart growth or success.\n\nExamples:\n\nWhich cities should we launch in?\nWhich customer segments should we focus on?\nHow much should we set the price of our product?\n\n\nProduct: This is when companies leverage data to drive a core product advantage, one that separates them from competitors.\n\nExamples:\n\nGmail’s “smart compose” auto-completion feature.\nUber’s Supply and Demand Optimization Algorithm\n\n\nBusiness opportunity: This involves using company data to find and create new business opportunities.\n\nExamples:\n\nNetflix Originals, where Netflix started to produce its own TV shows and movies based on its data about what people want to watch.\nTelecoms building Know-Your-Customer (KYC) services to monetize that data\n\n\n\nStages (X-Axis)\n\nBasic: This is a quick-and-dirty MVP that uses basic tools (e.g. SAAS products, Google Sheets, Zapier) and no data specialists.\n\nAble to quickly deploy and assess a solution\nCan start at Stage 2 if a data advantage is critical to your company and can be built on proper tooling from the start\n\nIntermediate: includes investments in data platform tooling and data specialists or teams\nAdvanced: includes specialized teams for each use case or project\n\n\nExample: SaaS software startup\n\nExample: Uber-like company",
    "crumbs": [
      "Job",
      "Organizational and Team Development"
    ]
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-dtroi",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-dtroi",
    "title": "Organizational and Team Development",
    "section": "Determining a Data Team’s ROI",
    "text": "Determining a Data Team’s ROI\n\nMisc\n\nExample of model ROI calculation: Domain Knowledge Notebook, Banking/Credit &gt;&gt; Fraud &gt;&gt; Misc\nhttps://towardsdatascience.com/calculating-the-business-value-of-a-data-science-project-3b282de9be3c\nVisualizing Machine Learning Thresholds to Make Better Business Decisions\n\n\nUses a Telecom subscriptions churn example and incorporates available resources (queue rate) that can review flagged events in order to choose a threshold\n\ni.e. If you can only review 50 cases, then you model need only flag 50 cases\n\nAdds uncertainty by using multiple train/test splits and creating quantiles for CIs\nOptimizes resources, costs, precision, and recall to produce a threshold\n\n\nThe best way is for other department heads to tell it. If there are cutbacks to your team, it should illicit howls from department heads and should be fighting for more data resources.\nFor automating tasks, get an estimate of how much time and how many people it takes for that task to be completed during the project’s planning stages.\n\nUsing their salaries, estimate hourly wage and calculate sum(employee_wage * time_spent_on_task)\nAfter you automate task or make it more efficient, make the same estimation and the difference will be the ROI that data science provided\n\nRecord improvements from baseline or old model to new model using cost function\nExample Metrics\n\nCost Reduction: Raw material cost reduction per unit, cost of goods sold per unit, cost of goods manufactured per unit etc.\nYield Improvement: Percentage of waste/rejection reduced\nHuman Time Saving: Reducing manual labor by 30% per day, per batch, per sprint or any other suitable metrics\nProcess Improvement: Reduce cycle time by 20 hours, reduce waiting time by 2 hours, reduced exit cycle time by 8%, and minimize the customer return processing time by 6%\nSpeed: Reduce time to market by 100 days average delivery time by 10 minutes.\nQuality: Reduction in number of customer returns by 20%, decrease in number of customer complaints by 14%, reduction in warranty calls by 10%, and decrease in number of bugs by 12%\nMarket Share: Growth in market share by 5% compared to the previous year for a given brand, geographic region, customer segment or overall consumer base\nCustomer Base: Growth in customer base by 6% year over year, increase in average time spent per user session by 10%, increase in customer conversion rate by 25%, and increase in customer retention by 12%\n\nApplying Metrics\n\nSchedule meetings with business users to walk through the potential metrics and help decide which ones apply to your project\nEstablish baselines for each metric\n\nIf historical data isn’t available, need to work with multiple business teams or users to make assumptions and approximate the raw numbers to come up with the required baseline value\n\nAfter a reasonable period (depends on context, e.g. 6 months), calculate the change between the baseline and post-treatment for each metric\n\nIf department heads aren’t willing to go to bat for your team, then examine three areas\n\nSiloed data teams\n\nOften data products are only part of the decision making process and when the data teams are separated from other departments it’s difficult to quantize their contribution.\n\nExample\n\nWhen deciding to purchase goods from a supplier, the demand forecast is obviously really important, but so does the allowed pack sizes, the minimum order quantity, the storage finite capacity, etc.\n\n\nSolutions\n\nConsider changing to an integrated schema, Models for Integrating Data Science Teams Within Organizations like the hybrid “product data science” schema\nTrack Decision Impact (DI) metrics ($), which doesn’t focus on intrinsic error (e.g. RMSE, MAE, F1, etc.), but rather on the quality of the decisions made that involved your data product.\n\nNotes from 10 Reasons to Implement the New Generation of Business Oriented Metrics\nAlso see Domain Knowledge Notebook, Logistics &gt;&gt; Decision Impact Metrics\nExample with forecasting\n\n\nOther viz of these values could include stacked bars, gauges, or areas\nTotal range that’s addressable for forecast improvement\n\nDIn-o = DIn - DIo\n\nDIn is the cost associated with the decisions made using a prior method or baseline or naive forecast\nDIo is the truth. What are the costs associated with a decision if perfect knowledge were available. Calculate one the future observed values become available\n\n\nAdded value delivered by the actual forecasting process\n\nDIn-a = DIn - DIa\n\nDIa is the cost associated with the decision made using your forecast\n\n\nThe maximum value that could still be delivered by improving the actual forecast process\n\nDIa-o = DIa - DIo\n\n\n\n\n\nPlanning\n\nObjectives and key results (OKR) is a goal-setting framework for defining and tracking objectives (O) and their outcomes (KR).\n\n\nInfrastructure-level objectives — like implementing a new data warehouse — can live separately, but should still have explicit callouts for how those investments are supporting the higher-level objectives.\n\nData people should be included in other department planning meetings and stakeholders should be involved in data team meetings where projects are prioritized. Gives them more insight into data team activities.\n\ne.g. If the Marketing team has a weekly planning meeting or daily stand-ups, the Data analysts supporting that team should be in the room (or Zoom, or whatever).\n\n\nTools\n\nInteractive apps (shiny) where stakeholders can run different scenarios can integrate them more into the process\n\nI think this is fine where applicable, but not to go overboard with it. This dude (article author) also builds these types of platforms and probably was pushing his product a bit.",
    "crumbs": [
      "Job",
      "Organizational and Team Development"
    ]
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-devdatplat",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-devdatplat",
    "title": "Organizational and Team Development",
    "section": "Developing a Data Stack",
    "text": "Developing a Data Stack\n\n\nPriority is from top to bottom\n\n\nBuild vs Buy Factors\n\nFactors for deciding on whether you should build, buy SaaS, or go with Open Source solutions\nThe size of your data team\n\nUnless you a big tech company (e.g. Airbnb), it’s better to go SaaS or Open Source\n\nUnless you have a large data team, engineers and analysts are too busy to maintain or onboard new ppl on custom tools\nIt’s costly and time consuming to build your own unless this is already part of your company’s dna (i.e a tech firm).\n\n\nThe amount of data your organization stores and processes\n\nSelect one that will scale with your business\n\nYour data team’s budget\n\nLimited budget but many hands, then open source\n\nCaveats\n\nOnly about 2 percent of projects see growth after their first few years\n\ni.e. The need to be stabl and actively maintain over an extended period\n\nTypically on your own, so make sure the project maintainers respond to problems and there’s an active communtiy that uses the tool\n\n\n\n\n\n\nData Ingestion\n\nTools\n\nIngestion\n\nFivetran, Singer, Stitch, AirByte, Kafka\nSee\n\nDomain Knowledge Notebook, Product Development &gt;&gt; Behavioral Data\nProduction, Tools &gt;&gt; Ingestion\n\n\nOrchestration\n\nApache Airflow, Prefect, and Dagster\nSee Production, Tools &gt;&gt; Orchestration\n\n\n\n\n\nData Storage and Processing\n\nSnowflake, Google BigQuery, Amazon Redshift, Firebolt, Microsoft Azure, Amazon S3, Databricks, Dremio\nAlso see Databases, Lakes, Databases, Warehouses\nResources\n\nChoosing Open Wisely - Pros/cons of going open source\n\n\n\n\nData Transformation and Modeling\n\nTools\n\ndbt – Short for data build tool, is the open source leader for transforming data once it’s loaded into your warehouse.\n\nSee\n\nProduction, Tools &gt;&gt; ETL, ELT Orchestration\nDatabases, dbt\n\n\nDataform – Now part of the Google Cloud, Dataform allows you to transform raw data from your warehouse into something usable by BI and analytics tools\nPython + Airflow\n\n\n\n\nBusiness Intelligence and Analytics\n\nTools\n\nLooker – A BI platform that is optimized for big data and allows members of your team to easily collaborate on building reports and dashboards.\nTableau – Often referred to as a leader in the BI industry, it has an easy-to-use interface.\nMode – A collaborative data science platform that incorporates SQL, R, Python, and visual analytics in one single UI.\nPower BI – A Microsoft-based tool that easily integrates with Excel and provides self-service analytics for everyone on your team\nR, Python\n\n\n\n\nData Observability\n\nAlso see Production, ML Monitoring\nCircumstances where it would be a good time to implement this stage:\n\nMigrating from trusted on-premises systems to the cloud or between cloud providers\n\nUsers of those older systems need to have trust that the new cloud-based technologies are as reliable as the older systems they’ve used in the past\n\nYour data stack is scaling with more data sources, more tables, and more complexity\n\nThe more moving parts you have, the more likely things are to break unless the proper focus is given to reliability engineering\nRule of thumb is more than 50 tables\n\nBut if you have fewer and the severity of data downtime for your organization is great, then data observability is still a very sensible investment\n\n\nYour data team is growing\n\nOften leads to changes in data team structures (from centralized to de-centralized), adoption of new processes, and knowledge with data sets living amongst a few early members of the data team.\nBecomes harder for data analysts to discern which tables are being actively managed vs. those that are obsolete\nTechnical debt will slowly pile up over time, and your data team will invest a large amount of their time into cleaning up data issues\n\nYour data team is spending at least 30% of their time firefighting data quality issues\nYour team has more data consumers than you did 1 year ago\nYour company is moving to a self-service analytics model\n\ni.e. allow every business user to directly access and interact with data\n\nData is a key part of the customer value proposition\n\ni.e. the company starts deriving substantial value from customer-facing applications\n\n\nMust be able to monitor and alert for the following pillars of observability:\n\nVolume: Has all the data arrived?\nSchema: What is the schema, and how has it changed? Who has made these changes and for what reasons?\nLineage: For a given data asset, what are the upstream sources and downstream assets which are impacted by it? Who are the people generating this data, and who is relying on it for decision-making?\nAvailability: Whether the data is available. Things like network issues or infra issues can prevent users from accessing data.\nFreshness: How up-to-date your data tables are, as well as the rhythm when your tables are updated.\n(In)Completeness:The percentage of unexpected missing data entries, can be both on the row level and column level. For example, is there any row from the upstream tables not being processed? Is there any field in the destination table missing data for &gt; X% of rows?\nDuplicates: The percentage of unexpected duplicated primary key(s).\nDistribution: The accepted range of certain fields. It can be an enumeration or a range of numbers.\nFormat: The expected format and schema of the data like CSV, or BigQuery table\nRelationship: Any test on complicated business logic which involves multiple columns or tables.\n\n\n\n\nData Reliability\n\n\nAlso see\n\nIt’s Time to Set SLA, SLO, SLI for Your Data Team — Only 3 Steps for more details on benefits\nSLOs, SLIs, SLAs, oh my—CRE life lessons\nGlossary: DS terms\n\nSLAs\n\nIt improves the communication between engineers and stakeholders by clearly defining the scope of data reliability and what “okay” and “not okay” means. Making it crystal clear avoids the needless discussion on what does and doesn’t need attention.\n\nGives engineers an error budget which is a metric to prevent engineers from burning out and it helps the team allocate their time wisely\n\nIt helps engineers decide how much time should be spent on delivering features and how much time should be spent on making the existing pipelines more reliable.\n\nSLOs\n\n\nIf data availability threshold is set to 99.9% in SLA, then it should be 99.99% in SLO.\nIf a service breaks SLO, on-call engineers need to react quickly to avoid it breaking SLA, otherwise, the company (or the team) will lose money (or reputation).\nTo achieve overall availability 99.9%, the team needs to monitor the up-time of a few internal tools and each of them has its own SLO threshold.\n\nSLIs\n\nMetrics in the monitoring system\nYour SLIs will depend on your specific use case, but here are a few metrics used to measure data trust, a common KPI:\n\nThe number of data incidents for a particular data asset (N). Although this may be beyond your control, given that you likely rely on external data sources, it’s still an important driver of data downtime and usually worth measuring.\nTime-to-detection (TTD): When an issue arises, this metric quantifies how quickly your team is alerted. If you don’t have proper detection and alerting methods in place, this could be measured in weeks or even months. “Silent errors” made by bad data can result in costly decisions, with repercussions for both your company and your customers.\nTime-to-resolution (TTR): When your team is alerted to an issue, this measures how quickly you were able to resolve it.\n\n\nSteps\n\nCreate the SLA with your stakeholders\n\nDefine what reliable data means together with your stakeholders\n\nData engineers can assess the historical performance of the data to gain a baseline and understand its usage pattern, what fields are mostly queried, and at what frequency\nWhat do stakeholders care about the most? Freshness? Accuracy? Availability? Duplicates?\n\nStart low so engineers don’t need to run 24/7 rotations and stakeholders are typically ok with a few hours of downtime initially (99% = ~1.68 hours downtime per week). As the situation gets more stable, you can increase it to the ideal number. (99.99% = few minutes downtime per month)\nExample: Critical Revenue Table on BigQuery\n\nAvailability 99.9%: Table should always be accessible.\n\nAvailability has a higher SLA because it mostly relies on BigQuery service which promises 99.99% up-time.\n\nFreshness 99%: Table should be refreshed daily before 9 am with yesterday’s revenue.\nUniqueness 99%: Table shouldn’t contain any duplication on the composite key.\nCorrectness 99%: Amount fields like gross_booking, net_booking, net_revenue should be calculated correctly.\n\n\nSelect SLIs\n\nSelect metrics that will help you meet the terms of the SLA\nThe selection of indicators is very specific to the data infrastructure the team is using\nExample: Airflow + BigQuery + dbt to deliver data\n\nAvailability: Seconds since the latest heartbeat (i.e. timestamp emitted by a job) from Airflow\n\nThe downtime of Airflow doesn’t have a direct impact on the BigQuery table’s availability, but as mentioned previously, it’s worth monitoring internal tools which contribute to the final SLA.\n\nFreshness: Hours since the table was updated last time.\nUniqueness: The test result of the uniqueness test in dbt.\nCorrectness: The test result of other value checking in dbt.\n\n\nDefine the SLO and set up alerts\n\nSet the internally acceptable range of failure per indicator.\nTo give on-call engineers enough reaction time before there’s a violation of the SLA, the SLO should be more strict than the SLA.\nCreate alerting rules and rate incidents by the level of severity.\nCan display on a dashboard, making it effective to communicate and resolve the issue\nExample: See SLO and SLI examples\n\nThe services you use will have dependencies, so you’ll need to take them into account when defining your SLOs\n\nIf the dependencies have stricter SLO than you have (i.e. what your customer needs to be happy), then there nothing more to do.\nIf some do NOT, then you need to mitigate those risks.\n\nSee article (bottom) which links blog, blog, spreadsheet tool (used to calculate risk)\n\n\n\n\n\n\n\nData Discovery\n\nNeed a reliable, scalable way to document and understand critical data assets\nFeatures\n\nSelf-service discovery and automation\n\nData teams should be able to easily leverage their data catalog without a dedicated support team.\nGreater accessibility naturally leads to increased data adoption, reducing the load for your data engineering team.\n\nScalability as data evolves\nReal-time visibility into data health\n\nUnlike a traditional data catalog, data discovery provides real-time visibility into the data’s current state, as opposed to its “cataloged” or ideal state.\nGlean insights such as which data sets are outdated and can be deprecated, whether a given data set is production-quality, or when a given table was last updated.\n\nSupport for governance and warehouse/lake optimization: From a governance perspective, querying and processing data in the lake often occurs using a variety of tools and technologies (Spark on Databricks for this, Presto on EMR for that, etc.), and as a result, there often isn’t a single, reliable source of truth for reads and writes (like a warehouse provides). A proper data discovery tool can serve as that central source of truth.",
    "crumbs": [
      "Job",
      "Organizational and Team Development"
    ]
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-daoce",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-daoce",
    "title": "Organizational and Team Development",
    "section": "Developing an On-Call Environment",
    "text": "Developing an On-Call Environment\n\nMisc\n\nNotes from How to Build an On-Call Culture in a Data Engineering Team\n\n\n\nCreate Table of Production Issues\n\n\nThe more detailed the guideline, the more streamlined the on-call process will be.\n\n\n\nCreate a Workflow\n\n\nOn-call workflow tells engineers how to approach a variety of production requests in a consistent manner.\nMore preparation means fewer decisions we have to make on the spot, leading to fewer mistakes.\nMain Steps\n\nDefine the sources of alerts. Redirect all production issues into one or two channels. For example, use Slack integration to redirect pipeline issues, infrastructure failures, and test failures into a centralized Slack channel for easy tracking.\nIdentify the category of alerts, the scale of the impact, and its urgency. Every on-call should be able to assess the urgency of the issue based on its category, impact, and SLA requirements. By creating “data products” with clear requirements, teams can benefit from the process that enables them to identify the impact and urgency efficiently. I recommend article — Writing data product pipelines with Airflow, a nice practice to write data requirements as code in Airflow dags.\nIdentify the root cause and solve the issue. When an urgent issue arises, on-call should do their best to find the root cause and solve the issue. However, not every data engineer knows all the nitty-gritty of data models maintained by data analysts. In such situations, following an escalation pattern can be helpful. It allows engineers to ask for help from other engineers or analysts with necessary expertise until the issue is resolved.\nPerform post-incident actions and update the on-call journal. Don’t forget to perform post-incident actions like backfilling to correct historical data for incremental models. It’s also recommended to keep an on-call journal for knowledge sharing.\nUser communication. In a parallel thread, it’s important to keep users in the loop. Effective communication during the “data downtime” builds trust between the data team and users. One of my articles — Status Page for Data Products — We All Need One introduces the status page as a method to improve effective communications during data downtime.\n\n\n\n\nOn-call ownership\n\nClearly, engineers are responsible for technical failures, but when it comes to data model failure, ownership becomes controversial.\nOptions\n\nAssign an owner to each data model as much as you can. Simply assigning an owner to the model tremendously improves efficiency during on-call.\nTreat data model owners as “external parties”. It’s not uncommon that software relies on an external party that is outside of engineers’ control such as an IoT service that relies on a network provider. Similarly, data engineers may need to work with model owners who are outside of their immediate team to address the model failures. When external knowledge is required, engineers should feel comfortable reaching out and proactively working with them while informing users of the progress. Do not put stress on on-call engineers by expecting them to solve issues on their own.\n\n\n\n\nOn-call rotation\n\nSchedule\n\nCan use spreadsheets to manage rotation schedules and a cron job to propagate the schedule into a calendar in near real-time. (e.g. Google Sheets + Apps Script + Google Calendar)\nPlatforms ($): Opsgenie and PagerDuty\n\nPermissions\n\nOn-Call engineers may need additional permissions at times\nOptions\n\nPerform a permission escalation, temporarily granting the engineer additional privileges.\nCreate a high-privileged user group and rotate group members.\n\nEssential to ensure that the rotation of the group members must be in sync with the on-call calendar rotation.\n\n\n\n\n\n\nCommunication channels\n\nFinding the right balance between being informed and not being overwhelmed by alerts is crucial\nCentralized data alerts channel (alerts -&gt; team)\n\nBy having a dedicated channel where all alerts are sent, it becomes easier to monitor and manage alerts, reducing the risk of critical information being missed or overlooked\nSlack is a popular choice because it can easily integrate with various data sources such as Opsgenie, GCP Cloud logging, Sentry, service desk, etc.\n\nEscalation policies (team -&gt; team)\n\nA set of procedures that outlines how an organization will respond to issues that require additional resources beyond the initial response.\n\nUser communication (team -&gt; users)\n\nNeeds to start as soon as the issue is identified. Keeping the channel centralized by setting up a tool like status page.\n\n\n\n\nOn-call runbook\n\nA set of instructions that on-call can follow when responding to issues.\nMust be regularly updated to reflect the changes.\nComponents\n\nMetadata around the data product: owner, model incrementality, priority, schedule, SLA, and SLO.\nEscalation procedures (if not handled automatically).\nTroubleshooting guides: how to solve common issues. For example, perform full-refresh, check source data, logs, data observability tools and etc.\nPost-incident verification: how to verify if the issue is properly solved. For a cron job, the issue can only be verified in the next run which can be a few hours or days later.\n\n\n\n\nOn-call journal\n\n\nTool for documenting production issues\nHelps engineers who look for tested solutions and managers who search for trends.\nA templated journal ensures engineers approached each issue with the same scientific rigor.\nEach record includes intensive metadata around the issues and the in-depth investigation and what they did to fix the issue.",
    "crumbs": [
      "Job",
      "Organizational and Team Development"
    ]
  },
  {
    "objectID": "qmd/job-reports.html",
    "href": "qmd/job-reports.html",
    "title": "Reports",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-misc",
    "href": "qmd/job-reports.html#sec-job-reports-misc",
    "title": "Reports",
    "section": "",
    "text": "Packages\n\n{narrator} - Creates a text summarization of descriptive statistics. The outputted text can be enhanced with ChatGPT. Available in both R and Python.\n\nChicago manual of style for citations\nReread after 3 days to make sure it makes sense before publishing\n\nDancho does his labs on Wed afternoons, so maybe that’s a good time to release articles.\n\nPrint out article and highlight topic sentences\n\nDoes each topic sentence describe the paragraph. Do all the other sentences in the paragraph support the topic sentence.\nDo the topic sentences produce a good outline about the subject you wanted to discuss. Do they follow a logical data storytelling sequence.\n\nPrimary interests of business people: business question, budget, whether research is conclusive or not conclusive, and value the research or product provides.\nKeep color schemes for categoricals, metrics, etc.\n\nIf you used a color palette for male/female in an earlier section/slide, keep that same palette throughout.\n\nKeep date and other variable formats consistent throughout\nNo more than 3 dimensions on a chart\nPick the chart, graph or table that best fits with the paragraph and move on to the next point. Don’t use multiple charts that show the same thing.\nNever introduce something into the conclusion that was not analyzed or discussed earlier in the report.\nDo not include more information than is necessary to support you report objectives\nPhrases for communicating uncertainty\n\n’Here’s something we expect to see a lot,”\n“Here’s something we expect to see sometimes”\n“Here’s something that could happen on rare occasions, but which is worth considering because of the high stakes.”",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-concepts",
    "href": "qmd/job-reports.html#sec-job-reports-concepts",
    "title": "Reports",
    "section": "Concepts",
    "text": "Concepts\n\nWhat is the problem we are solving\n\ne.g. Why are we losing so many customers?\n\nUnderstand the kind of story you want to tell -\n\nA one-time story: What caused the last month’s shortage?\nUpdated, ongoing story: Weekly rise and fall of sales, fraud detection\n\nKnow your audience\n\nWhat knowledge your audience brings to the story. What kind of preconceptions does the audience have.\n\nInclude the critical elements of a traditional story structure\n\nPoint of View: Someone has to ask the question that’s answered with data.\nEmpathy: Need to have human protagonist who’s solving the problem.\nAn Antagonist: Confusion or misunderstanding that makes achievement of the solution difficult.\nAn Explicit Narrative: This happened, then this happened, and then…\n\nDevelop the Right Hook\n\nWhat helps grab the attention of the managers? e.g. newspaper lead opening, startling statistics, teaser\n\nA picture is priceless\n\nPeople like visuals but good ones are really difficult to create\n\nWhat’s your point?\n\nResolve and close what does your story advise to do? e.g A Call to Action\n\nIterate\n\nSome stories need to be retold continuously when new data arrives, good stories live on",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-genguid",
    "href": "qmd/job-reports.html#sec-job-reports-genguid",
    "title": "Reports",
    "section": "General Guidelines",
    "text": "General Guidelines\n\nKnow your audience Don’t use technical terms when talking to non-technical people.\n\nFast-track the conversation to the technical stuff when talking to fellow data scientists.\nThe more senior the person you’re talking to, the more to the point your message has to be.\nSmall talk with long-term clients is always essential to maintain a strong relationship.\nThe CEO only wants to know the result of your analysis and what it means for their company.\n\nSimplified\n\nIntent: This is your overall goal for the project, the reason you are doing the analysis and should signal how you expect the analysis to contribute to decision-making.\nObjectives: The objectives are the specific steps you are taking or have taken to achieve your above goal. These should likely form the report’s table of contents.\nImpact: The result of your analysis has to be usable — it must offer solutions to the problems that led to the analysis, to impact business actions. Any information which won’t trigger action, no matter how small, is useless.\n\nWhat is the business question?\nWhy is it important?\n\nDoes your model enable us to better select our target audience?\nHow much better are we using your model?\nWhat will the expected response on our campaign be?\nWhat is the financial impact of using your model?\n\nWhat is the data science question?\nWhat is the data science answer?\nWhat is the business answer?\nShow a general form of the equation, definition of terms, before explaining how to fill it with values of particular to your problem\nHow would you recommend your model/results be used?\nBe direct. Communicate your thoughts in a forthright manner, otherwise the reader may begin to tune out.\nStart with an outline\n\nState your objective\nList out your main points\nNumber and underline your main points to guide the reader\nEnd with a summary.\n\nOpen with short paragraphs and short sentences\nUse short words. The goal is to reduce friction.\n\nUse adjectives and adverbs for precision, not exclamation points.\n\nCut lazy words like very, great, awfully, and basically. These do nothing for you.\n\nUse down-to-earth language and avoid jargon. Like explaining to a 6th grader\nDon’t use generalities (e.g. “Our campaign was a great success and we came in under budget”).\n\nBe specific (e.g. “We increased click-through rates by 21% while spending 19% less than expected.”)\n\nTake the time to build down what you have to say. Then, express it confidently in simple, declarative sentences.\n\nEspecially in memos and emails, put your declaration in the subject line or as the first line",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-narr",
    "href": "qmd/job-reports.html#sec-job-reports-narr",
    "title": "Reports",
    "section": "Narrative Structures",
    "text": "Narrative Structures\n\nDeveloping a narrative when presenting results is imperative in order for recommendations to gain traction with stakeholders\n\nBudget at least 50% of time in the project plan for insight generation, and structuring a narrative (seems a bit large)\nWith each iteration (potentially dozens) of improving your presentation, you are looking to address any insight gaps, and improve the effectiveness in conveying the insight and recommendations\nAnticipate potential follow up questions they might ask and preemptively address them\nEliminate any distractions to the key message such as ambiguous statements, or erroneous facts that can derail the presentation\nIf possible find someone with tenure in the organization, or has expertise in the business area you are analyzing to lend a critical eye to your presentation.\n\nAlso may provide insight on how best to win the trust of key decision makers and potential areas that can derail the effort\n\n\nExample 1\n\nExecutive Summary\n\nBrief Description of Problem\nApproach Taken\nModels Used\nResults\nConclusion\nRecommendations\n\nDescribe the status quo\n\nMaybe describe what each proceeding section will entail\n\nWhat’sthe problem that needs fixing or improved upon\nProposed solution\nIssues that arose during process, maybe a new path discovered not previously thought of\nSolution\n\nDescription of data\n\nRecommendations or next steps\n\nThe stakeholder must understand the expected outcome, and the levers that need to be pulled to achieve that outcome.\nAn effective analysis owner will take on the responsibility for the stakeholder’s understanding, through communicating both specific predictions and the supporting evidence in a consumable way.\n\n\nExample 2\n\nExecutive Summary\n\nBrief Description of problem\nApproach Taken\nModels Used\nResults\nConclusion\nRecommendations\n\nIntroduction\n\nQuestion\nBackground\nWhy Important\nDescribe Structure of the Report\n\nMaybe a Table of Contents\n\n\nMethodology (EDA and Models)\n\nDescribe the data you are using\nThe types of analyses you have conducted & why\n\nResults\n\nMain body of the report split into sections according to the various business questions the report attempts to answer\nThe results generated for each question.\n\nDiscussion\n\nBring together patterns seen in EDA, model interpretations\nCompare with your prior beliefs and/or other papers results\nObjective recommendations for business actions to be taken\n\nConclusion/Summary\n\nRestate Question\nSteps Taken\nAnswers to Auestions\nIssues Faced\nNext Steps",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-lay",
    "href": "qmd/job-reports.html#sec-job-reports-lay",
    "title": "Reports",
    "section": "Layouts",
    "text": "Layouts\n\nNotes from: How I create an Analyst Style Guide\nMost important details (i.e. the conclusion) always come first\n\ne.g. Executive summaries at the beginning of reports; Conclusions/useful sentences for titles of sections and slide titles\nThe goal is to reduce the time required by the reader to understand what you’re trying to tell them. If they want further details, they can read on further.\n\nUse consistent layouts so your audience can get used to where different types of information will be located\n\nExample: Driver layout\n\nPlot the trend of the Goal KPI on the left side with a text description in the same box.\nUse the larger space on the right side to plot the trends of the Driver KPIs that can explain the development of the Goal KPI\n\n\nThe Goal KPI is Sales Revenue and the Driver KPIs are Leads (#), Conversion Rate (%) and Order Value (EUR)\n\n\nExample: Contrast layout\n\nUseful to highlight the difference in two or more KPIs given the same segmentation\nDivide the space equally depending on the number of the metrics I want to compare with.\n\n\nThe contrast is between the metrics\nThe segmentation is gender and age groups\nTakeaway: Females generate most revenue and cost the least to obtain",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-eym",
    "href": "qmd/job-reports.html#sec-job-reports-eym",
    "title": "Reports",
    "section": "Explaining Your Model",
    "text": "Explaining Your Model\n\nMisc\n\nFor ML models use feature importance to pick predictors to use for partial dependence plots (with standardized predictors, these can also advise on feature importance) and go back to do descriptive/aggregated statistical explorations (box plots, bars, etc.). Explain what’s happening in the plots, potential reasons why it’s happening, and potential solutions.\n\nTypes\n\nWhen talking to a colleague or regulator you may need to give more technical explanations. In comparison, customers would expect simpler explanations. It is also unlikely that you would need to give a global explanation to a customer. This is because they would typically only be concerned with decisions that affect them personally.\nGlobal: Explain what trends are being captured by the model in general\n\n“Which features are the most important?” or “What relationship does feature X have with the target variable?”\n\nLocal: explain individual model predictions\n\nTypically needed to explain a decision that has resulted from a model prediction\n“Why did we reject this loan application?” or “Why was I given this movie recommendation?”\n\n\nCharacteristics\n\n\nTrue: Include uncertainty in your explanations of your model predictions\nCorrect level: Use the language of your audience instead of DS or statistical terminology\nNo. of Reasons & Significant: Only give the top features that are responsible for a prediction or trend, and those features should be responsible for a substantial contribution\nGeneral: Explain features that are important to large portion of predictions (e.g. feature importance, mean SHAP)\nAbnormal: Explain features that are important to extreme predictions or a representative prediction\n\nMight be a feature that isn’t globally important but important for an individual prediction or an outlier prediction\n\nContrasting: Explain contrasting decisions made by your model\n\n“Why was my application rejected and theirs accepted?”\nUse important features (ranges/levels of those features) that aren’t common to both decisions",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-bizpres",
    "href": "qmd/job-reports.html#sec-job-reports-bizpres",
    "title": "Reports",
    "section": "Business Presentation",
    "text": "Business Presentation\n\nThey’re only interested in the story the data tells and the actions it influences\nPrep\n\nCreate an outline\nSituation-Complication-Resolution Framework\n\nSituation: Facts about the current state.\nComplication: Action is required based on the situation.\nResolution: The action is taken or recommended to solve the complication.\nExample\n\n\nOne minute per slide rule\n\nIf you have a 20-minute presentation, aim for 20 slides with content\n\nTry to stick to 3 bullet points\n\nOr if you need to include more information, structure the slide with some sort of “3” framework\n\nExample: 3 columns\n\n\nEach column has 3 bullets\n\n\n\nFocus audience attention to important words\n\nBold, italics, a different color, or size for words you want to emphasize\n\n\nUse emotional elements as hooks to grab attention before starting the introduction. They generate these emotions but also curiosity about what comes next.\n\nGreed - “this has the potential to double revenue”\nFear - “layoffs may be coming”\nPride - “we can do this!”\nAnger - “It’s the competition’s fault!”\nSympathy - “they’re counting on us to help”\nSurprise - “you won’t believe what we found”\n\nUse meaningful sentences as slide titles.\n\nExamples\n\nInstead of “Sales outlook”, use “Sales outlook is promising in the next 12 months”.\nInstead of “Annual Sales”, use “Sales Up 22% In 2022”\nInstead of “Algorithm Training and Validation” use “Predict Customer Churn with 92% Accuracy”\nInstead of “Q1 Conversation Rates” use “Accounts With Direct Contact are 5x More Likely to Purchase”\nInstead of “Utilizing XGBoost to Classify Accounts” use “Machine Learning Improves Close Rates by 22%”\n\n\nRead (only) slide titles aloud\n\nBy reading just the tile and title only as you start each slide, the audience will be able to process the message much more easily than reading the written words and listening to you simultaneously.\nFor the rest of the slide, do not read the content, especially if you use a lot of bulleted or ordered lists. Reading all of your content can be monotonous\n\nIntroduction:\n\nProblem: “flat 4th quarter sales” and maybe a why? it happened\nGoal: “restore previous year’s growth”\nDescribe the presentation to come: “By analyzing blah blah, we can forecast blah, blah” and maybe a teaser on how it will be solved.\nDesired outcome: “Our goal here today is to leave with a budget, schedule, and brainstorm some potential advertising approaches that might be more successful”\nIf analysis is negative, it’s important to frame the story or somebody else will. Could become an investigation or witchhunt. Include something about the way the forward, so keep the focus positive and about teamwork.\nInclude disclaimers/assumptions but only those that directly pertain to the specific subject matter of the presentation\nLayout Q&A ground rules (questions only after or also during the presentation?)\n\nBody\n\nInterpret all visuals. Don’t let the audience reach their own conclusions.\nBullets\n\nShould only cover key concepts so don’t read\nYour narration should add more\n\nMore context\nMore interpretation\nMore content\nMore feeling\n\n\nPresentation Pattern: Present visual \\(\\rightarrow\\) interpret visual\n\nStart with a visual that illustrates the problem \\(\\rightarrow\\) discuss problem \\(\\rightarrow\\) present hypothesis that explains a cause of the problem\nPresent visual that is evidence for your hypothesis \\(\\rightarrow\\) interpret visual\n\nRepeat\nVisuals act as a chain of evidence\n\nProvide recommendation for a course of action \\(\\rightarrow\\) present visuals or data that support this action\n\ne.g. Historical results from previous instances of taking this action\n\nHow this situation mimics the successful instances\n\nForecasts that support the recommendation\n\nTalk about the uncertainty, consequences of lower and upper bounds\n\nSurvey Data\n\nInvite questions and comments about the data and visuals you shown if you have no recommendations or courses of action\n\nTake notes (yourself or assistent)\nIf you don’t have an answer:\n\n“I don’t have an answer for that offhand but I’ll get back to you after we look into that.”\n“I don’t have the answer to that. I can reanalyze the data and see if they support that idea.”\n\n\n\n\nSatisfying Conclusion\n\nSummarize (especially if a lot was covered)\nIf you asked for questions or comments above, summarize them and any conclusions from the discussion, which ones require further study, etc.\nIf you provided recommendations, review them and include the rationale for them ideally tied to the data, and the expected results of such actions\n\nExample: “The price reduction on  has resulted in a strong rebound in sales figures that analysis shows will increase further with additional marketing support. We recommend increasing the advertising budget for this line by 25% next quarter and would like the art department to take on design of a new campaign as their immediate action item.”\n\nDefine success metrics and what values would require a rethink of the strategy.\nDefine a timeframe\n\n“It is our hope that the additional 25% marketing investment in the  will result in Q4 revenue that is 50% over last year’s Q4 revenue for that line. We will review the results next January and meet again to discuss them and determine any changes in course going forward.”\n\nPotentially include consequences of not following recommendations\n\n“… it is unlikely sales will recover and we’ll continue to lose market share.”\n\nIf anyone made any commitments to other actions, note those.\nBring back emotional hook that you used in the intro\n\n” our analysis shows that blah, blah will justify the further investment and eliminate the need for layoffs.”\n“… should lead to a return to robust sales and profitability, along with stronger profit sharing.\nIf you used greed, conclude with how rewarding the action will be\nif you used fear, end with how the action will alleviate that fear\n\n\nQ&A\n\nPlant questions with collegues about info you wanted to include but the topic didn’t fit into the presentation\nPrepare for likely questions will have tables or other slides that answer those questions\nDisagreements or questions you don’t have an answer to:\n\nDON’T BE DISMISSIVE\n\nDon’t respone with any variant of, “you don’t trust data?” or blaming difficulties on someone’s lack of “data literacy.”\nWith so many potential sources of error or misunderstanding, it seems sensible for the data scientist to listen to concerns.\nClient questions provide an important counterweight against over-trust in data products.\n\nGive non-defensive responses\n\nA non-defensive response is helpful when you’re wrong, but pure gold when you are right (and both things will happen from time to time). If you are right, but are argumentative or dismissive, the client is likely to be upset. if you take a client’s concerns seriously and are thoughtful about addressing the situation, then turn out to be correct on top of that, you’re likely to make a very positive impression.\nPhrases\n\nThat’s a great question. We need to collect more data before we’ll be able to answer that.\nThank you for bringing that to my attention\nI need to think about that\nI’m not prepared to give that the consideration it deserves, but can we make an appointment to discuss it later?\nI hadn’t thought of it that way\nAnything is possible\n\n\nAnswer a question with a question (to clarify)\n\nA great many disagreements arise due to mismatched interpretation of goals and definitions. It’s important to fully understand the nature of the concern.\nReports sometimes are outdated or refer to a different product, department, etc.\nThey can speed up finding the root cause of your own error.\n\nUse email\n\nFollowing-up emails summarizing an issue, outlining plans, and suggesting timelines for investigations, are nearly always appreciated\n\nBe careful about taking lifelines from the audience\n\nDuring a disagreement, a helpful bystander will often offer a suggestion. Their ideas are usually generous, imagining a way that the data scientist might be correct. It might be tempting to agree, but be careful! Thoughtlessly taking a lifeline is a fast way to lose credibility.\nPhrases\n\n“That’s a possibility, John, thanks for the suggestion!”\n“Great idea, Sally, but I need more time to look at the data to be sure!”\n\n\n\nDon’t let anyone hijack q&a and turn it into a one and one conversations. Cut off or defer answering a follow up question.\n\n“Thanks for your great question, but we do need to let others ask their questions. Please follow-up with me afterwards.”\n\nThank everyone for attending and leave the front of the room.\n\nFollow-up\n\nKeep Promises\n\nAnswer questions to promised to look into\nPost slide deck if you said you would\nSchedule and attend a meeting if you said you would\n\nSend summary email to participants if any actions resulted from the meeting\nSet up monitoring of success metrics. Someone could want an interim report before the settled upon timeframe has been reached.",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-instrart",
    "href": "qmd/job-reports.html#sec-job-reports-instrart",
    "title": "Reports",
    "section": "Instructional Articles",
    "text": "Instructional Articles\n\nWhat?\n\nGiven a short description of the subject matter\n\nWhy?\n\nWhy is the subject matter important\nWhy is the subject matter useful\nWhy do it this way and not another\nState what each section will entail.\n\nBackground\n\nSome history\nContext surrounding the problem\nBusiness and Data Science interpretations of the problem or subject matter\n\nExample\n\nFramework\n\nDescribe the variables\nDescribe the model\nPotential issues/assumptions with approach\n\nAnalysis\nResults\n\nConclusion",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-domspec",
    "href": "qmd/job-reports.html#sec-job-reports-domspec",
    "title": "Reports",
    "section": "Domain Specific",
    "text": "Domain Specific\n\nTime Series\n\nNotes from Why Should I Trust Your Forecasts?\nIn Goodwin et al. (paper yet to be published, July 2021), people trusted forecasts more when they were presented as “best case” and “worst case” values rather than as “bounds of a 90% prediction interval.”\n\nWtf is “worst case”? Outside an 80% CI? If so that has a 20% chance of happening.\n\nIn some situations, managers who are not mathematically inclined may be suspicious of forecasts presented using technical terminology and obscure statistical notation (Taylor and Thomas, 1982).\n\nSuch a manager may respect the forecast provider’s quantitative skills, but simultaneously perceive that the provider has no understanding of managers’ forecasting needs – hence the manager distrusts the provider’s forecasts\n\nI don’t understand this one either. What could possibly be the different “forecasting need” that the manager needs?\n\n\nExplanations (i.e. justifications, rationale, etc.) of the forecast can improve people’s perceptions of a forecast. The higher the perceived value of the explanations, the higher the level of acceptance of the forecast. (Gönül et al, 2006)\n\nPeople enjoy the “stories” and it makes the forecasts more believable.\n\nProvide cues for how to evaluate the forecast in the report\nProvide accuracy metrics in relation to a reasonable benchmark\n\nExample: rolling average, naive, average for these days over the previous 5 yrs, whatever the current method is, etc.\nIn very unpredictable situations, this will help to show that relatively high forecast errors are unavoidable and not a result of the forecaster’s lack of competence.\n\nBeing transparent about assumptions, and even presenting multiple forecasts based on different assumptions, will most likely reassure the user about the integrity of the provider.",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-search.html",
    "href": "qmd/job-search.html",
    "title": "Search",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Job",
      "Search"
    ]
  },
  {
    "objectID": "qmd/job-search.html#sec-job-search-misc",
    "href": "qmd/job-search.html#sec-job-search-misc",
    "title": "Search",
    "section": "",
    "text": "Job board sites like LinkedIn, ZipRecruiter, and Indeed\nCompany career pages\nRecruiters\nUnemployment Benefits Finder - To look up your state’s policy\nPlan Finder - state-sponsored health insurance plans\nStandard of Living Calculator for various cities\nCaveats Before Signing a Non-Compete\n\nSometimes severance is associated with the noncompete to ensure that enough time has passed before laid-off employees start a new job, so as to make any information they have obsolete.\nIf you don’t have a noncompete, apply for jobs at your former company’s competitor. If you do have a noncompete, flag some companies’ career pages for later.\n\nSkill Breakdown",
    "crumbs": [
      "Job",
      "Search"
    ]
  },
  {
    "objectID": "qmd/js.html",
    "href": "qmd/js.html",
    "title": "JS",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-misc",
    "href": "qmd/js.html#sec-js-misc",
    "title": "JS",
    "section": "",
    "text": "Resources\n\nLearn Just Enough JavaScript\n\nBasics: variables, objects, arrays, functions, conditionals, loops\n\nHow to run R code in the browser with webR\n\nNice breakdown of generic JS code to run scripts on a webpage\n\nJavaScript for Data Science\n\nhrbmstr: “javascript has the advantage over R/Python for both visualization speed — thanks to GPU integration — and interface creation — thanks to the ubiquity of HTML5 — means that people will increasingly bring their own data to websites for initial exploration first”\nconsole.log is the print method",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-basics",
    "href": "qmd/js.html#sec-js-basics",
    "title": "JS",
    "section": "Basics",
    "text": "Basics\n\nOperators\n\n// : comments\n... : If you want to copy all the values in your array, and add some new ones, you can use the {…} notation.\n${&lt;code&gt;} :  Anything within the${} get ran as code\n\nExample: ${b.letter}: \\$(b.frequency*100).toFixed(2) }%\n\nBackticks indicate it’s like a glue string or f string (i.e. uses code)\nb.letter and b.frequency are properties in an array\nto.Fixed is a method that rounds the value to to 2 decimal places\nThis was an example of a tooltip, so output would look like “F: 12.23%”\n\n\n\nVariables\nmyNumber = 10 * 1000\nvariableSetToCodeBlock = {\n  const today = new Date();\n  return today.getFullYear()\n}\nObject: myObject = ({name: \"Paul\", age: 25})\n\nContained within curly braces, { }\nSubset property, name:\n\nmyObject.name which returns value, Paul\nmyObject[\"name\"] which is useful if you have spaces, etc. in your property names\n\nTypes\n\nMap: Object holds key-value pairs and remembers the original insertion order of the keys\n\ne.g. See Stats &gt;&gt; By Group\nD3 Groups, Rollup, Index Docs\n\n\n\nArrays\n\nList of objects\n\nContained within brackets, [ ]\nEach row is an object and each column is a property of that object and that property has a value associated with it\n\nBasic examples\nmyArray = [1, 2, 3, 4]\nmyArray = [[1, 2], [3, 4]] // arrays within arrays\nmyArray = [1, 'cat', {name: 'kitty'}] // objects within arrays\nDF-like array\nmyData = [\n  {name: 'Paul', city: 'Denver'},\n  {name: 'Robert', city: 'Denver'},\n  {name: 'Ian', city: 'Boston'},\n  {name: 'Cobus', city: 'Boston'},\n  {name: 'Ayodele', city: 'New York'},\n  {name: 'Mike', city: 'New York'},\n]\n\nEquivalent Functions: Traditional vs Arrow\n// traditional\nfunction myFunctionWithParameters(firstName, lastName) {\n  return `My first name is ${firstName}, and my last name is ${lastName}.`\n}\n// arrow\nmyModernFunctionWithParameters = (firstName, lastName) =&gt; {\n  return `My first name is ${firstName}, and my last name is ${lastName}.`\n}\n\nArrow: Arguments are in the parentheses and the function is inside the curly braces\nString with variables needs to be surrounded by backticks\n\nFunctions Inside Methods: Traditional vs Arrow\n// traditional\n[1, 2, 3, 4, 5].filter(function(d) { return d &lt; 3 })\n// arrow\n[1, 2, 3, 4, 5].filter(d =&gt; d &lt; 3)\n\nThe argument is d but without parentheses and the function is d &lt; 3 without the curly braces\nThe function inputs each row/value of the array, so d is a row/value of the array. Then, the function does something to that row.\n\nConditionals\n\n== vs ===\n1 == '1' // true\n1 === '1' // false\n\n== is a logical test to see if two values are the same\n\n=== is a logical test to see if two values are the same and also checks if the value types are the same\n\nIf/Then\nif(1 &gt; 2) {                              // If this statement is true\n    return 'Math is broken'              // return this\n} else {                                // if the first statement was not true\n    return 'Math still works!'          // return this\n}\n\n// using ternary operator \"?\"\n\nUsing ternary operator “?”\n\nSyntax: condition ? exprIfTrue : exprIfFalse\nExample: d =&gt; d.frequency &gt;= minFreq ? \"steelblue\" : \"lightgray\"\n\nSays if the frequency property is &gt;= the variable, minFreq, value, then use steelblue otherwise use lightgray\n\n\n\n\nFor-Loop\nlet largestNumber = 0; // Declare a variable for the largest number\n\nfor(let i = 0; i &lt; myValues.length - 1; i++) {    // Loop through all the values in my array\n    if(myValues[i] &gt; largestNumber) {              // Check if the value in the array is larger that the largestNumber\n      largestNumber = myValues[i]                  // If so, assign the value as the new largest number\n    }\n}\n\nreturn largestNumber\n\nThe first statement sets a variable (let i = 0)\nThe second statement provides a condition for when the loop will run (whenever i &lt; myValues.length - 1)\nThe third statement says what to do each time the code block is executed (i++, which means to add 1 to i)\n\nWhile-Loop\nlet largestNumber = 0;                        // Create a variable for the largest number\nlet i = 0;\nwhile(i &lt; myValues.length - 1) {\n    if(myValues[i] &gt; largestNumber) {        // Check if the value in the array is larger that the largestNumber\n      largestNumber = myValues[i]            // If so, assign the value as the new largest number\n    }\n    i++;\n}\nreturn largestNumber",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-cleaning",
    "href": "qmd/js.html#sec-js-cleaning",
    "title": "JS",
    "section": "Cleaning",
    "text": "Cleaning\n\nMisc\n\nNotes from: Horst article\n\nFilter objects: myData.filter(d =&gt; d.city == 'Denver')\nSelect properties: myNewArray = salesData.map(d =&gt; ({ date: d.date, product: d.product, totalRevenue: d.totalRevenue }))\n\nIn some contexts, this, d =&gt; d[\"mileage (mpg)\"] , is also used to select columns\n\nArrange objects: salesData.sort((a, b) =&gt; a.totalRevenue - b.totalRevenue)\n\nReorders salesData by totalRevenue (low to high)\n\nMutate properties: salesData.map(d =&gt; ({...d, discountedPrice: 0.9 * d.unitPrice }))\n\nAdds a new column to salesData with a discountedPrice, which takes 10% off each unitPrice.\n\nGroup_By: d3.rollup(salesData, v =&gt; d3.sum(v, d =&gt; d.totalRevenue), d =&gt; d.region)\n\nReturn the sum of totalRevenue for each region in salesData.\nrollup might actually be a summarize and the group_by is handled in the syntax\n\nRename: salesData.map(d =&gt; ({...d, saleDate: d.date }))\n\nAdds a new column called saleDate by storing a version of the date with new name saleDate and keeping all other columns.\n\nSubset value: salesData.map(d =&gt; d.description)[3]\n\nAccess the fourth value from the description property in salesData\n\nUnite:\nsalesData.map(d =&gt; ({...d, fullDescription: `${d.product} ${d.description}`}))\n\nUnite the product and description columns into a single column called fullDescription, using a comma as a separator.\n\nLeft Join: *using {{{arquero}}} tables* salesData.join_left(productDetails, ['product', 'product_id'])\n\nJoin information from a productDetails table to salesData. Join on product in salesData and product_id in productDetails.",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-stats",
    "href": "qmd/js.html#sec-js-stats",
    "title": "JS",
    "section": "Stats",
    "text": "Stats\n\nMisc\n\nNotes from: Horst article\n\nIn examples, waterUsage is the array; waterGallons is the property.\n\n\nMean: d3.mean(waterUsage.map(d =&gt; d.waterGallons))\n\nReturns a Value\n\nStd.Dev: d3.deviation(waterUsage.map(d =&gt; d.waterGallons))\nMedian: d3.median(waterUsage.map(d =&gt; d.waterGallons))\nMin/Max: d3.min(waterUsage.map(d =&gt; d.waterGallons))\nTotal Observations (i.e. nrow ): waterUsage.length\nBy Group:\n\npropertyId is the discrete, grouping variable\nMean: waterMeans = d3.rollup(waterUsage, v =&gt; d3.mean(v, d =&gt; d.waterGallons), d =&gt; d.propertyId)\n// Returns a map object\nwaterMeans\n{\n  \"A001\" =&gt; 39.53389830508475\n  \"B002\" =&gt; 53.57627118644068\n  \"C003\" =&gt; 27.45762711864407\n  \"D004\" =&gt; 80.1864406779661\n}\n\n// View in a JS Table\n// ** Must be in a separate cell **\nInputs.table(waterMeans.map(([propertyId, meanWaterGallons]) =&gt; ({propertyId, meanWaterGallons})))\nCount: d3.rollup(waterUsage, v =&gt; d3.count(v, d =&gt; d.waterGallons), d =&gt; d.propertyId)\n\nConditional Counts: waterUsage.filter(d =&gt; d.waterGallons &gt; 90 && d.propertyId == \"B002\").length\n\nApplies two conditionals and counts the observations\n\nRanks\nwaterUsage.map((d, i) =&gt; ({...d, rank: d3.rank(waterUsage.map(d =&gt; d.waterGallons), d3.descending)[i] + 1}))\n\n1 is added so that ranks start at 1 instead of 0\n\nPercentiles: d3.quantile(waterUsage.map(d =&gt; d.waterGallons), 0.9) (e.g. 90th)",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-obs",
    "href": "qmd/js.html#sec-js-obs",
    "title": "JS",
    "section": "Observable",
    "text": "Observable\n\nA collaborative, online notebook platform that comes with libraries loaded to make it fairly straightforward to dive into ad hoc data analysis or produce complete reports.\nIn Observable, if you’re running a JavaScript cell that contains more than just a simple variable assignment (like myVariable = 'Hello World' ), you need to run a code block (i.e. bracket lines of code in curly braces, {}).\nYou can open your notebook in Safe Mode and edit your work without running it.\n\nGood for debugging (e.g. infinite while-loops)",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-def",
    "href": "qmd/js.html#sec-js-def",
    "title": "JS",
    "section": "Definitions",
    "text": "Definitions\n\nJSON vs R List\n{                              list(\n    boolean: true,                boolean = TRUE,\n    string: \"hello\",              string = \"hello\",\n    vector: [1,2,3]                vector = c(1,2,3)\n}                              )\n\n// Access                      # Access\njson.vector                    list$vector\nDependencies\nHTML                                                  R (shiny)\n&lt;head&gt;                                                tags$head(\n    &lt;!-- JavaScript --&gt;                                  tags$script(src = \"path/to/file.js\")\n    &lt;script src=\"path/to/file.js\"&gt;&lt;/script&gt;              tags$link(\n    &lt;!-- CSS --&gt;                                          rel = \"stylesheet\",\n    &lt;link rel=\"stylesheet\" href=\"path/to/file.css&gt;        href = \"path/to/file.css\n&lt;/head&gt;                                                  ))\nd is each row and =&gt; is function\n(d) =&gt; d.year === 2020\n\nSays for each row in your data, the year column must equal 2020\n\nCallback Function - A function that is passed to another function as a parameter. In other words, a function “calls back” to previously defined function.\nfunction print(callback) { \n    callback();\n}\n\ncallback is the callback function and is a parameter of the print function\nCallbacks make sure that a function is not going to run before a task is completed but will run right after the task has completed.\nExample:\n// \"Click here\" button in a web app\n&lt;button id=\"callback-btn\"&gt;Click here&lt;/button&gt;\ndocument.queryselector(\"#callback-btn\")\n    .addEventListener(\"click\", function() {   \n      console.log(\"User has clicked on the button!\");\n});\n\nFirst, button selected by its id, and then we add an event listener with the addEventListener method. It takes 2 parameters. The first one is its type, click, and the second parameter is a callback function, which logs the message when the button is clicked.\n\n\nAnonymous Function - Same as a callback but unnamed. It’s a  function that is defined within another function.\nsetTimeout(function() { \n    console.log(\"This message is shown after 3 seconds\");\n}, 3000);\n\n// if the function were named\nconst message = function() { \n    console.log(\"This message is shown after 3 seconds\");\n}\n\n// as an arrow function\nsetTimeout(() =&gt; { \n    console.log(\"This message is shown after 3 seconds\");\n}, 3000);\n\nThe function used as a parameter has no name. console.log is the contents of the function.",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-nfcd",
    "href": "qmd/js.html#sec-js-nfcd",
    "title": "JS",
    "section": "Notes From Covidcast Dashboard",
    "text": "Notes From Covidcast Dashboard\n\nNotes from\n\nCovidcast Dashboard: reactable + sparkline tooltip (link)\n\ndiv = vertical label or container , span = horizontal\nFormat: type, styling, value\n2 divs would result in a 2 element vertical label while 2 spans would be a 2 element horizontal label\nExample: A div container holding 2 spans which creates a “date value” horizontal label\n\"function (_ref) {\nvar datum = _ref.datum;\nreturn React.createElement(\n  'div',\n  null,\n  datum.date && React.createElement(\n      'span',\n      {style: {\n          backgroundColor: 'black', color: 'white',\n          padding: '3px', margin: '0px 4px 0px 0px', textAlign: 'center'\n        }},\n      datum.date[0].split('-').slice(1).join('/')\n  ),\n  React.createElement(\n      'span',\n      {style: {\n        fontWeight: 'bold', fontSize: '1.1em',\n        padding: '2px'\n      }},\n      datum.y ? datum.y.toLocaleString(undefined, {maximumFractionDigits: 0}) : '--'\n  )\n  );\n}\"\n\nCSS: margin, padding\n\nFormat is top, right, bottom, left (ordered like a clock)\nRequires units like “px”\nNo commas separate the values\n{margin: '0px 4px', padding: '0px 0px 0px 4px'}\n\nMaybe for 0s it doesn’t matter\nSee bkmk in css/definitions for explanations behind specifications with less than 4 numbers\n\ne.g. 2 is ‘top/bottom left/right’\n\n\n\nString manipulation\ndatum.endDate[0].split('-').slice(1).join('/')\n\nTreats variable as a string object\nLooks in data arg, finds endDate variable\nIts a list variable so requires the [0] (0 part an index?)\nDate format is ymd, so splits value by “-” separator, removes 1st value (year), joins the rest of the values (month, day) with “/”\n\nIf slice(2), removes first 2 values (left to right)\n\n\nConditional\nlabelPosition = htmlwidgets::JS(\"(d, i) =&gt; (i === 0 || i === 1 ? 'right' : 'left')\")\n\nSays that if index of data value, d, is 0 or 1 then label should be positioned on the right of the point, else place the label on the left of the point",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/mathematics-linear-algebra.html",
    "href": "qmd/mathematics-linear-algebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "Resources",
    "crumbs": [
      "Mathematics",
      "Linear Algebra"
    ]
  },
  {
    "objectID": "qmd/mathematics-linear-algebra.html#sec-math-linalg-resc",
    "href": "qmd/mathematics-linear-algebra.html#sec-math-linalg-resc",
    "title": "Linear Algebra",
    "section": "",
    "text": "See Matrix Cookbook pdf in R &gt;&gt; Documents &gt;&gt; Mathematics\n\nderivatives, inverses, statistics, probability, etc.\n\nLink - A lot of matrix properties as related to regression, covariance, coefficients, etc.\nEBOOK statistical linear algebra: basics, transformations, decompositions, linear systems, regression - Matrix Algebra for Educational Scientists",
    "crumbs": [
      "Mathematics",
      "Linear Algebra"
    ]
  },
  {
    "objectID": "qmd/mathematics-linear-algebra.html#sec-math-linalg-matmult",
    "href": "qmd/mathematics-linear-algebra.html#sec-math-linalg-matmult",
    "title": "Linear Algebra",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication",
    "crumbs": [
      "Mathematics",
      "Linear Algebra"
    ]
  },
  {
    "objectID": "qmd/mathematics-linear-algebra.html#sec-math-linalg-matalg",
    "href": "qmd/mathematics-linear-algebra.html#sec-math-linalg-matalg",
    "title": "Linear Algebra",
    "section": "Matrix Algebra",
    "text": "Matrix Algebra\n\nAn expected value equation (VC stands for variance-covariance in example) multiplied by a matrix, C.\n\n\nC is factored out of an expected value as C\nC is factored out of a transpose as CT",
    "crumbs": [
      "Mathematics",
      "Linear Algebra"
    ]
  },
  {
    "objectID": "qmd/mathematics-linear-algebra.html#sec-math-linalg-fact",
    "href": "qmd/mathematics-linear-algebra.html#sec-math-linalg-fact",
    "title": "Linear Algebra",
    "section": "Factorization",
    "text": "Factorization",
    "crumbs": [
      "Mathematics",
      "Linear Algebra"
    ]
  },
  {
    "objectID": "qmd/mathematics-probability.html",
    "href": "qmd/mathematics-probability.html",
    "title": "32  Probability",
    "section": "",
    "text": "32.1 Misc",
    "crumbs": [
      "Mathematics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "qmd/mathematics-probability.html#sec-math-prob-misc",
    "href": "qmd/mathematics-probability.html#sec-math-prob-misc",
    "title": "32  Probability",
    "section": "",
    "text": "Scoring predictions\n\nExample: Probability of rain (1 = Rain, 0 = No Rain) scoring rules: -5 happiness for being rained on, -1 happiness for having to carry an umbrella Your chance of carrying an umbrella is equal to the forecast probability of rain. Your job is now to maximize your happiness by choosing a weatherperson\n\n\nForecaster total score: 3 × (−1) + 7 × (−0.6) = -7.2 happiness\nNewcomer total score: 3 x 5 + 7 x 0 = -15 happiness\nThis looks like an expected value — sum ( forecasted_probabilityevent * cost (or reward))",
    "crumbs": [
      "Mathematics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "qmd/mathematics-probability.html#sec-math-prob-fund",
    "href": "qmd/mathematics-probability.html#sec-math-prob-fund",
    "title": "32  Probability",
    "section": "32.2 Fundamentals",
    "text": "32.2 Fundamentals\n\nAddition Rule\n\n\nExclusive P(A) + P(B)\nNot exclusive P(A) + P(B) - P(A and B)\n\ne.g A - King, B - Clubs\n\n\nComplementary Rule\n\nConditional Rule\n\nMultiplication Rule\n\nPermutations\n\nPackages: {combinat}\nDefinition: A permutation of n elements is any arrangement of those n elements in a definite order. There are n factorial (n!) ways to arrange n elements. Note the bold: order matters!\nThe number of permutations of n things taken r-at-a-time is defined as the number of r-tuples that can be taken from n different elements and is equal to the following equation: \nExample Question: How many permutations does a license plate have with 6 digits? \nExample:\n# number of permutations\nchoose(3, 2) * factorial(2)\n#&gt; [1] 6\n\nCombinations\n\nPackages: {combinat}\nDefinition: The number of ways to choose r out of n objects where order doesn’t matter.\nThe number of combinations of n things taken r-at-a-time is defined as the number of subsets with r elements of a set with n elements and is equal to the following equation: \nExample Question: How many ways can you draw 6 cards from a deck of 52 cards? \nExample:\n# number of combinations\nchoose(3, 2)\n#&gt; [1] 3\n\n# combinations listed\ncombn(3, 2)\n#&gt;       [,1] [,2] [,3]\n#&gt; [1,]    1    1    2\n#&gt; [2,]    2    3    3",
    "crumbs": [
      "Mathematics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "qmd/mathematics-probability.html#sec-math-prob-not",
    "href": "qmd/mathematics-probability.html#sec-math-prob-not",
    "title": "32  Probability",
    "section": "32.3 Notation",
    "text": "32.3 Notation\n\npmfs use Pr( ) while pdfs use P( ) or p( ) or f( ) (despite when I have written below)\nW ~ Binomial(n,p) is read “the event W is distributed Binomially or assumed to be Binomial with sample size n and probability p.”\nP(B|A) - Conditional probability: meaning the probability of B given that A has occurred (example: Dependent events, P(A and B) –&gt; pick a queen, P(A) = 4/52; given a queen was chosen, pick a jack, P(B|A) = 4/51; multiply)",
    "crumbs": [
      "Mathematics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "qmd/mathematics-probability.html#sec-math-prob-terms",
    "href": "qmd/mathematics-probability.html#sec-math-prob-terms",
    "title": "32  Probability",
    "section": "32.4 Terms",
    "text": "32.4 Terms\n\nProbability - the number of occurrences of a certain event expressed as a proportion of all events that could occur.\n\nExample: In our black bag there are three blue balls, but there are ten balls in total, so the probability that you pull out a blue ball is three divided by ten which is 30% or 0.3.\nConvert probability of an event to odds for an event: O = P/(1 — P)\n\nOdds - the number of occurrences of a certain event expressed as a proportion of the number of non-occurrences of that event.\n\nExample: In our black bag there are three blue balls, but there are seven balls which are not blue, so the odds for drawing a blue ball are 3:7.\nOdds are often expressed as odds for, which in this case would be three divided by seven, which is about 43% or 0.43, or odds against, which would be seven divided by three, which is 233% or 2.33.\nConvert odds for an event to probability: P = O/(1 + O)\n\nOdds Ratio - describes the percent change in odds\nStochastic - no value of a variable is known with certainty. Some values may be more likely than others (probabilistic). Variable gets mapped onto a distribution.",
    "crumbs": [
      "Mathematics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "qmd/mathematics-probability.html#sec-math-prob-func",
    "href": "qmd/mathematics-probability.html#sec-math-prob-func",
    "title": "32  Probability",
    "section": "32.5 Functions",
    "text": "32.5 Functions\n\nProbability Mass Functions (pmf): The probability that discrete random variable, X = a value.\n\n\\(P(X = x) = f(x)\\) of a discrete random variable X is a function that satisfies the following properties:\n\n(1) says the probability that x is a certain value is &gt; 0 if x is in the support (set of possible values) of S\n(2) says all the probabilities for the supported values add up to 1\n(3) says to determine the probability associated with the event A, you just sum up the probabilities of the x values in A.\n\nProbability Density Function (pdf): The probability that a continuous random variable is within an interval.\n\nProbability density is the rate of change in cumulative probability. So the probability density can exceed 1, but the area under the density cannot.\nContinuous random variables are uncountably infinite (discrete are countably infinite). Therefore, the probability of X = a value is zero which is why finding the probability of finding the value within in interval is calculated, \\(P(a &lt; X &lt; b)\\).\nThe pdf of a continuous random variable \\(X\\) with support \\(S\\) is an integrable function \\(f(x)\\) satisfying the following:\n\n\nUses of integrals instead of summations\n\n\nCumulative Distribution Function (CDF): The probability that a random variable value is less than or equal another value. Also called the distribution function.\n\n\nSays that \\(F\\) is the cdf of \\(X\\) but is a function of \\(t\\)\nThe CDF has the following properties:\n\n\n(3) says any probability for a value less than the minimum value is zero in the sample space. The other properties are self-explanatory\n\n“Cumulative” because we’re asking, “what’s the probability of value 1 or value 2 or value 3 etc.?” which is the sum of the probabilities.\n\nSimulation of random variable values of a distribution using its cdf (what’s happening when you use rnorm( ))\n\nChange of Variable Technique\n\nRandomly draw 1000 (or whatever) numbers from \\(\\mathcal{U}(0,1)\\) (i.e. the values of the y-axis of the CDF which are bdd between 0 and 1)\nFind the inverse of the distribution’s CDF (solving for \\(X\\))\nPlug the random numbers into the inverse CDF to calculate \\(X\\) (i.e. the random variable values of the distribution) \n\nFig shows the example where 0.8 is randomly drawn from \\(\\mathcal{U}(0,1)\\) and plugged into the inverse CDF function to obtain x = 8\n\n\n\nFig illustrates the Change-of-Variable technique (the mu in y-axis label is a typo, should be a u).\n\nTaking the inverse of \\(Y = u(X)\\) yields \\(X = v(Y)\\)",
    "crumbs": [
      "Mathematics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "qmd/mathematics-probability.html#sec-math-prob-plot",
    "href": "qmd/mathematics-probability.html#sec-math-prob-plot",
    "title": "32  Probability",
    "section": "32.6 Plots",
    "text": "32.6 Plots\n\na Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nIf the two distributions being compared are similar, the points in the Q–Q plot will approximately lie on the line y = x.\nIf the distributions are linearly related after one is transformed, the points in the Q–Q plot will approximately lie on a line, but not necessarily on the line y = x\nUsing to compare two samples of data can be viewed as a non-parametric approach to comparing their underlying distributions\nInterpretation\n\nIf distribution on y-axis is more dispersed than the distribution on the x-axis, then the line is steeper than y = x.\nIf distribution on x-axis is more dispersed than the distribution on the y-axis, then the line is flatter than y = x.\nArcs or “S” shapes indicate one distribution is more skewed than the other or one of the distributions has heavier tails than the other",
    "crumbs": [
      "Mathematics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html",
    "href": "qmd/mathematics-statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-",
    "title": "Statistics",
    "section": "",
    "text": "Age Adjustment of per 100K rate\n\n\nAllows communities with different age structures to be compared\nThe crude (unadjusted)1994 cancer mortality rate in New York State is 229.8 deaths per 100,000 men. The age-adjusted rate is 214.7 deaths per 100,000 men.\n\nNotice that 214.7 isn’t 229.8*1 of course but the sum of all the individual age-group weighted rates.\n\nProcess (Formulas in column headers)\n\nCalculate the disease’s rate per 100K for each age group\nMultiply the age-specific rates of disease by age-specific weights\n\nThe weights are the proportion of the US population within each age group. (e.g. 0-14 year olds are 28.4% of the 1970 US population)\n\nThe weighted rates are then summed over the age groups to give the (total) age-adjusted rate",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-terms",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-terms",
    "title": "Statistics",
    "section": "Terms",
    "text": "Terms\n\nCoefficient of Variation (CV) - aka Relative Standard Deviation (RSD) - aka Dispersion Parameter - Measure of the relative dispersion of data points in a data series around the mean. Usually expressed as a percentage.\n\\[\nCV = \\frac{\\sigma}{\\mu}\n\\]\n\nWhile most often used to analyze dispersion around the mean, a quartile, quintile, or decile CV can also be used to understand variation around the median or 10th percentile, for example.\nShould only be used with variables that have minimum at zero (e.g. counts, prices) and not interval data (e.g celsius or fahrenheit)\nWhen the mean value is close to zero, the coefficient of variation will approach infinity and is therefore sensitive to small changes in the mean. This is often the case if the values do not originate from a ratio scale.\nA more robust possibility is the quartile coefficient of dispersion, half the interquartile range divided by the average of the quartiles (the midhinge)\n\\[\nCV_q = \\frac{0.5(Q_3 - Q_1)}{0.5(Q_3 + Q_1)}\n\\]\nFor small samples (Normal Distribution)\n\\[\nCV_*= (1 + \\frac{1}{4n}) CV\n\\]\nFor log-normal distribution\n\\[\nCV_{LN} = \\sqrt{e^{\\sigma^2_{ln}} - 1}\n\\]\n\nWhere \\(\\sigma_{ln}\\) is the standard deviation after a \\(\\ln\\) transformation of the data\n\n\nCovariance - Between two random variables is a measure of how correlated are their variations around their respective means\nGrand Mean - The mean of group means\n\nUse Cases\n\nHierarchical Group Comparison: Allows you to see how the average of each group relates to the overall average across all groups.\n\nExample: Imagine you collect data on the growth of various species of plants in different types of soil. Calculating the grand mean for plant height of each species allows you to see if any specific soil type consistently produces taller plants compared to the overall average.\n\nIdentifying Outliers: If the mean of a specific group deviates significantly from the grand mean, it might indicate the presence of outliers in that group.\nANOVA: It provides a baseline against which the individual group means are compared to assess if there are statistically significant differences between them.\n\nIt’s the intercept value when you use Sum-to-Zero (used in ANOVA) or Deviation contrasts (See Regression, Linear &gt;&gt; Contrasts &gt;&gt; Sum-to-Zero, Deviation)\n\nKernel Smoothing - Essence is the simple concept of a local average around a point, x; that is, a weighted average of some observable quantities, those of which closest to x being given the highest weights\nMargin of Error (MoE) - The range of values below and above the sample statistic in a confidence interval.\n\\[\n\\text{MOE}_\\gamma = z_\\gamma \\sqrt{\\frac{\\sigma^2}{n}}\n\\]\n\nZ-Score with confidence level γ ⨯ Standard Error\nIn general, for small sample sizes (under 30) or when you don’t know the population standard deviation, use a t-score to get the critical value. Otherwise, use a z-score.\n\nSee Null Hypothesis Significance Testing (NHST) &gt;&gt; Misc &gt;&gt; Z-Statistic Table for an example\n\nExample: a 95% confidence interval with a 4 percent margin of error means that your statistic will be within 4 percentage points of the real population value 95% of the time.\nExample: a Gallup poll in 2012 (incorrectly) stated that Romney would win the 2012 election with Romney at 49% and Obama at 48%. The stated confidence level was 95% with a margin of error of ± 2. We can conclude that the results were calculated to be accurate to within 2 percentages points 95% of the time.\n\nThe real results from the election were: Obama 51%, Romney 47%. So the Obama result was outside the range of the Gallup poll’s margin of error (2 percent).\n\n48 – 2 = 46 percent\n48 + 2 = 50 percent\n\n\n\nNormalization - Rescales the values into a specified range, typically [0,1]. This might be useful in some cases where all parameters need to have the same positive scale. However, the outliers from the data set are lost.\n\\[\n\\tilde{X} = \\frac{X-X_{\\text{min}}}{X_{\\text{max}}-X_{\\text{min}}}\n\\]\n\nSome functions have the option to normalize with range [-1, 1]\nBest option if the distribution of your data is unknown.\n\nParameter - Describes an entire population (also see statistic)\nP-Value - \\(\\text{p-value}(y) = \\text{Pr}(T(y_{\\text{future}}) &gt;= T(y) | H)\\)\n\n\\(H\\) is a “hypothesis,” a generative probability model\n\\(y\\) is the observed data\n\\(y_{\\text{future}}\\) are future data under the model\n\\(T\\) is a “test statistic,” some pre-specified specified function of data\n\nSampling Error - The difference between population parameter and the statistic that is calculated from the sample (such as the difference between the population mean and sample mean).\nStandard Error of the Mean (SEM) - Measures how far the sample mean (average) of the data is likely to be from the true population mean (Also see Fundamentals &gt;&gt; Interpreting s.d., s.e.m, and CI Bars)\n\\[\n\\text{SEM} = \\frac{\\text{SD}}{\\sqrt{n}}\n\\]\n\nAssumes a simple random sample with replacement from an infinite population\n\nStandardization rescales data to fit the Standard Normal Distribution which has a mean (μ) of 0 and standard deviation (σ) of 1 (unit variance).\n\\[\n\\tilde X = \\frac{X-\\mu}{\\sigma}\n\\]\n\nRecommended for PCA and if your data is known to come from a Gaussian distribution.\n\nStatistic - Describes a sample (also see parameter)\nVariance (\\(\\sigma^2\\))- Measures variation of a random variable around its mean.\nVariance-Covariance Matrix - Square matrix containing variances of the fitted model’s coefficient estimates and the pair-wise covariances between coefficient estimates.\n\\[\n\\begin{align}\n&\\text{Cov}(\\hat\\beta) = (X^TX)^{-1} \\cdot \\text{MSE}\\\\\n&\\text{where}\\;\\; \\text{MSE} = \\frac{\\text{SSE}}{\\text{DSE}} = \\frac{\\text{SSE}}{n-p}\n\\end{align}\n\\]\n\nDiagnonal is the variances, and the rest of the values are covariances\nThere’s also a variance/covariance matrix for error terms\nExample\nx &lt;- sin(1:100)\ny &lt;- 1 + x + rnorm(100)\nMSE &lt;- sum(residuals(lm(y ~ x))^2)/98 # where 98 is n-2\nvcov_mat &lt;- MSE * solve(crossprod(cbind(1, x)))\n\nvcov_mat is the same as vcov(lm(y ~ x))",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-nhst",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-nhst",
    "title": "Statistics",
    "section": "Null Hypothesis Significance Testing (NHST)",
    "text": "Null Hypothesis Significance Testing (NHST)\n\nMisc\n\nWhy would we not always use a non-parametric test so we do not have to bother about testing for normality? The reason is that non-parametric tests are usually less powerful than corresponding parametric tests when the normality assumption holds. Therefore, all else being equal, with a non-parametric test you are less likely to reject the null hypothesis when it is false if the data follows a normal distribution. It is thus preferred to use the parametric version of a statistical test when the assumptions are met.\nIf your statistic value is greater than the critical value, then it’s significant and you reject H0\n\n\nThink in terms of a distribution with statistic values on the x-axis, and greater than the critical value means you’re in the tail (one-sided)\n\nT-Statistic Table\nZ-Statistic Table\n\nExample: 95% CI → α = 100% - 95% = 0.05 → α/2 (1-tail) = 0.025\n\n1 - 0.025 = 0.975 (subtract from 1 because the z-score table cells are for the area left of the critical value\nThe z-score is 1.96 for a 95% CI\n\n\nZ-score comes from adding the row value with the column value that has the cell value of our area (e.g. 0.975) left of the critical value\nIf the area was between 0.97441 and 0.97500, then the z-score would be the row value, 1.9, added to the column value that’s half way between 0.05 and 0.06, which results in a z-score of 1.955\n\n\n\n\nType I Error - False-Positive; occurs if an investigator rejects a null hypothesis that is actually true in the population\n\nThe models perform equally well, but the A/B test still produces a statistically significant result. As a consequence, you may roll out a new model that doesn’t really perform better.\nYou can control the prevalence of this type of error with the p-value threshold. If your p-value threshold is 0.05, then you can expect a Type I error in about 1 in 20 experiments, but if it’s 0.01, then you only expect a Type I error in only about 1 in 100 experiments. The lower your p-value threshold, the fewer Type I errors you can expect.\n\nType II Error - False-Negative; occurs if the investigator fails to reject a null hypothesis that is actually false in the population\n\nThe new model is in fact better, but the A/B test result is not statistically significant.\nYour test is underpowered, and you should either collect more data, choose a more sensitive metric, or test on a population that’s more sensitive to the change.\n\nType S Error (Sign Error): The A/B test shows that the new model is significantly better than the existing model, but in fact the new model is worse, and the test result is just a statistical fluke. This is the worst kind of error, as you may roll out a worse model into production which may hurt the business metrics.\n\n{retrodesign} - Provides tools for working with Type S (Sign) and Type M (Magnitude) errors. (Vignette)\n\nType M error (Magnitude Error): The A/B test shows a much bigger performance boost from the new model than it can really provide, so you’ll over-estimate the impact that your new model will have on your business metrics.\n\n{retrodesign} - Provides tools for working with Type S (Sign) and Type M (Magnitude) errors. (Vignette)\n\nFalse Positive Rate (FPR)(\\(\\alpha\\)) - ; Probability of a type I error; Pr(measured effect is significant | true effect is “null”)\n\\[\n\\text{FPR} = \\frac{v}{m_0}\n\\]\n\n\\(v\\): Number of times there’s a false positive\n\\(m_0\\): Number of non-significant variables\n\nFalse Discovery Rate (FDR) - Pr(measured effect is null | true effect is significant)\n\n\\[\n\\text{FDR} = \\frac{\\alpha \\pi_0}{\\alpha \\pi_0 + (1-\\beta)(1-\\pi_0)}\n\\]\n\n\\(\\alpha\\) - Type I error rate (False Positive Rate)\n\\(\\beta\\) - Type II error rate (False Negative Rate)\n\\(1-\\beta\\) - Power\n\\(\\pi_0\\) - Count of true null effects\n\\(1−\\pi_0\\) - Count of true non-null effects\n\nPower - 1-β where beta is the Probability of a type II error\nFamily-Wise Error Rate (FWER) - the risk of at least one false positive in a family of S hypotheses.\n\\[\nFWER = \\frac{v}{R}\n\\]\n\n\\(v\\): Number of times there’s a false positive\n\\(R\\): Number of times we claim β ≠ 0\ne.g. Using the same data and variables to fit multiple models with different outcome variables (i.e. different hypotheses)\n\nRomano and Wolf’s Ccorrection\n\nAccounting for the dependence structure of the p-values (or of the individual test statistics) produces more powerful procedures than Bonferroi and Holms. This can be achieved by applying resampling methods, such as bootstrapping and permutations methods.\n\nPermutation tests of regression coefficients can result in rates of Type I error which exceed the nominal size, and so these methods are likely not ideal for such applications\n\nSee Stata docs of the procedure\nPackages\n\n{wildrwolf}: Implements Romano-Wolf multiple-hypothesis-adjusted p-values for objects of type fixest and fixest_multi from the fixest package via a wild cluster bootstrap.",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-boot",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-boot",
    "title": "Statistics",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nMisc\n\nPost-Hoc Analysis, General &gt;&gt; Frequentist &gt;&gt; Bootstrap\nDo NOT bootstrap the standard deviation (article)\n\nBootstrap is “based on a weak convergence of moments”\nif you use an estimate based standard deviation of the bootstrap, you are being overly conservative (i.e. you’re overestimating the sd and CIs are too wide)\n\nBootstrapping uses the original, initial sample as the population from which to resample, whereas Monte Carlo simulation is based on setting up a data generation process (with known values of the parameters of a known distribution). Where Monte Carlo is used to test drive estimators, bootstrap methods can be used to estimate the variability of a statistic and the shape of its sampling distribution\nUse bias-corrected bootstrapped CIs (article)\n\n“percentile and BCa methods were the only ones considered here that were guaranteed to return a confidence interval that respected the statistic’s sampling space. It turns out that there are theoretical grounds to prefer BCa in general. It is”second-order accurate”, meaning that it converges faster to the correct coverage. Unless you have a reason to do otherwise, make sure to perform a sufficient number of bootstrap replicates (a few thousand is usually not too computationally intensive) and go with reporting BCa intervals.”\n\nPackages\n\n{rsample}\n{DescTools::BootCI}\nboot and boot.ci\n{ebtools::get_boot_ci}\n\n\nSteps\n\nResample with replacement\nCalculate statistic of resample\nStore statistic\nRepeat 10K or so times\nCalculate mean, sd, and quantiles for CIs across all collected statistics\n\nBayesian Bootstrapping (aka Fractional Weighted Bootstrap)\n\nMisc\n\nNotes from\n\nThe Bayesian Bootstrap\nThread\n\nPackages\n\n{fwb}\n\n\nDescription\n\nDoesn’t resample the dataset, but samples a set of weights from the Uniform Dirichlet distribution and computes weighted averages (or whatever statistic)\nWeights sum to ‘n’ but may be non-integers\nEach row gets a frequency weight based on the number of times they appear\nIn this way, every row is included in the analysis but given a fractional weight that represents its contribution to the statistic\n\nIn a traditional bootstrap, some rows of data may not be sampled and therefore excluded from the calculation of the statistic\n\nParticularly useful with rare events, where a row excluded from a traditional bootstrap sample might cause the whole estimation to explode (e.g., in a rare-events logistic regression where one sample has no events!)\n\n\n\nShould be faster and consume less RAM\nPython implementation\ndef classic_boot(df, estimator, seed=1):\n    df_boot = df.sample(n=len(df), replace=True, random_state=seed)\n    estimate = estimator(df_boot)\n    return estimate\n\ndef bayes_boot(df, estimator, seed=1):\n    np.random.seed(seed)\n    w = np.random.dirichlet(np.ones(len(df)), 1)[0]\n    result = estimator(df, weights=w)\n    return result\n\nfrom joblib import Parallel, delayed\ndef bootstrap(boot_method, df, estimator, K):\n    r = Parallel(n_jobs=8)(delayed(boot_method)(df, estimator, seed=i) for i in range(K))\n    return r\n\ns1 = bootstrap(bayes_boot, dat, np.average, K = 1000)",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-desc",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-desc",
    "title": "Statistics",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nMeans\n\nGeometric Mean\nsummarize_revenue &lt;- function(tbl) {\n    tbl %&gt;%\n        summarize(geom_mean_revenue = exp(mean(revenue)),\n                  n = n())\n}\n\n\n\nProportions\n\nVariance of a proportion\n\nAssume that p applies equally to all n subjects\n\\[\n\\sigma^2_p = \\frac{p(1-p)}{n}\n\\]\n\nExample\n\nSample of 100 subjects where there are 40 females and 60 males\n10 of the females and 30 of the males have the disease\n\nMarginal estimate of the probability of disease is (30+10)/100 = 0.4\n\nVariance of the estimator assuming constant risk (i.e. assuming risk for females = risk for males)\n\n(prob_of_disease × prob_not_disease) / n = (0.4 × 0.6) / 100 = 0.0024\n\np = (10 + 30) / 100 = 0.40\n\n\n\n\nAssume p depends on a variable (e.g. sex)\n\\[\n\\sigma^2_p = \\frac{p^2_{1,n} \\cdot p_1(1-p_1)}{n_1} + \\frac{p^2_{2,n} \\cdot p_2(1-p_2)}{n_2}\n\\]\n\nExample\n\nDescription same as above\nAdjusted marginal estimate of the probability of disease is\n\n(prop_female × prop_disease_female) + (prop_male × prop_disease_male)\n(0.4 × 0.25) + (0.6 × 0.5) = 0.4\nSame marginal estimate as before\n\nVariance of the estimator assuming varying risk (i.e. assumes risk for females \\(\\neq\\) risk for males)\n\n1st half of equation:\n\nprop_female2 × (prop_disease_female × prop_not_disease_female) / n_female = [0.42 × (0.25 × 0.74) / 40]\n\n2nd half of equation\n\nprop_male2 × (prop_disease_male × prop_not_disease_male) / n_male = [0.62 × (0.5 × 0.5) / 60]\n\n1st half + 2nd half = 0.00224\n\nVariance is smaller than before\n\n\n\n\n\nCIs\n\nPackages:\n\n{binomCI} - 12 confidence intervals for one binomial proportion or a vector of binomial proportions are computed\n\nJeffrey’s Interval\n# probability of event\n# n_rain in the number of events (rainy days)\n# n is the number of trials (total days)\nmutate(pct_rain = n_rain / n, \n          # jeffreys interval\n          # bayesian CI for binomial proportions\n          low = qbeta(.025, n_rain + .5, n - n_rain + .5), \n          high = qbeta(.975, n_rain + .5, n - n_rain + .5))\n\n\n\n\nSkewness\n\nPackages:\n\n{moments} - Standard algorithm\n{e1071} - 3 alg options\n{DescTools::Skew} - Same algs but with bootstraps CIs\n\nFrom the paper referenced in e1071, b1 (type 3) is better for non-normal population distributions and G1 (type 2) is better for normal population distributions\nSymmetric: Values between -0.5 to 0.5\nModerated Skewed data: Values between -1 and -0.5 or between 0.5 and 1\nHighly Skewed data: Values less than -1 or greater than 1\nRelationship between Mean and Median under different skewness\n\n\n\n\nKurtosis\n\nA high kurtosis distribution has a sharper peak and longer fatter tails, while a low kurtosis distribution has a more rounded peak and shorter thinner tails.\nTypes\n\n\nMesokurtic: kurtosis = ~3\n\nExamples: normal distribution. Also binomial distribution when p = 1/2 +/- sqrt(1/12)\n\nLeptokurtic: This distribution has fatter tails and a sharper peak. Excess kurtosis &gt; 3\n\nExamples: Student’s t-distribution, Rayleigh distribution, Laplace distribution, exponential distribution, Poisson distribution and the logistic distribution\n\nPlatykurtic: The distribution has a lower and wider peak and thinner tails. Excess kurtosis &lt; 3\n\nExamples: continuous and discrete uniform distributions, raised cosine distribution, and especially the Bernoulli distribution\n\nExcess Kurtosis is the kurtosis value - 3\n\n\n\n\nUnderstanding CI, SD, and SEM Bars\n\narticle\nP-values test whether the sample means are different from each other\nsd bars: Show the population spread around each sample mean. Useful as predictors of the range of new sample.\n\nNever seen these and it seems odd to mix a sample statistic with a population parameter and that the range is centered on the sample mean (unless the sample size is large I guess).\n\ns.e.m. is the “standard error of the mean” (See Terms)\n\nIn large samples, the s.e.m. bar can be interpreted as a 67% CI.\n95% CI ≈ 2 × s.e.m. (n &gt; 15)\n\nFigure 1\n\n\nEach plot shows 2 points representing 2 sample means\nPlot a: bars of both samples touch and are the same length\n\nsem bars intepretation: Commonly held view that “if the s.e.m. bars do not overlap, the difference between the values is statistically significant” is NOT correct. Bars touch here but don’t overlap and the difference in sample means is NOT significant.\n\nPlot b: p-value = 0.05 is fixed\n\nsd bar interpretation: Although the means differ, and this can be detected with a sufficiently large sample size, there is considerable overlap in the data from the two populations.\nsem bar intepretation: For there to be a significant difference in sample means, sem bars have to much further away from each other than there just being a recognizable space between the bars.\n\n\nFigure 2\n\n\nPlot a: shows how a\n\n95% CI captures the population mean 95% of the time but as seen here, only 18 out of 20 sample CIs (90%) contained the population mean (i.e. this is an asymptotic claim)\nA common misconception about CIs is an expectation that a CI captures the mean of a second sample drawn from the same population with a CI% chance. Because CI position and size vary with each sample, this chance is actually lower.\n\nPlot b:\n\nHard to see at first but the outer black bars are the 95% CI and the inner gray bars are the sem.\nBoth the CI and sem shrink as n increases and the sem is always encompassed by the CI\n\n\nFigure 3\n\n\nsem bars must be separated by about 1 sem (which is half a bar) for a significant difference to be reached at p-value = 0.05\n95% CI bars can overlap by as much as 50% and still indicate a significant difference at p-value = 0.05\n\nIf 95% CI bars just touch, the result is highly significant (P = 0.005)",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-desc-pvfun",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-desc-pvfun",
    "title": "Statistics",
    "section": "P-Value Function",
    "text": "P-Value Function\n\nNotes from https://ebrary.net/72024/health/value_confidence_interval_functions\n{concurve} creates these curves\nGives a more complete picture than just stating the p-value (strength and precision of the estimate)\n\nShows level of precision of the point estimate via shape of the curve\n\nnarrow-based, spikey curves = more precise\n\nVisualizes strength of the effect along the x-axis. Helps in showing “significant” effect is not necessarily a meaningful effect.\n\nShows other estimate(s) that are also consistent with that p-value\nShows p-values associated with other estimates for the Null Hypothesis\n\nsee the end of the article for discussion on using this fact in an interpretation context\n\nThe P-value function is closely related to the set of all confidence intervals for a given estimate. (see example 3)\nExample 1\n\n\np-value of the point estimate is (always?) 1 which says, “given a null hypothesis =  is true (i.e. the true risk ratio = ),  the probability of seeing data produce this estimate or this estimate with more strength (ie smaller std error) is 100%.”\n\nI.e. the pt est is the estimate most compatible with the data.\nThis pval language is mine. The whole “this data or data more extreme” has never sit right with me. I think this is more correct if my understanding is right.\n\nThe pval for the data in this example is at 0.08 for a H0 of 1. So unlikely, but typically not unlikely enough in order to reject the null hypothesis.\nA pval of 0.08 is identical for a pt est = 1 or pt est = 10.5\nWide base of the curve indicates the estimate is imprecise. There’s potentially a large effect or little or no effect.\n\nExample 2\n\n\nmore data used for the second curve which indicates a precise point estimate.\npt est very close to H0\npval = 0.04 (not shown in plot)\n\nso the arbitrary pval = 0.05 threshold is passed and says a small effect is probably present\nIs that small of an effect meaningful even if it’s been deemed statistically present?\n\nIn this case a plot with the second curve helps show that “statistically significant” doesn’t necessarily translate to meaningful effect\n\nExample 3\n\n\nThe different confidence intervals reflect the same degree of precision (i.e. the curve width doesn’t change when moving from one CI to another).\nThe three confidence intervals are described as nested confidence intervals. The P-value function is a graph of all possible nested confidence intervals for a given estimate, reflecting all possible levels of confidence between 0% and 100%.",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/missingness.html",
    "href": "qmd/missingness.html",
    "title": "Missingness",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-misc",
    "href": "qmd/missingness.html#sec-missing-misc",
    "title": "Missingness",
    "section": "",
    "text": "Also see\n\nEDA &gt;&gt; Missingness\nModel Building, tidymodels &gt;&gt; Recipe &gt;&gt; Imputation\n\nBagging and knn methods for cross-sectional data\nRolling method for time series data\n\nHarrell RMS 3.5 Strategies for Developing an Imputation Model\n\n“But more precisely, even having the correct model of the analysis stage does not absolve the analyst of considering the relationship between the imputation stage variables, the causal model, and the missingness mechanism. It turns out that in this simple example, imputing with an analysis-stage collider is innocuous (so long as it is excluded at the analysis stage). But imputation-stage colliders can wreck MI even if they are excluded from the analysis stage.”\n\nSee Multiple Imputation with Colliders\n\n**Don’t impute missing values before your training/test split\nImputing Types full-information maximum likelihood\n\nMultiple imputation\nOne-Step Bayesian imputation\n\nMissness Types (MCAR, MAR, and MNAR)\n\nMultivariate Imputation with Chained Equation (MICE) assumes MAR\n\nMethod entails creating multiple imputations for each missing value as opposed to just one. The algorithm addresses statistical uncertainty and enables users to impute values for data of different types.\n\nStochastic Regression Imputation is problematic\n\nPopular among practitioners though\nIssues\n\nStochastic regression imputation might lead to implausible values (e.g. negative incomes).\nStochastic regression imputation has problems with heteroscedastic data\n\nBayesian PMM handles these issues\n\nMissingness in RCT due dropouts (aka loss to follow-up)\n\nNotes from To impute or not: the case of an RCT with baseline and follow-up measurements\n\n{mice} used for imputation\n\nBias in treatment effect due to missingness\n\nIf there are adjustment variables that affect unit dropout then bias increases as variation in treatment effect across units increases (aka hetergeneity)\n\nIn the example, a baseline measurement of the outcome variable, used an explanatory variable, was also causal of missingness. Greater values of this variable resulted in greater bias\nUsing multiple imputation resulted in less bias than just using complete cases, but still underestimated the treatment effect.\n\nIf there are no such variables, then there is no bias due to hetergeneous treatment effects\n\nComplete cases of the data can be used\n\n\nLast observation carried forward\n\nSometimes used in clinical trials because it tends to be conservative, setting a higher bar for showing that a new therapy is significantly better than a traditional therapy.\nMust assume that the previous value (e.g. 2008 score) is similar to the ahead value (e.g. 2010 score).\nInformation about trajectories over time is thrown away.\n\n\nAssessment of Imputations\n\nSee {naniar} vignette - Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations | Journal of Statistical Software",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-caim",
    "href": "qmd/missingness.html#sec-missing-caim",
    "title": "Missingness",
    "section": "Choosing an Imputation Method",
    "text": "Choosing an Imputation Method\n\n** Don’t use this. Just putting it here to be aware of **) Standard Procedure for choosing an imputation method\n\nIssues\n\nSome methods will be favored based on the metric used\n\nConditional means methods (RMSE)\nConditional medians methos (MAE) Chosen methods tend to artificially strengthen the association between variables. As a consequence, statistical estimation and inference techniques applied to the so-imputed data set can be invalid.\n\n\nSteps\n\nSelect some observations\nSet their status to missing\nImpute them with different methods\nCompare their imputation accuracy\n\nFor numeric variables, RMSE or MAE typically used\nFor categoricals, percentage of correct predictions (PCP)\n\n\n\nInitial Considerations\n\nIf a dataset’s feature has missing data in more than 80% of its records, it is probably best to remove that feature altogether.\nIf a feature with missing values is strongly correlated with other missing values, it’s worth considering using advanced imputation techniques that use information from those other features to derive values to replace the missing data.\nIf a feature’s values are missing not at random (MNAR), remove methods like MICE from consideration. I-Score {Iscores}, Paper\nA proper scoring rule metric\nConsistent for MCAR, but MAR requires additional assumptions\n\n“valid under missing at random (MAR) if we restrict the random projections in variable space to always include all variables, which in turn requires access to some complete observations”\n\nKinda complicated. I need to read the paper",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-bayes",
    "href": "qmd/missingness.html#sec-missing-bayes",
    "title": "Missingness",
    "section": "Bayesian",
    "text": "Bayesian\n\nPredictive Mean Matching (PMM)\n\nNotes from:\n\nPredictive Mean Matching Imputation (Theory & Example in R)\nPredictive Mean Matching Imputation in R (mice Package Example)\n\nUses a bayesian regression to predict a missing value, then randomly picks a value from a group of observed values that are closest to the predicted value.\nSteps\n\nEstimate a linear regression model:\n\nUse the variable we want to impute as Y.\nUse a set of good predictors as X (Guidelines for the selection of X can be found in van Buuren, 2012, p. 128).\nUse only the observed values of X and Y to estimate the model.\n\nDraw randomly from the posterior predictive distribution of β^ and produce a new set of coefficients β∗.\n\nThis bayesian step is needed for all multiple imputation methods to create some random variability in the imputed values.\n\nCalculate predicted values for observed and missing Y.\n\nUse β^ to calculate predicted values for observed Y.\nUse β∗ to calculate predicted values for missing Y.\n\nFor each case where Y is missing, find the closest predicted values among cases where Y is observed.\n\nExample:\n\nYi is missing. Its predicted value is 10 (based on β∗).\nOur data consists of five observed cases of Y with the values 6, 3, 22, 7, and 12.\nIn step 3, we predicted the values 7, 2, 20, 9, and 13 for these five observed cases (based on β^).\nThe predictive mean matching algorithm selects the closest observed values (typically three cases) to our missing value Yi. Hence, the algorithm selects the values 7, 9, and 13 (the closest values to 10).\n\n\nDraw randomly one of these three close cases and impute the missing value Yi with the observed value of this close case.\n\nExample continued:\n\nThe algorithm draws randomly from 6, 7, and 12 (the observed values that correspond to the predicted values 7, 9, and 13).\nThe algorithm chooses 12 and substitutes this value to Yi.\n\n\nIn case of multiple imputation (which I strongly advise), steps 1-5 are repeated several times.\n\nEach repetition of steps 1-5 creates a new imputed data set.\nWith multiple imputation, missing data is typically imputed 5 times.\n\n\nExample\ndata_imp &lt;- \n  complete(mice(data,\n           m = 5,\n           method = \"pmm\"))\n\nm is the number of times to impute the data\ncomplete formats the data into different shapes according to an action argument\nRunning parmice instead of mice imputes in parallel",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-multimp",
    "href": "qmd/missingness.html#sec-missing-multimp",
    "title": "Missingness",
    "section": "Multiple Imputation",
    "text": "Multiple Imputation\n\nAKA “multiply” imputed data\nPackages\n\n{NPBayesImputeCat}: Non-Parametric Bayesian Multiple Imputation for Categorical Data\n\nProvides routines to i) create multiple imputations for missing data and ii) create synthetic data for statistical disclosure control, for multivariate categorical data, with or without structural zeros\nImputations and syntheses are based on Dirichlet process mixtures of multinomial distributions, which is a non-parametric Bayesian modeling approach that allows for flexible joint modeling\nVignette\n\n\nFitting a regression model\n\nSee If you fit a model with multiply imputed data, you can still plot the line\nMethods\n\nPredict then Combine (PC)\nCombine then Predict (CP)",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-ts",
    "href": "qmd/missingness.html#sec-missing-ts",
    "title": "Missingness",
    "section": "Time Series",
    "text": "Time Series\n\nIf seasonality is present, mean, median, mode, random assignment, or previous value methods shouldn’t be used.",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/mlops.html",
    "href": "qmd/mlops.html",
    "title": "MLOps",
    "section": "",
    "text": "MLflow",
    "crumbs": [
      "MLOps"
    ]
  },
  {
    "objectID": "qmd/mlops.html#sec-mlops-mlflow",
    "href": "qmd/mlops.html#sec-mlops-mlflow",
    "title": "MLOps",
    "section": "",
    "text": "Tracking\n\nMisc\n\nmlflow_log_batch(df) - A dataframe with columns key, value, step, timestamp\n\nKey can be names of metrics, params\nStep is probably for loops\nTimestamp can be from Sys.time() probably\n\nmlflow.search_runs() - querying runs\n\nAvailable columns greatly exceed those available in the experiments GUI\nExample: py\n# Create DataFrame of all runs in *current* experiment\ndf = mlflow.search_runs(order_by=[\"start_time DESC\"])\n\n# Print a list of the columns available\n# print(list(df.columns))\n\n# Create DataFrame with subset of columns\nruns_df = df[\n    [\n        \"run_id\",\n        \"experiment_id\",\n        \"status\",\n        \"start_time\",\n        \"metrics.mse\",\n        \"tags.mlflow.source.type\",\n        \"tags.mlflow.user\",\n        \"tags.estimator_name\",\n        \"tags.mlflow.rootRunId\",\n    ]\n].copy()\nruns_df.head()\n\n# add additional useful columns\nruns_df[\"start_date\"] = runs_df[\"start_time\"].dt.date\nruns_df[\"is_nested_parent\"] = runs_df[[\"run_id\",\"tags.mlflow.rootRunId\"]].apply(lambda x: 1 if x[\"run_id\"] == x[\"tags.mlflow.rootRunId\"] else 0, axis=1)\nruns_df[\"is_nested_child\"] = runs_df[[\"run_id\",\"tags.mlflow.rootRunId\"]].apply(lambda x: 1 if x[\"tags.mlflow.rootRunId\"] is not None and x[\"run_id\"] != x[\"tags.mlflow.rootRunId\"]else 0, axis=1)\nruns_df\n\n\nSet experiment name and get experiment id\n\nSyntax: mlflow_set_experiment(\"experiment_name\")\n\nThis might require a path e.g. “/experiment-name” instead the name\n\nExperiment IDs can be passed to start_run() (see below) to ensure that the run is logged into the correct experiment\n\nExample: py\nmy_experiment = mlflow.set_experiment(\"/mlflow_sdk_test\")\nexperiment_id = my_experiment.experiment_id\nwith mlflow.start_run(experiment_id=experiment_id):\n\n\nStarting runs\n\nUsing a mlflow_log_ function automatically starts a run, but then you have to mlflow_end_run\nUsing with(mlflow_start_run(){}) stops the run automatically once the code inside the with() function is completed\nmlflow_start_run(\n    run_id = NULL,\n    experiment_id = only when run id not specified,\n    start_time = only when client specified,\n    tags = NULL,\n    client = NULL)\nExample: R\nlibrary(mlflow)\nlibrary(glmnet)\n\n# can format the variable outside the log_param fun or inside\nalpha &lt;- mlflow_param(args)\n\n# experiment contained inside start_run\nwith(mlflow_start_run( ) {\n\n    alpha_fl &lt;- mlflow_log_param(\"alpha\" = alpha)\n    lambda_fl &lt;- mlflow_log_param(\"lambda\" = mlflow_param(args))\n\n    mod &lt;- glmnet(args)\n\n    # preds\n    # error\n\n    # see Models section below for details\n    mod_crate &lt;- carrier::crate(~glmnet::glmnet.predict(mod, train_x), mod)\n    mlflow_log_model(mod_crate, \"model_folder\")\n\n    mlflow_log_metric(\"MAE\", error)\n\n})\n# this might go on the inside if you're looping the \"with\" FUN and want to log results of each loop\nmlflow_end_run()\n\n# not working, logs run, but doesn't log metrics\n# run saved script\nmlflow::mlflow_run(entry_point = \"script.R\")\nExample: python\n\n\n# End any existing runs\nmlflow.end_run()\n\n\nwith mlflow.start_run() as run:\n    # Turn autolog on to save model artifacts, requirements, etc.\n    mlflow.autolog(log_models=True)\n\n\n    print(run.info.run_id)\n\n\n    diabetes_X = diabetes.data\n    diabetes_y = diabetes.target\n\n\n    # Split data into test training sets, 3:1 ratio\n    (\n        diabetes_X_train,\n        diabetes_X_test,\n        diabetes_y_train,\n        diabetes_y_test,\n    ) = train_test_split(diabetes_X, diabetes_y, test_size=0.25, random_state=42)\n\n\n    alpha = 0.9\n    solver = \"cholesky\"\n    regr = linear_model.Ridge(alpha=alpha, solver=solver)\n\n\n    regr.fit(diabetes_X_train, diabetes_y_train)\n\n\n    diabetes_y_pred = regr.predict(diabetes_X_test)\n\n\n    # Log desired metrics\n    mlflow.log_metric(\"mse\", mean_squared_error(diabetes_y_test, diabetes_y_pred))\n    mlflow.log_metric(\n        \"rmse\", sqrt(mean_squared_error(diabetes_y_test, diabetes_y_pred))\n\nCustom run names\n\nExample: py\n\n\n# End any existing runs\nmlflow.end_run()\n\n\n# Explicitly name runs\ntoday = dt.today()\n\nrun_name = \"Ridge Regression \" + str(today)\n\nwith mlflow.start_run(run_name=run_name) as run:\nPreviously unlogged metrics can be retrieved retroactively with the run id\n# py\nwith mlflow.start_run(run_id=\"3fcf403e1566422493cd6e625693829d\") as run:\n    mlflow.log_metric(\"r2\", r2_score(diabetes_y_test, diabetes_y_pred))\n\nThe run_id can either be extracted by print(run.info.run_id) from the previous run, or by querying mlflow.search_runs() (See Misc above).\n\n\nNested Runs\n\nUseful for evaluating and logging parameter combinations to determine the best model (i.e. grid search), they also serve as a great logical container for organizing your work. With the ability to group experiments, you can compartmentalize individual data science investigations and keep your experiments page organized and tidy.\nExample: py; start a nested run\n# End any existing runs\nmlflow.end_run()\n\n\n# Explicitly name runs\nrun_name = \"Ridge Regression Nested\"\n\n\nwith mlflow.start_run(run_name=run_name) as parent_run:\n    print(parent_run.info.run_id)\n\n\n    with mlflow.start_run(run_name=\"Child Run: alpha 0.1\", nested=True):\n        # Turn autolog on to save model artifacts, requirements, etc.\n        mlflow.autolog(log_models=True)\n\n\n        diabetes_X = diabetes.data\n        diabetes_y = diabetes.target\n\n\n        # Split data into test training sets, 3:1 ratio\n        (\n            diabetes_X_train,\n            diabetes_X_test,\n            diabetes_y_train,\n            diabetes_y_test,\n        ) = train_test_split(diabetes_X, diabetes_y, test_size=0.25, random_state=42)\n\n\n        alpha = 0.1\n        solver = \"cholesky\"\n        regr = linear_model.Ridge(alpha=alpha, solver=solver)\n\n\n        regr.fit(diabetes_X_train, diabetes_y_train)\n\n        diabetes_y_pred = regr.predict(diabetes_X_test)\n\n\n        # Log desired metrics\n        mlflow.log_metric(\"mse\", mean_squared_error(diabetes_y_test, diabetes_y_pred))\n        mlflow.log_metric(\n            \"rmse\", sqrt(mean_squared_error(diabetes_y_test, diabetes_y_pred))\n        )\n        mlflow.log_metric(\"r2\", r2_score(diabetes_y_test, diabetes_y_pred))\n\nalpha 0.1 is the parameter value being evaluated\n\nExample: py; add child runs\n# End any existing runs\nmlflow.end_run()\n\nwith mlflow.start_run(run_id=\"61d34b13649c45699e7f05290935747c\") as parent_run:\n    print(parent_run.info.run_id)\n\n\n    with mlflow.start_run(run_name=\"Child Run: alpha 0.2\", nested=True):\n        # Turn autolog on to save model artifacts, requirements, etc.\n        mlflow.autolog(log_models=True)\n\n\n        diabetes_X = diabetes.data\n        diabetes_y = diabetes.target\n\n\n        # Split data into test training sets, 3:1 ratio\n        (\n            diabetes_X_train,\n            diabetes_X_test,\n            diabetes_y_train,\n            diabetes_y_test,\n        ) = train_test_split(diabetes_X, diabetes_y, test_size=0.25, random_state=42)\n\n\n        alpha = 0.2\n        solver = \"cholesky\"\n\n\n        regr = linear_model.Ridge(alpha=alpha, solver=solver)\n\n        regr.fit(diabetes_X_train, diabetes_y_train)\n\n\n        diabetes_y_pred = regr.predict(diabetes_X_test)\n\n\n        # Log desired metrics\n        mlflow.log_metric(\"mse\", mean_squared_error(diabetes_y_test, diabetes_y_pred))\n        mlflow.log_metric(\n            \"rmse\", sqrt(mean_squared_error(diabetes_y_test, diabetes_y_pred))\n        )\n        mlflow.log_metric(\"r2\", r2_score(diabetes_y_test, diabetes_y_pred))\n\nAdd to nested run by using parent run id, e.g. run_id=“61d34b13649c45699e7f05290935747c”\n\nObtained by print(parent_run.info.run_id) from the previous run or querying via mlflow.search_runs (see below)\n\n\n\nQuery Runs\n\nAvailable columns greatly exceed those available in the experiments GUI\nExample: py; Create Runs df\n# Create DataFrame of all runs in *current* experiment\ndf = mlflow.search_runs(order_by=[\"start_time DESC\"])\n\n\n# Print a list of the columns available\n# print(list(df.columns))\n\n\n# Create DataFrame with subset of columns\nruns_df = df[\n    [\n        \"run_id\",\n        \"experiment_id\",\n        \"status\",\n        \"start_time\",\n        \"metrics.mse\",\n        \"tags.mlflow.source.type\",\n        \"tags.mlflow.user\",\n        \"tags.estimator_name\",\n        \"tags.mlflow.rootRunId\",\n    ]\n].copy()\nruns_df.head()\n\n\n# add additional useful columns\nruns_df[\"start_date\"] = runs_df[\"start_time\"].dt.date\nruns_df[\"is_nested_parent\"] = runs_df[[\"run_id\",\"tags.mlflow.rootRunId\"]].apply(lambda x: 1 if x[\"run_id\"] == x[\"tags.mlflow.rootRunId\"] else 0, axis=1)\nruns_df[\"is_nested_child\"] = runs_df[[\"run_id\",\"tags.mlflow.rootRunId\"]].apply(lambda x: 1 if x[\"tags.mlflow.rootRunId\"] is not None and x[\"run_id\"] != x[\"tags.mlflow.rootRunId\"]else 0, axis=1)\nruns_df\nQuery Runs Object\n\nExample: Number of runs per start date\n\npd.DataFrame(runs_df.groupby(\"start_date\")[\"run_id\"].count()).reset_index()\nExample: How many runs have been tested for each algorithm?\n\npd.DataFrame(runs_df.groupby(\"tags.estimator_name\")[\"run_id\"].count()).reset_index()\n\n\n\n\n\nProjects\n\nName of the file is standard - “MLproject”\n\nyaml file but he didn’t give it an extension\nMulti-Analysis flows take the output of one script and input to another. The first script outputs the object somewhere in the working dir or a sub dir. The second script takes that object as a parameter with value = path.,\n\ne.g. dat.csv: path. See example https://github.com/mlflow/mlflow/tree/master/examples/multistep_workflow\n\n\nExample\nname: MyProject\n\nenvir: specify dependencies using packrat snapshot (didn't go into details)\n\nentry points:\n    # \"main\" is the default name used. Any script name can be an entry point name.\n    main:\n        parameters:\n            # 2 methods, looks like same args as mlflow_param or mlflow_log_param\n            # python types used, e.g. float instead of numeric used\n            alpha: {type: float, default: 0.5}\n            lambda:\n                type: float\n                default: 0.5\n        # CLI commands to execute the script\n        # sigh, he used -P in the video and -r on the github file\n                # he used a -P for each param when executing from CLI, so that might be correct\n                # Although that call to Rscript makes me think it might not be correct\n        command: \"Rscript &lt;script_name&gt;.R -P alpha={alpha} -P lambda={lambda}\"\n        # another one of their python examples\n        command: \"python etl_data.py --ratings-csv {ratings_csv} --max-row-limit {max_row_limit}\"\n        # This is similar to one of python their examples and it jives with Rscript practice, except there's a special function in the R script to take the args\n        # command: \"Rscript &lt;script_name&gt;.R {alpha} {lambda}\"\n\n    # second script, same format as 1st script\n    validate:\n        blah, blah\nRun script with variable values from the CLI\nmlflow\nmlflow run . --entry-point script.R -P alpha=0.5 -P lambda=0.7\n\nmlflow starts mlflow.exe\n. says run from current directory\nalso guessing entry point value is a path from the working directory\n\nRun script from github repo\n$mlflow run https://github.com/ercbk/repo --entry-point script.R -P alpha=0.5 -P lambda=0.7\n\nAdds link to repo in source col in ui for that run\nAdds link to commit (repo version) at the time of the run in the version col in the ui for that run\n\n\n\n\nModels\n\nTypically, models in R exist in memory and can be saved as .rds files. However, some models store information in locations that cannot be saved using save() or saveRDS() directly. Serialization packages can provide an interface to capture this information, situate it within a portable object, and restore it for use in new settings.\n\n{crate} - formats the model into a binary file so it can be run by a system (e.g. API) regardless of the language used to create it\n\nsaves it as a bin file, crate.bin\n\n{bundle} - similar for tidymodels’ objects\n\nmlflow_save_model  creates a directory with the bin file and a MLProject file\nExamples\n\nUsing a function\nmlflow_save_model(carrier::crate(function(x) {\n            library(glmnet)\n            # glmnet requires a matrix\n            predict(model, as.matrix(x))\n}, model = mod), \"dir_name\")\n\npredict usually takes a df but glmnet requires a matrix\nmodel = mod is the parameter being passed into the function environment\n[dir_name][var.text] is the name of the folder that will be created\n\nUsing a lambda function\nmlflow_save_model(carrier::crate(~glmnet::predict.glmnet(model, as.matrix(.x)), model = mod), \"penal_glm\")\n\nRemoved the library function (could’ve done that before as well)\n*** lambda functions require .x instead of just x ***\nThe folder name is penal_glm\n\n\nServing a model as an API from the CLI\n&gt;&gt; mlflow models serve -m file:penal_glm\n\nmlflow runs mlflow.exe\nserve says create an API\n-m is for specifying the URI of the bin file\n\nCould be an S3 bucket\nfile: says it’s a local path\n\nDefault host:port 127.0.0.1:5000\n\n-h, -p can specify others\n\n*** Newdata needs to be in json column major format ***\n\nPrediction is outputted in json as well\nExample: Format in column major\njsonlite::toJSON(newdata_df, matrix = \"columnmajor\")\nExample: Send json newdata to the API\n# CLI example for \ncurl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{    \"columns\": [\"a\", \"b\", \"c\"],    \"data\": [[1, 2, 3], [4, 5, 6]]}'\n\n\n\n\n\nUI\n\nmlflow_ui( )\nClick date,\n\nmetric vs runs\nnotes\nartifact\n\nIf running through github\n\nlink to repo in source column for that run\nlink to commit (repo version) at the time of the run in the version column",
    "crumbs": [
      "MLOps"
    ]
  },
  {
    "objectID": "qmd/mlops.html#sec-mlops-targets",
    "href": "qmd/mlops.html#sec-mlops-targets",
    "title": "MLOps",
    "section": "Targets",
    "text": "Targets\n\nMisc\n\nuse_targets\n\nCreates a “_targets.R” file in the project’s root directory\n\nConfigures and defines the pipeline\n\nload packages\nHPC settings\nLoad Functions from scripts\nTarget pipeline\n\n\nFile has commented lines to guide you through the process\n\nCheck Pipeline\n\ntar_manifest(fields = command)\n\nlists names of targets and the functions to execute them\n\ntar_visnetwork()\n\nShows target dependency graph\nCould be slow if you have a lot of targets, so may want to comment in/out sections of targets and view them in batches.\n\n\nRun tar_make in the background\n\nPut into .Rprofile in project\n\nmake &lt;- function() {\n    job::job(\n        {{ targets::tar_make() }},\n        title = \"&lt;whatever&gt;\"\n    )\n}\nGet a target from another project\nwithr::with_dir(\n          \"~/workflows/project_name/\",\n          targets::tar_load(project_name)\n      )\n\n\n\nTarget Pipeline\n\nExample\nlist(\n    tar_target(file, \"data.csv\", format = \"file\"),\n    tar_target(data, get_data(file)),\n    tar_target(model, fit_model(data)),\n    tar_target(plot, plot_model(model, data))\n)\n\n1st arg is the target name (e.g. file, data, model, plot)\n2nd arg is a function\n\nFunction inputs are target names\nExcept first target which has a file name for the 2nd arg\n\n“format” arg says that this target is a file and if the contents change, a re-hash should be triggered.\n\n\n\ntar_make() - Execute pipeline\n\nOutput saved in _targets &gt;&gt; objects\n\ntar_read(target_name) - Reads the output of a target\n\ne.g. If it’s a plot output, a plot will be rendered in the viewer.",
    "crumbs": [
      "MLOps"
    ]
  },
  {
    "objectID": "qmd/mlops.html#sec-mlops-dask",
    "href": "qmd/mlops.html#sec-mlops-dask",
    "title": "MLOps",
    "section": "Dask",
    "text": "Dask\n\nMisc\n\nNotes from Saturn Dask in the Cloud video\nXGBoost, RAPIDS, LightGLM libraries can natively recognize Dask DataFrames and use parallelize using Dask\n{{dask-ml}} can be used to simplify training multiple models in parallel\nPyTorch DDP (Distributed Data Parallel)\n\n{{dask_pytorch_ddp}} for Saturn\nEach GPU has it’s own version of the model and trains concurrently on a data batch\nResults are shared between GPUs and a combined gradient is computed\n\nfrom dask_pytorch_ddp import dispatch\nfutures = dispatch.run(dask_client, model_training_function)\n\n\n\nBasic Usage\n\nDask Collections\n\nDask DataFrames - Mimics Pandas DataFrames\n\nThey’re essentially collection of pandas dfs spread across workers\n\nDask Arrays - Mimics NumPy\nDask Bags - Mimics map, filter, and other actions on collections\n\nStorage\n\nCloud storage (e.g. S3, EFS) can be queried by Dask workers\nSaturn also provides shared folders that attach directly to Dask workers.\n\nUse Locally\nimport dask.dataframe as dd\nddf = dd.read.csv(\"data/example.csv\")\nddf.groupby('col_name').mean().compute()\n\ncompute starts the computation and collects the results.\n\nEvidently other functions can have this effect (see example). Need to check docs.\n\nFor a Local Cluster, the cluster = LocalCluster() and Client(cluster) commands are used. Recommende to initialize such a cluster only once in code using the Singleton pattern. You can see how it can be implemented in Python here. Otherwise, you will initialize a new cluster every launch;\n\nSpecify chunks and object type\nfrom dask import dataframe as dd\nddf = dd.read_csv(r\"FILEPATH\", dtype={'SimillarHTTP': 'object'},blocksize='64MB')\nFit sklearn models in parallel\nimport joblib\nfrom dask.distributed import Client\nclient = Client(processes=False)\n\nwith joblib.parallel_backend(\"dask\"):\n    rf.fit(X_train, y_train)\n\nNot sure if client is needed here\n\n\n\n\nEvaluation Options\n\nDask Delayed\n\nFor user-defined functions — allows dask to parallelize and lazily compute them\n\n@dask.delayed\ndef double(x):\n    return x*2\n\n@dask.delayed\ndef add(x, y):\n    return x + y\n\na = double(3)\nb = double(2)\ntotal = add(a,b) # chained delayed functions\ntotal.compute() # evaluates the functions\nFutures\n\nEvaluated immediately in the background\nSingle function\ndef double(x):\n    return x*2\nfuture = client.submit(double, 3)\nIterable\nlearning_rates = np.arange(0.0005, 0.0035, 0.0005)\nfutures = client.map(train_model, learning_rates) # map(function, iterable)\ngathered_futures = client.gather(futures)\nfutures_computed = client.compute(futures_gathered, resources = {\"gpu\":1})\n\nresources tells dask to only send one task per gpu-worker in this case\n\n\n\n\n\nMonitoring\n\nLogging\nfrom distributed.worker import logger\n@dask.delayed\ndef log():\n    logger.info(f'This is sent to the worker log')\n# ANN example\nlogger.info(\n    f'{datetime.datetime.now().isoformat(){style='color: #990000'}[}]{style='color: #990000'} - lr {lr} - epoch {epoch} - phase {phase} - loss {epoch_loss}'\n)\n\nDon’t need a separate log function. You can just include logger.info in the model training function.\n\nBuilt-in dashboard\n\n\nTask Stream - each bar is a worker; colors show activity category (e.g. busy, finished, error, etc.)\n\n\n\n\nError Handling\n\nThe Dask scheduler will continue the computation and start another worker if one fails.\n\nIf your code is what causing the error then it won’t matter\n\nLibraries\nimport traceback\nfrom distributed.client import wait, FIRST_COMPLETED\nCreate a queue of futures\nqueue = client.compute(results)\nfutures_idx = {fut: i for i, fut in enumerate(queue){style='color: #990000'}[}]{style='color: #990000'}\nresults = [None for x in range(len(queue))]\n\nSince we’re not passsing [sync = True]{arg-text}, we immediately get back futures which represent the computation that hasn’t been completed yet.\nEnumerate each item in the future\nPopulate the “results” list with Nones for now\n\nWait for results\nwhile queue:\n    result = wait(queue, return_when = FIRST_COMPLETED)\nFutures either succeed (“finished”) or they error (chunk included in while loop)\n    for future in result.done:\n        index = futures_idx[future]       \n        if future.status == 'finished':\n            print(f'finished computation #[{index}]{style='color: #990000'}')\n            results[index] = future.result()\n        else:\n            print(f'errored #[{index}]{style='color: #990000'}')\n            try:\n                future.result()\n            except Exception as e:\n                results[index] = e\n                traceback.print_exc()\n\n    queue = result.not_done\n\nfuture.status contains results of computation so you know what to retry\nSucceeds: Print that it finished and store the result\nError: Store exception and print the stack trace\nSet queue to those futures that haven’t been completed\n\n\n\n\nOptimization\n\nResources\n\nDask Dataframes Best Practices\n\nIn general, recommends partition size of 100MB, but this doesn’t take into account server specs, dataset size, or application (e.g model fitting). Could be a useful starting point though.\n\n\nNotes from:\n\nAlmost Everything You Want to Know About Partition Size of Dask Dataframes\n\nUse Case: 2-10M rows using 8 to 16GB of RAM\n\n\nPartition Size\n\nIssues\n\nToo Large: Takes too much time and resources to process them in RAM\nToo Small: To process all of them Dask needs to load these tables into RAM too often. Therefore, more time is spent on synchronization and uploading/downloading than on the calculations themselves\n\nExample: Runtime, 500K rows, 4 columns, XGBoost\n\n\nExact resource specs for this chart werent’ provided, but the experiment included 2, 3, and 4 vCPUs and 1, 2, and 4GB RAM per vCPU\n\n\n\n\n\nCloud\n\nSaturn\n\nStarting Dask from Jupyter Server that’s running JupyterLab, the Dask Cluster will have all the libraries loaded into Jupyter Server\nOptions\n\nSaturn Cloud UI\n\nOnce you start a Jupyter Server, there’s a button to click that allows you to specify and run a Dask Cluster\n\nDo work on a JupyterLab notebook\n\nBenefits\n\nIn a shared environment\nLibraries automatically get loaded onto the Dask cluster\n\n\nProgrammatically (locally)\n\nSSH into Jupyter Server (which is connected to the Dask Cluster) at Saturn\nConnect directly to Dask Cluster at Saturn\nCons\n\nHave to load packages locally and on Jupyter Server and/or Dask Cluster\nMake sure versions/environments match\n\nConnection (basic)\nfrom dask_saturn import SaturnCluster\ncluster = SaturnCluster()\nclient = Client(cluster)\n\n\nExample\n\nFrom inside a jupyterlab notebook on a jupyter server with a dask cluster running\nImports\nimport dask.dataframe as dd\nimport numpy as np\nfrom dask.diagnostics import ProgressBar\nfrom dask.distributed import Client, wait\nfrom dask_saturn import SaturnCluster\nStart Cluster\nn_workers = 3\ncluster = SaturnCluster()\nclient = Client(cluster)\nclient.wait_for_workers(n_workers = n_workers) # if workers aren't ready, wait for them to spin up before proceding\nclient.restart()\n\nFor bigger tasks like training ANNs on GPUs, you to specify a gpu instance type (i.e. “worker_size”) and scheduler with plenty of memory\ncluster = SaturnCluster(\n    n_workers = n_workers,\n    scheduler_size = 'large',\n    worker_size = 'g3dnxlarge'\n)\n\nIf you’re bringing back sizable results from your workers, your scheduler needs plenty of RAM.\n\n\nUpload Code files\n\n1 file - client.upload_file(\"functions.py\")\n\nUploads a single file to all workers\n\nDirectory\nfrom dask_saturn import RegesterFiles, sync_files\nclient.register_worker_plugin(RegisterFiles())\nsync_files(client, \"functions\")\nclient.restart()\n\nPlugin allows you to sync directory among workersjjj\n\n\nData\nddf = dd.read_parquet(\n    \"/path/to/file.pq\"\n)\n\nddf = ddf.persist()\n_ = wait(ddf) # halts progress until persistance is done\n\nPersist saves the data to the Dask workers\n\nNot necessary, but if you didn’t, then each time you call .compute() you’d have to reload the file\n\n\nDo work\nddf[\"signal\"] = (\n    ddf[\"ask_close\"].rolling(5 * 60).mean() - ddf[\"ask_close\"].rolling(20 * 60).mean()\n)\n\n# ... blah, blah, blah\n\nddf[\"total\"] = ddf[\"return\"].cumsum().apply(np.exp, meta = \"return\", \"float64\"))\n\nSyntax just like pandas except:\n\nmeta = (column, type) - Dask’s lazy computation sometimes gets column types wrong, so this specifies types explicitly\n\n\nCompute and bring back to client\ntotal_returns = ddf[\"total\"].tail(1)\nprint(total_returns)\n\nEvidently .tail does what compute is supposed to do.",
    "crumbs": [
      "MLOps"
    ]
  },
  {
    "objectID": "qmd/mlops.html#sec-mlops-targets-vetr",
    "href": "qmd/mlops.html#sec-mlops-targets-vetr",
    "title": "MLOps",
    "section": "Vetiver",
    "text": "Vetiver\n\nMisc\n\nDocs\nAvailable in Python\n\n\nHelps with 3 aspects of MLOps\n\nVersioning\n\nKeeps track of metadata\nHelpful during retraining\n\nDeploying\n\nUtilizes REST APIs to serve models\n\nMonitoring\n\nTracks model performance\n\n\nVersioning\n\nWrite model to a {pins} board (storage)\nlibrary(pins)\nboard &lt;- board_connect()\nboard %&gt;% vetiver_pin_write(v)\n\nboard_* has many options including the major cloud providers\n\nHere “connect” stands for Posit Connect\n\n“v” is a {tidymodels} workflow obj\n\nCreate a REST API\nlibrary(plumber)\npr() %&gt;%\n  vetiver_api(v)\n#&gt; # Plumber router with 2 endpoints, 4 filters, and 1 sub-router.\n#&gt; # Use `pr_run()` on this object to start the API.\n#&gt; ├──[queryString]\n#&gt; ├──[body]\n#&gt; ├──[cookieParser]\n#&gt; ├──[sharedSecret]\n#&gt; ├──/logo\n#&gt; │  │ # Plumber static router serving from directory: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/vetiver\n#&gt; ├──/ping (GET)\n#&gt; └──/predict (POST)\n## next pipe to `pr_run()` for local API\n\nThen next step is pipe this into pr_run() to start the API locally\nHelpful for development or debugging\n\n\nDeploy\n\nPosit Connect has a 1-liner: vetiver_deploy_rsconnect()\nCreate a docker file\nvetiver_prepare_docker()\n\nCreates a docker file, renv.lock file, and plumber app that can be uploaded and deployed anywhere (e.g. AWS, GCP, digitalocean)\n\n\nMonitor\n\nWrite metrics to storage\nnew_metrics &lt;-\n  augment(v, housing_val) %&gt;%\n  vetiver_compute_metrics(date, \"week\", price, .pred)\n\nvetiver_pin_metrics(\n  board,\n  new_metrics, \n  \"julia.silge/housing-metrics\",\n  overwrite = TRUE\n)\n#&gt; # A tibble: 90 × 5\n#&gt;    .index                 .n .metric .estimator  .estimate\n#&gt;    &lt;dttm&gt;              &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;\n#&gt;  1 2014-11-02 00:00:00   224 rmse    standard   206850.   \n#&gt;  2 2014-11-02 00:00:00   224 rsq     standard        0.413\n#&gt;  3 2014-11-02 00:00:00   224 mae     standard   140870.   \n#&gt;  4 2014-11-06 00:00:00   373 rmse    standard   221627.   \n#&gt;  5 2014-11-06 00:00:00   373 rsq     standard        0.557\n#&gt;  6 2014-11-06 00:00:00   373 mae     standard   150366.   \n#&gt;  7 2014-11-13 00:00:00   427 rmse    standard   255504.   \n#&gt;  8 2014-11-13 00:00:00   427 rsq     standard        0.555\n#&gt;  9 2014-11-13 00:00:00   427 mae     standard   147035.   \n#&gt; 10 2014-11-20 00:00:00   376 rmse    standard   248405.   \n#&gt; # ℹ 80 more rows\nAnalyze metrics\nnew_metrics %&gt;%\n  ## you can operate on your metrics as needed:\n  filter(.metric %in% c(\"rmse\", \"mae\"), .n &gt; 20) %&gt;%\n  vetiver_plot_metrics() + \n  ## you can also operate on the ggplot:\n  scale_size(range = c(2, 5))",
    "crumbs": [
      "MLOps"
    ]
  },
  {
    "objectID": "qmd/model-building-brms.html",
    "href": "qmd/model-building-brms.html",
    "title": "brms",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "brms"
    ]
  },
  {
    "objectID": "qmd/model-building-brms.html#sec-modbld-brms-misc",
    "href": "qmd/model-building-brms.html#sec-modbld-brms-misc",
    "title": "brms",
    "section": "",
    "text": "How brms fits models\n\nThe marginal Sigma posterior distribution is the distribution of residuals \nSampling the posterior\n\nLogistic\n# sampling the posterior\nf &lt;- \n  fitted(b3.1, \n        summary = F,            # says we want simulated draws and not summary stats \n        scale = \"linear\") %&gt;%    # linear outputs probabilities \n  as_tibble() %&gt;% \n  set_names(\"p\")\n\nOptimization\n\n{cmdstanr} as the backend\n\nIn the brms model function\nbackend=\"cmdstanr\", stan_model_args=list(stanc_options = list(\"O1\"))\n\nstanc_options = list(\"O1\") might be made default in a future update\nSee thread for details\n\nSet using options: options(brms.backend = \"cmdstanr\"",
    "crumbs": [
      "Model Building",
      "brms"
    ]
  },
  {
    "objectID": "qmd/model-building-brms.html#sec-modbld-brms-lr",
    "href": "qmd/model-building-brms.html#sec-modbld-brms-lr",
    "title": "brms",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nExample: lm, Cont ~ Cont (SR, Ch.4)\nb4.3 &lt;-  brm(data = dat, family = gaussian,\n      height ~ 1 + weight,\n      prior = c(prior(normal(178, 100), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(uniform(0, 50), class = sigma)),\n      iter = 41000, warmup = 40000, chains = 4, cores = 4,\n      seed = 4, backend = \"cmdstanr\",\n)\nExample: Multivariable, Cont ~ Cont + Cont (SR, Ch.5)\nb5.3 &lt;-  brm(data = dat, family = gaussian,\n      Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s,\n      prior = c(prior(normal(10, 10), class = Intercept),\n                prior(normal(0, 1), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 5, backend = \"cmdstanr\", file = \"fits/b05.03\")\n\nOutcome = Divorce rate, Predictor 1 = state’s marriage rate, Predictor 2 = state’s median marriage age\n\nExample: Binary/Cat, Cont ~ Binary (SR, Ch. 5)\ndata(Howell1, package = \"rethinking\") \nd &lt;- Howell1\nd &lt;-  d %&gt;% \n  mutate(sex = ifelse(male == 1, 2, 1), # create index variable\n        sex = factor(sex)) # transforming it into a factor tells brms it's indexed\nb5.8 &lt;-  brm(data = d, \n            family = gaussian, \n            height ~ 0 + sex, # \"0 +\" notation means calculate separate intercepts for each category\n            prior = c(prior(normal(178, 20), class = b), \n                      prior(exponential(1), class = sigma)), \n            iter = 2000, warmup = 1000, chains = 4, cores = 4, \n            seed = 5, backend = \"cmdstanr\",\n            file = \"fits/b05.08\")\nprint(b5.8)\n\nFor nominal variable, increase the s.d. of the α prior (as compared to the binary prior) “to allow the different  to disperse, if the data wants them to.”\n“I encourage you to play with that prior and repeatedly re-approximate the posterior so you can see how the posterior differences among the categories depend upon it.”\n\nExample: Multi-Categorical, Cont ~ Cat + Cat (SR, Ch. 5)\nb5.11 &lt;-  brm(data = d, \n      family = gaussian, \n      # bf() is an alias for brmsformula() that lets you specify model formulas\n      bf(kcal.per.g_s ~ 0 + a + h, \n        a ~ 0 + clade, \n        h ~ 0 + house, \n        # tells brm we're using non-linear syntax\n        nl = TRUE), \n      prior = c(prior(normal(0, 0.5), nlpar = a), \n                prior(normal(0, 0.5), nlpar = h), \n                prior(exponential(1), class = sigma)), \n      iter = 2000, warmup = 1000, chains = 4, cores = 4, \n      seed = 5, backend = \"cmdstanr\",\n      file = \"fits/b05.11\")\n\n(As of May 2022) When using the typical formula syntax with more than one categorical variable, {brms} drops a category from every categorical variable except for the first one in the formula.\n{brms} was orginally designed to wrap Stan multi-level models w/lme4 syntax, so maybe that has something do with it.\nKurz has links and discussion in Section 5.3.2 of his ebook\n\nExample: Interaction, continuous * categorical (SR, Ch.8)\n# same as coding for cat vars except adding a slope that is also conditioned on the index\nb8.3 &lt;- \n  brm(data = dd, \n      family = gaussian, \n      # bf = \"brms formula\"\n      bf(log_gdp_std ~ 0 + a + b * rugged_std_c, \n        a ~ 0 + cid, \n        b ~ 0 + cid, \n        # nl = \"nonlinear\" syntax\n        nl = TRUE), \n      prior = c(prior(normal(1, 0.1), class = b, coef = cid1, nlpar = a), \n                prior(normal(1, 0.1), class = b, coef = cid2, nlpar = a), \n                prior(normal(0, 0.3), class = b, coef = cid1, nlpar = b), \n                prior(normal(0, 0.3), class = b, coef = cid2, nlpar = b), \n                prior(exponential(1), class = sigma)), \n      iter = 2000, warmup = 1000, chains = 4, cores = 4, \n      seed = 8, backend = \"cmdstanr\",\n      file = \"fits/b08.03\")\n\nCategorical variable needs an interaction spec and an intercept (main effect) spec\n\nExample: Interaction, continuous * continuous (SR, Ch.8)\nb8.5 &lt;- \n  brm(data = d, \n      family = gaussian, \n      blooms_std ~ 1 + water_cent + shade_cent + water_cent:shade_cent, \n      prior = c(prior(normal(0.5, 0.25), class = Intercept), \n                prior(normal(0, 0.25), class = b, coef = water_cent), \n                prior(normal(0, 0.25), class = b, coef = shade_cent), \n                prior(normal(0, 0.25), class = b, coef = \"water_cent:shade_cent\"), \n                prior(exponential(1), class = sigma)), \n      iter = 2000, warmup = 1000, chains = 4, cores = 4, \n      seed = 8, backend = \"cmdstanr\",\n      file = \"fits/b08.05\")\nExample: Interaction, categorical * categorial (mc-stan question)(SR, Lecture 9 video, Ch. 11 Note)\nfit &lt;-\n  brm(bf(y ~ 0 + a + b + c,\n        a ~ 0 + F1,\n        b ~ 0 + F2,\n        # this is the interaction\n        c ~ (0 + F1) : (0 + F2),\n        nl = TRUE),\n    data = dat)\n\n# interaction-only model (should include main effects)\nbrm_mod &lt;- brm(data = ucb_01,\n              family = bernoulli,\n              bf(admitted ~ 0 + gd, \n                  # this is the interaction \n                  gd ~ (0 + gender) : (0 + dept), \n                  nl = TRUE), \n              prior = prior(normal(0,1), nlpar = gd), \n              iter = 2000, warmup = 1000, cores = 3, chains = 3, \n              seed = 10, backend = \"cmdstanr\")",
    "crumbs": [
      "Model Building",
      "brms"
    ]
  },
  {
    "objectID": "qmd/model-building-brms.html#sec-modbld-brms-logreg",
    "href": "qmd/model-building-brms.html#sec-modbld-brms-logreg",
    "title": "brms",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nExample: Logistic, cont ~ cat + cat (SR, Ch.11)\nb11_bern &lt;- brm(data = dat_sim, \n                    family = bernoulli(link = \"logit\"), \n                    bf(admit ~ a + d, \n                      a ~ 0 + gid, \n                      d ~ 0 + dept, \n                      nl = TRUE), \n                    prior = c(prior(normal(0, 1), nlpar = a), \n                              prior(normal(0, 1), nlpar = d)), \n                    iter = 4000, warmup = 1000, cores = 4, chains = 4, \n                    seed = 11, backend = \"cmdstanr\")\nprint(b11_bern)\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_gidmale      -1.06      0.51    -2.05    -0.07 1.00    1765    2356\na_gidfemale    -0.97      0.50    -1.96    0.00 1.00    1758    2366\nd_dept1        -1.54      0.51    -2.55    -0.54 1.00    1773    2419\nd_dept2        -0.50      0.50    -1.46    0.48 1.00    1763    2249\n\nb11.4_bin &lt;-  brm(data = dat_sim,   \n                  family = binomial, \n                  bf(admit | trials(1) ~ a + d, \n                      a ~ 0 + gid,   \n                      d ~ 0 + dept, \n                      nl = TRUE), \n                  prior = c(prior(normal(0, 1), nlpar = a), \n                            prior(normal(0, 1), nlpar = d)), \n                  iter = 2000, warmup = 1000, chains = 4, cores = 4, \n                  seed = 11, backend = \"cmdstanr\")\nprint(b11_bin)\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_gidmale      -1.02      0.52    -2.05    0.01 1.01      702      832\na_gidfemale    -0.94      0.52    -1.98    0.06 1.01      688      808\nd_dept1        -1.58      0.53    -2.60    -0.53 1.01      693      787\nd_dept2        -0.53      0.52    -1.53    0.49 1.01      685      718\n\nSlightly different results (100ths). I feel more comfortable using the bernoulli spec if it’s just a typical logistic regression.\nThe “1” in “|trials(1)” says that this is case-level data\n\nIncluding a | bar on the left side of a formula indicates we have extra supplementary information about our criterion. In this case, that information is that each pulled_left value corresponds to a single trial (i.e., trials(1)), which itself corresponds to the  n = 1 in the model specification (above) for the outcome variable (e.g. pulled_left).\n\nThe rest of the brms specification is standard for having two categorial explanatory variables (e.g. actor, treatment) (see Ch.5 &gt;&gt; categoricals &gt;&gt; multiple nominal)\n\ni.e. ingredients for Logistic Regression: family = Binomial and binary_outcome|trials(1)\n\n\nExample: Multinomial Logistic Regression (SR, Ch. 11)\nb11.13io &lt;- \n  brm(data = d,       \n      # refcat sets the reference category to the 3rd level \n      family = categorical(link = logit, refcat = 3), \n      career ~ 1, \n      prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1), \n                prior(normal(0, 1), class = Intercept, dpar = mu2)), \n      iter = 2000, warmup = 1000, cores = 4, chains = 4, \n      seed = 11, backend - \"cmdstanr\",\n      file = \"fits/b11.13io\")\nb11.13io\n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS \n## mu1_Intercept    -2.01      0.15    -2.30    -1.73 1.00    3324    2631 \n## mu2_Intercept    -1.53      0.12    -1.77    -1.29 1.00    2993    2768\n\nas of brms 2.12.0, “specifying global priors for regression coefficients in categorical models is deprecated.” Meaning — if we want to use the same prior for both, we need to use the dpar argument for each\nThe reference level, refcat ,  was set to the 3rd level so the scores for the levels 1 and 2 are shown\n\nDefault is level 1.\nHad we used the brms default and used the first level of “career” as the pivot (aka reference category), those lines would have instead been dpar = mu2 , dpar = mu3\n\nThese are the score values that we’d get if we centered all the scores at level 3’s score value\n\nNothing to infer from these. We want the probabilities which are in the next section.\n\nAlternate ways to fit this model\n# verbose syntax\nb11.13io_verbose &lt;-\n  brm(data = d, \n      family = categorical(link = logit, refcat = 3),\n      bf(career ~ 1,\n        mu1 ~ 1,\n        mu2 ~ 1),\n      prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1),\n                prior(normal(0, 1), class = Intercept, dpar = mu2)),\n      iter = 2000, warmup = 1000, cores = 4, chains = 4,\n      seed = 11,\n      file = \"fits/b11.13io_verbose\")\n\n# nonlinear syntax\nb11.13io_nonlinear &lt;-\n  brm(data = d, \n      family = categorical(link = logit, refcat = 3),\n      bf(career ~ 1,\n        nlf(mu1 ~ a1),\n        nlf(mu2 ~ a2),\n        a1 + a2 ~ 1),\n      prior = c(prior(normal(0, 1), class = b, nlpar = a1),\n                prior(normal(0, 1), class = b, nlpar = a2)),\n      iter = 2000, warmup = 1000, cores = 4, chains = 4,\n      seed = 11,\n      file = \"fits/b11.13io_nonlinear\")\n\nThe verbose (and nonlinear) syntax makes it clear that we are fitting k-1 models, since “career” has 3 categories\nHad we used the brms default and used the first level of career as the pivot (aka reference category), those lines would have instead been mu2 ~ 1, mu3 ~ 1",
    "crumbs": [
      "Model Building",
      "brms"
    ]
  },
  {
    "objectID": "qmd/model-building-brms.html#sec-modbld-brms-discrete",
    "href": "qmd/model-building-brms.html#sec-modbld-brms-discrete",
    "title": "brms",
    "section": "Discrete Distribution Models",
    "text": "Discrete Distribution Models\n\nBinomial (SR Ch. 11)\nb11.6 &lt;- \n  brm(data = d_aggregated, \n      family = binomial, \n      bf(left_pulls | trials(18) ~ a + b, \n        a ~ 0 + actor, \n        b ~ 0 + treatment, \n        nl = TRUE), \n      prior = c(prior(normal(0, 1.5), nlpar = a), \n                prior(normal(0, 0.5), nlpar = b)), \n      iter = 2000, warmup = 1000, chains = 4, cores = 4, \n      seed = 11, backend = \"cmdstanr\",\n      file = \"fits/b11.06\")\n\nIncluding a | bar on the left side of a formula indicates we have extra supplementary information about our criterion.\n\nTo fit an aggregated binomial model with brms, we augment the  | trials() syntax where the value that goes in trials() is either a fixed number, as in this case, or variable (e.g. trials(applications) ) in the data indexing the number of trials, n.\n\nCan also use a logistic model, but need case-level data (e.g. 0/1)\n\nDeaggregate count data into 0/1 case-level data\n\ndata(UCBadmit, package = \"rethinking\")\nucb &lt;- UCBadmit %&gt;% \n  mutate(applicant.gender = relevel(applicant.gender, ref = \"male\"))\n\n# deaggregate to 1/0\ndeagg_ucb &lt;- function(x, y) {\n  UCBadmit %&gt;%\n    select(-applications) %&gt;%\n    group_by(dept, applicant.gender) %&gt;%\n    tidyr::uncount(weights = !!sym(x)) %&gt;%\n    mutate(admitted = y) %&gt;%\n    select(dept, gender = applicant.gender, admitted)\n}\nucb_01 &lt;- purrr::map2_dfr(c(\"admit\", \"reject\"),\n                          c(1, 0),\n                          ~ disagg_ucb(.x, .y)\n)\n\nLogistic (SR Ch. 11)\nfull_mod &lt;- brm(bf(admitted ~ 0 + g + d + gd, \n                  g ~ 0 + gender, \n                  d ~ 0 + dept, \n                  # this is the interaction \n                  gd ~ (0 + gender) : (0 + dept), \n                  nl = TRUE), \n                prior = c(prior(normal(0,1), nlpar = g),\n                          prior(normal(0,1), nlpar = d),\n                          prior(normal(0,1), nlpar = gd)),\n                data = ucb_01,\n                family = bernoulli,\n                iter = 2000, warmup = 1000, cores = 3, chains = 3,\n                seed = 10, backend = \"cmdstanr\")\nPoisson (SR Ch. 11)\n# cat * cont interaction model \nb11.10 &lt;- \n  brm(data = d, \n      family = poisson, \n      bf(total_tools ~ a + b * log_pop_std, \n        a + b ~ 0 + cid, \n        nl = TRUE), \n      prior = c(prior(normal(3, 0.5), nlpar = a), \n                prior(normal(0, 0.2), nlpar = b)), \n      iter = 2000, warmup = 1000, chains = 4, cores = 4, \n      seed = 11, backend = \"cmdstanr\",\n      file = \"fits/b11.10\")",
    "crumbs": [
      "Model Building",
      "brms"
    ]
  },
  {
    "objectID": "qmd/model-building-brms.html#sec-modbld-brms-nonlin",
    "href": "qmd/model-building-brms.html#sec-modbld-brms-nonlin",
    "title": "brms",
    "section": "Non-linear",
    "text": "Non-linear\n\nExample: Basis Splines (SR, Ch.4)\n\n# get recommended prior specifications\n# s is the basis function brms imports from mgcv pkg\nbrms::get_prior(data = d2, \n                family = gaussian, \n                doy ~ 1 + s(year))\n##                  prior    class    coef group resp dpar nlpar bound      source \n##                  (flat)        b                                          default \n##                  (flat)        b syear_1                            (vectorized) \n##  student_t(3, 105, 5.9) Intercept                                          default \n##    student_t(3, 0, 5.9)      sds                                          default \n##    student_t(3, 0, 5.9)      sds s(year)                            (vectorized) \n##    student_t(3, 0, 5.9)    sigma                                          default\n\n# multi-level method\nb4.11 &lt;- brm(data = d2, \n            family = gaussian, \n            # k = 19, corresponds to 17 basis functions I guess ::shrugs:: \n            # The default for s() is to use what’s called a thin plate regression spline \n            # bs uses a basis spline \n            temp ~ 1 + s(year, bs = \"bs\", k = 19), \n            prior = c(prior(normal(100, 10), class = Intercept), \n                      prior(normal(0, 10), class = b), \n                      prior(student_t(3, 0, 5.9), class = sds), \n                      prior(exponential(1), class = sigma)), \n            iter = 2000, warmup = 1000, chains = 4, cores = 4, \n            seed = 4, backend = \"cmdstanr\",\n            control = list(adapt_delta = .99))",
    "crumbs": [
      "Model Building",
      "brms"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html",
    "href": "qmd/model-building-concepts.html",
    "title": "Concepts",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html#sec-modbld-misc",
    "href": "qmd/model-building-concepts.html#sec-modbld-misc",
    "title": "Concepts",
    "section": "",
    "text": "Packages\n\n{multiverse} - makes it easy to specify and execute all combinations of reasonable analyses of a dataset\n\n\n\nPaper, Summary of it’s usage\nLots of vignettes\n\n\nRegression Workflow (Paper)\n\nBasic approach to algorithm choice\n\nModel is performing well on the training set but much worse on the validation/test set\n\n\nAndrew Ng calls the validation set the “Dev Set” 🙄\nTest: Random sample the training set and use that as your validation set. Score your model on this new validation set\n\n“Train-Dev” is the sampled validation set\nPossibilities\n\nVariance: The data distribution of the training set is the same as the validation/test sets\n\n\nThe model has been overfit to the training data\n\nData Mismatch: The data distribution of the training set is NOT the same as the validation/test sets\n\n\nUnlucky and the split was bad\n\nSomething maybe is wrong with the splitting function\n\nSplit ratio needs adjusting. Validation set isn’t getting enough data to be representative.\n\n\n\n\nModel is performing well on the validation/test set but not in the real world\n\nInvestigate the validation/test set and figure out why it’s not reflecting real world data. Then, apply corrections to the dataset.\n\ne.g. distributions of your validation/tests sets should look like the real world data.\n\nChange the metric\n\nConsider weighting cases that your model is performing extremely poorly on.\n\n\nSplits\n\nHarrell: “not appropriate to split data into training and test sets unless n&gt;20,000 because of the luck (or bad luck) of the split.”\nIf your dataset is over 1M rows, then having a test set of 200K might be overkill (e.g. ratio of 60/20/20).\n\nMight be better to use a ratio of 98/1/1 for big data projects and 60/20/20 for smaller data projects\n\nlink\n\nShows that simple data splitting does not give valid confidence intervals (even asymptotically) when one refits the model on the whole dataset. Thus, if one wants valid confidence intervals for prediction error, we can only recommend either data splitting without refitting the model (which is viable when one has ample data), or nested CV.",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/model-building-h2o-automl.html",
    "href": "qmd/model-building-h2o-automl.html",
    "title": "H2O AutoML",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "H2O AutoML"
    ]
  },
  {
    "objectID": "qmd/model-building-h2o-automl.html#sec-h2o-misc",
    "href": "qmd/model-building-h2o-automl.html#sec-h2o-misc",
    "title": "H2O AutoML",
    "section": "",
    "text": "The amount of ram needed on local machine equals four to five times the size of your data (usually) but really depends on max number of models to be trained.\nDiagnostic metrics used in classification models are AUC, log loss, and mean per class error\nSeems to use generic variable importance (mean decrease gini)\nI think a glm was used to ensemble its models\nIn the binary classification example, the top models were “extremely randomized trees”, xgboost, and gbm.\nEnsemble model reproducibility requires arguments: set seed and exclude_model = “DNN”  (deep neural network)\nExample: CV with LIME Explainer",
    "crumbs": [
      "Model Building",
      "H2O AutoML"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html",
    "href": "qmd/model-building-sklearn.html",
    "title": "sklearn",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-misc",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-misc",
    "title": "sklearn",
    "section": "",
    "text": "Extensions\n\n{{DeepChecks}} - for model diagnostics\n{{feature-engine}} - multiple transformers to engineer and select features to use in machine learning models.\n\nPreventing data leakage in sklearn\n\nUse fit_transform on the train data - this ensures that the transformer learns from the train set only and transforms it simultaneously. Then, call the transformmethod on the test set to transform it based on the information learned from the training data (i.e mean and variance of the training data).\n\nPrevents data leakage\nSomehow the transform parameters calculated on the training data have to saved, so they can be applied to the production data. This can be done with Pipelines (see Pipelines &gt;&gt; Misc) by serializing the Pipeline objects.\n\nu_transf = LabelEncoder()\nitem_transf = LabelEncoder()\n# encoding\ndf['user'] = u_transf.fit_transform(df['user'])\ndf['item'] = item_transf.fit_transform(df['item'])\n# decoding\ndf['item'] = item_transf.inverse_transform(df['item'])\ndf['user'] = u_transf.inverse_transform(df['user'])\n\nA more robust way is to use sklearn’s Pipelines (see Pipelines below)\n\n\nTuning\n\nPick a metric for GridSearchCV and RandomizedSearchCV\n\nDefault is accuracy for classification which is rarely okay\nUsing metric from sklearn\ngs = GridSearchCV(estimator=svm.SVC(), \n                  param_grid={'kernel':('linear', 'rbf'), 'C':[1, 10]},\n                  scoring=‘f1_micro’)\n\nList of metrics available\n\nUsing a custom metric\nfrom sklearn.metrics import fbeta_score, make_scorer\ncustom_metric = make_scorer(fbeta_score, beta=2)\ngs = GridSearchCV(estimator=svm.SVC(),\n                  param_grid={'kernel':('linear','rbf'), 'C':[1,10]},\n                  scoring=custom_metric)\nAlso see Diagnostics, Classification &gt;&gt; Scores &gt;&gt; Lift Score\n\n\nScore model\n\nAlso see Pipelines &gt;&gt; Tuning a Pipeline\nBasic\nmodel = RandomForestClassifier(max_depth=2, random_state=0, warm_start=True, n_estimators=1)\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n\nClassification\nfrom sklearn.metrics import classification_report\nrep = classification_report(y_test, y_pred, output_dict = True)\n\nSee Diagnostics, Classification &gt;&gt; Multinomial for definitions of multinomial scores\n\nMultiple metrics function\nfrom sklearn.metrics import precision_recall_fscore_support\ndef score(y_true, y_pred, index):\n    \"\"\"Calculate precision, recall, and f1 score\"\"\"\n\n    metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n    performance = {'precision': metrics[0], 'recall': metrics[1], 'f1': metrics[2]}\n    return pd.DataFrame(performance, index=[index])\npredict_proba outputs probabilities for classification models",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-opt",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-opt",
    "title": "sklearn",
    "section": "Optimization",
    "text": "Optimization\n\nMisc\n\nFor fast iteration and if you have access to GPUs, it’s better to use {{XGBoost, LightGBM or CatBoost}} since sklearn is only CPU capable.\n\nFastest to slowest: CaBoost, LightGBM, XGBoost\n\n\n{{sklearnex}}\n\nIntel® Extension for Scikit-learn that dynamically patches scikit-learn estimators to use Intel(R) oneAPI Data Analytics Library as the underlying solver\n\nRequirements\n\nThe processor must have x86 architecture.\nThe processor must support at least one of SSE2, AVX, AVX2, AVX512 instruction sets.\nARM* architecture is not supported.\nIntel® processors provide better performance than other CPUs.\n\n\nMisc\n\nDocs\nBenchmarks\nAlgorithms\nCurrently extension does not support multi-output and sparse data for the Random Forest and K-Nearest Neighbors\n\nNon-interactively\npython -m sklearnex my_application.py\nInteractively for all algorithms\nfrom sklearnex import patch_sklearn\npatch_sklearn()\n# then import sklearn estimators\nInteractively for specific algorithms\nfrom sklearnex import patch_sklearn\npatch_sklearn([\n    'RandomForestRegressor,\n    'SVC',\n    'DBSCAN'\n])\nUnpatch unpatch_sklearn()\nNeed to reimport estimators after executing\n\nSpeeding up retraining a model (article)\n\nPotentially useful for slow training models that need to be retrained often.\nNot all models have these methods available\nwarm_start = True permits the use of the existing fitted model attributes to initialize a new model in a subsequent call to fit\n\nDoesn’t learn new parameters so shouldn’t be used to fix concept drift\n\ni.e. The new data should have the same distribution as the original data which maintains the same relationship with the output variable\n\nExample\n# original fit\nmodel = RandomForestClassifier(max_depth=2, random_state=0, warm_start=True, n_estimators=1)\nmodel.fit(X_train, y_train)\n\n# fit with new data\nmodel.n_estimators+=1\nmodel.fit(X2, y2)\n\n\nPartial Fit\n\nDoes learn new model parameters\nExample\n# original fit\nmodel = SGDClassifier() \nmodel.partial_fit(X_train, y_train, classes=np.unique(y))\n\n# fit with new data\nmodel.partial_fit(X2, y2)",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-preproc",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-preproc",
    "title": "sklearn",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nMisc\n\nResources\n\nSciKit-Learn Transformation Docs\n\ntrain_test_split doesn’t choose random rows to be in each partition by default. For example, a 90-10 split has the first 90% of the rows in Train which leaves the last 10% of the rows to be in Test\n\n“shuffle=True” will randomly shuffle the rows before it partitions which would be equivalent to randomly selecting rows for each partition\n\n\nStratified train/test splits\nnp.random.seed(2019)\n# Generate stratified split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random.state=42)\n# Train set class weights\n&gt;&gt;&gt; pd.Series(y_train).value_counts(normalize=True)\n1    0.4\n0    0.4\n2    0.2\ndtype: float64\n# Test set class weights\n&gt;&gt;&gt; pd.Series(y_test).value_counts(normalize=True)\n1    0.4\n0    0.4\n2    0.2\n\nOr you can use StratifiedKFold which stratifies automatically.\n\nStratified Train/Validate/Test splits\nX_train, X_, y_train, y_ = train_test_split(\n    df['token'], tags, train_size=0.7, stratify=tags, random_state=RS\n)\n\nX_val, X_test, y_val, y_test = train_test_split(\n    X_, y_, train_size=0.5, stratify=y_, random_state=RS\n)\n\nprint(f'train: {len(X_train)} ({len(X_train)/len(df):.0%})\\n'\n      f'val: {len(X_val)} ({len(X_val)/len(df):.0%})\\n'\n      f'test: {len(X_test)} ({len(X_test)/len(df):.0%})')\nCheck proportions of stratification variable (e.g. “tags”) in splits\nsplit = pd.DataFrame({\n    'y_train': Counter(', '.join(i for i in row) for row in mlb.inverse_transform(y_train)),\n    'y_val': Counter(', '.join(i for i in row) for row in mlb.inverse_transform(y_val)),\n    'y_test': Counter(', '.join(i for i in row) for row in mlb.inverse_transform(y_test))\n}).reindex(tag_dis.index)\n\nsplit = split / split.sum(axis=0)\n\nsplit.plot(\n    kind='bar', \n    figsize=(10,4), \n    title='Tag Distribution per Split', \n    ylabel='Proportion of observations'\n);\nBin numerics: KBinsDiscretizer(n_bins=4)\nImpute Nulls/Nas\n\nSimpleImputer (Docs)\ncol_transformer = ColumnTransformer(\n    # Transformer name, Transformer Object and columns\n    [(\"ImputPrice\", SimpleImputer(strategy=\"median\"), [\"price\"])],\n    # Any other columns are ignored\n    remainder= SimpleImputer(strategy=\"constant\", fill_value=-1)\n  )\n\nTakes “price” and replaces Nulls with median; all other columns get constant, -1, to replace their Nulls\n“most frequent” also available\n\nIterativeImputer (Docs) - Multivariate imputer that estimates values to impute for each feature with missing values from all the others.\nKNNImputer (Docs) - Multivariate imputer that estimates missing features using nearest samples.\n\nLog: FunctionTransformer(lambda value: np.log1p(value))\nStandardize\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nstandardized_data = scaler.fit_transform(df)\nstandardized_df = pd.DataFrame(standardized_data, columns=df.columns)\n\nCheck: standardized_df.apply([\"mean\", \"std\"]\n\nScaling\n\nMin/Max\nfrom sklearn.preprocessing import MinMaxScaler\n# Create a scaler object\n\nmm_scaler = MinMaxScaler()\n\n# Transform the feature values\nchurn[[\"Customer_Age\", \"Total_Trans_Amt\"]] = mm_scaler.fit_transform(churn[[\"Customer_Age\", \"Total_Trans_Amt\"]])\n\n# check the feature value range after transformation\nchurn[\"Customer_Age\"].apply([\"min\", \"max\"])\n\n“feature_range=(0,1)” parameter allows you to change the range\n\ndefault range for the MinMaxScaler is [0,1]\n\n\n\nOrdinal\nencode_cat = ColumnTransformer(\n  [('cat', OrdinalEncoder(), make_column_selector(dtype_include='category'))],\n  remainder='passthrough'\n)\nCategorical DType (like R factor type)\n\nAssign predefined unordered values so that whenever some value for some column does not exist in the training set, receiving such a value in the test set would not lead to incorrectly assigned labels or any other logical error.\n\nWith multiple categorical columns, it can be difficult to stratify them in the training/test splits\nMust know all possible categories for each categorical variable\n\nimport pandas as pd\nfrom pandas import CategoricalDtype\n\ndef transform_data(df: pd.DataFrame, target: pd.Series, frac: float = 0.07,\n                  random_state: int = 42) -&gt; pd.DataFrame:\n\n    \"Transform non-numeric columns into categorical type and clean data.\"\n\n    categories_map = {\n          'gender': {1: 'male', 2: 'female'},\n          'education': {1: 'graduate', 2: 'university', 3: 'high_school',\n                        4: 'others', 5: 'others', 6: 'others', 0: 'others'},\n          'marital_status': {1: 'married', 2: 'single', 3: 'others', 0: 'others'}\n        }\n\n    for col_id, col_map in categories_map.items():\n            df[col_id] = df[col_id].map(col_map).astype(\n                CategoricalDtype(categories=list(set(col_map.values())))\n            )\n\nOne-hot encoding\n\n** Don’t use pandas get_dummies because it doesn’t handle categories that aren’t in your test/production set **\n\nWonder if get_dummies creates 1 less dummy than the number of categories like it should\n\nBasic\n# Create a one-hot encoder\nonehot = OneHotEncoder()\n\n# Create an encoded feature\nencoded_features = onehot.fit_transform(churn[[\"Marital_Status\"]]).toarray()\n\n# Create DataFrame with the encoded features\nencoded_df = pd.DataFrame(encoded_features, columns=onehot.categories_)\n\nPipeline\nfrom sklearn.preprocessing import OneHotEncoder\n# one hot encode categorical features\nohe = OneHotEncoder(handle_unknown='ignore')\n\nfrom sklearn.pipeline import Pipeline\n# store one hot encoder in a pipeline\ncategorical_processing = Pipeline(steps=[('ohe', ohe)])\n\nfrom sklearn.compose import ColumnTransformer\n# create the ColumnTransormer object\npreprocessing = ColumnTransformer(transformers=[('categorical', categorical_processing, ['gender', 'job'])],\n                                  remainder='passthrough')\nClass Imbalance\n\nSMOTE with {{imblearn}}\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE\nX_train, y_train = make_classification(n_samples=500, n_features=5, n_informative=3)\nX_res, y_res = SMOTE().fit_resample(X_train, y_train)",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-pip",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-pip",
    "title": "sklearn",
    "section": "Pipelines",
    "text": "Pipelines\n\nMisc\n\nHelps by creating more maintainable and clearly written code. Reduces mistakes by transforming train/test sets automatically.\nPipeline objects are estimators and can be serialized and saved like any other estimator\nimport joblib\n\n#saving the pipeline into a binary file\njoblib.dump(pipe, 'wine_pipeline.bin')\n\n#loading the saved pipeline from a binary file\npipe = joblib.load('wine_pipeline.bin')\n\n\n\nBasic Feature Transforming and Model Fitting Pipeline\n\nformat: Pipeline(steps = [(‘&lt;step1_name&gt;’, function), (‘&lt;step2_name&gt;’, function)])\nExample\nnp.random.seed(2019)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# creating the pipeline with its different steps\n# fitting the pipeline with data\n# making predictions\npipe = Pipeline([\n    ('feature_selection', VarianceThreshold(threshold=0.1)),\n    ('scaler', StandardScaler()),\n    ('model', KNeighborsClassifier())\n])\npipe.fit(X_train, y_train)\npredictions = pipe.predict(X_test)\n\n#checking the accuracy\naccuracy_score(y_test, predictions) #sklearn function; multi-class: accuracy, binary: jaccard index (similarity)\n\nPipeline transforms according to the sequence of the steps inserted into the list of tuples\n\n\n\n\nColumn Transformers\n\nExample: transform by column type\n# creating pipeline for numerical features\nnumerical_pipe = Pipeline([\n    ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# creating pipeline for categorical features\ncategorical_pipe = Pipeline([\n    ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n    ('one_hot', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer([\n    ('numerical', numerical_pipe, make_column_selector(dtype_include=['int', 'float'])),\n    ('categorical', categorical_pipe, make_column_selector(dtype_include=['object', 'category'])),\n])\npipe = Pipeline([\n    ('column_transformer', preprocessor),\n    ('model', KNeighborsClassifier())\n])\n\npipe.fit(X_train, y_train)\npredictions = pipe.predict(X_test)\n\nColumnTransformertakes list of tuples: name, transformer, columns\n\nn_jobs, verbose args also available\nremainder=“passthrough” says all other columns not listed are ignored (might be a default)\n\nCan also provide a transformer object\n\nmethods: fit_transform, get_feature_names_out, get_params, etc\n\nmake_column_selector allows your to select the type of column to apply the transformer (docs)\n\nExample: Apply sequence of transformations to a column\ncol_transformer = ColumnTransformer(\n    [\n      (\n          \"PriceTransformerPipeline\",\n          # Pipeline -&gt; multiple transformation steps\n          Pipeline([\n            (\"MeanImputer\"      , SimpleImputer(strategy=\"median\")),\n            (\"LogTransformation\", FunctionTransformer(lambda value: np.log1p(value)) ),\n            (\"StdScaler\",        StandardScaler() ),\n          ]),\n          [\"price\"]\n        ),\n    ],\n    remainder=SimpleImputer(strategy=\"constant\", fill_value=-1)\n  )\n\nFor the “price” columns, it replaces Nulls with median, then log transforms, then standardizes. All other columns get their Nulls replaces with -1.\n\n\n\n\nFunction Transformers\n\nMisc\n\nIf using a method in the sklearn.preprocessing module, then able to use fit, transform, and fit_transform methods (I think)\nData must be the first argument of the function\ninverse_func argument for FunctionTransformer allows you to include a back-transform function\n\nSteps\n\nCreate function that transforms data\nCreate FunctionTransformer object using function as the argument\nAdd function-tranformer to the pipeline by including it as an argument to make_pipeline\n\nExample: Make numerics 32-bit instead 64-bit to save memory\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\ndef reduce_memory(X: pd.DataFrame, y=None):\n    \"\"\"Simple function to reduce memory usage by casting numeric columns to float32.\"\"\"\n    num_cols = X.select_dtypes(incluce=np.number).columns\n    for col in num_cols:\n        X[col] = X.astype(\"float32\")\n    return X, y\n\nReduceMemoryTransformer = FunctionTransformer(func = reduce_memory)\n\n# Plug into a pipeline\n&gt;&gt;&gt; make_pipeline(SimpleImputer(), ReduceMemoryTransformer)\n\nData goes through the SimpleImputer first then the ReduceMemoryTransformer\n\n\n\n\nCustom Transformers Classes\n\nMisc\n\nFor more complex transforming tasks\n\nSteps\n\nCreate a class that inherits from BaseEstimator and TransformerMixin classes of sklearn.base\n\nInheriting from these classes allows Sklearn pipelines to recognize our classes as custom estimators and automatically adds fit_transform to your class\n\nAdd transforming methods to Class\n\nClass Skeleton\nclass CustomTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self):\n        pass\n    def transform(self):\n        pass\n    def inverse_transform(self):\n        pass\nExample: Log transforming outcome variable\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import PowerTransformer\n\nclass CustomLogTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self):\n        self._estimator = PowerTransformer()  # init a transformer\n\n    def fit(self, X, y=None):\n        X_copy = np.copy(X) + 1  # add one in case of zeroes\n        self._estimator.fit(X_copy)\n        return self\n\n    def transform(self, X):\n        X_copy = np.copy(X) + 1\n        return self._estimator.transform(X_copy)  # perform scaling\n\n    def inverse_transform(self, X):\n        X_reversed = self._estimator.inverse_transform(np.copy(X))\n        return X_reversed - 1  # return subtracting 1 after inverse transform\n\nreg_lgbm = lgbm.LGBMRegressor()\nfinal_estimator = TransformedTargetRegressor(\n    regressor=reg_lgbm, transformer=CustomLogTransformer()\n)\nfinal_estimator.fit(X_train, y_train)\n\nfit returns the tranformer itself since it returns self\n\nEstimates the optimal parameter lambda for each feature\n\ntransform returns transformed features\n\nApplies the power transform to each feature using the estimated lambdas in fit\n\nfit_transform does both at once\ncustom_log.fit(tps_df) \ntransformed_tps = custom_log.transform(tps_df)\n# or\ntransformed_tps = custom_log.fit_transform(tps_df)\ninverse_transform returns the back-transformed features\nTransformedTargetRegressor transforms the targets y before fitting a regression model. The predictions are mapped back to the original space via an inverse transform. It takes as an argument the regressor that will be used for prediction, and the transformer that will be applied to the target variable\n\nThe regressor parameter accepts both regressors or pipelines that end with regressors\nIf the transformer is a function, like np.log, you can pass it to func argument\n\n\nExample: Dummy transformer + FeatureUnion (article)\n\nThis classes (below) that inherit this class with get these methods along with fit_transform\nclass DummyTransformer(BaseEstimator, TransformerMixin):\n  \"\"\"\n  Dummy class that allows us to modify only the methods that interest us,\n  avoiding redudancy.\n  \"\"\"\n  def __init__(self):\n      return None\n\n  def fit(self, X=None, y=None):\n      return self\n\n  def transform(self, X=None):\n      return self\nTransformer classes\nclass Preprocessor(DummyTransformer):\n\"\"\"\n  Class used to preprocess text\n\"\"\"\ndef __init__(self, remove_stopwords: bool):\n    self.remove_stopwords = remove_stopwords\n    return None\n\ndef transform(self, X=None):\n    preprocessed = X.apply(lambda x: preprocess_text(x, self.remove_stopwords)).values\n    return preprocessed\n\nclass SentimentAnalysis(DummyTransformer):\n\"\"\"\n    Class used to generate sentiment\n    \"\"\"\n    def transform(self, X=None):\n         sentiment = X.apply(lambda x: get_sentiment(x)).values\n         return sentiment.reshape(-1, 1) # &lt;-- note the reshape to transfor\n\nEach class inherits the dummy transformer and its methods\npreprocess_text and get_sentiment are user functions that are defined earlier in the article\n\nCreate pipeline\nvectorization_pipeline = Pipeline(steps=[\n  ('preprocess', Preprocessor(remove_stopwords=True)), # the first step is to preprocess the text\n  ('tfidf_vectorization', TfidfVectorizer()), # the second step applies vectorization on the preprocessed text\n  ('arr', FromSparseToArray()), # the third step converts a sparse matrix into a numpy array in order to show it in a            dataframe\n])\n\nTfidVectorizer and FromSparseArray are other classes in the article that I didn’t include in the Transformer classes chunk to save space\n\nCombine transformed features\n# vectorization_pipeline is a pipeline within a pipeline\nfeatures = [\n  ('vectorization', vectorization_pipeline),\n  ('sentiment', SentimentAnalysis()),\n  ('n_chars', NChars()),\n  ('n_sentences', NSententences())\n]\ncombined = FeatureUnion(features) # this is where we merge the features together\n\n# Get col names: subsets the second step of the second object in the vectorization_pipeline to retrieve \n# the terms generated by the tf-idf then adds the other three column names to it\ncols = vectorization_pipeline.steps[1][1].get_feature_names() + [\"sentiment\", \"n_chars\", \"n_sentences\"]\nfeatures_df = pd.DataFrame(combined.transform(df['corpus']), columns=cols)\n\nPipelines are combined with FeatureUnion, features are transformed, and coerced into a pandas df which can be used to train a model\nNChars and NSentences are other classes in the article that I didn’t include in the Transformer classes chunk to save space\n\n\n\n\n\nTuning a Pipeline\n\nExample\nparameters = {\n    'column_transformer__numerical__imputer__strategy': ['mean', 'median'],\n    'column_transformer__numerical__scaler': [StandardScaler(), MinMaxScaler()],\n    'model__n_neighbors': [3, 6, 10, 15],\n    'model__weights': ['uniform', 'distance'],\n    'model__leaf_size': [30, 40]\n}\n\n# defining a scorer and a GridSearchCV instance\nmy_scorer = make_scorer(accuracy_score, greater_is_better=True)\nsearch = GridSearchCV(pipe, parameters, cv=3, scoring=my_scorer, n_jobs=-1, verbose=1)\n\n# search for the best hyperparameter combination within our defined hyperparameter space\nsearch.fit(X_train, y_train)\n\n# changing pipeline parameters to gridsearch results\npipe.set_params(**search.best_params_)\n\n# making predictions\npredictions = pipe.predict(X_test)\n\n# checking accuracy\naccuracy_score(y_test, predictions)\n\nSee Column Transformer section example for details on this pipeline\nNote the double underscore used in the keys of the parameter dict\n\nDouble underscores separate names of steps inside a nested pipeline with last name being the argument of the transformer function being tuned\nExample: ‘column_transformer__numerical__imputer__strategy’\n\ncolumn_transformer (pipeline step name) &gt;&gt; numerical (pipeline step name) &gt;&gt; imputer (pipeline step name) &gt;&gt; strategy (arg of SimpleImputer function)\n\n\nView tuning results\nbest_params = search.best_params_\nprint(best_params)\n\n# Stores the optimum model in best_pipe\nbest_pipe = search.best_estimator_\nprint(best_pipe)\n\nresult_df = DataFrame.from_dict(search.cv_results_, orient='columns')\nprint(result_df.columns)\n\n\n\n\nDisplay Pipelines in Jupyter\n\nfrom sklearn import set_config \nset_config(display=\"diagram\")\ngiant_pipeline",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-algs",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-algs",
    "title": "sklearn",
    "section": "Algorithms",
    "text": "Algorithms\n\nMisc\nHistogram-based Gradient Boosting Regression Tree\n\nHistogram-based models are more efficient since they bin the continuous features\nInspired by LightGBM. Much faster than GradientBoostingRegressor for big datasets (n_samples &gt;= 10 000).\n\nsklearn.ensemble.HistGradientBoostingRegressor\n\nuser guide\n\nStochastic Gradient Descent (SGD)\n\nalgorithm\nNot a class of models, just merely an optimization technique\n\nSGDClassifier(loss='log') results in logistic regression, i.e. a model equivalent to LogisticRegression which is fitted via SGD instead of being fitted by one of the other solvers in LogisticRegression.\nSGDRegressor(loss='squared_error', penalty='l2') and Ridge solve the same optimization problem, via different means.\n\nPenalyzed regression hyperparameters are labelled different than in R\n\nlambda (R) is alpha (py)\nalpha (R) is 1 - L1_ratio (py)\n\nCan be successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing Advantages:\n\nEfficiency.\nEase of implementation (lots of opportunities for code tuning). Disadvantages:\nSGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\nSGD is sensitive to feature scaling.\n\nProcessing\n\nMake sure you permute (shuffle) your training data before fitting the model or use shuffle=True to shuffle after each iteration (used by default).\nFeatures should be standardized using e.g. make_pipeline(StandardScaler(), SGDClassifier())\n\n\nBisectingKMeans\n\nCentroid is picked progressively (instead of simultaneously) based on the previous cluster. We would split the cluster each time until the number of K is achieved\nAdvantages\n\nIt would be more efficient with a large number of clusters\nCheaper computational costs\nIt does not produce empty clusters\nThe clustering result was well ordered and would create a visible hierarchy.\n\n\nXGBoost with GPU\ngbm = xgb.XGBClassifier(\n                    n_estimators=100000,\n                    max_depth=6,\n                    objective=\"binary:logistic\",\n                    learning_rate=.1,\n                    subsample=1,\n                    scale_pos_weight=99,\n                    min_child_weight=1,\n                    colsample_bytree=1,\n                    tree_method='gpu_hist',\n                    use_label_encoder=False\n                    )\neval_set=[(X_train,y_train),(X_val,y_val)] \nfit_model = gbm.fit( \n                X_train, y_train, \n                eval_set=eval_set,\n                eval_metric='auc',\n                early_stopping_rounds=20,\n                verbose=True \n              )\n\ntree_method='gpu_hist' specifies the use of GPU\nPlot importance\nfig,ax2 = plt.subplots(figsize=(8,11))\nxgb.plot_importance(gbm, importance_type='gain', ax=ax2)\n\nGBM with Quantile Loss (PIs)\ngbm_lower = GradientBoostingRegressor(\n    loss=\"quantile\", alpha = alpha/2, random_state=0\n)\ngbm_upper = GradientBoostingRegressor(\n    loss=\"quantile\", alpha = 1-alpha/2, random_state=0\n)\ngbm_lower.fit(X_train, y_train)\ngbm_upper.fit(X_train, y_train)\ntest['gbm_lower'] = gbm_lower.predict(X_test)\ntest['gbm_upper'] = gbm_upper.predict(X_test)",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-cv",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-cv",
    "title": "sklearn",
    "section": "CV/Splits",
    "text": "CV/Splits\n\nK-Fold\n\nfrom sklearn.model_selection import KFold\ncv = KFold(n_splits=7, shuffle=True)\n\nShuffling: minimizes the risk of overfitting by breaking the original order of the samples\n\n\nStratifiedKFold\n\nfrom sklearn.model_selection import StratifiedKFold\ncv = StratifiedKFold(n_splits=7, shuffle=True, random_state=1121218)\n\nFor classification, class ratios are held to the same ratios in both the training and test sets.\n\nclass ratios are preserved across all folds and iterations.\n\n\nLeavePOut: from sklearn.model_selection import LeaveOneOut, LeavePOut\n\nData is so limited that you have to perform a CV where you set aside only a few rows of data in each iteration\nLeaveOneOut is P = 1 for LeavePOut\n\nShuffleSplit, StratifiedShuffleSplit\n\nfrom sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\ncv = ShuffleSplit(n_splits=7, train_size=0.75, test_size=0.25)\ncv = StratifiedShuffleSplit(n_splits=7, test_size=0.5)\n\nNot a CV, just repeats the train/test split process multiple times\nUsing different random seeds should resemble a robust CV process if done for enough iterations\n\nTimeSeriesSplit\n\nfrom sklearn.model_selection import TimeSeriesSplit\ncv = TimeSeriesSplit(n_splits=7)\n\nWith time series data, the ordering of samples matters.\n\nGroup Data\n\nData is not iid (e.g. multi-level data)\nOptions\n\nGroupKFold\nStratifiedGroupKFold\nLeaveOneGroupOut\nLeavePGroupsOut\nGroupShuffleSplit\n\nWorks just like the non-group methods but with a group arg for the grouping variable",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html",
    "href": "qmd/model-building-tidymodels.html",
    "title": "tidymodels",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modlbld-tidymod-misc",
    "href": "qmd/model-building-tidymodels.html#sec-modlbld-tidymod-misc",
    "title": "tidymodels",
    "section": "",
    "text": "CV dataset terminology: training, validation, test –&gt; analysis, assessment, test\n(I don’t think this is necessary anymore. Outcome may remain categorical) Transform categorical outcome variables to factors\n# outside recipe\nmutate(rain_tomorrow = factor(ifelse(rain_tomorrow, \"Rained\", \"Didn't Rain\")))\n\n# skip = TRUE is so this is tried to be applied the test set which won't have the outcome var when its being preprocessed\nstep_mutate(rain_tomorrow = factor(ifelse(rain_tomorrow, \"Rained\", \"Didn't Rain\"),\n            skip = TRUE)\n\n# this would have 0 and 1 values though\nstep_num2factor(rain_tomorrow)\nWith long running models, you can work with a sample of the data in order to iterate through initial ranges of tuning grids. Once you start to narrow down the range of hyperparameter values, then you can adjust the portion of the data and/or number of folds.\ntrain_fold_small &lt;- train %&gt;%\n            sample_n(4000) %&gt;%\n            vfold_cv(v = 2)\n# change data obj in resamples arg of tune_grid\nAccessing a fitted model object to see coefficient values, hyperparameter values, etc.\nlin_trained &lt;- lin_workflow %&gt;%\n        finalize_workflow(select_best(lin_tune)) %&gt;%\n        fit(split_obj) # or train_obj or test_ob, etc.\n\nlin_trained$fit$fit %&gt;%\n        broom::tidy()\n\n# returns a parsnip object\nextract_fit_parsnip(lin_trained) %&gt;%\n    tidy()\n# returns an engine specific object\nxgboost::xgb.importance(model = extract_fit_engine(xgb_fit)\nUtilizing a sparse matrix with blueprint arg\nwf_sparse &lt;- \n  workflow() %&gt;%\n  add_recipe(text_rec, blueprint = sparse_bp) %&gt;%\n  add_model(lasso_spec)\n\nestimates, model performance metrics, etc. are unaffected\nAdvantages\n\nSpeed is gained from any specialized model algorithms built for sparse data.\nThe amount of memory this object requires decreases dramatically.\n\nEngines that can utilize a sparse matrix: glmnet, xgboost, and ranger.\n\nIf your CV results have close scores –&gt; increase from 5-fold to 10 fold cv\n\nCan take much longer though\n\nBack-transform predictions if you made a non-recipe transformation of the target variable\n# log 10 transformed target variable\npreds_intervals &lt;- predict(\n  workflows::pull_workflow_fit(lm_wf),\n  workflows::pull_workflow_prepped_recipe(lm_wf) %&gt;% bake(ames_holdout),\n  type = \"pred_int\",\n  level = 0.90\n) %&gt;% \n  mutate(across(contains(\".pred\"), ~10^.x))\nGet CV coefs (?)\nget_glmnet_coefs &lt;- function(x) {\n  x %&gt;%\n    extract_fit_engine() %&gt;%\n    tidy(return_zeros = TRUE) %&gt;%\n    rename(penalty = lambda)\n}\nparsnip_ctrl &lt;- control_grid(extract = get_glmnet_coefs)\n\nglmnet_res &lt;-\n  glmnet_wflow %&gt;%\n  tune_grid(\n    resamples = bt,\n    grid = grid,\n    control = parsnip_ctrl\n  )\nShowing table of results from multiple models\nlist(\n  \"Original\" = unbalanced_model, \n  \"Undersampled\" = balanced_model, \n  \"Smote\" = smote_model\n) %&gt;%\n  map(tidy) %&gt;%\n  imap(~ select(.x, \"Term\" = term, !!.y := \"estimate\")) %&gt;%\n  reduce(inner_join, by = \"Term\") %&gt;%\n  pretty_print()",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-steps",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-steps",
    "title": "tidymodels",
    "section": "Steps",
    "text": "Steps\n\nset-up\nrecipe\nSpecify model(s)\nworkflow, workflow_set\nfit, fit_resamples, or tune_grid\nautoplot\ncollect metrics\nRepeat as necessary\nfinalize_workflow\nlast_fit",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-debug",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-debug",
    "title": "tidymodels",
    "section": "Checks, Debugging",
    "text": "Checks, Debugging\n\ncheck number of predictors and preprocessing results\ncheck_recipe &lt;- function(recipe_obj) {\nrecipe_obj %&gt;%  # already created recipe instructions\n    prep() %&gt;%  # instantiates recipe (makes necessary calcs)\n    bake() %&gt;% # executes recipe on data\n    glimpse()\n}\ncheck tuning error\ntune_obj$.notes[[2]] # 2 is the fold number\ncheck a particular fold after preprocessing\nrecipe_obj %&gt;%\ntraining(cv_obj$splits[[2]]) %&gt;% # 2 is the fold number\nprep()\nshow_best: Display the top sub-models and their performance estimates\nshow_best(tune_obj, \"rmse\")\n\nexample orders models by rmse\n\nCheck learning curves after tuning a model\nautoplot(tune_obj)",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-helpers",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-helpers",
    "title": "tidymodels",
    "section": "Helpers",
    "text": "Helpers\n\npredictions + newdata tbl for workflow and stacks objects\naugment.workflow &lt;- function(x, newdata, ...) {\npredict(x, newdata, ...) %&gt;%\nbind_cols(newdata)\n}\n# model_stack is the class of a stack obj\naugment.model_stack &lt;- function(x, newdata, ...) {\npredict(x, newdata, ...) %&gt;%\nbind_cols(newdata)\n}",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-setup",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-setup",
    "title": "tidymodels",
    "section": "Set-up",
    "text": "Set-up\n\nSome packages\n\ntidyverse - cleaning\ntidymodels - model building\nscales - loss metrics\nlubridate - ts cleaning functions\ntextrecipes - tokenizers, etc.\nthemis - up/down-sampling for imbalanced/unbalanced outcomes\nstacks - ensembling\n\nSplit\nset.seed(2021)\nspl &lt;- initial_split(dataset, prop = .75)\ntrain &lt;- training(spl)\ntest &lt;- testing(spl)\ntrain_folds &lt;- train %&gt;%\n        vfold_cv(v = 5)\n\nMight be useful to create a row id column before splitting to keep track of observations after the split\n\ndf %&gt;% rowid_to_column(var = \"row_id\")  then use step_rm(row_id) to remove it in the recipe\n\n\nMetrics\nmset &lt;- metric_set(mn_log_loss)\nParallel\ndoFuture::registerDoFuture()\ndoFuture::plan(multisession, workers = 4)\nGrid control options\ngrid_control &lt;- control_grid(\n                        # save the out-of-sample predictions\n                        save_pred = TRUE,\n                        save_workflow = TRUE,\n                        extract = extract_model)\nggplot theme\ntheme_set(theme_light)\nLiked this theme for calibration curves",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-wkflw",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-wkflw",
    "title": "tidymodels",
    "section": "Workflow",
    "text": "Workflow\n\nformat: workflow function, add_recipe, add_model\nimb_wf &lt;-\n  workflow() %&gt;%\n  add_recipe(bird_rec) %&gt;%\n  add_model(bag_spec)\nCan also add fit(training(splits)) to the pipe to go ahead and fit the model if you’re not tuning, etc.\nFit multiple models and multiple recipes\npreproc &lt;- list(none = basic_recipe, pca = pca_recipe, sp_sign = ss_recipe)\nmodels &lt;- list(knn = knn_mod, logistic = lr_mod)\nwf_set &lt;- workflow_set(preproc, models, cross = TRUE)\n\ncross = TRUE says fit each model with each recipe",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-fit",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-fit",
    "title": "tidymodels",
    "section": "Fit",
    "text": "Fit\n\nfit:  fit a model on a dataset\nimb_fit &lt;- fit(workflow_obj, data = dat)\nfit_resample: cross-validation\nimb_rs &lt;-\n  fit_resamples(\n    workflow_obj,\n    resamples = cv_folds_obj,\n    metrics = metrics_obj\n  )\ncollect_metrics(imb_rs)",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-tune",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-tune",
    "title": "tidymodels",
    "section": "Tune",
    "text": "Tune\n\nCV and Tune Model with Multiple Recipes (article)\ndoParallel::registerDoParallel()\nset.seed(345)\n\ntune_res &lt;-\n    workflow_map(\n        wf_set,\n        \"tune_grid\",\n        resamples = cv_folds,\n        grid = 15,\n        metrics = metric_set(accuracy, roc_auc, mn_log_loss, sensitivity, specificity),\n        param_info = parameters(narrower_penalty)\n    )\n\n“wf_set” is a workflow_set object\n“grid = 15”: this workflow set is only tuning one model, glmnet, and the parameter being tuned is the penalty. So “grid = 15” says try 15 different values.\n“narrow_penalty” is narrower_penalty &lt;- penalty(range = c(-3, 0)\nViz: autoplot(tune_res)\n\n\nInterpretation:\n\nWhen accuracy is high, specificity (accuracy at picking negative class) is very high but sensitivity (accuracy at picking positive class) is very low.\nAdd middling values of specificity and sensitivity, mean log loss is also middling\nmean log loss is high when sensitivity is high\nAUROC is high at middling values of specificity and sensitivity indicating a best model for discriminating both classes. (but which recipe is this; see next section)\n\nWorkflow Rank (X-Axis): In this example there were 15 values of penalty considered and 2 differerent recipes, so I’m guessing that’s where the 30 comes from.\n\nExtract a particular model/recipe combo and viz\n\ndownsample_rs &lt;-\n  tune_rs %&gt;%\n  extract_workflow_set_result(\"downsampling_glmnet\")\nautoplot(downsample_rs)\n\nY-Axis: metric value, X-Axis: parameter value (e.g. glmnet penalty value)\nWhere “downsampling_glmnet” is the model/recipe combo you want to extract\nShows how downsampling affects the glmnet model\nInterpretation:\n\nLooks like the 1st half of workflow set in above viz is the downsampling recipe\nShows downsampling without recalibration of the predicted probabilities results in a model that can only pick 1 class well based on the penalty\nThe GOF metrics: high accuracy, low mean log loss, high auroc prefer the model that picks the negative class really well.\n\n\nShow results ordered by a metric\nrank_results(tune_rs, rank_metric = \"mn_log_loss\")\n\nresults ordered from best to worst according to the metric\n\n\ntune_grid\n\nFormat: workflow, data, grid, metrics, control\n\nlin_tune &lt;- lin_wf %&gt;%\n  tune_grid(train_fold,\n            grid = crossing(penalty = 10 ^ seq(-6, -.5, .05)),\n            metrics = mset,  # see set-up section\n            control = grid_control) # see set-up section\nTuning multiples of the same hyperparameter\n\nadd id to specification (e.g. recipe step)\nuse id name instead of parameter name in grid\n\nstep_ns(var1, deg_free = tune(\"df_var1\"))\nstep_ns(var2, def_free = tune(\"df_var2\"))\n\ntune_obj &lt;- workflow_obj %&gt;%\n    tune_grid(train_fold,\n              grid = crossing(df_var1 = 1:4, df_var2 = 4:6),\n              metrics = mset,\n              control = grid_control)\nGrids\n\n{dials}\npurrr::crossing\ngrid = crossing(mtry = 2:7,\n                trees - seq(100, 1000, 100),\n                learn_rate = 0.1)\nLatin Hypercube\nxgb_grid &lt;- grid_latin_hypercube(\n    tree_depth(),\n    min_n = sample(2, 10, 3)\n    loss_reduction = sample(runif(min = 0.01, max = 0.1), 3),\n    sample_size = sample(0.5:0.8, 3),\n    mtry = sample(3:6, 3),\n    learn_rate = sample(runif(min = 0.03, max = 0.1), 3)\n)\n\nsize: total number of parameter value combinations (default: 3)\n\nRandom\nxgb_grid &lt;- grid_random(\n    tree_depth(),\n    min_n(),\n    loss_reduction(),\n    learn_rate(),\n    finalize(sample_prop(), train_data),\n    finalize(mtry(), train_baked),\n    size = 20\n)\n\nsize: default: 5\n\n\nGet coefficients for folds\nget_glmnet_coefs &lt;- function(x) {\n  x %&gt;% \n    extract_fit_engine() %&gt;% \n    tidy(return_zeros = TRUE) %&gt;% \n    rename(penalty = lambda)\n} \nglmnet_res &lt;- \n  glmnet_wflow %&gt;% \n  tune_grid(\n    resamples = bt,\n    grid = grid,\n    control = control_grid(extract = get_glmnet_coefs)\n  )\nglmnet_coefs &lt;- \n  glmnet_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) %&gt;% \n  select(id, mixture, .extracts) %&gt;% \n  group_by(id, mixture) %&gt;%          # ┐\n  slice(1) %&gt;%                      # │ Remove the redundant results\n  ungroup() %&gt;%                      # ┘\n  unnest(.extracts)\nglmnet_coefs %&gt;% \n  select(id, penalty, mixture, term, estimate) %&gt;% \n  filter(term != \"(Intercept)\")\n#&gt; # A tibble: 300 × 5\n#&gt;    id        penalty mixture term      estimate\n#&gt;    &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n#&gt;  1 Bootstrap1 1          0.1 Clark_Lake    0.391\n#&gt;  2 Bootstrap1 0.464      0.1 Clark_Lake    0.485\n#&gt;  3 Bootstrap1 0.215      0.1 Clark_Lake    0.590\n#&gt;  4 Bootstrap1 0.1        0.1 Clark_Lake    0.680\n#&gt;  5 Bootstrap1 0.0464      0.1 Clark_Lake    0.746\n#&gt;  6 Bootstrap1 0.0215      0.1 Clark_Lake    0.793\n#&gt;  7 Bootstrap1 0.01        0.1 Clark_Lake    0.817\n#&gt;  8 Bootstrap1 0.00464    0.1 Clark_Lake    0.828\n#&gt;  9 Bootstrap1 0.00215    0.1 Clark_Lake    0.834\n#&gt; 10 Bootstrap1 0.001      0.1 Clark_Lake    0.837\n#&gt; # … with 290 more rows\n\nextract_fit_engineadds an additional column “.extracts” to tuning output\n“return_zeros” keeps records of variables with penalized coefficients to zero during tuning\nViz\n\nglmnet_coefs %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(mixture = format(mixture)) %&gt;% \n  ggplot(aes(x = penalty, y = estimate, col = mixture, groups = id)) + \n  geom_hline(yintercept = 0, lty = 3) +\n  geom_line(alpha = 0.5, lwd = 1.2) + \n  facet_wrap(~ term) + \n  scale_x_log10() +\n  scale_color_brewer(palette = \"Accent\") +\n  labs(y = \"coefficient\") +\n  theme(legend.position = \"top\")\n\nInterpretation\n\nWith a pure lasso model (i.e., mixture = 1), the Austin station predictor is selected out in each resample. With a mixture of both penalties, its influence increases.\n\nAlso, as the penalty increases, the uncertainty in this coefficient decreases.\n\nThe Harlem predictor is either quickly selected out of the model or goes from negative to positive.",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-final",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-final",
    "title": "tidymodels",
    "section": "Finalize",
    "text": "Finalize\n\nMisc\n\nMake sure to set control option, ctrl &lt;- control_grid(save_workflow = TRUE)\ncollect_notes(tuning_results) - If you need more information about errors or warnings that occur\n\nTo turn off the interactive logging, set the verbose control option to TRUE\n\n\nfit_best(x, metric = NULL, parameters = NULL, verbose = FALSE, ...) is a shortcut for:\nbest_param &lt;- select_best(tune_results, metric) # or other `select_*()`\nwflow &lt;- finalize_workflow(wflow, best_param)  # or just `finalize_model()`\nwflow_fit &lt;- fit(wflow, data_set)\n\nx: results of class, “tune_results”, from functions like tune_*\n\nSelect a simpler model or a more complex model\ndownsample_rs &lt;- \n  tune_rs %&gt;% \n  extract_workflow_set_result(\"downsampling_glmnet\")\n\nbest_penalty &lt;- \n  downsample_rs %&gt;%\n  select_by_one_std_err(-penalty, metric = \"mn_log_loss\")\n\nfinal_fit &lt;- \n  wf_set %&gt;% \n  extract_workflow(\"downsampling_glmnet\") %&gt;%\n  finalize_workflow(best_penalty) %&gt;%\n  last_fit(feeder_split)\n\nSelecting a simpler model can reduce chances of overfitting\nSelects a model that has a mean log loss score that’s 1 standard deviation from the top mean log loss value\n\nselect_by_pct_loss() also available\n\nThe first argument is passed to dplyr::arrange\n\nSo with “-penalty,” penalty is a column in the results tibble that you want to sort by and the “-” says you want start at the higher penalty parameter values\n\nA higher penalty (lasso model) means a simpler model\n“tune_rs” is a workflow_map object\nThe last chunk finalizes the original tuneable workflow with this value for the penalty, and then last_fit   fits the model one time to the (entire) training data and evaluates one time on the testing data.\nMetrics\ncollect_metrics(final_fit)\ncollect_predictions(final_fit) %&gt;%\n  conf_mat(squirrels, .pred_class)\nVariable Importance\nlibrary(vip)\nfeeder_vip &lt;-\n  extract_fit_engine(final_fit) %&gt;%\n  vi()\nfeeder_vip\n\nfeeder_vip %&gt;%\n  group_by(Sign) %&gt;%\n  slice_max(Importance, n = 15) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(Importance, fct_reorder(Variable, Importance), fill = Sign)) + \n  geom_col() +\n  facet_wrap(vars(Sign), scales = \"free_y\") +\n  labs(y = NULL) +\n  theme(legend.position = \"none\")\n\nfinalize_model, recipe, workflow\n\nafter finding the best parameter values, this updates the object\n\nlin_wf_best &lt;- lin_wf %&gt;%\n  finalize_workflow(select_best(lin_tune))\n\n# or manually list parameter/vale pairs\nxgb_wf_best &lt;- lin_wf %&gt;% \n  finalize_workflow(list(mtry = 6, trees = 1200, learn_rate = 0.01)\nlast_fit: fit finalized model on entire learning dataset and collect metrics\nxgb_wf_best %&gt;%\n    last_fit(initial_split_obj, metrics = metric_set_obj) %&gt;%\n    collect_metrics()\n\nManually\n        xgb_wf_best %&gt;%\n            fit(train) %&gt;%\n            augment(test, type_predict = \"prob\") %&gt;%\n            mn_log_loss(churned, .pred_yes)\n\nPredict on new data\nxgb_wf_best %&gt;%\n    fit(new_data) %&gt;%\n    predict(newdata, type = \"prob\") %&gt;%\n    bind_cols(newdata)\nPredict on test set and score\nwflw_fit %&gt;%\n    predict(testing(splits) %&gt;% select(outcome_var)) %&gt;%\n    metric_set(rmse, mae, rsq)(outcome_var, .pred)",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-modspec",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-modspec",
    "title": "tidymodels",
    "section": "Model Specification",
    "text": "Model Specification\n\nFormat: model function, set_engine, set_mode (“classification” or “regression”)\nbag_spec &lt;-\n  bag_tree(min_n = 10) %&gt;%\n  set_engine(\"rpart\", times = 25) %&gt;%\n  set_mode(\"classification\")\nLogistic/Linear\n# lasso\nlogistic_reg(penalty = tune()) %&gt;%\n    set_engine(\"glmnet\")\n\nfunction: logistic_reg, linear_reg\n\nargs: penalty (λ), mixture (α) (number or tune())\n\npenalty grid examples: 10^seq(-7, -1, .1), 10 ^ seq(-6, -.5, .05)\npenalty default: penalty(range = c(-10, 0), trans = log10_trans())\n\nSilge uses range = c(-3, 0)\n\n\n\nengines\n\ntypes: glm, glmnet, LiblineaR (only logistic), stan, spark, keras\nLiblinear\n\nbuilt for large data\n\nfaster than using glmnet even w/sparse matrix\n\nrequires mixture = 0 or 1; default: 0 (ridge)\nalso regularizes intercept which affects coef values\n\nglmnet\n\nmixture default = 1 (lasso)\n\nspark\n\nmixture default = 0 (ridge)\n\nargs\n\npath_values: sequence of  values for penalty\n\nassigns a collection of penalty values used by each glmnet fit (regardless of the data or value of mixture)\n\nWithout this arg, glmnet uses multiple penalty values which depend on the data set; changing the data (or the mixture amount) often produces a different set of values\n\nSo, if you want to look at results for a specific penalty value, you need to use this arg to make sure its included.\n\n\nmust still give penalty arg a number\nif using ridge, include 0 in sequence of values for penalty\n\npath_values = c(0, 10^seq(-10, 1, length.out = 20))\n# example in docs\n10^seq(-3, 0, length.out = 10)\n# drob's range\n10 ^ seq(-6, -.5, .05)\n\n\n\nSupport Vector Machines (SVM)\nsvm_linear() %&gt;% \n  set_engine(\"LiblineaR\") %&gt;% \n  set_mode(\"regression\")\n\nmodes: regression or classification\nengines\n\nLiblineaR\n\nbuilt for bigdata (C++ library)\nNo probability predictions for classification models\n\nIn order to use tune_grid, metrics must be hard classification metrics (e.g. accuracy, specificity, sensitivity, etc.)\nWon’t be able to adjust thresholds either\n\nL2-regularized\n\nkernlab\n\nkernel = “vanilladot”\n\nargs\n\ncost: default = 1\nmargin: default = 0.1\n\n\n\nBoosted Trees\nxgb_mod &lt;- boost_tree(\"regression\",\n                      mtry = tune(),\n                      trees = tune(),\n                      learn_rate = 0.01,\n                      stop_iter = tune()) %&gt;%\n        set_engine(\"xgboost\")\n\nXGBoost\n\ndrob says\n\nlogging predictors has no effect on this algorithm\n\nargs\n\nmtry\ntrees\nlearn_rate\ntree_depth\nstop_iter: early stopping; stops if no improvement has been made after this many iterations\n\n\n\nRandom Forest\nrand_forest(\n  mode = \"regression\",\n  engine = \"ranger\",\n  mtry = tune(),\n  trees = tune(),\n  min_n = tune()\n) %&gt;%\n  set_engine(importance = \"permutation\",\n            seed = 63233,\n            quantreg = TRUE)\n\nranger doc\nargs\n\nmode: “classification”, “regression”, “unknown”\nengine: ranger (default), randomForest, spark\ndefault hyperparameters: mtry, trees, min_n\nimportance: ‘none’, ‘impurity’, ‘impurity_corrected’, ‘permutation’\nnum.threads = 1 (default)\nquantreg = TRUE for quantile regression\npreds &lt;- regr_fit$fit %&gt;% predict(airquality, type = 'quantiles')\nhead(preds$predictions)\n\nCurrently have to extract the ranger model in order to obtain quantile predictions\nissue still open",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-recipe",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-recipe",
    "title": "tidymodels",
    "section": "Recipes",
    "text": "Recipes\n\nMisc\n\nDocs, FunctionReference\nStep args are tunable. Just use tune() for value in the arg and include it in tune_grid\n\ne.g spline degrees in step_ns or bs, number of bootstraps in step_impute_bag, or number of neighbors in step_impute_knn\n\nrecipe(formula, data)\nrecipe(rain_tomorrow ~ date + location + rain_today +\n                min_temp + max_temp + rainfall +\n                wind_gust_speed + wind_speed9am +\n                wind_speed3pm + humidity9am + humidity3pm + pressure9am +\n                pressure3pm + cloud9am + cloud3pm + temp9am + temp3pm +\n                wind_gust_dir + wind_dir9am + wind_dir3pm +\n                rain_today, data = train)\nVariable selectors\n\nbased on role\n\nhas_role, all_outcomes, all_predictors, all_numeric_predictors, all_nominal_predictors\n\nbased on type\n\nhas_type, all_numeric, all_nominal\n\n\n\n\n\nOther\n\nzv, nzv\n\nRemoves predictors with zero variance or near-zero variance\n\nstep_nsv(all_predictors(), freq_cut = 90/10)\n\nfreq_cut: ratio that determines when indicator variables with too few 1s or too few 0s are removed.\n\ndefault: 95/5\n\nunique_cut: percentage (not a decimal), number of unique values divided by the total number of samples\n\ndefault: 10\n\n\nstep_indicate_na\nexample_data &lt;- tibble(\n  x1 = c(1, 5, 8, NA, NA, 3),\n  x2 = c(1, NA, 3, 6, 2, 2),\n  x3 = c(NA, NA, NA, NA, NA, NA),\n  x4 = c(7, 8, 4, 2, 1, 1)\n)\n\nrecipe(~ ., data = example_data) |&gt;\n  step_indicate_na(all_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 6 × 8\n#&gt;      x1    x2 x3       x4 na_ind_x1 na_ind_x2 na_ind_x3 na_ind_x4\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt;     &lt;int&gt;     &lt;int&gt;     &lt;int&gt;     &lt;int&gt;\n#&gt; 1     1     1 NA        7         0         0         1         0\n#&gt; 2     5    NA NA        8         0         1         1         0\n#&gt; 3     8     3 NA        4         0         0         1         0\n#&gt; 4    NA     6 NA        2         1         0         1         0\n#&gt; 5    NA     2 NA        1         1         0         1         0\n#&gt; 6     3     2 NA        1         0         0         1         0\n\nCreates dummys for the NAs in a column(s)\nIf you’re doing this, I think you’re assuming the missingness is MNAR (See Missingness)\n\nstep_clean_names - From {textrecipes}, a step_* function for the {janitor} function from tidying column names\nstep_clean_levels - From {textrecipes}, some levels will produce bad column names if used for other things such as dummy variables, so this function cleans the bad category names.\n\n\n\nTransformations\n\nstep_log, BoxCox, YeoJohnson, sqrt\nstep_log(rainfall, offset = 1, base = 2)\n\nIf variable has zeros, offset adds 1 to the value.\nbase is the log base\n\nsplines\n\nstep_bs (B-Splines); step_ns (Natural Splines), step_poly (Orthogonal polynomial)\n\ndocs\n\ndeg_free: where larger values –&gt; more complex curves\ndegree: bs-only, degree of polynomial spline (e.g. 2 quadratic, 3 cubic)\nOriginal variable replaced with “varname_ns_1”\n{splines2} versions\n\nA supplementary package to {splines}. The practical benefits\nI-splines as integrals of M-splines are used for monotone regressions; C-splines as integrals of I-splines are used for shape-restricted regressions\ntime-to-event analysis, the derivatives or integrals of the basis functions are needed in inferences\nallows basis functions of degree zero which is useful in some applications\nnatural cubic splines more computationally efficient\n\nstep_spline_nonnegative\ndata(ames, package = \"modeldata\")\n\nrecipe(Sale_Price ~ Lot_Frontage + Lot_Area, data = ames) |&gt;\n  step_spline_nonnegative(starts_with(\"Lot_\"), deg_free = 3) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 × 7\n#&gt;    Sale_Price Lot_Frontage_1 Lot_Frontage_2 Lot_Frontage_3 Lot_Area_1 Lot_Area_2\n#&gt;         &lt;int&gt;          &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1     215000        0.00522       0.00428       0.00117      5.87e-6    9.76e-7\n#&gt;  2     105000        0.00543       0.00186       0.000213     2.45e-6    1.24e-7\n#&gt;  3     172000        0.00545       0.00190       0.000221     3.00e-6    1.94e-7\n#&gt;  4     244000        0.00563       0.00238       0.000335     2.35e-6    1.14e-7\n#&gt;  5     189900        0.00528       0.00164       0.000169     2.91e-6    1.81e-7\n#&gt;  6     195500        0.00539       0.00179       0.000198     2.09e-6    8.85e-8\n#&gt;  7     213500        0.00379       0.000572      0.0000287    9.17e-7    1.58e-8\n#&gt;  8     191500        0.00392       0.000624      0.0000331    9.38e-7    1.65e-8\n#&gt;  9     236500        0.00366       0.000521      0.0000247    1.03e-6    2.01e-8\n#&gt; 10     189000        0.00480       0.00114       0.0000900    1.53e-6    4.57e-8\n#&gt; # ℹ 2,920 more rows\n#&gt; # ℹ 1 more variable: Lot_Area_3 &lt;dbl&gt;\n\n\n\n\nCategorical\n\nAlso see\n\nDOCS: Handling Categorical Predictors\nFeature Engineering, General &gt;&gt; Categoricals\n\nstep_dummy: if there are C levels of the factor, there will be C - 1 dummy variables created and all but the first factor level are made into new columns\nstep_other\n\ncollapses categories that have a frequency below a specified threshold\nthreshold: proportion of total rows that a category will be collapsed into other\n\ndefault = 0.05\ngrid value examples: 0.001, 0.003, 0.01, 0.03\n\n\nstep_unknown\n\nIf NAs aren’t random, they can be pooled into a category called “Unknown”\n\n\n\n\nImputation\n\nAlso see Missingness\ntldr;\n\nTry bag first, knn second\n\nSeems to me that bag would be more flexible\n\nrolling would be useful for time series\n\ncommon args\n\nimpute_with: arg for including other predictors (besides outcome and variable to be imputed) in the imputation model.\nstep_impute_&lt;method&gt;(var, impute_with = imp_vars(pred1, pred2))\n“skip” arg: When set to false, this step will not be used when the recipe is baked (unless newdata=NULL)\n\nuseful when down/up-sampling an outcome variable. New data should be asis in the wild. So usecases where the step is good for training but not production.\ndefault = T for step_filter, slice, sample, naomit\n\n\nbag\n\nbagged tree model\ncategoricals and numerics can be imputed\nPredictors can have NAs but can’t be included in the list of variables to be imputed\n\nGuess you could do separate imputation steps if you want to include them as predictors\n\nspeed up by lowering bootstrap replications\n\n# keepX = FALSE says don't return a df of predictions - not sure why that's included (memory optimization?)\noptions = list(nbagg = 5, keepX = FALSE))\nknn\n\nK-Nearest Neighbors\ncategoricals and numerics can be imputed\n\ngower distance used (only distance available)\n\npossible that missing values will still occur after imputation if a large majority (or all) of the imputing variables (predictors?) are also missing\nCan be computationally intensive and is NOT robust to outliers\nneighbors = 5 by default\noptions: nthreads, eps (threshold for values to be set to 0, default = 1e-8)\n\nlinear, mean, median\n\nregression model, mean, median\nonly numerics can be imputed\nif missingness is missing not at random (MNAR) then mean shouldn’t be used (probably not median either)\n\nmode\n\nmost common value\ndiscrete variable (maybe only nominal variables)\nif more than one mode, one of them will be chosen at random\n\nroll\n\nrolling window\nwindow: window of data that you want to use to calculate the statistic\n\nFor time series, maybe look at a acf, pacf plot to get an idea for a good window size\n\nstatistic: stat to calculate over the window (e.g. mean, median, etc)\n\nprobably takes a custom function with a single arg (data)\n\nvalues in the window that would be imputed in previous steps aren’t used to impute the current value.\n\n\n\n\nUp/Down-Sampling\n\n{themis} for unbalanced outcome variables\nCommon args\n\n“skip”: When set to false, this step will not be used when the recipe is baked (unless newdata=NULL)\n\nuseful when down/up-sampling an outcome variable. New data should be asis in the wild. So usecases where the step is good for training but not production.\ndefault = T for step_filter, slice, sample, naomit, and themis data balancing steps\n\n“over_ratio”: desired ratio of num_rows of  the minority level(s) to num_rows of the majority level. Default = 1 (i.e. all levels have equal number of rows)\n\nExample: Bring the minority levels up to about 200 rows each where the majority level has 16562 rows in the data. Therefore, 200/16562 is approx 0.0121. Set over_ratio to 0.0121 and each minority level will have 200 rows in the upsampled dataset.\n\n“neighbors”:  number of clusters, default = 5\n\nUpsampling\n\nAll methods handle multi-level factor variables except rose.\nMisc\n\nApplying SMOTE without re-calibration is bad (paper)\n\ntldr;\n\nIf time isn’t an issue AND you have a binary variable, rose sounds like a good approach, otherwise use bsmote (probably should tune the all_neighbor option)\n\nupsample\n\nrepeats rows of minority level until dataset has desired ratio\n\nsmote\n\nSMOTE alg performs knn, generates points between the that level’s points and the centroid for each cluster\nPreprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\nGenerated points can form “bridges” between that level’s outlier points and the main distribution. This is like adding a feature that doesn’t exist in the sample.\n\nbsmote\n\nborderline-SMOTE alg is the same as SMOTE except that it generates points in the border region between classes. Ignores outliers in the resampling process.\nPreprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\nall_neighbors; default = FALSE\n\nFALSE then points will be generated between nearest neighbors in its own class.\n\nThis can produce mini-clusters of points\n\nTRUE then points will be generated between neighbors of the upsampled class and the other classes.\n\nMay blur the line between clusters (and levels? Therefore potentially making classification tasks more difficult)\n\n\nOnly selecting from the borderline region can be an issue if there aren’t that many borderline points in the original dataset.\n\nadasyn\n\nsimilar to SMOTE, but generates the most points from points that have the smallest representation in the data, i.e. generates a lot of outlier or extreme value points.\n\nMight produce much more stable results with tree models as compared to regression models\n\nPreprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\n\nrose\n\nUses a “smoothed” bootstrap approach to generate new samples. (paper was incoherent as fuck)\nOnly for binary (factor) variables\nPreprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\nno mention of missing data in the docs but probably still a good idea.\n\n\nminority_prop: same function as over_ratio. Default = 0.5\nminority_smoothness and majority_smoothness\n\ndefault = 1. Reducing this value will shrink the (resampling?) region in the feature space associated with that class (minority or majority).\nIf these regions are too large, the boundary between class regions blurs. If too small, maybe you aren’t getting the variance out of the resampling that you could be getting.\n\n\n\nDownsampling\n\nMisc\n\nManually\nminority_class &lt;- data %&gt;%\n  count(class) %&gt;%\n  filter(n == min(n))\n\nbalanced &lt;- data %&gt;%\n  group_split(class, .keep = TRUE) %&gt;%\n  map_dfr(\n    ~ {\n      if (.x$class[1] == minority_class$class) {\n        .x\n      } else {\n        slice_sample(\n          .x,\n          n = minority_class$n,\n          replace = FALSE\n        )\n      }\n    }\n  )\n\nbalanced %&gt;%\n  count(class) %&gt;%\n  pretty_print()\n\ntldr;\n\nnone of the methods below sound too bad.\n\ndownsample\n\nremoves majority level(s) rows randomly to match the under_ratio (see over_ratio defintion above)\n\nnearmiss\nremoves majority level(s) rows by undersampling points in the majority level(s) based on their distance to other points in the same level. - Default: neighbors = 5 - Preprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\ntomek\n\nundersamples by removing pairs of points that each other’s nearest neighbor but are of different levels (tomek links)\n\nDoesn’t make sense to me this way. SInce it’s binary, maybe the point that’s the majority class gets removed the other point in the pair (minority class) gets saved.\n\nOnly for binary (factor) variables\nPreprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\n\n\n\n\n\nFeature Reduction\n\nstep_discretize_xgb, step_pca_sparse, step_pca_sparse_bayes, step_pca_truncated\n\nRequires {embed}\n\nstep_discretize_cart\nlibrary(embed)\n\ndata(ames, package = \"modeldata\")\n\nrecipe(Sale_Price ~ Lot_Frontage + Lot_Area, data = ames) |&gt;\n  step_discretize_cart(all_numeric_predictors(), outcome = \"Sale_Price\") |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 × 3\n#&gt;    Lot_Frontage Lot_Area              Sale_Price\n#&gt;    &lt;fct&gt;        &lt;fct&gt;                      &lt;int&gt;\n#&gt;  1 [118.5, Inf] [1.341e+04, Inf]          215000\n#&gt;  2 [60.5,81.5)  [1.093e+04,1.341e+04)     105000\n#&gt;  3 [60.5,81.5)  [1.341e+04, Inf]          172000\n#&gt;  4 [81.5,94.5)  [1.093e+04,1.341e+04)     244000\n#&gt;  5 [60.5,81.5)  [1.341e+04, Inf]          189900\n#&gt;  6 [60.5,81.5)  [8639,1.093e+04)          195500\n#&gt;  7 [24.5,49.5)  [-Inf,8639)               213500\n#&gt;  8 [24.5,49.5)  [-Inf,8639)               191500\n#&gt;  9 [24.5,49.5)  [-Inf,8639)               236500\n#&gt; 10 [49.5,60.5)  [-Inf,8639)               189000\n#&gt; # ℹ 2,920 more rows\n\nFits a decision tree using the numeric predictor against the outcome. Then replaces it with levels, according to the leafs of the tree.\n\nstep_umap\ndata(diamonds, package = \"ggplot2\")\n\nset.seed(1234)\n\nrecipe(price ~ ., data = diamonds) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_umap(all_numeric_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 53,940 × 3\n#&gt;    price   UMAP1   UMAP2\n#&gt;    &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1   326  -0.605   5.56 \n#&gt;  2   326  -1.17  -16.8  \n#&gt;  3   327  -1.78   -5.43 \n#&gt;  4   334 -10.7   -12.3  \n#&gt;  5   335  14.2     2.66 \n#&gt;  6   336   2.50    1.79 \n#&gt;  7   336  -5.48    0.914\n#&gt;  8   337   7.39   -5.29 \n#&gt;  9   337  -1.43   -5.95 \n#&gt; 10   338  -6.38   -0.785\n#&gt; # ℹ 53,930 more rows\nstep_nnmf_sparse\nlibrary(Matrix) # needs to be loaded for step to work\n\ndata(ames, package = \"modeldata\")\n\nrecipe(Sale_Price ~ ., data = ames) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_nzv(all_numeric_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; \n  step_nnmf_sparse(all_numeric_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 × 3\n#&gt;    Sale_Price   NNMF1  NNMF2\n#&gt;         &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1     215000  0.192  -0.473\n#&gt;  2     105000 -0.366   0.245\n#&gt;  3     172000 -0.165   0.141\n#&gt;  4     244000  0.255  -0.269\n#&gt;  5     189900  0.261  -0.160\n#&gt;  6     195500  0.311  -0.316\n#&gt;  7     213500  0.187  -0.307\n#&gt;  8     191500  0.0956 -0.475\n#&gt;  9     236500  0.315  -0.238\n#&gt; 10     189000  0.268  -0.112\n\nConverts numeric data into one or more components via non-negative matrix factorization with lasso penalization\n\nstep_kmeans\nlibrary(MachineShop)\n\nset.seed(1234)\n\nrecipe(~., data = mtcars) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_kmeans(all_numeric_predictors(), k = 3) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 32 × 3\n#&gt;    KMeans1   KMeans2 KMeans3\n#&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1  0.583  -0.217     -0.823\n#&gt;  2  0.583  -0.165     -0.666\n#&gt;  3  0.634  -1.01       0.771\n#&gt;  4 -0.624  -0.309      1.00 \n#&gt;  5 -0.703   0.439     -0.666\n#&gt;  6 -0.910  -0.327      1.22 \n#&gt;  7 -0.857   0.918     -0.996\n#&gt;  8  0.125  -0.734      1.16 \n#&gt;  9  0.166  -0.655      1.97 \n#&gt; 10  0.0166  0.000617   0.684\n#&gt; # ℹ 22 more rows\nstep_depth\ndata(penguins, package = \"modeldata\")\n\nrecipe(species ~ bill_length_mm + bill_depth_mm, data = penguins) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_depth(all_numeric_predictors(), class = \"species\") |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 344 × 6\n#&gt;    bill_length_mm bill_depth_mm species depth_Adelie depth_Chinstrap\n#&gt;             &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n#&gt;  1           39.1          18.7 Adelie       0.349            0     \n#&gt;  2           39.5          17.4 Adelie       0.145            0     \n#&gt;  3           40.3          18   Adelie       0.217            0     \n#&gt;  4           43.9          17.2 Adelie       0.00658          0.0735\n#&gt;  5           36.7          19.3 Adelie       0.0789           0     \n#&gt;  6           39.3          20.6 Adelie       0.0395           0     \n#&gt;  7           38.9          17.8 Adelie       0.329            0     \n#&gt;  8           39.2          19.6 Adelie       0.132            0     \n#&gt;  9           34.1          18.1 Adelie       0.0263           0     \n#&gt; 10           42            20.2 Adelie       0.0461           0     \n#&gt; # ℹ 334 more rows\n#&gt; # ℹ 1 more variable: depth_Gentoo &lt;dbl&gt;\n\nRequires {ddalpha}\nCalculates a centroid from continuous variables and measures the distance the observation is from the centroid. There are various distance measures. The vignette has visual of a 2 dimension case for each distance measure. Contours are drawn around the centroid which give you an idea about the formulation of the distance measure.\nIt needs a categorical variable. I think there’s a centroid for each category. Probably works best if that categorical variable is the outcome, but I can also seeing it being useful with an informative categorical predictor.\nA higher depth value means the observation is closer to the centroid.",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-ex",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-ex",
    "title": "tidymodels",
    "section": "Examples",
    "text": "Examples\n\nBasic workflow (article)\npacman::p_load(tidymodels, embed, finetune, vip)\n\n# \"Accredation\" is the binary outcome\nset.seed(123)\nmuseum_split &lt;- initial_split(museum_parsed, strata = Accreditation)\nmuseum_train &lt;- training(museum_split)\nmuseum_test &lt;- testing(museum_split)\nset.seed(234)\nmuseum_folds &lt;- vfold_cv(museum_train, strata = Accreditation)\n\n# \"Subject_Matter\" is a high cardinality categorical predictor\nmuseum_rec &lt;- \n  recipe(Accreditation ~ ., data = museum_train) %&gt;%\n  update_role(museum_id, new_role = \"id\") %&gt;%\n  step_lencode_glm(Subject_Matter, outcome = vars(Accreditation)) %&gt;%\n  step_dummy(all_nominal_predictors())\n\n# specify model and workflow\nxgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\nxgb_wf &lt;- workflow(museum_rec, xgb_spec)\n\n# fit\ndoParallel::registerDoParallel()\nset.seed(345)\nxgb_rs &lt;- tune_race_anova(\n  xgb_wf,\n  resamples = museum_folds,\n  grid = 15,\n  control = control_race(verbose_elim = TRUE)\n)\n\n# evaluate\ncollect_metrics(xgb_rs)\n# viz for race_anova grid search strategy\nplot_race(xgb_rs)\n\n # fit on whole training set\nxgb_last &lt;- xgb_wf %&gt;%\n  finalize_workflow(select_best(xgb_rs, \"accuracy\")) %&gt;%\n  last_fit(museum_split)\n\n# evalute final model on test\ncollect_metrics(xgb_last)\ncollect_predictions(xgb_last) %&gt;%\n    conf_mat(Accreditation, .pred_class)\nxgb_last %&gt;%\n  extract_fit_engine() %&gt;%\n  vip()",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/networks-analysis.html",
    "href": "qmd/networks-analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Networks",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-misc",
    "href": "qmd/networks-analysis.html#sec-net-anal-misc",
    "title": "Analysis",
    "section": "",
    "text": "Packages\n\n{statnet} - statnet is a collection of software packages for statistical network analysis that are designed to work together, with a common data structure and API, to provide seamless access to a broad range of network analytic and graphical methodology.\n\nList of individual package tutorials\nModels fit with MCMC, so can be slow.\n\n\nResources\n\nNetwork Analysis: Integrating Social Network Theory, Method, and Application with R\n\nAnalysis Questions\n\nAt a given moment in time:\n\nWho is connected to whom? Who is not connected?\nWhere, and who are the hubs?\nWhere and about what are the clusters? Are there silos?\n\nChanges over time:\n\nAre new connection forming?\nAre new patterns of connectivity forming?\nHow was our network before and after the introduction of an activity?\n\n\nIssues with Statistics\n\nThey are unable to leverage node features at all. All nodes with the same values for these summary statistics are indistinguishable from each other.\nThere is no learnable component in the production of these features. We cannot fit a custom objective or train them jointly with a downstream task.",
    "crumbs": [
      "Networks",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-terms",
    "href": "qmd/networks-analysis.html#sec-net-anal-terms",
    "title": "Analysis",
    "section": "Terms",
    "text": "Terms\n\nCentrality\n\nMeasures, abstractly, how important a given graph is to the connectivity of the overall graph\nHigher for nodes which lie in paths that efficiently connect many nodes to each other.\nTypes:\n\nBetweenness - Nodes with high betweenness centrality tend to be the “crossroads” between nodes, i.e. when seeking to connect with another node that isn’t immediately adjacent, it will typically involve a node with high betweeness centrality.\n\nThese nodes are important in keeping the network connected. Likely important intermediaries or bridges\nCalculated by counting the number of shortest paths that pass through a node and dividing by the total number of shortest paths in the network.\n\nCloseness - Nodes with high closeness centrality have quick access to many other nodes. These nodes have the shortest distance, in network terms, to all other nodes.\n\nThese nodes are important in spreading information to all other nodes as quickly and efficiently as possible.\nCloseness Centrality = 1 / (Sum of SPD from the node to all other nodes)\n\nWhere SPD is the shortest path distance. In practice, this would be done with a shortest path algorithm like Breadth-First Search or Dijkstra’s algorithm.\n\n\n\n\nClusters\n\nCluster Clique - a cluster that has at least one node that’s connected to another node outside of the cluster\nCluster Silo - a cluster that has no node connected to any other node outside of the cluster\n\nClustering coefficient\n\nMeasures the density of a node’s local portion of the graph.\nNodes who have neighbors that are all connected to each other will have a higher clustering coefficient\n\nDegeneracy - A network model is degenerate when the space that an MCMC sampler can explore is so constrained that the only way to get the observed g(y) is essentially to flicker between full and empty graphs in the right proportion.\n\nA good indication that you have a degenerate model is that you have NA values for standard errors on your model parameter estimates. You can’t calculate a variance – and, therefore, a standard error – if you simply flicker between full and empty graphs.\n\nDegree aka Degree Centrality - Total edges a given node has.\nDensity - the number of edges in the observed network divided by the number of possible edges\nEdges (aka Dyads)- connection between two nodes. Depending on the type of connection, the edge can have a direction.\nEdge Bundling - visually bundle together similar edges to reduce the visual clutter within a graph\nMultiplexity - The number of connections between two nodes\n\nCould be represented by the thickness, darkness, etc. of an edge between 2 nodes\nNodes that have high multiplicity with each other typically form clusters\n\nRandom Graph - A network with n nodes where the edges between nodes occur randomly with probability P (each potential edge is one Bernoulli trial). Network density is typically used for P.\nTransitivity of a relation means that when there is a tie from i to j, and also from j to h, then there is also a tie from i to h: friends of my friends are my friends\n\n\nTransitivity Index (aka Clustering Index) = # Transitive Triads / # Potentially Transitive Triads\n\nHas a range between 0 and 1 where 1 is a transitive graph.\nFor random graphs, the expected value of the transitivity index is close to the density of the graph.",
    "crumbs": [
      "Networks",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-layalg",
    "href": "qmd/networks-analysis.html#sec-net-anal-layalg",
    "title": "Analysis",
    "section": "Layouts",
    "text": "Layouts\n\nSpring\n\nFruchterman-Reingold force-directed algorithm\n\narranges the nodes so the edges have similar length and minimum crossing edges\n\n\nRandom - nodes positioned uniformly at random in the unit square\nCircular - nodes on a circle\nBipartite - nodes in two straight lines\nSpectral - nodes positioned using the eigenvectors of the graph Laplacian",
    "crumbs": [
      "Networks",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-ergm",
    "href": "qmd/networks-analysis.html#sec-net-anal-ergm",
    "title": "Analysis",
    "section": "Exponential Random Graph Models (ERGM)",
    "text": "Exponential Random Graph Models (ERGM)\n\nAnalogous to logistic regression: ERGMs predict the probability that a pair of nodes in a network will have a tie between them.\nMisc\n\nNotes from Introduction to ERGMs\nPackages\n\n{statnet} - See Misc &gt;&gt; Packages\n{ergmito} - Simulation and estimation of Exponential Random Graph Models (ERGMs) for small networks (up to 5 vertices for directed graphs and 7 for undirected networks) using exact statistics\n\nIn the case of small networks, the calculation of the likelihood of ERGMs becomes computationally feasible, which allows us avoiding approximations and do exact calculation, ultimately obtaining MLEs directly.\n\n\nCan be used for directed, undirected, valued, unvalued, and bipartite networks.\nFitting a model with just edges is kind of like an intercept-only regression model.\nLess informative for dense networks.\n\nExamples: From Introduction to ERGMs\n\nDense\n\n\n\nNot Dense\n\n\nTriads aka triangles (i.e. transitive relationships) cause problems in ERGMs (more triads → denser graph). They often lead to degeneracy.\n\nNAs for standard error estimates are a good indication of degeneracy.\nSince ERGMs don’t handle triads well, it is NOT recommended using “triangle” as an adjustment variable in your model\n\n\n\nGoal: Describe the local selection forces that shape the global structure of a network\nExamples of networks examined using ERGM include knowledge networks, organizational networks, colleague networks, social media networks, and networks of scientific development.\nThe basic principle underlying the method is comparison of an observed network to Exponential Random Graphs.\n\nThe Null Hypothesis is a Erdos-Renyi graph\n\nA random graph where the degree of any node is binomially distributed (with n-1 Bernoulli trials per node, for a directed graph). (n is the number of nodes)\n\n\nEquation:\n\\[\n\\text{logit}(Y_{ij} = 1 \\; | \\; y_{ij}^c) = \\theta^T \\delta (y_{ij})\n\\]\n\n\\(\\theta\\) is a vector of coefficients\n\\(y_{ij}\\) denotes ijth dyad in graph \\(y\\)\n\nIf \\(y_{ij} = 1\\), then i and j are connected by an edge.\nIf \\(y_{ij} = 0\\), then i and j are NOT connected by an edge.\n\\(y_{ij}^c\\) is the complement (i.e. all other pairs of vertices in \\(y\\) other than (i, j)).\n\n\\(\\delta(y_{ij})\\) is the change statistic. A measure of how the graph statistic \\(g(y)\\) changes if the ijth vertex is toggled on or off.\n\n\\(\\delta(y_{ij}) = g(y_{ij}^+) - g(y_{ij}^-)\\)\n\\(y_{ij}^+\\) is the same network as \\(y\\) except that \\(y_{ij} = 1\\).\n\\(y_{ij}^-\\) is the same network as \\(y\\) except that \\(y_{ij} = 0\\).\n\n\nExample:{statnet} Edges model\nbottmodel.01 &lt;- ergm(bott[[4]] ~ edges)\n\n## Evaluating log-likelihood at the estimate.\nsummary(bottmodel.01)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges\n## \n## Iterations:  4 out of 20 \n## \n## Monte Carlo MLE Results:\n##       Estimate Std. Error MCMC %  p-value    \n## edges  -0.7621     0.2047      0 0.000313 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 137.6  on 109  degrees of freedom\n##  \n## AIC: 139.6    BIC: 142.3    (Smaller is better.)\nExample:{statnet} Edges and Triads model\nsummary(bott[[4]]~edges+triangle)\n##    edges triangle \n##       35       40\n\nbottmodel.02 &lt;- ergm(bott[[4]]~edges+triangle)\nsummary(bottmodel.02)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges + triangle\n## \n## Iterations:  2 out of 20 \n## \n## Monte Carlo MLE Results:\n##          Estimate Std. Error MCMC % p-value\n## edges    -0.55772    0.58201      0   0.340\n## triangle -0.05674    0.15330      0   0.712\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 137.5  on 108  degrees of freedom\n##  \n## AIC: 141.5    BIC: 146.9    (Smaller is better.)\nattributes of the individuals who make up our graph vertices may affect their propensity to form (or receive) ties\ntest this hypothesis, we can employ nodal covariates using the nodecov() term.\nbottmodel.03 &lt;- ergm(bott[[4]]~edges+nodecov('age.month'))\nsummary(bottmodel.03)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges + nodecov(\"age.month\")\n## \n## Iterations:  4 out of 20 \n## \n## Monte Carlo MLE Results:\n##                    Estimate Std. Error MCMC % p-value\n## edges             -1.526483   1.335799      0   0.256\n## nodecov.age.month  0.009501   0.016352      0   0.562\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 137.3  on 108  degrees of freedom\n##  \n## AIC: 141.3    BIC: 146.7    (Smaller is better.)\n\nresult suggests that in this dataset, the combined age of the children involved in a dyad as no effect on the probability of a tie between them, in either direction.\n\nimitation is a directed relationship. We might expect that older children are more likely to be role models to others. To test this hypothesis, we can use the directed variants of nodecov(): nodeocov() (effect of an attribute on out-degree) and nodeicov() (effect of an attribute on in-degree). We note that the nodecov() group of terms are for numeric attributes; nodefactor() terms are available for categorical attributes.\nbottmodel.03b &lt;- ergm(bott[[4]]~edges+nodeicov('age.month'))\n\nsummary(bottmodel.03b)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges + nodeicov(\"age.month\")\n## \n## Iterations:  4 out of 20 \n## \n## Monte Carlo MLE Results:\n##                    Estimate Std. Error MCMC % p-value   \n## edges              -2.89853    0.96939      0 0.00345 **\n## nodeicov.age.month  0.05225    0.02272      0 0.02340 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 132.1  on 108  degrees of freedom\n##  \n## AIC: 136.1    BIC: 141.5    (Smaller is better.)\n\nThe number of other children who imitated a child increase with the child’s age",
    "crumbs": [
      "Networks",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-ndembd",
    "href": "qmd/networks-analysis.html#sec-net-anal-ndembd",
    "title": "Analysis",
    "section": "Node Embeddings",
    "text": "Node Embeddings\n\nLearnable vectors of numbers that can be mapped to each node in the graph, allowing us to learn a unique representation for each node.\n\nUse as features in a downstream model.\n\nMethods\n\nDeepWalk and Node2vec papers\n\nUse the concept of a random walk, which involves beginning at a given node and randomly traversing edges, to produce pairs of nodes that are nearby each other.\nTrained by maximizing the cosine similarity between nodes that co-occurred in random walks.\n\nThis training objective leverages the homophily assumption, which states that nodes that are connected to each other tend to be similar to each other.\n\n\n\nIssues with Embeddings\n\nThey do not use node features at all. They assume that close-by nodes are similar without actually using the node features to confirm this assumption.\n\nThey rely on a fixed mapping from node to embedding (i.e. this is a transductive method).\n\nFor dynamic graphs, where new nodes and edges may be added, the algorithm must be re-ran from scratch, and all node embeddings need to be recalculated. In real-world problems, this is quite a big issue, as most online platforms have new users signing up every day, and new edges being created constantly.",
    "crumbs": [
      "Networks",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-gcn",
    "href": "qmd/networks-analysis.html#sec-net-anal-gcn",
    "title": "Analysis",
    "section": "Graph Convolutional Networks (GCN)",
    "text": "Graph Convolutional Networks (GCN)\n\nLearns representations of nodes by learning a function that aggregates a node’s neighborhood (the set of nodes connected to the original node), using both graph structure and node features.\n\nThese representations are a function of a node’s neighborhood and are not hardcoded per node (i.e. this is an inductive method), so changes in graph structure do not require re-training the model.\n\nFor unsupervised learning tasks, the method is similar to Node2vec/DeepWalk\nLayers\n\nA layer takes a weighted average of the node features in the original node’s neighborhood, and the weights are learned by training the network\nAdding layers produces aggregations that use more of the graph.\n\nThe span of the subgraph used to produce a node’s embedding is expanded by 1 hop.",
    "crumbs": [
      "Networks",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html",
    "href": "qmd/networks-knowledge-graphs.html",
    "title": "Knowledge Graphs",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Networks",
      "Knowledge Graphs"
    ]
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#sec-net-kg-misc",
    "href": "qmd/networks-knowledge-graphs.html#sec-net-kg-misc",
    "title": "Knowledge Graphs",
    "section": "",
    "text": "Notes from:\n\nWhat is a knowledge graph?\nHow to Convert Any Text Into a Graph of Concepts\n\nUse Cases\n\nCalculate the centralities for any node, to understand how important a concept (node) is to the body of work\nAnalyze connected and disconnected sets of concepts, or calculate communities of concepts for a deep understanding of the subject matter.\nUsed to implement Graph Retrieval Augmented Generation (GRAG or GAG). Can give much better results than RAG when querying an LLM about documents.\n\nRetrieving the context that is the most relevant for the query with a simple semantic similarity search is not always effective. Especially, when the query does not provide enough context about its true intent, or when the context is fragments across a large corpus of text.\n\nRetail: Knowledge graphs have been for up-sell and cross-sell strategies, recommending products based on individual purchase behavior and popular purchase trends across demographic groups.\nEntertainment: Knowledge graphs are also leveraged for artificial intelligence (AI) based recommendation engines for content platforms, like Netflix, SEO, or social media. Based on click and other online engagement behaviors, these providers recommend new content for users to read or watch.\nFinance: This technology has also been used for know-your-customer (KYC) and anti-money laundering initiatives within the finance industry. They assist in financial crime prevention and investigation, allowing banking institutions to understand the flow of money across their clientele and identify noncompliant customers.\nHealthcare: Knowledge graphs are also benefiting the healthcare industry by organizing and categorizing relationships within medical research. This information assists providers by validating diagnoses and identifying treatment plans based on individual needs.",
    "crumbs": [
      "Networks",
      "Knowledge Graphs"
    ]
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#sec-net-kg-terms",
    "href": "qmd/networks-knowledge-graphs.html#sec-net-kg-terms",
    "title": "Knowledge Graphs",
    "section": "Terms",
    "text": "Terms\nKnowledge Graph - Also known as a semantic network, represents a network of real-world entities — i.e. objects, events, situations, or concepts — and illustrates the relationship between them. Each node represents a concept and each edge is a relationship between a pair of such concepts. This information is usually stored in a graph database and visualized as a graph structure, prompting the term knowledge “graph.”",
    "crumbs": [
      "Networks",
      "Knowledge Graphs"
    ]
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#sec-net-kg-proc",
    "href": "qmd/networks-knowledge-graphs.html#sec-net-kg-proc",
    "title": "Knowledge Graphs",
    "section": "Process",
    "text": "Process\n\n\nCorpus Example:\nMary had a little lamb,\nYou’ve heard this tale before;\nBut did you know she passed her plate,\nAnd had a little more!\nSteps\n\nSplit the corpus of text into chunks. Assign a chunk_id to each of these chunks.\nFor every text chunk, extract concepts and their semantic relationships using a LLM. This relation is assigned a weight of W1. There can be multiple relationships between the same pair of concepts. Every such relation is an edge between a pair of concepts.\nConsider that the concepts that occur in the same text chunk are also related by their contextual proximity. This relation is assigned a weight of W2. Note that the same pair of concepts may occur in multiple chunks.\nGroup similar pairs, sum their weights, and concatenate their relationships. So now we have only one edge between any distinct pair of concepts. The edge has a certain weight and a list of relations as its name.\nPopulate nodes (concepts) and edges (relations) in a graph data structure or a graph database.\nVisualize",
    "crumbs": [
      "Networks",
      "Knowledge Graphs"
    ]
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#examples",
    "href": "qmd/networks-knowledge-graphs.html#examples",
    "title": "Knowledge Graphs",
    "section": "Examples",
    "text": "Examples",
    "crumbs": [
      "Networks",
      "Knowledge Graphs"
    ]
  },
  {
    "objectID": "qmd/nlp-fine-tuning.html",
    "href": "qmd/nlp-fine-tuning.html",
    "title": "Fine Tuning",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "NLP",
      "Fine Tuning"
    ]
  },
  {
    "objectID": "qmd/nlp-fine-tuning.html#sec-nlp-fintun-misc",
    "href": "qmd/nlp-fine-tuning.html#sec-nlp-fintun-misc",
    "title": "Fine Tuning",
    "section": "",
    "text": "Tuning an LLM\n\nNotes from:\n\nHacker’s Guide to Language Models (Howard)\n\nStages\n\nLM Pre-Training - Trained on a large corpus (e.g. much of the internet) to predict the next word in a sentence or to fill in a word in a sentence\nLM Fine-Tuning - Trained on a specific task (e.g. solve problems, answer questions). Instruction Tuning is often used. OpenOrca is an example of a Q&A dataset to train a LM to answer questions. Still predicting the next word, like in Pre-Training, but more target-based on a specific task.\nClassifier Fine-Tuning - Reinforcement Learning from Human Feedback (RLHF)is often used. The LLM being trained gives a few answers to a question and then a human or better LLM will pick which one is best.\n\nPre-Trained LLMs are ones that are typically the open source ones being released and available for download\n\nThey will need to be Fine-Tuned, but not necessarily Classifier Fine-Tuned. Often times, LM Fine-Tuning is enough.\n\n\nWorkflow Example (paper)\n\nRaschka - An introduction to the core ideas and approaches to Finetuning Large Language Models - by Sebastian Raschka\nWith ChatGPT, you can have it answer questions from context that contains thousands of documents.\n\n\nStore all these documents as small chunks of text (allowable size of the context window) in a database.\nCreate embeddings of documents and question\nThe documents of relevance can then be found by computing similarities between the question and the document chunks. This is done typically by converting the chunks and question into word embedding vectors, and computing cosine similarities between chunks and question, and finally choosing only those chunks above a certain cosine similarity as relevant context.\nFinally, the question and context can be combined into a prompt as below, and fed into an LLM API like ChatGPT: prompt=f\"Answer the question. Context: {context}\\n Question: {question}\"",
    "crumbs": [
      "NLP",
      "Fine Tuning"
    ]
  },
  {
    "objectID": "qmd/nlp-general.html",
    "href": "qmd/nlp-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "NLP",
      "General"
    ]
  },
  {
    "objectID": "qmd/nlp-general.html#sec-nlp-gen-misc",
    "href": "qmd/nlp-general.html#sec-nlp-gen-misc",
    "title": "General",
    "section": "",
    "text": "Also see\n\nFeature Engineering, Tokenization\nFeature Engineering, Embeddings\nDiagnostics, NLP\nEDA, Text\n\nUse cases for discovering hyper/hyponym relationships\n\nTaxonomy prediction: identifying broader categories for the terms, building taxonomy relations (like WikiData GraphAPI)\nInformation extraction (IE): automated retrieval of the specific information from text is highly reliable on relation to searched entities.\nDataset creation: advanced models need examples to be learned to identify the relationships between entities.\n\nBaseline Models\n\nRegularized Logistic Regression + Bag-of-Words (BoW) (recommended by Raschka)\n\nExample: py, kaggle notebook (sentiment analysis)\n\nUses Compressed Sparse Row (CSR) type of sparse matrix\nUses a time series cv folds and scores by precision\n\nPrecision because “Amazon would be more concerned about the products with negative reviews rather than positive reviews”\n\nSays random search &gt; grid search with regularized regression\nAlso fits a model with Tf-idf instead of BoW\n\nPreprocess\n\nTokenize\nRemove stopwords and punctuation\nStem\nunigrams and bigrams\nSparse token count matrix\nNormalize the matrix\n\nModel with regularized logistic regression\n\n\nGPT-4\n\nAccepts prompts of 25,000 words (GPT-3 accepted 1500-2000 words)\nAllegedly around 1T parameters (GPT-3 had 175B parameters)\nSome use cases: translation, q/a, text summaries, writing/getting news, creative writing\nMulti-modal training data (i.e. text and audio, pictures, etc.)\nStill hallucinates",
    "crumbs": [
      "NLP",
      "General"
    ]
  },
  {
    "objectID": "qmd/nlp-general.html#sec-nlp-gen-terms",
    "href": "qmd/nlp-general.html#sec-nlp-gen-terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nFlood Words - Words that are too common in the domain (i.e. noise)\nHypernym - A word with a broad meaning constituting a category into which words with more specific meanings fall\n\nExample: A device can use multiple storage units such as a hard drive or CD\n\nhyponym of storage units: hard drive, cd\nhypernym of hard drive/cd: storage units\n\n\nHyponym - Opposite of Hypernym; a word of more specific meaning than a general term applicable to it.\nNamed Entity Recognition (NER) - A subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n\n\nExample: An automated NER system will identify the incoming customer request (e.g. installation, maintenance, complaint, and troubleshoot of a particular product) and send it to the respective support desk\naka Named Entity Identification, Entity Chunking, and Entity Extraction\nOther use cases: filtering resumés, diagnose patients based on symptoms in healthcare data\n\nSequence to Sequence (aka String Transduction) - problems where the input and output is text\n\ne.g. Text summarization, Text simplification, Question answering, Chatbots, Machine translation\n\nSpam Words - Words that don’t belong in the domain (i.e. noise)",
    "crumbs": [
      "NLP",
      "General"
    ]
  },
  {
    "objectID": "qmd/nlp-general.html#sec-nlp-gen-hstpat",
    "href": "qmd/nlp-general.html#sec-nlp-gen-hstpat",
    "title": "General",
    "section": "Hearst Patterns",
    "text": "Hearst Patterns\n\n\nA set of test patterns that can be employed to extract Hypernyms and Hyponyms from text.\nIn the table, X is the hypernym and Y is the hyponym\n“rhyper” stands for reverse-hyper\nUsually, you don’t want to extract all possible hyponyms relations, but only entities in the specific domain\nPackages {{SpaCy}} - example",
    "crumbs": [
      "NLP",
      "General"
    ]
  },
  {
    "objectID": "qmd/nlp-general.html#sec-nlp-gen-uscas",
    "href": "qmd/nlp-general.html#sec-nlp-gen-uscas",
    "title": "General",
    "section": "Use Cases",
    "text": "Use Cases\n\nCreating labels for text that can later be used for supervised learning tasks like classification\nCreate metadata for columns in datasets (i.e. data dictionary)\n\nPredicting Metadata for Humanitarian Datasets Using GPT-3\n\nPrompt: Column name; sample of data from that column.\nCompletion: metadata which is a tag that includes column attributes.\nPotential Improvements\n\nTrying other models (‘ada’ used here) to see if this improves performance (though it will cost more)\nModel hyperparameter tuning. The log probability cutoff will likely be very important\nMore prompt engineering to perhaps include column list on the table might provide better context, we well as overlying columns on two-row header tables.\nMore preprocessing. Not much was done for this article, blindly taking tables extracted from CSV files, so the data is can be a bit messy\n\n\nUsing GPT-3.5-Turbo and GPT-4 for Predicting Humanitarian Data Categories\n\nGPT-4 resulted in 96% accuracy when predicting category and 89% accuracy when predicting both category and sub-category.\n\nGPT-3.5-turbo for the same prompts, with 96% accuracy versus 66% for category.\n\nLimitations exist due to the maximum number of tokens allowed in prompts affecting the amount of data that can be included in data excerpts, as well as performance and cost challenges — especially if you’re a small non-profit! — at this early stage of commercial generative AI.\nLikely related to being an early preview, GPT-4 model performance was very slow, taking 20 seconds per prompt to complete\n\n\nGeneralizations from OSINT (article)\n\nNote: most of these use a combination of the others to improve their overall performance — a document clustering model might use topic extraction and NER to improve the quality of their clusters.\nRecommendation Engines: Bespoke recommendation engines can be fine-tuned to recommend related documentation that may not be within the user’s immediate sphere of interest, but still relevant.\n\nEnables the discovery of “unknown unknowns”.\n\nTopic extraction and Document clustering: generate topics from multiple texts and detect similarities between documents publishing by dozens, sometimes hundred information feeds.\n\nYou don’t have the time to read every single document to get a higher view of the main problematics evolving within your multiple information feeds\n\nNamed (and unnamed) Entity Extraction and Disambiguation (NER / NED) (see Terms): identifying and categorizing named entities\n\nThe extraction part involves locating and tagging entities, while the disambiguation part involves determining the correct identity or meaning of an entity, especially when it can have multiple interpretations or references in a text.\nAllow you to build entire NLP logics to keep tracks of meaningful facts about this entity, order it by timeliness and relevance. This will allow you to start building bespoke, expert curated profiles.\n\nRelationship Extraction: identify the nature and type of relationships between different entities, such as individuals, organizations, and locations, and to represent them in a structured format that can be easily analyzed and interpreted.\n\nGenerating accurate connections between across thousands of documents will build expert driven, queryable knowledge graphs in a matter of days\n\nMulti-document abstractive summarization: automatically generated a concise and coherent summary of multiple documents on a given topic, by creating new sentences that capture the most important information from the original texts.\n\nEnable users to obtain a concise and coherent summary of the most important information from a large amount of text data.\n\n\nSearch Engine for Docs\n\nHow I Turned My Company’s Docs into a Searchable Database with OpenAI\nSteps\n\nConverted all of the docs to a unified format\n\nConverts all html files to markdown\n\nSplit docs into blocks and added some automated cleanup\n\nLinks to code for parsing markdown files to get text and code blocks\n\nComputed embeddings for each block\n\nUsed OpenAI’s text-embedding-ada-002 model which is cheap and provides good performance\n\nGenerated a vector index from these embedding\n\nUsed Qdrant for the vector embeddings storage\n\nDefined the index query\n\nAdded ablilities for the user to specify looking only in text or code and included other meta data in the output\n\nWrapped it all in a user-friendly command line interface and Python API\n\nPotential extensions of the project\n\nHybrid search: combine vector search with traditional keyword search\nGo global: Use Qdrant Cloud to store and query the collection in the cloud\nIncorporate web data: use requests to download HTML directly from the web\nAutomate updates: use Github Actions to trigger recomputation of embeddings whenever the underlying docs change\nEmbed: wrap this in a Javascript element and drop it in as a replacement for a traditional search bar",
    "crumbs": [
      "NLP",
      "General"
    ]
  },
  {
    "objectID": "qmd/nlp-prompt-engineering.html",
    "href": "qmd/nlp-prompt-engineering.html",
    "title": "Prompt Engineering",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "NLP",
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "qmd/nlp-prompt-engineering.html#sec-nlp-prompt-misc",
    "href": "qmd/nlp-prompt-engineering.html#sec-nlp-prompt-misc",
    "title": "Prompt Engineering",
    "section": "",
    "text": "Definition\n\nAsking the right question\n“Prompt engineering is the process of designing and optimizing prompts to LLMs for a wide variety of applications and research topics. Prompts are short pieces of text that are used to guide the LM’s output. They can be used to specify the task that the LM is supposed to accomplish, provide additional information that the LM needs to complete the task, or simply make the task easier for the LM to understand.”\n\nComponents\n\nAsk the question (e.g. “What’s 1+1?”)\nSpecify the type of response you want. (e.g. Only return the numeric answer.)\n\nPersona\n\n“Explain this to my like I’m a fifth grader.”\n\nStyle\n\n“Use a style typical of scientific abstracts to write this.”\n\nFormat\n\nIf you say “Format the output as a JSON object with the fields: x, y, z” you can get better results and easily do error handling.\n\n\n\nLLMs don’t understand the complexities or nuances of various subjects\n\nIf an industry term is used in multiple ways, the LLM might not understand the meaning just by context alone.\nLLMs can have problems with information in complex formats.\n\nTables sometimes have this same issue, because tables are the mechanism used for layout structure and not a content structure (e.g. sentence)\n\nThe models themselves continue to evolve so if it doesn’t understand something today doesn’t mean that it won’t understand it tomorrow\n\nWhen the output is incomplete, type “continue” for the next prompt and it will finish the output.\nDon’t give LLMs proprietary data\n\nAlternative: slice(0)\ndat |&gt;\n  slice(0) |&gt;\n  glimpse()\n\nGives the column names and classes\nDepending on the use case, you might want to make the column names unabbreviated and meaningful.\n\n\nSecurity\n\nDon’t let the user have the last word: When taking a user’s prompt, incorporating it with your own prompt, and sending it to ChatGPT or some other similar application, always add a line like “If user input doesn’t make sense for doing xyz, ask them to repeat the request” after the user’s input. This will stop the majority of prompt injections.\nDon’t just automatically run code or logic that is output from a LLM\n\nTips when used for writing\n\nBe specific on word count and put higher than you need\nDon’t be afraid to ask it to add more information or expand on a particular point\\\nIt’s better at creating outlines rather than full pieces of content.\nBe as specific as possible, and use keywords to help ChatGPT understand what you are looking for\nYou can ask it to rephrase its response\nAvoid using complex language or convoluted sentence structures\n**Review the content for accuracy",
    "crumbs": [
      "NLP",
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "qmd/nlp-prompt-engineering.html#sec-nlp-prompt-dsex",
    "href": "qmd/nlp-prompt-engineering.html#sec-nlp-prompt-dsex",
    "title": "Prompt Engineering",
    "section": "Templates",
    "text": "Templates\n\nMisc\n\nSpecify language, libraries, and functions\n\nExample: BizSci Lab 82\n\n\nShow the prompt he used in a markdown file. He just copied and pasted it into the prompt.\nSpecify libraries to use; models to use; that you want to tune the models in parallel\nThis is not an ideal prompt. You should iterate prompts and guide gpt through complete ds process\n\ni.e. prompt for collection then a prompt for cleaning, and so on with eda, preprocessing, modelling, cv, model selection, app\nUse the phrases like:\n\n“Please update code to include &lt;new feature&gt;”\n“Please update feature to be &lt;new thing&gt; instead of &lt;old thing&gt;”\n\n\n\nExample: Various DS Activities\n\nExample: Student Feedback\n\nFrom Now is the time for grimoires\nComponents\n\nRole: Tell the AI who it is. Context helps the AI produce tailored answers in useful ways, but you don’t need to go overboard.\nGoal: Tell the AI what you want it to do.\nStep-by-Step Instructions: Research has found that it often works best to give the AI explicit instructions that go step-by-step through what you want.\n\nOne approach, called Chain of Thought prompting, gives the AI an example of how you want it to reason before you make your request\nYou can also give it step-by-step directions the way we do in our prompts.\n\nConsider Examples: Few-shot prompting, where you give the AI examples of the kinds of output you want to see, has also proven very effective in research.\nAdd Personalization: Ask the user for information to help tailor the prompt for them.\nAdd Your Own Constraints: The AI often acts in ways that you may not want. Constraints tell it to avoid behaviors that may come up in your testing.\nFinal Step: Check your prompt by trying it out, giving it good, bad, and neutral input. Take the perspective of your users– is the AI helpful? Does the process work? How might the AI be more helpful? Does it need more context? Does it need further constraints? You can continue to tweak the prompt until it works for you and until you feel it will work for your audience.\n\nPrompt",
    "crumbs": [
      "NLP",
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "qmd/nlp-sentiment.html",
    "href": "qmd/nlp-sentiment.html",
    "title": "Sentiment",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "NLP",
      "Sentiment"
    ]
  },
  {
    "objectID": "qmd/nlp-sentiment.html#sec-nlp-sent-misc",
    "href": "qmd/nlp-sentiment.html#sec-nlp-sent-misc",
    "title": "Sentiment",
    "section": "",
    "text": "Lexicons\n\nFinance: Loughran-McDonald\nECB conferences: Picault-Renault",
    "crumbs": [
      "NLP",
      "Sentiment"
    ]
  },
  {
    "objectID": "qmd/nlp-sentiment.html#sec-nlp-sent-asba",
    "href": "qmd/nlp-sentiment.html#sec-nlp-sent-asba",
    "title": "Sentiment",
    "section": "Aspect-based Sentiment Analysis (ASBA)",
    "text": "Aspect-based Sentiment Analysis (ASBA)\n\nNotes from: NLP Project With Augmentation, Attacks, & Aspect-Based Sentiment Analysis\nModel\n\nIdentify observations from the dataset that are relevant to our aspect.\n\nExample: Analyze consumer sentiment specifically related to the color of dresses\n\nColor is the aspect\n\nOptions\n\nFilter text column to rows that only contain the aspect key word (e.g. color, colors)\n\nregex?\n\nSometimes with more abstract aspects, such as Experience, Service, or Location, you may need to leverage topic modeling to predict which aspect is most relevant to a text.\n\n\nTokenize text into smaller pieces\n\nSome simple options would be to split text by punctuation (e.g. commas) and/or conjunctions\n\nCalculate the Polarity (aka sentiment) relating to the aspect\n\nApply a pre-trained sentiment classifier model\n\nModels: TextBlob\n\nPolarity range: -1 to 1\n\nExtract the descriptors associated with our aspect\n\nDescriptors help explain the “why” behind the sentiment.\n\ne.g. the customer comment had a positive sentiment about color because they thought it was beautiful. “Beautiful” is the descriptor.\n\nOptions\n\nspaCy’s token classification features to automatically analyze the linguistic structure of the sentence and extract what adjectives/adverbs are associated with our noun\n\n\n\nAnalysis\n\nPolarity histogram related to aspect\n\n\nInterpretation\n\nCustomers typically respond positively to dress colors.\nTherefore, color is not a primary contributor to Not Recommended reviews\n\n\nDescriptors\n\n\nInterpretation\n\nCustomers love products with bright, vivid, and vibrant colors.\nCustomers tend to complain about a product’s color when it appears darker in person than on online pictures or is too muted or dull\nCompany should focus on using bright, vibrant colorways and materials",
    "crumbs": [
      "NLP",
      "Sentiment"
    ]
  },
  {
    "objectID": "qmd/nlp-topic.html",
    "href": "qmd/nlp-topic.html",
    "title": "Topic",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "NLP",
      "Topic"
    ]
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-misc",
    "href": "qmd/nlp-topic.html#sec-nlp-top-misc",
    "title": "Topic",
    "section": "",
    "text": "Unsupervised Machine Learning problem\n\nTakes a set of documents (far left), and returns a set of k topics (middle, e.g. Magic, Evil, Travel, Creatures) that summarize the documents (far right)\n\n“Wand” has the highest probability in the Magic topic\nTopic probablilities can be used to classify documents\n\nTopics don’t come with titles like are shown in figure. Researchers need to determine these topic categories on their own\n\nExample: 2020 Election Tweets\n\nEach row is a topic\nTopics could be: Gun Violence (row 1), Political Parties (row 2), a general Twitter topic (row 3), Covid-19 (row 4), Pro-Biden Phrases and Hashtags (row 5), Democratic Voters (row 6), and Pro-Trump Phrases and Hashtags (row 7)\n\n\n\nUse Cases\n\nAnnotation - ‘automatically’ label, or annotate, unstructured text documents based on the major themes that run through them\n\nWith labels, supervised ML models can now be used\n\neDiscovery - Legal discovery involves searching through all the documents relevant for a legal matter, and in some cases the volume of documents to be searched is very large. A 100% search of the documents isn’t always viable, so it’s easy to miss out on relevant facts. (article)\nContent recommendation - The NYT uses topic modeling in two ways—firstly to identify topics in articles and secondly to identify topic preferences amongst readers. The two are then compared to find the best match for a reader.\n\nIncoherent topics in this context means difficult for humans to interpret",
    "crumbs": [
      "NLP",
      "Topic"
    ]
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-conc",
    "href": "qmd/nlp-topic.html#sec-nlp-top-conc",
    "title": "Topic",
    "section": "Concepts",
    "text": "Concepts\n\nMisc\n\nNotes from\n\nTopic Modeling with LSA, pLSA, LDA, NMF, BERTopic, Top2Vec: a Comparison\n\nAt the end of the article\n\nTable that compares algorithms on a number of categories (too large to put in this note)\nGreat discussion on topic representation: human vs model\n\nUse Cases\n\nFind trending topics in Tweets with little pre-processing effort –&gt; BERTopic/Top2Vec better\nFinding how a given document may contain a mixture of multiple topics –&gt; LDA/NMF better\n\nSee BERTopic/Top2Vec &gt;&gt; Cons &gt;&gt; Single Topic for potential hack though\n\n\n\n\n\nLatent Semantic Analysis (LSA) (Deerwester¹ et al. 1990), probabilistic Latent Semantic Analysis (pLSA) (Hofmann², 1999), Latent Dirichlet Allocation (LDA) (Blei³ et al., 2003) and Non-Negative Matrix Factorization (Lee³ et al., 1999)\n\nRepresent a document as a bag-of-words and assume that each document is a mixture of latent topics\nDocument-Term Matrix (DTM) aka Term-Document Matrix (TDM) created\n\nEach cell (i, j) contains a count, i.e. how many times the word j appears in document i",
    "crumbs": [
      "NLP",
      "Topic"
    ]
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-lsa",
    "href": "qmd/nlp-topic.html#sec-nlp-top-lsa",
    "title": "Topic",
    "section": "Latent Semantic Analysis (LSA)",
    "text": "Latent Semantic Analysis (LSA)\n\nDescription\n\nTopics corresponding to largest singular values from SVD are used to summarize the corpus\n\nProcess\n\nTruncated SVD applied to the Document-Term Matrix (DTM) (See above) \n\nDecomposed into the product of three distinct matrices: DTM = U ∙ Σ ∙ Vᵗ\n\nU and V are of size m x m and n x n respectively\n\nm the number of documents in the corpus\nn the number of words in the corpus.\n\nΣ (Topic Importance) is m x n and only its main diagonal is populated: it contains the singular values of the DTM.\n\n\n\nResults\n\nFirst t  largest singular values selected from Σ\n\nt ≤ min(m, n)\n\nThe topics that correspond to the selected singular values are considered representative of the corpus\nTopics are open to human interpretation through the V matrix\n\nIssues\n\nThe DTM disregards the semantic representation of words in a corpus. Similar concepts are treated as different matrix elements. Pre-processing techniques may help, but only to some extent. For example, stemming may help in treating “Italy” and “Italian” as similar terms (as they should be), but close words with a different stem like “money” and “cash” would still be considered as different. Moreover, stemming may also lead to less interpretable topics.\nLSA requires an extensive pre-processing phase to obtain a significant representation from the textual input data.\nThe number of singular values t (topics) to maintain in the truncated SVD must be known a priori.\nA property of SVD is that the basis vectors are orthogonal to each other, forcing some elements in the bases to be negative. Therefore, U and V may contain negative values. This poses a problem for interpretability. We don’t know how a component contributes to the whole.",
    "crumbs": [
      "NLP",
      "Topic"
    ]
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-plsa",
    "href": "qmd/nlp-topic.html#sec-nlp-top-plsa",
    "title": "Topic",
    "section": "Probabilistic Latent Semantic Analysis (pLSA)",
    "text": "Probabilistic Latent Semantic Analysis (pLSA)\n\nModels the joint probability \\(P(d, w)\\) of seeing a word, \\(w\\), and a document, \\(d\\), together as a mixture of conditionally independent multinomial distributions\n\\[\n\\begin{align}\nP(d,w) &= P(d)P(w|d) \\\\\nP(w|d) &= \\sum_{z \\in Z} P(w|Z)P(z|d)\n\\end{align}\n\\]\n\n\\(w\\) indicates a word.\n\\(d\\) indicates a document.\n\\(z\\) indicates a topic.\n\\(P(z|d)\\) is the probability of topic z being present in a document d.\n\\(P(w|z)\\) is the probability of word w being present in a topic z.\nWe assume \\(P(w|z, d) = P(w|z)\\).\n\nModel equations (above) equivalent to (See LSA algorithm above)\n\\[\nP(d,w) = \\sum_{z\\in Z} P(z)P(d|z)P(w|z)\n\\]\n\n\\(P(d, w)\\) corresponds to the DTM.\n\\(P(z)\\) is analogous for the main diagonal of \\(\\Sigma\\).\n\\(P(d|z)\\) and \\(P(w|z)\\) correspond to \\(U\\) and \\(V\\), respectively.\n\nModel can be fit using the expectation-maximization algorithm (EM)\n\nEM performs maximum likelihood estimation in the presence of latent variables (in this case, the topics)\n\nIssues\n\nThe number of parameters grows linearly with the number of documents, leading to problems with scalability and overfitting.\nIt cannot assign probabilities to new documents.",
    "crumbs": [
      "NLP",
      "Topic"
    ]
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-lda",
    "href": "qmd/nlp-topic.html#sec-nlp-top-lda",
    "title": "Topic",
    "section": "Latent Dirichlet Allocation (LDA)",
    "text": "Latent Dirichlet Allocation (LDA)\n\nUses Dirichlet priors to estimate the document-topic and term-topic distributions in a Bayesian approach\nModel (Plate Notation)\n\n\n\\(\\alpha\\) - The parameter of the Dirichlet prior on the per-document topic distributions\n\\(\\theta\\) - Topic distribution for a document\n\\(z\\) - Topic\n\\(w\\) - Word\nSubscripts\n\n\\(M\\) - Indicates the number of documents\n\\(N\\) - The number of words in a document\n\\(d\\) - Cocument\n\nTop to Bottom\n\nFrom the Dirichlet distribution with parameter \\(\\alpha\\), we draw a random sample representing the topic distribution \\(\\theta\\) for a document/article/corpus.\n\nExample: Topic distribution for an article the politics section of a newspaper\n\nA mixture (0.99 politics, 0.05 sports, 0.05 arts) describing the distribution of topics for an article\n\n\nFrom the selected mixture \\(\\theta\\), we draw a topic \\(z_i\\) based on the distribution (in our example, politics).\n\nBottom to Top\n\nFrom the Dirichlet distribution with parameter \\(\\beta\\), we draw a random sample representing the word distribution \\(\\phi\\) for the per-topic word distribution given the topic \\(z_i\\).\nFrom selected mixture φ, we draw a word \\(w_i\\) based on the distribution.\n\n\nProcess\n\nEstimate the probability of a topic \\(z\\) given a document \\(d\\) and the parameters \\(alpha\\) and \\(\\beta\\), i.e. \\(P(z|d, \\alpha, \\beta)\\)\n\nFind parameters by minimizing the Kullback-Leibler divergence between the approximate distribution and the true posterior \\(P(\\theta, z|d, \\alpha, \\beta)\\)\n\\[\np(\\theta,z|d,\\alpha,\\beta) = \\frac{p(\\theta,z, d|\\alpha,\\beta)}{p(d|\\alpha,\\beta)}\n\\]\nCompute \\(P(z|d, \\alpha, \\beta)\\), which, in a sense, corresponds to the document-topic matrix U. Each entry of \\(\\beta_1, \\beta_2, \\ldots, \\beta_t\\) is \\(p(w|z)\\), which corresponds to the term-topic matrix \\(V\\)\n\n\\(U\\) and \\(V\\) are matrices from SVD in the LSA section. See above for details.\n\n\n\nPros:\n\nIt provides better performances than LSA and pLSA.\nUnlike pLSA, LDA can assign a probability to a new document thanks to the document-topic Dirichlet distribution.\nIt can be applied to both short and long documents.\nTopics are open to human interpretation.\nAs a probabilistic module, LDA can be embedded in more complex models or extended. Cons:\nThe number of topics must be known beforehand.\nThe bag-of-words approach disregards the semantic representation of words in a corpus, similarly to LSA and pLSA.\nThe estimation of Bayes parameters α and 𝛽 lies under the assumption of exchangeability for the documents.\nIt requires an extensive pre-processing phase to obtain a significant representation from the textual input data.\nStudies report LDA may yield too general (Rizvi et al., 2019) or irrelevant (Alnusyan et al., 2020) topics. Results may also be inconsistent across different executions (Egger et al., 2021).",
    "crumbs": [
      "NLP",
      "Topic"
    ]
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-nmf",
    "href": "qmd/nlp-topic.html#sec-nlp-top-nmf",
    "title": "Topic",
    "section": "Non-Negative Matrix Factorization (NMF)",
    "text": "Non-Negative Matrix Factorization (NMF)\n\nNMF is the same as SVD used in LSA except that it applies the additional constraints that U and Vᵗ can only contain non-negative elements.\n\nDecomposition has no sigma matrix, just U ∙ Vᵗ\n\nMisc\n\nAlso see\n\nAlgorithms, Recommendation &gt;&gt; Collaboritive Filtering &gt;&gt; Non-Negative Matrix Factorization (NMF)\n\nDeep dive into the algorithm, packages, example, etc.\n\n\n\nOptimize problem by minimizing the difference between the DTM and its approximation.\n\nFrequently adopted distance measures are the Frobenius norm and the Kullback-Leibler divergence\n\nPros:\n\nLiterature argues the superiority of NMF compared to SVD (hence LSA) in producing more interpretable and coherent topics (Lee et al. 1999, Xu et al. 2003; Casalino et al. 2016).\n\nCons:\n\nThe non-negativity constraints make the decomposition more difficult and may lead to inaccurate topics.\nNMF is a non-convex problem. Different U and Vᵗ may approximate the DTM, leading to potentially inconsistent outcomes for different runs.",
    "crumbs": [
      "NLP",
      "Topic"
    ]
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-bertttv",
    "href": "qmd/nlp-topic.html#sec-nlp-top-bertttv",
    "title": "Topic",
    "section": "BERTopic and Top2Vec",
    "text": "BERTopic and Top2Vec\n\nCreates semantic embeddings from input documents\nBERTopic\n\nUsed BERT Sentence Transformers (SBERT) to manufacture high-quality, contextual word and sentence vector representations.\nCurrently has a broader coverage of embedding models (Oct 2022)\n\n\nTop2Vec\n\nUsed Doc2Vec to create jointly embedded word, document, and topic vectors.\n\nClustering the Embeddings\n\nBoth original papers used HDBSCAN\nBERTopic supports K-Means and agglomerative clustering algorithms\nK-Means allows to select the desired number of clusters and forces every document into a cluster. This avoids the generation of outliers, but may also result in poorer topic representation and coherence.\n\nTopic Representation\n\nBERTopic - concatenates all documents within the same cluster (topic) and applies a modified TF-IDF (class-based TF-IDF or cTF-IDF)\n\nThe “modification” is that documents are replaced with clusters in the original TF-IDF formula\ncTF-IDF estimates the importance of words in clusters instead of documents.\n\nTop2Vec - manufactures a representation with the words closest to the cluster’s centroid\n\nFor each dense area obtained through HDBSCAN, it calculates the centroid of document vectors in original dimension, then selects the most proximal word vectors\n\n\nPros\n\nThe number of topics is not necessarily given beforehand.\n\nBoth BERTopic and Top2Vec support for hierarchical topic reduction to optimize the number of topics.\n\nHigh-quality embeddings take into account the semantic relationship between words in a corpus, unlike the bag-of-words approach. This leads to better and more informative topics.\nDue to the semantic nature of embeddings, textual pre-processing (stemming, lemmization, stopwords removal, …) is not needed in most cases.\nBERTopic supports dynamic topic modeling.\nModularity. Each step (document embedding, dimensionality reduction, clustering) is virtually self-consistent and can change or evolve depending on the advancements in the field, the peculiarities of a specific project or technical constraints.\n\nexample: use BERTopic with Doc2Vec embeddings instead of SBERT, or apply K-Means clustering instead of HDBSCAN.\n\nThey scale better with larger corpora compared with conventional approaches.\nBoth BERTopic and Top2Vec provide advanced built-in search and visualization capabilities. They make simpler to investigate the quality of the topics and drive further optimization, as well as producing high-quality charts for presentations.\n\nCons\n\nThey work better on shorter text, such as social media posts or news headlines.\n\nMost transformers-based embeddings have a limit on the number of tokens they can consider when they build a semantic representation. It is possible to use these algorithms with longer documents. One may, for example, split the documents in sentences or paragraphs before the embedding step. Nevertheless, this may not necessarily benefit the generation of meaningful and representative topics for longer documents.\n\nEach document is assigned to one topic only.\n\nTraditional approaches like LDA, instead, were built on the assumption that each document contains a mixture of topics.\nThe probability distribution of the HDBSCAN may be used as proxy of the topics distribution\n\nThey are slower compared to conventional models (Grootendorst, 2022).\n\nFaster training and inference may require more expensive hardware accelerators (GPU).\n\nAlthough BERTopic leverages transformers-based large language models to manufacture document embeddings, the topic representation still uses a bag-of-word approach (c TF-IDF).\nThey may be less effective for small datasets (&lt;1000 docs) (Egger et al., 2022).",
    "crumbs": [
      "NLP",
      "Topic"
    ]
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-tnm",
    "href": "qmd/nlp-topic.html#sec-nlp-top-tnm",
    "title": "Topic",
    "section": "Topic-Noise Models",
    "text": "Topic-Noise Models\n\nNotes from\n\nPaper\nAn Introduction to Topic-Noise Models\n\nGenerate more coherent, interpretable topics than traditional topic models when using text from noisy domains like social media\n\nTopic models are pretty good at identifying topics in traditional documents like books, newspaper articles, and research papers, because they have less noise\nExample: Domain = Covid-19 pandemic\n\n\nUses a fake dataset with Harry Potter characters tweeting about the pandemic\nRed words are noise.\n\nVariations of “Covid” are repeated in each topic\nPotter words not relevant to the domain\n\n\n\n*Recommended to ensemble these models with traditional topic models to produce the best results*\n\nTND topics are not always as intuitive as those generated using other topics models\n\nMisc\n\n{{gdtm}}\nIt is not unusual with noisy data that the model focuses on unigrams due to the issue of sparsity in short documents. Two word phrases tend to appear less frequently in a document collection than single words. That means that when you have a topic model with unigrams and ngrams, the ngrams will naturally not rise to the top of the topic word list\nBecause social media documents are much shorter than traditional texts, they rarely contain all of the topics in the topic set\nBest results on data sets of tens or hundreds of thousands of tweets or other social media posts\n\nThe training of the noise distribution is accomplished using a randomized algorithm. With smaller data sets, TND is not always able to get an accurate noise distribution\n\n\nTopic-Noise Discriminator (TND)\n\nJointly approximates the topic and noise distributions on a data set to allow for more accurate noise removal and more coherent topics\nOriginal topic-noise model (Churchill and Singh, 2021)\nHas been empirically shown that TND works best when combined with more traditional topic models\nAssigns each word in each document to either a topic or noise distribution, based on the word’s prior probability of being in each distribution\nProcess:\n\nFor each document \\(d\\) in \\(D\\):\n\nProbabilistically choose a topic \\(z\\)ᵢ from the topic distribution of \\(d\\)\nFor each word \\(w\\) in \\(d\\), assign \\(w\\) to \\(z\\)ᵢ or the noise distribution \\(H\\), based on the probabilities of \\(w\\) in \\(z\\)ᵢ and \\(H\\).\nRe-approximate the topic distribution of \\(d\\) given the new topic-word assignments\n\nRepeat the above for X iterations (usually 500 or 1,000 in practice)\n\nOver a large number of iterations, each word will have a probability of being a topic word and a probability of being a noise word.\n\n\nHyperparameters:\n\n\\(k\\) - the number of topics in the topic set\n\\(\\alpha\\) - how many topics there are per document\n\\(\\beta_0\\) - how many topics a word can be in\n\\(\\beta_1\\) - the skew of a word towards a topic (and away from the noise distribution)\n\nHigh \\(\\beta_1\\) \\(\\rightarrow\\) the more likely any given word will be a topic word instead of a noise word\nLike placing an extra weight on the Topic Frequency side of the scales\n\nRecommendations for domain-specific social media data:\n\n\\(k ≥ 20\\)\n\nTwitter: \\(k = 30\\)\n\n\\(9 \\geq \\beta ≤ 25\\).\n\nTwitter: \\(\\beta_1 = 25\\)\nReddit: \\(\\beta_1 = 9\\) or \\(16\\) or \\(16\\) for Reddit comments.\nIncrease when data sets are noisier to try to keep more words in topics that truly belong there.\n\n\n\n\nNoiseless Latent Dirichlet Allocation (NLDA)\n\nTND provides accurate noise removal while maintaining the same topic quality and performance that people expect from state-of-the-art topic models used on more traditional document collections.\nEnsemble of TND model and a LDA model\n\nThe noise distribution of TND and the topic distribution of LDA are combined to create more coherent, interpretable topics than we would get with either TND or LDA alone\n\nA scaling parameter \\(\\phi\\) allows the noise and topic distributions to be compared even if they are not generated using the same number of topics, \\(k\\), by scaling the underlying frequencies to relatable values.",
    "crumbs": [
      "NLP",
      "Topic"
    ]
  },
  {
    "objectID": "qmd/optimization-equation-systems.html",
    "href": "qmd/optimization-equation-systems.html",
    "title": "Optimization",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/optimization-equation-systems.html#sec-opt-misc",
    "href": "qmd/optimization-equation-systems.html#sec-opt-misc",
    "title": "Optimization",
    "section": "",
    "text": "Packages\n\nCRAN Task View",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/optimization-equation-systems.html#sec-opt-solvers",
    "href": "qmd/optimization-equation-systems.html#sec-opt-solvers",
    "title": "Optimization",
    "section": "Solvers",
    "text": "Solvers\n\nBase R\n\nsolve - The default method is an interface to the LAPACK routines DGESV and ZGESV. LAPACK is from https://netlib.org/lapack/.\nExample\n\\[\n\\begin{align}\n3x + 2y - z &= 7\\\\\nx - y + 2z &= -1\\\\\n2x + 3y + 4z &= 12\n\\end{align}\n\\]\na &lt;- \n  matrix(c(3, 2, -1, 1, -1, 2, 2, 3, 4),\n         nrow = 3, \n         byrow = TRUE)\nb &lt;- c(7, -1, 12)\nsolve(a, b)\n#&gt; [1] 0.6571429 2.8000000 0.5714286\n\nOpen Source\n\nSee packages in bookmarks\nCBC\nGoogle OR Tools\n\nCommercial\n\nXPress, CPLEX\nGurobi\n\nFor a DoorDash’s vehicle routing problem, Gurobi was 34 times faster on average than CBC\nPros\n\nScalability of the solvers, ease of abstracting feasible solutions when optimality is hard, ability to tune for different formations, relatively easy API integration to Python and Scala, flexibility of the prototype and deployment licensing terms from the vendors, and professional support.",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/optimization-equation-systems.html#sec-opt-bmp",
    "href": "qmd/optimization-equation-systems.html#sec-opt-bmp",
    "title": "Optimization",
    "section": "Bipartite Matching Problem",
    "text": "Bipartite Matching Problem\n\nA bipartite graph has 2 sets of vertices (e.g. Dashers and Merchants) and the edges (ETA to merchants) only travel between boths sets (Dashers and Merchants) and not within the sets (e.g. Dasher to Dasher).\nSolution: Hungarian Algorithm and a lecture (15 min)\nExample: Optimally assign Dashers to Merchants\n\n\nShows DoorDash delivery persons (Dashers) with ETA times to merchants, estimated food pick-up times from merchants, estimated customer drop-off times\n\nIssues:\n\nRuntime of large instances is excessive for a real-time dynamic system (polynomial time)\nDoesn’t support more complicated routes with 2 or more tasks\n\nExample shows 5 merchants and only 4 Dashers, so at least one Dasher needs to travel to 2 Merchants",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/optimization-equation-systems.html#sec-opt-vrp",
    "href": "qmd/optimization-equation-systems.html#sec-opt-vrp",
    "title": "Optimization",
    "section": "Vehicle Routing Problem",
    "text": "Vehicle Routing Problem\n\nhttps://en.wikipedia.org/wiki/Vehicle_routing_problem\n\nAllows multiple deliveries in a route\n\nExample: Optimizing Deliverers (Dashers) with food orders (Merchant/Customers) (See example in bipartite matching section)\n \nCan be solved using Mixed Integer Programming (MIP)\n\n{ompr}\nAlso see Logistics &gt;&gt; Case Study: DoorDash &gt;&gt; Optimization",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/optimization-equation-systems.html#sec-opt-ba",
    "href": "qmd/optimization-equation-systems.html#sec-opt-ba",
    "title": "Optimization",
    "section": "Budget Allocation",
    "text": "Budget Allocation\n\nNotes from Automate Budget Planning Using Linear Programming\nAlso see Project Management &gt;&gt; Decision Models (details on calculations for a project budget application)\nMisc\n\nTop Management Guidelines are for constraints but there may be other management objectives that are part of the decision making process\n\nConsiderations\n\nReturn on investment (ROI) of each project after three years (€)\nTotal costs and budget limits per year (€/Year)\n\nTop management guidelines\n\nSustainable Development (CO2 Reduction)\nDigital Transformation (IoT, Automation and Analytics)\nOperational Excellence (Productivity, Quality and Continuous Improvement)\n\n\n\nProcess\n\nScenario: Budget Planning Process\n\nAs a Regional Director you need to allocate your budget on projects\n\n\n8 “Market Verticals”: Luxury, Cosmetics, Fashion, etc.\n\n\nBuild your Model\n\nExploratory Data Analysis\n\nAnalyze the budget applications received\n\n\nLinear Programming Model\n\nDecisions variables, objective function and constraints\nObjective Function: Maximize Total ROI\n\n\nDecision Variable: P is a boolean for whether project i is accepted or rejected\n\nConstraints\n\nBudget\n\n4.5 M€ that you split in three years (1.25M€, 1.5M€, 1.75M€)\n\n\nStrategic (Top Management Guidelines)\n\n\nOmin, Smin, and Dmin = 1 M€\n\nOther potential constraints\n\nMaximum budget allocation per country, market vertical, or warehouse\nBudget allocation target (95% of the budget should be allocated)\n\n\n\nInitial Solution: Maximum ROI\n\nWhat would be the results if you focus only on ROI maximization? (i.e. 0 constraints)\n\nMight need to apply budget constraints in this step.\n\n\nFinal Solution: Apply Budget and Strategic constraints to the model\n\nConclusion & Next Steps\n\nCreate an app or dashboard\n\nApp may involve the user being able to select different constraints or values of those constraints\nUploads budget applications (e.g. spreadsheets)\nVisuals for EDA\n\nBar or circular chart: count or percent of applications per Market Vertical\nBar: count of applications per Management Objective\n\nVisuals for results of optimization\n\nBar chart for each variation of constraints chosen: allocation per Management Objective",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html",
    "href": "qmd/post-hoc-analysis-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Post-Hoc Analysis",
      "General"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-misc",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-misc",
    "title": "General",
    "section": "",
    "text": "Also see Mathematics, Statistics &gt;&gt; Descriptive Statistics&gt;&gt; Understanding CI, SD, and SEM Bars\nNotes from\n\nhttps://www.andrewheiss.com/blog/2019/01/29/diff-means-half-dozen-ways/\n\nPackages\n\n{rstatix}\n{dabestr} for visualization\n\nVisualization for differences (Thread)",
    "crumbs": [
      "Post-Hoc Analysis",
      "General"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-eda",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-eda",
    "title": "General",
    "section": "EDA",
    "text": "EDA\n\nMisc\n\nIn code examples, movies_clean is the data; rating is the numeric; and genre (action vs comedy) is the group variable\n\n\n\nCharts\n\nBox, Histogram, Density for two groups (factor(genre))\npacman::p_load(ggplot2movies,ggplot2, ggridges, patchwork)\n\n# Make a custom theme\n# I'm using Asap Condensed; download from \n# https://fonts.google.com/specimen/Asap+Condensed\ntheme_fancy &lt;- function() {\n  theme_minimal(base_family = \"Asap Condensed\") +\n    theme(panel.grid.minor = element_blank())\n}\neda_boxplot &lt;- ggplot(movies_clean, aes(x = genre, y = rating, fill = genre)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"#0288b7\", \"#a90010\"), guide = FALSE) + \n  scale_y_continuous(breaks = seq(1, 10, 1)) +\n  labs(x = NULL, y = \"Rating\") +\n  theme_fancy()\neda_histogram &lt;- ggplot(movies_clean, aes(x = rating, fill = genre)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  scale_fill_manual(values = c(\"#0288b7\", \"#a90010\"), guide = FALSE) + \n  scale_x_continuous(breaks = seq(1, 10, 1)) +\n  labs(y = \"Count\", x = \"Rating\") +\n  facet_wrap(~ genre, nrow = 2) +\n  theme_fancy() +\n  theme(panel.grid.major.x = element_blank())\neda_ridges &lt;- ggplot(movies_clean, aes(x = rating, y = fct_rev(genre), fill = genre)) +\n  stat_density_ridges(quantile_lines = TRUE, quantiles = 2, scale = 3, color = \"white\") + \n  scale_fill_manual(values = c(\"#0288b7\", \"#a90010\"), guide = FALSE) + \n  scale_x_continuous(breaks = seq(0, 10, 2)) +\n  labs(x = \"Rating\", y = NULL,\n      subtitle = \"White line shows median rating\") +\n  theme_fancy()\n(eda_boxplot | eda_histogram) / \n    eda_ridges + \n  plot_annotation(title = \"Do comedies get higher ratings than action movies?\",\n                  subtitle = \"Sample of 400 movies from IMDB\",\n                  theme = theme(text = element_text(family = \"Asap Condensed\"),\n                                plot.title = element_text(face = \"bold\",\n                                                          size = rel(1.5))))\n\n\n\nTest for Equal Variances\n\nMisc\n\n“Glass and Hopkins (1996 p. 436) state that the Levene and B-F tests are”fatally flawed”; It isn’t clear how robust they are when there is significant differences in variances and unequal sample sizes. ”\n\nBartlett test: Check homogeneity of variances based on the mean\nbartlett.test(rating ~ genre, data = movies_clean)\nLevene test: Check homogeneity of variances based on the median, so it’s more robust to outliers\ncar::leveneTest(rating ~ genre, data = movies_clean)\n\nAlso {DescTools}\nOther tests are better\n\nFligner-Killeen test: Check homogeneity of variances based on the median, so it’s more robust to outliers\nfligner.test(rating ~ genre, data = movies_clean)\nBrown-Forsythe (B-F) Test (link)\n\nAttempts to correct for the skewness of the Levene Test transformation by using deviations from group medians.\n\nLess likely than the Levene test to incorrectly declare that the assumption of equal variances has been violated.\n\nThought to perform as well as or better than other available tests for equal variances\n\nonewaytests::bf.test(weight_loss ~ program, data = data)\n\np-value &lt; 0.05 means the difference in variances is statistically significant",
    "crumbs": [
      "Post-Hoc Analysis",
      "General"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-fdim",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-fdim",
    "title": "General",
    "section": "Frequentist Difference-in-Means",
    "text": "Frequentist Difference-in-Means\n\nT-test\n\nTest whether the difference between means is statistically different from 0\nDefault is for non-equal variances\nt_test_eq &lt;- t.test(rating ~ genre, data = movies_clean, var.equal = TRUE)\nt_test_eq_tidy &lt;- tidy(t_test_eq) %&gt;%\n  # Calculate difference in means, since t.test() doesn't actually do that\n  mutate(estimate_difference = estimate1 - estimate2) %&gt;%\n  # Rearrange columns\n  select(starts_with(\"estimate\"), everything())\nFor unequal variances, Welch’s T-Test:\n\nvar.equal = FALSE\nRecommended for large datasets\n\n\n\n\nHotelling’s T2\n\nMultivariate generalization of Welch’s T-Test\n{DescTools::HotellingsT2Test}\n{ICSNP}\n\n\n\nNon-Parametric Difference-in-Means Tests\n\nWilcoxon Rank Sum and Signed Rank: wilcox.test\n\n1 or 2 variables/“samples”\n2 variable aka “Mann-Whitney”\ncoin::wilcox_test\n\nfor exact, asymptotic and Monte Carlo conditional p-values, including in the presence of ties\n\n\nKruskal-Wallis: kruskal.test(rating ~ genre, data = movies_clean)\n\nMore than 2 variables/“samples”\n\n\n\n\nKolomogorov-Smirnov\n\nks.test\nCalculates the difference in cdf of each sample\n2 variables/“samples”\nFor mixed or discrete, see {KSgeneral}\n\n\n\nBootstrap\n\nAlso see Statistical Concepts &gt;&gt; Bootstrapping\nSteps\n\nCalculate a sample statistic, or δ: This is the main measure you care about: the difference in means, the average, the median, the proportion, the difference in proportions, the chi-squared value, etc.\nUse simulation to invent a world where δ is null: Simulate what the world would look like if there was no difference between two groups, or if there was no difference in proportions, or where the average value is a specific number.\nLook at δ in the null world: Put the sample statistic in the null world and see if it fits well.\nCalculate the probability that δ could exist in null world: This is the p-value, or the probability that you’d see a δ at least that high in a world where there’s no difference.\nDecide if δ is statistically significant: Choose some evidentiary standard or threshold (like 0.05) for deciding if there’s sufficient proof for rejecting the null world.\n\nStandard Method\nlibrary(infer)\n\n# Calculate the difference in means\ndiff_means &lt;- movies_clean %&gt;% \n  specify(rating ~ genre) %&gt;%\n  # Order here means we subtract comedy from action (Action - Comedy)\n  calculate(\"diff in means\", order = c(\"Action\", \"Comedy\"))\n\nboot_means &lt;- movies_clean %&gt;% \n  specify(rating ~ genre) %&gt;% \n  generate(reps = 1000, type = \"bootstrap\") %&gt;% \n  calculate(\"diff in means\", order = c(\"Action\", \"Comedy\"))\n\nboostrapped_confint &lt;- boot_means %&gt;% get_confidence_interval()\n\nboot_means %&gt;% \n  visualize() + \n  shade_confidence_interval(boostrapped_confint,\n                            color = \"#8bc5ed\", fill = \"#85d9d2\") +\n  geom_vline(xintercept = diff_means$stat, size = 1, color = \"#77002c\") +\n  labs(title = \"Bootstrapped distribution of differences in means\",\n      x = \"Action − Comedy\", y = \"Count\",\n      subtitle = \"Red line shows observed difference; shaded area shows 95% confidence interval\") +\n  theme_fancy()\nDowney’s Process: Generate a world where there’s no difference by shuffling all the action/comedy labels through permutation\n# Step 1: δ = diff_means (see above)\n\n# Step 2: Invent a world where δ is null\ngenre_diffs_null &lt;- movies_clean %&gt;% \n  specify(rating ~ genre) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 5000, type = \"permute\") %&gt;% \n  calculate(\"diff in means\", order = c(\"Action\", \"Comedy\"))\n\n# Step 3: Put actual observed δ in the null world and see if it fits\ngenre_diffs_null %&gt;% \n  visualize() + \n  geom_vline(xintercept = diff_means$stat, size = 1, color = \"#77002c\") +\n  scale_y_continuous(labels = comma) +\n  labs(x = \"Simulated difference in average ratings (Action − Comedy)\", y = \"Count\",\n      title = \"Simulation-based null distribution of differences in means\",\n      subtitle = \"Red line shows observed difference\") +\n  theme_fancy()\n\nIf line is outside null distribution, then the difference value doesn’t fit in a world where the null hypothesis is the truth\n\nGenerate a p-value\n# Step 4: Calculate probability that observed δ could exist in null world\ngenre_diffs_null %&gt;% \n  get_p_value(obs_stat = diff_means, direction = \"both\") %&gt;% \n  mutate(p_value_clean = pvalue(p_value))",
    "crumbs": [
      "Post-Hoc Analysis",
      "General"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-bdim",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-bdim",
    "title": "General",
    "section": "Bayesian Difference-in-Means",
    "text": "Bayesian Difference-in-Means\n\nMisc\n\nNotes from\n\nHalf a dozen frequentist and Bayesian ways to measure the difference in means in two groups | Andrew Heiss\n\nFrequentist null hypothesis significance testing (NHST) determines the probability of the data given a null hypothesis (i.e. \\(P(data|H)\\), yielding results that are often unwieldy, phrased as the probability of rejecting the null if it is true (hence all that talk of “null worlds”). In contrast, Bayesian analysis determines the probability of a hypothesis given the data (i.e.P(H|data)), resulting in probabilities that are directly interpretable.\n\n\n\nRegression (Equal Variances)\n\nWith {brms}\nbrms_eq &lt;- brm(\n  # bf() is an alias for brmsformula() and lets you specify model formulas\n  bf(rating ~ genre), \n  # Reverse the levels of genre so that comedy is the base case\n  data = mutate(movies_clean, genre = fct_rev(genre)),\n  prior = c(set_prior(\"normal(0, 5)\", class = \"Intercept\"),\n            set_prior(\"normal(0, 1)\", class = \"b\")),\n  chains = 4, iter = 2000, warmup = 1000, seed = 1234,\n  file = \"cache/brms_eq\"\n)\n# median of posterior and CIs\nbrms_eq_tidy &lt;- \n  broom::tidyMCMC(brms_eq, conf.int = TRUE, conf.level = 0.95, \n          estimate.method = \"median\", conf.method = \"HPDinterval\")\n\nfamily = gaussian (default)\nb_intercept: mean comedy score while the\nb_genreAction: difference from mean comedy score (i.e. difference in means)\n\nsSays “We’re 95% confident that the true population-level difference in rating is between -0.968 and -0.374, with a median of -0.666.”\n\n\n\n\n\nRegression (unequal variances)\n\nWith {brms}\nbrms_uneq &lt;- brm(\n  bf(rating ~ genre, sigma ~ genre), \n  data = mutate(movies_clean, genre = fct_rev(genre)),\n  prior = c(set_prior(\"normal(0, 5)\", class = \"Intercept\"),\n            set_prior(\"normal(0, 1)\", class = \"b\"),\n            # models the variance for each group (e.g. comedy and action)\n            set_prior(\"cauchy(0, 1)\", class = \"b\", dpar = \"sigma\")),\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  file = \"cache/brms_uneq\"\n)\n\n# median of posterior and CIs\nbrms_uneq_tidy &lt;- \n  tidyMCMC(brms_uneq, conf.int = TRUE, conf.level = 0.95, \n          estimate.method = \"median\", conf.method = \"HPDinterval\") %&gt;% \n  # sigma terms on log-scale so exponentiate them to get them back to original scale\n  mutate_at(vars(estimate, std.error, conf.low, conf.high),\n            funs(ifelse(str_detect(term, \"sigma\"), exp(.), .)))\n\nInterpretation for intercept and main effect estimates same as before\nb_sigma_intercept and b_sigma_genreAction are the std.devs for those posteriors\n\n\n\n\nBayesian Estimation Supersedes the T-test (BEST)\n\nUnequal Variances, student-t distribution\nSame as before but with a coefficient for ν, the degrees of freedom, for the student-t distribution.\nModels each group distribution (by removing intercept w/ 0 + formula notation), then calculates difference in means by hand\nbrms_uneq_robust_groups &lt;- brm(\n  bf(rating ~ 0 + genre,\n    sigma ~ 0 + genre), \n  family = student,\n  data = mutate(movies_clean, genre = fct_rev(genre)),\n  prior = c(\n    # Set group mean prior\n    set_prior(\"normal(6, 2)\", class = \"b\", lb = 1, ub = 10),\n    # Ser group variance priors. We keep the less informative cauchy(0, 1).\n    set_prior(\"cauchy(0, 1)\", class = \"b\", dpar = \"sigma\"),\n    set_prior(\"exponential(1.0/29)\", class = \"nu\")),\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  file = \"cache/brms_uneq_robust_groups\"\n)\n\nbrms_uneq_robust_groups_tidy &lt;- \n  tidyMCMC(brms_uneq_robust_groups, conf.int = TRUE, conf.level = 0.95, \n          estimate.method = \"median\", conf.method = \"HPDinterval\") %&gt;% \n  # Rescale sigmas\n  mutate_at(vars(estimate, std.error, conf.low, conf.high),\n            funs(ifelse(str_detect(term, \"sigma\"), exp(.), .))\n\nbrms_uneq_robust_groups_post &lt;- posterior_samples(brms_uneq_robust_groups) %&gt;% \n  # We can exponentiate here!\n  mutate_at(vars(contains(\"sigma\")), funs(exp)) %&gt;% \n  # For whatever reason, we need to log nu?\n  mutate(nu = log10(nu)) %&gt;% \n  mutate(diff_means = b_genreAction - b_genreComedy,\n        diff_sigma = b_sigma_genreAction - b_sigma_genreComedy) %&gt;% \n  # Calculate effect sizes, just for fun\n  mutate(cohen_d = diff_means / sqrt((b_sigma_genreAction + b_sigma_genreComedy)/2),\n        cles = dnorm(diff_means / sqrt((b_sigma_genreAction + b_sigma_genreComedy)), 0, 1))\n\nbrms_uneq_robust_groups_tidy_fixed &lt;- \n  tidyMCMC(brms_uneq_robust_groups_post, conf.int = TRUE, conf.level = 0.95, \n          estimate.method = \"median\", conf.method = \"HPDinterval\")\n## # A tibble: 9 x 5\n##  term                estimate std.error conf.low conf.high\n##  &lt;chr&gt;                  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n## 1 b_genreComedy        5.99      0.109    5.77      6.19 \n## 2 b_genreAction        5.30      0.107    5.09      5.50 \n## 3 b_sigma_genreComedy  1.47      0.0882    1.30      1.64 \n## 4 b_sigma_genreAction  1.47      0.0826    1.31      1.62 \n## 5 nu                  1.48      0.287    0.963    2.04 \n## 6 diff_means          -0.690      0.151    -1.01    -0.415\n## 7 diff_sigma          0.00100    0.111    -0.212    0.217\n## 8 cohen_d            -0.571      0.126    -0.818    -0.327\n## 9 cles                0.368      0.0132    0.341    0.391\nCohen’s d:  standardized difference in means (Also see Post-Hoc Analysis, Multilevel &gt;&gt; Cohen’s D)\n\n\nMedium effect size in the example above\nThe denominator in this calculation is the square root of the average std.dev, but it doesn’t look like any of the ones used in the wiki article\n\nLooks closer to the Strictly Standardized Mean Difference (SSMD) (wiki)\n\n\nCommon language effect size (CLES): Probability that a rating sampled at random from one group will be greater than a rating sampled from the other group.\n\n36.8% chance that we could randomly select an action rating from the comedy distribution",
    "crumbs": [
      "Post-Hoc Analysis",
      "General"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-dd",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-dd",
    "title": "General",
    "section": "Dichotomous Data",
    "text": "Dichotomous Data\n\nMean (probability-of-event) + CI Estimation\n\nLarge Population\n\\[\n\\hat p \\pm z_{\\alpha/2} \\sqrt {\\frac{\\hat p(1-\\hat p)}{n}}\n\\]\nbinom::binom.asymp(x=x, n=n, conf.level=0.95)\n##      method  x  n      mean      lower    upper\n## 1 asymptotic 52 420 0.1238095 0.09231031 0.1553087\nSmall/Finite Population\n\\[\n\\hat p \\pm z_{\\alpha/2} \\sqrt {\\frac{\\hat p(1-\\hat p)}{n} \\cdot \\frac{N-n}{N-1}}\n\\]\n\nSee ?binom::binom.confint for many methods\n“When the intracluster correlation coefficient is high and the prevalence, p, is less than 0.10 or greater than 0.90, the Agresti-Coull and Clopper-Pearson intervals perform best. In other settings, the Clopper-Pearson interval is unnecessarily wide. In general, the Logit, Wilson, Jeffreys, and Agresti-Coull intervals perform well, although the Logit interval may be intractable when the standard error is equal to zero.” (paper)\n\n1-Sample Proportion Test\n\nExample: Do 50% of infants start walking by 12 months of age?\n&gt; table(walkby12)\n\n#&gt; walkby12\n#&gt; 0 1\n#&gt; 14 36\n\nprop.test(36,50,p=0.5,correct=FALSE)\n\n#&gt; 1-sample proportions test without continuity correction\n#&gt; data: 36 out of 50, null probability 0.5\n#&gt; X-squared = 9.68, df = 1, p-value = 0.001863\n#&gt; alternative hypothesis: true p is not equal to 0.5\n#&gt; 95 percent confidence interval:\n#&gt; 0.5833488 0.8252583\n#&gt; sample estimates:\n#&gt; p\n#&gt; 0.72\n\np-value &lt; 0.05 therefore the null hypothesis of 50% of infants walking is rejected\ncorrect = FALSE says this is a large sample (See assumptions in difference of proportions &gt;&gt; Z-Test)\nCI for the population proportion estimate is given.\n\n\nBayesian\n# Mean proportion estimated with prior that mean lies between 0.05 and 0.15\n\n##Function to determine beta parameters s.t. the 2.5% and 97.5% quantile match the specified values\ntarget &lt;- function(theta, prior_interval, alpha=0.05) {\n  sum( (qbeta(c(alpha/2, 1-alpha/2), theta[1], theta[2]) - prior_interval)^2)\n}\n## Find the prior parameters\nprior_params &lt;- optim(c(10,10),target, prior_interval=c(0.05, 0.15))$par\n## [1]  12.04737 116.06022\n# not really sure how this works. Guessing theta1,2 is c(10,10) but then there doesn't seem to be an unknown to optimize for.\n\n## Compute credibile interval from a beta-binomial conjugate prior-posterior approach\nbinom::binom.bayes(x=x, n=n, type=\"central\", prior.shape1=prior_params[1], prior.shape2=prior_params[2])\n##  method  x  n  shape1  shape2      mean      lower    upper  sig\n## 1  bayes 52 420 64.04737 484.0602 0.1168518 0.09134069 0.1450096 0.05\n\n##Plot of the beta-posterior\np &lt;- binom::binom.bayes.densityplot(ci_bayes)\n##Add plot of the beta-prior\ndf &lt;- data.frame(x=seq(0,1,length=1000)) %&gt;% mutate(pdf=dbeta(x, prior_params[1], prior_params[2]))\np + geom_line(data=df, aes(x=x, y=pdf), col=\"darkgray\",lty=2) +\n  coord_cartesian(xlim=c(0,0.25)) + scale_x_continuous(labels=scales::percent)\n\n# Estimated with a flat prior (essentially equivalent to the frequentist approach)\nbinom::binom.bayes(x=x, n=n, type=\"central\", prior.shape1=1, prior.shape2=1))\n##  method  x  n shape1 shape2      mean      lower    upper  sig\n## 1  bayes 52 420    53    369 0.1255924 0.09574062 0.158803 0.05\n\nFrom https://staff.math.su.se/hoehle/blog/2017/06/22/interpretcis.html\nInterpretation\n\nTechnical: “95% equi-tailed credible interval resulting from a beta-binomial conjugate Bayesian approach obtained when using a prior beta with parameters such that the similar 95% equi-tailed prior credible interval has limits 0.05 and 0.15. Given these assumptions the interval 9.1%- 14.5% contains 95% of your subjective posterior density for the parameter.”\nNontechnical: the true value is in that interval with 95% probability or just this 95% Bayesian confidence interval is 9.1%- 14.5%.\n\n\n\n\n\nDifference in Proportions\n\nCochran-Mantel-Haenszel Test: This test is appropriate when you have data from multiple 2x2 tables (strata) and want to test the association between two categorical variables while controlling for the effects of a third variable (confounding variable).\n\nSee Discrete Analysis Notebook\n\nDo NOT use Fisher’s Exact Test.\n\nSeveral different p-values can be associated with a single table, making scientific inference inconsistent\nDespite the fact that Fisher’s test gives exact p-values, some authors have argued that it is conservative, i.e. that its actual rejection rate is below the nominal significance level. The issue has to do with Fisher’s test conditioning on the margin totals.\n\nLikelihood ratios, posterior probabilities and mid-p-values - lead to more consistent inferences. Recommendations from this paper:\n\nA Bayesian interval for the log odds ratio with Jeffreys’ reference prior\nConditional Likelihood Ratio Test\n\n{ProfileLikelihood}: LR.pvalue(y1, y2, n1, n2, interval=0.01)\n\n\nZ-Test\n\\[\n\\begin{align}\n&Z = \\frac{(\\hat p_1 - \\hat p_2)}{\\sqrt{\\hat p (1 - \\hat p) \\left(\\frac{1}{n_1}-\\frac{1}{n_2}\\right)}} \\\\\n&\\text{where} \\;\\; \\hat p = \\frac{Y_1 + Y_2}{n_1 + n_2}\n\\end{align}\n\\]\n\nThe z-test comparing two proportions is equivalent to the chi-square test of independence\nTerms\n\n\\(\\hat p_i\\) is the sample proportion\n\\(\\hat p\\) is the overall proportion\n\\(Y_i\\) is the sample count\n\\(n_i\\) is the sample size\n2-tail hypothesis test; If Z &gt; 1.96 or Z &lt; -1.96 (i.e. p-value &lt; 0.05), then the sample proportions are NOT equal.\n\nCheck Assumptions\n\nIndependent observations and sufficient sample sizes.\n\nFor each sample, i:\n\\[\n\\begin{align}\n&n_i \\cdot \\hat p_i &gt; 5 \\\\\n&n_i \\cdot (1-\\hat p_i) &gt; 5\n\\end{align}\n\\]\nThere is a “continuity correction” arg (correct = TRUE)that when set to TRUE can correctly compute the CIs for when proportions for each/one event are less than 5\n\n\nExample\nres &lt;- prop.test(x = c(490, 400), n = c(500, 500), correct = FALSE)\n\n#&gt; 2-sample test for equality of proportions with continuity correction\n\n#&gt; data:  c(490, 400) out of c(500, 500)\n#&gt; X-squared = 82.737, df = 1, p-value &lt; 2.2e-16\n#&gt; alternative hypothesis: two.sided\n#&gt; 95 percent confidence interval:\n#&gt;  0.1428536 0.2171464\n#&gt; sample estimates:\n#&gt; prop 1 prop 2 \n#&gt;   0.98   0.80 \n\np-value &lt; 0.05, so proportions are statistically different\nThe confidence interval given for the true proportion if there is one group, or for the difference in proportions if there are 2 groups and p argument isn’t provided\n2-sided test is default\nchisq.test() is exactly equivalent to prop.test() but it works with data in matrix form.\n\n\nMcNemar’s test\n\nFor comparing paired data of 2 groups\nMainly useful when the measurements are on the nominal or ordinal scale\nTests for significant difference in frequencies of paired samples when it has binary responses\n\nH0: There is no significant change in individuals after the treatment\nH1: There is a significant change in individuals after the treatment\n\n\n# data is unaggregated (i.e. paired measurements from individuals)\ntest &lt;- mcnemar.test(table(data$pretreatment, data$posttreatment))\n#&gt; McNemar's Chi-squared test with continuity correction\n#&gt; data: table(data$before, data$after)\n#&gt; McNemar's chi-squared = 0.5625, df = 1, p-value = 0.4533\n\ncorrect = TRUE (default) - Continuity correction (increases “usefulness and accuracy of the test” so probably better to leave it as TRUE)\nx & y are factor vectors\nx can be a matrix of aggregated counts\n\nIf the 1st row, 2nd cell or 2nd row, 1st cell have counts &lt; 50, then use the Exact Tests to get accurate p-values (For details see above for page in notebook)\n\nInterpretation: p-value is 0.45, above the 5% significance level and therefore the null hypothesis cannot be rejected",
    "crumbs": [
      "Post-Hoc Analysis",
      "General"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-catdat",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-catdat",
    "title": "General",
    "section": "Categorical Data",
    "text": "Categorical Data\n\nExample: Bayesian; 3 level Categorical Variable\n\nData and Model\n\nCalculate Differences\n\nVisualize (Code in previous chunk)",
    "crumbs": [
      "Post-Hoc Analysis",
      "General"
    ]
  },
  {
    "objectID": "qmd/production-data-validation.html",
    "href": "qmd/production-data-validation.html",
    "title": "Data Validation",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Production",
      "Data Validation"
    ]
  },
  {
    "objectID": "qmd/production-data-validation.html#sec-prod-datval-misc",
    "href": "qmd/production-data-validation.html#sec-prod-datval-misc",
    "title": "Data Validation",
    "section": "",
    "text": "Packages\n\n{pointblank} - This is a heavy-duty package that helps you validate all aspects of datatsets with extensive reporting capabilities.\n{{great_expectations}}\n{{pydantic}}\n\nAlso see DB, Engineering &gt;&gt; Data Quality",
    "crumbs": [
      "Production",
      "Data Validation"
    ]
  },
  {
    "objectID": "qmd/production-data-validation.html#sec-prod-datval-pipe",
    "href": "qmd/production-data-validation.html#sec-prod-datval-pipe",
    "title": "Data Validation",
    "section": "Pipeline",
    "text": "Pipeline\n\nCollection\n\ne.g. people working in your stores, your call centrrs or perhaps as online support agents. It could be your online sign-up forms or physical documents that your agents must manually input into your systems\nChecks\n\nCompleteness: The data being collected and captured is complete (not NULLs), i.e. all mandatory fields have information added, and it is not missing any key data points.\nUniqueness: The data is kept as unique as possible, i.e. if a client already has an account, another account is not being set up. If the mobile number already exists in the system, the current order is linked to the old order etc.\nValidity: The data being captured conforms to the corporate standards, i.e. account number is eight digits long and starts with a number 9 is conformed with at the time of capturing\n\n\nTransfer\n\nMake sure that the data sent is the same as the data received\nCheck\n\nConsistency: The data is consistent across all the tables with the same values. This could translate to well-reconciled data between source and target, i.e. 100 records sent, 100 records received. Or that the table has specific values like date of birth and is consistent with other tables that have the same or similar information. Orphaned records (exist in A and not in B) should be highlighted, monitored and remediated.\n\n\nStorage\n\nData spends most of its time here, so take advantage of the time that it’s not being used in a product.\nChecks\n\nCompleteness: Null reporting — how many columns are Null, and why are they Null? Can we change the data capture process to avoid these Nulls coming through?\nUniqueness: Are non-mandatory attributes unique? Are duplications going to impact downstream reporting?\n\n\nTransformation\n\nOften the place where most validation takes place.\nChecks\n\nTimeliness: Ensure data is available promptly to meet agreed SLAs.\nConsistency: Reconciliation checks from source to target\n\nExample: tolerance checks on tables processed; we generally receive 100 records, and we have received just two records today; how do we alert the user of this discrepancy?\n\nValidity: Non-conformance under the validity dimension could render the transformation and subsequent consumption useless. This is especially helpful when data capture doesn’t have robust controls.\n\n\nConsumption\n\nEnsure the business problem is solved\nChecks\n\nAccuracy: The data is accurate enough for reporting, such as board metrics. Account numbers are associated with the correct customer segments, or the date of birth is not the default value like 01/01/1901.\nTimeliness: It is not early that it excludes some recent records. It is not late that it misses the deadline for reporting. All agreed SLAs must be met to ensure the data consumption layer has the data available when required and stays fit for purpose.",
    "crumbs": [
      "Production",
      "Data Validation"
    ]
  },
  {
    "objectID": "qmd/production-data-validation.html#sec-prod-datval-py",
    "href": "qmd/production-data-validation.html#sec-prod-datval-py",
    "title": "Data Validation",
    "section": "Python",
    "text": "Python\n\nMisc\n\nComparison between {{pydantic}} and {{pandas_dq}}\n\nDeclarative syntax: arguably, Pydantic allows you to define the data schema and validation rules using a more concise and readable syntax. This can make it easier to understand and maintain your code. I find it super helpful to be able to define the ranges of possible values instead of merely the data type.\nBuilt-in validation functions: Pydantic provides various powerful built-in validation functions like conint, condecimal, and constr, which allow you to enforce constraints on your data without having to write custom validation functions.\nComprehensive error handling: When using Pydantic, if the input data does not conform to the defined schema, it raises a ValidationError with detailed information about the errors. This can help you easily identify issues with your data and take necessary action.\nSerialization and deserialization: Pydantic automatically handles serialization and deserialization of data, making it convenient to work with different data formats (like JSON) and convert between them.\n\n\nExample: {{pydantic}}\n\nSet schema and create sample data\n# data validation on the data dictionary\nfrom pydantic import BaseModel, Field, conint, condecimal, constr\nclass LoanApplication(BaseModel):\n    Loan_ID: int\n    Gender: conint(ge=1, le=2)\n    Married: conint(ge=0, le=1)\n    Dependents: conint(ge=0, le=3)\n    Graduate: conint(ge=0, le=1)\n    Self_Employed: conint(ge=0, le=1)\n    ApplicantIncome: condecimal(ge=0)\n    CoapplicantIncome: condecimal(ge=0)\n    LoanAmount: condecimal(ge=0)\n    Loan_Amount_Term: condecimal(ge=0)\n    Credit_History: conint(ge=0, le=1)\n    Property_Area: conint(ge=1, le=3)\n    Loan_Status: constr(regex=\"^[YN]$\")\n\n# Sample loan application data\nloan_application_data = {\n    \"Loan_ID\": 123456,\n    \"Gender\": 1,\n    \"Married\": 1,\n    \"Dependents\": 2,\n    \"Graduate\": 1,\n    \"Self_Employed\": 0,\n    \"ApplicantIncome\": 5000,\n    \"CoapplicantIncome\": 2000,\n    \"LoanAmount\": 100000,\n    \"Loan_Amount_Term\": 360,\n    \"Credit_History\": 1,\n    \"Property_Area\": 2,\n    \"Loan_Status\": \"Y\"\n}\n\n# Validate the sample data using the LoanApplication Pydantic model\nloan_application = LoanApplication(**loan_application_data)\nValidate and print report\n# data validation on the data dictionary\nfrom pydantic import ValidationError\nfrom typing import List\n\n# Function to validate DataFrame and return a list of failed LoanApplication objects\ndef validate_loan_applications(df: pd.DataFrame) -&gt; List[LoanApplication]:\n    failed_applications = []\n    for index, row in df.iterrows():\n        row_dict = row.to_dict()\n\n        try:\n            loan_application = LoanApplication(**row_dict)\n        except ValidationError as e:\n            print(f\"Validation failed for row {index}: {e}\")\n            failed_applications.append(row_dict)\n    return failed_applications\n\n# Validate the entire DataFrame\nfailed_applications = validate_loan_applications(df_loans.reset_index())\n\n# Print the failed loan applications or \"No data quality issues\"\nif not failed_applications:\n    print(\"No data validation issues\")\nelse:\n    for application in failed_applications:\n        print(f\"Failed application: [{application}]{style='color: #990000'}\")\n\nExample: {{pandas_dq}}\n\nCheck schema\n\nfrom pandas_dq import DataSchemaChecker\nschema = {\n    'Loan_ID': 'int64',\n    'Gender': 'int64',\n    'Married': 'int64',\n    'Dependents': 'int64',\n    'Graduate': 'int64',\n    'Self_Employed': 'int64',\n    'ApplicantIncome': 'float64',\n    'CoapplicantIncome': 'float64',\n    'LoanAmount': 'float64',\n    'Loan_Amount_Term': 'float64',\n    'Credit_History': 'int64',\n    'Property_Area': 'int64',\n    'Loan_Status': 'object'\n}\nchecker = DataSchemaChecker(schema)\nchecker.fit(df_loans.reset_index())\n\nShows 3 variables with incorrect types\n\nFix issues\n\ndf_fixed = checker.transform(df_loans.reset_index())\ndf_fixed.info()\n\nVariables have been cast into the correct types according to the schema",
    "crumbs": [
      "Production",
      "Data Validation"
    ]
  },
  {
    "objectID": "qmd/production-deployment.html",
    "href": "qmd/production-deployment.html",
    "title": "Deployment",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Production",
      "Deployment"
    ]
  },
  {
    "objectID": "qmd/production-deployment.html#sec-prod-deploy-misc",
    "href": "qmd/production-deployment.html#sec-prod-deploy-misc",
    "title": "Deployment",
    "section": "",
    "text": "Questions:\n\nDoes it need to be &lt;10ms or offline?\nDo you know your approximate “optimizing” and “satisficing” metrics thresholds?\nDid you verify that your input features can be looked up in a low-read-latency DB?\nCould you find anything that can be precomputed and cached?\n\nAdditional Stages\n\nRun a silent deployment\n\nfile outputs, alerts, emails are silenced\nUseful for finding bugs\n\nRun a pilot deployment\n\nOnly a few groups are given permission to use the product\nReceive feedback (e.g. weekly meetings), fix bugs, and make changes",
    "crumbs": [
      "Production",
      "Deployment"
    ]
  },
  {
    "objectID": "qmd/production-deployment.html#sec-prod-deploy-auth",
    "href": "qmd/production-deployment.html#sec-prod-deploy-auth",
    "title": "Deployment",
    "section": "Authentification",
    "text": "Authentification\n\nUsers of your product need to go through some kind of authentification\nPosit Connect\n\nInteracts with your local directory so users can use their company usernames and passwords\n\nalso allows you to only give access for specific applications to specific user groups\n\nDevelopers don’t need to keep track of extra usernames and passwords",
    "crumbs": [
      "Production",
      "Deployment"
    ]
  },
  {
    "objectID": "qmd/production-deployment.html#sec-prod-deploy-moddepl",
    "href": "qmd/production-deployment.html#sec-prod-deploy-moddepl",
    "title": "Deployment",
    "section": "Model Deployment Strategies",
    "text": "Model Deployment Strategies\n\nMisc\n\ntl;dr\n\nThe model predicts on data generated by a user action (e.g. text entry), and the user/UI is waiting for a response\n\nServe the model with an HTTP endpoint (i.e. API)\n\nA system event/user action (e.g. a file upload) triggers the model prediction and a separate process acts on the model prediction asynchronously\n\nPrediction is NOT needed immediately\nSpark Streaming (a little different)\n\nStreaming data gets input into model thats in Spark as it becomes available, Spark outputs predictions to S3 bucket that is used by an app.\nRefreshing the app, reads data from the S3 bucket and therefore has the latest predictions\n\n\nPredictions are made on periodic batches of data\n\nNotes from Deploying ML Models Using Containers in Three Ways\n\nHas py code for each strategy\nProvides tips and best practices\n\n\nStrategies\n\nThe model predicts on data generated by a user action (e.g. text entry), and the user/UI is waiting for a response\n\nServe the model with an HTTP endpoint (i.e. API)\n\n\nmodel prediction time should be less than 300 ms (standard response time expected for any HTTP service)\n\n&gt; 300ms means the service is at risk of running behind prediction requests volume, to a point where they start getting timed out\n\nTools: plumber, vetiver, flask\n\nFlask\n\nServing flask in production uses gunicorn and gevent behind a nginx server. The serve file gets these set up.\nIf you want to change some timeouts and the number of workers, the SERVER_TIMEOUT and SERVER_WORKERS env var can be used.\n\n\nYou can set an auto-scale property with your container orchestration system to scale on CPU utilization (e.g. auto-scale the model service to up to 10 pods if average CPU utilization is above 40%).\nThere are ways to hold multiple requests in memory (e.g. using cache) for a really short time (25ms) so that your model can fully utilize memory and perform a mini-batch prediction. Takes effort to implement and ensure threads can respond well.\n\n\nA system event/user action (e.g. a file upload) triggers the model prediction and a separate process acts on the model prediction asynchronously\n\nPrediction is NOT needed immediately\nSpark Streaming\n\nStreaming data gets input into model thats in Spark as it becomes available, Spark outputs predictions to S3 bucket that is used by an app.\n\nRefreshing the app, reads data from the S3 bucket and therefore has the latest predictions\n{sparklyr} + {tidymodels} has many models available.\n\nSee Apache, Spark &gt;&gt; Streaming\nBuild low-latency and scalable ML model prediction pipelines using Spark Structured Streaming and MLflow\n\npy example; shows how to include MLflow model versioning/tracking\n\n\nUse a message queue architecture\n\n\nPerform mini-batch predictions - pull a few requests at a time and make predictions\nTools: redis, RabbitMQ, ActiveMQ, pm2\n\nFor loads that need robust message queue stability, and multiple models working on the same data — you can use multiple consumer groups features in Kafka.\n\nClearly mark topics (?) for prediction requests to this model and the outputs of the model\nOnce the model makes a prediction, you’ll need to have a sync service that does something with the prediction (e.g. update the database)\n\n\nPredictions are made on periodic batches of data\n\nCreate a long-running task\n\n\nMount the elastic volume to the container when it comes up\n\n\n\nInclude the model in the container build\n\nThis is better than downloading a model each time you want to scale up\n\ne.g. hugginface model or a pickled model from a S3 bucket or other storage\n\nDrawback is the large size of the container to push and load from the repo\nMake sure that the model gets downloaded in the right place, especially if you’re using a non-root user in the Dockerfile\nOptions\n\nDownload the model in the Dockerfile\n\nThe model is downloaded when the container is built and is included in the image pushed to the repo.\nWhen the container instance is created, the model is right there, and we can load it faster.\n\nInclude the model in the code repo\n\nWhen you copy code into the container, the model get copied as well\nAllows you to version your model.\nTo overcome the file size limitation that most repos have, use Git LFS.\n\nStore in a volume\n\nSave your model in a block store, so that it can be mounted as a volume to your container\nNeed to provision the container memory (as the model will be loaded from the file system into memory)\nMaking the volume storage part of your model versioning operations.\nHaving to managing different paths for different environments complicates things.\n\n\n\nLoad model during health check\n# Flask\n@app.route(\"/health_check\", methods=[\"GET\"])\ndef health_check():\n  ZeroShotTextClassifier.load()\n  return jsonify({\"success\": True}), 200\n\nOrchestration systems periodically hit a /ping or /health_check endpoint to check if the container started correctly and/or if the database connection is alive.\nOnly once the health check is successful does the load balancer start sending the container the HTTP traffic.\nMake sure you let the DevOps team know the time it’ll take to get the model warmed up so that they can configure the health check accordingly. Also, do quick mental arithmetic to know how long the check would have taken if the model was being downloaded. (see below)",
    "crumbs": [
      "Production",
      "Deployment"
    ]
  },
  {
    "objectID": "qmd/production-deployment.html#sec-prod-deploy-batch",
    "href": "qmd/production-deployment.html#sec-prod-deploy-batch",
    "title": "Deployment",
    "section": "Batch Prediction",
    "text": "Batch Prediction\n\n\nPeriodically generate predictions offline and store in a db with fast reading capabilities\nMisc\n\nNotes from ML Latency No More\n{butcher} can reduce model storage sizes\nFor Batch jobs in general, business requirements and efficiency determine the size of the batch\n\nExample: A company aims to generate hourly reports to assess the total transactions within the hour for each payment method (e.g. MasterCard, Visa, Alipay, etc). How would you design a batch? A batch per day? per hour? per minute? per payment method?\n\nCreate one batch for each payment method per hour, compute the sum for each batch in parallel, and combine the results in the end. Smaller batches could improve efficiency and cost. (i.e. partitioning then distributed computing on multiple small machines or a parallelized on one bigger machine)\n\n\nStore batch metadata in a separate location with a batch id.\n\ne.g. batch id, ingestion time, processing start time, processing end time, processing script version, and status (ingested, processing, done)\n\n\nInputs\n\nEntity: prediction service receives a known entity ID (e.g. product_id, movie_id, order_id, device_id, etc)\nFeature combo: the prediction service receives a combo of feature values.\n\nAlternative when data privacy is a concern\n\nYou’ll need a static hashing method to generate a key for each combination of values\n\n**order is important here: a hash(country, gender, song_category) will differ from a hash(song_category, country, gender)**\nStore a prediction for each key\n\n\nExample: Ad targeting\n\n\nStrategies for High Cardinality Entities/Feature Combos High cardinality entities or feature combos can be expensive to compute\n\nTop-n\n\nEntities: generate predictions for top-n entities (e.g. top viewed, most purchased)\n\nFor the remaining entities, you make the client wait while you call the model directly instead of pulling the predictions from the prediction store\n\nFeature Combos: generate predictions for top-n most frequent feature combinations\n\nFor the remaining feature combinations, you make the client wait while you call the model directly instead of pulling the predictions from the prediction store\n\n\nSimilarity Matching\n\nProcess\n\nTrain a model on the entity (e.g products’) similarity using entity-user interactions or entity-entity co-location.\nExtract the embeddings of the entities.\nBuild an index of the embeddings using an approximate nearest neighbor method.\nLoad the index in the ML prediction service.\nUse the index at prediction time to retrieve the similar entity IDs.\nPeriodically update the index to keep things fresh and relevant.\n\nIf the index is too large, or the prediction latency is too high, reduce the embedding size to get a smaller index.\n\nReduce until model’s prediction metric falls below the acceptable threshold or the latency (aka satisficing metric) decreases to an acceptable level.\n\n\nReduce the number of features included in the combination until model’s prediction metric falls below the acceptable threshold or the latency (aka satisficing metric) decreases to an acceptable level.\n\nTips\n\nThe DB will have lots of rows, but only a few columns. Choose a DB that handles single key lookups well.\nKeep an eye on the categories’ cardinality and the number of keys generated. If you have a batch job doing this, then monitor the cardinality and raise alarms if you get a spike in new categories to count. That will prevent blowing up the DB lookup latency.\nContinuous values are going to need to be binned. That’s going to be a hyper-parameter that you need to tune.\nAny technique that can be used to lower the cardinality of categories is your friend. Lower the cardinality as much as your optimizing metric allows.",
    "crumbs": [
      "Production",
      "Deployment"
    ]
  },
  {
    "objectID": "qmd/production-deployment.html#sec-prod-deploy-online",
    "href": "qmd/production-deployment.html#sec-prod-deploy-online",
    "title": "Deployment",
    "section": "Online Prediction",
    "text": "Online Prediction\n\n\nMisc\n\nNotes from ML Latency No More\n{butcher} can reduce model storage sizes\n\nUse cases for online prediction\n\nGenerating ad recommendations for an ad request when the browser loads a page.\nOptimizing a bid in a competitive real-time bidding ad marketplace.\nPredicting if a critical piece of equipment will fail in the next few seconds (based on sensor data).\nPredicting the grocery delivery time based on the size of the order, the current traffic situation, and other contextual information about the order.\n\nAsync Predictions\n\n\nthe caller will ask for a prediction, but the generated predictions will be delivered later\nOptions\n\nPush: caller sends the required data to generate the predictions but does not wait for the response\n\nExample: When using your credit card, you don’t want to wait for a fraud check response for every transaction. Normally, the bank will push a message to you if they find a fraudulent transaction.\n\nPoll: caller sends the required data and then periodically checks if a prediction is available. The models are set up to generate predictions and store the predictions in a read-optimized low latency DB\n\n\nSynchronous Online Predictions\n\nBasic networking tasks\n\nSecuring the endpoint\nLoad balancing the prediction traffic\nAuto-scaling the number of ML gateways\n\n2 levels need to be optimized in order to reduce latency\n\nPrediction construction - This is where you reduce the time it takes a model to construct predictions from a fully formed, well-behaving, enriched and massaged prediction request.\n\nRemove supporting components such as logging, hooks, monitoring, transformation pipelines, etc. that are used to help train, evaluate, and debug the model during development\nChoose the model that balances the prediction metric and the satisficing metric (e.g. 50ms latency)\n\n“Satisficing” refers to the context in which the model will be served.\n\nIs the model going to fit on my device in terms of storage size?\nCan the model run with the type of CPUs on the device? Does it require GPUs?\nCan the feature preprocessing finish within specific time bounds?\nDoes the model prediction satisfy the latency limits that our use case requires?\n\nIn general, the lower the complexity of the model and the fewer feature, the faster the response time\n\nTrim the number of levels in a tree model\nTrim the number of trees in a random forest and gradient boosting tree model\nTrim the number of layers in a neural network\nTrim the number of variables in a logistic regression model\n\n\nSelect the proper hardware to generate the predictions at the right price/latency point\n\nTry to use custom hardware, such as GPUs or specific inference chips.\nTry to use custom compilation methods to optimize the model components.\n\n\nPrediction serving - Includes any pre-computing, pre-processing, enriching, massaging of input prediction events as well as any post-processing, caching, and optimizing the delivery of the output predictions.\n\nThis is where the most of the latency can be reduced\nStructuring the supporting historical datasets in quick-enough data stores and computing real-time contextual dynamic features\nInput feature types\n\nUser-supplied features: These come directly from the request.\nStatic reference features: These are infrequently updated values.\nDynamic real-time features: These values will come from other data streams. They are processed and made available continuously as new contextual data arrives.\n\nStatic features\n\n\nProcess\n\nThe client sends an entity ID that needs predictions. For example, recommend a list of movies for user_id=“x”.\nThe entity is enriched/hydrated by the attributes present in the feature lookup API.\nThe ML gateway then consolidates the input features into a prediction request forwarded to the ML Model API.\nWhen the ML Model API returns predictions, the ML gateway post-processes them and returns them to the client.\n\n“Singles” are numeric values (e.g. number of rooms in a house, or the ID for the advertiser associated with a campaign)\n“Aggregates” are summary stats (e.g. median house price in the zip code or the average ad budget of campaigns targeting a specific audience segment)\n“Static Features Storage” - optimized for for singleton lookup (i.e. read) operations\n\nML gateway fetching pattern for static features is: “I need a single row with one column for each of the features of customer X.”\nThe typical data warehouse is not optimized for low latency queries. Instead, data warehouses are optimized for large aggregations, joins, and filtering on extensive star schemas\n\nBatch jobs that update static features cost quite a bit of cash if you run it every 15 minutes. So you\n\nSolution:\n\nExponentially lower the frequency of the update until the model’s prediction metric falls below the acceptable threshold.\nThen raise the frequency to its previous value.\nAutomate that.\n\n\n\nDynamic Real-time Features\n\n\nProcess\n\nFresh events land in your favorite messaging system. Then, they get picked up by the streaming pipeline. The generated features, probably aggregated over time windows, land in a low-latency feature store. Exiting features are updated with fresh values.\nThe streaming pipeline generates the predictions using the features and the model API.\nThe ML gateway receives client prediction requests. The gateway then checks if there are any predictions in the database, or the messaging system. Then the gateway returns them to the client. Finally, it optionally push them to the messaging system if some other system downstream is interested. (e.g. governance team)\n\n“low latency feature store” should have fast read and write abilities",
    "crumbs": [
      "Production",
      "Deployment"
    ]
  },
  {
    "objectID": "qmd/production-deployment.html#sec-prod-deploy-aws",
    "href": "qmd/production-deployment.html#sec-prod-deploy-aws",
    "title": "Deployment",
    "section": "AWS",
    "text": "AWS\n\nContainers\n\nIf working alone or extra flexibility isn’t necessary –&gt; ECS\nIf working with a team and extra flexibility is necessary –&gt; Kubernetes\n\nNo Containers\n\nProduction –&gt; Lambda\nNot production –&gt; EC2\n\nEC2 Solution for users who need to do quick demos or just showcase something temporarily\n\nPros\n\nQuick and dirty\nCheap (potentially free)\nEasy setup/teardown\nLittle to no infrastructure/networking experience required\n\nCons\n\nNot very scalable\nNot production grade\nLittle to no automation\nNot robust to errors\n\n\nLambda\n\nNeed to work with some other services such as API Gateway, but the setup will be far more robust than deploying your app to a standalone EC2 machine.\nFor production, this would probably be the cheapest option. Pros:\n\nProduction grade\nGreat for small simple apps/functions\nServerless (extremely cheap) Cons:\nLess flexible than other solutions\nRequires knowledge of additional AWS services\n\n\nKubernetes\n\nautomates many production-level concerns such as load balancing or autoscaling\nHave to deal with deploying an application and managing a cluster which can prove no simple task\nKubernetes networking is complex, and requires lots of experience to understand and operate in depth.\nWhile a Kubernetes cluster may also seem cheaper than a more managed ML solution, a poorly managed cluster can lead to even worse unexpected monetary costs. Pros:\n\nVery scalable\nGood amount of automation\nProduction grade\nLots of community support\nHighly flexible\nExperience with a popular framework and lower-level CS! Cons:\nPotentially lots of work\nRisky for beginners\nIn some cases, just straight up unnecessary\nLots of setup require for feature parity with managed services\n\n\nECS\n\nIn terms of flexibility, it sits in between Lambda and the highly flexible Kubernetes.\nPros:\n\nSignificantly easier setup than Kubernetes\nMore features out of the box\nEasier to manage as an individual developer (with container experience)\nFirst-class support for containerized applications\n\nCons:\n\nLess granular controls\nPotentially more expensive\n\n\nSagemaker Endpoint\n\nFeels like creating deployments locally on your machine\nComes with a whole suite of services that empower users to build and deploy production ready ML apps with all the bells and whistles you’d have to manually configure for other options Pros:\n\nFirst-class machine learning support\nManaged infrastructure and environments\nProduction grade, scalable Cons:\nPotentially more expensive than some other solutions\nPotentially less flexible",
    "crumbs": [
      "Production",
      "Deployment"
    ]
  },
  {
    "objectID": "qmd/production-deployment.html#sec-prod-deploy-kub",
    "href": "qmd/production-deployment.html#sec-prod-deploy-kub",
    "title": "Deployment",
    "section": "Kubernetes",
    "text": "Kubernetes\n\nCanary Deployment\n\n\nTo start, 90% of client traffic is directed to the old app, and 10% is directed to the new app.\n\nkind: Service\nspec:\n    selector:\n        app: my-app\n\nVersion isn’t specified in the selector so traffic is sent to all pods regardless of the version label which will allow Service route traffic to both app deployments\n$ kuberctl apply -f my-app-v2.yaml\n$ kuberctl scale deploy/my-app-v2 -replicas=10\n\nOnce the new app is deemed stable, 100% of the traffic can be routed to the new app\n$ kuberctl delete -f my-app-v1.yaml\nEach connection is treated independently. So every client may be exposed to the new deployment\n\nIn Service, the session affinity field can be set to client ip if you want a clients first connection to be determinate for all future connections, i.e. each client only experiences 1 version of the app.\nIstio, https://istio.io/latest/ , can be used to more finely control traffic.\n\nOther methods: Shadow testing, A/B testing\nRolling back deployment\n$ kuberctl rollout undo deployment\n{DEPOYMENT_NAME}\n\nReverts deployment back to previous revision\n\n$ kuberctl rollout undo deployment\n{DEPOYMENT_NAME} --to-revision=2\n\nReverts deployment back to a specific revision (e.g. 2)\n\n$ kuberctl rollout history deployment\n{DEPOYMENT_NAME} --to-revision=2\n\ninspect diff of a specific revision\n\nby default, 10 revisions are saved",
    "crumbs": [
      "Production",
      "Deployment"
    ]
  },
  {
    "objectID": "qmd/production-development.html",
    "href": "qmd/production-development.html",
    "title": "Development",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Production",
      "Development"
    ]
  },
  {
    "objectID": "qmd/production-development.html#sec-prod-dev-plan",
    "href": "qmd/production-development.html#sec-prod-dev-plan",
    "title": "Development",
    "section": "Planning",
    "text": "Planning\n\nNotes from Automating the end-to-end lifecycle of Machine Learning applications and https://www.thoughtworks.com/insights/blog/curse-data-lake-monster\n\nQuestions\n\nWhat are your priorities and burning issues? — prioritize the use cases the data platform should resolve for you promptly, which can generate immediate business value.\nWhat are your constraints? — think and quantify everything — from software and human resources to time and effort required, level of internal knowledge, and monetary resources.\n\nGoals\n\nStart with quick wins — don’t dive directly into data science and machine learning model development, but instead start with quick win use cases (usually descriptive statistic use cases).\nBe realistic — when setting the data platform goals, the important thing is to be realistic about what’s feasible to achieve given current constraints.\n\nComponents\n\nBuilding data pipelines — properly developed data pipelines will save you money, time, and nerves. Developing pipelines is the most crucial part of the development, i.e. that your pipelines are properly tested and deliver new data to business users without constant brakes due to various data and system exceptions.\nOrganizing and maintaining the data warehouse — with the new data sources, a data warehouse can quickly become messy. Implement development standards and naming conventions for a better data warehouse organization.\nData preprocessing — think about acquiring data preprocessing tool(s) as early as possible to improve the dashboard performance and reduce computational costs by de-normalizing your datasets.\nData governance and security — set the internal standards and data policies on the data lifecycle (data gathering, storing, processing, and disposal).",
    "crumbs": [
      "Production",
      "Development"
    ]
  },
  {
    "objectID": "qmd/production-development.html#sec-prod-dev-check",
    "href": "qmd/production-development.html#sec-prod-dev-check",
    "title": "Development",
    "section": "Checklist",
    "text": "Checklist\n\nMisc\n\nIf you’re using database for cross-validation splits, make sure the correct order of data is maintained by making sure a logic to sort or keep the ordering of data is set.\nNotes From:\n\nHow to Build a Data Product that Won’t Come Back to Haunt You\n\n\nBroad strokes\n\na way to ingest data for training\na way to ingest data for prediction\na place to output the predictions\na place to train the model (a machine)\nsome insight into what the process is doing: logging\nnotifications to the right people when something goes wrong: alerting\n\nMore finely grained\n\nlintr run / all probs fixed on model-related code\nModel can be run independently (single R file)\nSign-off from engineering on model input / output / purpose\nCode review\nAlgorithm review\nStandardized headers in R file that describe input, output, data, algorithm, description\nCode in Bitbucket\nAll acceptance criteria are met\nModel validation documented\nModel validation review meeting held\n\nSelf-Explanatory\n\nThe more self-explanatory the data product is, the easier it will be to support future users and maintain by future developers. Data Inputs:\n\nIs the data I am using well-documented?\nAm I using the data in line with its intended use?\nAm I reusing as much of the existing engineering pipeline as possible to lower overall maintenance effort and ensure that my use is consistent with other uses of the same data items? Pipeline:\nAre the requirements, constraints and implementation of the data process documented well enough that someone else who may be taking over maintenance from me can understand it? Final Product:\nIs the report or dashboard presented in a way that is easily accessible and understandable even to people who will be viewing it, and without explanation from me?\nAm I using vocabulary and visualizations that the end user understands, and that they understand in exactly the same way I do?\nAm I providing good supporting documentation and information in a way that is intuitive and easily accessible from the data product itself?Data Inputs: Trustworthy\n\nTrust is hard to gain and easy to lose. Usually, when you first deliver a data product, there is a lot of hope and some trust. Data Inputs:\n\nAm I connected into the input data in a way that is well-supported in the production pipeline?\nDo I have explicit buy-in from those maintaining the data sets I am using?\nIs this input data likely to be maintained for a significant time in the future?\nWhen might I need to check back to be sure the data is still being maintained?\nHow do I report any problems that I see in the input data?\nWho is responsible for notifying me of issues with the data?\nWho is responsible for fixing those issues? Pipeline:\nHave I set up a regular schedule to review the data and report to ensure that the data pipeline is still functioning well and the report is conforming to the requirements?\nWhat are the conditions under which this report should be marked as deprecated?\nHow can I ensure that the user is informed should the report become deprecated? Final Product:\nHow does the user know when they can trust the report is accurate and up to date?\nIs there an efficient and/or automated way of communicating possible problems to the end user?\nIs there a clear and accessible process in place for the user to report concerns with the data or report, and for the user to be notified of any remediation processes in place? Adaptable\n\nData inputs and shapes change over time. In addition to monitoring relevant issues, when someone notices a problem, you need to be set up to react and formulate a solution without undue complications. Data Inputs:\n\nWhat features in the input data am I depending on for my analysis?\nHow will I know if those features stop being supported, are affected by a schema change, or change shape in a way that may affect my analysis?\nHow will I know if the scale of the data grows to a point where I need to refactor my process in order to keep up with my product’s requirements for freshness? Pipeline:\nHave I set up a regular schedule for re-examining the requirements to ensure that I am still producing what the user needs and expects?\nWhat is the process for users to indicate changes in requirements, and for those changes to be addressed?\nWhat is the process for refactoring and retesting the data pipeline when the inputs change in some relevant way? Final Product:\nIs the product or report set up in a way that it is easy to request a change and/or a new feature? Reliable\n\nEnsure that the different parts that make up your pipeline are reliable and the processes that orchestrate those parts are robust. Data Inputs:\n\nDoes my process fit well into the data practices and engineering production system in my organization?\nDo I have an automatic notification system in place to monitor the availability, freshness and reliability of my input data? Pipeline:\nIs each stage in my pipeline executing and completing in a timely manner?\nIs there drift in the processing time and/or amount of data being processed at any stage that may indicate a degradation in pipeline function? Final Product:\nWho is responsible for the ongoing monitoring, reviewing, troubleshooting, and maintenance of the dashboard itself?\nAre responsibilities and procedures clearly in place for reporting and resolving issues internally?",
    "crumbs": [
      "Production",
      "Development"
    ]
  },
  {
    "objectID": "qmd/production-development.html#sec-prod-dev-doc",
    "href": "qmd/production-development.html#sec-prod-dev-doc",
    "title": "Development",
    "section": "Documentation",
    "text": "Documentation\n\nModel / architecture selection\nHyper-parameters\nRough description of the data (origin, size, date, features…)\nResults (ie: precision, recall, f1…).\nA link to a snapshot of data (if possible)\nCommentary and learnings\nModels\n\nmodel object, training, testing data\nModel name and description (origin, goal, size, date, features…)\nDevelopment stage (Implemented for use, under development or recently retired)\nDiagnostic Metrics\nModel assumptions\nModel limitations\nThe model owner: the model owner is responsible for ensuring that the models are appropriately developed, implemented and used.\nThe model developers: model developers follow the lead of the model owner to create and implement the Machine Learning models.\nThe model users: model users can be either internal to the business or external. For both cases it is important to clearly identify their needs and expectations. They should also be involved in the model development and can help validate the model’s assumptions.\nOther comments\n\nWhat was learned\n\n\nProjects\n\nAn introduction\nA description of the problem\nA description of the data set\nThe methodology that you used:\nMethodology to prepare the data\nMachine Learning / statistical analysis approach taken to achieve the results\nThe Results\nRecommendations based off the results",
    "crumbs": [
      "Production",
      "Development"
    ]
  },
  {
    "objectID": "qmd/production-development.html#sec-prod-dev-mat",
    "href": "qmd/production-development.html#sec-prod-dev-mat",
    "title": "Development",
    "section": "Levels of Maturity",
    "text": "Levels of Maturity\n\nFrom 5 Levels of MLOps Maturity\n\nAuthor’s recommendations based on reading Google’s and Microsofts docs on MLOps\nAdditional Thoughts\n\nYou should monitor your model as soon as business decisions are taken based on its output, regardless of maturity level. At the very least, performance monitoring to be employed. In addition to determining model failure, results can be used to calculate ROI i.e. business value.\n\nLevel 1- Manual\n\n\nData processing, experimentation, and model deployment processes are entirely manual.\nRelies heavily on skilled data scientists, with some assistance from a data engineer to prepare the data and a software engineer to integrate the model with the product/business processes if needed.\nUse Cases\n\nEarly-stage start-ups and proof of concept projects\nSmall-scale ML applications with minimal data dependencies and real-time requirements — applications with limited scope or a small user base, like a small online fashion store.\nAd hoc ML tasks — In specific scenarios like marketing campaigns, one-time ML tasks or analyses may not require full MLOps implementation.\n\nLimitations\n\nLack of monitoring system — there’s no visibility on the model’s performance. If it degrades, it will have a negative business impact.\nNo frequent retrains of production models. Releases of the models happen only a couple of times per year.\nLimited documentation and no versioning\n\n\nLevel 2 - Repeatable\n\n\nDevOps aspect added to the infrastructure by converting the experiments to the source code and storing them in the source repository using a version control system like Git.\nData catalog — a centralized repository that includes information such as data source, data type, data format, owner, usage, and lineage. It helps to organize, manage, and maintain large volumes of data in a scalable and efficient manner.\nAdd Automated testing — unit tests, integration tests, or regression tests. These will help us deploy faster and make things more reliable by ensuring our code changes don’t cause errors or bugs.\nStill lacks a monitoring system\n\nLevel 3 - Reproducible\n\n\nTwo key reasons why reproducibility is crucial: troubleshooting and collaboration.\nNew Features\n\nAutomated training pipeline — handles the end-to-end process of training models, from data preparation to model evaluation.\nMetadata store — a database is a way to track and manage metadata, including data sources, model configurations, hyperparameters, training runs, evaluation metrics, and all the experiments data.\nModel registry — is a repository to store ML models, their versions, and their artifacts necessary for deployment, which helps to retrieve the exact version if needed.\nFeature store — which is there to help data scientists and machine learning engineers to develop, test, and deploy machine learning models more efficiently by providing a centralized location for storing, managing, and serving features. It also can be used to track the evolution of features over time and preprocess and transform features as needed.\n\n\nLevel 4 - Automated\n\n\nNew Features\n\nCI/CD — where Continuous Integration (CI) ensures integration of code changes from different team members into a shared repository, while Continuous Deployment (CD) automates the deployment of validated code to production environments. This allows for rapid deployment of model updates, improvements, and bug fixes.\nA/B testing of models — this model validation method involves comparing predictions and user feedback between an existing model and a candidate model to determine the better one.\n\n\nLevel 5 - Continuous Improvement\n\n\nModel is automatically retrained based on the trigger from the monitoring system. This process of retraining is also known as continuous learning. The objectives of continuous learning are:\n\nCombat sudden data drifts that may occur, ensuring the model remains effective even when faced with unexpected changes in the data.\nAdapt to rare events such as Black Friday, where patterns and trends in the data may significantly deviate from the norm.\nOvercoming the cold start problem, which arises when the model needs to make predictions for new users lacking historical data\n\n\n\nGoogle (E-Commerce Analysis Pipeline)\n\nInitial Stage\n\n\nData collection layer: presents the most relevant data sources that had to be initially imported to our data warehouse.\nData integration layer: presents cron jobs used for importing e-commerce datasets and the Funnel.io platform for importing performance marketing datasets to our data warehouse.\nData storage layer: presents the selected data warehouse solution, i.e. BigQuery.\nData modelling and presentation layer: presents the data analytics platform of choice, i.e. Looker.\nResources\n\n2 tools — BigQuery and Looker,\n6 people — for managing data pipelines (cron jobs + Funnel.io platform) and initial analytical requirements (data modelling),\n3 months —from acquiring Google Cloud to presenting the first analytical insights.\n\n\nAdvanced Stage\n\n\nCloud storage — for storing our external files in Google Cloud.\nCloud Run — used for deploying analytical pipelines developed in Python and wrapped as Flask applications.\nGoogle Functions — for writing simple, single-purpose functions attached to events emitted from the cloud services.\nGoogle Workflows — used for orchestrating connected analytical pipelines that needed to be executed in a specific order.\nGoogle Colab — for creating quick PoC data science models.\nResources\n\nFrom 2 to 7 tools — from using only BigQuery and Looker, we started using Cloud storage, Cloud Run, Google Functions, Google Workflows, and Google Colab.\nFrom 6 people in two teams (IT and Business Development) to 8 people in one team (Data and Analytics) — the Data and Analytics team was established and now has complete ownership over all data layers.\nFrom 3 months for creating initial insights to 2+ years of continuous development — we are gradually developing more advanced analytical use cases.",
    "crumbs": [
      "Production",
      "Development"
    ]
  },
  {
    "objectID": "qmd/production-development.html#sec-prod-dev-arch",
    "href": "qmd/production-development.html#sec-prod-dev-arch",
    "title": "Development",
    "section": "Architecture Examples",
    "text": "Architecture Examples\n\nEmbedded model” deployment. (i.e.) Take model object, insert into app, and deploy.\n\nClean\n\nremove values not pertinent to analysis\ncolumn names, types\ndummy variables, other categorical coding\nvariable transformations\nfeature engineering\n\ncan this at least be partially done with spark?\n\nJoin dataframes\nsave to file (.csv, rds, .feather, .fst, .parquet)\n\nTransfer data file to a storage system. \n\nIf project subject matter isn’t company-wide, then transfer the cleaned, pertinent subset of data to a data mart. (A data mart is focused on a single functional area of an organization and contains a subset of data stored in a Data Warehouse or Lake)\nMay have data from multiple sources (lakes, warehouses, etc)\nDoes apache airflow coordinate multiple sources? see bkmk’ed video\n\nuse spark to feed data to model\n\n\nModel Selection experiments in git branches. MLflow ($?) can monitor experiments and their metrics\n\nresearch potential models\npick a base model for comparison\ntune and train models\ncross-validation\nchoose model\n\nVersion control pipeline (DVC, Git, etc) - DVC (dvc.org) (open source) allows for version control of the large files like data sets and model objects. Works is conjunction with Git and cloud storage platforms.\n\npull data\nvalidate data\nsplit data to training and validation sets (and test set?)\ntrain model\ntest with validation set\nvalidate model within metric thresholds\nvalidate model fairness and not biased for specific variable values (race, gender, etc)\nmodel, training data, and metrics need to be linked, i.e. version controlled\ncommunicate metrics (See FluentD service below)\nwrite model object to file\nwrite training set (or maybe all data sets?) to file\ndvc push files to storage\n\nEmbed model obj + application (shiny app) in docker image\n\ndvc pull model file\nthe Docker image becomes our application+model artifact that gets versioned and deployed to production\n(before or after creating docker image?)Perform integration test with validation data: make sure the model produces the same results inside the application as in the modelling pipeline\npush image\n\nDeploy image to a Kubernetes production cluster\nCI/CD Pipeline skeleton can be orchestrated through a GoCD ($?)\nMonitor Cluster performance using the EFK stack which is composed of three main tools:\n\nElasticsearch: an open source search engine.\nFluentD: an open source data collector for unified logging layer, i.e. for log ingestion and forwarding\nKibana: an open source web UI that makes it easy to explore and visualize the data indexed by Elasticsearch.\n Add code to model script to log the model inputs and predictions as an event in FluentD\nOther tools: Logstash (alternate to FluentD), Splunk\n\n\n\n\n\n\n\n\n\nDescribed in An introduction to Monzo’s data stack\n“analytics events processor + shipper” is something custom they built to “sanitize” (i.e. remove personally identifiable information (PII)) events data\nI think NSQ is like Kafka and it’s there for redundancy(?)\n\nPaper: Machine Learning Operations (MLOps): Overview, Definition, and Architecture",
    "crumbs": [
      "Production",
      "Development"
    ]
  },
  {
    "objectID": "qmd/production-ml-monitoring.html",
    "href": "qmd/production-ml-monitoring.html",
    "title": "37  ML Monitoring",
    "section": "",
    "text": "37.1 Stuff I haven’t organized",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>ML Monitoring</span>"
    ]
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#stuff-i-havent-organized",
    "href": "qmd/production-ml-monitoring.html#stuff-i-havent-organized",
    "title": "37  ML Monitoring",
    "section": "",
    "text": "articles\n\nhttps://eugeneyan.com/writing/practical-guide-to-maintaining-machine-learning/?utm_campaign=Data_Elixir&utm_medium=social\nhttps://towardsdatascience.com/the-playbook-to-monitor-your-models-performance-in-production-ec06c1cc3245\nInferring Concept Drift Without Labeled Data\n\nExample: RStudio Connect and {pins} (article)\n\nDeploy a model as a RESTful API using Plumber\nCreate an R Markdown document to regularly assess model performance by:\n\nSending the deployed model new observations via httr\nEvaluating how the model performed with these new predictions using model metrics from yardstick\nVersioning the model metrics using the pins package\nSummarize and visualize the results using flexdashboard\n\nSchedule the R Markdown dashboard to regularly evaluate the model and notify us of the results\n\nMisc\n\nDL Monitoring points\n\n\nData Drift Monitor and SoftMax Monitor (i.e. monitor prediction metrics) are typical of any monitoring system\nModel Monitor is an auxiliary model (or models) trained to recognize basic patterns that emerge in the baseline operations of the primary model.\n\nExample: monitor the values of normalized outputs of various layers within the model.\n\nThese values could be input to a neural network trained to distinguish the patterns of normal operation from out-of-range examples included in the training set. This model monitor could then flag potential drift during operation of the primary system\n\n\n\nIf predictions result in an intervention, it will be difficult to determine drift.\n\nExample: patient is determined to be at high risk by a model, gets treated by clinician, and lives. Was this a false positive by the model or was the model correct and the reason for the patient living is the clinician’s intervention.\n\nOther tracking\n\nNumber of alerts triggered by model\n\nWork with users to minimize false positives and hence unnecessary alerts. Otherwise model alerts might be treated as the boy that cried wolf and will be ignored or taken less seriously.\n\nInterventions after a trigger\n\nThis will let you know if your model predictions are being adhered to\nExample: model that predicts a patient is at high risk of death and triggers an alert to a clinician\n\nPart of the standard protocol for intervening after a patient is labelled high risk is to perform additional vitals measurements. The number of vitals measurements for a patient are recorded, so this metric can be used a proxy. If there is an increase in a high risk patient’s vital measurments, then the data team can infer that it’s model’s alerts are being adhered to.\n\n\nAutomated Retraining Triggers\n\nRequirements:\n\nThe number of models in production is limited so retraining costs are low.\n\nRetail, logistics, etc. may involve thousands of related (e.g geography) models\n\nThe frequency of triggering events is rare.\nThere are no strict requirements on model availability.\n\n\nPredicting model drift\n\nImportant, so:\n\nSLA requirements related to model availability can be met.\nAnalysis can be done on such cases for root cause analysis in pre-production settings.\nComputational requirements for retraining can be calculated in advance knowing the frequency and prevalence of these events across the models.\n\nsurvival analysis to model performance decay in predictive models.\n\nData requirements:\n\nSufficient historical scoring data is available on 1 model\nOr scoring data is on a large number of closely related models\n\nOnce you have the probability of survival distribution, you can could use the 95th percentile survival time to trigger a model retrain to ensure that the model degradation is below the performance threshold with the specified probability\n\n\nModel calls\n\nIf model usage drops or becomes irratic, it could be a signal something is wrong\nOnly useful for frequently used apps\nDepending on the model environment, you might want to check requests and responses separately.\n\nWas the model not asked (e.g., because a recommendation widget crashed) or failed to answer (e.g., the model timed out and we had to use a static recommendation instead)? The answer would point to where you should start debugging.\n\n\nCompetitor feature distributions\n\nWould be useful in diagnosing if the changes in data are just your company or happening to the industry sector\n\nSHAP for Drift Detection: Effective Data Shift Monitoring Feeds distribution of SHAP values into a logistic regression, then applies ks test on probability predictions for Y==1, Y==0\nMeasuring Precision and Recall\n\nActions taken by model\n\nSoft Actions - Events are flagged for humans to take action\nHard Actions - Events are flagged and actions are automatically taken by an algorithm\n\nPrecision: ratio of positive predictions that are correct (Precision = TP / (TP + FP))\n\nSoft Action models - Record human decisions that are made on the flagged events\nHard Action models - Set up a control group: for a fraction of the volume, let a human decide, instead of your model. Then the precision in the control group is an estimate of the precision of your model\n\nRecall: ratio of all positive events that the model is detecting (Recall = TP / (TP + FN))\n\nNeed to audit your negative predictions to count the number of False Negatives (FN)\nIssue: Imbalanced classes (e.g. fraud model)\n\nIf recall is expected to be 99.9%. That means that, on average, you’d need to audit at least 1K negatives per day just to find just one false negative, and even more for that number to be statistically meaningful.\n\nSolution: Importance Sampling (Google blog post)\n\nSample negative predictions based on their model scores\nSample more heavily from datapoints with high model scores because that’s where we expect most of our false negatives to be\n\n\n\n\nIf you are dealing with a high-risk domain, it is best to design model fallbacks from the get-go in case of model failure from broke code, concept drift, etc.\n\nExamples\n\nMake a human do what the model was doing (e.g. insurance claims processing, manufacturing quality control, or sales lead scoring)\nRule-based approach: You can often design a set of rules or heuristics that will be less precise but more robust than a rogue model. (e.g. simply show the most popular items to all customers)\n\n\nNotes from https://blog.anomalo.com/effective-data-monitoring-8bce3ddf87b4\n\nAll the stuff that platform, Anomalo, does\n\nIdentifying issues and retraining models will minimize losses\n\ndeviations between baseline and production distributions\nfeature and cohort performance\n\nML transparency regulation requires it to understand why a model made a particular prediction to ensure broader governance, fairness, and mitigate bias\n\nKeep track of predictions by group\n\nDynamic testing can prevent false positives/negatives\n\nUse a forecasting method to generate prediction intervals. If data values for a variable fall outside the PIs, then a data quality alert is triggered\n\nLimiting tests to the most recent data can reduce data warehouse costs\nCreate application (or subscribe to a service) that enables a user to adjust commonly changed rules/thresholds without writing code\n\nShould be well documented\nThe types of changes users often make include:\n\nWidening the expected range for a data outcome\nNarrowing the scope of a rule using a where SQL clause\nWaiting for updated-in-place data to arrive before applying a rule\nChanging thresholds for machine learning alerts\neasy reversion to prior state\n\nso will need a log/versioning\n\n\n\nFor data that is critical. Checks should be included in the pipeline\n\nIf a check does fail, you could run automated tasks to fix the bad data, abort the remainder of the DAG (sometimes, no data is better than bad data), or quarantine bad records using SQL produced in the API to query for good and bad data.\n\nIf you have hundreds of variables, managing data quality rules for each column may be untenable.\n\nUse unsupervised data monitoring\n\nno increase in NULL values\n\nA constrained model looking for significant increases in NULL values\n\nno anomalous records\n\nmachine learning algorithm, which identifies changes in continuous distributions, categorical values, time durations, or even relationships between columns\nscore severity of anomaly somehow\n\n\n\nRoute alerts (slack) for a particular table only to teams that regularly use or maintain that table\n\nAs alerts arrive, they can use emoji reactions to classify their response to alerts. Common reactions include: * ✅ the issue has been fixed * 🔥 an important alert * 🛠️ a fix is underway * 🆗 expected behavior, nothing needed * 👀 under review\n\nAlerts should have contextual information\n\nExample\n\nWhy does this alert matter?\nWhat # and % of user_id values are affected?\nHow often has this alert failed in the recent past?\nWho configured this alert, and why?\nWhat dashboards or ML models depend on fact_table?\nWhat raw data source contributed user_id to fact_table ?\n\nInclude image with a row with a value that triggered the alert and a row that has a valid value\n\nRoot cause analysis\n\nUses statistical method to find out where the issue is\n\ngroup_by(cat) %&gt;% summarize(pct_bad = … whatever)\npermute rows of anomalous column and see where there’s a relationship change between anomalous column/rows and other columns. The columns where the relationship changes might be potentially involved in the anomalous values of the permuted column\n\nclustering or correlation?\n\n\n\nGet user feedback on alerts (useful or not useful?)\n\nGround Truth Latency - how long does it take to know if your prediction was right\n\nRealtime/near realtime\n\nstocks, gambling, food delivery time estimates, digital advertising\nAble to determine whether there’s an issue with your model almost immediately\n\ndelayed\n\ncredit card fraud\nRequires monitoring of proxy metrics (metrics that are associated with the ground truth) until ground truth arrives\n\n\nProblematic ground truth types\n\nBiased\n\nExample: loan default model\n\nThe ground truth only includes results from customers who were approved for a loan. So there’s no information about whether a person who was denied a loan would’ve repaid it back or not\n\nSolution: An occasional A/B test where a group of customers applying for the loan isn’t subject to model predictions of whether they’re credit worthy or not (control group) and a group that is subject to model predictions (treatment group)\n\nLittle to zero or sporadic ground truth feedback\n\nRequires monitoring of proxy metrics (metrics that are associated with the ground truth)\nManual collection of ground truth data\n\ncan be expensive but high quality ground truth data is very important\n\n\n\ndata required for monitoring\n\nIf customer facing, then the data should that which is necessary to calculate service level indicators (SLI) which will help the company keep its customer obligations which are outlined in the service level agreement (SLA) (see link for more details)\nTypes\n\nUnique Id per request provided by the system that called the ML model. This unique identifier will be stored with each log and will allow us to follow the path of a prediction before, during and after the ML model.\nInput features before feature engineering\nInput features after feature engineering\nOutput probabilities\nPredicted value\n\nData size required to be able to measure a metric accurately\n\nHow many observations do I need so that I know the metric I’m measuring is accurate.\nDepends on metric and threshold for the accuracy of that metric.\nExample:\n\nclassification with imbalanced dataset, 1/100 is a positive event\nRecall = 90%\n\nThe model should get 90% of all true positive correct\n\nThreshold (aka shot noise) = 0.1%\n\nThe maximum error in measuring recall is 0.1%. So, a model with 90.0009% recall would trigger a model retrain\n\nshot_noise &lt;- ((pos_rate * data_size) * (1-recall)) / data_size\n\nI don’t see how to solve this for data_size (i.e. data_size cancels out) so I guess this is an optimization problem\n\n\n\n\nChanges in the distribution of input and output features\n\nTriggers for retraining model (if possible, use both)\n\nusing performance based triggers is good for use-cases where there is fast feedback and high volume of data, like real time bidding, where you are able to measure the model’s performance as close as possible to the time of predictions, in short time intervals and with high confidence (high volume).\n\nThe main limitation when relaying on performance only, is the time it takes for you to obtain your ground truth — if you obtain it at all. In user conversion prediction cases, it can take 30 or 60 days until you will get a ground truth, or even 6 months or more in cases such as transaction fraud detection or LTV. If you need to wait so long to have full feedback, that means you’ll retrain the model too late, after the business has already been impacted.\n\nBy measuring changes in the input data, i.e. changes in the distribution of features that are used by the model, you can detect data drifts that indicate your model may be outdated and needs to be retained on fresh data.\n\nMissing values can occur regularly at ML model inference. Even when missing values are allowed in features, a model can see a lot more missing values than in the training set. An example of a missing value error is an ML model making an inference based on a form input where a previously optional field is now always sending a null value input due to a code error.\nRange violation happens when the model input exceeds the expected range of its values. It is quite common for categorical inputs to have typos and cardinality mismatches to cause this problem, e.g. free form typing for categories and numerical fields like age, etc. An unknown product SKU, an incorrect country, and inconsistency in categorical values due to pipeline state are all examples of range violation.\nType mismatch arises when the model input type is different from the one provided at inference time. One way types get mismatched is when column order gets misaligned during some data wrangling operations.\n\n\n\nTrack distance metrics between reference variable distribution and a production variable distribution\n\nThe production variable distributions should include feature variable data entering the pipeline and prediction output from the models\nUpward trends in the distance between baseline and operational data distributions can be the basis of data drift detection.\nPotential reference distributions (i.e. monitoring window)\n\na distribution across a fixed time window (distribution doesn’t change). Examples:\n\ntraining distribution\nvalidation/test set distribution.\ninitial model deployment distribution (or a time when the distribution was thought to be stable)\n\na distribution across a moving time window (distribution can change)\n\nlast week’s input data with this week’s input data\n\nConsiderations\n\nRepresentation differences: Class ratios across windows may not be the same (e.g., the fraction of positives in one window may be very different from the fraction of positives in another window).\nVarying sample sizes: The number of data points in each window may vary (e.g., the number of requests received on a Sunday is less than the number of requests received on a Monday).\nDelayed feedback: Due to reasonable events (e.g., loss of Internet connection), labels may come in at a lag, making it impossible to factor in predictions without a label into the current window’s evaluation metric.\n\n\nTrack predictions by group (cohort performance)\n\nDrift may impact groups differently.\n\ne.g. persons with high net worth, low FICO scores, recent default, etc.\n\nGroups that are very important to the company should be tracked at the very least.\nMakes misclassified or poorly predicted observations exportable, so they can be studied further\n\nmaybe version these\n\n\nOther stuff to track\n\nNumber of predictions made\nPrediction latency — How long it takes to make a prediction\nIf the product is customer facing, then customer satisfaction/usage type metrics should also be tracked\n\nThresholds\n\nSee Automating Data Drift Thresholding in Machine Learning Systems\n\nAuthor was lame. He coded the impractical solution and didn’t code the good solution.\n\nComputationally intensive method (may not be practical)\n\nBootstrap or MC simulate the feature at the size of the production dataset or whatever the amount of data you’re going to test for drift\nFor each simulation, measure the drift (e.g. PSI, JS Divergence) between the simulated dataset and the training dataset.\nThreshold =  mean(drift) of all the simulations If any production/inference dataset has a drift &gt; threashold then that should trigger an alert\n\nClosed method for PSI\n\nSupposedly can be calculated for other distance measures.\nFinal Solution for the threshold 🥴\n\n\nK is the number of bins (numeric) or levels (categorical)\nPk is the percentage of total training observations == level_K or the percent of total training observations in bin K\nαk = 1 + Nq + Pk\n\nNq = sample size of the production/inference set, Q. So it should be a constant for all K.\nHe used lower-case pk here instead of Pk, but my understanding was that they were the same thing. May want to double check that.\n\nΨ is called the digamma function. Will need to look that one up.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>ML Monitoring</span>"
    ]
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#sec-prod-mlmon-ddmet",
    "href": "qmd/production-ml-monitoring.html#sec-prod-mlmon-ddmet",
    "title": "37  ML Monitoring",
    "section": "37.2 Distribution Distance Metrics",
    "text": "37.2 Distribution Distance Metrics\n\nAlso see this article for examples of computing wasserstein (aka Kantorovich) distance in R, py, Julia.\nPackages\n\n{philentropy} has a ton of distribution distance measures + some helper functions, Docs\n{{cinnamon}} LIB handles py models and calculates wasserstein and kolmogorov-smirnov\n{KSgeneral} can perform KS tests between continuous, mixed, or discrete distributions\n\nSee Distributions &gt;&gt; Tests for more details on kolmogorov-smirnov\nPaper: Two-sample KS test with approxQuantile in Apache Spark provides code that uses Spark’s approxQuantile to perform a (currently) unavailable 2-sample KS test in Spark for big data situations.\n\n\nPopulation Stability Index (PSI)\n\nFormula\n\n\ni ∈ length(bins)\np is the percent of total observations in bin i\nref is the reference variable\nprod is the production variable\n\nMisc\n\nNotes from\n\nPopulation Stability Index | Matthew’s Blog\nChecking model stability and population shift with PSI and CSI\n\nOften seen in the finance industry\nFor both numeric and categorical features\nSometimes referred to as Characteristic Stability Index (CSI) when used on predictor variables\n\nSteps\n\nDivide the reference variable variable range into 10 bins (arbitrary but seems to be common) or however many bins you want depending on how fine a resolution you want.\n\nFor categorical variables, the levels can be used as bins or levels can be collapsed into fewer bins\nContinuous\n\nSlicing the range of the reference variable into sections of the same interval length\nSlicing the reference variable into quantiles where each bin has the same number of observations\n\n\nCount the number of values in each of those bins.\nDivide each bin count by the sample size to get a percentage\nRepeat for the production variable\nCalculate PSI\n\nGuidelines Also see Thresholds section above from https://scholarworks.wmich.edu/cgi/viewcontent.cgi?article=4249&context=dissertations\n\n\nN is reference sample size and M is production sample size (although it’s symmetric so doesn’t matter which you column/row you use for each)\nB is the number of bins used.\nCells have the PSI values for 95% level significance\nPSIs &gt;= the appropriate cell value can reliably be interpreted that a shift in the variable distribution has occurred.\nSee paper tables in Appendix B for other significance levels and B values. Can also use a chisq distribution.\n\nExample: model predictions\n\n\n\nEquation is slightly different but may be equivalent\n\ninitial - model predictions that are used as a reference (e.g. predictions from when current model first went into production)\nnew - current model predictions\n\nAverage PSI is used to represent the model\nGuidelines\n\nPSI &lt; 0.1 = The population hasn’t changed, and we can keep the model\n0.1 ≤ PS1 &lt; 0.2 = The population has slightly changed, and it is advisable to evaluate the impacts of these changes\nPSI ≥ 0.2 = The changes in population are significant, and the model should be retrained or even redesigned.\n\n\nExample from arize ai (model monitoring platform)\n\n\nA comparison in the distributions of how a person spent their money this last year as compared to the year prior\n\nY-axis represents the percentage of the total money spent in each category, as denoted on the X-axis.\n\nSteps\n\nCalculate the difference in percentage between the reference distribution A (budget last year) and the actual distribution B (budget this year)\nMultiply that difference by the natural log of (A %/ B%)\n\nThe larger the PSI, the less similar your distributions are.\nYou can set up thresholding alerts on the drift in your distributions.\n\n\nJensen-Shannon Divergence\n\nMisc\n\nNotes from https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-data-bias-metric-jensen-shannon-divergence.html\nAlso see How to Understand and Use the Jensen-Shannon Divergence (Haven’t read but looks more in-depth)\n\nFormula \n\nWhere pmix = 0.5(pref + pprod) and\n\n\ni ∈ length(bins)\np is the percent of total observations in bin i\nSimilar for KL(pprod || pmix)\n\n\n\nSteps: Same preparation steps as with the PSI distance metric (binning, proportions, etc.)\nSymmetric version of K-L divergence ( see Information Theory &gt;&gt; K-L Divergence) which means it satisfies the triangle inequality which means it’s a true distance metric\nfyi using log2 means KL is in units of “bits” and using ln means KL is in “nats”\nThe range of JS values for binary, multicategory, continuous outcomes\n\nUsing ln, JS ∈ [0, ln(2) ≈ 0.693]\nUsing log2, JS ∈ [0, 1] Values near zero mean the labels are similarly distributed.\n\nPositive values mean the label distributions diverge, the more positive the larger the divergence.\nSupposedly the usage of the mixture reference makes this an unstable metric for using a moving window. It makes the JS score not comparable to past values.\n\nSeems like all moving window reference distributions, mixed or not, will have some variability to it, but maybe this produces extra variability that makes it unreliable.\n\nOther measures\n\nKullback-Leibler Divergence (KL Divergence)\nWasserstein’s Distance",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>ML Monitoring</span>"
    ]
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#sec-prod-mlmon-ddarc",
    "href": "qmd/production-ml-monitoring.html#sec-prod-mlmon-ddarc",
    "title": "37  ML Monitoring",
    "section": "37.3 Data Drift Architectures",
    "text": "37.3 Data Drift Architectures\n\nExample: How to Build a Fully Automated Data Drift Detection Pipeline\n\n\nUses Kestra for orchestration\nExample relies on scheduled data pulls for detecting drift. Kestra can make use of Graphana to create a real-time detection pipeline.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>ML Monitoring</span>"
    ]
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#sec-prod-mlmon-nlpm",
    "href": "qmd/production-ml-monitoring.html#sec-prod-mlmon-nlpm",
    "title": "37  ML Monitoring",
    "section": "37.4 NLP monitoring",
    "text": "37.4 NLP monitoring\n\nNotes from Monitoring NLP models in production\nDescriptive Statistics\n\nFeatures: length of text, out-of-vocabulary (OOV) words %, and the share of non-letter character %\nExample: {{evidently}}\ncolumn_mapping = ColumnMapping() \ncolumn_mapping.target = 'is_positive'        # binary target\ncolumn_mapping.prediction = 'predict_proba'  # predicted probabilities\ncolumn_mapping.text_features = ['review']    # text feature\n\ndata_drift_report = Report( \n    metrics=[ \n        ColumnDriftMetric('is_positive'),\n        ColumnDriftMetric('predict_proba'), \n        TextDescriptorsDriftMetric(column_name='review'), # text feature\n    ] \n) \ndata_drift_report.run(reference_data=reference, \n                      current_data=valid_disturbed, \n                      column_mapping=column_mapping) \ndata_drift_report\n\nStat tests on distributions are performed (e.g. K-S test) with p-values given\nOther drill down charts are provided if drift is detected\n\n\nDomain Classifier\n\n{{evidently}}\n\nBuilds a binary classifier model to predict with the text feature data came from the reference dataset (1) or the current dataset (0)\nThe ROC AUC of the binary classifier shows if the drift is detected. If a model can reliably identify the samples that belong to the current or reference dataset, the two datasets are probably sufficiently different.\nCan be biased if there are time related text (e.g. month names or dates)(makes it easier for the classifier), but these can be detected by looking at feature importance plot and looking for date/time related tokens\nIf drift is detect, you can drill down further\n\nTypical words in the current and reference dataset - These words are most indicative when predicting which dataset a specific review belongs to.\nExamples of texts - from current and reference datasets that were the easiest for a classifier to label correctly (with predicted probabilities being very close to 0 or 1).\n\nExample\ndata_drift_dataset_report = Report(metrics=[ \n    ColumnDriftMetric(column_name='review') \n]) \ndata_drift_dataset_report.run(reference_data=reference, \n                              current_data=new_content, \n                              column_mapping=column_mapping) \ndata_drift_dataset_report\n\n\nInvariance testing\n\nTests whether an ML model produces consistent results under different conditions\nSee {{behave}}, Write Readable Tests for Your Machine Learning Models with Behave\nExample\n\n\nDirectional Testing\n\nStatistical method used to assess whether the impact of an independent variable on a dependent variable is in a particular direction, either positive or negative.\nFor NLP, this test checks whether the presence of a specific word has a positive or negative effect on the sentiment score of a given text.\nSee {{behave}}, Write Readable Tests for Your Machine Learning Models with Behave\nExample\n\n\nSentiment score should increase due to the new word’s (“awesome”)_ presence",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>ML Monitoring</span>"
    ]
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#sec-prod-mlmon-invdd",
    "href": "qmd/production-ml-monitoring.html#sec-prod-mlmon-invdd",
    "title": "37  ML Monitoring",
    "section": "37.5 Investigating Data Drift",
    "text": "37.5 Investigating Data Drift\n\nIf feature distributions change, it may be something else besides a true data generating process (dgp) shift.\n\nCheck for pipeline: code infrastructure, processing, data sources, hardware, and input model issues. (see below for details)\n\nIf it’s not a data quality, then is it concept drift or data drift?\n\nData Drift\n\nDistributions change but relationships between features remain\n\nConcept Drift\n\nDistributions might remain similar, but the relationships change instead: in between the features, or between the features and the model output.\n\n\nQuestions\n\nWhich features are drifting?\nHow strongly?\n\nMay require domain expert to determine whether there are substantial changes in associations\n\nWhat’s the process behind it?\n\nExamples\n\nA change in the socio-economic relations, such as inflation, diseases, or political changes;\nUnaccounted events, such as holidays, world cups, or even natural disasters;\nThe entrance of a new competitor in the market, and/or the shift of customers;\nChanges in the offered product, or the marketing campaign.\n\n\n\nActions\n\nData Pipeline: fix it\nData Drift:\n\nlow to moderate: leave it alone and see if the model handles it or gets worse\nhigh or meaningful: retraining the model should suffice\n\nConcept Drift: Model predictions should suffer, so a complete overhaul (EDA, algorithm selection, production tools, etc.) might be necessary.\nAdditional actions (may need if new labels/target variable data isn’t immediately available)\n\nTake the component of the application that uses the model offline (e.g remove recommendations for website)\nMake a human do what the model was doing (e.g. insurance claims processing, manufacturing quality control, or sales lead scoring)\nRule-based approach: You can often design a set of rules or heuristics that will be less precise but more robust than a rogue model. (e.g. simply show the most popular items to all customers)\nAdd business logic or an adjustment on top of the model output\n\nProbably won’t generalize well so should be context specific\n\n\n\nCode infrastructure\n\nWrong source - A pipeline points to an older version of the marketing table, or there is an unresolved version conflict.\nLost access - Someone moved the table to a new location but did not update the permissions.\nBroken queries - These JOINSs and SELECTs might work well until the first complication. Say, a user showed up from a different time zone and a new category of time zone is in the data. Some queries might not hold up.\nInfrastructure update - You got a new version of a database and some automated spring cleaning. Spaces replaced with underscores, all column names in lowercase. All looks fine until your model wants to calculate its regular feature as “Last month income/Total income” with hard-coded column titles.\n\nProcessing\n\nBroken feature code - For instance, the promo discounts were never more than 50% in training. Then marketing introduces a “free” offer and types 100% for the first time. Some dependent feature code suddenly makes no sense and returns negative numbers.\nDealing with outliers and missing values\n\nNotes from https://towardsdatascience.com/why-data-integrity-is-key-to-ml-monitoring-3843edd75cf5\nSkip the prediction or have a back-up system — If the data is bad, the serving system can skip the prediction to avoid erroring out or making an inaccurate prediction. While this can be a solution when the model makes a large number of non-critical decisions (e.g. product recommendation), it’s not an option when it makes business or life-critical decisions (e.g. healthcare). In those cases, there needs to be a backup decision-making system to ensure an outcome. However, these backup systems can further complicate the solution.\nImpute or predict missing values — A key challenge of this approach is that it hides the problems behind the data issue. Consistently replacing bad data can shift the expected feature’s distribution (aka data drift) causing the model to degrade. A drift as a result of this data replacement could be very difficult to catch, impacting the model’s performance slowly over time.\nSet default values — When the value is out of range, it can be replaced by a known high or low or unique value, e.g. replacing a very high or low age with the closest known minimum or maximum value. This can also cause gradual drift over time impacting performance.\nAcquire missing data — In some critical high value use cases like lending, ML teams also have the option to acquire the missing data to fill the gap. This is not typical for the vast majority of use cases.\nDo nothing — This is the simplest and likely the best approach to take depending on the criticality of your use case. It allows for bad data to surface upstream or downstream so that the problem behind it can be resolved. It’s likely that most inference engines might throw an error depending on the ML algorithm used to train the model. A prediction made on bad data can show up as an outlier of either the output or the impacted input helping surface the issue.\n\n\nData source\n\nnew data formats, types, and schemas\n\nAn update in the original business system leads to a change of unit of measurements (think Celsius to Fahrenheit) or dates formats (DD/MM/YY or MM/DD/YY?)\nNew product features in the application add the telemetry that the model never trained on.\nThere is a new 3rd party data provider or API, or an announced change in the format.\n\nThe website being scraped changes urls or webpage format\n\nHardware\n\nSensor breaks, Server goes down\nData collection can stop or the data that’s collected could be corrupted\n\nInput model\n\nA model’s results that are used as inputs to another model\nWould need to determine if it was model drift or data drift",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>ML Monitoring</span>"
    ]
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#sec-prod-mlmon-modd",
    "href": "qmd/production-ml-monitoring.html#sec-prod-mlmon-modd",
    "title": "37  ML Monitoring",
    "section": "37.6 Model Drift",
    "text": "37.6 Model Drift\n\nNotes from\n\nGetting a Grip on Data and Model Drift with Azure Machine Learning\nhttps://towardsdatascience.com/why-data-integrity-is-key-to-ml-monitoring-3843edd75cf5\n\nTesting for drift\n\n\nOption 1: You’ll already have a trained candidate ready for deployment if model 2 outperforms model 1.\n\nExample: An aggregated dataset consists of 45,000 timestamped observations which we spilt into 20,000 references, 20,000 current, and 5,000 most recent observations for the test.\n\nScore Models\nThe current model outperforms the reference model by a large margin. Therefore, we can conclude that we indeed have identified model drift and that the current model is a promising candidate for replacing the production model.\n\n\nOption 2: Will likely reduce false positives, at the expense of being less sensitive to drift.\nOption 3: Reduces unnecessary training cycles in cases where no drift is identified.\n\nArchitectures\n\nAzure ML\n\n\nIngest and version data in Azure Machine Learning\n\nFor automation, we use Azure Machine Learning pipelines which consume managed datasets. By specifying the version parameter (version=“latest”) you can ensure to obtain the most recent data.\n\nTrain model\n\nModel is trained on the source data. This activity can also be part of an automated Azure Machine Learning pipeline.\nRecommend adding a few parameters like the dataset name and version to re-use the same pipeline object across multiple dataset versions.\n\nBy doing so, the same pipeline can be triggered in case model drift is present.\n\nOnce the training is finished, the model is registered in the Azure Machine Learning model registry.\n\nEvaluate model\n\nBesides looking at performance metrics to see how good a model is, a thorough evaluation also includes reviewing explanations, checking for bias and fairness issues, looking at where the model makes mistakes, etc. It will often include human verification.\n\nDeploy model - Deploy a specific version of the model\nMonitor model - Collect telemetry about the deployed model.\n\nAn Azure AppInsights workbook can be used to collect the number of requests made to the model instance as well as service availability and other user-defined metrics.\n\nCollect inference data and labels\n\nAs part of a continuous improvement of the service, all the inferences that are made by the model should be saved into a repository (e.g., Azure Data Lake) alongside the ground truth (if available).\n\nAllows us to figure out the amount of drift between the inference and the reference data.\n\nShould the ground truth labels not be available, we can monitor data drift but not model drift.\n\nMeasure data drift - Use the reference data and contrast it against the current data\nMeasure model drift - Determine if the model is affected by data or concept drift\nTrigger re-training\n\nIn case of model or concept drift, we can trigger a full re-training and deployment pipeline utilizing the same Azure ML pipeline we used for the initial training.\nRe-training triggers can either be:\n\nAutomatic — Comparing performance between the reference model and current model and automatically deploying if the current model performance is better than the reference model.\nHuman in the loop — Inspect data drift visualization alongside performance metrics between reference and current model and deploy with a data scientist/ model owner in the loop. This scenario would be suitable for highly regulated industries. This can be done using PowerApps, Azure DevOps pipelines, or GitHub Actions.\n\n\n\nNLP\n\nIssue\n\nAfter writing the new predictions with assigned labels (e.g. good/bad review) to a database., you typically do not get immediate feedback. There is no quick way to know if the predicted labels are correct. However, you do need something to keep tabs on the model’s performance to ensure it works as expected.\n\nOptions for collecting ground truth labels (reactive sol’ns, so there will be delay in awareness of drift)\n\nYou can have a feedback mechanism directly in the website UI. For example, you can allow the review authors or readers to report incorrectly assigned categories and suggest a better one. If you get a lot of reports or corrections, you can react to this and investigate.\nManual labeling as quality control. In the simplest form, the model creator can look at some of the model predictions to see if it behaves as expected. You can also engage external labelers from time to time to label a portion of the data. This way, you can directly evaluate the quality of the model predictions against expert-assigned labels.\n\nLead indicators of model drift are often data quality issues and changes in the input data distributions\n\nRegarding data quality, there might be corruption due to wrong encoding, the presence of special symbols, text in different languages, emojis, etc. being newly introduced into the data pipeline\nSee NLP Monitoring\n\n\n\nInvestigating\n\nAnalyze locally - For critical use cases the best practice is to begin with a fine grained approach of prediction analysis by replaying the inference with the issue and seeing its impact on the model.\n\nFor ML models, use model-agnostic diagnostic scores (e.g. shap, dalex, iml, etc.) to compare previous model prediction features to the drifted model prediction features.\n\nDefine the segments of low performance\n\nHave previously important features become not-so important. Is that a data issue or some outside event causing the change?\n\n\n\nAnalyze Globally - This involves analyzing the data for that feature over a broader range of time to see when the issue might have begun. Data changes typically coincide with product releases. So querying for data change timeline can tie the issue to a specific code and data release helping revert or address it quickly.\n\nImputing or other missing data methods may only gradually affect model results after a lengthy period and therefore may be difficult for data validation monitoring to detect.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>ML Monitoring</span>"
    ]
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#sec-prod-mlmon-retrain",
    "href": "qmd/production-ml-monitoring.html#sec-prod-mlmon-retrain",
    "title": "37  ML Monitoring",
    "section": "37.7 Retraining Model",
    "text": "37.7 Retraining Model\n\nRisks of Automated Retraining\n\nRetraining on delayed data\n\nIn some real-world scenarios, like loan-default prediction, labels may be delayed for months or even years. The ground truth is still coming, but you are retraining your model using the old data, which may not represent the current reality well.\n\nFailure to determine the root cause of the problem\n\nIf the model’s performance drops, it doesn’t always mean that it needs more data. There could be various reasons for the model’s failure, such as changes in downstream business processes, training-serving skew, or data leakage. You should first investigate to find the underlying issue and then retrain the model if necessary.\n\nHigher risk of failure\n\nRetraining amplifies the risk of model failure. Besides the fact that it adds complexity to the infrastructure, the more frequently you update, the more opportunities the model has to fail. Any undetected problem appearing in the data collection or preprocessing will be propagated to the model, resulting in a retrained model on flawed data.\n\nHigher costs\n\nStoring and validating the retraining data\nCompute resources to retrain the model\nTesting a new model to determine if it performs better than the current one\n\n\nPerform an analysis on the data that caused the trigger\n\nDid the mean or sd (i.e. distribution) change?\nIs there a new seasonality or cyclic component present?\nAre there new correlations between variables?\nWhat’s behind the change (expansion into a different area, new product, new vendor, new competitor, etc.)\n\nUsing the characteristics of the data found in the analysis, decide on the most relevant block of data that is representative the current state of the data being collected. This is the retraining dataset\n\nA sufficient sample size should also be a consideration.\nMaybe use upsampling or simulation to obtain a sufficiently sized dataset with the necessary characteristics\n\nPotential next steps. Which one or combination of steps depends on the severity and causes of the data drift/model performance degradation, time and budget constraints.\n\nRetrain using current algorithm\n\nRetrain with more weight on recent data points\nRetrain with recent data only\nRetrain on all your past dataset\n\nUpdate using current algorithm but with a new dataset\n\nUse initial weights and batch training\nThis might only be for DL models or I think there might be something in {sklearn} and/or {tidymodels}\n\nRedo the algorithm selection process\nRedo feature engineering\nRedo everything",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>ML Monitoring</span>"
    ]
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#sec-prod-mlmon-notif",
    "href": "qmd/production-ml-monitoring.html#sec-prod-mlmon-notif",
    "title": "37  ML Monitoring",
    "section": "37.8 Notifications",
    "text": "37.8 Notifications\n\nMisc\n\nAlso see Project, Management &gt;&gt; Event Auditing\nPrioritize different alerts (e.g. highly important, important, normal, warning, note)\nIf your model is triggering alerts to often, generate additional rules with users to limit the number of alerts\n\nExample: model that predicts patient risk of death can only trigger a re-alert to the clinician if:\n\nit’s been 48 hrs since the previous alert\nif the patient has not just came from the ICU\n\n\n\nEmail\n\nExample: to developers\n\n\n“jarvis” is an internal package and this function probably wraps a template and email package function\n\nExample: to users\n\n\n“CHARTWatch” is the name of the data product\nAlerts user that the product is down and other pertinent information\n\n\nSlack\n\nExample\n\n\n“jarvis” is an internal package and this function probably wraps a template and slack package function",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>ML Monitoring</span>"
    ]
  },
  {
    "objectID": "qmd/production-testing.html",
    "href": "qmd/production-testing.html",
    "title": "Testing",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Production",
      "Testing"
    ]
  },
  {
    "objectID": "qmd/production-testing.html#sec-prod-test-misc",
    "href": "qmd/production-testing.html#sec-prod-test-misc",
    "title": "Testing",
    "section": "",
    "text": "Example: Dependencies (e.g. APIs), Databases, Filesystems\n\n{jarvis} is an internal package",
    "crumbs": [
      "Production",
      "Testing"
    ]
  },
  {
    "objectID": "qmd/project-development.html",
    "href": "qmd/project-development.html",
    "title": "39  Development",
    "section": "",
    "text": "39.1 Misc",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Development</span>"
    ]
  },
  {
    "objectID": "qmd/project-development.html#sec-proj-dev-misc",
    "href": "qmd/project-development.html#sec-proj-dev-misc",
    "title": "39  Development",
    "section": "",
    "text": "Notes from\n\nAdapting Project Management Methodologies to Data Science\n\nAlong with overview of various methodologies, provides list of Agile foundational values and key principles\n\n\nLoose implementation of CRISP-DM with agile-based practices is recommended\nWaterfall or the newer variation with feedback loops between adjoining stages should not be used\n\nDesigned for manufacturing and construction where the progressive movement of a project is sequential. DS typically requires a lot of experimentation and a modification of requirements.\nAlthough can be useful for certain stages of the data science project such as planning, resource management, scope, and validation\n\nPrior to full deployment, run a pilot deployment\n\nOnly a few groups are given permission to use the product\nReceive feedback (e.g. weekly meetings), fix bugs, and make changes\n\nAfter full deployment\n\nHave an education and training session for users\n\nNote problem areas. These may be potential next steps to improving the product\n\nCheck-in periodically with users to get feedback\n\nProtyping and Testing\n\nSee Lean Data Science\n\nThe idea is to build things that deliver value quickly\n\nIterative Building Steps\n\nBuild ‘good enough’ versions of the tool or project (MVPs)\nGive these to stakeholders to use and get feedback\nIncorporate feedback\nReturn to stakeholders to use and get more feedback\nIterate until project the stakeholder and you feel it has reached production-level\n\nBreak each project down into a set of smaller projects\n\nExample:\n\nMVP to test if the idea is feasible\nMore functional version of the MVP\nProductionized version of the product.\n\nTrack the impact of each of the sub-projects that comprise the larger projects\nAt each of these milestones, decide on whether to progress further on a project by using taking the impact score of the subproject into account\n\nExample: rule-based chatbot manages to\n\nChatbot: successfully helps 10,000 customers a month\n\n10,000 customers ⨯ 3 min average call = 30,000 mins = 500 hours.\n\nCall Center Agent\n\nCall-center agent’s time costs $200/hr in terms of salary and infrastructure,\n\nConclusion: MVP chatbot saves $100K a month and you could likely save even more with a more sophisticated chatbot.",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Development</span>"
    ]
  },
  {
    "objectID": "qmd/project-development.html#sec-proj-dev-comms",
    "href": "qmd/project-development.html#sec-proj-dev-comms",
    "title": "39  Development",
    "section": "Communication",
    "text": "Communication\n\nRemind stakeholders of what it is we agreed in last meeting you’d do, what you did and how to interpret the results\nState what it is you need from the stakeholder.\nState whether the project progress in the middle, at it’s end, you’re wrapping up or what’s going on?\nA summary slide or results peppered with comments leading me through what it is I am looking at\nThe Minto Pyramid\n\nOrganize the message so that it starts with a conclusion which leads to the arguments that support it and ends in detailed information.\nProcess\n\nWrite conclusion (2-3 sentences max)\nSupporting arguments: Try to make them concise bullet points\nLink to a more detailed explanation at the bottom if need be\n\n\nMight be useful to time the arrival when the stakeholder is most likely able to read it.\n\ne.g. If a stakeholder has a meeting at 9:30 every morning, it may be better to time the sending of the report to before or after that meeting.",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Development</span>"
    ]
  },
  {
    "objectID": "qmd/project-development.html#sec-proj-dev-kanban",
    "href": "qmd/project-development.html#sec-proj-dev-kanban",
    "title": "39  Development",
    "section": "39.2 Kanban",
    "text": "39.2 Kanban\n\n\nPhysical or digital board where tasks are then outlined as story cards.\nEvery card will be extracted from left to right until it is completed.\nflexibility to execute tasks without getting constant deadlines\nMisc\n\nSeems like this could be used within a sprint (columns would have to be defined according to the sprint plan)\n\nAdvantages\n\nbottlenecks, overworked steps, etc. easily identified\neffective at communicating the work in progress for stakeholders and team members\noriented towards individual tasks instead of batches like in scrums\n\nDisadvantages\n\nlack of deadlines can lead to longer project times\nchallenging to define the columns for a data science Kanban board\nCustomer interaction is undefined. As such, customers may not feel dedicated to the process without the structured cadence of sprint reviews",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Development</span>"
    ]
  },
  {
    "objectID": "qmd/project-development.html#sec-proj-dev-crisp",
    "href": "qmd/project-development.html#sec-proj-dev-crisp",
    "title": "39  Development",
    "section": "39.3 CRISP-DM",
    "text": "39.3 CRISP-DM\n\n\nCross-Industry Standard Process for Data Mining\nDefacto standard for data mining\nSupports replication, generalizable to any DS project\nPhases (not all are sequential, some phases are iterative):\nBizSci Version\n\n\nView Business as a Machine\n\nIsolating business units\n\nInternal: Sales, Manufacturing, Accounting, etc\nExternal: customers, suppliers\nVisualizing the connections\n\nDefining objectives\nCollecting outcomes\n\nUnderstand The Drivers\n\nInvestigate if objectives are being met\nSynthesize outcomes\nHypothesize drivers\n\nAt this stage, it’s critical to meet with subject-matter experts (SMEs). These are people in the organization that are close to process and customers. We need to understand what are the potential drivers of lead time. Form a general equation that they help create.\n\n\nMeasure Drivers\n\nCollect Data\n\nCollect data related to the high level drivers. This data could be stored in databases or it may need to be collected. We could collect competitor data, supplier data, sales data (Enterprise Resource Planning or ERP data), personnel data, and more.\nMay require effort to set up processes to collect it, but developing strategic data sources becomes a competitive advantage over time.\n\nDevelop KPIs\n\nRequires knowledge of customers and industry. Realize that a wealth of data is available outside of your organization. Learn where this data resides, and it becomes a tremendous asset.\n\n\nUncover Problems And Opportunities\n\nEvaluate performance vs KPIs\nHighlight potential problem areas\nReview the our project for what could have been missed\n\nTalk with SME’s to make sure they agree with your findings so far.\n\n\nEncode Decision Making Algorithms\n\nDevelop algorithms to predict and explain the problem\nOptimize decisions to maximize profit\n\ne.g. For classification, threshold optimization using a custom cost function to optimize resources, costs, precision, and recall (See Diagnostics, Classification &gt;&gt; Scores &gt;&gt; Custom Cost Functions\n\nUse systematic decision-making algorithms to improve decision making\n\nExample: Employee Churn\n\n\nApp uses LIME to get prediction-level feature importance\n\n\nMeasure The Results\n\nCapture outcomes\nSynthesize results\nVisualize outcomes over time\n\nWe are looking for progress. If we have experienced good outcomes, then we need to recognize what contributed to those good outcomes.\nQuestions\n\nWere the decision makers using the tools?\nDid they follow the systematic recommendation?\nDid the model accurately predict risk?\nWere the results poor? Same questions apply.\n\n\n\nReport Financial Impact\n\nMeasure actual results\nTie to financial benefits\nReport financial benefit to key stakeholders\n\nIt’s insufficient to say that we saved 75 employees or 75 customers. Rather, we need to say that the average cost of a lost employee or lost customer is $100,000 per year, so we just saved the organization $7.5M/year. Always report as a financial value.\n\n\nExample: Customer Churn\n\nView business as a machine\n\nIsolating business units: The interaction occurs between Sales and the Customer\nDefining objectives: Make customers happy\nCollecting outcomes: We are slowly losing customers. It’s lowering revenue for the organization $500K per year.\n\nUnderstand The Drivers\n\nInvestigate if objectives are being met\n\nCustomer Satisfaction: Loss of customers generally indicates low satisfaction. This could be related to availability of products, poor customer service, or competition offering lower prices and/or better service or quality.\n\nSynthesize outcomes\n\nCustomers are leaving for a competitor. In speaking with Sales, several customers have stated “Competition has faster delivery”. This is an indicator that lead time, or the ability to quickly service customers, is not competitive.\n\nHypothesize Drivers\n\nLead time is related to supplier delivery, inventory availability, personnel, and the scheduling process.\n\n\nMeasure Drivers\n\nAverage Lead Time: The level is 2-weeks, which is based on customer feedback on competitors.\nSupplier Average Lead Time: The level is 3 weeks, which is based on feedback related to our competitor’s suppliers.\nInventory Availability Percentage: The level of 90% is related based on where customers are experiencing unmet demand. This data comes from the ERP data comparing sale requests to product availability.\nPersonnel Turnover: The level of 15% is based on the industry averages.\n\nUncover Problems and Opportunities\n\nOur average lead time is 6 weeks compared to the competitor average lead time of 2 weeks, which is the first order cause for the customer churn\nOur supplier average lead time is on par with our competitor’s, which does not necessitate a concern.\nOur inventory percentage availability is 80%, which is too low to maintain a high customer satisfaction level. This could be a reason that churn is increasing.\nOur personnel turnover in key areas is zero over the past 12 months, so no cause for concern.",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Development</span>"
    ]
  },
  {
    "objectID": "qmd/project-development.html#sec-proj-dev-agile",
    "href": "qmd/project-development.html#sec-proj-dev-agile",
    "title": "39  Development",
    "section": "39.4 Agile",
    "text": "39.4 Agile\n\n39.4.1 Misc\n\nResources\n\nhttps://www.atlassian.com/agile/project-management/overview\n\nfeatures adaptability, continuous delivery, iteration, and short time frames\n\n\n\n39.4.2 Terms\n\nEpic - collection of high level tasks that may represent several user stories\n\nHelps to map the model outcome and define the correct stakeholders for the project\nA helpful way to organize your work and to create a hierarchy.\nThe idea is to break work down into shippable pieces so that large projects can actually get done and you can continue to ship value to your customers on a regular basis\nDelivered over a set of sprints\n\nInitiatives - collections of epics that drive toward a common goal\nProduct Roadmap - plan of action for how a product or solution will evolve over time\n\nexpressed and visualized as a set of initiatives plotted along a timeline\n\nScrum - a framework that’s objective is to fulfill customer needs through transparent communication, continuous progress, and collective responsibility\n\nData-Driven Scrum (DDS) - Scrums, as defined, have fixed lengths which can be an issue with DS projects\n\nSprints - short periodic blocks that make up a scrum\n\neach usually ranges from 2-4 weeks\nEach sprint is an entity that delivers the full result.\nComposed of a starting point and requirements that complete the project plan\nTheme - an organization goal that drive the creation of epics and initiatives\n\nUser Story - smallest unit of work or a task; an informal, general explanation of a software feature written from the perspective of the end user. Its purpose is to articulate how a software feature will provide value to the customer.\n\nAfter reading a user story, the team knows why they are building, what they’re building, and what value it creates.\n\n\n\n\n39.4.3 Values for Data Analysis\n\nDecisions over dashboards: By focusing on what people want to do with data, we can move past the first set of questions they ask, focus on the valuable iteration and follow-up questions, build trust, cultivate curiosity and drive action.\nFunctional analysis over perfect outputs: To enable quick iterations, we’re going to have to spend less time crafting perfect outputs and focus on moving from one question to the next as quickly as possible.\nSharing data over gatekeeping data: We’re going to have to share responsibility for our data and data “products” with our business partners. This will help build trust, and keep us all accountable for cultivating great data products and data-driven cultures.\nIndividuals and interactions over processes and tools: When in doubt, we need to rely on the relationships we’ve built with the business over the tools we’ve put in to help guide those relationships.\n\n\n\n39.4.4 Data Science Lifecycle\n\n\narticle\nIf at any point we are not satisfied with our results or faced with changing requirements we can return to a previous step since this methodology is focused on iterative development\nSteps\n\nBusiness Understanding\n\nDefine objectives: Work with customers/stakeholders to identify the business problem we are trying to solve.\nIdentify data sources: Identify the data sources that we will need to solve it.\n\nData Acquisition and Understanding\n\nIngest the data: Bring the data into our environment that we are using for analytics.\nExplore the data: Exploratory data analysis (EDA) and determinine if it is adequate for model development.\nSet up a data pipeline: Build a process to ingest new data. A data pipeline can either be batch-based, real-time or a hybrid of the previous options.\nNote: While the data scientists on the team are working on EDA, the data engineers may be working on setting up a data pipeline, which allows us to complete this stage quicker\n\nModeling\n\nFeature engineering: Creat data features from raw data for model training.\n\nEnhanced by having a good understanding of the data.\n\nModel training: Split the data into training, validation, and testing sets. Train models\nModel evaluation: Evaluate those models by answering the following questions:\n\nWhat are the metrics that the model achieves on the validation/testing set?\nDoes the model solve the business problem and fit the constraints of the problem?\nIs this model suitable for production?\n\nNote: Could train one model and find that the results are not satisfactory and return to the feature engineering and model training stages to craft better features and try different modeling approaches.\n\nDeployment (Options)\n\nExposing the model through an API that can be consumed by other applications.\nCreating a microservice or containerized application that runs the model.\nIntegrating the model into a web application with a dashboard that displays the results of the predictions.\nCreating a batch process that runs the model and writes the predictions to a data source that can be consumed.\n\nStakeholder/customer acceptance\n\nSystem validation: Confirm that the data pipeline, model, and deployment satisfy the business use case and meet the needs of the customers.\nProject hand-off: Transfer the project to the group that will manage it in production. l\n\n\n\n\n\n39.4.5 Product Roadmap Examples\n\nExample\n\nInitiative: build a forecast system to predict sales for an ice cream company\nEpics:\n\n“As a Sales Manager, I need to understand which regions I need to focus my outbound effort based on the sales forecast”\n“As a Logistics Manager, I need to estimate demand so that I can prepare our production accordingly”\n\nUser Story:\n\n“As a Logistics Manager, I need to see the forecast on my Production Dashboard”;\n“As a Logistics Manager, I need to have simulations around how weather predictions can change the forecast”;\n\n\n\n\n\n39.4.6 Sprint Workflow\n\n\nBad flow chart, should be a circle where review loops back to planning\nsprint review - the scrum team and stakeholders review what was accomplished in the sprint, what has changed in their environment, and collaborate on what to do next\n\nThese are necessary to avoiding issues that might destroy a project. (see below)\n\nData scientist participation will help with their communication skills and increase transparency in what they’re doing\nStakeholders might think a feature or ML result is feasible with the current data and tech stack. These are important to opportunities to explain why they aren’t feasible.\nRoles often bleed together. The planning portion is a good way to converge on a strategy of what to do next.\n\n\nsprint planning (~15 minutes every two weeks)\n\nDevelop the next sprint’s goals\n\nDo the next sprint’s goals align with our goals in 3 months\nDo the next sprint’s goals align with our annual team goals/strategic vision\nRevise the next sprint’s goals to align with these goals if necessary\n\nBreak the sprint goals into tasks and sub tasks\nAssign the tasks/subtasks to members and estimate time to completion of these tasks\nExtended sprint planning (Every 3 months to roughly plan the next 3 months)\nStrategic meetings (6 months)\n\nSome technical details to starting a project\n\nNotes from The Technical Blockers to Turning Data Science Teams Agile\nStart a repo\n\nin the organization acct not under a personal acct\nuse readme as onboarding document\n\nlast person to join is in-charge of it\n\nThe last person will be best suited to edit/add details that clear up any confusion that they had when they were onboarded\nInclude “This document is maintained by the most recent person to join the team. That person is currently: ____”\n\nExplicitly state that anyone can review code in your README. If someone isn’t familiar with a part of the code, they become so by reviewing it.\n\nEdit the settings of your repo. Make the main branch protected, don’t allow anyone to push directly to the main branch, and only allow PRs that have passed unit-tests (more about this later) and have undergone a code review.\n\nUpdate the team’s skills related to Agile\n\nIn the beginning, may not have a lot of tasks to assign as there may be design/requirements discussions\nMake sure everyone knows git and how to write unit tests\n\nCheck team members’ personal accts to see how many commits they have, “https://github.com/search?q=author:”\nCheck team members’ projects for unit tests\nIf it doesn’t look like they don’t have much experience, assign a udemy, etc. course on the subject and require a certificate in order to be assigned tasks\n\n\nAssign tasks through Agile tools like ZenHub, Jira, or Trello\nSet-up a CI tool\n\nexamples: Github Actions, TravisCI, CircleCI, or Jenkins\nadd learning this tool as part of your ZenHub task boards and don’t allow people to move on until they’ve learned it.\nrun your unit tests every time someone makes a PR\n\nDaily Stand-ups\n\nused to discuss what your daily work will be, and it should be short\nProject strategy meeting should be immediately after the stand-up\nEach team member answers only 3 questions:\n\nWhat will you do today?\nWhat did you do yesterday?\n\nRather than a simple verbal status update. It can be better the show what you did.\n\ne.g. show your coding screen and walk everyone through you code\n\nBenefits\n\nSomeone else on the team will have an idea for a better, faster, or simpler way to solve the problem\nEasier to catch a flaw after a few lines of code than after 1000 during a code review\nIf you find out that someone on my team is doing something very similar, and you can save time by reusing code.\nCool to see incremental progress every day instead of just the final product\n\n\nWhat are you blocked by?\nScreen-share these three questions written out on a PowerPoint slide. \n\nCongratulate people on finishing the courses\nAssign a weekly changing role of scrum master\n\nThe scrum master makes sure the 3 questions above are answered by everyone.\nThis person doesn’t have to be the boss or most senior person.",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Development</span>"
    ]
  },
  {
    "objectID": "qmd/project-management.html",
    "href": "qmd/project-management.html",
    "title": "Management",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Projects",
      "Management"
    ]
  },
  {
    "objectID": "qmd/project-management.html#sec-prod-mang-misc",
    "href": "qmd/project-management.html#sec-prod-mang-misc",
    "title": "Management",
    "section": "",
    "text": "Processing Checklist for publicly releasing datasets",
    "crumbs": [
      "Projects",
      "Management"
    ]
  },
  {
    "objectID": "qmd/project-management.html#sec-prod-mang-eaudit",
    "href": "qmd/project-management.html#sec-prod-mang-eaudit",
    "title": "Management",
    "section": "Event Auditing",
    "text": "Event Auditing\n\nEvents that get flagged for further investigation need to be audited by humans\nMisc\n\nAlso see Production, ML Monitoring &gt;&gt; Notifications\n\nTerms\n\nBacklog - amount of events that you have flagged for human investigation.\nQueue Rate - ratio of all events that you’re flagged.\nCapacity - how much you can at most queue in a given time period.\n\nExample: if you hire 100 human annotators/auditors of events working 8 hours per day, and a single annotation takes 5 minutes, then you can queue around 10K items per day, and that’s your system’s capacity.\n\n\nMaintain a stable backlog\n\nIf your queue rate is higher than your capacity, your backlog increases.\nIf your queue rate is lower than your capacity, your backlog decreases.\n\nHave elasticity in your labeling/auditing workforce\n\ni.e. being able to quickly increase and decrease the size of the workforce as needed.\nSpikes in events (e.g. fraud, new products, etc.) can lead to increased backlog\nWalmart’s system (paper)\n\nUses a combination of both expert human analysts and “crowd workers” for product classification\nhuman experts do regular audits of the crowd-generated labels to ensure their quality",
    "crumbs": [
      "Projects",
      "Management"
    ]
  },
  {
    "objectID": "qmd/project-management.html#sec-prod-mang-pmover",
    "href": "qmd/project-management.html#sec-prod-mang-pmover",
    "title": "Management",
    "section": "Project Management Overview",
    "text": "Project Management Overview\n\nMisc\nInitiation\n\nUnderstand the key stakeholders of the project;\nUnderstand which data sources are available for the project;\nEvaluate the complexity of the data integration and the expected time effort;\nDefine the problem and draw the plan for available solutions;\nDefine the due date for the project\n\nPlanning\n\nDevelop the timeline of model development — including where key checkpoints and stakeholders meetings will sit.\nDevelop the timeline for data integration, baseline model development and model improvements.\nEvaluate available constraints for project development (for example, explainability of the models, reasonable training time, etc.);\nLeave some room for “creative tasks.”\n\nIt’s very common that new ideas will come up during the project development and having that time planned for some experimentation will be key to set expectations with stakeholders.\n\nRisk Management\n\nSee The Risk Management Process in Project Management for details on planning and mitigation solutions\nDraw plans to mitigate risks. Examples:\n\nWhat happens if one of my data scientists leave the team (due to sickness, voluntary leave, etc.)?\nWhat happens if we can’t work with the data sources we need to train our model?\nIs there a regulatory risk involving any of the data contained in my data sources?\n\n\nThis high-level plan should be approved by all stakeholders\nTasks should be divided up among team members\n\nSee Development Frameworks section\n\n\nExecution & Controlling\n\nExecution is the development of the project and controlling is the constant evaluation of execution vs. the original plan.\nCommon tasks\n\nKeeping track of the timeline vs. planning. Comparing if there are project delays or assessing project risk.\nKeep track of business stakeholder management — particularly if the initial requirements are being met.\nRegular stand ups between technical teams and business stakeholders.\nAnd of course, development of the model(s) itself.\n\n\nClosure\n\nThe project is presented and main conclusions are discussed with the business stakeholders\nUnderstand if the business goals have been met.\nDiscussing the project result — in case of a failed hypothesis, discuss the gain from this new found knowledge.\nEncapsulate relevant documentation and handover the project to the final stakeholder\n\nKeep in mind the audiences when writing the documentation\n\nOther developers/data scientists that will need to maintain the solution in the future\nBusiness users that will use the output in specific use cases.",
    "crumbs": [
      "Projects",
      "Management"
    ]
  },
  {
    "objectID": "qmd/project-management.html#sec-prod-mang-iss",
    "href": "qmd/project-management.html#sec-prod-mang-iss",
    "title": "Management",
    "section": "Issues",
    "text": "Issues\n\nScope creep\n\nStakeholders said, “I love the work you did on feature X, can you also do that for A, B, C)”. Without further thought or consultation, you agree to work on that.\nResult: It’s too late when you realize that you don’t have time to work on the core feature you planned to deliver. Deadlines get pushed back.\n\nTech creep\n\nStakeholders may see a new technology that needs to be incorporated as a feature, etc.\n\nPriority Shift\n\nNew issues keep popping up that take priority over the project\n\nTech debt\n\nYou commit to a task (e.g. a new feature for your model, more training data) that should really be in the backlog.\nResult: you hack it together to finish on a deadline\n\nLack of communication\n\nDevelopers say, “If I deliver on my work on time, why do I need to be in meetings”.\nResult: Project priorities change and team is out-of-sync.\nResult: Stakeholders become disengaged with the project because they aren’t receiving progress updates.",
    "crumbs": [
      "Projects",
      "Management"
    ]
  },
  {
    "objectID": "qmd/project-management.html#sec-prod-mang-dashm",
    "href": "qmd/project-management.html#sec-prod-mang-dashm",
    "title": "Management",
    "section": "Dashboard Management",
    "text": "Dashboard Management\n\nLifespan Factors\n\nDetermine how likely the underlying data sources for your dashboard will change.\n\nIf you know of plans for a schema change, your stakeholder may decide to push the dashboard into the backlog until the changes have been implemented.\n\nExtend your dashboard lifespan by keeping the data updated.\nAssess if the metrics for the dashboard are main KPIs used across the company and are likely to remain relevant.\n\nStartups especially may pivot business models and dashboards become outdated quickly. Your time may be better spent on other tasks than creating this dashboard.\n\n\nDashboard Literacy\n\nCreate documentation that explains the metric definitions. The best is to add a ‘definitions’ tab to the dashboard or a link to a page with more information.\nSchedule a meeting with potential users of your new dashboard and explain how to interpret the data.\n\nBetter yet, record the meeting for new users to save you time from having to present the same information again.\n\n\nOnboard New Hires\n\nConsider onboarding new users to dashboards relevant to them. Often new hires aren’t aware of the dashboards used by their predecessors.\nCreate a list of widely used dashboards for new hires to review. This helps familiarize them with the company KPIs and promotes dashboard adoption.\n\n\n\nDeprecation Process\n\nMisc\n\nNotes from Why and How to Deprecate Dashboards\n\nExample contains py code for Looker SDK\n\nDashboard bloat  is the effect on an organization when time is wasted finding the relevant visual to answer a question or recreating visuals that already exist.\n\n\n\nWrite a script to dump all BI metadata into the warehouse\n\nGet data through SDKs/APIs (e.g. {{looker_sdk}}, {{Domo}}) or APIs (e.g. Tableau Server, PowerBI)\n\nExamples\n\nLooker: fetch all dashboards, looks, and users\nTableau: fetch workbooks, views, and users\n\nClean the response by either casting it as a JSON or extracting only specific fields of relevance (like ID, name, created date, user)\n\nGet last access date for each visual\n\nLooker (i__looker metadata) and PowerBI (raw audit logs), or pre-built reports with Tableau and Domo\nAlso other usage data if available\n\nWrite all these outputs into warehouse tables\n\nOverwrite tables with data dumps (like all visuals), and append data that builds over time (like historical access)\nExample: Looker\n\nTables for each visual (dashboards and looks for the Looker example). Call it `looker_dashboard` and `looker_look`\nTable of users. Call it `looker_user`\nTable of historical access (either raw or aggregated to latest access date per visual). Call it `looker_historical_access`\n\n\nSet script to run on a schedule (preferrably daily)\n\n\n\nQuery table to summarize ownership and last access date for each visual\n\nSometimes, creating the visual doesn’t count as accessing it, so you’ll need to make sure recently created visuals aren’t flagged for deletion.\nTo alert users via Slack, you’ll need to map their email to a Slack username.\nIf it’s a table not a view, update this on a schedule.\nCan be a dbt model\nExample\nwith history as (\n    select visual_id,\n          max(access_date) as latest_access_date\n    from looker_historical_access\n    group by visual_id\n), dashboards as (\n    select\n        id as visual_id,\n        name as visual_name,\n        user_id as visual_created_user_id,\n        created_at as visual_created_at,\n        'dashboard' as type\n    from dashboard\n), looks as (\n    select\n        id as visual_id,\n        name as visual_name,\n        user_id as visual_created_user_id,\n        created_at as visual_created_at,\n        'look' as type\n    from look\n), visuals as(\n    select * from dashboards union all select * from looks\n)\nselect\n    v.*,\n    coalesce(h.latest_access_date, v.visual_created_at) as latest_access_date,\n    u.email\nfrom visuals as v\nleft join history as h on h.visual_id = \nleft join user as u on v.visual_created_user_id;\n\nResult: one row per visual, when it was created, the user who created it, and the last date it was viewed or edited\n\n\n\n\nAutomatically warn users before deprecation, then delete visuals\n\n60 or 90 days as the threshold for “not recent” is recommended\nProcess\n\nCommunicate the reason for deprecating visuals\n\nDocument and communicate the benefits of keeping a clean BI instance to the broader organization\nThe purpose is not to delete others’ work; it’s to enable everyone in the company to get insights from data faster.\nSee “Dashboard Bloat” above\n\nCreate a deprecation Slack channel for automated communication\n\nAnyone who is a user of the BI tool should be added to this channel.\n\nGive people a week to save the dashboard/visual\n\nQuery visuals that haven’t been accessed in X-7 days and send a Slack message.\ne.g. Visuals included should be ones unused for 53 days if deleting at 60 days of idle time, or 83 days if deleting at 90 days of idle time.\nExample\n# Everything below is pseudo-code, with utility methods abstracted away\ndeprecation_days = 60\nwarn_visuals = get_warehouse_data( # Pseudo method\n    f'''\n    select visual_name, created_by_user\n    from modeled_looker_data\n    where current_date - last_accessed_date = {deprecation_days - 7}\n    ''')\nslack_message_template = '''\n    Visual {{visual_name}} created by @{{slack_username}} will be\n    deprecated in 7 days. If this is incorrect, please contact the\n    analytics team.\n'''\nfor visual in warn_visuals:\n    send_slack_message(slack_message_template, visual) # Pseudo method\n\nQuery visuals that are ready for deletion and delete them programatically\ndeprecation_days = 60\ndelete_visuals = get_warehouse_data( # Pseudo method\n    f'''\n    select visual_id\n    from modeled_looker_data\n    where current_date - last_accessed_date &gt;= {deprecation_days}\n    ''')\nfor visual in delete_visuals:\n    visual_id = visual['visual_id']\n    if visual['type'] == 'look':\n        sdk.delete_look(visual_id)\n    else:\n        sdk.delete_dashboard(visual_id)\n\nTrial period: Run the automated process for a few weeks by commenting out the actual deletion to ensure the logic is sound.\nShould also be aware of data collection or dbt models that are used for the visual being deprecated. If they aren’t attached to anything else, they can be deleted, too.",
    "crumbs": [
      "Projects",
      "Management"
    ]
  },
  {
    "objectID": "qmd/python-classes.html",
    "href": "qmd/python-classes.html",
    "title": "Classes",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-misc",
    "href": "qmd/python-classes.html#sec-py-class-misc",
    "title": "Classes",
    "section": "",
    "text": "When to use classes:\n\nWhen you have a group of functions and they have many of the same arguments, this indicates a class might be helpful. Also, if one or more of the functions is used in the other functions, this is also an indication that creating a class would be better.\n\nSee Python, Snippets &gt;&gt; Refactor a group of functions into a class\n\nWhen you have code and data that go together and need to keep track of the current state\n\ne.g. managing a bunch of students and grades\n\nWhen you see hierarchies, using classes leads to better code organization, less duplication, and reusable code.\n\nYou can make a single change in the base class (parent) and all child classes pick up the change\nExample: Report class\n\nYou can have a base class with shared attributes like report name, location and rows. But when you go into specifics like formats (xml, json, html), you could override a generate_report method in the subclass.\n\n\nEncapsulation\n\nWhen you want to separate external and internal interfaces in order to (ostensibly) hide internal code from the user.\nKeeps excess complexity from the user\n\n\nBest Practices\n\nUse camel case for class names\nUse snake case for methods and attributes\nAlways use self as the first argument of a method\nWrite docstrings for your classes so that your code is more understandable to potential collaborators and future you.\n\nCreate a Class that allows method chaining\n\n\nreturn self is what allows the chaining to happen",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-terms",
    "href": "qmd/python-classes.html#sec-py-class-terms",
    "title": "Classes",
    "section": "Terms",
    "text": "Terms\n\nClass inheritance - mechanism by which one class takes on the attributes and methods of another\nclass Employee:\n    def __init__(self, name, salary=0):\n        self.name = name\n        self.salary = salary\n__init__() is a constructor method. It assigns (or initializes) attributes that every object (aka instance) for this class must have.\nself is the 1st argument in any method definition. It refers to a particular instance.\nself.salary  = salary creates an attribute called salary and assigns to it the value of the salary parameter (default set to 0)\nClass attributes are attributes that have the same value for all class instances.\n\nAccessing a class attribute\n# access first employee's name attribute\ne1.name\n# access second employee's salary attribute\ne2.salary\n\nInstance Attributes - Values for these attribute depend on the instance (e.g. they vary depending on each employee)\nInstantiate - Creating a new object from a class\ne1 = Employee(\"yyy\", 5000)  # name, salary\ne2 = Employee(\"zzz\", 8000)",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-meth",
    "href": "qmd/python-classes.html#sec-py-class-meth",
    "title": "Classes",
    "section": "Methods",
    "text": "Methods\n\nMisc\n\nIn most classes, best practice to at least include __init__ and __repr__ methods\n\n\n\nInstance Method\n\nFunctions that can only be called by an object from this class\n\nSimilar to regular functions with the difference of having “self” as the first parameter\n\nclass Employee:\n    def __init__(self, name, salary=0):\n        self.name = name\n        self.salary = salary\n\n#Instance method\n    def give_raise(self, amount):\n        self.salary += amount\n        return f\"{self.name} has been given a {amount} raise\"\n\n# calling an instance method\n# instantiate object first\nobject = MyClass() \nobject.method()\n\n\n\nMagic Methods (aka Dunder or Special Methods)\n\nAren’t meant to be called, usually invoked by an operation\n\nExamples\n\n__add__ invoked by myclass() + myclass()\n__str__ invoked by str(myclass())\n\nExample\nclass Address:\n    def __init__(self, street, city, state, zipcode, street2=''):\n        self.street = street\n        self.street2 = street2\n        self.city = city\n        self.state = state\n        self.zipcode = zipcode\n    def __str__(self):\n        lines = [self.street]\n        if self.street2:\n            lines.append(self.street2)\n        lines.append(f'{self.city}, {self.state} {self.zipcode}')\n        return '\\n'.join(lines)\n\n&gt;&gt;&gt; address = Address('55 Main St.', 'Concord', 'NH', '03301')\n&gt;&gt;&gt; print(address)\n55 Main St.\nConcord, NH 03301\n\nCan be an instance or class type of method\nfrom datetime import datetime, timedelta\nfrom typing import Iterable\nfrom math import ceil\nclass DateTimeRange:\n    def __init__(self, start: datetime, end_:datetime, step:timedelta = timedelta(seconds=1)):\n        self._start = start\n        self._end = end_\n        self._step = step\n\n    def __iter__(self) -&gt; Iterable[datetime]:\n        point = self._start\n        while point &lt; self._end:\n            yield point\n            point += self._step\n\n    def __len__(self) -&gt; int:\n            return ceil((self._end - self._start) / self._step)\n\n    def __contains__(self, item: datetime) -&gt; bool:\n            mod = divmod(item - self._start, self._step)\n            return item &gt;= self._start and item &lt; self._end and mod[1] == timedelta(0)\n\n    def __getitem__(self, item: int) -&gt; datetime:\n        n_steps = item if item &gt;= 0 else len(self) + item\n        return_value = self._start + n_steps * self._step\n        if return_value not in self:\n            raise IndexError()\n        return return_value \n\n    def __str__(self):\n        return f\"Datetime Range [{self._start}, {self._end}) with step {self._step}\"\n\nClass DateTimeRange has methods that allows you to treat a date-range object like a list\n\nJust for illustration. Think methods in pandas can do this stuff\n\n__iter__ method - generator function that creates one element at a time, yields it to the caller, and allows the caller to process it\n\nExample creates datetime ranges instead of numeric ranges\n\n__len__ - find out the number of elements that are part of your range\n__getitem__- uses indexing syntax to retrieve entries from your objects\n__contains__- checks if an element is part of your range. T/F\n\ndivmod returns quotient and remainder.\n\nUsing these magic methods\nmy_range = DateTimeRange(datetime(2021,1,1), datetime(2021,12,1), timedelta(days=12)) #instantiate\nprint(my_range)              # __init__ or maybe __str__\nfor r in my_range:          # __iter__\n    do_something(r)\nlen(my_range)                # __len__\nmy_range[-2] in my_range    # __getitem__ (neg indexing), __contains__\n\nOthers\n\n__repr__ - Creates a string representation of the class object\n\nExample\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def __repr__(self):\n        return f\"Person(name='{self.name}', age={self.age})\"\n\nperson = Person(\"John\", 25)\nprint(person)\n#&gt; Person(name='John', age=25)\n\n__eq__ - Provides a method for comparing two class objects by their values.\n\nExample\nclass Person:\n  def __init__(self, age):\n    self.age = age\n\n  def __eq__(self, other):\n    return self.age == other.age\n\nalice = Person(18)\nbob = Person(19)\ncarl = Person(18)\n\nprint(alice == bob)\n#&gt; False\n\nprint(alice == carl)\n#&gt; True",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-inher",
    "href": "qmd/python-classes.html#sec-py-class-inher",
    "title": "Classes",
    "section": "Inheritance",
    "text": "Inheritance\n\n\nSome notes from this much more detailed example (article)\n\nShows how to combine different scripts  (aka modules) and diagram the hierarchies\nSome good debugging too\n\nRunning &lt;class&gt;.__mro__will show you the order of inheritance.\n\n\nInheritance models what is called an “is a” relationship. This means that when you have a Derived (aka subclass, child) class that inherits from a Base (aka super, parent) class, you created a relationship where Derived is a specialized version of Base.\nChild classes inherit all of their parent’s attributes and methods, but they can also define their own attributes and methods.\nCan override or extend parent class attributes and methods\nclass Manager(Employee):\n    pass\n\nm1 = Manager(\"aaa\", 13000)\n\nManager is the child class and Employee is the parent class (see top)\nChild classes don’t require a constructor method for an object to be created\n\nExtending the instance attributes of the parent class\nclass Manager(Employee):\n    def __init__(self, name, salary=0, department):\n        Employee.__init__(self, name, salary=0)\n        self.department = department\n\nContructor method for the child class with the new attribute, “department,” in the arguments.\nParent class (Employee) constructor method is called and a new attribute, department, is defined.\n\n\n\nsuper()\n\nAlternative way of extending instance attributes through inheritance\nExample\nclass Rectangle:\n    def __init__(self, length, width):\n        self.length = length\n        self.width = width\n    def area(self):\n        return self.length * self.width\n    def perimeter(self):\n        return 2 * self.length + 2 * self.width\nclass Square(Rectangle):\n    def __init__(self, length):\n        super().__init__(length, length)\nclass Cube(Square):\n    def surface_area(self):\n        face_area = super().area()\n        return face_area * 6\n    def volume(self):\n        face_area = super().area()\n        return face_area * self.length\n\n&gt;&gt;&gt; cube = Cube(3)\n&gt;&gt;&gt; cube.surface_area()\n54\n&gt;&gt;&gt; cube.volume()\n27\n\nClass Cube inherits from Square and extends the functionality of .area() (inherited from the Rectangle class through Square) to calculate the surface area and volume of a Cube instance Also notice that the Cube class definition does not have an .__init__(). Because Cube inherits from Square and .__init__() doesn’t really do anything differently for Cube than it already does for Square, you can skip defining it, and the .__init__() of the other child class (Square) will be called automatically.\nsuper(Square, self).__init__(length, length) is equivalent to calling super without parameters (see above example)\nUsing super(Square, self).area() in class Cube. Setting the 1st parameter to Square instead of Cube causes super() to start searching for a matching method (in this case, .area()) at one level above Square in the instance hierarchy, in this case Rectangle.\n\nIf Square had an .area method, but you wanted to use Rectangle’s instead, this would be a way to do that.\n\nNote another difference between using super() and using the class name (e.g. the first example) — “self” is NOT one of the args in super()\nExample: Child class of two separate hierarchies\n# Super class\nclass Rectangle:\n    def __init__(self, length, width, **kwargs):\n        self.length = length\n        self.width = width\n        super().__init__(**kwargs)\n    def area(self):\n        return self.length * self.width\n    def perimeter(self):\n        return 2 * self.length + 2 * self.width\n# Child class: Square class inherits from the Rectangle class\nclass Square(Rectangle):\n    def __init__(self, length, **kwargs):\n        super().__init__(length=length, width=length, **kwargs)\n# Child class: Cube class inherits from Square and also from Rectangle classes\nclass Cube(Square):\n    def surface_area(self):\n        face_area = super().area()\n        return face_area * 6\n    def volume(self):\n        face_area = super().area()\n        return face_area * self.length\n\n# Class (separate)\nclass Triangle: \n    def __init__(self, base, height, **kwargs): \n        self.base = base \n        self.height = height \n        super().__init__(**kwargs) \n    def tri_area(self): \n        return 0.5 * self.base * self.height\n\n# Inherits from a child class (and super class) and a class\nclass RightPyramid(Square, Triangle):\n    def __init__(self, base, slant_height, **kwargs):\n        self.base = base\n        self.slant_height = slant_height\n        kwargs[\"height\"] = slant_height\n        kwargs[\"length\"] = base\n        super().__init__(base=base, **kwargs)\n    def area(self):\n        base_area = super().area()\n        perimeter = super().perimeter()\n        return 0.5 * perimeter * self.slant_height + base_area\n    def area_2(self):\n        base_area = super().area()\n        triangle_area = super().tri_area()\n        return triangle_area * 4 + base_area\n\n&gt;&gt;&gt; pyramid = RightPyramid(base=2, slant_height=4)\n&gt;&gt;&gt; pyramid.area()\n20.0\n&gt;&gt;&gt; pyramid.area_2()\n20.0\n\nRightPyramid inherits from a child class (Square) and a class Triangle\n\nSince Square inherits from Rectangle, so does RightPyramid\nTriangle is a separate class not part of any hierarchy\n\nIf there’s an .area method in either of the classes, then super will search hierarchy of the class listed first (Square), then the hierarchy of the class listed second (Triangle)\n\nBest practice to make sure each class has different method names.\n\nEach class with an __init__ constructor gets a super().__init__() and **kwargs added to its args\n\nWhich is every class sans Cube. If Cube was inherited by a class, I think it would require an __init__ constructor and the super().__init__(**kwargs) expression.\nWithout doing this, calling .area_2() in RightPyramid will give us an AttributeError since .base and .height don’t have any values. (don’t completely understand this explanation)\n\nkwarg flow through super().__init__()\n\nIn RightPyramid __init__,  slant_height and base values are assigned to height and length keys in the kwargs dict\nsuper() passes base and the kwargs dict up to Square and Triangle\n\nTriangle uses height from kwargs and base\nSquare uses length from kwargs\n\nSquare passes length to Rectangle as values for both width and length\n\n\nAll classes now have the argument values necessary for their functions to work.\n\nNow RightPyramid can call those other classes’ methods (e.g. .area and .perimeter from Rectangle and tri_area from Triangle)\n\nI believe since every class has **kwargs arg in their super().__init__, each has every value in the kwarg dict even if they don’t need it.\n\nSo probably possible to add functions to those classes that would use those values\n\n\n\nExample: Using super with method other than __init__\nclass SalaryPolicy: \n    def __init__(self, weekly_salary): \n        self.weekly_salary = weekly_salary \n    def calculate_payroll(self): \n        return self.weekly_salary\n\nclass CommissionPolicy(SalaryPolicy): \n    def __init__(self, weekly_salary, commission): \n        super().__init__(weekly_salary) \n        self.commission = commission \n    def calculate_payroll(self): \n        fixed = super().calculate_payroll() \n        return fixed + self.commission\n\nIn CommissionPolicy’s calculate_payroll, super() accesses SalaryPolicy’s calculate_payroll method to get the weekly_salary value\n\n\n\n\n\nDiamond Problem\n\n\nAppears when you’re using multiple inheritance and deriving from two classes that have a common base class.\n\nThis can cause the wrong version of a method to be called.\ne.g. TemporarySecretary uses multiple inheritance to derive from two classes that ultimately also derive from Employee. This causes two paths to reach the Employee base class, which is something you want to avoid in your designs.\n\n\n\n\nMixin Class\n\nOperates the same as Inheritance, but since it only provides simple behavior(s), it is easy to reuse with other classes without causing problems\nExample: Take certain class attributes and create a dict\n# In representations.py\nclass AsDictionaryMixin:\n    def to_dict(self):\n        return {\n            prop: self._represent(value)\n            for prop, value in self.__dict__.items()\n            if not self._is_internal(prop)\n        }\n    def _represent(self, value):\n        if isinstance(value, object):\n            if hasattr(value, 'to_dict'):\n                return value.to_dict()\n            else:\n                return str(value)\n        else:\n            return value\n    def _is_internal(self, prop):\n        return prop.startswith('_')\n\nto_dict is a dictionary comprehension\n\nmydict = {key:val for key, val in dict}\nReturns a dict with key:value (e.g. property (aka attribute):value) pairs from a class’s __dict__ if the property (prop) doesn’t have an underscore\n\n_represent makes sure the “value” is a value and not object\n_is_interna1 checks whether the attribute has an underscore in the name\n\nApply the Mixin class to any class the same way as using Inheritance\nclass Employee(AsDictionaryMixin):\n    def __init__(self, id, name, address, role, payroll):\n        self.id = id\n        self.name = name\n        self.address = address\n        self._role = role\n        self._payroll = payroll\n\nAsDicitionaryMixin is used as an arg to the class\n“self._role” and “self._payroll” have underscores which tells to_dict not to include them in the resulting dictionary\nNot important to using a mixin class but note that “address” is from the Address class via composition (see below). Therefore the Address class would also need to inherit AsDictionaryMixin for this to work\n\nUtilize\nimport json\n\ndef print_dict(d):\n    print(json.dumps(d, indent=2))\n\nfor employee in EmployeeDatabase().employees:\n    print_dict(employee.to_dict())\n\nprint_dict takes the dict output of employee._to_dict and converts it to a json format",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-comp",
    "href": "qmd/python-classes.html#sec-py-class-comp",
    "title": "Classes",
    "section": "Composition",
    "text": "Composition\n\n\nNotes from Inheritance and Composition: A Python OOP Guide\nComposition models a “has a” relationship. In composition, a class known as composite contains an object of another class known to as component.\nComposition design is typically more flexible than inheritance and is preferable to Inheritance\n\nPrevents “class explosion”\n\nFor complex projects, too many classes can lead to conflicts and errors because of the inevitable complex network of classes that are connected to each other.\nYou change your program’s behavior by providing new components that implement those behaviors instead of adding new classes to your hierarchy.\n\nOnly loose class connections in composition\n\nChanges to the component class rarely affect the composite class, and changes to the composite class never affect the component class\n\n\ntl;dr\n\nClasses are written in different py scripts and imported as “modules” in another script.\nAttribute(s) from a component class (e.g. Address) are used in the composite class (e.g. Employee)\nThen a composite class attribute object is assigned to the instantiated composite class’s empty attribute\n\nThis is the magic. One class’s attribute can be used as input into another class’s attribute without being tightly coupled to that other class (aka inheritance).\n\nThat input isn’t a value. It’s class type object.\nSee Utilize code block below\n\n\n\n\n\nComposition Through __init__ Attributes\n\nExample:\n# In contacts.py\n# Component class\nclass Address:\n    def __init__(self, street, city, state, zipcode, street2=''):\n        self.street = street\n        self.street2 = street2\n        self.city = city\n        self.state = state\n        self.zipcode = zipcode\n\n# In employees.py\n# Composite class\nclass Employee:\n    def __init__(self, id, name):\n        self.id = id\n        self.name = name\n        self.address = None\n\n# ManagerRole and SalaryPolicy are classes from different modules\nclass Manager(Employee, ManagerRole, SalaryPolicy):\n    def __init__(self, id, name, weekly_salary):\n        SalaryPolicy.__init__(self, weekly_salary)\n        super().__init__(id, name)\n\nYou would import these two modules into a third script and do stuff (see next code block)\nYou initialize the Address.address attribute to “None” for now to make it optional, but by doing that, you can now assign an Address to an Employee.\n\ni.e. the attributes of an Address instance from its __init__ are now available to be assigned to a Employee instance.\n\nManager is a child class of multiple other classes (inheritance) including Employee and therefore gets an .address attribute\n\nAside: ManagerRole doesn’t have an __init__ (i.e. no attributes), so I’m not sure why super() is used here\n\nwhy not just use Employee.__init__?\nI would’ve thought that ManagerRole would’ve required the same inputs as Employee, so super() is used here to cover both at the same time.\n\nBut that’s not the case, MangerRole is there just for it’s method and not it’s attributes\n\nDoes being able to use ManageRole methods require super() (i.e. necessary for Inheritance)?\n\n\nUtilize\nmanager = employees.Manager(1, 'Mary Poppins', 3000)\nmanager.address = contacts.Address(\n    '121 Admin Rd', \n    'Concord', \n    'NH', \n    '03301'\n)\n# ... create other intances of different jobs in the company\n\n# guess this would be like a json\nemployees = [\n    manager,\n    secretary,\n    sales_guy,\n    factory_worker,\n    temporary_secretary,\n]\n\n# do work with the list class objs\nproductivity_system = productivity.ProductivitySystem()\nproductivity_system.track(employees, 40)\n\nThe Address class instance is assigned to the .address attribute of the Manager instance (which gets its .address attribute from Employee)\n\n\n\n\n\nComposition Through a Function Argument\n\nExample:\n# In hr.py\nclass PayrollSystem:\n    def calculate_payroll(self, employees):\n        print('Calculating Payroll')\n        print('===================')\n        for employee in employees:\n            print(f'Payroll for: {employee.id} - {employee.name}')\n            print(f'- Check amount: {employee.calculate_payroll()}')\n            if employee.address:\n                print('- Sent to:')\n                print(employee.address)\n            print('')\n\npayroll_system = hr.PayrollSystem()\npayroll_system.calculate_payroll(employees)\n\nThe input for calculate_payroll, employees, is a list of instantiated Employee class objects in the previous code chunk.\nName of the class is iterated and represents each instance\n\nHas attributes and methods available\n\n\n\n\n\nOther Module Classes Used as Attributes\n\nExample:\n# In employees.py\nfrom productivity import ProductivitySystem\nfrom hr import PayrollSystem\nfrom contacts import AddressBook\nclass EmployeeDatabase:\n    def __init__(self):\n        self._employees = [\n            {\n                'id': 1,\n                'name': 'Mary Poppins',\n                'role': 'manager'\n            },\n            {\n                'id': 2,\n                'name': 'John Smith',\n                'role': 'secretary'\n            }\n        ]\n        self.productivity = ProductivitySystem()\n        self.payroll = PayrollSystem()\n        self.employee_addresses = AddressBook()\n    @property\n    def employees(self):\n        return [self._create_employee(**data) for data in self._employees]\n    def _create_employee(self, id, name, role):\n        address = self.employee_addresses.get_employee_address(id)\n        employee_role = self.productivity.get_role(role)\n        payroll_policy = self.payroll.get_policy(id)\n        return Employee(id, name, address, employee_role, payroll_policy)\n\nProductivitySystem, PayrollSystem, AddressBook are classes imported from various modules\nAs attributes, these classes’ methods are used in the _create_employee function\n\nReturn invokes the Employee class with values obtained by the various class methods\nEmployee class (not shown in this block) is already present in this module so it doesn’t have to be imported.",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-dec",
    "href": "qmd/python-classes.html#sec-py-class-dec",
    "title": "Classes",
    "section": "Decorators",
    "text": "Decorators\n\nThey can add additional features to a function\n\nUseful because you don’t have to refactor downstream code\n\nFunctions that take a function as input\n\nSee use cases throughout note and check bkmks\n\n@ is placed above a “decorated” function\n\nExample\n@decorator_1\ndef temperature():\nreturn temp\n\nCalling temperature() is actually calling decorator_1(temperature())\n\n\nExample: add additional features to a function\n\n\nAdds a timer to a function\n\nExample: Multiple decorators for a function\n@log_execution\n@timing\ndef my_function(x, y):\n    time.sleep(1)\n    return x + y\nSee Custom Examples for the “log_execution” decorator\n\n\nProperty\n\nArguments\nproperty(fget=None, fset=None, fdel=None, doc=None)\nBuilt-in decorator\nConstitutes a family of decorators\n\n@property: Declares the method as a property.\n\nfget - function to get value of the attribute\n\n@.setter: Specifies the setter method for a property that sets the value to a property.\n\nfset - function to set value of the attribute\nmust have the value argument that can be used to assign to the underlying private attribute\n\n@.deleter: Specifies the delete method as a property that deletes a property.\n\nfdel - function to delete the attribute\nmust have the value argument that can be used to assign to the underlying private attribute\n\n\nExample\n# Using @property decorator\nclass Celsius:\ndef __init__(self, temperature=0):\n    self._temperature = temperature\ndef to_fahrenheit(self):\n    return (self._temperature * 1.8) + 32\n\n\n# decorators\n# attribute getter\n@property\ndef temperature(self):\n    print(\"Getting value...\")\n    return self._temperature\n\n# also adds constraint to the temperature input\n@temperature.setter\ndef temperature(self, value):\n    print(\"Setting value...\")\n    if value &lt; -273.15:\n        raise ValueError(\"Temperature below -273 is not possible\")\n    self._temperature = value\n\n@temperature.deleter\ndef temperature(self, value):\n    print(\"Deleting value...\")\n    del self._temperature\n\n&gt;&gt; human = Celsius(37)\nSetting value...\n&gt;&gt; print(human.temperature)\nGetting value...\n37\n&gt;&gt; print(human.to_fahrenheit())\nGetting value...\n98.60000000000001\n&gt;&gt; del human.temperature\nDeleting value...\n&gt;&gt; coldest_thing = Celsius(-300)\nSetting value...\nTraceback (most recent call last):\nFile \"\", line 29, in\nFile \"\", line 4, in __init__\nFile \"\", line 18, in temperature\nValueError: Temperature below -273 is not possible\n\n.deleter didn’t work for me and neither did the conditional. Don’t my python version or what\n\n\n\n\nClass Method\n\nMethod that is bound to the class and not the object (aka instance) of the class.\nInstance attributes cannot be referred to with this method\nCan modify the class state that applies across all instances of the class\nUse Cases\n\nTo create new instances of the class without going trhough its normal __init__\nTo create a class instance that requires some async calls when instantiated, since __init__ cannot be async.\n\nStarts with a “classmethod” decorator\nclass MyClass:   \n@classmethod\ndef classmethod(cls):\n    return 'class method called', cls\nCalling a class method vs an instance method\n# calling a class method\n# no instantiation\nMyClass.classmethod()\n\n# calling an instance method\n# instantiates object first\nobject = MyClass()\nobject.method()\n\n\n\nStatic Method\n\nMethod bound to the class instance, not the class itself.\nDoes not take the class as a parameter.\nIt cannot access or modify the class at all.\nStarts with a “staticmethod” decorator\nDoesn’t have any access to what the class is—it’s basically just a function, called syntactically like a method, but without access to the object and its internals (fields and other methods), which classmethod does have.\nSee SO thread for discussion on the differences between the static and class decorators and their uses\nclass Person:\n  def __init__(self, name, age):\n    self.name = name\n    self.age = age\n\n  @staticmethod\n  def isAdult(age):\n    return age &gt; 18\n\nisAdult(age) function doesn’t require the usual self argument, so it couldn’t reference the class even if it wanted to.\nMost often used as utility functions that are completely independent of a class’s state\nSee classmethod decorator for details on calling this method\n\n\n\n\nCustom Examples\n\nAlso see Code, Optimization &gt;&gt; Python &gt;&gt; Profile decorator\nUsing functools and decorators\nfrom functools import singledispatch\n\n@singledispatch\ndef process_data(data):\nraise NotImplementedError(f\"Type {type(data)} is unsupported\")\n\n@process_data.register\ndef process_dict(data: dict):\nprint(\"Dict is processed\")\n\n@process_data.register\ndef process_list(data: list):\nprint(\"List is processed\")\nMultiprocessing Function Execution Time Limiter\nimport multiprocessing\nfrom functools import wraps\n\nclass TimeExceededException(Exception):\n    pass\n## PART 1\n    def function_runner(*args, **kwargs):\n        \"\"\"Used as a wrapper function to handle\n        returning results on the multiprocessing side\"\"\"\n\n        send_end = kwargs.pop(\"__send_end\")\n        function = kwargs.pop(\"__function\")\n        try:\n            result = function(*args, **kwargs)\n        except Exception as e:\n            send_end.send(e)\n            return\n        send_end.send(result)\n\n    @parametrized\n    def run_with_timer(func, max_execution_time):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            recv_end, send_end = multiprocessing.Pipe(False)\n            kwargs[\"__send_end\"] = send_end\n            kwargs[\"__function\"] = func\n\n            ## PART 2\n            p = multiprocessing.Process(target=function_runner, args=args, kwargs=kwargs)\n            p.start()\n            p.join(max_execution_time)\n            if p.is_alive():\n                p.terminate()\n                p.join()\n                raise TimeExceededException(\"Exceeded Execution Time\")\n            result = recv_end.recv()\n\n            if isinstance(result, Exception):\n                raise result\n\n            return result\n\n        return wrapper\n\nFrom Limiting Python Function Execution Time with a Parameterized Decorator via Multiprocessing\n\nRetry (e.g. for an API)\nimport time\nfrom functools import wraps\n\ndef retry(max_tries=3, delay_seconds=1):\n    def decorator_retry(func):\n        @wraps(func)\n        def wrapper_retry(*args, **kwargs):\n            tries = 0\n            while tries &lt; max_tries:\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    tries += 1\n                    if tries == max_tries:\n                        raise e\n                    time.sleep(delay_seconds)\n        return wrapper_retry\n    return decorator_retry\n\n@retry(max_tries=5, delay_seconds=2)\ndef call_dummy_api():\n    response = requests.get(\"https://jsonplaceholder.typicode.com/todos/1\")\n    return response\n\nTries to get an API response. If it fails, we retry the same task 5 times. Between each retry, we wait for 2 seconds.\n\nCache Function Results\ndef memoize(func):\n    cache = {}\n    def wrapper(*args):\n        if args in cache:\n            return cache[args]\n        else:\n            result = func(*args)\n            cache[args] = result\n            return result\n    return wrapper\n\n@memoize\ndef fibonacci(n):\n    if n &lt;= 1:\n        return n\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)\n\nUses a dictionary, stores the function args, and returns values. When we execute this function, the decorated will check the dictionary for prior results. The actual function is called only when there’s no stored value before.\nUsing a dictionary to hold previous execution data is a straightforward approach. However, there is a more sophisticated way to store caching data. You can use an in-memory database, such as Redis.\n\nLogging (e.g. ETL pipeline)\nimport logging\nimport functools\n\nlogging.basicConfig(level=logging.INFO)\n\ndef log_execution(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        logging.info(f\"Executing {func.__name__}\")\n        result = func(*args, **kwargs)\n        logging.info(f\"Finished executing {func.__name__}\")\n        return result\n    return wrapper\n\n@log_execution\ndef extract_data(source):\n    # extract data from source\n    data = ...\n    return data\n@log_execution\ndef transform_data(data):\n    # transform data\n    transformed_data = ...\n    return transformed_data\n@log_execution\ndef load_data(data, target):\n    # load data into target\n    ...\n\ndef main():\n    # extract data\n    data = extract_data(source)\n    # transform data\n    transformed_data = transform_data(data)\n    # load data\n    load_data(transformed_data, target)\n\noutput\nINFO:root:Executing extract_data\nINFO:root:Finished executing extract_data\nINFO:root:Executing transform_data\nINFO:root:Finished executing transform_data\nINFO:root:Executing load_data\nINFO:root:Finished executing load_data\n\nEmail Notification\nimport smtplib\nimport traceback\nfrom email.mime.text import MIMEText\ndef email_on_failure(sender_email, password, recipient_email):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except Exception as e:\n                # format the error message and traceback\n                err_msg = f\"Error: {str(e)}\\n\\nTraceback:\\n{traceback.format_exc()}\"\n\n                # create the email message\n                message = MIMEText(err_msg)\n                message['Subject'] = f\"{func.__name__} failed\"\n                message['From'] = sender_email\n                message['To'] = recipient_email\n\n                # send the email\n                with smtplib.SMTP_SSL('smtp.gmail.com', 465) as smtp:\n                    smtp.login(sender_email, password)\n                    smtp.sendmail(sender_email, recipient_email, message.as_string())\n\n                # re-raise the exception\n                raise\n\n        return wrapper\n\n    return decorator\n\n@email_on_failure(sender_email='your_email@gmail.com', password='your_password', recipient_email='recipient_email@gmail.com')\ndef my_function():\n    # code that might fail",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-opt",
    "href": "qmd/python-classes.html#sec-py-class-opt",
    "title": "Classes",
    "section": "Optimizations",
    "text": "Optimizations\n\nMisc\n\nNotes from How to Write Memory-Efficient Classes in Python\n\nUse __slots__ when creating large numbers of instances\n\n\nBy default, Python classes store their instance attributes in a private dictionary (__dict__), which dictionary allows you to add, modify, or delete the class attributes at runtime, but it creates a large memory burden when substantial numbers of instances are created.\nSlots reserves only a fixed amount of space for the specified attributes directly in each instance, instead of using the default dictionary.\nAny attempt to assign an attribute that is not listed in __slots__ will raise an AttributeError.\n\nThis can help prevent creating accidental attributes due to typos, but it can also be restrictive if you need to add additional attributes later in development.\n\nExample:\nclass Ant:\n  __slots__ = ['worker_id', 'role', 'colony']\n\n  def __init__(self, worker_id, role, colony):\n      self.worker_id = worker_id\n      self.role = role\n      self.colony = colony\n\nLazy Initialization for memory intensive operations\n\nScenario: Data Loading in an app\n\nUser wants to look at map or examine features before analyzing data.\nWithout Lazy Loading, the entire dataset is loaded upfront, leading to slower startup and potentially exceeding memory limits.\n\nCaching is also a good idea if you’re performing the same intensive computation more than once.\nExample: Lazy Loader\nfrom functools import cached_property\n\nclass DataLoader:\n\n    def __init__(self, path):\n        self.path = path\n\n    @cached_property\n    def dataset(self):\n        # load the dataset here\n        # this will only be executed once when the dataset property is first accessed\n        return self._load_dataset()\n\n    def _load_dataset(self):\n        print(\"Loading the dataset...\")\n\n        # load a big dataset here\n        df = pd.read_csv(self.path)\n        return df\n\nclass DataProcessor:\n    def __init__(self, path):\n          self.path = path\n        self.data_loader = DataLoader(self.path)\n\n    def process_data(self):\n        dataset = self.data_loader.dataset\n        print(\"Processing the dataset...\")\n        # Perform complex data processing steps on the loaded dataset\n        ...\n\n# instantiate the DataLoader class\npath = \"/[path_to_dataset]/mnist.csv\"\n\n# instantiate the DataProcessor class with the data file path\n# 👉 no data will be loaded at this stage! ✅\nprocessor = DataProcessor(path)\n\n# trigger the processing\nprocessor.process_data()  # The dataset will be loaded and processed when needed\n\nUse generators to reduce memory usage of loops\n\nSee Python, General &gt;&gt; Loops &gt;&gt; Generators\n\nAlso, in Loops &gt;&gt; Misc, there’s a chart that shows the memory benefits to using a generator vs list comprehension.",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-examp",
    "href": "qmd/python-classes.html#sec-py-class-examp",
    "title": "Classes",
    "section": "Examples",
    "text": "Examples\n\nRead file chunks, process, and write to parquet\nimport pandas as pd\n\nclass PandasChunkProcessor:\n    def __init__(self, filepath, chunk_size, verbose=True):\n        self.filepath = filepath\n        self.chunk_size = chunk_size\n        self.verbose = verbose\n\n    def process_data(self):\n        for chunk_id, chunk in enumerate(pd.read_csv(self.filepath, chunksize=self.chunk_size)):\n            processed_chunk = self.process_chunk(chunk)\n            self.save_chunk(processed_chunk, chunk_id)\n\n    def process_chunk(self, chunk):\n        # process each chunk of data\n        processed_chunk = processing_function(chunk)\n        return processed_chunk\n\n    def save_chunk(self, chunk, chunk_id):\n        # save each processed chunk to a parquet file\n        chunk_filepath = f\"./output_chunk_{chunk_id}.parquet\"\n        chunk.to_parquet(chunk_filepath)\n        if self.verbose:\n            print(f\"saved {chunk_filepath}\")\nTransforms variables by logging, can add 1 if necessary, back-transform\nfrom sklearn.base import BaseEstimator, TransformerMixin \nfrom sklearn.preprocessing import PowerTransformer \n\nclass CustomLogTransformer(BaseEstimator, TransformerMixin): \n    def __init__(self): \n        self._estimator = PowerTransformer()  # init a transformer \n    def fit(self, X, y=None): \n        X_copy = np.copy(X) + 1  # add one in case of zeroes \n        self._estimator.fit(X_copy) \n        return self \n    def transform(self, X): \n        X_copy = np.copy(X) + 1 \n        return self._estimator.transform(X_copy)  # perform scaling \n    def inverse_transform(self, X): \n        X_reversed = self._estimator.inverse_transform(np.copy(X)) \n        return X_reversed - 1  # return subtracting 1 after inverse transform\nPredictions for a Huggingface classifer\nimport sys\nfrom transformers import pipeline\nfrom typing import List\nimport numpy as np\nfrom time import perf_counter\nimport logging\n\n# Set up logger\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlog = logging.getLogger(__name__)\n\nclass ZeroShotTextClassifier:\n    \"\"\"Class with only class methods\"\"\"\n    # Class variable for the model pipeline\n    classifier = None\n    @classmethod\n    def load(cls):\n        # Only load one instance of the model\n        if cls.classifier is None:\n            # Load the model pipeline.\n            # Note: Usually, this would also download the model.\n            # But, we download the model into the container in the Dockerfile\n            # so that it's built into the container and there's no download at\n            # run time (otherwise, each time we'll download a 1.5GB model).\n            # Loading still takes time, though. So, we do that here.\n            # Note: You can use a GPU here if needed.\n            t0 = perf_counter()\n            cls.classifier = pipeline(\n                \"zero-shot-classification\", model=\"facebook/bart-large-mnli\"\n            )\n            elapsed = 1000 * (perf_counter() - t0)\n            log.info(\"Model warm-up time: %d ms.\", elapsed)\n    @classmethod\n    def predict(cls, text: str, candidate_labels: List[str]):\n        assert len(candidate_labels) &gt; 0\n        # Make sure the model is loaded\n        cls.load()\n        # For the tutorial, let's create\n        # a custom object from the huggingface prediction.\n        # Our prediction object will include the label and score\n        t0 = perf_counter()\n        # pylint: disable-next=not-callable\n        huggingface_predictions = cls.classifier(text, candidate_labels)\n        elapsed = 1000 * (perf_counter() - t0)\n        log.info(\"Model prediction time: %d ms.\", elapsed)\n        # Create the custom prediction object.\n        max_index = np.argmax(huggingface_predictions[\"scores\"])\n        label = huggingface_predictions[\"labels\"][max_index]\n        score = huggingface_predictions[\"scores\"][max_index]\n        return {\"label\": label, \"score\": score}\nPayroll System\n\nEmployees\n# In employees.py\nfrom hr import (\n    SalaryPolicy,\n    CommissionPolicy,\n    HourlyPolicy\n)\nfrom productivity import (\n    ManagerRole,\n    SecretaryRole,\n    SalesRole,\n    FactoryRole\n)\nclass Employee:\n    def __init__(self, id, name):\n        self.id = id\n        self.name = name\nclass Manager(Employee, ManagerRole, SalaryPolicy):\n    def __init__(self, id, name, weekly_salary):\n        SalaryPolicy.__init__(self, weekly_salary)\n        super().__init__(id, name)\nclass Secretary(Employee, SecretaryRole, SalaryPolicy):\n    def __init__(self, id, name, weekly_salary):\n        SalaryPolicy.__init__(self, weekly_salary)\n        super().__init__(id, name)\nclass SalesPerson(Employee, SalesRole, CommissionPolicy):\n    def __init__(self, id, name, weekly_salary, commission):\n        CommissionPolicy.__init__(self, weekly_salary, commission)\n        super().__init__(id, name)\nclass FactoryWorker(Employee, FactoryRole, HourlyPolicy):\n    def __init__(self, id, name, hours_worked, hour_rate):\n        HourlyPolicy.__init__(self, hours_worked, hour_rate)\n        super().__init__(id, name)\nclass TemporarySecretary(Employee, SecretaryRole, HourlyPolicy):\n    def __init__(self, id, name, hours_worked, hour_rate):\n        HourlyPolicy.__init__(self, hours_worked, hour_rate)\n        super().__init__(id, name)\nProductivity\n# In productivity.py\nclass ProductivitySystem:\n    def track(self, employees, hours):\n        print('Tracking Employee Productivity')\n        print('==============================')\n        for employee in employees:\n            result = employee.work(hours)\n            print(f'{employee.name}: [{result}]')\n        print('')\nclass ManagerRole:\n    def work(self, hours):\n        return f'screams and yells for [{hours}] hours.'\nclass SecretaryRole:\n    def work(self, hours):\n        return f'expends [{hours}] hours doing office paperwork.'\nclass SalesRole:\n    def work(self, hours):\n        return f'expends [{hours}] hours on the phone.'\nclass FactoryRole:\n    def work(self, hours):\n        return f'manufactures gadgets for [{hours}] hours.'\nHR\n# In hr.py\nclass PayrollSystem:\n    def calculate_payroll(self, employees):\n        print('Calculating Payroll')\n        print('===================')\n        for employee in employees:\n            print(f'Payroll for: {employee.id} - {employee.name}')\n            print(f'- Check amount: {employee.calculate_payroll()}')\n            print('')\nclass SalaryPolicy:\n    def __init__(self, weekly_salary):\n        self.weekly_salary = weekly_salary\n    def calculate_payroll(self):\n        return self.weekly_salary\nclass HourlyPolicy:\n    def __init__(self, hours_worked, hour_rate):\n        self.hours_worked = hours_worked\n        self.hour_rate = hour_rate\n    def calculate_payroll(self):\n        return self.hours_worked * self.hour_rate\nclass CommissionPolicy(SalaryPolicy):\n    def __init__(self, weekly_salary, commission):\n        super().__init__(weekly_salary)\n        self.commission = commission\n    def calculate_payroll(self):\n        fixed = super().calculate_payroll()\n        return fixed + self.commission",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-general.html",
    "href": "qmd/python-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-misc",
    "href": "qmd/python-general.html#sec-py-gen-misc",
    "title": "General",
    "section": "",
    "text": "Tools (see article, article for installation and usage)\n\nruff - linter and sorts imports\n\nFast, sensible default settings, focuses on more important things out of the box, and has less legacy burden\n\npydocstring - tool for checking compliance with Python docstring conventions\nblack - code formatter\nisort - sorts your imports\npytest, pytest-watch - unit tests\ncommitizen - guides you through a series of steps to create a commit message that conforms to the structure of a Conventional Commit\nnbQA - linting in jupyter notebooks\nmypy - type checker; good support and docs\npylance - checks type hinting in VSCode (see Functions &gt;&gt; Documentation &gt;&gt; Type Hinting)\ndoit - task runner; {targets}-like tool; tutorial\npre-commit - specify which checks you want to run against your code before committing changes to your git repository\nREADME templates - link\n\nPut as much config as possible into pyproject.toml. A lot of configurations tools will happily read from it, and it will give you one source of truth.\nAn underscore _ at the beginning is used to denote private variables in Python.\ndef set_temperature(self, value):\n        if value &lt; -273.15:\n            raise ValueError(\"Temperature below -273.15 is not possible.\")\n        self._temperature = value\n\nyou can still access “_temperature” but it’s just meant for internal use by the class and the underscore indicates this\n\n{{warnings::warnings.filterwarnings(‘ignore’)}}\nsys.getsizeof(obj) to get the size of an object in memory.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#terms",
    "href": "qmd/python-general.html#terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nclasses - code template for creating objects, we can think of it as a blueprint. It describes the possible states and behaviors that every object of a certain type could have.\nobject - data structure storing information about the state and behavior of a certain entity and is an instance of a class\nstub file - a file containing a skeleton of the public interface of that Python module, including classes, variables, functions – and most importantly, their types. (Source)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#base",
    "href": "qmd/python-general.html#base",
    "title": "General",
    "section": "Base",
    "text": "Base\n\nInfo method\n\nX.info()\nRemove an object: del\nCheck object type\n\ntype() : outputs the type of an object\nisinstance() : outputs type and inheritance of an object\nSee article for details on differences\n\nImport Libraries\nimport logging\nimport bentoml\nfrom transformers import (\n    SpeechT5Processor,\n    SpeechT5ForTextToSpeech,\n    SpeechT5HifiGan,\n    WhisperForConditionalGeneration,\n    WhisperProcessor,\n)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-fund",
    "href": "qmd/python-general.html#sec-py-gen-fund",
    "title": "General",
    "section": "Fundamentals",
    "text": "Fundamentals\n\nSlicing\n\nFormat my_list[start:stop:step]\n\n** start value is inclusive and the end value is exclusive\n\n1 element and more than 1 element\n\"Python\"[0] # P\n\"Python\"[0:1] # P\n\"Python\"[0:5] # Pytho\nillustrates how when using a range, the last element is exclusive\nNegative indexing my_list[0:-1]\n\nEverything but the last object\n\nSkip every second element\nmy_list = list(\"Python\")\nmy_list[0:len(my_list):2]\n&gt;&gt; ['P', 't', 'o']\nstart at 0, end at len(my_list), step = 2\nShortcuts\nmy_list[0:-1] == my_list[:-1]\nmy_list[0:len(my_list):2] == my_list[::2]\n\"Python\"[::-1] == \"Python\"[-1:-7:-1]\n\nDefaults\n\n0 for the start value\nlen(list) for the stop value\n1 for the step value\n\nDefaults for negative step value\n\n-1 for the start value\n-len(list) - 1 for the stop value\n\n\nAlias vs new object\nb = a # alias\nb = a[:] # new object\n\nWith the alias, changes to a will happen to b as well\n\nCommon use cases\n\n\n\n\n\n\n\nEvery element but the first and the last one\n[1:-1]\n\n\nEvery element in reverse order\n[::-1]\n\n\nEvery element but the first and the last one in reverse order\n[-2:0:-1]\n\n\nEvery second element but the first and the last one in reverse order\n[-2:0:-2]\n\n\n\nUsing slice function\nsequence = list(\"Python\")\nmy_slice = slice(None, None, 2) # equivalent to [::2]\nindices = my_slice.indices(len(sequence))\n&gt;&gt; (0, 6, 2)\n\nShows start = 0, stop = 6, step = 2\n\n\n\n\nF-Strings\n\nParameterize with {}\n&gt;&gt; x = 5\n&gt;&gt; f\"One icecream is worth [{x}]{style='color: #990000'} dollars\"\n'One icecream is worth 5 dollars'\n! - functions\n\n!r — Shows the string delimiter, calls the repr() method.\n\nrepr’s goal is to be unambiguous and str’s is to be readable. For example, if we suspect a float has a small rounding error, repr will show us while str may not\n\n!a — Shows the Ascii for the characters.\n!s — Converts the value to a string.\n\nGuessing this the str() method (see !r for details)\n\n\nfood2brand = \"Mcdonalds\"\nfood2 = \"French fries\"\nf\"I like eating {food2brand} {food2!r}\"\n\"I like eating Mcdonalds 'French fries'\"\nChange format with “:”\n&gt;&gt; import datetime\n&gt;&gt; date = datetime.datetime.utcnow()\n&gt;&gt; f\"The date is {date:%m-%Y %d}\"\n'The date is 02-2022 15'\n\n\n\nOperators\n\n(docs)\nExponential: 5**3\nInteger division: 5//3\nModulo: 5%3\nIdentity: is\nx = 5\ny = 3\nprint(\"The result for x is y is\", x is y)\nThe result for x is y is false\n\nThink you can also use == here too\n\nLogical: and and or\nprint(\"The result for 5 &gt; 3 and 6 &gt; 8 is\", 5 &gt; 3 and 6 &gt; 8)\nprint(\"The result for 5 &gt; 3 or 6 &gt; 8 is\", 5 &gt; 3 or 6 &gt; 8)\nThe result for 5 &gt; 3 and 6 &gt; 8 is False\nThe result for 5 &gt; 3 or 6 &gt; 8 is True\nSubset: in and not in\nprint(\"Is the number 3 in the list [1,2,3]?\", 3 in [1,2,3])\nIs the number 3 in the list [1,2,3]? True\n\nprint(\"Is the number 3 not in the list [1,2,3]?\", 3 not in [1,2,3])\nIs the number 3 not in the list [1,2,3]? False\nAssignment",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-dattyp",
    "href": "qmd/python-general.html#sec-py-gen-dattyp",
    "title": "General",
    "section": "Types",
    "text": "Types\n\nScalars\n\nCreate scalars by subsetting a list\ninputs = [1, 0.04, 0.9]\n# 1 numeric \nrmse = inputs[0] # rmse = 1 and is type 'float'\n# multiple numerics\nrmse, mape, rsq = inputs\n\nTuples\n\nLists are mutable and tuples are not\n\ni.e. we can add or remove elements to a list after we create it but we cannot do such thing to a tuple\n\nSyntax: name_of_tuple = (a, b)\n\nLists\n\nCreate list of objects (e.g. floats)\nacc_values = [rmse, mape, rsq]\n\nalt method: asterisk-comma notation\n*acc_names, = \"RMSE\", \"MAPE\", \"R-SQ\"\n\nasterisk is “unzipping operator”\n\n\nMake a copy\nold_list = [2, 3, 4]\nnew_list = list(old_list)\n\nDictionaries\n\n** if creating a simple dict, more performant to use curly braces **\n\nAvoid d = dict(1=1, x='x')\n\nJoin 2 dicts -  d.update(d2)\n\nIf d and d2 share keys, d2’s values for those keys will be used\n\nAccess a value from a key: sample_dict['key_name']\nMake a copy\nold_dict = {stuff: 2, more_stuff: 3}\nnew_dict = dict(old_dict)\nConvert list of tuples to a dict\nacc_dict = dict(acc_list)\n\nzip creates lists of tuples (See Loops &gt;&gt; zip section)\n\nAdd key, value pair to a dict\ntransaction_data['user_address'] = '221b Baker Street, London - UK'\n# or\ntransaction_data.update(user_address='221b Baker Street, London - UK')\nUnpack dict into separate tuples for key:value pairs\nrmse, mape, rsq = acc_dict.items()\nrmse\n('RMSE', 1)\n\n** fastest way to iterate over both keys and values in a dict **\ncan also use zip to unpack pairs into a list (see loops &gt;&gt; zip)\n\nUnpack dict into separate lists for keys and values\nacc_keys = list(acc_dict.keys()) \nacc_values = list(acc_dict.values())\n\n** fastest way to iterate over a dict’s keys or values **\n\nUnpack values from dicts into separate scalars\nrmse, mape, rsq = acc_dict.values()\nrmse\n1\nPull the value for a key (e.g. k) or return the default value - d.get(k, default)\n\nDefault is “None”. I think this can be set with d.setdefault(k, default)\n\nCheck for specific key (logical)\n‘send_currency’ in transaction_data\n‘send_currency’ in transaction_data.keys()\n‘send_currency’ not in transaction_data.keys()\n\nLike %in% in R\n\nCheck for specific value (logical)\n‘GBP’ in transaction_data.values()\nCheck for key, value pair\n(‘send_currency’, ‘GBP’) in transaction_data.items()\nPretty printing of dictionaries\n    _ = [print(k, \":\", f'{v:.1f}') for k,v in acc_dict.items()]\n    RMSE : 1.00\n    MAPE : 0.04\n    R-sq : 0.90\n\nfor-in loop format (see Loops &gt;&gt; Comprehension)\nprint returns “none” for each key:value at the bottom of the output for some reason. Assigning the print statement to a variable fixes it.\n\ndefaultdict\n\nCreates a key from a list element and groups the properties into a list of values where the value may also be a dict.\nFrom {{collections}}\nAlso see\n\nPybites video\nJSON &gt;&gt; Python &gt;&gt; Example: Parse Nested JSON into a dataframe\n\n\n\nSets\n\nIf performing set logic, always more performant to use sets instead of dicts or lists\n\n\nIf using numpy/pandas, using the .unique() syntax is more efficient for arrays/series’ with numeric values\nIf using strings, it’s more efficient to use list(set(my_array))\n\n\nStrings\n\nOperators\nOperator Description\n%d Signed decimal integer\n%u unsigned decimal integer\n%c Character\n%s String\n%f Floating-point real number\nExample\n\nname = \"india\"\nage = 19\nmarks = 20.56\nstring1 = 'Hey %s' % (name)\nprint(string1)\nstring2 = 'my age is %d' % (age)\nprint(string2)\nstring3= 'Hey %s, my age is %d' % (name, age)\nprint(string3)\nstring3= 'Hey %s, my subject mark is %f' % (name, marks)\nprint(string3)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-pip",
    "href": "qmd/python-general.html#sec-py-gen-pip",
    "title": "General",
    "section": "pip",
    "text": "pip\n\nLooks for packages on https://pypi.org, downloads, and installs it\nMisc\n\nIf you installed python using the app-store, replace python with python3.\nDon’t use sudo to install libraries, since it will install things outside of the virtual environment.\nNor should you use “–user”, since it’s made to install things outside of the virtual environment.\nDon’t mix pip, venv and Anaconda. Avoid Anaconda if you can. If you have to use Anaconda, don’t use pip and venv. Limit yourself to Anaconda tools.\nIf you get SSL errors (common if you are in a hotel or a company network) use the –trusted-host pypi.org –trusted-host files.pythonhosted.org options with pip to work around the problem.\n\ne.g. python -m pip install pendulum --trusted-host pypi.org --trusted-host files.pythonhosted.org\n\nIf you are behind a corporate proxy that requires authentication (common if you are in a company network), you can use the –proxy option with pip to give the proxy address and your credentials.\n\ne.g. python -m pip install pendulum --proxy http://your_username:yourpassword@proxy_address\nIt also works with the https_proxy environment variables\n\n\nInstall library\n$ python -m pip install &lt;library_name&gt;\n\n# inside ipython or a colab notebook, \"!\" signifies a shell command\n!pip install &lt;library_name&gt;\nInstall library from github\npython -m pip install git+https://github.com/bbalasub1/glmnet_python.git@1.0\n\n“@1.0” is the version number\n\nUninstall library\n$ python -m pip uninstall &lt;library_name&gt;\n\nWon’t uninstall the dependencies of this library.\nIf you wish to also uninstall the unused dependencies as well, take a look at pip-autoremove\n\nRemove all packages in environment\n$ python -m pip uninstall -y -r &lt;(pip freeze)\nRemove all packages in environment but write the names of the packages to a requirements.txt file first\n$ python -m pip freeze &gt; requirements.txt && python3 -m pip uninstall -r         requirements.txt -y\nInstall requirements.txt\n$ python -m pip install -r requirements.txt\nWrite names of all the packages in your environment to a requirement.txt file\n$ python -m pip freeze &gt; requirements.txt\n\nWrites the specific version of the packages that you have installed in your environment (e.g. pandas==1.0.0)\n\nThis may not be what you always want, so you’ll need to manually change to just the library name in that case (e.g. pandas)\n\nOnly aware of the packages installed using the pip install command\n\ni.e. any packages installed using a different approach such as peotry, setuptools, condaetc. won’t be included in the final requirements.txt file.\n\nDoes not account for dependency versioning conflicts\nSaves all packages in the environment including those that are not relevent to the project\nIf you are not using a virtual environment, pip freeze generates a requirement file containing all the libraries in including those beyond the scope of your project.\n\nList your installed libraries\n$ python -m pip list\nSee if you have a particular library installed\n$ python -m pip list | grep &lt;library_name&gt;\nGet library info (name, version, summary, license, dependencies and other)\n$ python -m pip show &lt;library_name&gt;\nCheck that all installed packages are compatible\n$ python -m pip check\nUpdate package\n$ python -m pip install package_name --upgrade\nSearch for PyPI libraries (pip source for libraries)\n$ python -m pip search &lt;search_term&gt;\n\nreturns all libraries matching search term\n\nDownload a package without installing it\npython -m pip download &lt;library name&gt;\n\nIt will download the package and all its dependencies in the current directory (the files, called, wheels, have a .whl extension).\nYou can then install them offline by doing python -m pip install on the wheels.\n\nBuild Wheel archives for the libraries and dependencies in your environment\n$ python -m pip wheel\n\nI think these are binaries, so they don’t need compiled if installed in a future environment\nReal Python Tutorial\n\nManage configuration\n$ python -m pip config &lt;action name&gt;\n\nActions: edit, get, list, set or unset\nExample\n$ python -m pip config set global.index-url https://your.global.index\n\nDisplay debug information specific to pip\n$ python -m pip debug",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-anac",
    "href": "qmd/python-general.html#sec-py-gen-anac",
    "title": "General",
    "section": "Anaconda",
    "text": "Anaconda\n\nCheck configuration of current environment\nconda list\n\nShows python version used, package names installed and their versions\n\nInstall packages\nconda install &lt;package1&gt; &lt;package2&gt;\nInstall a package from a specific channel\nconda install &lt;package_name&gt; -c &lt;channel_name&gt; -y # Short form\nconda install &lt;package_name&gt; --channel &lt;channel_name&gt; -y # Long form\nPackage installation channels (some packages not available in default channel)\n\nCheck current channels\nconda config --show channels\n\nThe order in which these channels are displayed shows the channel priority.\n\nWhen a package is installed, anaconda will the check the channel at the top of list first then work it’s way down\n\n\nAdd a channel\nconda config --add channels conda-forge\n\nAdds “conda-forge” to list of available channels\n\nRemove a channel\nconda config --remove channels conda-forge\n\nRemoves the “conda-forge” channel",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-env",
    "href": "qmd/python-general.html#sec-py-gen-env",
    "title": "General",
    "section": "Environments",
    "text": "Environments\n\nMisc\n\nWhen you’re in a virtual environment\n\nAnytime you use the “python” command while your virtual environment is activated, it will be only the one from this env.\nIf you start a Python shell now, it will see only the things in the current directory, and the things installed in the virtual environment.\nIf you run a script, it will see only the things in the current directory, and the things installed in the virtual environment.\nIf you run a command, the command will be taken from the virtual environment.\nAnd they will only use exactly the version of Python of the virtual environment.\n\nStore environment files with project\nBreakage\n\nYou cannot move a virtual environment, it will stop working. Create a “requirements.txt” file, delete the virtual environment and create a new one.\nDon’t rename a directory containing a virtual environment. Or if you do, prepare yourself to create a “requirements.txt” file, delete the virtual environment and create a new one.\nIf you change the Python used in the virtual environment, such as when uninstalling it, it will stop working.\n\nCreate one big virtual environment for all small scripts.\n\nIf you make a lot of venv, you may be tempted to install everything at the system level for convenience. After all, it’s a bore to create and activate a virtual environment each time you want to write a five liner. A good balance is one single virtual environment you use for all things quick and dirty.\n\nCreate several virtual environments per versions of python if your project needs to support several versions. You may need several requirements.txt files as well, one for each env.\nRecommendations for a stable dependency environment for your project (article)\n\nDon’t install the latest major version of Python\n\nMaximum: 1 version under the latest version\nMinimum: 4 versions under the latest version (e.g. latest = 3.11, min = 3.7)\n\nUse only the python.org installer on Windows and Mac, or official repositories on Linux.\nNever install or run anything outside of a virtual environment\nLimit yourself to the basics: “pip” and “venv”\nIf you run a command, use “-m”\n\nIt lets you run any importable Python module, no matter where you are. Because most commands are Python modules, we can use this to say, “run the module X of this particular python”.\nThere is currently no way for you to run any python command reliably without “-m”.\nExamples:\n# Don't do :\npip install\n# Do:\npython -m pip install\n\n# Don't do :\nblack\n# Do:\npython -m black\n\n# Don't do :\njupyter notebook\n# Do:\npython -m jupyter notebook\n\nWhen creating a virtual environment, be explicit about which Python you use\n\nGet current python versions installed: py --list-paths (windows)\n\n\n\n\n\npyenv\n\nJust a simple Python version manager — think {rig}\n{{pyenv}}, {{pyenv-win}}\nSet-up in RStudio (article)\nCompiles Python under the hood when you install it. But compiling can fail in a thousand ways\npyenv install --list: To see what python versions are available to install\npyenv install &lt;version number&gt;: To install a specific version\npyenv versions: To see what python versions are installed on your system\npyenv global &lt;version number&gt;: The set one python version as a global default\npyenv local &lt;version number&gt;: The set a python version to be used within a specific directory/project\\\n\n\n\npdm\n\nDocs\nPackage and dependency manager similar to npm. Doesn’t require virtual environments.\nFeatures: auto-updating pyproject.toml, isolating dependencies from dependencies-of-dependencies, active development and error handling\n\n\n\nvenv\n\nMisc\n\nShipped with Python\nDon’t mix pip, venv and Anaconda. Avoid Anaconda if you can. If you have to use Anaconda, don’t use pip and venv. Limit yourself to Anaconda tools.\n\nCreate\n\nWindows: py -&lt;py version&gt; -m venv &lt;env name&gt;\nMac/Linux: python3.8 -m venv .venv\n\nWhere the python version is 3.8 and the environment name is “.venv”\nMac and Linux hide folders with names that have preceding “.” by default, so make sure you have “display hiddent folders” activated or you won’t see it.\n\nNaming Environments\n\nName your environment directory “.venv”, because:\n\nSome editors check for this name and automatically load it.\nIt contains “venv”, so it’s explicit.\nIt has a dot, so it’s hidden on Linux.\n\nIf you have more than one environment directory, use a suffix to distinguish them.\n\ne.g. A project that must work with two different versions of Python (3.9 and 3.10), I will have a “.venv39” and a “.venv310”\n\nNaming enviroments for misc uses\n\n“.venv_test”: located in a personal directory to install new tools you want to play with. It’s disposable, and often broken, so you can delete it and recreate it regularly.\n“.venv_scripts”: Used for all the small scripts. You don’t want to create one virtual environment for each script, so centralize everything. It’s better than installing outside of a virtual environment, but is not a big constraint.\n\n\n\nActivate\n\nWindows: .venv\\Scripts\\activate\n\nWhere .venv is the name of the virtual environment\nMay need .bat as extension to activate\n\nMac/Linux: source .venv/bin/activate\n\nAfter that, you can use python -m pip install to install packages.\nDeactivate: deactivate\n\n\n\nvirtualenv\n\nDocs\nCreate a virtual environment\n python3 -m venv &lt;env_name&gt;\n\n-m venv tells python to run the virtual environment module, venv\nMake sure you’re in your projects directory\nRemember to add “&lt;env_name&gt;/” to .gitignore\n\nActivate environment\nsource venv/bin/activate # Mac or Linux\nvenv\\Scripts\\activate # Windows\n\n&gt;&gt; (&lt;env_name&gt;) $\n\nPrompt should change if the environment is activated\nAll pip  installed packages will now be installed into the “&lt;env_name&gt;/lib/python3.9/site-packages” directory\n\nUse the python contained within your virtual environment\npython main.py\n\nNot sure why you wouldn’t just activate the environment.\n\nDeactivate environment\ndeactivate\n\nno python  or env_name needed?\n\nReproducing environment\n\nDone using requirements.txt (see pip section for details on writing and installing)\n\nI don’t think the python version is included, so that will need to communicated manually\n\n\n\n\n\nAnaconda\n\nList environments\nconda env list # method 1\nconda info --envs # method 2\n\ndefault environment is called “base”\nActive environment will be in parentheses\nActive environment will be the one in the list with an asterix\n\nCreate a new conda environment\nconda create -n &lt;env name&gt;\nconda activate &lt;env name&gt;\nCreate a new conda environment with a specific python version\nconda create -n py310 python=3.10\nconda activate py310\nconda install jupyter jupyterlab\njupyter lab\n\nAlso install and launch jupyter lab\n\nCreate an environment from a yaml file\nconda env create -f environment.yml # Short form\nconda env create --file environment.yml # Long form\nRemove an environment\nconda deactivate &lt;env_name&gt; # Need to deactivate the environment first\nconda env remove -n &lt;env_name&gt;\n\nShould also delete environment folders (conda env list shows path to folders)\n\nClone an existing environment\nconda create -n testclone --clone test # Short form\nconda create --name testclone --clone test # Long form\n\n“testclone” is a copy of “test”\n\nActivate an environment\nconda activate &lt;env_name&gt;\nActivate environment with reticulate in R\nreticulate::use_python(\"/usr/local/bin/python\")   \nreticulate::use_condaenv(\"&lt;env name&gt;\", \"/home/jtimm/anaconda3/bin/conda\")\nDeactivate an environment\nconda activate # Option 1: activates base\nconda deactivate test # Option 2\nExport the specifications of the current environment into a YAML file into the current directory\nconda env export &gt; environment.yml # Option 1\nconda env export -f environment.yml # Option 2\nExample: Conda workflow\n\nCreate an environment that uses a specific python version\n\nWithout a specified python version, the environment will use the same version as “base”\n\nconda create -n anothertest python=3.9.7 -y\n\n-n is the name flag and “anothertest” is the name of the environment\nUses Python 3.9.7\nWithout the -y flag, there’d be a prompt you’d have to answer “yes” to\n\nActivate the environment\nconda activate anothertest\nInstall packages\n\n\nInstalling packages one at time can lead to dependency conflicts.\nConda’s official documentation recommends to install all packages at the same time so that the dependency conflicts are resolved\nconda install \"numpy&gt;=1.11\" nltk==3.6.2 jupyter -y # install specific versions\nconda install numpy nltk jupyter -y # install all latest versions\n\nDo work and deactivate environment\nconda deactivate anothertest\n\nExample Raschka workflow\n# create & activate\nconda create  --prefix ~/code/myproj python=3.8\nconda activate ~/code/myproj\n# export env\nconda env export &gt; myproj.yml\n# create new env from yaml\nconda env create --file myproj.yml --prefix ~/code/myproj2\n\n\n\nPoetry\n\nDocs (like renv)\nApparently buggy (article)\npip’s dependency resolver is more flexible and won’t die on you if the package specifies bad metadata, while poetry’s strictness may mean you can’t install some packages at all.\nCreate project\n\npoetry new &lt;project-dir-name&gt;\n\nautomatically creates a directory for your project with a skeleton\n“pyproject.toml” maintains dependencies for the project with the following sections:\n\ntool.poetry provides an area to capture information about your project such as the name, version and author(s).\ntool.poetry.dependencies lists all dependencies for your project.\ntool.poetry.dev-dependencies lists dependencies your project needs for development that should not be present in any version deployed to a production environment.\nbuild-system references the fact that Poetry has been used to manage the project.\n\n\nAdd library and create lock file: poetry add &lt;library name&gt;\n\nWhen the first library is added, a “poetry.lock” file wil be generated\n\nActivate environment: poetry shell\n\nDeactivate environment: exit\n\nRun script: poetry run python my_script.py\nPackage the project: poetry build\n\nCreates tar.gz and wheel files (.whl) in “dist” dir\n\nExample: poetry workflow (+pyenv, virtualenv)\n# Create a virtual environment called \"my-new-project\"\n# using Python 3.8.8\npyenv virtualenv 3.8.8 my-new-project\n# Activate the virtual environment\npyenv activate my-new-project\n\n{{pyenv}} - For managing the exact version of Python and activating the environment\nName your package the same name as the directory which is the same name as the virtual environment.\n\nDashes for the latter two and underscores for the package\n\nIntitialize the project and add packages (similar to renv) bash              poetry init     poetry add numpy\nReinstall dependencies\n# navigate to my project directory and run\npoetry install\nTurn off virtualenv management\n# right after installing poetry, run:\npoetry config virtualenvs.create false\n\nDefault poetry behavior is that it will manage your virtual environments for you. This may not be desirable because:\n\nCan’t just run a script from the command line. Instead, have to run poetry run my-script\n\nAwkward when you want to dockerize your code\n\nEnforces a virtual environment management framework on everybody in a shared codebase\nYour Makefile now needs to know about poetry",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-deps",
    "href": "qmd/python-general.html#sec-py-gen-deps",
    "title": "General",
    "section": "Dependencies",
    "text": "Dependencies\n\nMisc\n\nAfter mastering pip, {{pip-tools}} is recommended.\n\nGet a complete list of dependencies (e.g. dependencies of dependencies) with {{deptree}}\ndeptree\n# output\nFlask==2.2.2  # flask\n  Werkzeug==2.2.2  # Werkzeug&gt;=2.2.2\n    MarkupSafe==2.1.1  # MarkupSafe&gt;=2.1.1\n  Jinja2==3.1.2  # Jinja2&gt;=3.0\n    MarkupSafe==2.1.1  # MarkupSafe&gt;=2.0\n  itsdangerous==2.1.2  # itsdangerous&gt;=2.0\n  click==8.1.3  # click&gt;=8.0\n# deptree and pip trees\n\nFlask depends on Werkzeug which depends on MarkupSafe\n\nWerkzeug and MarkupSafe qualify as transitive dependencies for this project\n\nCommented part on the right is the compatible range\n\nrequirements.txt format\n# comment\npandas==1.0.0\npyspark\npip: write names of all the packages in your environment to a requirement.txt file\n$ python3 -m pip freeze &gt; requirements.txt\n\nSee pip section for issues with this method\n\n{{pipx}}\n\nA tool for installing Python CLI utilities that gives them their own hidden virtual environment for their dependencies\nAdds the tool itself to your PATH - so you can install stuff without worrying about it breaking anything else\nInstall\npipx install datasette\n\n{{pipreqs}}\n\nScans all the python files (.py) in your project, then generates the requirements.txt file based on the import statements in each python file of the project\nSet-up: pip install pipreqs\nGenerate requirements.txt file: pipreqs /&lt;your_project_root_path&gt;/\nUpdate requirements.txt:  pipreqs --force /&lt;your_project_root_path&gt;/ \nIgnore the libraries of some python files from a specific subfolder\npipreqs /&lt;your_project_root_path&gt;/ --ignore  /&lt;your_project_root_path&gt;/folder_to_ignore/\n\n{{pip-compile-multi}}\n\nNotes from:\n\nEnd Python Dependency Hell with pip-compile-multi\n\nCreates and nests multiple requirement files\n\ne.g. Able to keep dev environment from production environment separate\n\nAutoresolution of cross-requirement file conflicts\n\nDependency DAG (how all requirement files are connected) must have exactly one “sink” node\n\nOrganize your most ubiquitous dependencies into a single “core” set of dependencies that all other nodes require (a source node), and all of your development dependencies in a node that requires all others (directly or indirectly) require (a sink).\n\nSimplifies and allows use of autoresolution functionality\n\nExample: DAG (directionality of the arrows is opposite compared to library docs)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-loadsav",
    "href": "qmd/python-general.html#sec-py-gen-loadsav",
    "title": "General",
    "section": "Loading/Saving",
    "text": "Loading/Saving\n\nMisc\n\n{{pickle}} needs custom class(es) to be defined in another module/file and then imported. Otherwise, PicklingError will be raised.\n\n\n\nFile paths\n\nMisc\n\n{{pathlib}} is recommended\n\n{{os}}\n\nGet current working directory: os.getcwd()\nList all files and directories in working directory: os.listdir()\nList all files and directories from a subdirectory: os.listdir(os.getcwd()+'\\\\01_Main_Directory')\nUsing os.walk(): gathers paths, folders, and files\n\nPaths\n\n\npath = os.getcwd()+'\\\\01_Main_Directory'\nfor folder_path, folders, files in os.walk(path):\n    print(folder_path)\nFolders\n\n\nSimilar code, just replace print(folder_path) with print(folders)\n\nFiles\n\n\n{{glob}}\n\nGet a file path string\nimport glob\npath = os.getcwd()+'\\\\01_Main_Directory'\nfor filepath in glob.glob(path):\n    print(filepath)\n# C:\\Users\\Suraj\\Challenges\\01_Main_Directory\nList all files and subdirectories from a path\npath = os.getcwd()+'\\\\01_Main_Directory\\\\*'\nfor filepath in glob.glob(path):\n    print(filepath)\n\nNote the * wildcard\n\nList all files and subdirectories with a “1” in the name\npath = os.getcwd()+'\\\\01_Main_Directory\\\\*1.*'\nfor filepath in glob.glob(path):\n    print(filepath)\nGet a list of csv file paths from a directory: all_files = glob.glob(\"C:/Users/path/to/dir/*.csv\")\n\nNote that you don’t need a loop to save to an object\n\nList all files and subdirectories and files in those subdirectories\npath = os.getcwd()+'\\\\01_Main_Directory\\\\**\\\\*.txt'\nfor filepath in glob.glob(path, recursive=True):\n    print(filepath)\n#Output\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_2.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_3.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_4.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_5.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_1\\File_1_in_SubDict_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_1\\File_2_in_SubDict_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_2\\File_1_in_SubDict_2.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_2\\File_2_in_SubDict_2.txt\n\nComponents: “**” and “recursive=True”\n\n\n{{pathlib}}\n\nProvides a single Path class with a range of methods (instead of separate functions) that can be used to perform various operations on a path.\nCreate a path object for a directory\nfrom pathlib import Path\npath = Path('origin/data/for_arli')\nCheck if a folder or a file is available in a given path\nif path.exists():\n    print(f\"[{path}]{style='color: #990000'} exists.\")\n    if path.is_file():\n        print(f\"[{path}]{style='color: #990000'} is a file.\")\n    elif path.is_dir():\n        print(f\"[{path}]{style='color: #990000'} is a directory.\")\nelse:\n    raise ValueError(f\"[{path}]{style='color: #990000'} does not exists\")\n\nChecks if the path ‘origin/data/for_arli’ exists\n\nif it does, it will check whether it is a file or a directory.\nIf the path does not exist, it will print a raise an Error indicating that the path does not exist.\n\n\nList all files/folders in a path\nfor f in path.iterdir():\n    print(f)\n\nUse it in combination with the previous is_dir() and is_file()  methods to list either files or directories.\n\nDelete files/folders in a path\nfor f in path.iterdir():\n    f.unlink()\n\npath.rmdir()\n\nunlink deletes each file in the path\nrmdir deletes the directory.\n\ndirectory must be empty\n\n\nCreate a sequence of directories\n# existing directory: D:\\scripts\\myfolder\np = Path(\"D:\\scripts\\myfolder\\logs\\newfolder\")\np.mkdir(parents=True, exist_ok=True)\n\nCreate path object with desired sequence of directories (e.g. logs\\newfolder)\nmkdir with parents=True creates the sequence of directories\n\nW/exist_ok=True no error with occur if the directory already exists\n\n\nRename directory: path.rename('origin/data/new_name')\nConcatenate a path with string\npath = Path(\"/origin/data/for_arli\")\n# Join another path to the original path\nnew_path = path.joinpath(\"la\")\nprint(new_path) # prints 'origin/data/for_arli/bla'\n\nIt also handles the join between two Path objects\n\nDirectory stats\nprint(path.stat()) # print statistics \nprint(path.owner()) # print owner\n\ne.g. creation time, modification time, etc.\n\nWrite to a file\n# Open a file for writing\npath = Path('origin/data/for_arli/example.txt')\nwith path.open(mode='w') as f:\n    # Write to the file\n    f.write('Hello, World!')\n\nYou do not need to create manually example.txt.\n\nRead a file\npath = Path('example.txt')\nwith path.open(mode='r') as f:\n    # Read from the file\n    contents = f.read()\n    print(contents) # Output: Hello World!\n\n\n\n\nModels\n\nSaving and Loading an estimator as a binary using {{joblib}} (aside: pipelines are estimators)\nimport joblib\n#saving the pipeline into a binary file\njoblib.dump(pipe, 'wine_pipeline.bin')\n#loading the saved pipeline from a binary file\npipe = joblib.load('wine_pipeline.bin')\nSaving and loading a trained model as a pickle file\nimport pickle\n# open file connection\npickle_file = open('model.pkl', 'ab')\n# save the model\npickle.dump(model_obj, pickle_file)\n# close file connection                     \npickle_file.close()\n\n# Open conn and save\ntest_dict = {\"Hello\": \"World!\"}\nwith open(\"test.pickle\", \"wb\") as outfile:\n# \"wb\" argument opens the file in binary mode\npickle.dump(test_dict, outfile)\n\n# open file connection\npickle_file = open('model.pkl', 'rb')\n# load saved model\nmodel = pickle.load(pickle_file)\n\n# open conn and load\n# Deserialization\nwith open(\"test.pickle\", \"rb\") as infile:\n    test_dict_reconstructed = pickle.load(infile)\n\nCan serialize almost everything including classes and functions\n\n\n\n\nEnvironment Variables\n\n{{os}}\n\nCheck existence\nenv_var_exists = 'ENV' in os.environ\n# or\nenv_var_exists = os.environ.has_key('ENV')\nList environment variables: print(os.environ)\nLoading\nimport os\n# Errors when not present\nenv_var = os.environ['ENV'] # where ENV is the name of the environment variable\n# Returns None when not present\nenv_var = os.environ.get('ENV', 'DEFAULT_VALUE') # using default value is optional\nSet/Export or overwrite\nos.environ['ENV'] = 'dev'\nLoad or create if not present\ntry:\n    env_var = os.environ['ENV']\nexcept KeyError:\n    os.environ['ENV'] = 'dev'\nDelete\nif 'ENV' in os.environ:\n    del os.environ['ENV']\n\n{{python-decouple}}\n\nAccess environment variables from whatever environment it is running in.\nCreate a .env file in the project root directory: touch .env\nOpen .env in nano text editor: nano .env\n\nNano text editor is pre-installed on macOS and most Linux distros\nCheck if installed/version: nano --version\nBasic usage tutorial\n\nAdd environment variables to file\nUSER=alex\nKEY=hfy92kadHgkk29fahjsu3j922v9sjwaucahf\n\nSave: Ctrl+o\nExit: Ctrl+x\n\n* Add .env to your .gitignore file *\nAccess\nfrom decouple import config\nAPI_USERNAME = config('USER')\nAPI_KEY = config('KEY')\n\n{{python-dotenv}}\n\nReads .env files\nProbably more popular than {{python-decouple}}\nHas a companion R package, {dotenv}, so .env files can be used in projects that use both R and Python.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-funs",
    "href": "qmd/python-general.html#sec-py-gen-funs",
    "title": "General",
    "section": "Functions",
    "text": "Functions\n\nMisc\n\nBenchmarking a function\n\nUsing IPython function\n%time dat['col1001'] = some_function(dat['col1'], dat['col2'], dat['col3'])\n\n%%time prints the wall time for the entire cell whereas %time gives you the time for first line only\n\nUsing a decorator\n\n\nAssigning functions based on arg type\ndef process_data(data):\n    if isinstance(data, dict):\n        process_dict(data) \n    else:\n        process_list(data) \ndef process_dict(data: dict):\n    print(\"Dict is processed\")\ndef process_list(data: list):\n    print(\"List is processed\")\n\nAssigns data to a particular function depending on whether it’s a dict or a list\nisinstance checks that the passed argument is of the proper type or a subclass\n\nWrapping functions\nfrom functools import partial\nget_count_df = partial(get_count, df=df)\n\nWraps function to make df the default value for df arg\n\n\n\n\nDocumentation\n\nFunctions should at least include docstrings and type hinting\nDocstrings\n\nTypes: Google-style, Numpydoc, reStructured Text, EpyTex\nInformation to include\n\nFunction description, arg description, return value description, Description of errors, Optional extra notes or examples of usage.\n\nAccess functions docstring:\n\nprint(func_name.__doc__)\nFor large docstrings\nimport inspect\nprint(inspect.getdoc(func_name))\n\nExample: Google-style\ndef send_request(key: str, lat: float = 0, lon: float = 0):\n    \"\"\"Send a request to Climacell Weather API\n    to get weather info based on lat/lon.\n\n    Climacell API provides realtime weather\n    information which can be accessed using\n    their 'Realtime Endpoint'.\n\n    Args:\n      key (str): an API key with length of 32 chars.\n      lat (float, optional): value for latitude.\n        Default=0\n      lon (float, optional): value for longitude.\n        Default=0\n\n    Returns:\n      int: status code of the result \n      dict: Result of the call as a dict\n\n    Notes:\n      See https://www.climacell.co/weather-api/ \n      for more info on Weather API. You can get\n      API key from there, too.\n    \"\"\"\n\nFirst sentence should contain the purpose of the function\n\nExample: Numpydoc\ndef send_request(key: str, lat: float = 0, lon: float = 0):\n    \"\"\"\n    Send a request to Climacell Weather API\n    to get weather info based on lat/lon.\n\n    Climacell API provides realtime weather\n    information which can be accessed using\n    their 'Realtime Endpoint'.\n\n    Parameters\n    ----------\n      key (str): an API key with length of 32 chars.\n      lat (float, optional): value for latitude.\n        Default=0\n      lon (float, optional): value for longitude.\n        Default=0\n\n    Returns\n    -------\n      int: status code of the result \n      dict: Result of the call as a dict\n\n    Notes\n    -----\n      See https://www.climacell.co/weather-api/ \n      for more info on Weather API. You can get\n      API key from there, too.\n    \"\"\"\n\nType Hinting\n\nThis doesn’t check the type; it’s just metadata\n\nsee isinstance (see below), NotImplementedError (see below), or {{typecheck}} and {{mypy} (see bkmks) for type checking that will throw errors\n\nUsing type hints enables you to perform type checking. If you use an IDE like PyCharm or Visual Studio Code, you’ll get visual feedback if you’re using unexpected types:\nVariables: my_variable_name: tuple[int, ...]\n\nvariable should be a tuple that contains only integers. The ellipsis says the total quantity is unimportant.\n\nFunctions\ndef get_count(threshold: str, column: str, df: pd.DataFrame) -&gt; int:\n    return (df[column] &gt; threshold).sum()\n\n“threshold”, “column” should be strings (str)\n“df” should be a pandas dataframe (pd.DataFrame)\nOutput should be an integer (int)\n\nFunction as an arg: Callable[[Arg1Type, Arg2Type], ReturnType]\n\nExample:\nfrom collections.abc import Callable\ndef foo(bar: Callable[[int, int], int], a: int, b: int) -&gt; int:\n    return bar(a, b)\n\n“bar” is a function arg for the function, “foo”\n“bar” is supposed to take: 2 integer args ([int, int]) and return an integer (int)\n\nExample:\ndef calculate(i: int, action: Callable[..., int], *args: int) -&gt; int:\n    return action(i, *args)\n\n“action” takes any number and type of arguments but must return an integer.\nWith *args: int, you also allow a variable number of optional arguments, as long as they’re integers.\n\nExample: Lambda\nf: Callable[[int, int], int] = lambda x, y: 3*x + y\n\nMay not work\n\n\n\n\n\n\nArgs and Operators\n\nMisc\n\n** Args are not reset to default values after each call **\n\nExample:\n\ndef func(list1=[]):      # here l1 is a default argument set to []\n    list1.append(\"Temp\")\n    return list1\n\n“None” + conditional must be used to get the arg to reset back to the default value\n\ndef func(l1=None):     \n    if l1 is None: \n        l1 = []\n    l1.append(\"Temp\") \n    return l1\n\n*\n\nUnpacks Lists\n\nnum_list = [1,2,3,4,5]\nnum_list_2 = [6,7,8,9,10]\n\nprint(*num_list)\n# 1 2 3 4 5\nnew_list = [*num_list, *num_list_2] # merge multiple lists\n# [1,2,3,4,5,6,7,8,9,10]\n*args\n\nFunctions that can accept a varying number of values\n\ndef names_tuple(*args):\n    return args\n\nnames_tuple('Michael', 'John', 'Nancy')\n# ('Michael', 'John', 'Nancy')\nnames_tuple('Jennifer', 'Nancy')\n# ('Jennifer', 'Nancy')\n**\n\nUnpacks Dictionaries\n\nnum_dict = {‘a’: 1, ‘b’: 2, ‘c’: 3}\nnum_dict_2 = {‘d’: 4, ‘e’: 5, ‘f’: 6}\n\nprint(*num_dict) # only keys printed\n# a b c\nnew_dict = {**num_dict, **num_dict_2} # merge dictionaries\n# {‘a’: 1, ‘b’: 2, ‘c’: 3, ‘d’: 4, ‘e’: 5, ‘f’: 6}\n**kwargs\n\nFunctions that can accept a varying number of variable/value pairs (like a … in R)\n\ndef names_dict(**kwargs):\n    return kwargs\n\nnames_dict(Jane = 'Doe')\n# {'Jane': 'Doe'}\nnames_dict(Jane = 'Doe', John = 'Smith')\n# {'Jane': 'Doe', 'John': 'Smith'}\nFunction as an arg\ndef classic_boot(df, estimator, seed=1):\n    df_boot = df.sample(n=len(df), replace=True, random_state=seed)\n    estimate = estimator(df_boot)\n    return estimate\n\nBootstrap function with an “estimator” function (e.g. mean) as arg\nUsing a Callable\n\nClass as an arg\nfrom dataclasses import dataclass\n\n@dataclass\nclass Person:\n    name: str\n    age: int\n    address: str\n    phone: str\n    email: str\n\ndef process_data(person: Person):\n    print(f\"Processing data for {person.name}, {person.age}, living at {person.address}. Contact info: {person.phone}, {person.email}\")\n\nperson = Person(\"Alice\", 30, \"123 Main St\", \"555-1234\", \"alice@example.com\")\nprocess_data(person)\n\nMakes function more readable when the function requires a bunch of args\n\nClass as an arg (safer alternative)\nfrom typing import NamedTuple\n\nclass Person(NamedTuple):\n    name: str\n    age: int\n    address: str\n    phone: str\n    email: str\n\ndef process_data(person: Person):\n    print(f\"Processing data for {person.name}, {person.age}, living at {person.address}. Contact info: {person.phone}, {person.email}\")\n\nperson = Person(\"Alice\", 30, \"123 Main St\", \"555-1234\", \"alice@example.com\")\nprocess_data(person)\n\nUsing NamedTuple means that the attributes cannot be overridden\n\ne.g. Executing person.name = \"Bob\" will result in an error because tuples can’t be modified.\n\n\nMake an arg optional\nlass Address:\n    def __init__(self, street, city, state, zipcode, street2=''):\n        self.street = street\n        self.street2 = street2\n        self.city = city\n        self.state = state\n        self.zipcode = zipcode\n\n“street2” has default value of an empty string, so it’s optional\n\n\n\n\nLambda\n\nUseful if you just have 1 expression that you need to execute.\nBest Practices\n\nlambda is an anonymous function, hence it is not a good idea to store it in a variable for future use\nDon’t use lambdas for single functions (e.g. sqrt). Make sure it’s an expression.\n\nExample\n# bad\nsqrt_list = list(map(lambda x: math.sqrt(x), mylist))\n# good\nsqrt_list = list(map(math.sqrt, mylist))\n\nAffects performance\n\n\nDon’t use for complex expressions that require more than 1 line (meh)\n\nPer PEP8 guidelines, Limit all lines to a maximum of 79 characters\nExample\n# bad (118 characters)\ndf[\"FinalStatus\"] = df[\"Status\"].map(lambda x: 'Completed' if x ==\n'Delivered' or x == 'Shipped' else 'Not Completed')\n# instead\ndf[\"FinalStatus\"] = ''\ndf.loc[(df[\"Status\"] == 'Delivered') |\n      (df[\"Status\"] == 'Shipped'),\n      'FinalStatus'] = 'Completed'\ndf.loc[(df[\"Status\"] == 'Not Delivered') |\n      (df[\"Status\"] == 'Not Shipped'),\n      'FinalStatus'] = 'Not Completed'\n\n\nExample: 1 arg\n# py\nlambda x: np.sin(x / period * 2 * np.pi)\n# r\n~sin(.x / period * 2 * pi)\n# r\n\\(x) {sin(x / period * 2 * pi)}\nExample: 2 args\nGreater = lambda x, y : x if(x &gt; y) else y\nGreater(0.002, 0.5897)\nLambda-Filter\n\nFaster than a comprehension\n\nsee Loops &gt;&gt; Comprehensions\n\nFormat: filter(function, data_object)\n\nReturns a filter object, which needs to be converted into data structure such as list or set\n\nExample: Basic\nyourlist = list(np.arange(2,50,3))\nlist(filter(lambda x:x**2&lt;100, yourlist))\n# Output \n[2, 5, 8]\nExample: Filter w/logical\nimport pandas as pd\nimport datetime as dt\n# create a list of 10,000 dates\ndatlist = pd.date_range(dt.datetime.today(), periods=10000).tolist() \n# convert the dates to strings via list comprehension\ndatstrlist = [d.strftime(\"Day %d in %B of year %Y is a %A\") for d in datlist]\ndatstrlist[:4]\n['Day 21 in October of year 2021 is a Thursday', 'Day 22 in October of year 2021 is a Friday', 'Day 23 in October of year 2021 is a Saturday', 'Day 24 in October of year 2021 is a Sunday']\n\nstrLamb = filter(lambda d: ((d.endswith(\"urday\") or d.endswith(\"unday\")) and \"Oc\" in d), datstrlist)\n\nSearches for Saturdays and Sundays in the month of October of all years in list of strings\n\nExample: Nested Lists\ngroup1 = [1,2,3,43,23,42,8,3,7]\ngroup2 = [[3, 34, 23, 32, 42], [6, 11, 9], [1, 3,9,7,2,8]]\n[list(filter(lambda x: x in group1, sublist)) for sublist in group2]\n&gt;&gt; [[3, 23, 42], [], [1, 3, 7, 2, 8]]\n\nProbably useful for json\nfor-loop attached to the end of the list-filter combo\nEach sublist of group 2 is fed into the lambda-filter and compared to the group 1 list\n\n\nIterating over each element of a list\n\nExample: map\nlist(map(lambda x: x**2+x**3, yourlist))\n\nmap returns a map object that needs to be converted\n\nExample: 2 Lists\nmylist = list(np.arange(4,52,3))\nyourlist = list(np.arange(2,50,3))\nlist(map(lambda x,y: x**2+y**2, yourlist, mylist))\n\nLike a pmap\n\n\nNested lambdas\n\nExample: map\narr = [1,2,3,4,5]\nlist(map(lambda x: x*2, filter(lambda x: x%2 == 0, arr)))\n&gt;&gt; [4,8]\n\nWork inside out (locate where the data object, arr, appears)\n“arr” is filtered by the first lambda function for even numbers then iterated by map to be squared by the second lambda function\n\n\nIterate over rows of a column in a df\n\nExample: Using formula over rows\ngrade['evaluate']=grade['MathScore'].apply(lambda x: round((x**x)/2,2))\n\n“grade” is the df; “MathScore” is a numeric column; “evaluate” is the new column in the df\nFormula applied to each value of “MathScore” to generate each value of evaluate\n\nExample: Conditional over rows\ngrade['group']=grade['MathScore'].apply(lambda x: 'Excellent' if x&gt;=3.0 else 'Average')\n\n“grade” is the df; “MathScore” is a numeric column; “group” is the new column in the df\nConditional applied to each value of “MathScore” to generate each value of “group”\n\nUsing {{swifter}} for parallelization\nimport swifter\ndf['e'] = df.swifter.apply(lambda x: infer(x['a'], x['b'], x['c'], x['d']), axis = 1)\n\nIn a Pivot Table (like a crosstab)\n\nExample\n\ngrades_df\n\n2 names (“name”)\n6 scores (“score”)\nOnly 2 letter grades associated with these scores (“letter grade”)\n\nTask: drop lowest score for each letter grade, then calculate the average score for each letter grade\n\ngrades_df.pivot_table(index='name',\n                      columns='letter grade',\n                      values='score',\n                      aggfunc = lambda series : (sorted(list(series))[-1] + sorted(list(series))[-2]) / 2)\n\nletter grade    A    B\nname\nArif          96.5  87.0\nKayla        95.5  84.0\n\nindex: each row will be a “name”\ncolumns: each column will be a “letter grade”\nvalues: value in the cells will be from the “score” column according to each combination columns in the index and columns args\naggfunc: uses a lambda to compute the aggregated values\n\n“series” is used a the variable  in the lambda function\nsorts series (ascending), takes the top two values (using negative list indexing), and averages them\n\n\n\n\n\n\nScope\n\nPopulated objects within functions persist if you instantiate the object in the argument\n\n\n“all_numbers” retained it’s previous value when the 2nd call to the function was made\n\n\n\n\nClosures\n\nInner functions that can access values in the outer function, even after the outer function has finished its execution\nExample\n\n# closure way\ndef balanceOwed(roomN,rate,nights):\n    def increaseByMeals(extra):\n        amountOwned=rate*nights+extra\n        print(f\"Dear Guest of Room [{roomN}]{style='color: #990000'}, you have\", \n        \"a due balance:\", \"${:.2f}\".format(amountOwned))\n        return amountOwned\n    return increaseByMeals\n\nba = balanceOwned(201,400,3)\nba(200)\nba(150)\nba(180)\nba(190)\nDear Guest of Room 201, you have a due balance: $1400.00\nDear Guest of Room 201, you have a due balance: $1350.00\nDear Guest of Room 201, you have a due balance: $1380.00\nDear Guest of Room 201, you have a due balance: $1390.00\n\nTedious way: For each value of “extra” (e.g. meals), the function needs to be called even if the other values of the arguments don’t change.\nClosure way:\n\nincreaseByMeals() is a closure function, because it remembers the values of the outer function balanceOwed(), even after the execution of the latter\nbalanceOwed() is called with its three arguments only once and then after its execution, we call it four times with the meal expenses (“extra”).",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-mods",
    "href": "qmd/python-general.html#sec-py-gen-mods",
    "title": "General",
    "section": "Modules",
    "text": "Modules\n\n.py files are called “modules.”\nA directory with .py files in which one of the files is an “__init__.py” is called a package.\nMisc\n\nResource: Make your Python life easier by learning how imports find things\nsys.path contains the list of paths where Python is looking for things to import. Your virtual environment and the directory containing your entry point are automatically added to sys.path.\n\nsys.path is a list. Which means you can .append(). Any directory you add there will have its content importable. It’s a useful hack, but use it as a last resort.\n\nWhen using -m flag to run a script, if you pass a package instead of a module, the package must contain a “__main__.py” file for it to work. This __main__.py module will run.\nIf you have scripts in your projects, don’t run them directly. Run them using “-m”, and you can assume everything starts from the root.\n\nExample:\ntop_dir\n├── foo\n│   ├── bar.py\n│   ├── __init__.py\n│   └── blabla.py\n└── blabla.py\n\nRunning python foo/bar.py, “top_dir” is the current working directory, but “foo” is added to the sys.path.\nRunning python -m foo.bar, “top_dir” is the current working directory and added to sys.path.\n\nImports can all start from the root of the project and opened file paths as well.\n\n\n\n\nUsage\n\nProject Structure\n├── main.py\n├── packages\n│  └── __init__.py\n│  └── module_1.py\n│  └── module_2.py\n│  └── module_3.py\n└── └── module_4.py\n\n“__init__.py” contains only 1 line which declares all the functions (or classes?) that are in the modules\n__all__ = [\"func1\", \"func2\"]\n\nIf the module files contained classes with multiple functions, I think you’d just declare the classes and not every function in that class.\n\nIf using classes, each module should only have 1 class.\n\n\nScripts need to include “_main_” in order to used in other scripts\n# test_function.py\ndef function1(): \n    print(\"Hello world\") \nfunction1()\n\n# Define the __main__ script\nif __name__ == '__main__':   \n    # execute only if run as a script\n    function1()\n\nSays if this file is being run non-interactively (i.e. as a script), run this chunk\nAdd else: chunk, then that chunk will be run only if the file is imported as a module\nAllows you to allow or prevent parts of code from being run when the modules are imported\nImporting a module without _main_ in a jupyter notebook results in this\n\n\nLoading\n\nDO NOT USE from &lt;library&gt; import *\n\nThis will import anything and everything from that library and causes several problems:\n\nYou don’t know what is in that package, so you have no idea what you just imported, or even if what you want is in there.\nYou just filled your local namespace with an unknown quantity of mysterious names, and you don’t know what they will shadow.\nYour editor will have a hard time helping you since it doesn’t know what you imported.\nYour colleague will hate you because they have no idea what variables come from where.\n\nException: In the shell, it’s handy. Sometimes, you want to import all things in __init__.py and you have “__all__” defined (see above)\n\nFrom the working directory, it’s like importing from a library: from file1 import function1\nFrom a subdirectory, from subdirectory.file1 import function1\nFrom a directory outside the project, add the module to sys.path before importing it\nimport sys\nsys.path.append('/User/moduleDirectory')\n\nWhen a module is imported, it first searches for built-in modules, then the paths listed in sys.path\nThis appends the new path to the end of the sys.path\nimport sys\nsys.path.insert(1, '/User/moduleDirectory')\nPuts this path at the front of the sys.path directory list.\nimport sys\nsys.path.remove('/User/NewDirectory')\n\n*delete path from sys.path after you finish*\nPython will also search this path for future projects unless they are removed",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-cond",
    "href": "qmd/python-general.html#sec-py-gen-cond",
    "title": "General",
    "section": "Conditionals",
    "text": "Conditionals\n\nIf-Else\n\nSyntax\nif &lt;expression&gt;:\n    do something\nelse:\n    do something else\nExample\nregenerate = False\nif regenerate:\n    concepts_list = df2Graph(df, model='zephyr:latest')\n    dfg1 = graph2Df(concepts_list)\n    if not os.path.exists(outputdirectory):\n        os.makedirs(outputdirectory)\n\n    dfg1.to_csv(outputdirectory/\"graph.csv\", sep=\"|\", index=False)\n    df.to_csv(outputdirectory/\"chunks.csv\", sep=\"|\", index=False)\nelse:\n    dfg1 = pd.read_csv(outputdirectory/\"graph.csv\", sep=\"|\")\n\nTry-Except\n\nExample\nimport os\ntry:\n    env_var = os.environ['ENV']\nexcept KeyError:\n    # Do something\n\nIf “ENV” is not a present a KeyError is thrown. Then, except section executed.\n\n\nMatch (&gt; Python 3.10) (switch function)\nmatch object:\n    case &lt;pattern_1&gt;:\n        &lt;action_1&gt;\n    case &lt;pattern_2&gt;:\n        &lt;action_2&gt;\n    case &lt;pattern_3&gt;:\n        &lt;action_3&gt;\n    case _:\n        &lt;action_wildcard&gt;\n\n“object” is just a variable name; could be anything\n“case_” is the value used when none of the other cases are a match\nExample: function input inside user function\ndef http_error(status):\n    match status:\n        case 200:\n            return 'OK'\n        case 400:\n            return 'Bad request'\n        case 401 | 403 | 404:\n            return 'Not allowed'\n        case _:\n            return 'Something is wrong'\nExample: dict input inside a function\ndef get_service_level(user_data: dict):\n    match user_data:\n        case {'subscription': _, 'msg_type': 'info'}:\n            print('Service level = 0')\n        case {'subscription': 'free', 'msg_type': 'error'}:\n            print('Service level = 1')\n        case {'subscription': 'premium', 'msg_type': 'error'}:\n            print('Service level = 2')\nExample: inside a class\nclass ServiceLevel:\n    def __init__(self, subscription, msg_type):\n        self.subscription = subscription\n        self.msg_type = msg_type\n\n    def get_service_level(user_data):\n        match user_data:\n            case ServiceLevel(subscription=_, msg_type='info'):\n                print('Level = 0')\n            case ServiceLevel(subscription='free', msg_type='error'):\n                print('Level = 1')\n            case ServiceLevel(subscription='premium', msg_type='error'):\n                print('Level = 2')\n            case _:\n                print('Provide valid parameters')\n\nNote that inside the function, the change from “:” to “=”  and “()” following the class name in the “case” portion of the match\n\n\nAssert\n\nUsed to confirm a condition\n\nIncorrect: assert condition, message \n\nCorrect method: \nif not condition: \n    raise AssertionError\n\nassert is useful for debugging code because it lets you test if a condition in your code returns True, if not, the program will raise an AssertionError.\n** Do not use in production, because when code is executed with the -O (optimize) flag, the assert statements are removed from the bytecode. **",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-loops",
    "href": "qmd/python-general.html#sec-py-gen-loops",
    "title": "General",
    "section": "Loops",
    "text": "Loops\n\nMisc\n\nList Comprehensions vs Generators in terms of memory usage\n\n{{tqdm}} - progress bar for loops\nfrom tqdm import tqdm\nfor i in tqdm(range(10000))\n    ...\nbreak terminates the loop containing it\n\nIf in a nested loop, it will terminate the inner-most loop containing it\n\ncontinue is used to skip the remaining code inside a loop for the current iteration only; forces the start of the next iteration of the loop\npass does nothing\n\nused when a statement or a condition is required to be present in the program but we do not want any command or code to execute\n\n\n\n\nIterators\n\nRemembers values\nExample\nD = {\"123\":\"Y\",\"111\":\"PT\",\"313\":\"Y\",\"112\":\"Y\",\"201\":\"PT\"}\nff = filter(lambda e:e[1]==\"Y\", D.items())\n\nprint(next(ff))\n&gt;&gt; ('123', 'Y')\nprint(next(ff))\n&gt;&gt; ('313', 'Y')\napply\n\naxis\n\n0 or ‘index’: apply function to each column.\n1 or ‘columns’: apply function to each row.\n\nExample: Function applied to rows of a column of a dataframe (i.e. cells)\ndef df2Graph(dataframe: pd.DataFrame, model=None) -&gt; list:\n  # dataframe.reset_index(inplace=True)\n  results = dataframe.apply(\n    lambda row: graphPrompt(row.text, {\"chunk_id\": row.chunk_id}, model), axis=1\n  )\n\ntext and chunk_id are column names of the dataframe\nrow is the row of the dataframe since axis=1, and from that row, the columns text and chunk_id are subsetted in the arguments of user-defined function.\n\nExample: Formula applied to rows of a column of a dataframe (i.e. cells)\ngrade['evaluate']=grade['MathScore'].apply(lambda x: round((x**x)/2,2))\n\ngrade is the df; MathScore is a numeric column; evaluate is the new column in the df\n\n\n\n\n\nGenerators\n\nGenerators are iterators, a kind of iterable you can only iterate over once. (normal iterators like lists, strings, etc. can be repeatedly iterated over)\nGenerators do not store all the values in memory, they generate the values on the fly\n\nyield - Pauses the function saving all its states and later continues from there on successive calls.\n\nAllows you to consume one element at a time and work with it without requiring you to have every element in memory.\nProduces a generator\n\n\nMisc\n\n{{itertools}} islice can slice a generator.\nAlso see APIs &gt;&gt; {{requests}} for an example\n\nExample: Using a comprehension \nmygenerator = (x*x for x in range(3))\nfor i in mygenerator:\n...    print(i)\n\nProduce a list and ( ) produce a generator \n\nExample: Using a function\ndef create_generator():\n    mylist = range(3)\n    for i in mylist:\n        yield i*i\n\nfor i in mygenerator:\n    print(i)\n0\n1\n4\n\nThe first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then it’ll return the first value of the loop.\nThen, each subsequent call will run another iteration of the loop you have written in the function and return the next value.\nThis will continue until the generator is considered empty, which happens when the function runs without hitting yield.\n\nThat can be because the loop has come to an end, or because you no longer satisfy an “if/else”\n\n\nExample: Sending values to (yield)\ndef grep(pattern):\nprint \"Looking for %s\" % pattern\nwhile True:\n    line = (yield)\n    if pattern in line:\n        print line,\ng = grep(\"python\")  # instantiate with \"python\" pattern to search for\n\ng.next() # Prime it\n&gt;&gt; Looking for python\n\ng.send(\"A series of tubes\") # \"python\" not present so returns nothing\ng.send(\"python generators rock!\") # \"python\" present so returns line\n&gt;&gt; python generators rock!\ng.close() # closes coroutine\n\n(yield) receives the input of the .send method and creates a generator object which is assigned to “line”.\nAll coroutines must be “primed” by first calling .next() (or send(None))\n\nThis advances execution to the location of the first yield expression\n\n\nExample: Sending values to (yield)\ndef writer():\n    \"\"\"A coroutine that writes data *sent* to it to fd, socket, etc.\"\"\"\n    while True:\n        w = (yield)\n        print('&gt;&gt; ', w)\ndef writer_wrapper(coro):\n    # TBD\n    pass\nw = writer()\nwrap = writer_wrapper(w)\nwrap.send(None)  # \"prime\" the coroutine\nfor i in range(4):\n    wrap.send(i)\n&gt;&gt;  0\n&gt;&gt;  1\n&gt;&gt;  2\n&gt;&gt;  3\n\nA more complex framework if you want to break the workflow into multiple functions\n\n\n\nUsing yield from\n\nAllows for two-way usage (reading/sending) of generators\nExample (reading from a generator)\ndef reader():\n    \"\"\"A generator that fakes a read from a file, socket, etc.\"\"\"\n    for i in range(4):\n        yield '&lt;&lt; %s' % i\n\n# with yield\ndef reader_wrapper(g):\n    # Manually iterate over data produced by reader\n    for v in g:\n        yield v\n# OR with yield from\ndef reader_wrapper(g):\n    yield from g\nwrap = reader_wrapper(reader())\nfor i in wrap:\n    print(i)\n\nBasic; only eliminates 1 line of code\n\nExample (sending to a generator)\n# with (yield)\ndef writer_wrapper(coro):\n    coro.send(None)  # prime the coro\n    while True:\n        try:\n            x = (yield)  # Capture the value that's sent\n            coro.send(x)  # and pass it to the writer\n        except StopIteration:\n            pass\n# OR with yield from\ndef writer_wrapper(coro):\n    yield from coro\n\nNeed to see example 4 for the writer() code and the use case\nShows the other advantage of using “yield from”: it automatically includes the code to stop prime and stop the loop.\n\nReusable generator\n\n\nreading example using “yield from”\n\nSlicing a generator\nfrom itertools import islice\ndef gen():\n    yield from range(1,11)\ng = gen()\nmyslice = islice(g, 2)\n&gt;&gt; list(myslice)\n[1, 2]\n&gt;&gt; [i for i in g]\n[3,4,5,6,7,8,9,10]\n\n\n\n\nFor\n\n\nSyntax - for &lt;sequence&gt;: &lt;loop body&gt;\nNumeric Range\nfor i = 1 to 10\n    &lt;loop body&gt;\n\n# from 0 to 519\nfor i in range(520)\n    &lt;loop body&gt;\n\nres = 0\nfor idx in np.arange(0, 100000):\n  res += df.loc[idx, 'int']\n\nnp.arange() ran 8000 times faster than the same chunk using range()\n\nList\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\neven_numbers = []\nfor item in numbers:\n    if item % 2 == 0:\n        even_numbers.append(item)\nprint(even_numbers)\n\n# results: [2, 4, 6, 8]\nList: index and value\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nfor index, element in enumerate(numbers):\n    if element % 2 != 0:\n        numbers[index] = element * 2\n    else:\n        continue\nprint(numbers)\n# results: [2, 2, 6, 4, 10, 6, 14, 8, 18]\n\nenumerate also gets the index of the respective element at the same time\n\nWith three expressions\nfor (for i = 1; i &lt;= 10; i+=1)\n    &lt;loop body&gt;\nCollection-Based\n\nIf the collection is a dict, then this just iterates over the keys\n\nfor i in &lt;collection&gt;:\n    &lt;loop body&gt;\nIterate over a sliding window\n\nOver dictionary keys and values of a dict\nfor a,b in transaction_data.items():\n    print(a,’~’,b)\n\nThe .items method includes both key and value, so it iterates over the pairs.\n\nOver nested dictionaries\nfor k, v in transaction_data_n.items():\n    if type(v) is dict:\n        for nk, nv in v.items(): \n            print(nk,’ →’, nv)\n\nIf the item of the dict is itself a dict then another loop iterates through its items.\nnk and nv stand for nested key and nested value\n\nSelecting a specific item in a nested dictionary\nfor k, v in transaction_data_n.items():\n    if type(v) is dict and k == 'transaction_2':\n        for sk, sv in v.items():\n            print(sk,'--&gt;', sv)\n\nOnly transaction_2’ s items are printed\n\nRows of a data.frame\nres = 0\nfor row in df.itertuples():\n  res += getattr(row, 'int')\n\nitertuples()  is 30x faster than iterrows()\n\n\n\n\nzip\n\nCombine lists into 1 list of tuples\nacc_values = [1, 0.04, 0.9]\nacc_names = [\"RMSE\", \"MAPE\", \"R-sq\"]\nacc_list = list(zip(acc_names, acc_values))\nacc_list\n[('RMSE', 1), ('MAPE', 0.04), ('R-sq', 0.9)]\n\nzip does take lists of different lengths but will create shortest length list with corresponding elements\nCombine lists of unequal lengths but keep the non-paired elements\nfrom itertools import zip_longest\nacc_names3 = [\"RMSE\", \"MAPE\", \"R-sq\", \"MSE\"]\nacc_values3 = [rmse, mape, rsq] \nacc_list3 = list(zip_longest(acc_names3, acc_values3))\n\nUnzip list of tuples into separate lists\nnames, values = zip(*acc_list)\n\nAsterisk is the “unzipping operator”\n\nUnpack dict into a list of separate tuples for key:value pairs\nacc_tuples = list(zip(acc_dict.keys(), acc_dict.values()))\nacc_tuples\n[('RMSE', 1), ('MAPE', 0.04), ('R-sq', 0.9)]\n\n\n\nComprehensions\n\nMisc\n\n‘for — in’ construct within comprehensions is faster than the traditional for-loops\n\nnot faster than (all?) lambda-filters (see functions &gt;&gt; lambda)\n\nReturns lists or dicts (just change the bracket types)\n\nDicts\n\nSyntax: mydict = {key:val for key, val in zip(keys_list, vals_list)}\nCombine key:value lists into a dictionary\nacc_dict = {k:v for k,v in zip(acc_names, acc_values)}\nReturn value and output of expression\nmydict = {v: v**2 for v in numberslist}\nIf numberslist =[1,2,3], then mydict = {1:1, 2:4, 3:9}\n\nLists\n\nSyntax: newlist = [expression for item in iterable if condition == True]\nWith expression\nmylist = [x**2 for x in numberslist]\n\nif numberslist =[1,2,3], then mylist = [1,4,9]\n\nSet values in a list to uppercase\nnewlist = [x.upper() for x in fruits]\nWith conditional expression (if — else)\n\nAppend to the comprehension to filter the dictionary or list\nSyntax: mylist = [expressionA if (condition2==True) else expressionB for item in list if (condition1==True)]\nExample: newlist = [x if x != \"banana\" else \"orange\" for x in fruits]\n\nReturn “orange” instead of “banana”\n\nExample: new_list = [(x**2) if (x&gt;90) else (x**3) for x in old_list if (x%2==0)]\n\nSays\n\nSquare an argument if it exceeds 90, else cube it \nReturn all the exponentiated results only if the argument was an even number\n\n\nExample: c = [d for d in datstrlist if ((d.endswith(\"urday\") or d.endswith(\"unday\")) and \"Oc\" in d)]\n\nString filter than looks for strings with saturdays and sundays in october\n*Slower than a lamda-filter* (See Functions &gt;&gt; lambda)\n\n\n\nNested\n\nSyntax: myset = {{expression(itemA, itemB) for itemA in setA} for itemB in setB}\nExample: {j for i in range(2, int(N**0.5)+1) for j in range(i**2, N, i)}\n\nN = 100000\nCreates a set of all the integers from 2 to 100,000.\nPaces through all the integers i up to the square root of N\nDiscards from the set of 100,000 those numbers j which are equal or larger than the square of i\n\nExample: From link\n# Function to get set labels\ndef get_prediction_set_labels(prediction_set, class_labels):\n    # Get set of class labels for each instance in prediction sets\n    prediction_set_labels = [\n        set([class_labels[i] for i, x in enumerate(prediction_set) if x]) for prediction_set in \n        prediction_sets]\n    return prediction_set_labels\n\nReturns a list where each object in the list is a set object (e.g. {green}, {green, orange})",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-debug",
    "href": "qmd/python-general.html#sec-py-gen-debug",
    "title": "General",
    "section": "Debugging",
    "text": "Debugging\n\nMisc\nTerms\n\nException Errors - Raised when the syntax is correct but the program results in an error.\nSyntax Errors - Occur when the interpreter detects invalid syntax (relatively easier to fix)\n\ne.g. unmatched parenthesis\n\nTraceback - A report that helps us understand the reason for an exception.\n\nContains function calls made in the code along with their line numbers",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-errhand",
    "href": "qmd/python-general.html#sec-py-gen-errhand",
    "title": "General",
    "section": "Error Handling",
    "text": "Error Handling\n\ntry + except\n\nSays try the main code snippet, but if an exception (error) occurs, run the secondary code snippet, the workaround.\n\ndef pct_difference_error_handling(n1, n2):\n  '''Function that takes two numbers and return the percentual difference\n  between n1 and n2, being n1 the reference number'''\n\n  # Try the main code\n  try:\n    pct_diff = (n1-n2)/n1\n    return f'The difference between {n1} and {n2} is {n1-n2}, which is {pct_diff*100}% of {n1}'\n\n  # If you find an error, use this code instead\n  except:\n    pct_diff = (int(n1)-int(n2))/int(n1)\n    return f'The difference between {n1} and {n2} is {int(n1)-int(n2){style='color: #990000'}[}]{style='color: #990000'}, which is {pct_diff*100}% of {n1}'\n\n  # Optional\n  finally:\n    print(\"Code ended\")\n\nAssumes the error will be the user enters a string instead of a numeric. If errors, converts string to numeric and calcs.\nfinally: - This argument will always run, regardless if the try block raises an error or not. So it could be a completion message or a summary, for example.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-jupyter.html",
    "href": "qmd/python-jupyter.html",
    "title": "Jupyter",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Python",
      "Jupyter"
    ]
  },
  {
    "objectID": "qmd/python-jupyter.html#sec-py-jupy-misc",
    "href": "qmd/python-jupyter.html#sec-py-jupy-misc",
    "title": "Jupyter",
    "section": "",
    "text": "Docs\nKernel Menu Commands (navbar)\n\nInterrupt: This command stops the processes that are currently running in a cell. This can be used, for example, to stop the training of a model, even if not all training epochs have been reached yet.\nRestart & Run All: With this command, all cells can be executed again and the previous variables were deleted. This can be useful if you want to read a newer data set into the existing program.\nRestart: The sole command “Restart” leads to the same result, but not all cells are executed again.\nReconnect: When training large models, the kernel can “die” because the memory is full. Then a reconnect makes sense.\nShutdown: As long as a kernel is still running, it also ties up memory. If you run other programs in parallel for which you want to free memory, the “Shutdown” command can make sense.\n\nAdd HTML to markdown cells\n\nLimit Length of Cell Output\n\nRight click output cell &gt;&gt; Click “Enable Scrolling for Outputs”",
    "crumbs": [
      "Python",
      "Jupyter"
    ]
  },
  {
    "objectID": "qmd/python-jupyter.html#sec-py-jupy-shortcuts",
    "href": "qmd/python-jupyter.html#sec-py-jupy-shortcuts",
    "title": "Jupyter",
    "section": "Shortcuts",
    "text": "Shortcuts\n\nPreloaded\n\n\n\n\n\n\n\n\nDescription\nShortcut\n\n\n\n\nshift + Enter\nRun current cell and select the cell below\n\n\nctrl/cmd + Enter\nRun current cell\n\n\nalt/option + Enter\nRun current cell and insert another cell below\n\n\nctrl/cmd + s\nSave notebook\n\n\ni, i\nInterupt cell calculation\n\n\n0, 0\nRestart cell calculation\n\n\n\n\nCustom\n\nCreate/Edit\n\nNavbar &gt;&gt; Settings &gt;&gt; Advanced Settings Editor\nClick JSON Settings Editor (Top Right)\nCLick on User Preferences tab\n\nExample\n\n{\n    \"command\": \"notebook:move-cell-up\",\n    \"keys\": [\n        \"Ctrl Shift ArrowUp\"\n    ],\n    \"selector\": \".jp-Notebook:focus\"\n},\n{\n    \"command\": \"notebook:move-cell-down\",\n    \"keys\": [\n        \"Ctrl Shift ArrowDown\"\n    ],\n    \"selector\": \".jp-Notebook:focus\"\n},",
    "crumbs": [
      "Python",
      "Jupyter"
    ]
  },
  {
    "objectID": "qmd/python-jupyter.html#sec-py-jupy-ops",
    "href": "qmd/python-jupyter.html#sec-py-jupy-ops",
    "title": "Jupyter",
    "section": "Operations",
    "text": "Operations\n\nFirst click inside cell &gt;&gt; Press Esc to enter Command mode (Cursor should stop blinking)\n\n\nInsert/Delete Cells\n\na - Insert new cell above current cell\ndd (press d twice) - Delete current cell\nb - Insert new cell below current cell\n\n\n\nChange Cell Type\n\nm - Markdown mode (for writing comments and headers)\ny - Code mode\n\n\n\nSelect Multiple Cells\n\nWhile holding Shift + use the \\(\\uparrow\\) or \\(\\downarrow\\) to expand the selection",
    "crumbs": [
      "Python",
      "Jupyter"
    ]
  },
  {
    "objectID": "qmd/python-pandas-recipes.html",
    "href": "qmd/python-pandas-recipes.html",
    "title": "Pandas, Recipes",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Python",
      "Pandas, Recipes"
    ]
  },
  {
    "objectID": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-misc",
    "href": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-misc",
    "title": "Pandas, Recipes",
    "section": "",
    "text": "{{sketch}}: AI code-writing assistant for pandas users that understands the context of your data, greatly improving the relevance of suggestions. Sketch is usable in seconds and doesn’t require adding a plugin to your IDE. It’s just a regular function + method.",
    "crumbs": [
      "Python",
      "Pandas, Recipes"
    ]
  },
  {
    "objectID": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-trans",
    "href": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-trans",
    "title": "Pandas, Recipes",
    "section": "Transformations",
    "text": "Transformations\n\nBin a numeric\ndf = pd.DataFrame({\"value\": np.random.randint(0, 100, 20){style='color: #990000'}[}]{style='color: #990000'})\nlabels = [\"{0} - {1}\".format(i, i + 9) for i in range(0, 100, 10)]\ndf[\"group\"] = pd.cut(df.value, range(0, 105, 10), right=False, labels=labels)\ndf.head(10)\n  value    group\n0    65  60 - 69\n1    49  40 - 49\n2    56  50 - 59\n3    43  40 - 49\n4    43  40 - 49\n5    91  90 - 99\n6    32  30 - 39\n7    87  80 - 89\n8    36  30 - 39\n9      8    0 - 9\n\nqcut will create labels, so this probably isn’t needed",
    "crumbs": [
      "Python",
      "Pandas, Recipes"
    ]
  },
  {
    "objectID": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-strs",
    "href": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-strs",
    "title": "Pandas, Recipes",
    "section": "Strings",
    "text": "Strings\n\nGenerate list of strings from a variable\ns = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\nnew_categories = [\"Group %s\" % g for g in s.cat.categories]\nnew_categories\n['Group a', 'Group b', 'Group c']\nExtract city, state, and zip code from an address variable",
    "crumbs": [
      "Python",
      "Pandas, Recipes"
    ]
  },
  {
    "objectID": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-eda",
    "href": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-eda",
    "title": "Pandas, Recipes",
    "section": "Comparisons",
    "text": "Comparisons\n\nequals\n# series\nseries1 = pd.Series([1,2,3,4])\nseries2 = pd.Series([2,1,3,4])\nseries1.equals(series2)\n\n# dfs\ndf[\"device_id\"].equals(df1[\"device_id\"])\n#&gt; True\n\n# List of the columns having different values in the DataFrames df1 and df\nfor column in df.columns:\n  if df[column].equals(df1[column]):\n     pass\n  else:\n      print(column)\n\nFlags differences in order, dimensions, and of course, differences in data\n\ncompare\n\ndf4 = df.compare(df1)\ndf4\n\ndevice-temperature and device-status are the two common columns being compared\nself indicates the first DataFrame df and other indicates the other DataFrame df1.\nEssentially merges both the DataFrames and adds a MultiIndex to show both the DataFrames columns side by side, which helps you to see the columns and positions where the values have been changed.",
    "crumbs": [
      "Python",
      "Pandas, Recipes"
    ]
  },
  {
    "objectID": "qmd/python-snippets.html",
    "href": "qmd/python-snippets.html",
    "title": "Snippets",
    "section": "",
    "text": "Refactoring",
    "crumbs": [
      "Python",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/python-snippets.html#sec-py-snip-refclfun",
    "href": "qmd/python-snippets.html#sec-py-snip-refclfun",
    "title": "Snippets",
    "section": "",
    "text": "Convert group of functions into a class\n\nFrom pybites video\nGroup of functions before refactoring\napi_config = {\n    \"api_url\": \"https://example.com/api\",\n    \"api_key\": \"1234567890abcdef\",\n}\n\ndef setup_connection(api_url, api_key, user_id, session_token):\n    print(f\"Setting up connection to {api_url} with API key {api_key}, for user {user_id} with session {session_token}\")\n\ndef fetch_data(user_id, session_token):\n    setup_connection(api_config['api_url'], api_config['api_key'], user_id, session_token)\n    print(f\"Fetching data for user {user_id} with session {session_token}\")\n\ndef process_data(user_id, session_token, data):\n    setup_connection(api_config['api_url'], api_config['api_key'], user_id, session_token)\n    print(f\"Processing data {data} for user {user_id} with session {session_token}\")\n\ndef save_data(user_id, session_token, data):\n    setup_connection(api_config['api_url'], api_config['api_key'], user_id, session_token)\n    print(f\"Saving data {data} for user {user_id} with session {session_token}\")\n\nMany of the functions have the same arguments\nMany of the functions call the same function\n\nThe created class after refactoring\nclass ApiClient:\n\n    def __init__(self, config, user_id, session_token):\n        self.api_url = config['api_url']\n        self.api_key = config['api_key']\n        self.user_id = user_id\n        self.session_token = session_token\n        self._setup_connection()\n\n    def _setup_connection(self):\n        print(f\"Setting up connection to {self.api_url} with API key {self.api_key}, for user {self.user_id} with session {self.session_token}\")\n\n    def fetch_data(self):\n        print(f\"Fetching data for user {self.user_id} with session {self.session_token}\")\n\n    def process_data(self, data):\n        print(f\"Processing data {data} for user {self.user_id} with session {self.session_token}\")\n\n    def save_data(self, data):\n        print(f\"Saving data {data} for user {self.user_id} with session {self.session_token}\")\n\napi_config = {\n    \"api_url\": \"https://example.com/api\",\n    \"api_key\": \"1234567890abcdef\",\n}\nclient = ApiClient(api_config, 123, \"abc\")\nclient.fetch_data()\nclient.process_data(\"some data\")\nclient.save_data(\"some other data\")\n\n\n\nGather a group of constants into an enum classs\n\nFrom pybites video\nGroup of constants all related to a common concept (e.g. user status)\nEnums are a class that makes code more organized and readable by grouping constants with common concepts into classes\nConstants before refactoring\nSTATUS_ACTIVE = 1\nSTATUS_INACTIVE = 2\nSTATUS_PENDING = 3\nSTATUS_CANCELLED = 4\nSTATUS_COMPLETED = 5\n\ndef update_user_status(user_id: int, status: int):\n    if status == STATUS_ACTIVE:\n        print(\"Activating user\")\n    elif status == STATUS_INACTIVE:\n        print(\"Deactivating user\")\n    # etc\n\nupdate_user_status(123, STATUS_ACTIVE)\n#&gt; Activating user\nConstants after refactoring into an enum\nfrom enum import Enum\n\nclass Status(Enum):\n    ACTIVE = 1\n    INACTIVE = 2\n    PENDING = 3\n    CANCELLED = 4\n    COMPLETED = 5\n\ndef update_user_status(user_id: int, status: Status):\n    if status is Status.ACTIVE:\n        print(\"Activating user\")\n    elif status is Status.INACTIVE:\n        print(\"Deactivating user\")\n    # etc\n\nupdate_user_status(123, Status.INACTIVE)\n#&gt; Deactivating user\nStatus.ACTIVE.name\n#&gt; 'ACTIVE'\nStatus.INACTIVE.value\n#&gt; 2\nStatus.__members__\n#&gt; mappingproxy({'ACTIVE': &lt;Status.ACTIVE: 1&gt;,\n#&gt;               'INACTIVE': &lt;Status.INACTIVE: 2&gt;, \n#&gt;               ...etc})\ntype(Status.ACTIVE)\n#&gt; &lt;enum 'Status'&gt;",
    "crumbs": [
      "Python",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/python-snippets.html#sec-py-snip-mlset",
    "href": "qmd/python-snippets.html#sec-py-snip-mlset",
    "title": "Snippets",
    "section": "ML Set-Up",
    "text": "ML Set-Up\n# Suppress (annoying) warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nignore_warnings(category=ConvergenceWarning)\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n    os.environ[\"PYTHONWARNINGS\"] = ('ignore::UserWarning,ignore::RuntimeWarning')\n\n# Ensure logging\nlogging.basicConfig(\n    format='%(asctime)s:%(name)s:%(levelname)s - %(message)s',\n    level=logging.INFO,\n    handlers=[\n        logging.FileHandler(\"churn_benchmarking.log\"),\n        logging.StreamHandler()\n    ],\n    datefmt='%Y-%m-%d %H:%M:%S')\n\n# Determine number of cpus available\nn_cpus = mp.cpu_count()\nlogging.info(f\"{n_cpus} cpus available\")\n\n# Visualize pipeline when calling it\nset_config(display=\"diagram\")\n\n# Load prepared (pre-cleaned) files for benchmarking\nfile_paths = [f for f in glob.glob(\"00_data/*\") if f.endswith('_cleaned.csv')]\nfile_names = [re.search('[ \\w-]+?(?=\\_cleaned.)',f)[0] for f in file_paths]\ndfs = [pd.read_csv(df, low_memory=False) for df in file_paths]\ndata_sets = dict(zip(file_names, dfs))\nif not data_sets:\n    logging.error('No data sets have been loaded')\n    raise ValueError(\"No data sets have been loaded\")\nlogging.info(f\"{len(data_sets){style='color: #990000'}[}]{style='color: #990000'} data sets have been loaded.\")",
    "crumbs": [
      "Python",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/python-snippets.html#sec-py-snip-dlunzip",
    "href": "qmd/python-snippets.html#sec-py-snip-dlunzip",
    "title": "Snippets",
    "section": "Download and Unzip helper",
    "text": "Download and Unzip helper\nimport urllib.request\nfrom zipfile import ZipFile\nimport os\ndef extract(url: str, dest: str, target: str = '') -&gt; None:\n    \"\"\"\n    Retrieve online data sources from flat or zipped CSV.\n    Places data in data/raw subdirectory (first creating, as needed).\n    For zip file, automatically unzip target file. \n    Args:\n        url (str): URL path to the source file to be downloaded \n        dest (str): File  for the destination file to land\n        target (str, optional): Name of file to extract (in case of zipfile). Defaults to ''.\n    \"\"\"\n    # set-up expected directory structure, if not exists\n    if not os.path.exists('data'):\n        os.mkdir('data')\n    if not os.path.exists('data/raw'):\n        os.mkdir('data/raw')\n\n    # download file to desired location\n    dest_path = os.path.join('data', 'raw', dest)\n    urllib.request.urlretrieve(url, dest_path)\n    # unzip and clean-up (remove zip) if needed\n    if target != '':\n        with ZipFile(dest_path, 'r') as zip_obj:\n            zip_obj.extract(target, path = \"data//raw\")\n        os.remove(dest_path)\n\nfrom helpers.extract import extract\nurl_cps_suppl = 'https://www2.census.gov/programs-surveys/cps/datasets/2020/supp/nov20pub.csv'\nextract(url_cps_suppl, 'cps_suppl.csv')\n\nFrom Riederer (github, article)",
    "crumbs": [
      "Python",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/python-snippets.html#sec-py-snip-easot",
    "href": "qmd/python-snippets.html#sec-py-snip-easot",
    "title": "Snippets",
    "section": "Extract a section of text",
    "text": "Extract a section of text\n\n\nDesired section of text is split between 2 “~~~” strings\nProcess\n\nString is split into lines\nFind the start and stop indexes for the 2 “~~~”\nExtract lines between to the two indexes",
    "crumbs": [
      "Python",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/python-snippets.html#sec-py-snip-shstup",
    "href": "qmd/python-snippets.html#sec-py-snip-shstup",
    "title": "Snippets",
    "section": "Shell Start-Up",
    "text": "Shell Start-Up\n\nA start-up script automatically imports libraries, definines functions, or sets variables, etc. when the python interpreter is started.\n\nEvery time you start a shell, the first thing you usually do is import a bunch of stuff, or frenetically press the top arrow key to recall something from your history. This is aggravated by the fact Python has very limited support for reloading changed modules in a shell, so restarting it is a common thing.\n\nSteps\n\nChoose a location for your script which can be anywhere\nCreate python script at the location and fill in whatever you want to happen when you start a python REPL\n\nName can be pythonstartup.py or whatever\n\nSet the PYTHONSTARTUP environment variable to the path of the file\n\nWindows:\n\nCMD\nset PYTHONSTARTUP=C:\\path\\to\\pythonstartup.py\nPowershell\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nMac/Linux:\nexport PYTHONSTARTUP=/path/to/pythonstartup.py\n\n\nExample: From Happiness is a good PYTHONSTARTUP script\n\nimport atexit\n\n# First, a lot of imports. I don't use all of them all the time, \n# but I like to have them available.\n\nimport csv\nimport datetime as dt\nimport hashlib\nimport json\nimport math\nimport os\nimport random\nimport re\nimport shelve\nimport subprocess\nimport sys\nimport tempfile\nfrom collections import *\nfrom functools import partial\nfrom inspect import getmembers, ismethod, stack\nfrom io import open\nfrom itertools import *\nfrom math import *\nfrom pprint import pprint as pretty_print\nfrom types import FunctionType\nfrom uuid import uuid4\nfrom unittest.mock import patch, Mock, MagicMock\nfrom datetime import datetime, date, timedelta\n\n\nimport pip\n\n# Set ipython prompt to \"&gt;&gt;&gt; \" for easier copying\ntry:\n    from IPython import get_ipython\n\n    get_ipython().run_line_magic(\"doctest_mode\", \"\")\n    get_ipython().run_line_magic(\"load_ext\", \"ipython_autoimport\")\nexcept:\n    pass\n\n\n\ntry:\n    import asyncio \n    # for easier pasting\n    from typing import * \n    from dataclasses import dataclass, field\nexcept ImportError:\n    pass\n\n# Mostly to parse strings to dates\ntry:\n    import pendulum\nexcept ImportError:\n    pass\n\n# I think you know why\ntry:\n    import requests\nexcept ImportError:\n    pass\n\n# If I'm in a regular Python shell, at least activate tab completion\ntry:\n    import readline\n\n    readline.parse_and_bind(\"tab: complete\")\nexcept ImportError:\n    pass\n\ntry:\n    # if rich is installed, set the repr() to be pretty printted\n\n    from rich import pretty \n    pretty.install() \n\nexcept ImportError:\n    pass\n\n# I wish Python had a Path literal but I can get pretty close with this:\n# Tiis let me to p/\"path/to/file\" to get a Path object\nfrom pathlib import Path\ntry:\n    class PathLiteral:\n        def __truediv__(self, other):\n            try:\n                return Path(other.format(**stack()[1][0].f_globals))\n            except KeyError as e:\n                raise NameError(\"name {e} is not defined\".format(e=e))\n\n        def __call__(self, string):\n            return self / string\n\n    p = PathLiteral()\nexcept ImportError:\n    pass\n\n\n# Force jupyter to print any lone variable, not just the last one in a cell\ntry:\n    from IPython.core.interactiveshell import InteractiveShell\n\n    InteractiveShell.ast_node_interactivity = \"all\"\n\nexcept ImportError:\n    pass\n\n\n# Check if I'm in a venv\nVENV = os.environ.get(\"VIRTUAL_ENV\")\n\n#  Make sure I always have a temp folder ready to go\nTEMP_DIR = Path(tempfile.gettempdir()) / \"pythontemp\"\ntry:\n    os.makedirs(TEMP_DIR)\nexcept Exception as e:\n    pass\n\n# I'm lazy\ndef now():\n    return datetime.now()\n\n\ndef today():\n    return date.today()\n\n\n# Since restarting a shell is common, I like to have a way to persit\n# calculations between sessions. This is a simple way to do it.\n# I can do store.foo = 'bar' and get store.foo in the next session.\nclass Store(object):\n    def __init__(self, filename):\n\n        object.__setattr__(self, \"DICT\", shelve.DbfilenameShelf(filename))\n        # cleaning the dict on the way out\n        atexit.register(self._clean)\n\n    def __getattribute__(self, name):\n        if name not in (\"DICT\", \"_clean\"):\n            try:\n                return self.DICT[name]\n            except:\n                return None\n        return object.__getattribute__(self, name)\n\n    def __setattr__(self, name, value):\n        if name in (\"DICT\", \"_clean\"):\n            raise ValueError(\"'%s' is a reserved name for this store\" % name)\n        self.DICT[name] = value\n\n    def _clean(self):\n        self.DICT.sync()\n        self.DICT.close()\n\n\npython_version = \"py%s\" % sys.version_info.major\ntry:\n    store = Store(os.path.join(TEMP_DIR, \"store.%s.db\") % python_version)\nexcept:\n    # This could be solved using diskcache but I never took the time\n    # to do it.\n    print(\n        \"\\n/!\\ A session using this store already exist.\"\n    )\n\n\n# Shorcurt to pip install packages without leaving the shell\ndef pip_install(*packages):\n    \"\"\" Install packages directly in the shell \"\"\"\n    for name in packages:\n        cmd = [\"install\", name]\n        if not hasattr(sys, \"real_prefix\"):\n            raise ValueError(\"Not in a virtualenv\")\n        pip.main(cmd)\n\n\ndef is_public_attribute(obj, name, methods=()):\n    return not name.startswith(\"_\") and name not in methods and hasattr(obj, name)\n\n\n# if rich is not installed\ndef attributes(obj):\n    members = getmembers(type(obj))\n    methods = {name for name, val in members if callable(val)}\n    is_allowed = partial(is_public_attribute, methods=methods)\n    return {name: getattr(obj, name) for name in dir(obj) if is_allowed(obj, name)}\n\n\nSTDLIB_COLLECTIONS = (\n    str,\n    bytes,\n    int,\n    float,\n    complex,\n    memoryview,\n    dict,\n    tuple,\n    set,\n    bool,\n    bytearray,\n    frozenset,\n    slice,\n    deque,\n    defaultdict,\n    OrderedDict,\n    Counter,\n)\n\ntry:\n    # rich a great pretty printer, but if it's not there, \n    # I have a decent fallback\n    from rich.pretty import print as pprint\nexcept ImportError:\n\n    def pprint(obj):\n        if isinstance(obj, STDLIB_COLLECTIONS):\n            pretty_print(obj)\n        else:\n            try:\n                name = \"class \" + obj.__name__\n            except AttributeError:\n                name = obj.__class__.__name__ + \"()\"\n            class_name = obj.__class__.__name__\n            print(name + \":\")\n            attrs = attributes(obj)\n            if not attrs:\n                print(\"    &lt;No attributes&gt;\")\n            for name, val in attributes(obj).items():\n                print(\"   \", name, \"=\", val)\n\n\n# pp/obj is a shortcut to pprint(obj), it work as a postfix operator as \n# well, which in the shell is handy\nclass Printer(float):\n    def __call__(self, *args, **kwargs):\n        pprint(*args, **kwargs)\n\n    def __truediv__(self, other):\n        pprint(other)\n\n    def __rtruediv__(self, other):\n        pprint(other)\n\n    def __repr__(self):\n        return repr(pprint)\n\n\npp = Printer()\npp.__doc__ = pprint.__doc__\n\n# Same as the printer, but for turning something into a list with l/obj\nclass ToList(list):\n    def __truediv__(self, other):\n        return list(other)\n\n    def __rtruediv__(self, other):\n        return list(other)\n\n    def __call__(self, *args, **kwargs):\n        return list(*args, **kwargs)\n\n\nl = ToList()\n\n# Those alias means JSON is now valid Python syntax that you can copy/paste \nnull = None\ntrue = True\nfalse = False\n\nAlso has a class for creating fake data. See article for the code.",
    "crumbs": [
      "Python",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/regression-discrete.html",
    "href": "qmd/regression-discrete.html",
    "title": "Discrete",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Regression",
      "Discrete"
    ]
  },
  {
    "objectID": "qmd/regression-discrete.html#sec-reg-disc-misc",
    "href": "qmd/regression-discrete.html#sec-reg-disc-misc",
    "title": "Discrete",
    "section": "",
    "text": "Also see Regression, Other\nPackages\n\n{glmnet}\n\nFor Diagnostics see:\n\n{DHARMa} - Built for Mixed Effects Models for count distributions but handles lm, glm (poisson) and MASS::glm.nb (neg.bin)\nDiagnostics, Probabilistic &gt;&gt; Visual Inspection &gt;&gt; Visual Inspection\nDiagnostics, GLM\n\nWith aggregated counts that are bound within a certain range, it can be better to turn the range of counts into percentages (see example) and model those as your outcome\n\nDistributions\n\nZero-One Inflated Beta\nmod_zoib &lt;-\n  brm(bf(outcome_pct ~ 1),\n      data = example_data,\n      family = zero_one_inflated_beta(),\n      cores = 4)\npp_check(mod_zoib)\n\nZero-One Inflated Binomial\n\nIn general, you can have a zero-N inflated binomial\n\n\nExample: Aggregated counts from 1 to 32 (Thread)\n\n\nIf something specific is generating 1 and 32 counts\n\nIdeally you’d do this, but these require creating bespoke distribution families which is possible in STAN\n\nIf you cannot get zero, then a 0-31-inflated binomial works fine.\nIf 0 is possible but it didn’t happen, then do a 1-32-inflated binomial.\n\nMore conveniently, you’d transform the range (1-32) to percentages where 100% = 32, and use zero-one inflated beta (currently available in {brms} or zero-one inflated binomial\n\nIf there is NOT something specific generating the 1 and 32 counts(?)\n\nYou can keep the counts and treat them as an ordered factor\n\nCollapse the counts from 2-31 into a category, so you have 3 categories: 1, 2-31, 32.\nModel as an ordered logit\nmod_ologit &lt;- \n  brm(bf(outcome_factor ~ 1),\n      data = example_data,\n      family = cumulative,\n      cores = 4)\npp_check(mod_ologit)",
    "crumbs": [
      "Regression",
      "Discrete"
    ]
  },
  {
    "objectID": "qmd/regression-discrete.html#sec-reg-disc-terms",
    "href": "qmd/regression-discrete.html#sec-reg-disc-terms",
    "title": "Discrete",
    "section": "Terms",
    "text": "Terms\n\nA saturated model is a regression model that includes a discrete (indicator) variable for each set of values the explanatory variables can take.\n\nAnother case is when there are as many estimated parameters as data points.\n\ne.g. if you have 6 data points and fit a 5th-order polynomial to the data, you would have a saturated model (one parameter for each of the 5 powers of your independant variable plus one for the constant term).\n\nMulti-variable models require interactions to be able to cover each set of values that the explanatory variables can take (see 3rd example)\nSince saturated models, perfectly model the sample, they don’t generalize to the population well.\n\nNo data left to estimate variance.\n\nExamples of Saturated Models\n\nWages ~ College Graduation (binary)\n\\[\n\\operatorname{Wages}_i = \\alpha + \\beta \\:\\mathbb{I}\\{\\operatorname{College Graduate}\\}_i + \\epsilon_i\n\\]\nWages ~ Schooling (discrete, yrs).\n\\[\n\\begin{align}\n\\operatorname{Wages} &= \\alpha + \\beta_1 \\:\\mathbb{I}\\{s_i = 1\\} + \\beta_2 \\:\\mathbb{I}\\{s_i = 2\\} + ⋯ + \\beta_T \\:\\mathbb{I}\\{s_i = T\\}\n&\\text{where}\\quad s_i \\in \\{0, 1, 2,...T\\}\n\\end{align}\n\\]\n\n0 is the reference level; \\(\\beta\\) is the effect of j years of schooling.\n\nWages ~ College Graduation + Gender + Interaction.\n\\[\n\\operatorname{Wages} = \\alpha + \\beta_1 \\:\\mathbb{I}{\\operatorname{College Graduate}} + \\beta_2 \\:\\mathbb{I}\\{\\operatorname{Female}\\} + \\beta_3 \\:\\mathbb{I}\\{\\operatorname{College Graduate}\\} \\times \\:\\mathbb{I}\\{\\operatorname{Female}\\} + ε\n\\]\n\n\\(\\mathbb{E}[\\operatorname{Wages}_i | \\operatorname{College Graduate}_i = 0, \\operatorname{Female}_i = 0] = \\alpha\\)\n\nExpected value of Wages for individual i given they’re not a college graduate and are male\n\n\\(\\mathbb{E}[\\operatorname{Wages}_i | \\operatorname{College Graduate}_i = 1, \\operatorname{Female}_i = 0] = \\alpha + \\beta_1\\)\n\\(\\mathbb{E}[\\operatorname{Wages}_i | \\operatorname{College Graduate}_i = 0, \\operatorname{Female}_i = 1] = \\alpha + \\beta_2\\)\n\\(\\mathbb{E}[\\operatorname{Wages}_i | \\operatorname{College Graduate}_i = 1, \\operatorname{Female}_i = 1] = \\alpha + \\beta_1 + \\beta_2 + \\beta_3\\)\n\n\n\nNull Model has only one parameter, which is the intercept.\n\nThis is essentially the mean of all the data points.\nFor a bivariate model, this is a horizontal line with the same prediction for every point\n\nDeviance\n\\[\nD = 2(L_S - L_P) = 2(\\operatorname{loglik}(y\\;|\\;y) - \\operatorname{loglik}(\\mu\\;|\\;y))\n\\]\n\n\\(L_S\\) is the saturated model\n\\(L_P\\) is the “proposed model” (i.e. the model being fit)",
    "crumbs": [
      "Regression",
      "Discrete"
    ]
  },
  {
    "objectID": "qmd/regression-discrete.html#sec-reg-disc-binom",
    "href": "qmd/regression-discrete.html#sec-reg-disc-binom",
    "title": "Discrete",
    "section": "Binomial",
    "text": "Binomial\n\nExample: UCB Admissions\n# Array to tibble (see below for deaggregation this to 1/0)\nucb &lt;- \n    as_tibble(UCBAdmissions) %&gt;% \n    mutate(across(where(is.character), ~ as.factor(.))) %&gt;% \n    pivot_wider(\n        id_cols = c(Gender, Dept),\n        names_from = Admit,\n        values_from = n,\n        values_fill = 0L\n      )\n\n## # A tibble: 12 × 4\n##    Gender Dept     Admitted Rejected\n##    &lt;fct&gt;  &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n##  1 Male      A          512      313\n##  2 Female    A           89       19\n##  3 Male      B          353      207\n##  4 Female    B           17        8\n##  5 Male      C          120      205\n##  6 Female    C          202      391\n##  7 Male      D          138      279\n##  8 Female    D          131      244\n##  9 Male      E           53      138\n## 10 Female    E           94      299\n## 11 Male      F           22      351\n## 12 Female    F           24      317\n\nglm(\n  cbind(Rejected, Admitted) ~ Gender + Dept,\n  data = ucb,\n  family = binomial\n)\n## Coefficients:\n## (Intercept)  GenderMale        DeptB        DeptC        DeptD        DeptE \n##    -0.68192     0.09987      0.04340      1.26260      1.29461      1.73931 \n##      DeptF \n##    3.30648 \n## \n## Degrees of Freedom: 11 Total (i.e. Null);  5 Residual\n## Null Deviance:     877.1 \n## Residual Deviance: 20.2 AIC: 103.1\n\ncbind(Rejected, Admitted) says that “Rejected” is the response variable since it is listed first in the cbind function\nCan also use a logistic model, but need case-level data (e.g. 0/1)\n\nDeaggregate count data into 0/1 case-level data\ndata(UCBadmit, package = \"rethinking\")\nucb &lt;- UCBadmit %&gt;%\n  mutate(applicant.gender = relevel(applicant.gender, ref = \"male\"))\n\n# deaggregate to 1/0\ndeagg_ucb &lt;- function(x, y) {\n  UCBadmit %&gt;%\n    select(-applications) %&gt;%\n    group_by(dept, applicant.gender) %&gt;%\n    tidyr::uncount(weights = !!sym(x)) %&gt;%\n    mutate(admitted = y) %&gt;%\n    select(dept, gender = applicant.gender, admitted)\n}\nucb_01 &lt;- purrr::map2_dfr(c(\"admit\", \"reject\"),\n                          c(1, 0),\n                          ~ disagg_ucb(.x, .y)\n)\n\n\nExample: Treatment/Control\n            Disease      No Disease\nTreatment        55                67\nControl          42                34\n\ndf &lt;- tibble(treatment_status = c(\"treatment\", \"no_treatment\"),\n      disease = c(55, 42),\n      no_disease = c(67,34)) %&gt;% \n  mutate(total = no_disease + disease,\n        proportion_disease = disease / total) \n\nmodel_weighted &lt;- glm(proportion_disease ~ treatment_status,\n                      data = df,\n                      family = binomial,\n                      weights = total)\nmodel_cbinded &lt;- glm(cbind(disease, no_disease) ~ treatment_status,\n                    data = df,\n                    family = binomial)\n\n# Aggregated counts expanded into case-level data\ndf_expanded &lt;- tibble(disease_status = c(1, 1, 0, 0), \n                      treatment_status = rep(c(\"treatment\", \"control\"), 2)) %&gt;%\n                        .[c(rep(1, 55), rep(2, 42), rep(3, 67), rep(4, 34)), ]\n# logistic\nmodel_expanded &lt;- glm(disease_status ~ treatment_status, data = df_expanded, family = binomial(\"logit\"))\n\nAll methods are equivalent\n“disease” is listed first in the cbind function, therefore it is the response variable.",
    "crumbs": [
      "Regression",
      "Discrete"
    ]
  },
  {
    "objectID": "qmd/regression-discrete.html#sec-reg-disc-pois",
    "href": "qmd/regression-discrete.html#sec-reg-disc-pois",
    "title": "Discrete",
    "section": "Poisson",
    "text": "Poisson\n\n\nMisc\nInterpretation\n\nEffect of a binary treatment\n\\[\ne^\\beta = \\mathbb{E}[Y(1)/\\mathbb{E}Y(0)] = \\theta_{\\text{ATE%}} + 1\n\\]\n\n\\(\\theta\\) is the effect interpreted as a percentage\n\\(\\mathbb{E}[Y(1)]\\) is the expected value of the outcome for a subject assigned to Treatment.\nTherefore, \\(e^\\beta - 1\\) is the average percent increase or decrease from baseline to treatment\nParameter may difficult to interpret in contexts where Y spans several order of magnitudes.\n\nExample: The econometrician may perceive a change in income from $5,000 to $6,000 very differently from a change in income from $100,000 to $101,000, yet both those changes are treatment effects in levels of $1,000 and thus contribute equally to θATE%.",
    "crumbs": [
      "Regression",
      "Discrete"
    ]
  },
  {
    "objectID": "qmd/regression-interactions.html",
    "href": "qmd/regression-interactions.html",
    "title": "Interactions",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Regression",
      "Interactions"
    ]
  },
  {
    "objectID": "qmd/regression-interactions.html#sec-reg-inter-misc",
    "href": "qmd/regression-interactions.html#sec-reg-inter-misc",
    "title": "Interactions",
    "section": "",
    "text": "Also see\n\nPost-Hoc Analysis, emmeans &gt;&gt; Interactions\nIntroduction: Adding Partial Residuals to Marginal Effects Plots for detecting interactions using residuals of a regression model.\nDiagnostics, Model Agnostic &gt;&gt; DALEX &gt;&gt; Instance Level &gt;&gt; Break-Down &gt;&gt; Example: Assume Interactions\n\nResources\n\n{marginaleffects} book - Docs for the package but also tons of examples; python and r code\n\nExperiments, Observational data, Causal inference with G-Computation, Machine learning models, Bayesian modeling, Multilevel regression with post-stratification (MRP), Missing data, Matching, Inverse probability weighting, Conformal prediction\n\n\nRule of Thumb: In a balanced experiment, “you need 16 times the sample size to estimate an interaction than to estimate a main effect” in order to measure the interaction effect with the same precision as the main effect. There are contexts where it’s lower depending on the effect size. Gelman\nSelecting values of continuous predictors to use for calculating marginal effects\n\nMean/Median and quantiles are popular choices\nCheck if they’re multi-modal. If so, then the mean or median may not be a useful value to use for that variable when computing marginal effects.\n\nModes for the distribution would be a suitable alternative.\n\nMay also be useful to try to find intervals of values where there is no significant marginal effect.\nUsing all combinations of values in the observed data to compute individual marginal effects and then taking the average will result in the average interaction effect\n\nHelpful in generating inferences about the whole population of interest instead of scenarios of interest or groups of interest\n\n\nGelman and Hill: “Interactions can be important. In practice, inputs that have large main effects also tend to have large interactions with other inputs. (However, small main effects do not preclude the possibility of large interactions.)”\nWe can probe or decompose interactions by asking the following research questions:\n\nWhat is the predicted Y given a particular X and W? (predicted value)\nWhat is relationship of X on Y at particular values of W? (simple slopes/effects)(marginal effects/predicted means)\nIs there a difference in the relationship of X on Y for different values of W? (comparing marginal effects)\n\nPredictors that are uncorrelated with each other still might have interaction effects on the response variable.\n\nExample: plant growth requires sunlight and rain. Without rain, sunlight will not increase growth and vice versa, but rain with sunlight does cause growth. This interaction effect exists even though amounts of sunlight and rain aren’t (very) correlated with each other\n\nQuadratic interaction example from {marginaleffects} vignette\nDropping main effects\n\nFor the cont x cat interaction, as long as you don’t drop a categorical main effect, I don’t think it affects the model. It may or may not reparameterize the interaction coefficients so they’re the marginal effects instead contrasts. (see article)\n\ne.g. for admit ~ dept + gender:dept , the interpretation of an interaction coefficient might be something like male-female at dept A.\n\nWhether it’s okay to drop the categorical variable, may depend on whether the main effects are significant or not\nDropping a continuous main effects changes the model, so best not to ever do that in cat x cont or cont x cont interactions.\nHarrell said in a SO post that you should never do it.\n\nHe had a link to blog post he’d written but the link was dead and there wasn’t anything in his RMS book.\n\n\nFalse linearity assumptions of main effects can inflate apparent interaction effects because interactions may be colinear with the omitted non-linear effect\nInteractions may also be non-linear, but shouldn’t do this w/smaller sample sizes\n\nWith smaller sample sizes, make the main effect non-linear and the interaction component linear if the non-linear relationship is present between the interaction and outcome.",
    "crumbs": [
      "Regression",
      "Interactions"
    ]
  },
  {
    "objectID": "qmd/regression-interactions.html#sec-reg-inter-terms",
    "href": "qmd/regression-interactions.html#sec-reg-inter-terms",
    "title": "Interactions",
    "section": "Terms",
    "text": "Terms\n\nDisordinal Interactions (aka crossover or antagonistic) - Interaction results whose lines do cross. (see fig in Ordinal Interactions)\n\nWhen an interaction is significant and “disordinal”, interpretation of main effects is likely to be misleading.\n\nTo determine exactly which parts of the interaction are significant, the omnibus F test must be followed by more focused tests or comparisons.\n\n\nMarginal Effects - Partial derivatives of the regression equation with respect to each variable in the model for each unit (i.e. observation) in the data; average marginal effects are simply the mean of these unit-specific partial derivatives over some sample\n\nThe Simple Slope or Simple Effect calculation is the same as the Marginal Effectcalculation from everything I’ve read. The only difference seems to be that the Marginal Effect can pertain a regression model without an interaction while Simple Slopes and Simple Effects are terms only used in describing effects in models with an interaction.\ni.e. the slope of the prediction function, measured at a specific value of the regressor\nIn OLS regression with no interactions or higher-order term, the estimated slope coefficients are average marginal effects\nIn other cases and for generalized linear models, the coefficients are NOT marginal effects at least not on the scale of the response variable\n\nModerator Variable (MV) - A predictor that changes the relationship of a IV on the DV, and it can be continuous or categorical. Used in an interaction to estimate its effect on that relationship. (also see Causal Inference &gt;&gt; Moderation Analysis)\nOrdinal Interactions - Interaction results whose lines do not cross\n\nThis looks like an EDA plot but also see OLS &gt;&gt; continuous:continuous &gt;&gt; Plot Simple Slopes\n\nSimilar plot but for post-hoc analysis of a regression with an interaction\n\nExponential Interaction: If the lines are not parallel but one line is steeper than the other, the interaction effect will be significant (i.e. detected) only if there’s enough statistical power.\nIf the lines are exactly parallel, then there is no interaction effect\n\nSimple Effect - When a Independent Variable interacts with a moderator variable (MV), it’s the differences in predicted values of the outcome variable at a particular level of the MV\n\nSimple Effect, rather than Simple Slope, is the term used when the moderator is a categorical variable instead of continuous, otherwise they’re essentially the same thing.\n\nIt may also be the case that “Simple” is only used with the moderator is equal to 0.\n\nThe Simple Slope or Simple Effect calculation is the same as the Marginal Effect calculation from everything I’ve read. The only difference seems to be that the Marginal Effect can pertain a regression model without an interaction while Simple Slopes and Simple Effects are terms only used in describing effects in models with an interaction.\nEach of these predicted values is a Marginal Mean (term used in {emmeans}). So, the Simple Effect would be the difference (aka contrast) of Marginal Means.\n\n{marginaleffects} calls these “Adjusted Predictions” or “Marginal Means” but both seem to be the same thing.\n\nThe interaction coefficient is the difference of Simple Effects (aka the “difference in differences”)\nExample:\nm_int &lt;- lm(Y ~ group * X, data = d)\nsummary(m_int)\n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   833.130    173.749   4.795 6.99e-05 ***\n#&gt; groupGb     -1308.898    233.218  -5.612 8.90e-06 ***\n#&gt; groupGc      -752.718    307.747  -2.446   0.0222 *  \n#&gt; X              -4.041      1.942  -2.081   0.0482 *  \n#&gt; groupGb:X      13.041      2.419   5.390 1.55e-05 ***\n#&gt; groupGc:X       7.731      3.178   2.432   0.0228 *  \n\n-4.041 is the Simple Effect of X when group = Ga (i.e. When both dummy variables are fixed at 0, as Ga is the reference level)\nAlso note that the Simple Effect depends on what the reference level of the moderator is, since the coefficient of X is the expected change in Y when the moderator, group = 0, which is the reference level.\n\n\nSimple Slopes - When a Independent Variable interacts with a moderator variable (MV), it is the slope of that IV at a particular value of the MV.\n\nSimple Slope, rather than Simple Effect, is used when the moderator is a continuous variable instead of categorical, otherwise they’re essentially the same thing.\n\nIt may also be the case that “Simple” is only used with the moderator is equal to 0.\n\nThe Simple Slope or Simple Effect calculation is the same as the Marginal Effect calculation from everything I’ve read. The only difference seems to be that the Marginal Effect term can pertain a regression model without an interaction while Simple Slopes and Simple Effects are terms only used in describing effects in models with an interaction.\nThe Simple Slope represents a conditional effect\n\\[\n\\begin{aligned}\n\\hat Y &= \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1X_2 \\\\\n       &= \\beta_0 + (\\beta_1 + \\beta_3X_2)X_1 + \\beta_2X_2 \\\\\n&\\begin{aligned}\\\\\n\\therefore \\quad \\text{Slope of}\\; X_1 = \\beta_1 + \\beta_3X_2\n\\end{aligned}\n\\end{aligned}\n\\]\n\nInterpretation: With the moderator, \\(X_2\\), fixed at 0, the Simple Slope, \\(\\beta_1\\), is the expected change in \\(Y\\) after 1 unit change in \\(X_1\\) when \\(X_2 = 0\\)\n\ni.e. The difference in predicted values when \\(X_1\\) increases by one unit while keeping \\(X_2\\) held constant at 0\n# Predict for X1 = [0, 1] and X2 = 0\npred_grid &lt;- data.frame(X1 = c(0, 1), X2 = 0)\n(predicted_means &lt;- predict(model, newdata = pred_grid))\n#&gt;        1        2 \n#&gt; 3.133346 3.228691\n\n# Conditional slope of X which is the value of the b1 coefficient\npredicted_means[2] - predicted_means[1]\n#&gt;          2 \n#&gt; 0.09534522\n\n\n\nSpotlight Analysis: Comparison of marginal effects to see analyze the effect of the variable of interest at different levels of the moderator\n\nWhen an interaction is significant, choose representative values for the moderator variable and plot the marginal effects of explanatory variable to decompose the relationship between the variables involved in the interaction.\nMade popular by Leona Aiken and Stephen West’s book Multiple Regression: Testing and Interpreting Interactions (1991).",
    "crumbs": [
      "Regression",
      "Interactions"
    ]
  },
  {
    "objectID": "qmd/regression-interactions.html#sec-reg-inter-preproc",
    "href": "qmd/regression-interactions.html#sec-reg-inter-preproc",
    "title": "Interactions",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nCenter continuous variables\n\nCentering fixes collinearity issues when creating powers and interaction terms (CV post)\n\nCollinearity between the created terms and the main effects\nPossibly may not always be the case. See paper, Centering in Multiple Regression Does Not Always Reduce Multicollinearity: How to Tell When Your Estimates Will Not Benefit From Centering\n\nInterpretation of effects with be at the mean of the explanatory variable\n\ne.g. Since X = 0 implies , the intercept, marginal effects and simple effects are interpreted at X = mean(x).\nDiscourages unreasonable extrapolations",
    "crumbs": [
      "Regression",
      "Interactions"
    ]
  },
  {
    "objectID": "qmd/regression-interactions.html#sec-reg-inter-lin",
    "href": "qmd/regression-interactions.html#sec-reg-inter-lin",
    "title": "Interactions",
    "section": "Linear",
    "text": "Linear\n\nMisc\n\nNotes from Article\nIf the interaction term is not significant but the main effect is significant then the intercepts (i.e. response means) of at least one pair of the levels DO differ.\n\n{emmeans::emmeans} can explore and test these types of scenarios\n\nfactor(var) might be necessary for character variables with more than one level (to get the contrasts) but isn’t necessary for 0/1 variables (same coefficient estimates with or without).\n{emtrends} takes all differences (aka contrasts) from the reference group. So signs may be different from the summary stats values\n\n\n\nContinuous:Continuous\n\nExample: Does the effect of workout duration (IV) on weight loss (Outcome) change due to the intensity of your workout (Moderator)?\ncontcont &lt;- lm(loss ~ hours + effort + hours:effort, data=dat)\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|) \n#&gt; (Intercept)   7.79864  11.60362   0.672   0.5017 \n#&gt; hours        -9.37568   5.66392  -1.655   0.0982 .\n#&gt; effort       -0.08028   0.38465  -0.209   0.8347 \n#&gt; hours:effort  0.39335   0.18750   2.098   0.0362 *\n\n(Intercept): The intercept, or the predicted outcome when Hours = 0 and Effort = 0.\nhours (Indepenedent Variable (IV)): For a one unit change in hours, the predicted change in weight loss at effort = 0.\neffort (Moderator): For a one unit change in effort the predicted change in weight loss at \\(\\text{hours}=0\\).\nhours:effort: The change in the slope of hours for every one unit increase in effort (or vice versa).\nCalculate representative values for the moderator variable\neff_high &lt;- round(mean(dat$effort) + sd(dat$effort), 1) # 34.8\neff &lt;- round(mean(dat$effort), 1) # 29.7\neff_low &lt;- round(mean(dat$effort) - sd(dat$effort), 1) # 24.5\n\nmean, +1 sd, -1 sd\n\nCalculate marginal effects for the IV at 3 representative values for the moderator variable\n# via {emmeans}\nmylist &lt;- list(effort = c(eff_low, eff, eff_high)) \nemtrends(contcont, ~effort, var = \"hours\", at = mylist)\n#&gt; effort    hours.trend    SE      df     lower.CL upper.CL\n#&gt; 24.5      0.261          1.352   896   -2.392    2.91\n#&gt; 29.7      2.307          0.915   896    0.511    4.10\n#&gt; 34.8      4.313          1.308   896    1.745    6.88\n#&gt; Confidence level used: 0.95\n\n# via {marginaleffects}\nloss_grid &lt;- datagrid(effort = c(eff_high, eff, eff_low),\n                      model = contcont)\nslopes(contcont, newdata = loss_grid)\n#&gt;   rowid    type      term   dydx      std.error  hours     effort\n#&gt; 1 1        response  hours  4.3127872 1.30838659 2.002403  34.8\n#&gt; 2 2        response  hours  2.3067185 0.91488236 2.002403  29.7\n#&gt; 3 3        response  hours  0.2613152 1.35205286 2.002403  24.5\n#&gt; 4 1        response  effort 0.7073625 0.08795998 2.002403  34.8\n#&gt; 5 2        response  effort 0.7073625 0.08795998 2.002403  29.7\n#&gt; 6 3        response  effort 0.7073625 0.08795999 2.002403  24.5\n\n{emmeans}\n\ncontcont is the model (e.g. lm)\n~effort tells emtrends to stratify by the moderator variable effort\ndf is 896 because we have a sample size of 900, 3 predictors and 1 intercept (df = 900 - 3 -1 = 896)\n\n{marginaleffects}\n\nShows how hours is held constant at its mean when it’s marginal effect is calculated at the values specified for effort\nAdditionally gives you the marginal effect for effort at the mean of hours\n\nInterpretation:\n\nWhen \\(\\text{effort} = 24.5\\), a one unit increase in hours results in a 0.26 increase in weight loss on average\nAs we increase levels of effort, the effect of hours on weight loss seems to increase\nThe marginal effect of hours is significant only for mean effort levels and above (i.e. CIs for mean and high effort don’t include 0)\n\n\nPlot marginal effects for the IV (hours)\n\n# via {emmeans}\nmylist &lt;- list(hours = seq(0,4, by = 0.4), effort = c(eff_low, eff, eff_high))\nemmip(contcont, effort ~ hours, at = mylist, CIs = TRUE)\n\n# via {marginaleffects}\nplot_predictions(contcont, condition = c(\"hours\", \"effort\")) # creates it's own subset of values for effort\n\nPredicted Outcome vs IV (newdata) by Moderator (new data)\n\nFor each value of effort, the slope of the hours line at the mean of hours (for that effort level) is the value of the hours marginal effect\n\nInterpretation:\n\nSuggests that hours spent exercising is only effective for weight loss if we put in more effort. (i.e. as effort increases so does the effect of hours)\nAt the highest levels of effort, we achieve higher weight loss for a given time input.\n\nggplot\n\n# via {emmeans}\ncontcontdat &lt;- emmip(contcont,effort~hours,at=mylist, CIs=TRUE, plotit=FALSE)\ncontcontdat$feffort &lt;- factor(contcontdat$effort)\nlevels(contcontdat$feffort) &lt;- c(\"low\",\"med\",\"high\")\nggplot(data=contcontdat, aes(x=hours,y=yvar, color=feffort)) +\n    geom_line() +\n    geom_ribbon(aes(ymax=UCL, ymin=LCL, fill=feffort), alpha=0.4) +\n    labs(x=\"Hours\", y=\"Weight Loss\", color=\"Effort\", fill=\"Effort\")\n\n# via {marginaleffects}\nmoose &lt;- predictions(contcont, newdata = datagrid(hours = seq(0,4, by = 0.4), \n                                                  effort = c(eff_low, eff, eff_high),\n                                                  grid.type = \"counterfactual\"))\nggplot(data=moose, aes(x=hours,y=predicted, color=factor(effort))) +\n  geom_line() +\n  geom_ribbon(aes(ymax=conf.high, ymin=conf.low, fill=factor(effort)), alpha=0.4) +\n  labs(x=\"Hours\", y=\"Weight Loss\", color=\"Effort\", fill=\"Effort\")\n\nTest pairwise differences (contrasts) of marginal effects\nemtrends(contcont, pairwise ~effort, var=\"hours\",at=mylist)\n#&gt; $contrasts\n#&gt;    contrast    estimate     SE   df t.ratio p.value\n#&gt; 24.5 - 29.7       -2.05  0.975  896  -2.098  0.0362 \n#&gt; 24.5 - 34.8       -4.05  1.931  896  -2.098  0.0362 \n#&gt; 29.7 - 34.8       -2.01  0.956  896  -2.098  0.0362 \n#&gt; Results are averaged over the levels of: hours\n\n{marginaleffects} doesn’t do this from what I can tell and IMO looking at contrasts of simple effects (next section) is a better method\n{emmeans}\n\npairwise ~effort tells the function that we want pairwise differences of the marginal effects of var=“hours” for each level of effort specified “at=mylist”\n\nhours not specified in mylist just the 3 values for effort\n\nThe results are with the arg, adjust=“none”, which removes the Tukey multiple test adjustment. Removed it from the code, because it’s bad practice.\n\nInterpretation\n\nA one unit change in hours when effort is high (34.8) results is 4 lbs more weight loss than a one unit change in hours when effort is low (24.5)\nBased on the NHST, all the marginal effects are significantly different from each other\n\nFor continuous:continuous, p-values will all be the same and identical the interaction p-value of the model (has to do with the slope formula)\n\nTesting the simple effects contrasts (see below) may be the better option\n\n\nTest pairwise differences (contrasts) of Simple Slopes\n# via {emmeans}\nmylist &lt;- list(hours = 4, effort = c(eff_low, eff_high))\nemmeans(contcont, pairwise ~ hours*effort, at=mylist)\n#&gt; $emmeans\n#&gt; hours  effort  emmean   SE   df lower.CL upper.CL\n#&gt;     4    24.5    6.88  2.79 896      1.4     12.4\n#&gt;     4    34.8   22.26  2.68 896     17.0     27.5\n#&gt; Confidence level used: 0.95 \n#&gt; $contrasts\n#&gt;    contrast        estimate    SE   df t.ratio p.value\n#&gt; 4,24.5 - 4,34.8       -15.4  3.97  896  -3.871  0.0001\n\n\n# via {marginaleffects}\ncomparisons(contcont,\n            variables = list(effort = c(eff_low, eff_high)),\n            newdata = datagrid(hours = 4)) %&gt;% \n  tidy()\n#&gt; rowid    type        term   contrast_effort comparison std.error statistic      p.value conf.low conf.high   effort hours\n#&gt; 1    1 response interaction     34.8 - 24.5   15.37904   3.97295  3.870938 0.0001084175 7.592203  23.16588 29.65922     4\n\n# alternate (difference in results could be due to rounding of eff_low, eff_high)\ncomparisons(contcont,\n            variables = list(effort = \"2sd\"),\n            newdata = datagrid(hours = 4, \n                              grid.type = \"counterfactual\")) %&gt;% \n  tidy()\n#&gt;       type        term     contrast_effort estimate std.error statistic      p.value conf.low conf.high\n#&gt; 1 response interaction (x + sd) - (x - sd) 15.35743  3.967367  3.870938 0.0001084175 7.581535  23.13333\n\nOne reason to look at the tests for the predicted means contrast instead of the slope contrasts is the constant p-value across slope contrasts (see above)\nIf hours isn’t specified emmeans holds hours at the sample mean.\n\nUsing {marginaleffects} and holding hours at sample mean (and probably every other variable in the model), comparisons(contcont, variables = list(effort = \"2sd\")) %&gt;% tidy()\n\nSpecification\n\nhours has a fixed value and the moderator is allowed to vary between high and low effort\nemmeans\n\npairwise ~ hours*effort to tell the function that we want pairwise contrasts of each distinct combination of [hours{.var-text} and effort specified at=mylist\n\nmarginaleffects\n\neffort = “2d” says compare (effort + 1 sd) to (effort - 1 sd)\ngrid.type = “counterfactual” because there is no observation in the data with hours = 4\n\n\nInterpretation\n\nBased on the NHST, the difference between the predicted means of high and low effort are significantly different from each other when hours = 4\nAfter 4 hours of exercise, high effort (34.8) will result in about 15 pounds more weight loss than low effort (24.5) on average.\n\n\n\n\n\n\nContinuous:Binary\n\nThe change in the slope of the continuous (IV) after a change from the reference level to the other level in the binary (Moderator)\n\nNote that this value is an additional change to the effect of the continuous (i.e when binary = 1, the effect = continuous effect + interaction effect)\n\nSignificant means that the factor variable does increase/decrease the effect of the continuous variable\nExample: Does the effect of workout duration in hours (IV) on weight loss (Outcome) change because you are a male or female (Moderator)?\ndat &lt;- dat %&gt;% \n  mutate(gender = factor(gender, labels = c(\"male\", \"female\")) # male = 1, female = 2,\n        gender = relevel(gender, ref = \"female\")\n  )\ncontcat &lt;- lm(loss ~ hours * gender, data=dat)\n#&gt;                  Estimate Std. Error t value Pr(&gt;|t|) \n#&gt; (Intercept)         3.335      2.731   1.221    0.222 \n#&gt; hours               3.315      1.332   2.489    0.013 *\n#&gt; gendermale          3.571      3.915   0.912    0.362 \n#&gt; hours:gendermale   -1.724      1.898  -0.908    0.364\n\nIntercept (\\(\\beta_0\\)): Predicted weight loss when Hours = 0 in the reference group of Gender, which is females.\nhours (\\(\\beta_1\\), Independent Variable): The marginal effect of hours for the reference group (e.g. females).\n\nhours marginal effect for males is \\(\\beta_1 + \\beta_3\\) or emtrends(contcat, ~ gender, var=\"hours\") will give both marginal effects\n\ngendermale (\\(\\beta_2\\), Moderator) (reference group: female): The difference in the effect of being male compared to female at hours = 0.\nhours:gendermale (\\(\\beta_3\\)): The difference in the marginal effects of hours for males to hours for females.\n\nemtrends can also give the summary stats for the interaction (sign different in this case, since difference is from the reference group, female)\nhours_slope &lt;- emtrends(contcat, pairwise ~ gender, var=\"hours\")\nhours_slope$contrasts\n#&gt; contrast      estimate   SE   df t.ratio p.value\n#&gt; female - male     1.72  1.9  896   0.908  0.3639\n\nMarginal effects of continuous (IV) by each level of the categorical (moderator)\n# via {emmeans}\nemtrends(contcat, ~ gender, var=\"hours\")\n#&gt; gender  hours.trend    SE    df  lower.CL  upper.CL\n#&gt; female         3.32  1.33  896      0.702      5.93\n#&gt; male           1.59  1.35  896     -1.063      4.25\n\n# via {marginaleffects}\nslopes(contcat) %&gt;%\n    tidy(by = \"gender\")\n#&gt;       type  term        contrast gender   estimate std.error statistic    p.value   conf.low conf.high\n#&gt; 1 response  hours          dY/dX   male 1.59113665 1.3523002 1.1766150 0.23934921 -1.0593230  4.241596\n#&gt; 2 response  hours          dY/dX female 3.31506746 1.3316491 2.4894453 0.01279426  0.7050833  5.925052\n#&gt; 3 response gender  male - female   male 0.09616813 0.9382760 0.1024945 0.91836418 -1.7428191  1.935155\n#&gt; 4 response gender  male - female female 0.14194405 0.9382968 0.1512784 0.87975610 -1.6970840  1.980972\n\n{emmeans}\n\n~gender says obtain separate estimates for females and males\nvar=“hours” says obtain marginal effects (or trends) for hours\n\n{marginaleffects}\n\nCan also use summary(by = \"gender\")\n\nInterpretation\n\nhours by female is the same as the model coefficient above.\n\nSee model interpretation above\n\n\n\nSimple effects at values of the moderator\n# via {emmeans}\nmylist &lt;- list(hours=c(0,2,4),gender=c(\"male\",\"female\"))\nemcontcat  &lt;- emmeans(contcat, ~ hours*gender, at=mylist)\ncontrast(emcontcat, \"pairwise\",by=\"hours\")\n#&gt; hours = 0:\n#&gt; contrast      estimate    SE  df t.ratio p.value\n#&gt; male - female    3.571 3.915 896   0.912  0.3619 \n#&gt; hours = 2:\n#&gt; contrast      estimate    SE  df t.ratio p.value\n#&gt; male - female    0.123 0.938 896   0.131  0.8955 \n#&gt; hours = 4:\n#&gt; contrast      estimate    SE  df t.ratio p.value\n#&gt; male - female  -3.325 3.905 896   -0.851  0.3948\n\n# via {marginaleffects}\ncomparisons(contcat,\n            variables = \"gender\",\n            newdata = datagrid(hours = c(0, 2, 4)))\n#&gt;   rowid    type        term contrast_gender comparison std.error  statistic   p.value   conf.low conf.high gender hours\n#&gt; 1    1 response interaction   male - female  3.5710607  3.914762  0.9122038 0.3616614  -4.101731 11.243853   male     0\n#&gt; 2    2 response interaction   male - female  0.1231991  0.937961  0.1313478 0.8955002  -1.715171  1.961569   male     2\n#&gt; 3    3 response interaction   male - female -3.3246625  3.905153 -0.8513527 0.3945735 -10.978622  4.329297   male     4\n\nThe moderator is now hours\nNote that output for hours = 0, the effect 3.571 matches the \\(\\beta_2\\) coefficient\nInterpretation\n\nAs hours increases, the male versus female difference becomes more negative (females are losing more weight than males)\n\n\nPlotting the continuous by binary interaction\n\n# via {emmeans}\nmylist &lt;- list(hours=seq(0,4,by=0.4),gender=c(\"female\",\"male\"))\nemmip(contcat, gender ~hours, at=mylist,CIs=TRUE)\n\nFor ggplot, see continuous:continuous &gt;&gt; Plot marginal effects for IV section (Should be similar code)\nInterpretation\n\nAlthough it looks like there’s a cross-over interaction, the large overlap in confidence intervals suggests that the slope of hours is not different between males and females\n\nConfirmed by looking at the standard error of the interaction coefficient (1.898) is large relative to the absolute value of the coefficient itself (1.724)\n\n\n\n\n\n\n\nCategorical:Categorical\n\nFrom {marginaleffects} Contrasts vignette, “Since derivatives are only properly defined for continuous variables, we cannot use them to interpret the effects of changes in categorical variables. For this, we turn to contrasts between adjusted predictions (aka Simple Effects).”\n\ni.e. Contrasts of marginal effects aren’t possible\n\n2x3 cat:cat interaction\nExample: Do males and females (IV) lose weight (Outcome) differently depending on the type of exercise (Moderator) they engage in?\ndat &lt;- dat %&gt;% \n  mutate(gender = factor(gender, labels = c(\"male\", \"female\")),\n        gender = relevel(gender, ref = \"female\"),\n        prog = factor(prog, labels = c(\"jog\",\"swim\",\"read\")),\n        prog = relevel(prog, ref=\"read\")\n  )\ncatcat &lt;- lm(loss ~ gender * prog, data = dat)\n#&gt;                     Estimate Std. Error t value Pr(&gt;|t|)   \n#&gt; (Intercept)          -3.6201     0.5322  -6.802 1.89e-11 ***\n#&gt; gendermale           -0.3355     0.7527  -0.446    0.656   \n#&gt; progjog               7.9088     0.7527  10.507  &lt; 2e-16 ***\n#&gt; progswim             32.7378     0.7527  43.494  &lt; 2e-16 ***\n#&gt; gendermale:progjog    7.8188     1.0645   7.345 4.63e-13 ***\n#&gt; gendermale:progswim  -6.2599     1.0645  -5.881 5.77e-09 ***\n\nDescription\n\nIntercept (\\(\\beta_0\\)): The intercept, or the predicted weight loss when gendermale = 0, progjog = 0, and progswim = 0\n\ni.e. The intercept for females in the reading program\n\ngendermale (\\(\\beta_1\\)): The simple effect of males when progjog = 0 and progswim = 0\n\ni.e. The male – female weight loss in the reading group\n\nprogjog (\\(\\beta_2\\)): the simple effect of jogging when gendermale = 0\n\ni.e. The difference in weight loss between jogging versus reading for females\n\nprogswim (\\(\\beta_3\\)): The simple effect of swimming when gendermale = 0\n\ni.e. The difference in weight loss between swimming versus reading for females\n\ngendermale:progjog (\\(\\beta_4\\)): The male effect (male – female) in the jogging condition versus the male effect in the reading condition.\n\nAlso, the jogging effect (jogging – reading) for males versus the jogging effect for females.\n\ngendermale:progswim (\\(\\beta_5\\)): the male effect (male – female) in the swimming condition versus the male effect in the reading condition.\n\nAlso, the swimming effect (swimming – reading) for males versus the swimming effect for females.\n\ngender is the independent variable\nProg is the moderator\nReference Groups\n\nprog == read\ngender == female\n\n\\(\\beta_1 + \\beta_4\\): The marginal effect of gender == males in the prog == jogging group.\n\\(\\beta_1 + \\beta_5\\): The marginal effect of gender == males in the prog == swimming group.\n\nMarginal Means for all combinations of the interaction variable levels\n# via {emmeans}\nemcatcat &lt;- emmeans(catcat, ~ gender*prog)\n#&gt; gender  prog  emmean     SE   df lower.CL upper.CL\n#&gt; female  read   -3.62  0.532  894    -4.66    -2.58\n#&gt;   male  read   -3.96  0.532  894    -5.00    -2.91\n#&gt; female   jog    4.29  0.532  894     3.24     5.33\n#&gt;   male   jog   11.77  0.532  894    10.73    12.82\n#&gt; female  swim   29.12  0.532  894    28.07    30.16\n#&gt;   male  swim   22.52  0.532  894    21.48    23.57\n\n# via {marginaleffects}\npredictions(catcat, variables = c(\"gender\", \"prog\"))\n#&gt;   rowid    type predicted std.error  conf.low conf.high gender prog\n#&gt; 1    1 response 11.772028 0.5322428 10.727437 12.816619   male  jog\n#&gt; 2    2 response 22.522383 0.5322428 21.477792 23.566974   male swim\n#&gt; 3    3 response -3.955606 0.5322428 -5.000197 -2.911016   male read\n#&gt; 4    4 response  4.288681 0.5322428  3.244091  5.333272 female  jog\n#&gt; 5    5 response 29.117691 0.5322428 28.073100 30.162282 female swim\n#&gt; 6    6 response -3.620149 0.5322428 -4.664740 -2.575559 female read\n\nMarginal means generated using the regression coefficients\n\n\nKinda weird. Since these are predicted means (i.e. outcome values), I wouldn’t have thought they could be generated by the model coefficients. ¯\\_(ツ)_/¯\n\nInterpretation\n\nfemales in the reading program have an estimated weight gain of 3.62 pounds\nmales in the swimming program have an average weight loss of 22.52 pounds\n\n\nSimple Effects of the IV for all values of the moderator variable\n# via {emmeans}\ncontrast(emcatcat, \"revpairwise\", by=\"prog\") # see above code chunk for emcatcat\n#&gt; prog = read:\n#&gt; contrast      estimate    SE  df t.ratio  p.value\n#&gt; male - female   -0.335 0.753 894  -0.446   0.6559 \n#&gt; prog = jog:\n#&gt; contrast      estimate    SE  df t.ratio  p.value\n#&gt; male - female    7.483 0.753 894   9.942   &lt;.0001 \n#&gt; prog = swim:\n#&gt; contrast      estimate    SE  df t.ratio  p.value\n#&gt; male - female   -6.595 0.753 894  -8.762   &lt;.0001\n\n# via {marginaleffects}\ncomparisons(catcat, \n            variables = \"gender\", \n            newdata = datagrid(prog = c(\"read\", \"jog\", \"swim\")))\n#&gt;   rowid    type        term contrast_gender comparison std.error  statistic      p.value  conf.low conf.high gender prog\n#&gt; 1    1 response interaction   male - female -0.3354569 0.7527049 -0.4456686 6.558367e-01 -1.810731  1.139818   male read\n#&gt; 2    2 response interaction   male - female  7.4833463 0.7527049  9.9419388 2.734522e-23  6.008072  8.958621   male  jog\n#&gt; 3    3 response interaction   male - female -6.5953078 0.7527049 -8.7621425 1.915739e-18 -8.070582 -5.120033   male swim\n\n{emmeans}\n\nrevpairwise rather than pairwise because by default the reference group (female) would come first\nThe results are with the arg, adjust=“none”, which removes the Tukey multiple test adjustment. Removed it from the code, because it’s bad practice.\n\nInterpretation\n\nmale effect for jogging and swimming are significant at the 0.05 level (with no adjustment of p-values), but not for the reading condition.\nmales lose more weight in the jogging condition (positive) but females lose more weight in the swimming condition (negative)\n\n\nPlotting the categorical by categorical interaction\n\n{emmeans} emmip(catcat, prog ~ gender,CIs=TRUE)\n\n{ggplot2}\n\ncatcatdat &lt;- emmip(catcat, gender ~ prog, CIs=TRUE, plotit=FALSE)\n#&gt;   gender prog      yvar        SE  df       LCL       UCL   tvar xvar\n#&gt; 1 female read -3.620149 0.5322428 894 -4.664740 -2.575559 female read\n#&gt; 2   male read -3.955606 0.5322428 894 -5.000197 -2.911016   male read\n#&gt; 3 female  jog  4.288681 0.5322428 894  3.244091  5.333272 female  jog\n#&gt; 4   male  jog 11.772028 0.5322428 894 10.727437 12.816619   male  jog\n#&gt; 5 female swim 29.117691 0.5322428 894 28.073100 30.162282 female swim\n#&gt; 6   male swim 22.522383 0.5322428 894 21.477792 23.566974   male swim\n\n# via {marginaleffects}\npredictions(catcat, variables = c(\"gender\", \"prog\"))\n\nggplot(data=catcatdat, aes(x=prog,y=yvar, fill=gender)) +\n    geom_bar(stat=\"identity\",position=\"dodge\") +\n    geom_errorbar(position=position_dodge(.9),width=.25, aes(ymax=UCL, ymin=LCL),alpha=0.3)\n    labs(x=\"Program\", y=\"Weight Loss\", fill=\"Gender\")\n\nI think I like the emmeans plot better\nValues come from the Marginal Means for all combinations of the interaction variable levels section above",
    "crumbs": [
      "Regression",
      "Interactions"
    ]
  },
  {
    "objectID": "qmd/regression-interactions.html#sec-reg-inter-logreg",
    "href": "qmd/regression-interactions.html#sec-reg-inter-logreg",
    "title": "Interactions",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nMisc\n\n* categorical:categorical and continuous:continuous should be should be similar to analysis for binary:continuous and the code/analysis in the OLS section *\n\nFor more details on these other interaction types see the article linked below\n\nMisc\n\nNotes from\n\nHow Can I Understand A Categorical By Continuous Interaction In Logistic Regression\nDeciphering Interactions In Logistic Regression\n\nSee this article for continuous:continuous, cat:cat\n\n\nAlso see\n\nRegression, Logistic &gt;&gt; Marginal Effects\nPost-Hoc Analysis, emmeans &gt;&gt; Logistic Regression\n\nReminder: OR significance is being different from 1 and log odds ratios (logits) significance is being different from 0\nAlthough the interaction is quantified appropriately by the interaction coefficient when interpreting effects on the scale of log-odds or log-counts, the interaction effect will not reduce to the coefficient of the the interaction coefficient when describing effects on the natural response scale (i.e. probabilities, counts) because of the inverse-link transformation.\n\nIn order to get e.g. probabilities, the inverse-link function must be used, and this complicates the derivatives of the marginal effect calculation. The chain rule must be appied. This means the interaction effect is no longer equivalent to the interaction coefficient.\nTherefore, effects or contrasts are differences in marginal effects rather than coefficients\n\nInteraction effects represent change in a marginal effect of one predictor for a change in another predictoreraction effects represent change in a marg\n\n\nStandard errors of point estimate values of interaction effects (as well as their corresponding statistical significance) may also vary as a function of the predictors.\nNeed to decide whether the interaction is to be conceptualized in terms of log odds (logits) or odds ratios or probability\n\nAn interaction that is significant in log odds may not be significant in terms of probability or vice versa\n\nEven if they do agree, the p-values do not have the be the same. (See below, binary:continuous &gt;&gt; Simple Effects &gt;&gt; second example &gt;&gt; probabilities)\n\n\n\n\n\n\nBinary:Continuous\n\nExample:\n\nDescription\ndat_clean &lt;- dat %&gt;% \n  mutate(f = factor(f, labels = c(\"female\", \"male\")))\nconcat &lt;- glm(y ~ f*s, family = binomial, data = dat_clean)\nsummary(concat)\n#&gt;             Estimate  Std. Error  z value  Pr(&gt;|z|)   \n#&gt; (Intercept) -9.25380     1.94189   -4.765  1.89e-06 ***\n#&gt; fmale        5.78681     2.30252    2.513    0.0120 * \n#&gt; s            0.17734     0.03644    4.867  1.13e-06 ***\n#&gt; fmale:s     -0.08955     0.04392   -2.039    0.0414 *\n\ny: Binary Outcome\ns (IV): Continuous Variable\nf (moderator): Gender variable with female as the reference\ncv1: Continuous Adjustment Variable\n\nMarginal effects of continuous (IV) by each level of the categorical (moderator)\n# via {marginaleffects}\nslopes(concat, variables = \"s\") %&gt;%\n    tidy(by = \"f\")\n#&gt;       type term       f   estimate   std.error statistic     p.value   conf.low  conf.high\n#&gt; 1 response    s    male 0.01473352 0.003237072  4.551498 5.32654e-06 0.00838898 0.02107807\n#&gt; 2 response    s  female 0.02569069 0.001750066 14.679842 0.00000e+00 0.02226062 0.02912075\n\nInterpetation\n\nIncreasing s by 1 unit given that the person is male will result in around a 1% increase on average in the probability of the event, y.\n\n\nSimple effects at values of the IV\n# via {marginaleffects}\ncomp &lt;- comparisons(concat2, \n                    variables = \"f\", \n                    newdata = datagrid(s = seq(20, 70, by = 2)))\nhead(comp)\n#&gt;   rowid    type        term    contrast_f comparison  std.error statistic    p.value     conf.low conf.high    f  s\n#&gt; 1    1 response interaction male - female  0.1496878 0.09871431  1.516374 0.12942480 -0.043788674 0.3431643 male 20\n#&gt; 2    2 response interaction male - female  0.1724472 0.10430544  1.653291 0.09827172 -0.031987695 0.3768821 male 22\n#&gt; 3    3 response interaction male - female  0.1975119 0.10886095  1.814351 0.06962377 -0.015851616 0.4108755 male 24\n#&gt; 4    4 response interaction male - female  0.2246979 0.11209440  2.004542 0.04501204  0.004996938 0.4443989 male 26\n#&gt; 5    5 response interaction male - female  0.2536377 0.11376972  2.229395 0.02578761  0.030653136 0.4766222 male 28\n#&gt; 6    6 response interaction male - female  0.2837288 0.11375073  2.494303 0.01262048  0.060781436 0.5066761 male 30\n\nShows the predicted value contrasts for the moderator at different values of the independent variable.\n\nconcat2 model defined in “Simple Effects at values of the moderator and adjustment variable” (see below)\n\nFor comparisons , the default is type = “response” so the output is probabilities. (type = “link” for log-ORs aka logits)\nInterpretation\n\nFor s = 26, 28, and 30, there is substantial evidence that there are differences in male and female probabilites of the event, y.\n\nExample: (article)\n\nDescription\n\nlowbwt: Binary Outcome, low birthweight = 1\nage (IV): Continuous, Age of the mother\nftv (moderator): Binary, whether or not the mother made frequent physician visits during the first trimester of pregnancy\n\nSimple Effects: Odds Ratios\n\n\nORftv is the odds-ratio contrast of (ftv = 1) - (ftv = 0) at the specified age of the mother\nInterpretation\n\nA mother at the age of 30 who visits the physician frequently has 0.174 times the odds of having a low birth weight baby as compared to those of the same age who don’t visit the doctor frequently\nFor women whose ages are between 17 and 24, the 95% confidence intervals of the odds ratios include the null value of 1, so we do not have strong evidence of an association between frequent doctor visits and low birth weight for that age range.\nFor mothers aged 25 years and older, the odds of having a low birth weight baby significantly decrease if the mother frequently visits her physician.\n\n\nSimple Effects: Probabilities\n\n\n“Difference in probability” is the probability contrast of (ftv = 1) - (ftv = 0) at the specified age of the mother\nNote that the results agree with the OR results, but the pvals are different\n\nSo it’s probably possible that in terms of “significance,” you could get different conclusions based on the metric used.\n\nInterpretation\n\nFor young mothers (less than 24 years old), we do not have strong evidence of an association between low birth weight and frequent physician [visits]{.var-text.\nFor mothers aged 25 years and older, we reject the null hypothesis of no difference in probability between those with frequent physician visits and those without.\nOverall, the difference between the probability of low birth weight comparing those with frequent visits to those without increases as the mother’s age increases.\n\n\n\n\nPlot the simple effects\n\nggplot(comp, aes(x = s, y = comparison)) + \n  geom_line() +\n  geom_ribbon(aes(ymax=conf.high, ymin=conf.low), alpha=0.4) + \n  geom_hline(yintercept=0, linetype=\"dashed\") +\n  labs(title = \"Male-Female Difference\", y=\"Probability Contrast\")\n\n{marginaleffects}\n\nYou change the comparisons (see above) type arg to type = link to get log-ORs (aka logits) instead of probabilities and then plot\n\n\nInterpretation\n\nThe difference in probabilities for male and females is statistically significant between values of s approximately between 28 to 55.\n\n\nSimple Effects at values of the moderator and adjustment variable\nconcat2 &lt;- glm(y ~ f*s + cv1, family = binomial, data = dat_clean)\ncomp2 &lt;- comparisons(concat2, \n                    variables = \"f\", \n                    newdata = datagrid(s = seq(20, 70, by = 2),\n                                      cv1 = c(40, 50, 60)))\nhead(comp2)\n#&gt;   rowid    type        term    contrast_f comparison  std.error statistic      p.value    conf.low conf.high    f  s cv1\n#&gt; 1    1 response interaction male - female  0.2307214 0.15001700  1.537968 1.240564e-01 -0.06330656 0.5247493 male 20  40\n#&gt; 2    2 response interaction male - female  0.6603841 0.20879886  3.162777 1.562722e-03  0.25114590 1.0696224 male 20  50\n#&gt; 3    3 response interaction male - female  0.9135206 0.07865839 11.613773 3.507873e-31  0.75935304 1.0676882 male 20  60\n#&gt; 4    4 response interaction male - female  0.2361502 0.14294726  1.652009 9.853270e-02 -0.04402131 0.5163217 male 22  40\n#&gt; 5    5 response interaction male - female  0.6663811 0.19447902  3.426494 6.114278e-04  0.28520927 1.0475530 male 22  50\n#&gt; 6    6 response interaction male - female  0.9097497 0.07571970 12.014703 2.974362e-33  0.76134178 1.0581575 male 22  60\nPlot simple effects and facet by values of the adjustment variable\n\nggplot(comp2, aes(x = s, y = comparison, group = factor(cv1))) + \n  geom_line() +\n  geom_ribbon(aes(ymax=conf.high, ymin=conf.low), alpha=0.4) + \n  geom_hline(yintercept=0, linetype=\"dashed\") +\n  facet_wrap(~ cv1) +\n  labs(title = \"Male-Female Difference\", y=\"Probability Contrast\")\n\nInterpretation\n\nIt seems clear from looking at the three graphs that the male-female difference in probability increases as cv1 increases except for high values of s.",
    "crumbs": [
      "Regression",
      "Interactions"
    ]
  },
  {
    "objectID": "qmd/regression-interactions.html#sec-reg-inter-poisnegbin",
    "href": "qmd/regression-interactions.html#sec-reg-inter-poisnegbin",
    "title": "Interactions",
    "section": "Poisson/Neg.Binomial",
    "text": "Poisson/Neg.Binomial\n\nMisc\n\nNotes from Paper Interpreting interaction effects in generalized linear models of nonlinear probabilities and counts\nAlthough the interaction is quantified appropriately by the interaction coefficient when interpreting effects on the scale of log-odds or log-counts, the interaction effect will not reduce to the coefficient of the the interaction coefficient when describing effects on the natural response scale (i.e. probabilities, counts) because of the inverse-link transformation.\n\nIn order to get e.g. probabilities, the inverse-link function must be used, and this complicates the derivatives of the marginal effect calculation. The chain rule must be appied. This means the interaction effect is no longer equivalent to the interaction coefficient.\nTherefore, effects or contrasts are differences in marginal effects rather than coefficients\n\nInteraction effects represent change in a marginal effect of one predictor for a change in another predictoreraction effects represent change in a marg\n\n\nStandard errors of point estimate values of interaction effects (as well as their corresponding statistical significance) may also vary as a function of the predictors.\n\nTypes\n\nContinuous:Continuous: The rate-of-change in the marginal effect of one predictor given a change in another predictor\n\nAlso, the amount the effect of \\(x_1\\) on \\(\\mathbb{E}[Y\\;|\\;x]\\) changes for every one-unit increase in \\(x_2\\) (and vice versa), holding all else constant.\n\nContinuous:Discrete: The Difference in the marginal effect of the continuous predictor between two selected values of the discrete predictor\n\nExample: Poisson outcome (population)\n\n\n\\(x_1\\) is continuous, \\(x_2\\) is continuous, \\(x_3\\) is binary (e.g. sex where female = 1).\nThe interaction effect, \\(\\gamma^2_{13}\\), of \\(x_1x_3\\) is the difference between the marginal effect of \\(x_1\\) for females versus males\nNote that \\(x_2\\) is included in the effect calculation for an interaction it isn’t part of\nSpecific values for \\(x_1\\) and \\(x_2\\) must be chosen to the calculate the value of the effect\n\n\nFor \\(x_2\\), the 25th, 50th, 75th quartiles are used and \\(x_1\\) would be held at one value of substantive interest (e.g. mean)\nStandard errors via delta method for all 3 values can be computed along with p-values\n\nVisual\n\n\n\\(x_2\\) is held at its mean, and various values of \\(x_1\\) are used\nInterpretation: the expected population (count of \\(Y\\)) is larger among females than males across all values of \\(x_1\\) with \\(x_2\\) held at its mean.\n\n\n\nDiscrete:Discrete: The difference between the model evaluated at two categories of one variable (a, b) minus the difference in this model evaluated at two categories of another variable (c, d) (aka discrete double difference)",
    "crumbs": [
      "Regression",
      "Interactions"
    ]
  },
  {
    "objectID": "qmd/regression-logistic.html",
    "href": "qmd/regression-logistic.html",
    "title": "Logistic",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Regression",
      "Logistic"
    ]
  },
  {
    "objectID": "qmd/regression-logistic.html#sec-reg-log-misc",
    "href": "qmd/regression-logistic.html#sec-reg-log-misc",
    "title": "Logistic",
    "section": "",
    "text": "Also see\n\nClassification\nRegression, Regularized &gt;&gt; Misc, Firth’s Estimator\n\nPackages\n\n{glmnet}\n\nglm needs the outcome to be numeric 0/1 (not factor)\nRegularized Logistic Regression is most necessary when the number of candidate predictors is large in relationship to the effective sample size 3np(1−p) where p is the proportion of Y=1 Harrell\nSample size requirements\n\nThese are conservative estimates. Sample size estimates assume an event probability of 0.50.\nLogistic Regression: (Harrell, link)\n\nFor just estimating the intercept and a margin of error for predicted probabilities of 0.1\n\nWith no covariates (i.e. population is homogeneous), n = 96\nWith 1 categorical covariate, n = 96 for each level of the covariate\n\ne.g. For gender, you need 96 males and 96 females\n\n\nFor just estimating the intercept and a margin of error for predicted probabilities of 0.05\n\nWith no covariates (i.e. population is homogeneous), n = 384\nIf true probabilities of event (and non-event) are known to be extreme, i.e. \\(p \\notin [0.2, 0.8]\\), n = 246\n\nFor estimating predicted probabilities with 1 continuous predictor\n\nFor a margin of error of 0.1, n = 150\nFor a margin of error of 0.07, n = 300\n\n\nRF: 200 events per candidate feature (Harrell, link)\n\n“Stable” AUC requirements for 0/1 outcome:\n\npaper: Modern modelling techniques are data hungry: a simulation study for predicting dichotomous endpoints | BMC Medical Research Methodology | Full Text\nLogistic Regression: 20 to 50 events per predictor variable\nRandom Forest and SVM: greater than 200 to 500 events per predictor variable\n\nNon-Collapsibility: The conditional odds ratio (OR) or hazard ratio (HR) is different from the marginal (unadjusted) ratio even in the complete absence of confounding.\nDon’t use percents to report probabilities (Harrell)\n\nExamples (Good)\n\nThe chance of stroke went from 0.02 to 0.03\nThe chance of stroke increased by 0.01 (or the absolute chance of stroke increased by 0.01)\nThe chance of stroke increased by a factor of 1.5\nIf 0.02 corresponded to treatment A and 0.03 corresponded to treatment B: treatment A multiplied the risk of stroke by 2/3 in comparison to treatment B.\nTreatment A modified the risk of stroke by a factor of 2/3\nThe treatment A: treatment B risk ratio is 2/3 or 0.667.\n\n\nRCTs (notes from Harrell)\n\noutcome ~ treatment\n\nThese simple models are for homogeneous patient populations, i.e.,  patients with no known risk factors\nWhen heterogeneity (patients have strong risk factors) is present, patients come from a mixture of distributions and this causes the treatment effect to shrink towards 0 in logistic and cox-ph models. (see Ch. 13 for in Harrell’s biostats book details\n\nIn a linear model, this heterogeneity (i.e. risk factors) that’s unaccounted for gets absorbed into the error term (residual variance ↑, power ↓), but logistic/cox-ph models don’t have residuals so the treatment effect shrinks as a result.\n\n\noutcome ~ treatment + risk_factor_vars\n\nAdjusting for risk factors stops a loss of power but never increases power like it does for linear models.\n\nIn Cox and logistic models there are no residual terms, and unaccounted outcome heterogeneity has nowhere to go. So it goes into the regression coefficients that are in the model, attenuating them towards zero. Failure to adjust for easily accountable outcome heterogeneity in nonlinear models causes a loss of power as a result.\nModeling is a question of approximating the effects of baseline variables that explain outcome heterogeneity. The better the model the more complete the conditioning and the more accurate the patient-specific effects that are estimated from the model. Omitted covariates or under-fitting strong nonlinear relationships results in effectively conditioning on only part of what one would like to know. This partial conditioning still results in useful estimates, and the estimated treatment effect will be somewhere between a fully correctly adjusted effect and a non-covariate-adjusted effect.",
    "crumbs": [
      "Regression",
      "Logistic"
    ]
  },
  {
    "objectID": "qmd/regression-logistic.html#sec-reg-log-interp",
    "href": "qmd/regression-logistic.html#sec-reg-log-interp",
    "title": "Logistic",
    "section": "Interpretation",
    "text": "Interpretation\n\nMisc\n\nAlso see Visualization &gt;&gt; Log Odds Ratios vs Odds Ratios\n“Divide by 4” shortcut (Gelman)\n\nValid for coefficients and standard errors\n“1 unit increase in X results in a β/4 increase in probability that Y = 1”\nAlways round down\nBetter appproximations when the probability is close to 0.50\n\n\nDefinitions\n\nEffect is non-linear in p\n\nProbability prediction:\n\nOdds Ratio - Describes the percent change in odds of the outcome based on a one unit increase in the input variable.\n\nOR significance is being different from 1 and log odds ratios (logits) significance is being different from 0\nlogit(p) = 0, p = 0.5, OR = 1\nlogit(p) = 6, p = always (close to 1)\nlogit(p) = -6, p = never (close to 0)\nGuidelines\n\nOR &gt; 1 means increased occurrence of an event\nOR &lt; 1 means decreased occurrence of an event\nOR = 1 ↔︎ log OR = 0 ⇒ No difference in odds\nOR &lt; 0.5 or OR &gt; 2 ⇒ Moderate effect\nOR &gt; 4 is pretty strong and unlikely to be explained by unmeasured variables\n\nCode\nprostate_model %&gt;% \n      broom::tidy(exp = TRUE)\n##  term        estimate std.error statistic  p.value\n##  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)  0.0318    0.435    -7.93 2.15e-15\n## 2 fam_hx        0.407      0.478    -1.88 6.05e- 2\n## 3 b_gs          3.27      0.219      5.40 6.70e- 8\n\nExponentiates coefficients to get ORs\nInterpretation: the odds ratio estimate of 0.407 means that for someone with a positive family history (fam_hx) of prostate cancer, the odds of their having a recurrence are 59.3% ((1-0.407) x 100) lower than someone without a family history. Similarly, for each unit increase in the baseline Gleason score (b_gs), the odds of recurrence increase by 227% ((3.27-1) x 100).\n\n\nSummary\n## glm(formula = recurrence ~ fam_hx + b_gs, family = binomial, \n##    data = .)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q      Max \n## -1.2216  -0.5089  -0.4446  -0.2879  2.5315 \n## \n## Coefficients:\n##            Estimate Std. Error z value            Pr(&gt;|z|)\n## (Intercept)  -3.4485    0.4347  -7.932 0.00000000000000215\n## fam_hx      -0.8983    0.4785  -1.877              0.0605\n## b_gs          1.1839    0.2193  5.399 0.00000006698872947\n##               \n## (Intercept) ***\n## fam_hx      . \n## b_gs        ***\n## ---\n## Signif. codes: \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##    Null deviance: 281.88  on 313  degrees of freedom\n## Residual deviance: 246.81  on 311  degrees of freedom\n##  (2 observations deleted due to missingness)\n## AIC: 252.81\n## \n## Number of Fisher Scoring iterations: 5\n\nThe Deviance Residuals should have a Median near zero, and be roughly symmetric around zero. If the median is close to zero, the model is not biased in one direction (the outcome is not over- nor under-estimated).\nThe Coefficients estimate how much a change of one unit in each predictor will affect the outcome (in logit units).\n\nThe family history predictor (fam_hx) is not significant, but trends toward an association with a decreased odds of recurrence, while the baseline Gleason score (b_gs) is significant and is associated with an 18% increased log-odds of recurrence for each extra point in the Gleason score.\n\nNull Deviance and Residual Deviance. The null deviance is measured for the null model, with only an intercept. The residual deviance is measured for your model with predictors. Your residual deviance should be lower than the null deviance.\n\nYou can even measure whether your model is significantly better than the null model by calculating the difference between the Null Deviance and the Residual Deviance. This difference [281.9 - 246.8 = 35.1] has a chi-square distribution. You can look up the value for chi-square with 2 degrees (because you had 2 predictors) of freedom.\nOr you can calculate this in R with pchisq(q = 35.1, df=2, lower.tail = TRUE) which gives you a p value of 1.\n\nThe degrees of freedom are related to the number of observations, and how many predictors you have used. If you look at the mean value in the prostate dataset for recurrence, it is 0.1708861, which means that 17% of the participants experienced a recurrence of prostate cancer. If you are calculating the mean of 315 of the 316 observations, and you know the overall mean of all 315, you (mathematically) know the value of the last observation - recurrence or not - it has no degrees of freedom. So for 316 observations, you have n-1 or 315, degrees of freedom. For each predictor in your model you ‘use up’ one degree of freedom. The degrees of freedom affect the significance of the test statistic (T, or chi-squared, or F statistic).\nObservations deleted due to missingness - the logistic model will only work on complete cases, so if one of your predictors or the outcome is frequently missing, your effective dataset size will shrink rapidly. You want to know if this is an issue, as this might change which predictors you use (avoid frequently missing ones), or lead you to consider imputation of missing values.\n\nPredicted Risk - predicted probabilities from a logistic regression model that’s used to predict the risk of an event given a set of variables.\nstr(salespeople)\n## 'data.frame':    350 obs. of  4 variables:\n##  $ promoted    : int  0 0 1 0 1 1 0 0 0 0 ...\n##  $ sales        : int  594 446 674 525 657 918 318 364 342 387 ...\n##  $ customer_rate: num  3.94 4.06 3.83 3.62 4.4 4.54 3.09 4.89 3.74 3 ...\n##  $ performance  : int  2 3 4 2 3 2 3 1 3 3 ...\n\nmodel &lt;- glm(formula = promoted ~ ., data = salespeople, family = \"binomial\")\nexp(model$coefficients) %&gt;% \n  round(2)\n##  (Intercept)        sales customer_rate  performance2  performance3 \n##          0.00          1.04          0.33          1.30          1.98 \n##  performance4 \n##          2.08\n\nInterpretation:\n\nFor two salespeople with the same customer rating and the same performance, each additional thousand dollars in sales increases the odds of promotion by 4%.\n\nSales in thousands of dollars\n\nFor two salespeople with the same sales and performance, each additional point in customer rating decreases the odds of promotion by 67%\nIncreasing performance4 by 1 unit and holding the rest of the variables constant increases the odds of getting a promotion by 108%.\n\nI think this is being treated as a factor variable, and therefore the estimate is relative to the reference level (performance1).\nShould be: “Performance4 increases the odds of getting a promotion by 108% relative to having a performance1”\n\nFor two salespeople of the same sales and customer rating, performance rating has no significant effect on the odds of promotion.\n\nNone of the levels of performance were statistically significant",
    "crumbs": [
      "Regression",
      "Logistic"
    ]
  },
  {
    "objectID": "qmd/regression-logistic.html#sec-reg-log-ass",
    "href": "qmd/regression-logistic.html#sec-reg-log-ass",
    "title": "Logistic",
    "section": "Assumptions",
    "text": "Assumptions\n\nperformance::model_check(mod) - provides a facet panel of diagnostic charts with subtitles to help interprete each chart.\n\nLooks more geared towards a regression model than a logistic model.\n\nLinear relationship between the logit of the binary outcome and each continuous explanatory variable\n\ncar::boxTidwell\n\nMay not work with factor response so might need to as.numeric(response) and use 1,2 values instead of 0,1\np &lt; 0.05 indicates non-linear relationship which is what you want\n\nCan also look a scatter plot of logit(response) vs numeric predictor\n\nNo outliers\n\nCook’s Distance\n\nDifferent opinions regarding what cut-off values to use. One standard threshold is 4/N (where N = number of observations), meaning that observations with Cook’s Distance &gt; 4/N are deemed as influential\n\nStandardized Residuals\n\nAbsolute standardized residual values greater than 3 represent possible extreme outliers\n\n\nAbsence of Multicollinearity\n\nVIF\nCorrelation Heatmap\n\niid\n\nDeviance residuals (y-axis) vs row index (x-axis) should show points randomly around zero (y-axis)\n\nJesper’s response shows calculation and R code\nI think the dHARMA pkg also handles this\n\n\nSufficiently large sample size\n\nRules of thumb\n\n10 observations with the least frequent outcome for each independent variable\n&gt; 500 observations total\n\nI’m sure Harrell has thoughts on this somewhere",
    "crumbs": [
      "Regression",
      "Logistic"
    ]
  },
  {
    "objectID": "qmd/regression-logistic.html#diagnostics",
    "href": "qmd/regression-logistic.html#diagnostics",
    "title": "Logistic",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nMisc\n\n** The formulas for the deviances for a logistic regression model are slightly different from other GLMs since the deviance for the saturated logistic regression model is 0 **\nAlso see\n\nDiagnostics, Classification\nDiagnostics, GLM\n\n\nResidual Deviance (G2)\n\n-2 * LogLikelihood(proposed_mod)))\n\nNull Deviance\n\n-2 * LogLikelihood(null_mod)))\ni.e. deviance for the intercept-only model\n\nMcFadden’s Pseudo R2 = (LL(null_mod) - LL(proposed_mod)) / LL(null_mod))\n\nSee What are Pseudo-R Squareds? for formulas to various alternative R2s for logistic regression\nThe p-value for this R2 is the same as the p-value for:\n\n2 * (LL(proposed_mod) - LL(null_mod))\nNull Deviance - Residual Deviance\n\nFor the dof, use proposed_dof - null_dof\n\ndof for the null model is 1\n\n\n\nExample: Getting the p-value\nm1 &lt;- glm(outcome ~ treat)\nm2 &lt;- glm(outcome ~ 1)\n(ll_diff &lt;- logLik(m1) - logLik(m2))\n## 'log Lik.' 3.724533 (df=3)\n1 - pchisq(2*ll_diff, 3)\n\nCompare nested models\n\nModels\nmodel1 &lt;- glm(TenYearCHD ~ ageCent + currentSmoker + totChol, \n              data = heart_data, family = binomial)\nmodel2 &lt;- glm(TenYearCHD ~ ageCent + currentSmoker + totChol + \n                as.factor(education), \n              data = heart_data, family = binomial)\n\nAdd Education or not?\n\nExtract Deviances\n# Deviances\n(dev_model1 &lt;- glance(model1)$deviance)\n## [1] 2894.989\n(dev_model2 &lt;- glance(model2)$deviance)\n## [1] 2887.206\nCalculate difference and test significance\n# Drop-in-deviance test statistic\n(test_stat &lt;- dev_model1 - dev_model2)\n## [1] 7.783615\n\n# p-value\n1 - pchisq(test_stat, 3)  # 3 = number of new model terms in model2 (i.e. 3(?) levels of education)\n## [1] 0.05070196\n\n# Using anova\nanova(model1, model2, test = \"Chisq\")\n## Analysis of Deviance Table\n## \n## Model 1: TenYearCHD ~ ageCent + currentSmoker + totChol\n## Model 2: TenYearCHD ~ ageCent + currentSmoker + totChol + as.factor(education)\n##  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) \n## 1      3654    2895.0                       \n## 2      3651    2887.2  3  7.7836  0.0507 .\n\n|β| &gt; 10\n\nExtreme\nImplies probabilities close to 0 or 1 which is suspect\nConsider removal of the variable or outlier(s) influencing the model\n\nIntercepts ≈ -17\n\nIndicate a need for a simpler model (see bkmk - Troubleshooting glmmTMB)\n\nIf residuals are heteroskedastic, see {glmx}\n\nneg.bin, hurdle, logistic - Extended techniques for generalized linear models (GLMs), especially for binary responses, including parametric links and heteroskedastic latent variables\n\nBinned Residuals\n\nIt is not useful to plot the raw residuals, so examine binned residual plots\nMisc\n\n{arm} will mask some {tidyverse} functions, so don’t load whole package\n\nLook for :\n\nPatterns\nNonlinear trend may be indication that squared term or log transformation of predictor variable required\nIf bins have average residuals with large magnitude\nLook at averages of other predictor variables across bins\nInteraction may be required if large magnitude residuals correspond to certain combinations of predictor variables\n\nProcess\n\nExtract raw residuals\n\nInclude type.residuals = \"response\" in the broom::augment function to get the raw residuals\n\nOrder observations either by the values of the predicted probabilities (or by numeric predictor variable)\nUse the ordered data to create g bins of approximately equal size.\n\nDefault value: g = sqrt(n)\n\nCalculate average residual value in each bin\nPlot average residuals vs. average predicted probability (or average predictor value)\n\nExample: vs Predicted Values\narm::binnedplot(x = risk_m_aug$.fitted, y = risk_m_aug$.resid,\n                xlab = \"Predicted Probabilities\",\n                main = \"Binned Residual vs. Predicted Values\",\n                col.int = FALSE)\n\nExample: vs Predictor\narm::binnedplot(x = risk_m_aug$ageCent,\n                y = risk_m_aug$.resid,\n                col.int = FALSE,\n                xlab = \"Age (Mean-Centered)\",\n                main = \"Binned Residual vs. Age\")\n\nCheck that residuals have mean zero: mean(resid(mod))\nCheck that residuals for each level of categorical have mean zero\nrisk_m_aug %&gt;%\n  group_by(currentSmoker) %&gt;%\n  summarize(mean_resid = mean(.resid))",
    "crumbs": [
      "Regression",
      "Logistic"
    ]
  },
  {
    "objectID": "qmd/regression-logistic.html#marginal-effects",
    "href": "qmd/regression-logistic.html#marginal-effects",
    "title": "Logistic",
    "section": "Marginal Effects",
    "text": "Marginal Effects\n\nMisc\n\nNotes from\n\nR &gt;&gt; Documents &gt;&gt; Regression &gt;&gt; glm-marginal-effects.pdf\n\nAlso see\n\nRegression, Interactions &gt;&gt; Logistic Regression\nPost-Hoc Analysis, emmeans &gt;&gt; Logistic Regression\n\nMarginal Effects and Elasticities are similar except elasticities are percent change.\n\ne.g. a percentage change in a regressor results in this much of a percentage change in the response level probability\n\n\nIn general\n\nA “marginal effect” is a measure of the association between an infinitely small change in a regressor and a change in the response variable\n\n“infinitely small” because we’re using partial derivatives\nExample: If I change the cost (regressor) of taking the bus, how does that change the probability (not odds or log odds) of taking a bus to work instead of a car (response)\n\nAllows you to ask counterfactuals.\n\n\nIn OLS regression with no interactions or higher-order term, the estimated slope coefficients are marginal effects, but for glms, the coefficients are not marginal effects at least not on the scale of the response variable\nMarginal effects are partial derivatives of the regression equation with respect to each variable in the model for each unit in the data. (also see notebook, Regression &gt;&gt; logistic section)\n\nLogistic Regression\n\nThe partial derivative gives the slope of a tangent line at point on a nonlinear curve (e.g. logit) which is the linear change in probability at a single point on the nonlinear curve\n\\[\n\\frac {\\partial P}{\\partial X_{k}} = \\beta_{k} \\times P \\times (1 - P)\n\\]\n\nWhere P is the predicted probability and β is the model coefficient for the kth predictor\n\n\n3 types of marginal effects\n\nMarginal effect at the Means\n\nThe marginal effect that is “typical” of the sample\nEach model coefficient is multiplied times their respective independent variables mean and summed along with the intercept.\n\nThis sum is transformed from a logit to a probability, P, is used in the partial derivative equation to calculate the marginal effect\n\nInterpretation\n\ncontinuous: an infinitely small change in  while all other predictors are held at their means results in a  change in \n\ne.g. an infinitely small change in this predictor for a hypothetical average person results in this amount of change in the dependent variable\n\n\n“at the medians” can be easily calculated using marginaleffects::typical  (this is outdated) without any columns specified and then used to calculate marginal effects\n\nto get “at the means” you’d have to supply each column with its mean value\n\n\nMarginal effect at Representative (“Typical”) Values\n\nThe marginal effect that is “typical” of a group represented in the sample\nA real “average” person doesn’t usually have the mean/median values for predictor values (e.g  mean age of 54.68 years), so you might want to find the marginal effect for a “typical” person of a certain demographic or group you’re interested in by specifying values for predictors\nInterpretation\n\ncontinuous: an infinitely small change in  for a person with  results in a  change in \n\nCalculate using marginaleffects::typical  (this is outdated) to specify column values (this is outdated)\n\nAverage Marginal Effect (AME)\n\nThe marginal effect of a case chosen at random from the sample\n\nConsidered the best summary of an independent variable\n\nCalculate marginal effect for each observation (distribution of marginal effects) and then take the average\n\nMultiply the model coefficient times each value of an independent variable, repeat for each predictor, sum with intercept, use predicted probabilities to calculate marginal effect, and average all marginal effects across all obseravation for a predictor to get the AME\n\nInterpretation\n\ncontinuous: Holding all covariates constant, an infinitely small change in  results in a  change in  on average.\n\nUse marginaleffects::marginaleffects +summary or tidy\n\n\n{marginaleffects}\n\nExample: Palmer penguins data and logistic regression model\n\nCreate marginal effects object\nmfx &lt;- marginaleffects(mod)\nhead(mfx)\n#&gt;  rowid              term        dydx  std.error large_penguin bill_length_mm\n#&gt; 1    1    bill_length_mm 0.017622745 0.007837288            0          39.1\n#&gt; 2    1 flipper_length_mm 0.006763748 0.001561740            0          39.1\n#&gt; 3    2    bill_length_mm 0.035846649 0.011917159            0          39.5\n#&gt; 4    2 flipper_length_mm 0.013758244 0.002880123            0          39.5\n#&gt; 5    3    bill_length_mm 0.084433436 0.021119186            0          40.3\n#&gt; 6    3 flipper_length_mm 0.032406447 0.008159318            0          40.3\n#&gt;  flipper_length_mm species\n#&gt; 1              181  Adelie\n#&gt; 2              181  Adelie\n#&gt; 3              186  Adelie\n#&gt; 4              186  Adelie\n#&gt; 5              195  Adelie\n#&gt; 6              195  Adelie\nAverage Marginal Effect (AME)\nsummary(mfx)\n#&gt; Average marginal effects \n#&gt;      type              Term          Contrast    Effect Std. Error  z value\n#&gt; 1 response    bill_length_mm              &lt;NA&gt;  0.02757    0.00849  3.24819\n#&gt; 2 response flipper_length_mm              &lt;NA&gt;  0.01058    0.00332  3.18766\n#&gt; 3 response          species Chinstrap / Adelie  0.00547    0.00574 -4.96164\n#&gt; 4 response          species    Gentoo / Adelie  2.19156    2.75319  0.62456\n#&gt; 5 response          species Gentoo / Chinstrap 400.60647  522.34202  4.59627\n#&gt;    Pr(&gt;|z|)      2.5 %    97.5 %\n#&gt; 1  0.0011614    0.01093    0.04421\n#&gt; 2  0.0014343    0.00408    0.01709\n#&gt; 3 2.0906e-06  -0.00578    0.01673\n#&gt; 4  0.8066373  -3.20459    7.58770\n#&gt; 5 1.2828e-05 -623.16509 1424.37802\n\nInterpretation:\n\nHolding all covariates constant, for an infinitely small increase in bill length, the probability of being a large penguin increases on average by 2.757%\nSpecies contrasts are from {emmeans}(also see Post-Hoc Analysis, emmeans)\n\nContrasts from get_contrasts.R on package github using emmeans::contrast(emmeans_obj, method = \"revpairwise\")\nodds ratios\nYou can get response means per category on the probability scale using emmeans::emmeans(mod, \"species\", type = \"response\")\n\n\n\n\nOther features available for marginaleffects object\n\ntidy\n\ncoef stats with AME, similar to summary, just a different a object class I think\n\nglance\n\nmodel GOF stats\n\ntypical\n\ngenerate artificial predictor values and get marginal effects for them\nmedian (or mode depending on variable type) values used for columns that aren’t provided\n\ncounterfactual\n\nuse observed data for all but a one or a few columns. Provide values for those column(s) (e.g. flipper_length_mm = c(160, 180))\ngenereates a new larger dataset where each observation has each of the provided values\n\nViz\n\n{modelsummary} tables\nplot (error bar plot for AME) and plot_cme (line plot for interaction term)\n\noutputs ggplot objects\n\n\n\n\nCategorical Variables\n\nThe 3 types of marginal effects can be modified for categoricals\nSteps (“at the means”, binary)\n\nCalculate the predicted probability when the variable = 1 and the other predictors are equal to their means.\nCalculate the predicted probability when the variable = 0 and the other predictors are equal to their means.\nThe difference in predicted probabilities is the marginal effect for a change from the “base level” (aka reference category)\n\nThis can extended to categorical variables with multiple categories by calculating the pairwise differences between each category and the reference category (contrasts)",
    "crumbs": [
      "Regression",
      "Logistic"
    ]
  },
  {
    "objectID": "qmd/regression-logistic.html#probit",
    "href": "qmd/regression-logistic.html#probit",
    "title": "Logistic",
    "section": "Probit",
    "text": "Probit\n\n“For some reason, econometricians have never really taken on the benefits of the generalized linear modelling framework. So you are more likely to see an econometrician use a probit model than a logistic regression, for example. Probit models tended to go out of fashion in statistics after the GLM revolution prompted by Nelder and Wedderburn (1972).” (Hyndman)\nVery similar to Logistic\n\nWhere probit and logistic curves differ\n\nlink function: Φ-1(p) where Φ is the standard normal CDF:\n\nThe probability prediction, p:",
    "crumbs": [
      "Regression",
      "Logistic"
    ]
  },
  {
    "objectID": "qmd/regression-logistic.html#visualization",
    "href": "qmd/regression-logistic.html#visualization",
    "title": "Logistic",
    "section": "Visualization",
    "text": "Visualization\n\nMisc\n\nNotes from\n\nVisualizing logistic regression results for non-technical audiences (github, vignette, video)\n\nTables\n\n\nAppropriate for technical audiences who are familiar with logistic regression\nCan be taxing on the eyes making it difficult to absorb insights from your model\n\n\nLog Odds Ratio vs Odds Ratio\n\n\nUsing log odds ratio is recommended for visualizations for puposes of comparing effect magnitudes\n\nEach coefficient represents an additive change on the log odds scale; when we exponentiate to get odds, each coefficient represents a multiplicative change\nOdds ratios makes the chart asymmetric and squishes some bars (e.g. Pet: Fish)\nOdds ratios can be misleading in comparing negative vs postive variable effects\n\ne.g. Prior GPA looks like it has much bigger effect than Pet: Fish when using odds ratio\n\nThere’s a danger that the percent change in odds might be misinterpreted as the absolute probability of the outcome (or the change in its probability)\n\ne.g. A 300% change in the odds ratio is a tripling of the odds ratio, not an 300% increase in probability. (see example below)\n\n\nAlthough numerically, changes in odds ratios may be a bit easier to describe for your audience than changes in log odds.\n\nExample: tripling the odds ratio is like going from 3-to-1 odds to 9-to-1 odds, or from 1-to-3 odds to an even chance.)\n\n\nError bar chart (caterpillar chart)\n\nChange in log odds\n\n\nThese are untransformed parameter estimates (See Misc &gt;&gt; Tables for estimate values)\n\nChange in probability from a baseline\n\nSince the effect is non-linear (See Interpetation &gt;&gt; Effect is non-linear in p), a baseline is needed in order to properly interpret the changes in probability due to the increases of 1 unit of a variable\nProcess:\n\nChoose an appropriate baseline\nCompute the marginal effect of a predictor given that baseline\n\nIntercept as a baseline\n\n\nThe x-axis is the problem. Values shown depict changes from the baseline\nBoth charts have the same values, but the chart on the left more clearly indicates the meaning while the chart on the right includes the CIs\nEstimates transformations\n\nIntercept: invlogit(intercept)\nOther parameters: invlogit(&lt;param&gt; + intercept)\n\nIncluding CI values\n\n\nVertical line is the output of the inverse logit of the intercept\n\nRepresenting the probability of passing for a student with average prior GPA, average height, and the baseline value of each categorical variable – not a Mac user, doesn’t wear glasses, has no pet, favorite color is blue, and didn’t go to tutoring.\n\n\n\n\nOutcome vs Predictor with Logistic Curve\n\n# Plot of data with a logistic curve fit\n  ggplot(data, aes(x = z_homr, y = as.numeric(alive) - 1)) +\n    geom_jitter(height = 0.1, size =1, alpha = 0.5) +\n    geom_smooth(method = \"glm\",\n                method.args = list(family = \"binomial\")) +\n    theme_minimal() +\n    scale_y_continuous(breaks = c(0, 1), labels = c(\"Alive\", \"Dead\")) +\n    ylab(\"\") +\n    xlab(\"HOMR Linear Predictor\")\n\nDead/Alive is the outcome and HOMR is the predictor\n\nPredictor by outcome (count)\n\ng1 &lt;- ggplot(data, aes(x = z_homr, fill = alive)) +\n    geom_histogram() +\n    theme_minimal() +\n    xlab(\"HOMR Linear Predictor\") +\n    ylab(\"Number of Participants\") +\n    scale_fill_brewer(\"Alive\", palette = \"Paired\")\nPredictor by outcome (proportion)\n\ng2 &lt;- ggplot(data, aes(x = z_homr, fill = alive)) +\n    geom_histogram(position = \"fill\") +\n    theme_minimal() +\n    xlab(\"HOMR Linear Predictor\") +\n    ylab(\"Proportion\") +\n    scale_fill_brewer(\"Alive\", palette = \"Paired\")\nPredictor vs Outcome (beeswarm)\n\nggplot(data, aes(y = z_homr, x = alive, fill = alive, color = alive)) +\n    geom_beeswarm() +\n    geom_boxplot(alpha = 0, color = \"black\") +\n    theme_minimal() +\n    ylab(\"HOMR Linear Predictor\") +\n    xlab(\"Alive at 1 year\") +\n    scale_fill_brewer(guide = FALSE, palette = \"Paired\") +\n    scale_color_brewer(guide = FALSE, palette = \"Paired\")",
    "crumbs": [
      "Regression",
      "Logistic"
    ]
  },
  {
    "objectID": "qmd/regression-multinomial.html",
    "href": "qmd/regression-multinomial.html",
    "title": "Multinomial",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Regression",
      "Multinomial"
    ]
  },
  {
    "objectID": "qmd/regression-multinomial.html#sec-reg-multin-misc",
    "href": "qmd/regression-multinomial.html#sec-reg-multin-misc",
    "title": "Multinomial",
    "section": "",
    "text": "AKA Random Utility or Choice Models\nGoal: Model the category probabilities for a polytomous (i.e. # of categoies &gt; 2) response\n\nLet \\(\\phi_{ij} \\equiv \\phi_j(\\vec{x_i})\\) be the probability of response category j for unit i given predictor \\(\\vec{x_i}\\) .\nSince the sum of the probabilities of all the categories equals 1, if you know m-1 probabilities for m categories, then you know the probability for the last category, \\(\\phi_{im}\\).\n\\[\n\\phi_{im} = 1 - \\sum_{j=1}^{m-1} \\phi_{ij}\n\\]\nThe essential idea is to construct a model for the polytomous response composed of m−1 logit comparisons among the response categories in a manner analogous to the treatment of factors in the predictor variables (i.e. creating dummy variables for the m-1 categories).\n\nNotes from ResEcon 703 Video Course\n\nWeeks 3, 4, 9, 10, 12, and 13\nVideo lectures talk through mathematics (interpretations, motivations, benefits, limitations, etc.) of the approaches. Each week ends with a coding session illustrating the approaches that’s not included in the videos but is included in the slides.\n\nThe slides are updated with new material while the videos have not.\n\ngithub with slides and problem sets/solutions with R code. Slides provide a deeper introduction to the application the algorithms and the econometrics. Problem set solutions have similar material but written out more clearly. {mlogit} used throughout.\n\nAlso see:\n\nEconometrics, Discrete Choice Models\nDiagnostics, Classification &gt;&gt; Multinomial\nClassification &gt;&gt; Discriminant Analysis &gt;&gt; Linear Discriminant Analysis (LDA)\nModel Building, brms &gt;&gt; Logistic Regression\n\nPackages\n\n{mlogit} - Enables the estimation of random utility models with choice situation and/or alternative specific variables. The main extensions of the basic multinomial model (heteroscedastic, nested and random parameter models) are implemented.\n\nVignette\n\n{apollo} - Both classical and Bayesian estimation is available, and multiple discrete continuous models are covered in addition to discrete choice. Multi-threading processing is supported for estimation and a large number of pre and post-estimation routines, including for computing posterior (individual-level) distributions.\n{mixl} - Fast (C++) estimation of complex mixed models for large datasets. Allows you to specify multiple utility functions, which is standard practice for more complicated models. (Paper)\n{nestedLogit} - Provides functions for fitting nested dichotomy logistic regression models for a polytomous response.\n\nNested dichotomies are statistically independent, and hence provide an additive decomposition of tests for the overall polytomous response. When the dichotomies make sense substantively, this method can be a simpler alternative to the standard multinomial logistic model which compares response categories to a reference level.\n\n{mnlogit} - (CRAN archived with no github) efficient estimation of MNL for large data sets\n\nOffers speedups of 10 - 50 times for modestly sized problems and more than 100 times for larger problems. Running in parallel mode on a multicore machine gives up to 4 times additional speedup on 8 processor cores. mnlogit achieves its computational efficiency by drastically speeding up computation of the log-likelihood function’s Hessian matrix through exploiting structure in matrices that arise in intermediate calculations.\n\n{{torch-choice}} - Choice modeling with PyTorch: logit model and nested logit model\n\nCan handle big data\n\n{{xlogit}} - A Python Package for GPU-Accelerated Estimation of Mixed Logit Models\n{{MODE.behave}} - Incorporates estimation routines for conventional multinomial logit models, as well as for mixed logit models with nonparametric distributions\n\nMakes use of latin hypercube sampling to increase the efficiency of the expectation maximization algorithm during the estimation process in order to decrease computation time.\nPre-estimated discrete choice simulation methods for transportation research are included\n\n{glmnet}\n{multgee}\n\nUse Cases:\n\nRespondents to a social survey are classified by their highest completed level of education, taking on the values (1) less than highschool, (2) highschool graduate, (3) some post-secondary, or (4) post-secondary degree.\nWomen’s labor-force participation is classified as (1) not working outside the home, (2) working part-time, or (3) working full-time.\nVoters in Quebec in a Canadian national election choose one of the (1) Liberal Party, (2) Conservative Party, (3) New Democratic Party, or (4) Bloc Quebecois.",
    "crumbs": [
      "Regression",
      "Multinomial"
    ]
  },
  {
    "objectID": "qmd/regression-multinomial.html#sec-reg-multin-terms",
    "href": "qmd/regression-multinomial.html#sec-reg-multin-terms",
    "title": "Multinomial",
    "section": "Terms",
    "text": "Terms\n\nAlternative - The levels of a polytomous response in Random Utility Models.\nChoice Probability - The probability that a decision-maker will chose a particular alternative. The predicted response for a Random Utility Model.\nDiscounted Utility - The utility of some future event, such as consuming a certain amount of a good, as perceived at the present time as opposed to at the time of its occurrence. It is calculated as the present discounted value of future utility, and for people with time preference for sooner rather than later gratification, it is less than the future utility.\nMarginal Utility - Coefficients in the random choice model. The added satisfaction that a consumer gets from having one more unit of a good or service. The concept of marginal utility is used by economists to determine how much of an item consumers are willing to purchase.\nMarket Level - Environment or category where a class or brand of products share the same attributes\n\ne.g. All Cokes should cost the same in Indianapolis but that price may be different from the price of Cokes in Nashville. Therefore, Indianapolis and Nashville are separate markets.\n\nMarket Share - The percentage of total sales in an industry or product category that belong to a particular company during a discrete period of time. For a Random Utility Model, when the data is at the market level instead of the individual level, the predicted response is the Market Share.\nOutside Product (aka Outside Option) - Typically a “purchase nothing” indicator variable/variable category\n\nCan just be a vector of 0s\n\nRandom Utility Models - These models rely on the hypothesis that the decision maker is able to rank the different alternatives by an order of preference represented by a utility function, the chosen alternative being the one which is associated with the highest level of utility. They are called random utility models because part of the utility is unobserved and is modeled as the realization of a random deviate.",
    "crumbs": [
      "Regression",
      "Multinomial"
    ]
  },
  {
    "objectID": "qmd/regression-multinomial.html#sec-reg-multin-mnl",
    "href": "qmd/regression-multinomial.html#sec-reg-multin-mnl",
    "title": "Multinomial",
    "section": "Multinomial Logit (MNL)",
    "text": "Multinomial Logit (MNL)\n\nAKA Generalized Logit\nWhen the polytomous response has m levels (aka Alternatives), the multinomial logit model comprises m−1 log-odds comparisons with a reference level, typically the first or last.\n\nThe likelihood under the model and the fitted response probabilities that it produces are unaffected by choice of reference level, much as choice of reference level for dummy regressors created from a factor predictor doesn’t affect the fit of a regression model.\n\nChoice Probability for alternative, i, and decision-maker, n:\n\\[\nP_{ni} = \\frac {e^{V_{ni}}}{\\sum_j e^{V_{nj}}} = \\frac {e^{\\hat{\\beta}x_{ni}}}{\\sum_j e^{{\\hat{\\beta}x_{nj}}}}\n\\]\n\nThe probability that decision-maker, \\(n\\), chooses alternative, \\(i\\)\nPredicted probability output from the model\n\nData sets used to estimate random utility models have therefore a specific structure that can be characterized by three indexes: the alternative, the choice situation, and the individual. The distinction between choice situation and individual indexes is only relevant if we have repeated observations for the same individual.\n\nExamples of variable types\n\nChoice Situation Specific\n\ndata1: Length of the vacation and the Season\ndata2: values of dist, income and urban are repeated four times.\n\ndist (the distance of the trip)\nincome (household income)\nurban (a dummy for trips which have a large city at he origin or the destination)\nnoalt the number of available alternatives\n\n\nIndividual Specific\n\ndata1: Income and Family Size\ndata2: None\n\nAlternative Specific\n\ndata1: Distance to Destination and Cost\ndata2:\n\ntransport modes (air, train, bus and car)\ncost for monetary cost\nivt for in vehicle time\novt for out of vehicle time\nfreq for frequency\n\n\n\nThe unit of observation is therefore the choice situation, and it is also the individual if thereis only one choice situation per individual observed, which is often the case\nData Descriptions\n\nEach individual has responded to several (up to 16) scenarios.\nFor every scenario, two train tickets A and B are proposed to the user, with different combinations of four attributes: price (the price in cents of guilders), time (travel time in minutes), change (the number of changes) and comfort (the class of comfort, 0, 1 or 2, 0 being the most comfortable class).\n\nDataset in wide format\n#&gt;    id choiceid choice price_A time_A change_A comfort_A price_B time_B\n#&gt; 1  1  1        A      2400    150    0        1         4000    150\n#&gt; 2  1  2        A      2400    150    0        1         3200    130\n#&gt; 3  1  3        A      2400    115    0        1         4000    115\n\n#&gt;    change_B comfort_B\n#&gt; 1  0        1\n#&gt; 2  0        1\n#&gt; 3  0        0\nTransform to long format and add index attribute with {dfidx}\nTr &lt;- \n  dfidx(Train, \n        shape = \"wide\", \n        choice = \"choice\",\n        varying = 4:11, \n        sep = \"_\", \n        idx = list(c(\"choiceid\", \"id\")),\n        idnames = c(\"chid\", \"alt\"),\n        opposite = c(\"price\", \"comfort\", \"time\", \"change\"))\n\nhead(Tr, 3)\n#&gt; ~~~~~~~\n#&gt; first 3 observations out of 5858\n#&gt; ~~~~~~~\n#&gt;   choice price     time  change comfort idx\n#&gt; 1 TRUE   -52.88904 -2.5  0      -1      1:A\n#&gt; 2 FALSE  -88.14840 -2.5  0      -1      1:B\n#&gt; 3 TRUE   -52.88904 -2.5  0      -1      2:A\n#&gt; ~~~ indexes ~~~~\n#&gt;   chid id alt\n#&gt; 1 1    1  A\n#&gt; 2 1    1  B\n#&gt; 3 2    1  A\n#&gt; indexes: 1, 1, 2\n\nIn “long” format (default) is a data.frame with a special column which containsthe corresponding indexes. A lot of these argument values are passed to reshape to produce the final format.\n“varying” - Indicates alternative specific variables which is a numeric vector that indicates their position in the data frame.\n“sep” - In wide format, variable names are of the form, &lt;var&gt;_choice (default “.”).\n“choice” - Indicates the response that has to be transformed in a logical value\n“idx” - Adds attribute which specifies the nesting structure of the id variables\n\nEach individual faces different choice situations, the “id” variable, which contains the individual index, nests the choice situation variable called “choiceid.”\nContains the three relevant indexes: choiceid is the choice situation index, alt the alternative index and id is the individual index.\n\n“opposite” - If you expect negative coefficients, taking the opposite of the covariates will lead to expected positive coefficients. (?)\n\nAdd idx attribute to an already long formatted df\nMC &lt;- \n  MC &lt;- dfidx(ModeCanada, \n              subset = noalt == 4,\n              idx = c(\"case\", \"alt\"))\n\nOrder of variables doesn’t seem to matter for the “idx” arg in order to specify index types\nThe primary dplyr verbs can be applied to dfidx objects\n\n\n\n\n{mlogit}\n\nIndexes: the alternative, the choice situation and the individual.\n\nThe distinction between choice situation id and individual id is only relevant if we have repeated observations for the same individual\nIn docs, the choice situation/individual id is indexed by \\(i\\) and the alternatives by \\(j\\)\n\nTypes of variables\n\n\n\n\n\n\n\n\nType\nVariable\nCoefficient\n\n\n\n\nAlternative and Choice Situation Specific\nAlternative Specific (only)\n\\(x_{ij}\\)\n\\(t_j\\)\n\\(\\beta\\) (generic)\n\\(\\nu\\) (generic)\n\n\nChoice Situation Specific\n\\(z_i\\)\n\\(\\gamma_j\\) (alternative specific)\n\n\nAlternative and Choice Situation Specific\n\\(w_j\\)\n\\(\\delta_j\\) (alternative specific)\n\n\nChoice Situation Specific\n\\(v_i\\)\nInfluence Variance of Errors\n\n\n\n\nLabelling your variables as one of these types isn’t clear-cut, and seems a little closer to an art than a science.\n\n\nFormula Syntax: y ~ a | b | c | d\n\na: Variables with common parameters\nb: Individual-specific variables with alternative-specific parameters\nc: Alternative-specific variables with alternative-specific parameters\nd: Individual-specific variables that affect the scale parameter\n\ndfidx Args\n\ndata: data frame you wanted to be converted\nshape: ‘wide’ or ‘long’ for the format of the original data frame\nchoice:\n\nFor shape = \"wide\": categorical variable that contains the all alternative categories\nFor shape = \"long\": Indicator variable where TRUE is for 1 category and FALSE for everything else\n\nThe fourth argument depends on the format of the original data frame\n\nshape = \"wide\" - varying: numeric vector defining which variables by column index which contain alternative-specific data\nshape = \"long\"- idx: two-element character vector defining which which variables contain identifiers for the choice situation and alternative\n\n\nExample: Convert data to {dfidx} format\n\ndata(Heating, package = \"mlogit\")\nRaw Data (wide)\n\ndepvar: Choice set\n\ngc: gas central\ngr: gas room\nec: electric central\ner: electric room\nhp: heat pump\n\nAlternative-specific data (Each variable has 1 column for each alternative)\n\nic: installation cost\noc: annual operating cost\n\nHousehold demographic data\n\nincome: annual income\nagehed: age of household head\nrooms: number of rooms\nregion: home location\n\nOthers\n\nidcase: id variable (1 to 1)\n\n\nConvert Wide to Long using {dplyr}\nheating_long &lt;- heating %&gt;%\n  pivot_longer(contains('.')) %&gt;%\n  separate(name, c('name', 'alt')) %&gt;%\n  pivot_wider() %&gt;%\n  mutate(choice = (depvar == alt)) %&gt;%\n  select(-depvar)\n\nChoice: logical; TRUE if alternative (alt) == gc, otherwise FALSE\nalt: categorical with all alternatives\ndemographic variables\nic and oc: all wide variables pivoted into 2 variables\n\nAs there were 5 variables for each, the id var now has 5 of each id\n\n\nConvert from Wide\nheating_dfidx &lt;- \n  dfidx(heating, \n        shape = 'wide',\n        choice = 'depvar', \n        varying = 3:12)\n\nheating_dfidx\n#&gt;    idcase depvar income agehed rooms region      ic     oc  idx\n#&gt; 1       1  FALSE      7     25     6 ncostl  859.90 553.34 1:ec\n#&gt; 2       1  FALSE      7     25     6 ncostl  995.76 505.60 1:er\n#&gt; 3       1   TRUE      7     25     6 ncostl  866.00 199.69 1:gc\n#&gt; 4       1  FALSE      7     25     6 ncostl  962.64 151.72 1:gr\n#&gt; 5       1  FALSE      7     25     6 ncostl 1135.50 237.88 1:hp\n\n#&gt; ~~~ indexes ~~~~\n#&gt;    id1 id2\n#&gt; 1    1  ec\n#&gt; 2    1  er\n#&gt; 3    1  gc\n#&gt; 4    1  gr\n#&gt; 5    1  hp\n#&gt; indexes:  1, 2 \n\nprint(tibble(heating_dfidx), n = 10)\n#&gt; # A tibble: 4,500 × 9\n#&gt;    idcase depvar income agehed rooms region    ic    oc idx$id1 $id2 \n#&gt;     &lt;dbl&gt; &lt;lgl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt; &lt;fct&gt;\n#&gt;  1      1 FALSE       7     25     6 ncostl  860.  553.       1 ec   \n#&gt;  2      1 FALSE       7     25     6 ncostl  996.  506.       1 er   \n#&gt;  3      1 TRUE        7     25     6 ncostl  866   200.       1 gc   \n#&gt;  4      1 FALSE       7     25     6 ncostl  963.  152.       1 gr   \n#&gt;  5      1 FALSE       7     25     6 ncostl 1136.  238.       1 hp \n\n“depvar” is a column with all the choice categories (i.e. alternatives)\n\nIt’s a factor type but I’m not sure if that’s necessary. Character type might be sufficient.\n\nColumns 3:12 are the “wide variable” columns. There’s a column for each alternative category\n\nEssentially 2 variables and there’s 5 alternatives, therefore, 5 \\(\\times\\) 2 = 10 columns\n\n\nConvert from Long\nheating_long_dfidx &lt;- \n  dfidx(heating_long, \n        shape = 'long',\n        choice = 'choice', \n        idx = c('idcase', 'alt'))\n\nOutput is the same except instead of the index columns being named “id1, id2”, they’re named “idcase”, “alt”.\nidx: Since the df is already in long format, the id variabler (“idcase”) has multiple observations for each id. Therefore, there needs be more than 1 variable to create a unique identifier (i.e. “alt”).",
    "crumbs": [
      "Regression",
      "Multinomial"
    ]
  },
  {
    "objectID": "qmd/regression-multinomial.html#sec-reg-multin-nestlog",
    "href": "qmd/regression-multinomial.html#sec-reg-multin-nestlog",
    "title": "Multinomial",
    "section": "Nested Logit",
    "text": "Nested Logit\n\nFits separate models for each of a hierarchically nested set of binary comparisons among the response categories. The set of m−1 models comprises a complete model for the polytomous response, just as the multinomial logit model does.\nMisc\n\nIIA still holds for two alternatives in the same dichotomy, but doesn’t hold for alternatives of different different dichotomies\nBoth MNL and Nested Logit methods have have p×(m−1) parameters. The models are not equivalent, however, in that they generally produce different sets of fitted category probabilities and hence different likelihoods.\n\nMultinomial logit model treats the response categories symmetrically\n\nExtensions\n\nPaired Combinatorial Logit\n\nAllows alternatives to be in multiple dichotomies and multiple hierarchies and for them to have more complex correlation structures\n\nIn nested logits, only alternatives within the same hierarchy are modeled as being correlated with each other.\n\nCreates pairwise dichotomies for each combination of alternatives (i.e. each alternative appears in J-1 nests)\nBest for data with fewer alternatives since the parameter space can explode fairly quickly\n\nGeneralized Nested Logit\n\nAllow alternatives to be in any dichotomy in any hierarchy, but assign a weight to each alternative in each dichotomy.\nEstimate Parameters: \\(\\lambda_k\\) (See Choice Probability below), \\(\\alpha_{jk}\\) : “weight” or proportion of alternative, \\(j\\), in dichotomy, \\(k\\)\nNeed to be careful about how many dichotomies that you place each alternative, since the parameter space can explode fairly quickly\n\nHeteroskedastic Logit\n\nHeteroskedacity in this sense means that the variance of the unobserved utility (aka residuals) can be different for each alternative\n\\[\n\\mbox{Var}(\\epsilon_{nj}) = \\frac {(\\theta_j \\pi)^2}{6}\n\\]\nSince there’s no closed form solution, simulation methods must be usded to get the choice probabilities and model parameters.\n\n\n\nConstruction of Dichotomies\n\nBy the construction of nested dichotomies, the submodels are statistically independent (because the likelihood for the polytomous response is the product of the likelihoods for the dichotomies), so test statistics, such as likelihood ratio (G2) and Wald chi-square tests for regression coefficients can be summed to give overall tests for the full polytomy.\nNested dichotomies are not unique and alternative sets of nested dichotomies are not equivalent: Different choices have different interpretations. Moreover, and more fundamentally, fitted probabilities and hence the likelihood for the nested-dichotomies model depend on how the nested dichotomies are defined.\nExample: 2 methods of splitting a 4-level response into dichotomies\n\n\nLeft: Y = {1, 2, 3, 4} → {1,2} vs {3,4} → {1} vs {2} and {3} vs {4}\nRight: (Continuous Logit) Y = {1, 2, 3, 4} → {1} vs {2, 3, 4} → {2} vs {3, 4} → {3} vs {4}\n\n{1} vs. {2,3,4} could represent highschool graduation\n{2} vs. {3,4} could represesnt enrollment in post-secondary education\n{3} vs. {4} could represent completion of a post-secondary degree.\n\n\n\nExample: {nestedLogit}\n\nSee Vignette for additional methods including effects plotting, hypothesis testing, GoF tables, etc.\nData:\n\n“partic”: labor force participation, the response, with levels:\n\n“fulltime”: working full-time\n“not.work”: not working outside the home\n“parttime” : working part-time.\n\n“hincome”: Husband’s income, in $1,000s.\n“children”: Presence of children in the home, \"absent\" or \"present\".\n\nSet-up sets of dichotomies that you want analyze\nset1 &lt;- \n  logits(work = dichotomy(\"not.work\", \n                          working = c(\"parttime\", \"fulltime\")),\n         full = dichotomy(\"parttime\", \"fulltime\"))\n\nset2 &lt;-\n  logits(full = dichotomy(nonfulltime = c(\"not.work\", \"parttime\"),\n                          \"fulltime\"),\n         part = dichotomy(\"not.work\", \"parttime\"))\n\n# How set1 response variables would be coded in base-R\nWomenlf &lt;- within(Womenlf, {\n  work = ifelse(partic == \"not.work\", 0, 1)\n  full = ifelse(partic == \"fulltime\",  1,\n                ifelse(partic == \"parttime\", 0, NA))\n})\n\nset1:\n\n“work”: Women who are NOT working outside the home vs. those who are working (either part-time or full-time).\n“full”: Women who work full-time time vs. part-time, but among only those who work.\n\nset2:\n\n“full”: {full-time} vs. {not working, part-time}\n\nThe rationale is that the real hurdle for young married women to enter the paid labor force is to combine full-time work outside the home with housework.\n\n“part”: {not working} vs. {part-time}.\n\n\nFit 1st Set\nwlf.nested.one &lt;- \n    nestedLogit(partic ~ hincome + children,\n                dichotomies = set1,\n                data = Womenlf)\n\nFits 2 glm models\n\nglm(formula = work ~ hincome + children, family = binomial, data = Womenlf)\nglm(formula = full ~ hincome + children, family = binomial, data = Womenlf)\n\nA combined model is also produced using the Delta Method\nEstimates are calculated for all models and predicted probabilities for the category levels are from the combined model.\n\nResults\n# show as odds ratios\nexp(coef(wlf.nested))\n#&gt;                   work     full\n#&gt; (Intercept)     3.8032 32.38753\n#&gt; hincome         0.9586  0.89829\n#&gt; childrenpresent 0.2069  0.07055\n\n\\(\\hat{\\beta}_{\\text{work, hincome}}\\) gives the estimated change in the log-odds of working vs. not working associated with a $1,000 increase in husband’s income and with having children present, each holding the other constant.\n\\(\\hat{\\beta}_{\\text{full, hincome}}\\) same as above, but gives the estimated change the log-odds of working full-time vs. part-time among those who are working outside the home.\nInterpretation (after exponentiation): The odds of both working and working full-time decrease with husband’s income, by about 4% and 10% respectively per $1000. Having young children also decreases the odds of both working and working full-time, by about 79% and 93% respectively.\n\nFit 2nd Set\nwlf.nested.two &lt;- \n  nestedLogit(partic ~ hincome + children,\n              dictotomies = set2,\n              data=Womenlf)\nResults\nsummary(wlf.nested.two)\n\n# full\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)       1.7696     0.4690    3.77  0.00016 ***\n#&gt; hincome          -0.0987     0.0277   -3.57  0.00036 ***\n#&gt; childrenpresent  -2.5631     0.3489   -7.35    2e-13 ***\n\n# part\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)  \n#&gt; (Intercept)     -1.42758    0.58222   -2.45    0.014 *\n#&gt; hincome          0.00687    0.02343    0.29    0.769  \n#&gt; childrenpresent  0.01629    0.46762    0.03    0.972  \n\nsummary also shows all the typical model summary info, but only for each dichotomy and not the combined model.\nAnova will perform Chi-Square tests on each explanatory variable in each dichotomy and the combined model.\nThe predictors husband’s income and presence of children affect the decision to work full-time, but not the decision to work part-time among those who aren’t engaged in full-time work. (See “Set-up sets of dichotomies” section describing this set’s rationale)\n\nCompare Models\nfit1 &lt;- predict(wlf.nested.one)$p\nfit2 &lt;- predict(wlf.nested.two)$p\n\ndiag(cor(fit1, fit2))\n#&gt; not.work parttime fulltime \n#&gt;   0.9801   0.9185   0.9961\n\nmean(as.matrix(abs(fit1 - fit2)))\n#&gt; [1] 0.01712\n\nmax(abs(fit1 - fit2))\n#&gt; [1] 0.1484\n\n# GoF\nlogLik(wlf.nested.one)\n#&gt; 'log Lik.' -212.1 (df=6)\nlogLik(wlf.nested.two)\n#&gt; 'log Lik.' -211.4 (df=6)\nAIC(wlf.nested.one, wlf.nested.two)\n#&gt;                df   AIC\n#&gt; wlf.nested      6 436.2\n#&gt; wlf.nested.alt  6 434.9\n\nThe fitted probabilities are similar but far from the same. (See next section for what “same” looks like)\nSince these models aren’t nested within each other, you can’t compare with an LR test (i.e. use a p-value), but you can still look at the Loglikelihoods.\nThe GoF comparison slightly favors the alternative nested-dichotomies model (set2).\n\nCompare Model 2 with Multinomial Logit\nwlf.multinom &lt;- nnet::multinom(partic ~ hincome + children, data = Womenlf)\n\nsummary(wlf.multinom)\n#&gt; Coefficients:\n#&gt;          (Intercept)   hincome childrenpresent\n#&gt; parttime      -1.432  0.006894         0.02146\n#&gt; fulltime       1.983 -0.097232        -2.55861\n#&gt;  AIC: 434.9\n\nlogLik(wlf.multinom)\n#&gt; 'log Lik.' -211.4 (df=6)\n\nfit3 &lt;- predict(wlf.multinom, type=\"probs\")[, c(\"not.work\", \"parttime\", \"fulltime\")]\ndiag(cor(fit2, fit3))\n#&gt; not.work parttime fulltime \n#&gt;        1        1        1\nmean(as.matrix(abs(fit2 - fit3)))\n#&gt; [1] 0.0001251\nmax(abs(fit2 - fit3))\n#&gt; [1] 0.0006908 \n\nThe multinomial logit model and the alternative nested-dichotomies (i.e. set2) model produce nearly identical fits with similar simple interpretations.",
    "crumbs": [
      "Regression",
      "Multinomial"
    ]
  },
  {
    "objectID": "qmd/regression-multinomial.html#sec-reg-multin-mixlog",
    "href": "qmd/regression-multinomial.html#sec-reg-multin-mixlog",
    "title": "Multinomial",
    "section": "Mixed Logit",
    "text": "Mixed Logit\n\nIndividual heterogeneity can be introduced in the parameters associated with the covariates entering the observable part of the utility or in the variance of the errors.\n\nModel creates a distribution of \\(\\beta\\)s, so \\(\\beta\\) is allowed to vary throughout a population.\n\nThe \\(\\theta\\) distribution parameters are what get estimated by the model in order to calculate a distribution of \\(\\beta\\)s (i.e. Bayesian posterior)\n\nDoesn’t exhibit IIA (See Econometrics, Discrete Choice Models &gt;&gt; Multinomial Logit &gt;&gt; Substitution Patterns) since correlations between alternatives are modeled.\n\nRandom Coefficients\n\\[\nU_{nj} = \\hat{\\alpha}\\boldsymbol{x}_{nj} + \\hat{\\mu}\\boldsymbol{z}_{nj} + \\epsilon_{nj}\n\\]\n\n\\(x_{nj}\\), \\(z_{nj}\\) - data for alternative j and decision maker n\n\\(\\hat{\\alpha}\\) - vector of fixed coefficients (i.e. same for all decision makers)\n\\(\\hat{\\mu}_n\\) - vector of random coefficients (i.e. a coefficient for each decision maker)\n\\(\\epsilon_{nj}\\) - residual from an extreme value distribution (e.g. Gumbel)\nCorrelated Random Utility\n\nLet \\(\\nu_{nj} = \\boldsymbol{\\hat{\\mu}}_n \\boldsymbol{z}_{nj} + \\epsilon_{nj}\\) be the random (unobserved) component of utility and \\(\\mbox{Cov}(\\nu_{ni}, \\nu_{nj}) = \\boldsymbol{z}_{ni} \\Sigma \\boldsymbol{z}_{nj}\\) the covariance between random utilities of different alternatives where \\(\\Sigma\\) is the variance/covariance matrix for \\(\\boldsymbol{\\hat{\\mu}}\\)\nThis is a structure to model the correlation between alternatives\n\n\nPanel Data\n\nData where each decision maker makes multiple choices over time periods\nModel allows for unobserved preference variation through random coefficients, which yields correlations in utility over time for the same decision maker\nDecision maker, n, chooses from a vector of alternatives over T time periods\nLag or lead predictors can be included\nLagged responses can be included\n\nUse cases:\n\nHabit Formation\nVariety-Seeking Behavior\nSwitching Costs\nBrand Loyalty\n\n\nOnly models a sequence of static choices\n\nLagged responses account for past choices affecting current choices, but not how current choices affect future choices\nA fully dynamic discrete choice model models how every choice affects subsequent choices.\n\n\nExample: {mlogit}\n\n“rpar” specifies the random coefficients",
    "crumbs": [
      "Regression",
      "Multinomial"
    ]
  },
  {
    "objectID": "qmd/regression-multinomial.html#multinomial-probit",
    "href": "qmd/regression-multinomial.html#multinomial-probit",
    "title": "Multinomial",
    "section": "Multinomial Probit",
    "text": "Multinomial Probit\n\nAssumes a Normal distribution of errors which can deal with heteroscedasticity and correlation of the errors.",
    "crumbs": [
      "Regression",
      "Multinomial"
    ]
  },
  {
    "objectID": "qmd/regression-other.html",
    "href": "qmd/regression-other.html",
    "title": "42  Other",
    "section": "",
    "text": "42.1 Misc",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-misc",
    "href": "qmd/regression-other.html#sec-reg-other-misc",
    "title": "42  Other",
    "section": "",
    "text": "Harrell: It is not appropriate to compute a mean or run parametric regression on “% change” unless you first compute log((%_change/100) + 1)  to undo damage done by % change\nReaction time data can be modelled using several families of skewed distributions (Lindeløv, 2019)",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-other",
    "href": "qmd/regression-other.html#sec-reg-other-other",
    "title": "42  Other",
    "section": "42.2 Other",
    "text": "42.2 Other\n\nRates between 0 and 1\n\nOutcome without 0s and 1s\n\nBeta Regression\n\nOutcome with 0s and 1s\n\nOrdered Beta Regression\n\npaper, {ordbetareg}, {glmmTMB}\nHeiss overview of the model\nHeiss example using a 0,1 Inf Ordered Beta Regression\n\nZero-inflated or Zero-1-inflated beta regression (ZOIB) (see {brms})\nIf the zeros and ones are censored, use tobit.\n\n\nOutcome variable is greater than 0\n\nGamma Regression - Can handle some dispersion with a log link\nCan model multiplicative dgp\nIf zero-inflated, use Tweedie Regression\n\nBounded Outcome Variable\n\nOrdered Beta Regression\n\npaper, {ordbetareg}, {glmmTMB}\nHeiss overview of the model\nHeiss example using a 0,1 Inf Ordered Beta Regression to model an outcome with values between 1 and 32\n\n\nTweedie Regression - Where the frequency of events follows a Poisson distrbution and the amount associated with each event follows an Exponential distribution\n\ne.g. Insurance claims, Operational loss (banking)\nTweedie distribution is a Gamma distribution with a spike at zero.\n\nGeneralized Least Squares\n\nPackages\n\n{nlme::gls}\n\nMath - A Deep-Dive into Generalized Least Squares Estimation\nAlso See Weighted Least Squares and Weighted Least Squares &gt;&gt; Feasible Generalized Least Squares",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-censtrunc",
    "href": "qmd/regression-other.html#sec-reg-other-censtrunc",
    "title": "42  Other",
    "section": "42.3 Censored and Truncated Data",
    "text": "42.3 Censored and Truncated Data\n\nCensored Data - Arise if exact values are only reported in a restricted range. Data may fall outside this range but those data are reported at the range limits (i.e. at the minimum or maximum of the range)\n\ni.e. instances outside the range are recorded in the data but the true values of those instances are not.\nTobit Regression - gaussian response, assumes constant (homoskedastic variance)\n\nLog-Likelihood function\n\n\ndᵢ = 0 if yi = 0, and 1 otherwise\n\n1st term (top line) is OLS likelihood\n2nd term (bottom line) accounts for the probability that observation i is censored.\n\n\nMarginal Effect\n\n\nβ₁ is multiplied by the CDF (Φ) of the Normal distribution\n\nβ₁ is weighted by the probability of y occurring at x\nExample: work_completed ~ hourly rate\n\nβ₁ is weighted by the probability that an individual is willing to work at the present hourly rate, as represented by the CDF.\n\n\nσ is the standard deviation of the model’s residuals\n\nExample: Right-Censored at 800\nVGAM::vglm(resp ~ pred1 + pred2, \n           family = tobit(Upper = 800), \n           data = dat)\n\n2-Part Models (e.g. Hurdle Models) - A binary (e.g. Probit) regression model fits the exceedance probability of the lower limit and a truncated regression model fits the value given the lower limit is exceeded.\n\nTruncated Data - Arise if exact values are only reported in a restricted range. If data outside this range are omitted completely, we call it truncated\n\ni.e. instances outside the range are NOT recorded. No evidence of these instances are in the data.\nTruncated Regression - Also assumes constant (homoskedastic variance)\nA poisson model will try to predict 0s even if 0s are impossible. Therefore, you need a zero-truncated model.\nThe truncated normal model is different from a glm, because μ and σ are not orthogonal and have to be estimated simultaneously. Misspecification of one parameter will lead to inconsistent estimation of the other. That’s why for these models, not only is μ often specified as a function of regressors but also σ, often in the framework of GAMLSS (generalized additive models of location, scale, and shape).\n\nExpectation: \\(E[y|x] = \\mu + \\sigma + \\frac {\\phi(\\mu / \\sigma)}{\\Phi(\\mu / \\sigma)}\\)\n\nWhere ϕ(⋅) and Φ(⋅) are the probability density and cumulative distribution function of the standard normal distribution, respectively. This intrinsically depends on both μ and σ.\n\n\n\nHeteroskadastic Variance - The variance of an underlying normal distribution does depend on covariates\n\n{crch}\n\nExamples\n\nInsurance: There is a claim on a policy that has a payout limit of u and a deductible of d,\n\nAny loss amount that is greater than u will be reported to the insurance company as a loss of u - d  because that is the amount the insurance company has to pay.\nInsurance loss data is left-truncated because the insurance company doesn’t know if there are values below the deductible d because policyholders won’t make a claim.\n\n“truncated” because the values (claims) are below d, so the instances aren’t recorded in the data.\n“left” because the values are below d and not above\n\nInsurance loss data is also right-censored if the loss is greater than u because u is the most the insurance company will pay. Thus, it only knows that your claim is greater than u, not the exact claim amount.\n\n“censored” because the values (claims) that are exactly u (policy limit) imply that claim is greater than u, so the instances are recorded but the true values are unknown.\n“right” because the values are above u and not below\n\n\nMeasuring Wind Speed: The instrument needs a minimum wind speed, m, to start working.\n\nIf wind speeds below this minimum are recorded as the minimum value, m, the data is censored.\n\ni.e. some other instrument detects a wind instance occurred and that instance is recorded as m even though the true speed of the wind of that instance is unknown.\n\nIf wind speeds below this minimum are NOT recorded at all, the data is truncated.\n\ni.e. any wind instances (detected or not) are not recorded. No evidence in the data that these instances will have ever occurred.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-fracreg",
    "href": "qmd/regression-other.html#sec-reg-other-fracreg",
    "title": "42  Other",
    "section": "42.4 Fractional Regression",
    "text": "42.4 Fractional Regression\n\nOutcome with values between 0 and 1\n** Use a fractional logit (aka quasi-binomial) only for big data situations **\n\nThe fractional logit model is not a statistical distribution, leading it to produce biased results.\nSee Kubinec article\n\nRecommends ordered beta regression, continuous bernoulli and provides examples\n\nIn a big data situation, it respects the bounds of proporitional/fractional outcomes, and is significantly easier to fit than the other alternatives.\nHaving a large dataset means that inefficiency or an incorrect form for the uncertainty of fractional logit estimates is unlikely to affect decision-making or inference.\n\nBeta: values lie between zero and one\n\nsee {betareg}, {DirichletReg}, {mgcv}, {brms}\n{ordbetareg}\n\nZero/One-Inflated Beta: larger percentage of the observations are at the boundaries (i.e. high amounts of 0s and 1s\n\nSee {brms}, {VGAM}, {gamlss}\n\nLogistic, Quasi-Binomial, or GAM w/robust std.errors: outcome, y, is 0 ≤ y ≤ 1 (i.e. 0s and 1s included)\n\nExample\nlibrary(lmtest)\nlibrary(sandwich)\n# logistic w/robust std.errors\nmodel_glm = glm(\n  prate ~ mrate + ltotemp + age + sole,\n  data = d,\n  family = binomial\n)\nse_glm_robust &lt;- coeftest(model_glm, vcov = vcovHC(model_glm, type=\"HC\"))\n# quasi-binomial w/robust std.errors\nmodel_quasi = glm(\n  prate ~ mrate + ltotemp + age + sole,\n  data = d,\n  family = quasibinomial\n)\nse_glm_robust_quasi = coeftest(model_quasi, vcov = vcovHC(model_quasi, type=\"HC\"))\n\n# Can also use a GAM to get the same results\n# Useful for more complicated model specifications\nmodel_gam_re = gam(\n  prate ~ mrate + ltotemp + age + sole + s(id, bs = 're'),\n  data = d,\n  family = binomial,\n  method = 'REML'\n)",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-zeroinftrunc",
    "href": "qmd/regression-other.html#sec-reg-other-zeroinftrunc",
    "title": "42  Other",
    "section": "42.5 Zero-Inflated and Zero-Truncated",
    "text": "42.5 Zero-Inflated and Zero-Truncated\n\nContinuous\n\nSome economists will use log(1 + Y) or arcsinh(Y) to model a skewed, continous Y with 0s. In this case, the treatment effects (ATE) can’t be interpreted as percents. The effect sizes will depend on the scale of Y. (see Thread)\nSolutions\n\nNormalize Y by a pretreatment baseline\n\nÝ = Y / Ypre\n\nwhere Ypre is the measured Y prior to treatment\n\nIn regression, average treatment effect (ATE) would then be\n\nWhere Y(1) is the value of Y for treated subjects\nInterpretation(e.g outcome = earnings): average treatment effect on earnings expressed as a percentage of pre-treatment earnings\n\n\nNormalizing Y by the expected outcome given observable covariates\n\nÝ = Y / E[Y(0) | X]\n\nY(0) are the observed outcome values for the control group\nThe “|X” is kind of confusing but I don’t think want the fitted values from a model where the outcome is the Y(0) values. I think they’d use a Y^ somewhere.\n\nSo I think E[Y(0) | X] just the mean of the Y(0) values\n\nInterpretation of this transformed variable (e.g. outcome = earnings)\n\nan individual’s earnings as a percentage of the average control group’s earnings for people with the same observable characteristics X.\n\n\nAverage Treatment Effect (ATE) Interpretation (e.g. outcome = earnings, X = pretreatment earnings, education)\n\nThe average change in earnings as a percentage of the control group’s earnings for people with the same education and previous earnings.\n\n\n\n\nML 2-step Hurdle\n\nSteps\n\nTransform Target variable to 0/1 where 1 is any count that isn’t a 0.\nUse a classifier to predict 0s (according to some probability threshold)\n\nRemember that models is predicting the probability of being a 1\n\nFilter rows that aren’t predicted to be 0, and predict counts using a regressor model\n\nround-up or round-down predictions based on which results in lower error?\n\n\n\nStatistical\n\nOptions: Poission, Neg.Binomial, Zero-Inf Poisson/Neg.Binomial, Poisson/Neg.Binomial Hurdle\n\nZero-Inf Poisson/Neg.Binomial\n\n\nUses a second underlying process that determines whether a count is zero or non-zero. Once a count is determined to be non-zero, the regular Poisson process takes over to determine its actual non-zero value based on the Poisson process’s PMF.\nϕi is the predicted probability from a logistic regression that yi is a 0. This vector of values is then plugged into both probability mass functions.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-multmod",
    "href": "qmd/regression-other.html#sec-reg-other-multmod",
    "title": "42  Other",
    "section": "42.6 Multi-modal",
    "text": "42.6 Multi-modal\n\nAlso see EDA &gt;&gt; Interactions &gt;&gt; Categorical Outcome\n{upsetr} - Might be useful to examine bimodal structure and determine cutpoints based on categorical predictor values and not just outcome values\n{gghdr} - Visualization of Highest Density Regions in ggplot2\nTest for more than one mode: multimode::modetest(dat)\n\nPerforms Ameijeiras-Alonso excess mass test/dip statistic\nHa: More than 1 mode\n\nFind location of modes: multimode::locmodes(dat, mod0 = 2, display = TRUE)\n\n\nAnti-Mode location might be a good place for a cutpoint\n\nIdeas\n\nQuantile Regression\nMixture Model\nEstablish cutpoints and model each modal distribution separately\nML",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-wls",
    "href": "qmd/regression-other.html#sec-reg-other-wls",
    "title": "42  Other",
    "section": "42.7 Weighted Least Squares (WLS)",
    "text": "42.7 Weighted Least Squares (WLS)\n\nOLS with Weighted Pbservations\nCommonly used to overcome binomial or “megaphone-shaped” types of heteroskedacity of OLS residuals\n\n\nMisc\n\nAlso see\n\nOther &gt;&gt; Generalized Least Squares\nReal Estate &gt;&gt; Appraisal Methods &gt;&gt; CMA &gt;&gt; Market Price &gt;&gt; Case-Shiller Method for an example\n\nResources\n\nR &gt;&gt; Documents &gt;&gt; Econometrics &gt;&gt; applied-econometrics-in-r-zeileis-kleiber &gt;&gt; pg 76\n\nFeasible Generalized Least Squares (FGLS) seems to have advantages over WLS Allows you to find the “form of the skedastic function to use and… estimate it from the data” * See Zeileis applied econometrics book or another econometrics book for details\n\nThe residual error to be minimized becomes:\n\n\nWhere wi is the weight assigned to observation i\n\n\\(\\hat \\beta\\) becomes\n\n\nWhere W is a diagonal matrix containing the weights for each observation\n\nFor “megaphone-shaped” and binomial types of heteroskedacity, it’s common to set the weights to equal to each observation’s squared residual error",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-gee",
    "href": "qmd/regression-other.html#sec-reg-other-gee",
    "title": "42  Other",
    "section": "42.8 Generalized Estimating Equations (GEE)",
    "text": "42.8 Generalized Estimating Equations (GEE)\n\nModels that are used when individual observations are correlated within groups. Often used when repeated measures (panel data) for an individual are collected over time.\n\nYou make a good guess on the within-subject covariance structure. The model averages over all subjects, and instead of assuming that data were generated from a certain distribution, it uses moment assumptions to iteratively choose the best β to describe the relationship between covariates and response.\nA semiparametric method: while we impose some structure on the data generating process (linearity), we do not fully specify its distribution. Estimating β is purely an exercise in optimization.\nLimitations\n\nLikelihood-based methods are not available for usual statistical inference. GEE is a quasi-likelihood method.\nUnclear on how to perform model selection, as GEE is just an estimating procedure. There is no goodness-of-fit measure readily available.\nNo subject-specific estimates; if that is the goal of your study, use a different method. (see below)\n\nOther option is Generalized Linear Mixed Model (GLMM), but GLMMs require some parametric assumptions (See Econometrics, Mixed Effects)\n\n*Note that the interpretations of the resulting estimates are different for GLMM and GEE*\n\nScenarios\n\nYou are a doctor. You want to know how much a statin drug will lower your patient’s odds of getting a heart attack.\n\nGLMM answers this question\n\nYou are a state health official. You want to know how the number of people who die of heart attacks would change if everyone in the at-risk population took the stain drug.\n\nGEE answers this question. GEE estimates population-averaged model parameters and their standard errors\n\n\n\nMisc\n\nNotes from\n\nGeneralized Estimating Equations (GEE)\n\nPackages\n\n{gee}: traditional implementations (only has a manual)\n{geepack}: traditional implementations (1 vignette)\n\nCan also handle clustered categorical responses\n\n{multgee}: GEE solver for correlated nominal or ordinal multinomial responses using a local odds ratios parameterization\n\nThe traditional GEE implementation has severe computation challenges and may not be possible when the cluster sizes (large numbers of individuals per cluster) get too large (e.g. &gt;1000)\n\nUse One-Step Generalized Estimating Equations method (article with code)\n\nOperates under the assumption of exchangeable correlation (see below)\nCharacteristics\n\nMatches the asymptotic efficiency of the fully iterated GEE;\nUses a simpler formula to estimate the [intra-cluster correlation] ICC that avoids summing over all pairs;\nCompletely avoids matrix multiplications and inversions for computational efficiency\n\n\n\n\nAssumptions (similar to the assumptions for GLMs)\n\nThe responses Y1, Y2, … , Yn are correlated or clustered\nThere is a linear relationship between the covariates and a transformation of the response, described by the link function, g.\nWithin-cluster covariance has some structure (“working covariance”)\nIndividuals in different clusters are uncorrelated\n\nCovariance Structure\n\nNeed to pick one of these working covariance structures in order to fit the GEE\nTypes\n\nIndependence: observations over time are independent)\nExchangeable (aka Compound Symmetry): all observations over time have the same correlation\n\nCorrelation across individuals is constant within a cluster\nIntra-Cluster Correlation (ICC) is the measure of this correlation.\n\nAR(1): correlation decreases as a power of how many timepoints apart two observations are\n\nReasable if measurements taken closer together (i.e. probably more highly correlated)\n\nUnstructured: correlation between all timepoints may be different)\n\nIf the wrong covariance structure is chosen, β will be estimated consistently, even if the working covariance structure is wrong. However, the standard errors computed from this will be wrong.\n\nTo fix this, use Huber-White “sandwich estimator” (HC standard errors) for robustness. (See Econometrics, General &gt;&gt; Standard Errors)\n\nThe idea behind the sandwich variance estimator is to use the empirical residuals to approximate the underlying covariance.\nProblematic if:\n\nThe number of independent subjects is much smaller than the number of repeated measures\nThe design is unbalanced – the number of repeated measures differs across individuals\n\n\n\n\nExample: geepack\n\nDescription: How does Vitamin E and copper level in the feeds affect the weights of pigs?\nData\nlibrary(\"geepack\")\ndata(dietox)\ndietox$Cu &lt;- as.factor(dietox$Cu)\ndietox$Evit &lt;- as.factor(dietox$Evit)\nhead(dietox)\n##     Weight      Feed Time  Pig Evit Cu Litter\n## 1 26.50000        NA    1 4601    1  1      1\n## 2 27.59999  5.200005    2 4601    1  1      1\n## 3 36.50000 17.600000    3 4601    1  1      1\n## 4 40.29999 28.500000    4 4601    1  1      1\n## 5 49.09998 45.200001    5 4601    1  1      1\n## 6 55.39999 56.900002    6 4601    1  1      1\n\nWeight of slaughter pigs measured weekly for 12 weeks\nStarting weight (i.e. the weight at week (Time) 1 and Feed = NA)\nCumulated Feed Intake (Feed)\nEvit is an indicator of Vitamin E treatment\nCu is an indicator of Copper treatment\n\nModel: Independence Working Covariance Structure\nmf &lt;- formula(Weight ~ Time + Evit + Cu)\ngeeInd &lt;- geeglm(mf, id=Pig, data=dietox, family=gaussian, corstr=\"ind\")\nsummary(geeInd)\n\n##  Coefficients:\n##            Estimate  Std.err    Wald Pr(&gt;|W|)   \n## (Intercept) 15.07283  1.42190  112.371  &lt;2e-16 ***\n## Time        6.94829  0.07979 7582.549  &lt;2e-16 ***\n## Evit2        2.08126  1.84178    1.277    0.258   \n## Evit3      -1.11327  1.84830    0.363    0.547   \n## Cu2        -0.78865  1.53486    0.264    0.607   \n## Cu3          1.77672  1.82134    0.952    0.329   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Estimated Scale Parameters:\n##            Estimate Std.err\n## (Intercept)    48.28  9.309\n## \n## Correlation: Structure = independenceNumber of clusters:  72  Maximum cluster size: 12\n\ncorstr=“ind” is the argument for the covariance structure - See article for examples of the other structures and how they affect estimates\n\nANOVA\nanova(geeInd)\n\n## Analysis of 'Wald statistic' Table\n## Model: gaussian, link: identity\n## Response: Weight\n## Terms added sequentially (first to last)\n## \n##      Df  X2 P(&gt;|Chi|)   \n## Time  1 7507    &lt;2e-16 ***\n## Evit  2    4      0.15   \n## Cu    2    2      0.41   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-quantile.html",
    "href": "qmd/regression-quantile.html",
    "title": "Quantile",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Regression",
      "Quantile"
    ]
  },
  {
    "objectID": "qmd/regression-quantile.html#sec-reg-quant-misc",
    "href": "qmd/regression-quantile.html#sec-reg-quant-misc",
    "title": "Quantile",
    "section": "",
    "text": "Resources\n\nHandbook of Quantile Regression - Koenker ({quantreg} book) (see R &gt;&gt; Documents &gt;&gt; Regression)\n\nPackages\n\n{quantregRanger} - uses Ranger to fit quantile RFs\n\nIn {tidymodels}, quantreg = TRUE tells ranger that you’re estimating quantiles rather than averages. Also predict(airquality, type = 'quantiles')\n\n{grf} - generalized random forest\n{quantreg} - Estimation and inference methods for models for conditional quantile functions: Linear and nonlinear parametric and non-parametric (total variation penalized) models for conditional quantiles of a univariate response.\n{partykit} - conditional inference trees; model-based recursive partitioning trees\n\n{bonsai}: tidymodels partykit conditional trees, forests; successor to treesnip - Model Wrappers for Tree-Based Models\n\n{{quantile-forest}} - Zillow’s sklearn compatible quantile forest. Compared to other python implementations, optimized for training and inference speed, enabling it to scale to millions of samples with a runtime that is orders of magnitude faster than less-optimized solutions. It also allows specifying prediction quantiles after training, permitting a trained model to be reused to estimate conditional quantiles as needed.\n\nOut-of-Bag Scoring: OOB scoring can be used to obtain unbiased estimates of prediction errors and quantile-specific metrics without the need for additional validation datasets.\nQuantile Rank Calculation: Provide a measure of relative standing for each data point in the distribution. Allows you to compare and rank observations based on their position within the quantile distribution, providing valuable insights for various applications, such as risk assessment and anomaly detection.\nProximity and Similarity Estimation: Quantifies the similarity between pairs of observations based on their paths through the forest. Useful for clustering, anomaly detection, and identifying influential observations.\n\n{{skgarden}} - Extension for sklearn tree and forest models. Produces online training models called Mondrian Forests (paper). Has a quantile random forest flavor.\n\nFor quantiles &gt; 0.80, see quantile models in Extreme Value Theory (EVT))\nHarrell: To characterize an entire distribution or in other words, have a “high degree of confidence that no estimated quantile will be off by more than a probability of 0.01, n = 18,400 will achieve this.\n\nFor example with n = 18,400, the sample 0.25 quantile (first quartile) may correspond to population quantiles 0.24-0.26.\nTo achieve a $$0.1 MOE requires n = 180, and to have $$0.05 requires n = 730 (see table)\n       n   MOE\n1     20 0.294\n2     50 0.188\n3    100 0.134\n4    180 0.100\n5    250 0.085\n6    500 0.060\n7    730 0.050\n8    750 0.049\n9   1000 0.043\n10  2935 0.025\n11  5000 0.019\n12 10000 0.014\n13 18400 0.010\n\nHarrell has a pretty cool text effect to display quantile values in his {HMisc::describe} that uses {gt} under the hood (See EDA &gt;&gt; Packages &gt;&gt; HMisc)\n\n\nHistogram is a sparkline",
    "crumbs": [
      "Regression",
      "Quantile"
    ]
  },
  {
    "objectID": "qmd/regression-quantile.html#sec-reg-quant-diag",
    "href": "qmd/regression-quantile.html#sec-reg-quant-diag",
    "title": "Quantile",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nMean Integrated Squared Error (MISE)",
    "crumbs": [
      "Regression",
      "Quantile"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html",
    "href": "qmd/regression-regularized.html",
    "title": "Regularized",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-misc",
    "href": "qmd/regression-regularized.html#sec-reg-reg-misc",
    "title": "Regularized",
    "section": "",
    "text": "Regularized Logistic Regression is most necessary when the number of candidate predictors is large in relationship to the effective sample size 3np(1−p) where p is the proportion of Y=1 Harrell\nIf using sparse matrix, then you don’t need to normalize predictors\nPreprocessing\n\nStandardize numerics\nDummy or factor categoricals\nRemove NAs, na.omit\n\nPackages\n\n{glmnet} - handles families: Gaussian, binomial, Poisson, probit, quasi-poisson, and negative binomial GLMs, along with a few other special cases: the Cox model, multinomial regression, and multi-response Gaussian.\n{robustHD}: Robust methods for high-dimensional data, in particular linear model selection techniques based on least angle regression and sparse regression\nIn {{sklearn}} (see Model building, sklearn &gt;&gt; Algorithms &gt;&gt; Stochaistic Gradient Descent (SGD)), the hyperparameters are different than in R\n\nlambda (R) is alpha (py)\nalpha (R) is 1 - L1_ratio (py)\n\n{SLOPE} pkg - lasso regression that handles correlated predictors by clustering them\n\nVariable Selection\n\nFor Inference, only Adaptive LASSO is capable of handling block and time series dependence structures in data\n\nSee A Critical Review of LASSO and Its Derivatives for Variable Selection Under Dependence Among Covariates\n\n“We found that one version of the adaptive LASSO of Zou (2006) (AdapL.1se) and the distance correlation algorithm of Febrero-Bande et al. (2019) (DC.VS) are the only ones quite competent in all these scenarios, regarding to different types of dependence.”\nThere’s a deeper description of the model in the supplemental materials of the paper. I think the “.1se” means it’s using the lambda.1se from cv.\n\nlambda.1se : largest value of λ such that error is within 1 standard error of the cross-validated errors for lambda.min.\n\nsee lambda.min, lambda.1se and Cross Validation in Lasso : Binomial Response for code to access this value.\n\n\n\nRe the distance correlation algorithm (it’s a feature selection alg used in this paper as benchmark vs LASSO variants)\n\n“the distance correlation algorithm for variable selection (DC.VS) of Febrero-Bande et al. (2019). This makes use of the correlation distance (Székely et al., 2007; Szekely & Rizzo, 2017) to implement an iterative procedure (forward) deciding in each step which covariate enters the regression model.”\nStarting from the null model, the distance correlation function, dcor.xy, in {fda.usc} is used to choose the next covariate\n\nguessing you want large distances and not sure what the stopping criteria is\n\nalgorithm discussed in this paper, Variable selection in Functional Additive Regression Models\n\nHarrell is skeptical. “I’d be surprised if the probability that adaptive lasso selects the”right” variables is more than 0.1 for N &lt; 500,000.”",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-conc",
    "href": "qmd/regression-regularized.html#sec-reg-reg-conc",
    "title": "Regularized",
    "section": "Concepts",
    "text": "Concepts\n\nShrinking effect estimates turns out to always be best\n\nOLS is the Best Linear Unbiased Estimator (BLUE), but being unbiased means the variance of the estimated effects is large from sample to sample and therefore outcome variable predictions using OLS don’t generalize well.\nIf you predicted y using the sample mean times some coefficient, it’s always(?) the case that you’ll have a better generalization error with a coefficient less than 1 (shrinkage).\n\nRegularized Regression vs OLS\n\nAs N ↑, standard errors ↓\n\nregularized regression and OLS regression produce similar predictions and coefficient estimates.\n\nAs the number of covariates ↑ (relative to the sample size), variance of estimates ↑\n\nregularized regression and OLS regression produce much different predictions and coefficient estimates\nTherefore OLS predictions are usually fine in a low dimension world (not usually the case)",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-ridge",
    "href": "qmd/regression-regularized.html#sec-reg-reg-ridge",
    "title": "Regularized",
    "section": "Ridge",
    "text": "Ridge\n\nThe regularization reduces the influence of correlated variables on the model because the weight is shared between the two predictive variables, so neither alone would have strong weights. This is unlike Lasso which just drops one of the variables (which one gets dropped isn’t consistent).\nLinear transformations in the design matrix will affect the predictions made by ridge regression.",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-lasso",
    "href": "qmd/regression-regularized.html#sec-reg-reg-lasso",
    "title": "Regularized",
    "section": "Lasso",
    "text": "Lasso\n\nWhen lasso drops a variable, it doesn’t mean that the variable wasn’t important.\n\nThe variable, x1, could’ve been correlated with another variable, x2, and lasso happens to drop x1 because in this sample, x2, predicted the outcome just a tad better.\n\n\n\nAdaptive LASSO\n\n\nPurple dot indicates that it’s a weighted (wj) version of LASSO\nGreen checkmark indicates it’s optimization is a convex problem\nBetter Selection, Bias Reduction are attributes that it has that are better than standard LASSO\nWeighted versions of the LASSO attach the particular importance of each covariate for a suitable selection of the weights. Joint with iteration, this modification allows for a reduction of the bias.\n\nZhou (2006) say that you should choose your weights so the adaptive Lasso estimates have the Oracle Property:\n\nYou will always identify the set of nonzero coefficients…when the sample size is infinite\nThe estimates are unbiased, normally distributed, and the correct variance (Zhou (2006) has the technical definition)…when the sample size is infinite.\n\nTo have these properties, wj = 1 / |βj_hat|γ, where γ &gt; 0 and βj_hat is an unbiased estimate of the true parameter, β\n\nGenerally, people choose the Ordinary Least Squares (OLS) estimate of β because it will be unbiased. Ridge regression produces coefficient estimates that are biased, so you cannot guarantee the Oracle Property holds.\n\nIn practice, this probably doesn’t matter. The Oracle Property is an asymptotic guarantee (when n→∞), so it doesn’t necessary apply to your data with a finite number of observations. There may be scenarios where using Ridge estimates for weights performs really well. Zhou (2006) recommends using Ridge regression over OLS when your variables are highly correlated.\n\n\n\nSee Adaptive LASSO for examples with a continuous, binary, and multinomial outcome",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-firth",
    "href": "qmd/regression-regularized.html#sec-reg-reg-firth",
    "title": "Regularized",
    "section": "Firth’s Estimator",
    "text": "Firth’s Estimator\n\nPenalized Logistic Regression estimator\nFor sample sizes less than around n = 1000 or sparse data, using Firth Estimator is recommended\nMisc\n\nNotes from\n\nThread\n\nPackages\n\n{brglm2} -\n{logistf} - Includes FLIC and FLAC extensions; uses profile penalized likelihood confidence intervals which outperform Wald intervals; includes a function that performs a penalized likelihood ratio test on some (or all) selected factors\n\nemmeans::emmeans is supported\n\n\nInvariant to linear transformations of the design matrix (i.e. predictor variables) unlike Ridge Regression\nWhile the standard Firth correction leads to shrinkage in all parameters, including the intercept, and hence produces predictions which are biased towards 0.5, FLIC and FLAC are able to exclude the intercept from shrinkage while maintaining the desirable properties of the Firth correction and ensure that the sum of the predicted probabilities equals the number of events.\n\nPenalized Likelihood\n\\[\nL^*(\\beta\\;|\\;y) = L(\\beta\\;|\\;y)\\;|I(\\beta)|^{\\frac{1}{2}}\n\\]\n\nEquivalent to penalization of the log-likelihood by the Jeffreys prior\n\\(I(\\beta)\\) is the Fisher information matrix, i. e. minus the second derivative of the log likelihood\n\nMaximum Likelihood vs Firth’s Correction\n\nBias\n\nVariance\n\nCoefficient and CI bar comparison on a small dataset (n = 35, k = 7)\n\n\nLimitations\n\nRelies on maximum likelihood estimation, which can be sensitive to datasets with large random sampling variation. In such cases, Ridge Regression may be a better choice as it provides some shrinkage and can stabilize the estimates by pulling them towards the observed event rate.\nLess effective than ridge regression in datasets with highly correlated covariates\nFor the Firth Estimator, the Wald Test can perform poorly in data sets with extremely rare events.",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html",
    "href": "qmd/regression-survival.html",
    "title": "43  Survival",
    "section": "",
    "text": "43.1 Misc",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#misc",
    "href": "qmd/regression-survival.html#misc",
    "title": "43  Survival",
    "section": "",
    "text": "Model for estimating the time until a particular event occurs\n\ne.g. death of a patient being treated for a disease, failure of an engine part in a vehicle\n\nPrediction models for survival outcomes are important for clinicians who wish to estimate a patient’s risk (i.e. probability) of experiencing a future outcome. The term ‘survival’ outcome is used to indicate any prognostic or time-to-event outcome, such as death, progression, or recurrence of disease. Such risk estimates for future events can support shared decision making for interventions in high-risk patients, help manage the expectations of patients, or stratify patients by disease severity for inclusion in trials.1 For example, a prediction model for persistent pain after breast cancer surgery might be used to identify high risk patients for intervention studies\nOutcome variable: Time until event occurs\nPackages\n\nCRAN Task View\n{survival}\n{censored} - tidymodels package for censored and survival modelling\n{quantreg} - quantile survival regression\n{msm} - multi-state models\n\nVignette for {msm}\nSee Multistate Models for Medical Applications\n\nTutorial using a heart transplant dataset\n\nStandard survival models only directly model two states: alive and dead. Multi-state models enable directly modeling disease progression where patients are observed to be in various states of health or disease at random intervals, but for which, except for death, the times of entering or leaving states are unknown.\nMulti-state models easily accommodate interval censored intermediate states while making the usual assumption that death times are known but may be right censored.\n\n{grf} - generalized random forest; causal forest with time-to-event data\n{partykit} - conditional inference trees; model-based recursive partitioning trees; can be used with {survival} to create random survival forests\n\n{bonsai}: tidymodels partykit conditional trees, forests; successor to treesnip - Model Wrappers for Tree-Based Models\n\n{aorsf} - optimized software to fit, interpret, and make predictions with oblique random survival forests (ORSFs) {{sklearn}} - Random Survival Forests, Survival Support Vector Machine\n\nNotes from\n\nWhat is Cox’s proportional hazards model?\n\nWhy not use a standard regression model?\n\nUnits that “survive” until the end of the study will have a censored survival time.\n\ni.e. we won’t have an observed survival time for these units because they survive for an unknown time after the study is completed.\nWe don’t want to discard these units though, as they still have useful information.\n\n\nSample Size\nModels\n\nKaplan Meier model (i.e. K-M survival curve)\n\nOften used as a baseline in survival analysis\nCan not be used to compare risk between groups and compute metrics like the hazard ratio\n\nExponential model, the Weibull model, Cox Proportional-Hazards, Log-logistic and the Accelerated Failure Time (AFT)\nMulti-State Models\nHazard rates and Cumulative Hazard rates are typical quantities of interest\n\nLog-Rank Test (aka Mantel-Cox test) - tests if two groups survival curves are different\n\nnon-parametric; a special case with one binary X\nThe intuition behind the test is that if the two groups have different hazard rates, the two survival curves (so their slopes) will differ.\nCompares the observed number of events in each group to what would be expected if the survival curves were identical (i.e., if the null hypothesis were true).\nExample\nlibrary(survival)\ndat &lt;- data.frame(\n  group = c(rep(1, 6), rep(2, 6)),\n  time = c(4.1, 7.8, 10, 10, 12.3, 17.2, 9.7, 10, 11.1, 13.1, 19.7, 24.1),\n  event = c(1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0)\n)\ndat\n##    group time event\n## 1      1  4.1    1\n## 2      1  7.8    0\n## 3      1 10.0    1\n## 4      1 10.0    1\n## 5      1 12.3    0\n## 6      1 17.2    1\n## 7      2  9.7    1\n## 8      2 10.0    1\n## 9      2 11.1    0\n## 10    2 13.1    0\n## 11    2 19.7    1\n## 12    2 24.1    0\nsurvdiff(Surv(time, event) ~ group,\n  data = dat\n)\n##        N Observed Expected (O-E)^2/E (O-E)^2/V\n## group=1 6        4    2.57    0.800      1.62\n## group=2 6        3    4.43    0.463      1.62\n## \n##  Chisq= 1.6  on 1 degrees of freedom, p= 0.2\n\n# plot curves with pval from test\nfit &lt;- survfit(Surv(time, event) ~ group, data = dat)\nggsurvplot(fit,\n  pval = TRUE,\n  pval.method = TRUE\n)\n\npval &gt; 0.05, so there isn’t enough evidence to that they’re different.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#components",
    "href": "qmd/regression-survival.html#components",
    "title": "43  Survival",
    "section": "43.2 Components",
    "text": "43.2 Components\n\nSurvival Time (T) (aka death, failure time, event time): time at which the event occurs\nCensoring Time (C): time at which censoring occurs\n\nFor each unit, we observe T or C.\n\nY = min(T, C)\n\nright censoring: occurs when the event has happened after the enrollment (but the time is unknown).\n\nThe patient does not experience the event for the whole duration of the study.\nThe patient withdraws from the study.\nThe patient is lost to follow-up.\n\nleft censoring: occurs when the event has happened before the enrollment (but the time is unknown).\n\nCumulative hazard function (aka Cumulative Hazard Rates)\n\nShows the total accumulated risk of an event occurring at time t\nThe area under the hazard function\n\nHazard Rate (aka Risk Score), h(t | X)\n\nThe hazard rate is the probability that a unit with predictors, X, will experience an event at time, t, given that the unit has survived just before time, t.\nThe formula for the Hazard Rate is the Hazard function.\n\nHazard Ratio (aka Relative Risk of an event): Risk of an event given category / risk of an event given by reference category\n\nThe ratio of two instantaneous event rates\nCoefficient of the Cox Proportional Hazards model (e.g. paper)\n\neβ &gt; 1 (or β &gt; 0) for an increased risk of event (e.g. death).\neβ &lt; 1 (or β &lt; 0) for a reduced risk of event.\nHR of 2 is equivalent to raising the entire survival curve for a control subject to the second power to get the survival curve for an exposed subject\n\nExample: if a control subject has 5y survival probability of 0.7 and the exposed:control HR is 2, the exposed subject has a 5y survival probability of 0.49\nIf the HR is 1/2, the exposed subject has a survival curve that is the square root of the control, so S(5) would be √0.7 = 0.837\n\n\n\nStatus indicator, δ\n\nδ = 1, if T ≤ C (e.g. unit fails before study ends)\n\nTrue survival time is observed\n\nδ = 0, if T &gt; C (e.g. unit survives until end of study or has dropped out)\n\nCensoring time is observed\n\n\nSurvival function (aka Survival Rate), S(T &gt; t):\n\nOutputs the probability of a subject surviving (i.e., not experiencing the event) beyond time t\nMonotonically decreasing (i.e. level or decreasing)\nBaseline survival curve illustrates the survival function when all the covariates are set to their median value",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#kaplan-meir",
    "href": "qmd/regression-survival.html#kaplan-meir",
    "title": "43  Survival",
    "section": "43.3 Kaplan-Meir",
    "text": "43.3 Kaplan-Meir\n\nMisc\n\nUseful for validation of Proportional Hazards assumption. When lines cross the assumption, hazards are found to be non-proportional.\nHarrell RMS (Ch. 20.3):\n\nFor external validation: at least 200 events\nNeed 184 subjects with an event, or censored late, to estimate  to within a margin of error of 0.1 everywhere, at the 0.95 confidence level\n\n\nOrder event times (T) of units from smallest to largest, t1 &lt; …. &lt; tk\nCalculate probability that a unit survives past event time, ti, given that they survived up until event time, ti (i.e. past ti-1) (conditional probability)\n\ne.g. for t1, it’s (n1 - d1) / n1\n\nn1 is the number of units that have survived at t1\nd1 is the number of units that have experienced the event (e.g. died) at t1\nSimilar for other t values\n\nMedian survival time is where the survival probability equals 0.5\n\nSurvival function[](./_resources/Regression,_Survival.resources/1-bp-FZyIs1WlqYgQMWn9Obw.gif]]\n\nThe survival  function computes the products of these probabilities resulting in the K-M survival curve\nThe product of these conditional probabilities reflects the fact that to survive past event time, t, a unit must have survived all previous event times and the current event time.\n\nExample: 50 patients\n\n\nDotted lines represent 95% CI\nRed dots indicate time when patients died (aka event times)\nMedian survival time is ~ 13yrs",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#exponential",
    "href": "qmd/regression-survival.html#exponential",
    "title": "43  Survival",
    "section": "43.4 Exponential",
    "text": "43.4 Exponential\n\nAssumes that the hazard rate is constant\n\ni.e. risk of the event of interest occurring remains the same throughout the period of observation\n\nSurvival function\n\nHazard function\n\n\nh(t) is the constand hazard rate\n\nEstimated parameter: λ",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#weibull",
    "href": "qmd/regression-survival.html#weibull",
    "title": "43  Survival",
    "section": "43.5 Weibull",
    "text": "43.5 Weibull\n\nAssumes the change in hazard rate is linear.\nSurvival function\n\nHazard function\n\nEstimated parameters: λ and ρ\n\nλ parameter indicates how long it takes for 63.2% of the subjects to experience the event.\nρ parameter indicates whether the hazard rate is increasing, decreasing, or constant.\n\nIf ρ is greater than 1, the hazard rate is constantly increasing.\nIf ρ is less than 1, the hazard rate is constantly decreasing.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#coxs-proportional-hazards",
    "href": "qmd/regression-survival.html#coxs-proportional-hazards",
    "title": "43  Survival",
    "section": "43.6 Cox’s Proportional Hazards",
    "text": "43.6 Cox’s Proportional Hazards\n\nMultivariable regression model\nAllows the hazard rate to fluctuate\nHarrell: “under PH [assumption] and absence of covariate interactions, HR is a good overall effect estimate for binary [treatment]”\nMisc\n\nPackages\n\n{glmnet} - Regularized Cox Regression\n{coxphf} - Cox Regression with Firth’s Penalized Likelihood\n\nSee Regression, Regularized &gt;&gt; Firth’s Estimator\n\n\nSample Size\n\nHarrell RMS (Ch. 20.3):\n\nTo achieve a Multiplicative Margin of Error (MMOE ) of 1.2 (?) in estimating eβ^ with equal numbers of events in the two groups (balanced, binary treatment variable) and α = 0.05 → requires a total of 462 events\n\n\n\nAssumes\n\nHazard ratios (ratio of hazard rates or exp(β) between groups/units remain constant\n\ni.e. no matter how the hazard rates of the subjects change during the period of observation, the hazard rate of one group relative to the other will always stay the same\n\nHazard Ratios are independent of time\nExample: Immunotherapy typically violates PH assumptions (post)\n\nThe survival probabilities between the treatment (blue) and the chemo (red) cross at around the 4.2 months\n\nThe distance between the lines should remain somewhat constant throughout the trial in order to adhere to the PH assumptions (1st assumption)\nAlso think the lines should be somewhat straight. (2nd assumption)\n\nPatients in immunotherapy drug trials often experience a period of toxicity, but if they survive this period, they have a much better outcome down the road.\n\nTests\n\nGrambsch and Therneau (G&T)\nSee Harrell RMS (Ch. 20.6.2)\n\nIf assumptions are violated,\n\nGelman says to try and “expand the model, at the very least by adding an interaction.” (post)\nSee Harrell RMS (Ch. 20.7)\nUse a different model\n\nAccelerated Failure Time (AFT) model (See ML &gt;&gt; Gradient Boosting Survival Trees)\nAdjusted Cox PH model with Time-Varying Coefficients\n\nMust choose a functional form describing how the effect of the treatment changes over time\n\nRecommended to use AIC criteria to guide one’s choice among a large number of candidates\n\n\n\n\n\nModels event time (T) outcome variable and outputs parameter estimates for treatment (X) effects\n\nProvides a way to have time-dependent (i.e. repeated measures) explanatory variables (e.g. age, health status, biomarkers)\nCan handle other types of censoring such as left or interval censoring.\nHas extensions such as lasso to handle high dimensional data\nDL and ML models also have versions of this method\n\nHazard function\n\nh(t |X) = h0(t)exβ\nThe hazard rate for a unit with predictors, X, is the product of a baseline hazard, h0(t) (corresponding to X = 0) and a factor that depends on X and the regression parameters, β\nOptimized to yield partial maximum likelihood estimates, β^.\n\nDoesn’t require the specification of h0(t) which makes the method flexible and robust\n\n\nHazard Ratio (aka relative risk) for a binary covariate (e.g. treatment)  = eᶜᵒᵉᶠ\nInterpretation:\n\nHazard Rate (risk of the event): Moving from the reference category to the other category changes the hazard rate by a factor of eᶜᵒᵉᶠ\n\ne.g. eᶜᵒᵉᶠ = 0.68 means the change in category results in a (1 - 0.68) = 0.32 = 32% decrease in the hazard rate on average.\n\nRelative Risk (of an event): Risk of an event given category / risk of an event given by reference category\nExample: Treatment = Smoking\n\nRisk Score (aka hazard rate) given by smoking: (xᵢ=1): h₀(t)exp(β⋅xᵢ) = h₀(t)exp(β*1) = h₀(t)exp(β)\nRisk Score (aka base hazard rate) given by not smoking: (xᵢ=0): h₀(t)exp(β⋅xᵢ) = h₀(t)exp(β*0) = h₀(t)\nRelative risk (aka hazard ratio) = risk given by smoking / risk given by not smoking: h₀(t)exp(β) / h₀(t) = exp(β)",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#ml",
    "href": "qmd/regression-survival.html#ml",
    "title": "43  Survival",
    "section": "43.7 ML",
    "text": "43.7 ML\n\n43.7.1 Misc\n\nSplit data so partitions have the same censoring distribution.\n\nThe censoring distribution might be obtained from a Kaplan-Meier estimator applied to the data.\n\nDynamic AUC is a recommended metric\n\n\n\n43.7.2 Random Survival Forests\n\nThe main difference from a standard RF lies in the metric used to assess the quality of a split: log-rank (see Misc) which is typically used when comparing survival curves among two or more groups.\nPackages\n\n{{sklearn}}\n{aorsf} - Optimized software to fit, interpret, and make predictions with oblique random survival forests (ORSFs)\n\nInstead of using one variable to split the data, use a weighted combination of variables, i.e. \\(\\text{instead of}\\;\\; x_1 &lt; \\text{cutpoint (left), use}\\;\\; c_1x_1 + c_2x_2 &lt; \\text{cutpoint (right)}\\)\n\nPredictions of Standard RF vs Oblique RF\n\n\n\n\n\n\n\n\nStandard Random Forest\n\n\n\n\n\n\n\nOblique Random Forest\n\n\n\n\n\n\n\nIn the standard rf, the decision boundaries are essentially perpendicular while the oblique rf boundaries are more angular. This should make the oblique model more flexible.\n\nKaplan-Meir Curves are fit in the leaves of the trees\n\n\nTime is on the x-axis and probability of survival on the y-axis\n\n\n\nExample: {aorsf}\n\nFrom Machine Learning for Risk Prediction using Oblique Random Survival Forests (Video; Slides & Code)\nVia package\n# equivalent syntaxes\nfit_orsf &lt;- orsf(data = pbc_orsf, \n                 formula = Surv(time + status) ~ . - id)\nfit_orsf &lt;- orsf(data = pbc_orsf, \n                 formula = time + status ~ . - id)\n\nTop model is fit with the typical survival::coxph syntax\ntime: time to event\nstatus: dummy variable indicating whether event occurred\nid: unit or patient id which is excluded\n\nVia {tidymodels}\nlibrary(parsnip)\nlibrary(censored) # must be version 0.2.0 or higher\nrf_spec &lt;- \n  rand_forest(trees = 200) %&gt;%\n  set_engine(\"aorsf\") %&gt;% \n  set_mode(\"censored regression\") \nfit_tidy &lt;- \n  rf_spec %&gt;% \n  parsnip::fit(data = pbc_orsf, \n               formula = Surv(time, status) ~ . - id)\nEstimated Expected Risk via Partial Dependence (PD)\n\nPD and importance rank for variables\norsf_summarize_uni(fit_orsf, n_variables = 1)\n## \n## -- bili (VI Rank: 1) ----------------------------\n## \n##         |---------------- risk ----------------|\n##   Value      Mean    Median     25th %    75th %\n##  &lt;char&gt;     &lt;num&gt;     &lt;num&gt;      &lt;num&gt;     &lt;num&gt;\n##    0.80 0.2343668 0.1116206 0.04509389 0.3729834\n##     1.4 0.2547884 0.1363122 0.05985486 0.4103148\n##     3.5 0.3698634 0.2862611 0.16196924 0.5533383\n## \n##  Predicted risk at time t = 1788 for top 1 predictors\n\nComputes expected risk (predicted probability) at different quantiles as a predictor variable varies.\n\nValue is 3 values of the predictor which are the 25th, 50th, and 75th quantile.\n\nn_variables says how many “important” variables to look at\n\ne.g. n_variables = 2 would look at the top 2 variables in terms of variable importance.\nVI Rank: 1 indicates the bili is ranked first in variable importance\n\nbili: serum bilirubin (mg/dl); continuous predictor variable\nIt choses time = 1788 because that’s the median\nAlso see Diagnostics, Model Agnostic &gt;&gt; DALEX &gt;&gt; Dataset Level &gt;&gt; Partial Dependence Profiles\n\nPD at specified predictor values and time values\npd_by_gender &lt;- orsf_pd_oob(fit_orsf, \n                  pred_spec = list(sex = c(\"m\", \"f\")),\n                  pred_horizon = 365 * 1:5)\npd_by_gender %&gt;% \n  dplyr::select(pred_horizon, sex, mean) %&gt;% \n  tidyr::pivot_wider(names_from = sex, values_from = mean) %&gt;% \n  dplyr::mutate(ratio = m / f)\n\n## # A tibble: 5 x 4\n##   pred_horizon      m      f ratio\n##          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1          365 0.0768 0.0728  1.06\n## 2          730 0.125  0.111   1.13\n## 3         1095 0.230  0.195   1.18\n## 4         1460 0.298  0.251   1.19\n## 5         1825 0.355  0.296   1.20\n\norsf_pd_oob - Computes expected risk using out-of-bag only\n\nBoth values (m,f) for sex are specified\npred_horizon specifies the time values\n\nHere, time is in days, so these values specify expected risk (predicted probabilities) at years 1 through 5.\n\n\nratio is the risk ratio of males compared to females.\nOthers\n\norsf_pd_inb - Computes expected risk using all training data\norsf_pd_new - Computes expected risk using new data\n\n\n\n\n\n\n\n43.7.3 Gradient Boosting Survival Trees\n\nLoss Functions\n\nPartial likelihood loss of Cox’s proportional hazards model\nSquared regression loss\nInverse probability of censoring weighted least squares error.\n\nAllows the model to accelerate or decelerate the time to an event by a constant factor. It is known as the Accelerated Failure Time (AFT). It contrasts with the Cox proportional hazards model where only the features influence the hazard function.\n\n\nPackages\n\n{{sklearn}}\n\n\n\n\n43.7.4 Survival Support Vector Machine\n\nPredictions cannot be easily related to the standard quantities of survival analysis, that is, the survival function and the cumulative hazard function.\nPackages\n\n{{sklearn}}",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#diagnostics",
    "href": "qmd/regression-survival.html#diagnostics",
    "title": "43  Survival",
    "section": "43.8 Diagnostics",
    "text": "43.8 Diagnostics\n\nMisc\n\nNotes from How to Evaluate Survival Analysis Models\nPackages\n\n{survex} - Explainable Machine Learning in Survival Analysis\n\nFrom Dalex group\n\n\n\nConcordance Index (C-Index, Harrell’s C)\n\nConsider a pair of patients (i, j). Intuitively, a higher risk should result in a shorter time to the adverse event. Therefore, if a model predicts a higher risk score for the first patient (ηᵢ &gt; ηⱼ), we also expect a shorter survival time in comparison with the other patient (Tᵢ &lt; Tⱼ).\nEach pair (i, j) that fulfills this expectation (ηᵢ &gt; ηⱼ : Tᵢ &lt; Tj or ηᵢ &lt; ηⱼ : Tᵢ &gt; Tⱼ) as concordant pair, and discordant otherwise.\n\nA high number of concordant pairs is an evidence of the quality of the model, as the predicted higher risks correspond to an effectively shorter survival time compared to lower risks\n\nFormula\n\n\nIf both patients i and j are censored, we have no information on Tᵢ and Tⱼ, hence the pair is discarded.\nIf only one patient is censored, we keep the pair only if the other patient experienced the event prior to the censoring time. Otherwise, we have no information on which patient might have experienced the event first, and the pair is discarded\n\nProgrammatic Formula\n\n\nWhere the variable Δⱼ indicates whether Tⱼ has been fully observed (Δⱼ = 1) or not (Δⱼ = 0). Therefore, the multiplication by Δⱼ allows to discard noncomparable pairs because the smaller survival time is censored (Δⱼ = 0).\n\nGuidelines\n\nC = 1: perfect concordance between risks and event times.\nC = 0: perfect anti-concordance between risks and event times.\nC = 0.5: random assignment. The model predicts the relationship between risk and survival time as well as a coin toss.\nDesirable values range between 0.5 and 1.\n\nThe closer to 1, the more the model differentiates between early events (higher risk) and later occurrences (lower risk).\n\n\nIssues\n\nThe C-index maintains an implicit dependency on time.\nThe C-index becomes more biased (upwards) the more the amount of censoring (see Uno’s C below)\n\n\nUno’s C\n\n** Preferable to Harrell’s C in the presence of a higher amount of censoring. **\nVariation of Harrell’s C that includes the inverse probability of censoring weighting\n\nWeights based on the estimated censoring cumulative distribution\nUses the Kaplan-Meier estimator for the censoring distribution\n\nSo the disribution of censored units should be independent of the covariate variables\n\nIn the paper, Uno showed through simulation this measure is still pretty robust even when the censoring is dependent on the covariates\n\n\n\n\nDynamic AUC\n\nAUC where the False Positive Rates (FPR) and True Positive Rates (TPR) are time-dependent\n\nSince a unit is a True Negative until the event then becomes a True Positive\nRecommended for tasks when you want to measure performance over a specific period of time (e.g. predicting churn in the first year of subscription).\n\nFormula\n\n\nf^s are predicted risk scores\nώ is the inverse probability of censoring weight (see Uno’s C)\n\ndisregard the freaking dash above the omega, microsoft deleted the regular one for some reason\n\nI(yi,j &gt; t) indicates whether the unit pair’s, i and j, event time is greater or less the time, t. (I think)\n\n\n\n\n\n\nStandard Random Forest\nOblique Random Forest",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-hosting.html",
    "href": "qmd/shiny-hosting.html",
    "title": "Hosting",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Shiny",
      "Hosting"
    ]
  },
  {
    "objectID": "qmd/shiny-hosting.html#sec-shiny-hosting-misc",
    "href": "qmd/shiny-hosting.html#sec-shiny-hosting-misc",
    "title": "Hosting",
    "section": "",
    "text": "Examples\n\nshiny server, linux, local and Digital Ocean - Deploy your own Shiny app server with debian | R with White Dwarf\n\nBenchmarks\n\nOverview\n\n\nPosit Connect &gt;&gt; ShinyProxy for scaling up even though its free (article)\nFree options\n\nshinyapps.io, heroku, Fly.io\nFly.io’s free tier is your best option if you want the apps running 24/7 and under a secured custom domain (none of this is offered for Shinyapp.io and Heroku).\nOther options\n\nIf you run out of free builds or usage limits, use multiple emails to create multiple free-tier accounts\nBuild mulitple apps on the same docker image\n\nShinyProxy\n\nBenefits\n\nOpen source – no subscription fee\nA wide range of Authorization options available (LDAP / Kerberos / SSO / SAML / Open ID / Keycloak / Social Media / Simple (flat file of users & passwords))\n\nCosts\n\nHigher implementation cost\nNo product support, therefore there is no possibility of SLA guarantees\nThe additional cost of post-implementation support\nThe additional layer of complexity – Docker/Kubernetes\nLong-term maintenance requiring a large workforce\n\n\nShiny Server\n\nPros - open source\nCons\n\nMust load it on the server\nKeep track of security patches\nManage dependencies and harmonize them if you have multiple apps on the same server\n\n\n\nDigital Ocean\n\ndigitalocean droplet - $4/month and host the apps with shinyproxy or shinyserver\n\nFor light, intermittent usage\n\n\nShinyapps.io\n\nAs of 2023, $13 for lowest paid tier.\n\nPosit Connect\n\nBenefits\n\nComes with great integration with R and Python and comes with a lot more than just hosting Shiny apps\nAbility to deploy R/Python applications, R/Python API, RMarkdown reports that regenerate automatically, etc.\nWorth it if you’re working with a team and delivering more than shiny apps (e.g. reports, cron jobs, etc.)\nStability with Software licensing and Standard Software Support from Posit included\nLower implementation costs\nEasier configuration and deployment of dashboard versions\nAuthorization is included in the Posit Connect Subscription, with a wide range of options to choose from: (LDAP and Active Directory / SAML / OAuth 2.0 using Google Apps accounts / PAM / Proxied Authentication / SLA)\nEasier maintenance in production\nOption to add Posit Workbench and Package Manager\nAutomatic scaling – no need to manually trigger new processes\nAn industry standard by Fortune 500 companies working with R\nAdmin panel to manage users and monitor the logs and machine\n\nCosts\n\nA yearly Posit Connect subscription starts at $14,995/year (USD)",
    "crumbs": [
      "Shiny",
      "Hosting"
    ]
  },
  {
    "objectID": "qmd/shiny-ui.html",
    "href": "qmd/shiny-ui.html",
    "title": "47  Shiny, UI",
    "section": "",
    "text": "TOC\n\nMisc\nDesign Principles\nStyle\nResponsiveness\n\nMisc\n\n\n\nDesign Principles\n\nMisc\n\nNotes from\n\nErik Kennedy thread\n\n\nDe-emphasize Dividing lines\nUse fonts that subtly convey brand\nContent cards should be lighter than their bg (in dark mode too)\nDon’t resize icons\n\nTheir level of detail and stroke weights are meant to work best at a certain size.\nInstead, try adding a border or container around them for some extra visual pop\n\nREMOVE-HIDE-LIGHTEN for cleaner designs\nBe consistent until it’s time not to be consistent\n\nBreak consistency example: when you’re trying to catch the user’s eye\n\nGood imagery\nRemove Congestion See Designing Accessible Research with R/Shiny UI – Part 2 for an example of an iterable workflow\n  Congestion is when an app shows everything, all at once and that’s stressful for the user. It doesn’t elicit the behavior we want for an engaging, learning experience. \n\n  This stress triggers two subconscious actions – run or freeze. Both lead to cognitive load and negative perception, and ultimately, failed adoption.\n      Running is expressed in bounce rate. A user will enter the app, become frustrated, and leave.\n\n      Freezing means that a user pauses with a delayed time to understand. This can be expressed in a number of ways, but most likely a combination of longer session times with aimless user behavior.\n\nMinimize options, legends, etc. and move to the edges of the app (e.g. header, sides)\n\nExample\n\nPrototype\n\nIssues\n\nThe tree selector consumes space on the dashboard\nSelecting via images doesn’t add a lot of value as not all tree species are easily identified by image\nThe UI is too dark\nThe visual accessibility feature was lost as some colors don’t work on a dark background\n\n\nFinal\n\nDashboard\n\nThe legend moved to the card (see mobile for more details\n\nTree card gives details about species and a summary\nClicking upper right icon flips the card to display the bar chart.\n\nSwitching languages ​​and information about the application moved to the header (right side of blue nav bar)\nClicking text in header opens pop-up with options. One for type of tree and one for type of scenario (beteen blue nav bar and map)\nAll other interactive elements that were scattered on the screen are now organized into the right vertical panel on the map. Here is also the color blindness option for people with visual disabilities, as the main focus of the application is on the color ratio on the map\n\nMobile\n\n“languages” and “about” moved to the top\n\n\n\n\n\n\nStyle\n\nMisc\n\nImages and style.css files should go into the “www” folder\nDancho full page map UI from app in learning lab 28\n\nCode only shown partially in learning lab 83\nControl panel had a transparent background\nClickable logos\n{shiny}, {fresh}, {shinyWidgets}\n\n\n\nmy_theme &lt;- create_theme(\n    theme = \"paper\",\n    bs_vars_global(\n        body_bg = \"black\",\n        text_color = \"#fff\"\n    ),\n    bs_vars_navbar(\n        default_bg = \"#75h8d1\",\n        default_color = \"#ffffff\",\n        default_link_color = \"#ffffff\",\n        default_link_active_color = \"#75b8d1\",\n        default_link_active_bg = \"#ffffff\",\n        default_link_hover_color = \"#2c3e50\"\n    ),\n    bs_vars_dropdown(\n        bg = \"#0006\",\n    ),\n    bs_vars_modal(\n        content_bg = \"#0006\"\n    )\n    bs_vars_wells(\n        bg = \"#75b8d1\n    )\n    bs_vars_input(\n        color = \"#FFF\",\n        color_placeholder = \"bdbdbd\"\n    ),\n    bas_vars_button(\n        default_color = \"black\",\n        primary_bg = \"black\",\n        success_bg = \"#188C9C\",\n        info_bq = \"#A6CEE3\",\n        info_color = \"#2c3e50\",\n        warning_bg = \"#CCBE93\",\n        danger_bg = \"#E31A1C\"\n    )\n    bs_vars_panel(\n        bg = \"#0006\",\n        default_heading_bg = \"#0006\",\n        default_text = \"white\"\n    )\n)\nui &lt;- bootstrapPage(\n\n    tags$style(type = \"text/css\", \"html, body {.lightbox width: ... not seen\n    tags$head(\n        HTML(\"&lt;style&gt;\n            h1,h2,h3,h4,h5,h6(color:#FFF !important;} .... possibly not seen\n            /form-control{color:#FFF;}\n            .dataTables_filter{color:white;}\n            thead{color:#FFF;}\n            &lt;/style&gt;\")\n    ),\n\n    use_theme(my_theme),\n    leafletOutput(\"map\", width = \"100%\", height = \"100%\" .... not seen\n\n    absolutePanel(\n        id = \"logos\",\n        style = \"z-index:300;bottom:50px;right:50px;\",\n        h2(\"Pharmacy Finder\")\n    ),\n\n    absolutePanel(\n        id = \"business-science\",\n        style = \"z-index:300;bottom:50px;left:50px;\", .... not seen\n        h4(\"Learn Shiny\", class = \"text_primary\"),\n        h5(\n            tags$img(src = \"https://www.business.scien .... not seen\n                    style = \"width:48px;-webkit-filte .... not seen\n                    \"Business Science\"\n        ) %&gt;%\n            tags$a(class = \"btn btn-primary btn-sm\", .... not seen\n    ),\n\n    absolutePanel(\n        id = \"controls\",\n        style = \"zindex:5000;top:10px;left:50px;\",  .... not seen\n        draggable = FALSE,\n        div(\n            class=\"panel panel-default\",\n            style = \"width:300px;\",\n            div(\n                class=\"panel-body\",\n                textInput(\"city\", \"City\", \"Pittsburgh\" .... not seen\n                selectInput(\"amenity\", \"Amenity Type\", .... not seen\n                shiny::actionButton(\n                    inputId = \"submit\",\n                    label = \"Search\",a\n                    class = \"btn-default\"\n                ),\n                downloadButton(\"download_csv\", \"Downlo .... not seen\n            )\n        )\n    )\n)\n\n\nImplementing a css file\n\nExample: basic\n\nmain.css file\n\n\n\n@import url('https://fonts.googleapis.com/css2?family=Poppins&display=swap');\n* {\n  margin: 0;\n  padding: 0;\n  box-sizing: border-box;\n  font-family: 'Poppins', sans-serif;\n}\nbody {\n  padding: 1rem;\n}\n#map {\n  height: 98vh !important;\n  border-radius: 0.5rem !important;\n}\n\napp.R\n\nui &lt;- fluidPage(\n  tags$head(tags$link(rel = \"stylesheet\", type = \"text/css\", href = \"main.css\")),\n  sidebarLayout(\n    ...\n  )\n)\nResponsiveness\n\nResponsiveness isn’t the same as performance. Performance is about completing an operation in the minimal amount of time, while responsiveness is about meeting human needs for feedback when executing an action\nMisc\n\nNotes from Improving the responsiveness of Shiny applications\n\nButton Click Registered\n\n(left) default arrow for your app\n(middle) hand indicates to the user that the app has registered their hovering over a button\n(right) arrow+pie indicates to the user that the app has registered their clicking the button\n(generic) entry into CSS file\n\n\nhtml.shiny-busy .container-fluid {\n  cursor: wait;\n}\n\nLooks like this code is to produce something like the right-side image\nFor touch devices (i.e. without cursor), you might want to take a look at {shinycssloaders} and/or {waiter}\nIf the user might have to wait longer than a few seconds for the process they’ve just set in motion to complete, you should consider a progress indicator.\n\n{shiny} has a progress indicator, while {waiter} also offers a nice built-in-to-button option (below)",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Shiny, UI</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-ui-examples.html",
    "href": "qmd/shiny-ui-examples.html",
    "title": "48  Shiny, UI Examples",
    "section": "",
    "text": "TOC\nMisc\nMap Apps\n\nDancho full page map UI from app in learning lab 28\n\nCode only shown partially in learning lab 83\n\nserver part also shown\n\nControl panel had a transparent background\nClickable logos\n{shiny}, {fresh}, {shinyWidgets}\n\n\nmy_theme &lt;- create_theme(\n    theme = \"paper\",\n    bs_vars_global(\n        body_bg = \"black\",\n        text_color = \"#fff\"\n    ),\n    bs_vars_navbar(\n        default_bg = \"#75h8d1\",\n        default_color = \"#ffffff\",\n        default_link_color = \"#ffffff\",\n        default_link_active_color = \"#75b8d1\",\n        default_link_active_bg = \"#ffffff\",\n        default_link_hover_color = \"#2c3e50\"\n    ),\n    bs_vars_dropdown(\n        bg = \"#0006\",\n    ),\n    bs_vars_modal(\n        content_bg = \"#0006\"\n    )\n    bs_vars_wells(\n        bg = \"#75b8d1\n    )\n    bs_vars_input(\n        color = \"#FFF\",\n        color_placeholder = \"bdbdbd\"\n    ),\n    bas_vars_button(\n        default_color = \"black\",\n        primary_bg = \"black\",\n        success_bg = \"#188C9C\",\n        info_bq = \"#A6CEE3\",\n        info_color = \"#2c3e50\",\n        warning_bg = \"#CCBE93\",\n        danger_bg = \"#E31A1C\"\n    )\n    bs_vars_panel(\n        bg = \"#0006\",\n        default_heading_bg = \"#0006\",\n        default_text = \"white\"\n    )\n)\nui &lt;- bootstrapPage(\n\n    tags$style(type = \"text/css\", \"html, body {.lightbox width: ... not seen\n    tags$head(\n        HTML(\"&lt;style&gt;\n            h1,h2,h3,h4,h5,h6(color:#FFF !important;} .... possibly not seen\n            /form-control{color:#FFF;}\n            .dataTables_filter{color:white;}\n            thead{color:#FFF;}\n            &lt;/style&gt;\")\n    ),\n\n    use_theme(my_theme),\n    leafletOutput(\"map\", width = \"100%\", height = \"100%\" .... not seen\n\n    absolutePanel(\n        id = \"logos\",\n        style = \"z-index:300;bottom:50px;right:50px;\",\n        h2(\"Pharmacy Finder\")\n    ),\n\n    absolutePanel(\n        id = \"business-science\",\n        style = \"z-index:300;bottom:50px;left:50px;\", .... not seen\n        h4(\"Learn Shiny\", class = \"text_primary\"),\n        h5(\n            tags$img(src = \"https://www.business.scien .... not seen\n                    style = \"width:48px;-webkit-filte .... not seen\n                    \"Business Science\"\n        ) %&gt;%\n            tags$a(class = \"btn btn-primary btn-sm\", .... not seen\n    ),\n\n    absolutePanel(\n        id = \"controls\",\n        style = \"zindex:5000;top:10px;left:50px;\",  .... not seen\n        draggable = FALSE,\n        div(\n            class=\"panel panel-default\",\n            style = \"width:300px;\",\n            div(\n                class=\"panel-body\",\n                textInput(\"city\", \"City\", \"Pittsburgh\" .... not seen\n                selectInput(\"amenity\", \"Amenity Type\", .... not seen\n                shiny::actionButton(\n                    inputId = \"submit\",\n                    label = \"Search\",a\n                    class = \"btn-default\"\n                ),\n                downloadButton(\"download_csv\", \"Downlo .... not seen\n            )\n        )\n    )\n)",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Shiny, UI Examples</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-design.html",
    "href": "qmd/surveys-design.html",
    "title": "52  Design",
    "section": "",
    "text": "52.1 Misc",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-design.html#sec-surv-design-misc",
    "href": "qmd/surveys-design.html#sec-surv-design-misc",
    "title": "52  Design",
    "section": "",
    "text": "For sampling methods, see Surveys, Sampling Methods\nItem-specific surveys perform better/more reliable than agree/disagree surveys. The reason behind this is that participants are more certain while choosing a position in item-specific surveys compared to agree/disagree surveys\nFor self-report surveys, ceiling/floor effects on score distributions disaappear when response options are greater than 2 or 3 (Thread, Paper)\nSurvey Monkey sample size calculator\nIf asking the Age or Income of a respondant:\n\nIf calculating mean or median, exact numbers are best but respondants are usually hesitant to give out exact numbers for such data.\nRanges give less accurate averages but respondants are more likely to answer.\nGroup based on population your studying\n\nIf it’s the general population, $20k or less is a good first rung; $21–39k is next; from there: $40–69k, $70–99k; $100–150k; and $150+\n\nIf it’s college students, the ranges will be lower.\n\nBreaks between groups should line-up with known charcteristics of the population you’re studying\n\n\nUse the phrases, jargon, and emotions that your customers are familiar with.\n\nGet a sense of this by getting on the phone and talking to your customers. Or run focus groups. Or run some on-site surveys.\n\nBest practices for the format of common demographic questions, link",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-design.html#sec-surv-design-terms",
    "href": "qmd/surveys-design.html#sec-surv-design-terms",
    "title": "52  Design",
    "section": "52.2 Terms",
    "text": "52.2 Terms\n\nSelf-Report Study - Type of survey, questionnaire, or poll in which respondents read the question and select a response by themselves without any outside interference. A self-report is any method which involves asking a participant about their feelings, attitudes, beliefs and so on.\nCeiling and Floor Effects - An artificial lower limit on the value that a variable can attain, causing the distribution of scores to be skewed.\n\nExample: The distribution of scores on an ability test will be skewed by a floor effect if the test is much too difficult for many of the respondents and many of them obtain zero scores.\nExample: The distribution of scores on an ability test will be skewed by a ceiling effect if the test is much too easy for many of the respondents and many of them obtain perfect scores",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-design.html#sec-surv-design-soe",
    "href": "qmd/surveys-design.html#sec-surv-design-soe",
    "title": "52  Design",
    "section": "52.3 Sources of Error",
    "text": "52.3 Sources of Error\n\n\nMisc\n\nNotes from Total Survey Error: Design, Implementation, and Evaluation (d/l pdf to see figures)\n\nsurvey error is defined as the deviation of a survey response from its underlying true value\nsurvey accuracy is defined as the deviation of a survey estimate from its underlying true parameter value\nTotal Survey error - accumulation of all errors that may arise in the design, collection, processing, and analysis of survey data. It includes sampling variability, interviewer effects, frame errors, response bias, and non-response bias\n\nNon-Sampling Error\n\nSpecification error occurs when the concept implied by the survey question differs from the concept meant to be measured in the survey.\n\nOften caused by poor communication between the researcher, data analyst, or survey sponsor and the questionnaire designer.\n\nCoverage or Frame error typically results from the frame construction process. Discrepancies between the (theoretical) target population of a survey and the frame which is used to draw a sample are equivalent to statistical errors.\n\nSee Surveys, Sampling Methods &gt;&gt; Terms&gt;&gt; sampling frame\nIn practice, frames are not error-free:\n\nthey can never encompass the whole target population (because it always takes time to administrativally record an individual in a register),\nthey also contain individuals which are no longer eligible (e.g. individuals who left the country and may keep recorded in the Register several months after they left).\ncoverage errors or frame errors.\n\nExamples\n\nSome units may be omitted or duplicated an unknown number of times\nSome ineligible units may be included on the frame, such as businesses that are not farms in a farm survey.\n\n\nNonresponse error - When the reason for nonresponse is related to the missing value, parameter estimates can be biased when nonresponse is not accounted for. Includes:\n\nunit nonresponse - sampling unit does not respond to any part of the questionnaire\n\ne.g. calling a person and them choosing not to answer or participate in the the survey\n\nitem nonresponse - the questionnaire is partially completed.\n\nMeasurement error occurs when the method of obtaining the measurement affects the recorded value, often involving simultaneously the respondent, the interviewer, and the survey questionnaire.\nProcessing error refers to errors that arise during the data processing stage, including errors in the editing of the data, data encoding, the assignment of survey weights, and tabulation of the survey data.\n\nSampling Error - - caused by collecting partial information over a fraction of the population rather than the whole population itself.\n\nSampling scheme (e.g., multistage or multiple-phase sample)\nSample size\nChoice of estimator (e.g., a ratio or regression estimator, levels of post-stratification)\n\n\nKeeping sampling errors under control\n\nSurvey questionnaires must be prepared with utmost care, intensively pre-tested and field-tested in order to detect issues in question wording, routing problems or any other inconsistency\nModes of data collection must be chosen and combined judiciously in order to get most people to cooperate\nInterviewers must be carefully recruited and properly trained\nCommunication and contact strategies towards participants must be designed and adapted in order to reach highest participation.",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-design.html#sec-surv-design-respfor",
    "href": "qmd/surveys-design.html#sec-surv-design-respfor",
    "title": "52  Design",
    "section": "52.4 Response Formats",
    "text": "52.4 Response Formats\n\nitem-specific (IS) - multiple choice response format\n\nAgree/Disagree (A/D) - response format where the response are degrees of strength of agreement or disagreement",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-design.html#sec-surv-design-respsc",
    "href": "qmd/surveys-design.html#sec-surv-design-respsc",
    "title": "52  Design",
    "section": "52.5 Response Scales",
    "text": "52.5 Response Scales\n\n52.5.1 Misc\n\nThe default values and ranges you use\nExamples:\n\nage: 5 yr default range (e.g. 20–25) instead of 10 yrs ranges\n\n\n\n\n52.5.2 Types\n\nDichotomous scales\n\nprecise data, but they don’t allow for nuance in respondents’ answers.\nExamples: “Yes” or “No”; “True” or “False”; “Fair” or “Unfair”\n\nQuantitative Scales\n\nRating Scales\n\nProvides more range than Dichotmous scales. Too generic for attitudes\n1–10; 1–7; 1–5 (or Likert scale, also see below)\n\nlabel all number or none\n\na 1–5 scale presented verbal descriptions for only the 1 and 5 endpoints and this led more people to choose the endpoints.\nA “school grade” scale is more reliable than other types of word labels\n\n\n\nOrdinal and interval scales\n\nWith ordinal, the numbers just have an intrinsic order\nWith interval, the distance between the numbers must also be equal in terms of context\n\ne.g. rating something a 2 vs 1 doesn’t mean that being a 2 is “twice as good” as being a 1\n\nunless being 5 is also twice as good as being a 4, etc.\n\n\nThere is no practical difference between ordinal or interval scales\n\nRatio Scales\n\nwhere there is a true zero and equal intervals between neighboring points. Unlike on an interval scale, a zero on a ratio scale means there is a total absence of the variable you are measuring. Length, area, and population are examples of ratio scales\n\n\nSemantic differential scales\n\n\nGather data and “interpret based on the connotative meaning of the respondent’s answer.”\nUsually have dichotomous words at either end of the spectrum\nThe more quantifiable the information is (behavior questions, for instance), the smaller the range should be.\n\nWhen you want to measure attitudes or feelings, using a 5- or 7-point semantic differential scale is a good strategy.\n\n\n\n\n\n52.5.3 Likert Scale\n\nNotes from: Likert Scales: Friend or Foe?\nquantitative, rating scale\nAvoid using agree/disagree wording (see biases section below)\nExamples:\n\n5-point:\n\n[1] Strongly Disagree, [2] Disagree\n[3] (4pt removes this response) Neither Disagree Nor Agree\n[4] Agree, [5] Strongly Agree\n\n7-point:\n\n[1] Strongly disagree, [2] Disagree, [3] Somewhat disagree\n[4] Neither agree nor disagree\n[5] Somewhat agree, [6] Agree, [7] Strongly agree\n\nMore examples, Link\n\nIssues\n\nOrdinal and not Interval\n\nAnswers are not all equidistant\n\nRespondents may perceive (4) Strongly Agree and (3) Agree very similarly and thus the difference between these two options might be much smaller than the difference between (3) Agree and (2) Disagree, despite having the same distance.\n\n\nNumerical values assigned to the response options cannot be treated as interval data\n\nParametric statistics (e.g., mean, standard deviation) and parametric statistical methods (e.g., summing up individual questions to find a total survey score, running a regression with survey scores) would NOT yield valid results\n\nAcceptable Analysis Methods:\n\nMedian and Mode\nOrdinal regression\nChi-Square Test of Independence\nItem Response Theory (IRT) modeling\nGraphical Tools (e.g. bar charts and correlation matrix plots)\n\n\nAdding a neutral option (i.e. Neither Disagree Nor Agree)\n\nMay increase the accuracy of survey data because respondents who do not have a strong preference may prefer to select the neutral response option, instead of randomly selecting a response option or skipping the question\nMay produce a bias: research shows that respondents often see the visual midpoint of a scale as representing the middle response option\nSolutions:\n\nuse a Likert scale that consists of an even number of response options without a neutral option\nselect survey questions for which respondents would not select the neutral option very easily.\n\n\nChoosing the number of responses\n\nResearch shows that Likert scales with 2 to 5 response options often yield precise results, although smaller numbers of response options may reduce the measurement precision of a survey\nResearchers found that there are no clear advantages of using beyond 6 response options on a Likert scale\n\nPositively and negatively worded questions together\n\nTypically used to prevent response bias\nPositively and negatively worded questions are not necessarily mirror images of each other.\n\nTherefore, when analyzing survey data, reverse-coding the Likert scale for negatively worded questions (e.g., 1-Strongly agree; 2-Agree; 3-Disagree; 4-Strongly disagree) may not necessarily put these questions in the same direction as positively worded questions\n\nResearch shows that negative wording may confuse respondents, leading to less accurate responses to the survey questions\n\nMay be increasing response bias instead of reducing it.\n\nStudies indicate that respondents are more likely to disagree with negatively worded questions than to agree with positive ones\n\nExample\n\nA respondent who would select Agree for “My room was clean” might prefer to select Strongly Disagree for “My room was dirty”.\n\n\nSolutions:\n\nKeep the number of negatively worded questions minimal, while taking the impact of negatively worded questions on responses into account.\n\n\nSliding Scales\n\nlink\n\n\n\n\n\n52.5.4 Guttman Scale\n\n\nDichotomous or Likert\nGradually increases in specificity. The intent of the scale is that the person will agree with all statements up to a point and then will stop agreeing.\nThe scale may be used to determine how extreme a view is, with successive statements showing increasingly extremist positions.\nAlso, a useful tool for measuring satisfaction\nIf needed, the escalation can be concealed by using intermediate questions.\n\n\n\n52.5.5 Net Promoter Score (NPS)\n\nCustomer loyalty metric, 0-9 (or 1-10)\n“How likely is it that you would recommend our company/product/service to a friend or colleague?”\nNPS = percent_promoters - percent_detractors\nResponses get broken up into 3 groups\n\nPromoters (9–10). These are your happiest and most loyal customers who are most likely to refer you to others. Use them for testimonials, affiliates, etc. These customers are key to business growth and thereby sustaining their customer experience is critical to the brand.\nPassives or Neutrals (7–8). These customers are happy but are unlikely to refer you to friends. They may be swayed to a competitor fairly easily. A business should look at ways and means of upgrading neutrals to promoters by understanding their requirements.\nDetractors (0–6). Detractors are unhappy customers who can be dangerous for your brand, spreading negative messages and reviews. Figure out their problems and fix them.\n\nAny NPS that is positive is usually perceived as good, and an NPS score of 50+ is considered excellent. The range of NPS is from -100 (all detractors) to +100 (all promoters).\n**Don’t use as a single predictor of customer loyalty**\n\nA customer might actually be very enthusiastic about the product, but they just might not ever feel the urge to recommend hemorrhoid cream to their pals\n\nCombine with other measures\n\nAsk follow-up questions.\n\nPromoters. What’s your favorite part about our product/service?\nPassives. What would make you love us?\nDetractors. What could we do to improve your experience?\n\nCombine it with user research.\n\nusability testing and other common conversion research techniques\nUser testing\nCustomer surveys\nLive chat\nHeat maps (e.g. buttons clicked on websites, scrolling actions)\n\nFind and fix issues.\n\nuse info to prioritize projects that enhance user experience\n\nMarket to promoters. Use Promoters, Passives, and Detractors as a segmentation tool.\n\nGive free stuff to “promoters” to incentize more buying or advocates to others by sharing on FB or twitter or write a review.\nFind correlations between certain product actions (heatmaps) and a higher NPS. This can help deduce what your product’s “magic moment” is when your users are truly activated and likely to derive delight from your product. Then you can focus on product optimizations to get more of your customer base to this point\n\n\nIssues\n\nresponse rate for surveys is relatively low\ntime-consuming and costly affair to collect a sizeable amount of survey data\nrelatively high-cost outlay associated with sending out surveys\nresponses are typically quantitative. There is rarely qualitative information explaining the reason for the response\n\nAlternative: Use sentiment analysis on customer reviews and social media posts to generate a proxy for NPS based on sentiment scores.\n\nSee Algorithms, Product &gt;&gt; Net Promoter Score",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-design.html#sec-surv-design-respsc-biases",
    "href": "qmd/surveys-design.html#sec-surv-design-respsc-biases",
    "title": "52  Design",
    "section": "52.6 Biases",
    "text": "52.6 Biases\n\nAcquiescence Bias\n\nA tendency to agree with statements rather than disagree\nOccurs with Agree/disagree (A/D) or Yes/No (Y/N) questions\nEffects can be stronger when the survey is administered by a person compared to self-administered surveys\n\nSome people’s personal inclination can lead them to be polite and avoid conflict, ultimately being aggregable.\nSome participants may consider themselves in lower social status than the interviewer/researcher. Therefore, they may believe what is offered in questions and unintentionally accept the ‘agree’ choice.\nIn many cultures, while interacting with another person, agreeing is more well-suited than disagreeing\n\nCan cause a correlational relation between similarly worded questions, thus, eliminates some important constructs\nEven a small survey error stemming from acquiescence bias decreases the quality of the inferences\nSolution:\n\nconvert Agree/Disagree (A/D) response format to item-specific (IS) response format\nExample\n\n\n\nResponse Order Effect\n\nChoices presented earlier are more probable to be selected\nIf the answer options are categorical, respondents tend to conceive them consecutively\nEffect is relatively small in rating scales\nSome respondents stop when they come to an acceptable answer and never see the rest of the options.\nIf respondents spend more time on the first half of the response scale, they are more likely to choose one of the answers here.\nSolution:\n\nIf using Agree/Disagree (A/D), convert to item-specific (IS) response format\n\nRespondants spend more time on IS format thus more likely to read all the anwsers\n\nChange the order of the responses randomly for different participants\n\nSome platforms like Survey Monkey and Qualdtrics websites have a randomize response order option\n\nEffect is smaller in vertical arrangement of responses\n\n\nCognitive Errors\n\nUnderstanding the question\n\nProvide definitions for key terms in the questions\n\nRetrieval of information (Remembering)\n\nUnless it’s something really memorable, don’t ask about something that happened 6 months ago\nUse memory cues\n\nlike sequencing the questions as the events would’ve been sequenced\neliciting life events in the question\n\nSomethings are encoded into memories\n\nWhen we paid cash, we paid more attention to prices of certain items, because we had to get out the cash and count it. Being able to just swipe a card doesn’t encode such information as well.\n\n\nIntegration of that information into an estimate or judgment\n\nParticipants may underreport or overreport a behavior when the frequency is asked\n\nReporting of that judgment (picking a response)\n\nThere’s a cognitive burden when a respondant tries to bin there answer into one of the provided responses\nGreater burden with Agree/Disagree (A/D) format\n\nitem-specific questions are less sequential compared to A/D questions. Thus, respondents may experience less burden on the cognitive process of reporting an answer",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html",
    "href": "qmd/surveys-sampling-methods.html",
    "title": "Sampling Methods",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Surveys",
      "Sampling Methods"
    ]
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-misc",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-misc",
    "title": "Sampling Methods",
    "section": "",
    "text": "Notes from:\n\nSurvey data in the field of economy and finance (ebook)",
    "crumbs": [
      "Surveys",
      "Sampling Methods"
    ]
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-terms",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-terms",
    "title": "Sampling Methods",
    "section": "Terms",
    "text": "Terms\n\nIn a survey setting,\n\nU denotes a finite population (i.e. [target population) of N units\nA sample s of n units (n≤N) is taken from U\n\nDesign Weights - The average number of units in the population that each sampled unit represents. This weight is determined by the sampling method and is an important part of the estimation process.\nEmpirical Design - When the inclusion probabilities (see below) are unknown\n\nSee Non-Probabilistic Sampling Methods\nExamples\n\nQuota Sampling - Units are selected so to reflect known structures for the population\nExpert Sampling - Units are selected according to expert advice\nNetwork Sampling - Existing sample units recruit future units from among their ‘network’.\n\n\nEstimator of the parameter, θ, is a function of sample observations\n\nExample: Sample Mean\n\\[\n\\hat{\\theta} = \\frac {\\sum_{i \\in S}y_i}{S} = \\bar{y}_S\n\\]\n\nPopulation mean of the study variable can be estimated by the mean value over the sample observations\n\n\nInclusion Probability - The probability for a unit to appear in the sample\nProbabilistic Design - When every element in the population has a fixed, known-in-advance inclusion probabilities\nSampling Bias - The probability distribution in the collected dataset deviates from its true natural distribution one would actually observe in the wilderness.\nSampling Frame - An exhaustive list of all the individuals which comprise the target population (Also see Surveys, Design &gt;&gt; Sources of Error &gt;&gt; Coverage or Frame Error)\n\nStudy Parameter (θ) - Linear parameter of the study variable, such as a mean, a total or a proportion, or a more complex one such as a ratio between two population means, a correlation or a regression coefficient, a quantile (e.g. median, quartile, quintile or decile) or an inequality measure such as the Gini or the Theil coefficient. (also see estimator)\nStudy Variable (y)\n\nQuantitative - Numerical information (e.g. the total disposable income or the total food consumption)\nQualitative - Categorical information (e.g. gender, citizenship, country of birth, marital status, occupation or activity status)",
    "crumbs": [
      "Surveys",
      "Sampling Methods"
    ]
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp",
    "title": "Sampling Methods",
    "section": "Probabilistic Sampling Methods",
    "text": "Probabilistic Sampling Methods\n\nSimple Random Sampling (SRS)\n\nA method of selecting n units out of N such that every sample s of size n has the same probability of selection\nSimple Inclusion Probability - the probability for a unit to appear in the sample\n\\[\n\\pi_i = \\mbox{Pr}(i \\in S) = \\sum \\limits_{i \\in s \\in S} \\mbox{Pr}(S = s) = \\frac {\\binom{N-1}{n-1}}{\\binom{N}{n}} = \\frac {n}{N}\n\\]\n\n\\(n\\) is the size of sample, \\(s\\), and \\(N\\) is the target population size\n\nDouble Inclusion Probability - the probability for 2 units to appear in the sample\n\\[\n\\pi_{ij} = \\mbox{Pr}(i,j \\in S) = \\sum \\limits_{i,j \\in s \\in S} \\mbox{Pr}(S = s) = \\frac {\\binom{N-2}{n-2}}{\\binom{N}{n}} = \\frac {n}{N} \\frac {n-1}{N-1}\n\\]\n\nWhere \\(i \\neq j\\)\n\\(n\\) is the size of sample, \\(s\\), and \\(N\\) is the target population size\n\nWithout Replacement (most common)\n\nAt the first extraction, each one of the population units will have an equal probability of selection, \\(1/N\\).\nAt the second extraction, the remaining N-1 units will have a selection probability equal to \\(1/(N-1)\\). Etc.\n\nWith Replacement - all the units of the population will have all the same probability of being selected \\(1/N\\) Advantages:\n\nIt’s simple and doesn’t use auxiliary information on the population\nThe selection is random and, then, any unit is favoured\nThe sample is representative Disadvantages:\nThe choice of the element is completely random\nA complete list of the population units is necessary\nIt’s time and cost consuming\n\nEstimated Total of the study variable, \\(\\hat{Y}\\)\n\\[\n\\hat{Y}_{SRS} = N \\bar{y}\n\\]\n\nWhere \\(N\\) is the target population size\n\nEstimated Mean of the study variable, \\(\\bar{Y}\\)\n\\[\n\\hat{\\bar{Y}}_{SRS} = \\bar{y}\n\\]\n\nWhere \\(\\bar{y}\\) is the sample mean\n\nVariance for Estimated Total\n\\[\nV(\\hat{Y}_{SRS}) = N^2 (1-f) \\frac {S^2_y}{n}\n\\]\n\n\\(S^2_y\\) is the dispersion of the study variable, \\(y\\), over the population \\(U\\)\n\\[\nS^2_y = \\frac {1}{N-1} \\sum_{i \\in U} (y_i - \\bar{Y})^2\n\\]\nSampling Rate or Sampling Fraction: \\(\\mbox{f} = n/N\\)\nFinite Population Correction Factor: \\(1-\\mbox{f}\\)\n\nVariance for Estimated Mean\n\\[\n\\hat{V}(\\bar{y}) = (1-\\mbox{f}) \\frac {s^2_y}{n}\n\\]\n\nSample Dispersion\n\\[\ns^2_y = \\frac{1}{n-1}\\sum_{i \\in s} (y_i - \\bar{y})^2\n\\]\n\nEstimated size of subpopulation, \\(A\\)\n\\[\n\\hat{N}_A = Np_A\n\\]\n\n\\(p_A\\) is the sample proportion of units from target subpopulation, \\(U_A\\)\n\ni.e. (I think) \\(n_A / N_A\\)\n\nExamples: Subpopulations\n\nTotal number of males or females in the population\nTotal number of elderly people aged more than 65 in the population\nTotal number of establishments having more than 50 employees in a certain geographical region or in a sector of activity.\n\n\nVariance of sample proportion of subpopulation, \\(A\\)\n\\[\n\\hat{V}(p_A) = \\frac{p_A(1-p_A)}{n}\n\\]\nDomain Parameter Estimation\n\nRefers to estimating population parameters for sub-populations of interest, called domains. For instance, one may wish to estimate the mean household disposable income broken down by personal characteristics such as age, gender or citizenship\n\nI think this is different from “Estimated size of subpopulation, A” (above) because we’re estimating a study variable of subpopulation vs the size of the subpopulation\n\nEstimated Total of the study variable\n\\[\n\\hat{Y}_D = \\frac{N \\cdot n_D}{n} \\; \\bar{y}_D\n\\]\n\n\\(\\bar{y}_D\\) - The sample mean of study variable, \\(y\\), within the domain, \\(D\\)\n\\(n_D\\) - The total number of sample units from the sample \\(s\\) which fall into domain, \\(D\\)\n\nSample size \\(n_D\\) is a random variable of mean \\(\\bar{n}D = nP_D\\) where \\(P_D = N_D / N\\)\n\nI guess this is a random variable because this is strictly SRS, so you aren’t stratifying by \\(D\\) when you sample the target population. Therefore, the number of samples from \\(D\\) you happen to get will be random and have a distribution.\n\n\nAlternative: When the size of the domain, \\(N_D\\), of \\(U_D\\) is known\n\n\\(\\hat{Y}_{D,\\mbox{alt}} = N_D \\cdot \\bar{y}D\\)\nThis formula has a provably (see ebook in Misc) lower variance than the original formula\n\n\nVariance for Estimated Total\n\\[\nV(\\hat{Y}_D) \\approx N^2_D \\left(\\frac{1}{\\bar{n}_D} - \\frac{1}{N_D}\\right)S^2_D \\left(1 + \\frac{1-P_D}{CV^2_D} \\right)\n\\]\n\nWhere\n\\[\n\\begin{align}\n&S^2_D = \\sum_{k \\in U_D} \\frac{(y_k - \\bar{Y}_D)^2}{N_D - 1}\\\\\n&CV_D = \\frac{S_D}{\\bar{Y}_D}\n\\end{align}\n\\]\nAssumes the population sizes, \\(N\\) and \\(N_D\\), are “large enough.”\nFor the Alternative Estimated Total formula (see above)\n\\[\nV(\\hat{Y}_{D,alt}) \\approx N^2_D \\left(\\frac{1}{\\bar{n}_D} - \\frac{1}{N_D} \\right)S^2_D\n\\]\n\nAssumes the sample size, \\(n_D\\), is “large enough.”\nA provably lower variance (see ebook in Misc)\n\n\n\n\n\n\nUnequal Probability Sampling\n\nDifferent units in the population will have different probabilities of being included in a sample.\n\nUnlike SRS, where each unit has an equal probability of being included in the sample\n\nUnequal probability sampling can result in estimators having higher precision than when simple random sampling or other equal probability designs are used.\n\nEmphasizes the importance of utilizing so-called “auxiliary” information as a way to boost sampling precision. (see πk below)\n\nHorvitz-Thompson estimator (without replacement selection)\n\nEstimated Total, \\(\\hat{Y}\\), for the study variable\n\\[\n\\hat{Y}_{HT}= \\sum_{k \\in S} \\frac{y_k}{\\pi_k} = \\sum_{k \\in s} d_ky_k\n\\]\n\\(d_k = 1/\\pi_k\\) is the design weight of unit, \\(k\\), of sample, \\(s\\)\n\\(\\pi_k\\) is the inclusion probability for unit, \\(k\\), of sample, \\(s\\)\n\nIn practice, as the study variable \\(y\\) is unknown, the inclusion probabilities should be taken proportional to an auxiliary variable \\(x\\) assumed to have a linear relationship with \\(y: π \\propto x\\) (probability proportional to size sampling)\nAn inclusion probability that is optimal with respect to one study variable may be far from optimal with other study variables. In case of multi-purpose surveys, this is a major problem which generally prevents from using unequal probability sampling.\n\nAlternatively, survey statisticians use stratification as we know it always make accuracy better no matter the study variable.\n\n\n\nHansen-Hurwitz estimator has been proposed in case of sampling with replacement.\n\n\n\nCluster Sampling\n\nAssumes population has natural clusters (e.g. family unit). Different from Stratified Sampling in that the clustering characteristic(s) is the same for all clusters (between cluster variation = 0) and the within cluster variation is heterogeneous (i.e. within cluster variation != 0).\n\nExample: Family units in NYC are chosen randomly chosen. The variation between family members is whats studied.\n\nAdvantages:\n\nit’s efficient when the clusters constitute naturally formed subgroups, for which we don’t possess the list of the population\nStudying clusters can be less expensive than simple random sampling.\n\nDisadvantages:\n\nThe conditions of the clusters aren’t always respected. The clusters may contain similar elements.\n\n\n\n\nStratified Sampling\n\n\nMisc\n\nNotes from Chapter 3 Stratification\nThe population is classified into subpopulations, called strata, based on some categorical characteristics, such as age, gender, education\nStratified sampling buckets the population into k strata (e.g., countries), and then the experiment random samples individuals from each stratum independently.\nAssumes between group variation is not 0 (i.e. heterogeneous) and within-group variation is 0 (i.e. homogeneous)\nReasons for stratification\n\nBaseline for group A different from group B\nReason to believe the effect for group A will be different from group B\n\nAdvantages\n\nIt can be more efficient than simple random sampling\nThere is less risk of obtaining non-representative samples\n\nDisadvantages\n\nIt needs the availability of auxiliary information on the population.\nThere are strict conditions for the strata\n\n\nEstimated Total, \\(\\hat{Y}\\), for the study variable and the\nEstimated Mean of the study variable, \\(\\bar{Y}\\) (respectively)\n\\[\n\\begin{align}\n&\\hat{Y}_\\mbox{STSRS}=\\sum_{h=1}^H N_h \\bar{y}_h \\\\\n&\\hat{\\bar{Y}}_\\mbox{STSRS} = \\sum_{h=1}^H W_h \\bar{y}_h\n\\end{align}\n\\]\n\nAssumes SRS within each strata\n\\(N\\) is the population size\n\\(N_h\\) is the population strata size for strata, \\(h\\)\n\\(W_h\\) is the frequency weight where \\(W_h = N_h / N\\)\n\\(\\bar{y}_h\\) is the sample mean of strata, \\(h\\)\n\nVariance for Estimated Total (assuming SRS within strata)\n\\[\nV(\\hat{Y}_\\mbox{STSRS}) = \\sum_{h=1}^H N_h^2 (1-\\mbox{f}_h)\\frac{S_h^2}{n_h}\n\\]\n\n\\(n_h\\) is the sample size for stratum, \\(h\\)\nStratum Sampling Fraction: \\(\\mbox{f}_h = n_h / N_h = n/N\\) (which is just \\(\\mbox{f}\\))\n\nAssumes \\(\\mbox{f}_h\\) is the same for each strata\n\nStratum Dispersion: \\(S_h^2\\) should be similar to the sample dispersion for SRS below, except the domain of the variables is within stratum, \\(h\\) (e.g. \\(n → n_h,\\) \\(ȳ → ȳ_h\\), etc.)\n\nSample Dispersion for SRS\n\\[\ns_y^2 = \\frac{1}{n-1}\\sum_{i \\in s} (y_i - \\bar{y})^2\n\\]\n\n\nVariance for Estimated Mean (assuming SRS within strata)\n\\[\nV(\\bar{Y}_\\mbox{STSRS}) = (1 - \\mbox{f}) \\frac{s^2_h}{n_h}\n\\]\n\n\\(n_h\\) will be the same for all \\(h\\), so it’s constant in this case\nSampling Fraction: \\(\\mbox{f} = N / n\\)\n\n\\(n\\) is the overall sample size\n\nWithin-Stratum Dispersion\n\\[\nS_w^2 = \\sum_{h=1}^H W_hS_h^2\n\\]\n\n\\(S^2_h\\): See above\n\\(N\\) is the population size and \\(N_h\\) is the population strata size for strata, \\(h\\)\n\\(W_h\\) is the frequency weight where \\(W_h = N_h / N\\)\n\n\nDesign Weights: \\(d_i = N_h / n_h \\;\\;\\forall \\in s_h\\)\n\nFor SRS, design weights are equal within each stratum\n\\(s_h\\) is the set of samples within stratum, \\(h\\)\n\nStratum sample size allocation methods\n\nLet assume the overall sample size, \\(n\\), has been fixed (generally out of budgetary considerations). We seek to determine which sample size, \\(n_h\\), is to be drawn out of each stratum in order to achieve statistical optimality under cost considerations.\nEqual Allocation\n\n\\(n^\\mbox{eq}_h = n / H\\)\n\\(H\\) is the number of strata\nPerforms poorly when the dispersions, \\(S^2_h\\), are different from one stratum to another\n\nProportional Allocation\n\nConsists of selecting samples in each stratum in proportion to the size, \\(N_h\\), of the stratum population\n\\(n^\\mbox{prop}_h = (n \\cdot N_h) / N = n \\cdot W_h\\)\nVariance\n\\[\n\\begin{align}\nV(\\hat{\\bar{Y}}_\\mbox{prop}) &= (1-\\mbox{f})\\frac{S^2_w}{n} \\\\\n&=\\frac{\\sum_{h=1}^h W_h S^2_h}{n} - \\frac{\\sum_{h=1}^h W_h S^2_h}{N}\n\\end{align}\n\\]\n\nOptimal or Neyman Allocation\n\nSeeks to minimize the variance under the cost constraint\n\\[\n\\sum_{h=1}^H c_h n_h = C_0\n\\]\n\n\\(C_0\\) is the overall budget available and \\(c_h\\) the average survey cost for an individual in stratum \\(h\\).\n\nStrata Sample Size with Cost Constraint\n\\[\n\\forall h \\;\\; n_h^\\mbox{opt} = \\frac{N_hS_h}{\\sqrt{c_h}} \\frac{C_0}{\\sum_{h=1}^H N_h S_h \\sqrt{c_h}}\n\\]\nStrata Sample Size without Cost Constraint\n\\[\nn_h^\\mbox{opt} = n \\frac{N_hS_h}{\\sum_{h=1}^H N_hS_h}\n\\]\nVariance\n\\[\nV(\\hat{\\bar{Y}}_\\mbox{SRS}) = \\frac{1}{n}\\sum_h W_h(\\bar{Y}_h - \\bar{Y})^2 - \\frac{1}{n}\\sum_h W_h(S_h - \\bar{S})^2\n\\]\n\n\\(\\bar{S}\\) must be the mean sqrt dispersion across all stratum\n\nContrary to proportional allocation, the Neyman allocation is variable-specific: optimality is defined with respect to one study variable, and what is optimal with respect to one variable may be far from optimal with respect to another.\nThe gain in accuracy as compared to proportional allocation is pretty small. That’s why in practice proportional allocation is often preferred to optimal allocation.\n\nBalanced Allocation\n\\[\n\\forall h \\;\\; n_h^\\mbox{bal} = \\frac{\\tilde{n}}{H} + (n - \\tilde{n})W_h\n\\]\n\n\\(\\tilde{n}\\) is a subsample of \\(n\\) that is equally allocated (see above) among the strata which insures minimal precision within the strata (i.e. locally)\nThe rest of the sample (\\(n-\\tilde{n}\\)) can be allocated using either proportional or optimal allocations (see above) in order to optimize accuracy for the overall sample (i.e. globally)\nBoth proportional and Neyman allocations increase sample accuracy at global level, but may happen to perform very poorly when it comes to strata (e.g. regional) level estimates.\n\n\n\n\n\nMulti-Stage Sampling\n\nMisc\n\nUseful when no sampling frame is available\nStages\n\nAt first-stage sampling, a sample of Primary Sampling Units (PSU) is selected using a probabilistic design (e.g. simple random sampling or other, with or without stratification)\nAt second-stage sampling, a sub-sample of Secondary Sampling Units (SSU) is selected within each PSU selected at first-stage. The selection of SSU is supposed to be independent from one PSU to another.\nAt third-stage sampling a sample of Tertiary Sampling Units can be selected with each of the SSU selected at second stage.\netc.\n\nExample: (given an absence of any frame of individuals)\n\nSelect a sample of municipalities (first-stage sampling),\nSelect a sample of neighbourhoods (second-stage sampling) within each selected municipality,\nSelect a sample of households (third-stage sampling) within each of the neighbourhoods selected a second stage\nSelect a sample of individuals (fourth-stage sampling) within each household.\n\nAdvantages:\n\nCan be more efficient than using only 1 of the sampling strategies\nCan decrease sample size if there are numerous units within strata or clusters\n\nDisadvantages:\n\nIf sampling assumptions aren’t valid, multi-stage sampling results to be less efficient than simple random sampling.\n\n\nExample: 2-stage cluster sampling\n\nAdds a second stage to cluster sampling. After clusters are chosen, units within those clusters are randomly sampled.\n\nExample: 2-Stage Stratified Sampling\n\nNotes from Two Stage Stratified Random Sampling — Clearly Explained\nUseful for when you have hierarchical strata (e.g. towns/blocks and households)\nExample: An education study of students where:\n\nSchools (first stage sampling units) may be selected with probabilities proportional to school size\nStudents (second stage units) within selected schools may be selected by stratified random sampling\n\nStage 1\n\n\n(Random?) Sample from group of First Stage Units (FSU)\n\nEach FSU usually has a population within a range\ne.g. census geographies (census block, metropolitan statistical area, etc.)\n\n\nStage 2\n\n\nAll Second Stage Units (SSU) within each FSU are pooled together to create a population\n\nSSUs are the base geography unit you want to measure\ne.g. households\n\nThen each SSU is binned into Second Stage Strata (SSS) according to a characteristic or set of characteristics\n\ne.g. race, age, income level, education, etc.\nThe SSS are stratified sampled\n\n\n\n\n\n\nSystematic Sampling\n\n\nSteps\n\nAfter choosing a sample size, n, calculate the sampling interval k = N/n, where N is the population size\n\nIn the example, we have 9 smiles and we want to obtain a sample of 3 units, then N = 9, n = 3 and k = 9/3 =3.\n\nSelect a random starting point, r, which is a random integer between 1 and k: 1≤r≤k.\n\nIn the example, r = 2, where 1≤r≤3.\n\nOnce the first unit is selected, we take every following kth item to build the sample: r, r+k, r+2k , … , r+(n-1)k.\n\nAdvantages:\n\nThe random selection is applied only on the first item, while the rest of the items selected depend on the position of the first item and a fixed interval at which items are picked.\n\nDisadvantages:\n\nIf the list of the population elements presents a determined order, there is the risk of obtaining a non-representative sample",
    "crumbs": [
      "Surveys",
      "Sampling Methods"
    ]
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp-nonprob",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp-nonprob",
    "title": "Sampling Methods",
    "section": "Non-Probabilistic Sampling Methods",
    "text": "Non-Probabilistic Sampling Methods\n\nMisc\n\nMostly used when probabilistic methods aren’t possible due to rarity or difficulty in obtaining a representative sample of the population being studied or cost constraints of the experiment\n\nQuota Sampling\n\nSimilar to Stratified Sampling (see Probabilistic Sampling Methods) except:\n\nEach stratum’s sample size is called its quota\nEach stratum’s sample size takes into account its distribution in the whole population.\n\nExample: If 80% of the population are males, then 80% of the sample should be males.\n\nWithin each stratum’s quota, the interviewer is free to choose the participants to interview.\n\nThis seems to be the main difference\n\n\nAdvantages:\n\nIt’s time and cost-effective, in particular with respect to the stratified sampling.\n\nDisadvantages:\n\nThe results can be distorted due to the discretion of the interviewers or the non-response bias\nThe quota sample can produce a selection bias\n\n\nJudgemental Sampling (aka Purposive Sampling)\n\nThe researcher selects the participants because he believes they are representative of the population\n\nUseful when there is only a limited number of people with specific traits\n\nAdvantages:\n\nIt’s time and cost-effective\nIt’s suitable to study a certain cultural domain, where the knowledge of an expert is needed\n\nDisadvantages:\n\nIt can lead to a high selection bias the bigger is the gap between the researcher’s knowledge and the actual situation of the population\n\n\nConvenience Sampling\n\nThe researcher chooses anyone that is “convenient” to him, i.e. people that are immediately available to answer the questions, without any specific criteria\n\nUsually volunteers\n\nAdvantages:\n\nIt’s very cheap and fast\n\nDisadvantages:\n\nIt leads to a non-representative sample\n\n\nSnowball Sampling\n\nThe researcher asks already recruited people to identify other potential participants, and so on\n\nUseful for rare populations, for which it’s not possible to have the list of the population or it’s difficult to locate the population.\n\ne.g. illegal immigrants\n\n\nAdvantages:\n\nIt’s useful for market studies or researches about delicate topics.\n\nDisadvantages:\n\nThe sample may be non-representative since it’s not random, but depends on the people contacted directly or indirectly by the researcher\nIt’s time-consuming",
    "crumbs": [
      "Surveys",
      "Sampling Methods"
    ]
  },
  {
    "objectID": "qmd/tuning.html",
    "href": "qmd/tuning.html",
    "title": "Tuning",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Tuning"
    ]
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-misc",
    "href": "qmd/tuning.html#sec-tuning-misc",
    "title": "Tuning",
    "section": "",
    "text": "Tree Parameter Categories (many overlap)\n\nTree Structure and Learning\nTraining Speed\nAccuracy\nOverfitting\n\nWhen the search space is quite large, try the particle swarm method or genetic algorithm for optimization.\nEarly Stopping can lower computational costs and decrease practitioner downtime",
    "crumbs": [
      "Tuning"
    ]
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-pairwise",
    "href": "qmd/tuning.html#sec-tuning-pairwise",
    "title": "Tuning",
    "section": "Pairwise Tuning",
    "text": "Pairwise Tuning\n\nTunes a pair of parameters at a time. Once the first pair of parameters is tuned, those values replace the default parameter values, and the next pair of parameters is tuned, etc.\nLimits the computational cost of performing a full grid search jointly with all parameters at once supposedly without sacrificing much in terms of predictive performance.\n\nA full grid search or other tuning method can be applied to each pair.\n\nMight be beneficial to create pairs that affect the same tuning area of the model fit (e.g. subsampling, regularization, tree complexity) so that the tuning process might capture any interaction effects between the parameters.\nExample: XGBoost (article)\n\nParameter Pairs\n\n(max_depth, eta)\n(subsample, colsample_bytree)\n(min_child_weight, gamma), and\n(reg_alpha, reg_lambda)\n\nProcess\n\nmax_depth and eta are tuned\nTuned values for max_depth and eta replace their default values\nsubsample and colsample_bytree are tuning using the model with the tuned values for max_depth and eta\netc.\n\n\nViz\n\n\n\nEach pair of parameters along with the loss metric are plotted in a 3D chart\nmatplotlib::plot_trisurf() uses Surface Triangulation is used to interpolate the gaps between the tested parameter values\nIf the chart has multiple pronounced dips and bumps (left chart):\n\nMay indicate that there’s a minima in one of the dips that might be better than the chosen parameter values as the interpolation process might have smoothed over that area a bit.\n\nMight want to play with the smoothing parameter a bit to try and get a clearer idea of the range of values to further investigate.\n\nIndicates the model is sensitive to this pair of parameters which might translate into a model instability, when we pass a new type of dataset into the tuned model in the deployment domain.",
    "crumbs": [
      "Tuning"
    ]
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-bayesopt",
    "href": "qmd/tuning.html#sec-tuning-bayesopt",
    "title": "Tuning",
    "section": "Bayesian Optimization",
    "text": "Bayesian Optimization\n\ntl;dr\n\nBuilds a surrogate model using Gaussian Processes that estimates model score\nThis surrogate is then provided with configurations picked randomly, and the one that gives the best score is kept for training\nEach new training updates the posterior knowledge of the surrogate model.\n\nComponents:\n\nThe black box function to optimize: f(x).\n\nWe want to find the value of x which globally optimizes f(x) (aka objective function, the target function, or the loss function)\n\nThe acquisition function: a(x)\n\nUsed to generate new values of x for evaluation with f(x).\na(x) internally relies on a Gaussian process model m(X, y) to generate new values of x.\n\n\nSteps:\n\nDefine the black box function f(x), the acquisition function a(x) and the search space of the parameter x.\nGenerate some initial values of x randomly, and measure the corresponding outputs from f(x).\nFit a Gaussian process model m(X, y) onto X = x and y = f(x) (i.e. surrogate model for f(x))\nThe acquisition function a(x) then uses m(X, y) to generate new values of x as follows:\n\nUse m(X, y) to predict how f(x) varies with x.\nThe value of x which leads to the largest predicted value in m(X, y) is then suggested as the next sample of x to evaluate with f(x).\n\nRepeat the optimization process in steps 3 and 4 until we finally get a value of x that leads to the global optimum of f(x).\n\nAll historical values of x and f(x) should be used to train the Gaussian process model m(X, y) in the next iteration — as the number of data points increases, m(X, y) becomes better at predicting the optimum of f(x).",
    "crumbs": [
      "Tuning"
    ]
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-tpe",
    "href": "qmd/tuning.html#sec-tuning-tpe",
    "title": "Tuning",
    "section": "Tree-based Parzen Estimators (TPE)",
    "text": "Tree-based Parzen Estimators (TPE)\n\nMisc\n\nNotes from HyperOpt Demystified\nLibraries\n\n{{hyperopt}}\n\nTypically outperforms basic bayesian optimization, but the main selling point is it handles complex hyperparameter relationships via a tree structure.\nSupports categorical variables (cat hyperparams?) which traditional Bayesian optimization does not.\n\nProcess\n\nTrain a model with several sets of randomly-selected hyperparameters, returning objective function values.\nSplit our observed objective function values into “good” and “bad” groups, according to some threshold gamma (γ).\nCalculate the “promisingness” score, which is just P(x|good) / P(x|bad).\nDetermine the hyperparameters that maximize promisingness via mixture models.\nFit our model using the hyperparameters from step 4.\nRepeat steps 2–5 until a stopping criteria.\n\nTips/Tricks\n\nHyperOpt is parallelizable via both Apache Spark and MongoDB. If you’re working with multiple cores, wether it be in the cloud or on your local machine, this can dramatically reduce runtime.\nIf you’re parallelizing the tuning process via Apache Spark, use a SparkTrialsobject for single node ML models (sklearn) and a Trails object for parallelized ML models (MLlib).\nMLflow easily integrates with HyperOpt.\nDon’t narrow down the search space too early. Some combinations of hyperparameters may be surprisingly effective.\nDefining the search space can be tricky, especially if you don’t know the functional form of your hyperparameters. However, from personal experience TPE is pretty robust to misspecification of those functional forms.\nChoosing a good objective function goes a long way. In most cases, error is not created equal. If a certain type of error is more problematic, make sure to build that logic into to your function.",
    "crumbs": [
      "Tuning"
    ]
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-dectr",
    "href": "qmd/tuning.html#sec-tuning-dectr",
    "title": "Tuning",
    "section": "Decision Trees",
    "text": "Decision Trees\n\nMisc\n\nAlso see Algorithms, ML &gt;&gt; Trees &gt;&gt; Decision Trees\n\nTranforming continuous features using PCA can improve predictive performance\n\n\nHyperparameters\n\nMaximum Depth (aka Tree Depth): maximum level a tree can “descend” during the training process\n\ntoo high may lead to overfit\ntoo low may lead to underfit\n\nMinimum Samples Split: control how many observations a node must contain to be available for a split\n\ntoo low may lead to overfit\ntoo high may lead to underfit\n\nMinimum Samples Leaf (aka Min N): number of observations in a node after the split has “potentially” happened\n\ntoo low may lead to overfit\ntoo high may lead to underfit\n\nMinimum Impurity Decrease: sets the threshold for the amount of impurity decrease that must occur in order for there to be another split\n\ntoo low may lead to overfit\ntoo high may lead to underfit\n\nMaximum Features: randomly choosing a set of features for each split\n\nUseful for high dimension datasets; adds some randomness\ntoo high can lead to long training times\ntoo low may lead to underfit",
    "crumbs": [
      "Tuning"
    ]
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-rf",
    "href": "qmd/tuning.html#sec-tuning-rf",
    "title": "Tuning",
    "section": "Random Forest",
    "text": "Random Forest\n\nMisc\n\nImplicit features selection is performed by splitting, but performance decreases substantially if over 100 noise-like features are added & drastically if over 500 noise-like features\n\n\nHyperparameters\n\nmtry: the number of trees at each node\n\nMost influential hyperparameter for random forests.\nIncreasing it improves performance in the presence of noise\n\n\nDefault: square root of the number of features\nSimilar improvements can be had with (explicit) feature selection (e.g. recursive feature elimation)",
    "crumbs": [
      "Tuning"
    ]
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-lgbm",
    "href": "qmd/tuning.html#sec-tuning-lgbm",
    "title": "Tuning",
    "section": "LightGBM",
    "text": "LightGBM\n\nNotes\n\nParameters are listed from most to least important\nSeems like the strategy should be to tune structure first, then move to accuracy or overfitting parameters based on results\nMissing values should be encoded as NA_integer_\nProcessing: it is recommended to rescale data before training so that features have similar mean and standard deviation\n\nHyperparameters\n\nStructure\n\nnum_leaves: the number of decision nodes in a tree\n\nkaggle recommendation: 2^(max_depth)\n\ntranslates to a range of 8 - 4096\n\n\nmax_depth: The complexity of each tree\n\nkaggle recommendation: 3 - 12\n\nmin_data_in_leaf: the minimum number of observations that fit the decision criteria in a leaf\n\nValue depends on sample size and num_leaves\nlightgbm doc recommendation: 100 - 10000 for large datasets\n\nlinear_tree (docs): fits piecewise linear gradient boosting tree\n\nTree splits are chosen in the usual way, but the model at each leaf is linear instead of constant\n\nThe first tree has constant leaf values\n\nHelps to extrapolate linear trends in forecasting\nCategorical features are used for splits as normal but are not used in the linear models\nIncreases memory use; no L1 regularization\n\n\nAccuracy\n\nn_estimators: controls the number of decision trees\nlearning_rate: step size parameter of the gradient descent\n\nKaggle recommendation: 0.01 - 0.3\n\nMoving outside this range is usually towards zero\n\n\nmax_bin: controls the maximum number of bins that continuous features will bucketed into\n\ndefault = 255\n\n\nOverfitting\n\nlambda_l1, lambda_l2: regularization\n\nDefault: 0\nKaggle recommendation: 0 - 100\n\nmin_gain_to_split: the reduction in training loss that results from adding a split point\n\nDefault: 0\nExtra regularization in large parameter grids\nReduces training time\n\nbagging_fraction: randomly select this percentage of data without resampling\n\nDefault: 1\n* must set bagging_freq to an integer *\n\nfeature_fraction: specifies the percentage of features to sample from when training each tree\n\nDefault: 1\n* Must set bagging_freq to an integer *\n\nbagging_freq: frequency for bagging\n\nDefault: 0 (disabled)\n(Integer) e.g. Setting to 2 means perform bagging at every 2nd iteration\n\nstopping_rounds: early stopping\n\n\nIssues (from docs)\n\nPoor Accuracy\n\nUse large max_bin (may be slower)\nUse small learning_rate with large num_iterations\nUse large num_leaves (may cause over-fitting)\nUse bigger training data\n\nOverfitting\n\nUse small max_bin\nUse small num_leaves\nUse min_data_in_leaf and min_sum_hessian_in_leaf\nUse bagging by set bagging_fraction and bagging_freq\nUse feature sub-sampling by set feature_fraction\nUse bigger training data\nTry lambda_l1, lambda_l2 and min_gain_to_split for regularization\nTry max_depth to avoid growing deep tree\nTry extra_trees\nTry increasing path_smooth",
    "crumbs": [
      "Tuning"
    ]
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-xgb",
    "href": "qmd/tuning.html#sec-tuning-xgb",
    "title": "Tuning",
    "section": "XGBoost",
    "text": "XGBoost\n\nNotes\n\nDrob starts with learning rate = 0.01 and tune other parameters before coming back to tune learning rate\nKuhn suggests setting trees to about 500 and tune stop_iter\n\nstop_iter: early stopping; stops if no improvement has been made after this many iterations\n\nUber found that the most important hyperparameters were:\n\ntree_depth, trees, learning_rate, and min_n\n\ntree_method (more details about exact, approx)- specify which tree construction algorithm you want to use. Trade-offs between accuracy and speed\n\n“exact” - accurate algorithm, but it is not very scalable as during each split find procedure it iterates over all entries of input data.\n\nInefficient when the data does not completely fit into memory\nDoesn’t support distributed training\n\n“approx” - uses quantile sketch and gradient histograms\n“hist” - method used in lightgbm with slight changes (binning continuous features)\n\nApplies some of approx performance enhancements (e.g. bin caching)\nTypically faster than approx\n\n“gpu_hist” - gpu implementation of “hist”\n\nMuch faster than “hist” and usually requires less memory\n\nGuessing because it uses the gpu memory\n\nNot available for some OSs (link)\n\n“auto” - Default. Chooses fastest method (exact or approx) based on data size\n\nFor larger datasets, approx will be used\n\n\n\nHyperparameters\n\ntrees: The number of trees in your forest\n\nGrid Value Examples: seq(50, 700, 50), seq(250, 1500, 25)\n\nmin_n: minimum number of data points in a node that is required for the node to be split further\n\naka minimum child weight\n\nmtry: The number of predictors to randomly sample for each split in a tree\n\nGrid Value Examples: c(3,5,7), c(5, 7, 9)\nMore Predictors –&gt; higher mtry\n\nShrinkage\n\nlearning_rate (aka eta) aka step size; explainer\n\nShrinks the feature weights to make the boosting process more conservative default: 0.3\nRange: [0,1]\nGrid value examples: 0.2, 0.01, 0.008, 0.005\nMore trees \\(\\rightarrow\\) lower learning rate\n\n\nTree-Booster Constraints\n\ntree_depth (aka max_depth): The complexity of each tree\n\nGrid value examples: 4 - 8 (also see lightgbm recommendations for max_depth)\nIncreasing makes the model more likely to overfit\nConsumes memory when training a deep tree.\n“exact” tree method requires non-zero value.\nDefault: 6\nRange: [0,∞]\n\nmin_child_weight\n\nMinimum sum of weights needed in a node in order for the algorithm to make another split\nIncreasing makes the model less likely to overfit\nDefault: 1\nRange: [0,∞]\n\n\nRegularization\n\nreg_lambda (aka lambda), reg_alpha (aka alpha), and gamma:\n\nThese are parameters of the regularization function\nAlso see\n\nAlgorithms, ML &gt;&gt; Boosting &gt;&gt; XGBoost &gt;&gt; Regularization\nArticle\n\nHigh Alpha: generates a sparses model (i.e. has many null leaf weights)\n\nL1 regularization term on weights\nSimplifies the model\nReduces the model size as it’s not necessary to store null values.\n\nHigh Lambda: more stringently penalizes weights of less influential samples\n\nL2 regularization term on weights\nSimplifies the model\nHas largest effect when data is smaller\n\nHigh Gamma: fewer nodes\n\nAffects the amount of gain needed to add another split\nDefault: 0\nRange: [0,∞]\n\n\n\nRandom Subsampling\n\nsubsample:\n\nSetting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting.\nSubsampling will occur once in every boosting iteration\nDefault: 1\nRange: (0,1]\n\ncolsample_bytree\n\nSubsample ratio of columns when constructing each tree.\nDefault: 1\n\n\nIteration Control\n\nnum_boost_round - Total number of boosting iterations\nearly_stopping_round - Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training",
    "crumbs": [
      "Tuning"
    ]
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-svm",
    "href": "qmd/tuning.html#sec-tuning-svm",
    "title": "Tuning",
    "section": "SVM",
    "text": "SVM\n\nMisc\n\nPackages: {e1071}, {kernlab}, {LiblineaR}, {{sklearn}}\nAlso see\n\nModel Building, tidymodels &gt;&gt; Model Specification &gt;&gt; Support Vector Machines\nAlgorithms, ML &gt;&gt; Support Vector Machines (SVM)\n\n\nHyperparameters\n\ngamma – All the kernels except the linear one require the gamma parameter. ({e1071} default: 1/(data dimension)\ncoef0 – Parameter needed for kernels of type polynomial and sigmoid ({e1071} default: 0).\ncost – The cost of constraints violation ({e1071} default: 1)—it is the ‘C’-constant of the regularization term in the Lagrange formulation.\n\nC = 1/λ (R) or 1/α (sklearn)\nWhen C is small, the regularization is strong, so the slope will be small\n\ndegree - Degree of the polynomial kernel function ({e1071} default: 3)\nepsilon - Needed for insensitive loss function (see Regression below) ({e1071} default: 0.1)\n\nWhen the value of epsilon is small, the model is robust to the outliers.\nWhen the value of epsilon is large, it will take outliers into account.\n\nnu - For {e1071}, needed for types: nu-classification, nu-regression, and one-classification",
    "crumbs": [
      "Tuning"
    ]
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-ensemb",
    "href": "qmd/tuning.html#sec-tuning-ensemb",
    "title": "Tuning",
    "section": "Ensembles",
    "text": "Ensembles\n\nMisc\n\nNotes from\n\nHyperparameters Tuning for Machine Learning Model Ensembles\n\nSee article for\n\nDetails on performance, durations, etc. between sequential and simultaneous tuning methods\nLinks to repo, experimental paper\n\ntldr;\n\nThey like the simultaneous tuning because had the best metric performance with the lowest variance (more stable results)\nI think the sequential tuning method was comparable (and better in some cases) to the simultaneous tuning method but is probably faster and less costly.\n\n\n\nComposite Structures\n\nW/sequential model pipelines (bottom to top)\n\n\n“dtr” = decision tree regression\n\nW/feature engineering models\n\n\nOn the left side are typical structures with individual predictive models being fed into an ensemble model (e.g. Random Forest)\nOn the right side, some kind of feature engineering process or modeling happens prior to the predictive model/ensemble model.\n\n\n\nSequential Tuning\n\nOnly one model is tuned at a time\nThe scoring, during the tuning process, happens on the ensemble model\nExample\n\n\nStructure\n\n(top pipe) KNN feeds its predictions to the ridge regression which feeds it’s predictions to the lasso regression (ensemble model)\n(bottom pipe) Ridge regression feeds its predictions to the random forest which feeds it’s predictions to the lasso regression (ensemble model)\nRed arrows indicate how far the data/predictions have travelled through the structure. Here it’s all the way to the ensemble model\nRed circle indictates which model is currently being tuned\nModels without hyperparameter values are models with default values for their hyperparameters\n\nFigure shows that the KNN and the RR (bottom pipe) models have already been tuned and the RR model (top pipe) is the model being tuned.\nThe red Y’ indicates that the prediction scoring, while ridge regression is being tuned, is happening at the ensemble model.\nRF gets tuned next then finally the ensemble model\n\n\nSimultaneous Tuning\n\nAll models, including the ensemble model, are tuned at the same time\nComputationally expensive\n\nI’d think it would be more monetarily expensive as well given that you likely have to provision more machines in any realistic scenario to get a decent training time (one for each pipe?)\n\nExample\n\n\nSee Sequential tuning example for details on the structure and what items in the figure represent.",
    "crumbs": [
      "Tuning"
    ]
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-dl",
    "href": "qmd/tuning.html#sec-tuning-dl",
    "title": "Tuning",
    "section": "DL",
    "text": "DL\n\nWeight Decay\n\n\nImproves data efficiency by &gt; 50%\nFrequently found in the best hyperparam configs\nAmong the most important hparams to tune\nTricky to tune\n\nLearning Rate Scheduling (article): The schedule reduces the learning rate as training progresses, so take smaller step sizes near and around the optimum\n\nTime-based Decay\n\\[\n\\alpha = \\frac{\\alpha_0}{1+\\text{decay} + \\text{epoch}}\n\\]\n\n\\(\\alpha\\): Learning Rate\n\\(\\alpha_0\\): Initial Learning Rate\n\\(\\text{decay}\\): Decay Rate\n\\(\\text{epoch}\\): Number of interations\n\nStep Decay\n\n\\[\n\\alpha = \\alpha_0 \\times \\operatorname{factor}^{\\frac{\\text{epoch}}{\\text{step}}}\n\\]\n\n\\(\\alpha\\): Learning Rate\n\\(\\alpha_0\\): Initial Learning Rate\n\\(\\text{factor}\\): Factor that the learning rate will be reduced by\n\\(\\text{epoch}\\): Number of interations\n\\(\\text{step}\\): Number of epochs after which the learning rate should be reduced\n\nExponential Decay\n\\[\n\\alpha = \\alpha_0 \\times e^{-\\text{decay} \\times \\text{epoch}}\n\\]\n\n\\(\\alpha\\): Learning Rate\n\\(\\alpha_0\\): Initial Learning Rate\n\\(\\text{epoch}\\): Number of interations\n\\(\\text{decay}\\): Decay Rate\n\nOthers: Performance Scheduling, 1Cycle Scheduling, and Power Scheduling.",
    "crumbs": [
      "Tuning"
    ]
  },
  {
    "objectID": "qmd/model-building-modeltime.html",
    "href": "qmd/model-building-modeltime.html",
    "title": "modeltime",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "modeltime"
    ]
  },
  {
    "objectID": "qmd/model-building-modeltime.html#sec-modbld-mdltm-misc",
    "href": "qmd/model-building-modeltime.html#sec-modbld-mdltm-misc",
    "title": "modeltime",
    "section": "",
    "text": "Plotting\nplot_times_series(data, date_var, value_var)\nCombining multiple models into a table\nmodel_tbl &lt;- modeltime_table(\n    model_arima,\n    model_prophet,\n    model_glmnet\n)\n\nAllows you to apply functions across multiple models\n\nAccessing a fitted model object to see coefficient values, hyperparameter values, etc.\nmodel_arima &lt;- arima_reg() %&gt;%\n    set_engine(\"auto_arima\") %&gt;%\n    fit(value_var ~ date_var, training(splits_obj))\nmodel_arima\nThe fit obj shows coefficients, AICc, etc.",
    "crumbs": [
      "Model Building",
      "modeltime"
    ]
  },
  {
    "objectID": "qmd/model-building-modeltime.html#sec-modbld-mdltm-steps",
    "href": "qmd/model-building-modeltime.html#sec-modbld-mdltm-steps",
    "title": "modeltime",
    "section": "Steps",
    "text": "Steps\n\nset-up\nrecipe\nspecify model(s)\nfit models\nforecast on test/assessment dataset\nassess performance\nchoose model\nforecast and assess performance on holdout dataset\nrefit on whole training set\nforecast the future",
    "crumbs": [
      "Model Building",
      "modeltime"
    ]
  },
  {
    "objectID": "qmd/model-building-modeltime.html#sec-modbld-mdltm-setup",
    "href": "qmd/model-building-modeltime.html#sec-modbld-mdltm-setup",
    "title": "modeltime",
    "section": "Set-up",
    "text": "Set-up\n\nSome packages\n\ntidyverse - cleaning\ntidymodels - model building\ntimetk - loss metrics\nlubridate - ts cleaning functions\nstacks - ensembling\n\nParallelize/Cluster\n\nFor all methods, you must also set allow_par = TRUE  in the control arg of the function\nParallel\nparallel_start(6, .method = \"parallel\") # uses 6 vcpus\nparallel_stop()\nSpark\nsc &lt;- spark_connect(master = \"local\")\nparallel_start(sc, .method = \"spark\")\nparallel_stop()\nspark_disconnect_all()\n\nFor Databricks, replace “local” with “databricks”\n\n\nSplits\nsplits &lt;- time_series_split(\n    data,\n    assess = \"3 months\",\n    cumulative = TRUE\n)\n\nThe above test set will be 3 months long\n\ndataset is daily so assess arg doesn’t haven’t to on the same scale as the dataset\n\n\nVisualize the split\nsplits %&gt;%\n    # extract cv plan from split obj\n    tk_time_series_cv_plan() %&gt;%\n    plot_time_series(date_var, value_var)",
    "crumbs": [
      "Model Building",
      "modeltime"
    ]
  },
  {
    "objectID": "qmd/model-building-modeltime.html#sec-modbld-mdltm-modspec",
    "href": "qmd/model-building-modeltime.html#sec-modbld-mdltm-modspec",
    "title": "modeltime",
    "section": "Model Specification, Fit",
    "text": "Model Specification, Fit\n\nWindow mean/median forecast\nmedian_fit &lt;- window_reg(id = \"id\", window_size = 6) %&gt;%\n    set_engine(\"window_function\", window_function = median) %&gt;%\n    fit(value ~ ., data = training(splits))\n\nBasically just calculates the median value for the window size that’s specified (not rolling)\n\nThe forecast is just this value repeated for the length of the forecast horizon\n\nNotes\n\nUsed mostly as a baseline forecast, but with tuning, it supposedly performs pretty well for some series\n\nArgs\n\nwindow_function - can be mean or whatever\nid - used for global modeling\n\nTuning\ngrid_tbl &lt;- tibble(\n    window_size = 1:12\n) %&gt;%\n    create_model_grid(\n        f_model_spec = \"window_reg\",\n        id = \"id\",\n        engine_name = \"window_function\",\n        engine_params = list(\n            window_function = ~ median(.)\n        )\n    )\n\nSlightly different than the normal procedure where you have to specify the window function like a lambda function\n\n\nSeasonal Naive\nnaive_fit &lt;- naive_reg(seasonal_period = 12, id = \"id\") %&gt;%\n    set_engine(\"snaive\") %&gt;%\n    fit(value ~ ., data = training(splits))\n\nSeasonal version of a naive forecast which just repeats the last value of the series for the length of the forecast horizon.\n\nauto_arima\nmodel_arima &lt;- arima_reg() %&gt;%\n    set_engine(\"auto_arima\") %&gt;%\n    fit(value_var ~ date_var, training(splits_obj))\nprophet\nmodel_prophet &lt;- prophet_reg(seasonality_yearly = TRUE) %&gt;%\n    set_engine(\"prophet\") %&gt;%\n    fit(value_var ~ date_var, training(split_obj))\n\nSeems to work better for highly seasonal or cyclical data otherwise it’s 💩\n\nglmnet\nmodel_glmnet &lt;- linear_reg(penalty = 0.01) %&gt;%\n    set_engine(\"glmnet\") %&gt;%\n    # date_var is daily in this example\n    fit(value_var ~ wday(date_var, label = TRUE) +\n                    month(date_var, label = TRUE) +\n                    # trend feature\n                    as.numeric(date_var),\n        training(splits_obj)\n    )\ngluonts deepar\nfit_deepar &lt;- deep_ar(\n    id = \"id\",\n    freq = \"M\",\n    prediction_length = 6,\n    lookback_length = 6*3,\n    epochs = 10\n) %&gt;%\n    # set_engine(\"gluonts_deepar\") %&gt;%\n    set_engine(\"torch\") %&gt;%\n    fit(value ~ date + id, data = training(splits))\n\nDescription\n\nDeveloped by Amazon; LSTM RNN\nRequires more data than arima or prophet\n\nUses weighted resampling techniques to help with lower n, larger p data\n\nHandles seasonality with minimal tuning\nOutputs probabilitistic forecasts\n\nin the form of Monte Carlo samples\ncan be used to develop quantile forecasts\n\nSupports non-normal likelihood functions\n\ne.g. discrete target variables\n\n\nNotes\n\n“gluonts_deepar” is the older style engine that uses reticulate to use the python gluonts library. “torch” uses the torch pkg which is written in R and is faster.\nStandardizing predictors is recommended\nimpute missing values\n\nRecommended the imputing algorithm uses predictor variables to calculate values\n\nlearning rate and encoder/decoder length are subject-specific and should be tuned manually (not sure how these are subject specific)\nTo ensure the model is not fitting based on the index of our dependent variable in the TS, the authors suggest training the model on “empty” data prior our start period. Just use a bunch of zeros as our dependent variable.\n\nNot sure if this can be implemented in modeltime\n\n\nargs\n\n“id” is for fitting a global model with panel data\nfreq - A pandas timeseries frequency such as “5min” for 5-minutes or “D” for daily. Refer to Pandas Offset Aliases.\nprediction_length - forecast horizon\nlookback_length - Number of steps to unroll the RNN for before computing predictions (default: NULL, in which case context_length = prediction_length)\n\ni.e. like the number of lags to use as predictors in a ML or AR model\n\nepochs - default 5, the number of backpropagations before stopping (I think)\nand many more…\n\n\ngluonts_gp_forecaster\nfit_gp_forecaster &lt;- gp_forecaster(\n    id = \"id\",\n    freq = \"M\",\n    prediction_length = 6,\n    epochs = 30\n) %&gt;%\n    set_engine(\"gluonts_gp_forecasster\") %&gt;%\n    fit(value ~ date + id, data = training(splits))\n\nGaussian Process (GP) Forecaster model\nNotes\nArgs\n\n“id” is for fitting a global model with panel data\nfreq - A pandas timeseries frequency such as “5min” for 5-minutes or “D” for daily. Refer to Pandas Offset Aliases.\nprediction_length - forecast horizon\nepochs - default 5, the number of backpropagations before stopping (I think)\nand many more…\n\n\ngluonts_deepstate\nfit_deepar &lt;- deep_state(\n    id = \"id\",\n    freq = \"M\",\n    prediction_length = 6,\n    lookback_length = 6*3,\n    epochs = 20\n) %&gt;%\n    set_engine(\"gluonts_deepstate\") %&gt;%\n    fit(value ~ date + id, data = training(splits))\n\ndeep learning state-space model\nArgs\n\n“id” is for fitting a global model with panel data\nfreq - A pandas timeseries frequency such as “5min” for 5-minutes or “D” for daily. Refer to Pandas Offset Aliases.\nprediction_length - forecast horizon\nlookback_length - Number of steps to unroll the RNN for before computing predictions (default: NULL, in which case context_length = prediction_length)\n\ni.e. like the number of lags to use as predictors in a ML or AR model\n\nepochs - default 5, the number of backpropagations before stopping (I think)\nand many more…",
    "crumbs": [
      "Model Building",
      "modeltime"
    ]
  },
  {
    "objectID": "qmd/model-building-modeltime.html#sec-modbld-mdltm-tune",
    "href": "qmd/model-building-modeltime.html#sec-modbld-mdltm-tune",
    "title": "modeltime",
    "section": "Tune",
    "text": "Tune\n\ncreate_model_grid\ngrid_tbl &lt;- tibble(\n    learn_rate = c(0.001, 0.01, 0.1)\n) %&gt;%\n    create_model_grid(\n        f_model_spec = boost_tree,\n        engine_name = \"xgboost\",\n        mode = \"regression\n    )\n\nAlternative\ngrid_tbl &lt;- dials::grid_regular(\n    learn_rate(),\n    levels = 3\n)\n\ngrid_tbl %&gt;%\n    create_model_grid(\n        f_model_spec = boost_tree,\n        engine_name  = \"xgboost\",\n        # Static boost_tree() args\n        mode = \"regression\",\n        # Static set_engine() args\n        engine_params = list(\n            max_depth = 5\n        )\n    )\nCreates a list of model specifications\n\nI think you could create more than 1 of these objects in order to tune multiple algorithms.  Then append the $.models together and feed it to workflow_set(models= ) (see below)\nExample of another way\nmodels &lt;- \n  union(snaive_grid_tbl %&gt;% \n          select(.models), \n        window_grid_median_tbl %&gt;%\n          select(.models))\n\nCombination of seasonal naive and window-median models\n\n\nArgs\n\nf_model_spec - the parsnip model function\nengine_name - the parsnip model engine\nmode - always regression for forecasting\n\n\nworkflow_set\nwfset &lt;- workflow_set() %&lt;%\n    preproc = list(\n        recipe_spec),\n    models = grid_tbl$.models,\n    cross = TRUE\n)\n\nInstantiates workflfow\nArgs\n\ncross = TRUE says fit each recipe to each model\n\n\nmodeltime_fit_workflowset\n# parallel_start(6)\n\nwkflw_fit &lt;- wfset %&gt;%\n    modeltime_fit_workflowset(\n        data = training(splits),\n        control = control_fit_workflowset(\n            verbose = TRUE,\n            allow_par = TRUE\n        )\n    )\n\n# parallel_stop()\n\nFits all the models with different hyperparameter combinations\nparallel_start/stop is a wrapper around parallel::doParallel stuff\n\nThink I’d rather use doFuture\n\nArgs\n\nallow_par - turns on parallel processing",
    "crumbs": [
      "Model Building",
      "modeltime"
    ]
  },
  {
    "objectID": "qmd/model-building-modeltime.html#sec-modbld-mdltm-perf",
    "href": "qmd/model-building-modeltime.html#sec-modbld-mdltm-perf",
    "title": "modeltime",
    "section": "Assessing Performance",
    "text": "Assessing Performance\n\nmodeltime_calibration: Calculate Residuals\ncalib_tbl &lt;- model_table %&gt;%\n    modeltime_calibration(testing(split_obj), id = \"id\")\n\nadds residuals to model table\n“id” allows you to assess accuracy for each group member\n\nmodeltime_accuracy :\ncalib_tbl %&gt;%\n    modeltime_accuracy(acc_by_id = TRUE) #%&gt;%\n    # select best model by rmse for each group\n    # slice_min(rmse)\n\nuses the residual data to calculate mae, mape, mase, smape, rmse, rsq\nacc_by_id = TRUE assess accuracy by group id (for global models)\n\ntable_modeltime_accuracy\ncalib_tbl %&gt;%\n    group_by(id)\n    table_modeltime_accuracy()\n\nhtml table of results (might be interactive)",
    "crumbs": [
      "Model Building",
      "modeltime"
    ]
  },
  {
    "objectID": "qmd/model-building-modeltime.html#sec-modbld-mdltm-fcast",
    "href": "qmd/model-building-modeltime.html#sec-modbld-mdltm-fcast",
    "title": "modeltime",
    "section": "Forecast",
    "text": "Forecast\n\nmodeltime_forecast : makes predictions, calculates prediction intervals\n\nOn the test set:\n\n# Using test set\nforcast_obj &lt;- calibration_table %&gt;%\n    modeltimes_forecast(\n        new_data = testing(splits_obj),\n        actual_data = whole_training_dataset,\n        conf_by_id = TRUE\n    )\n\nconf_by_id = TRUE is for global models to get individual PIs for each group\n\nmust have included an id arg in the model function\n\n\nmodeltime_refit : forecast the future\nfuture_forecast_tbl &lt;- calibration_tbl %&gt;%\n    modeltime_refit(whole_training_set) %&gt;%\n    modeltime_forecast(\n        h = \"3 months\",\n        actual_data = whole_training_set\n    )\n\nIn example, dataset is daily, so horizon time scale doesn’t have to match the dataset’s scale.\n\nplot_modeltime_forecast\nplot_modeltime_forecast(forecast_obj)",
    "crumbs": [
      "Model Building",
      "modeltime"
    ]
  },
  {
    "objectID": "qmd/big-data.html",
    "href": "qmd/big-data.html",
    "title": "Big Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-misc",
    "href": "qmd/big-data.html#sec-bgdat-misc",
    "title": "Big Data",
    "section": "",
    "text": "RcppArmadillo::fastLmPure Not sure what this does but it’s rcpp so maybe faster than lm for big data.\n.lm.fit is a base R lm function that is 30%-40% faster than lm.",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-hghperf",
    "href": "qmd/big-data.html#sec-bgdat-hghperf",
    "title": "Big Data",
    "section": "High Performance",
    "text": "High Performance\n\n{rpolars}: arrow product; uses SIMD which is a low-level vectorization that can be used to speed up simple operations like addition, subtraction, division, and multiplication\n\nResources\n\nCookbook Polars for R\n\nAlso see collapse &gt;&gt; vs arrow/polars\nExample: Read and Summarize\ndf &lt;- pl$scan_csv(file_name)$\n    group_by(\"state\")$\n    agg(\n        pl$\n          col(\"measurement\")$\n          min()$\n          alias(\"min_m\"),\n        pl$\n          col(\"measurement\")$\n          max()$\n          alias(\"max_m\"),\n        pl$\n          col(\"measurement\")$\n          mean()$\n          alias(\"mean_m\")\n    )$\n    collect()\n\nFastest at this operation according to this benchmark\n\nExample: groupby state + min, max, mean\n# polars sql\nlf &lt;- polars::pl$LazyFrame(D) \npolars::pl$SQLContext(frame = lf)$execute(\n  \"select min(measurement) as min_m, \n          max(measurement) as max_m, \n          avg(measurement) as mean_m \n  from frame \n  group by state\")$collect()\n\n# polars\npolars::pl$\n  DataFrame(D)$\n  group_by(\"state\")$\n  agg(polars::pl$\n        col(\"measurement\")$\n        min()$alias(\"min_m\"),\n      polars::pl$\n        col(\"measurement\")$\n        max()$alias(\"max_m\"),\n      polars::pl$\n        col(\"measurement\")$\n        mean()$alias(\"mean_m\"))\n\n{collapse}: Fast grouped & weighted statistical computations, time series and panel data transformations, list-processing, data manipulation functions, summary statistics and various utilities such as support for variable labels. Class-agnostic framework designed to work with vectors, matrices, data frames, lists and related classes i.e. xts, data.table, tibble, pdata.frame, sf.\n\noptions(collapse_mask = \"all\")\nlibrary(collapse)\n\nCode chunk above can optimize any script. No other changes necessary. Quick demo.\nvs arrow/polars (benchmark)\n\nDepends on the data/groups ratio\n\nIf you have “many groups and little data in each group” then use collapse\n\nIf your calculations involve “more complex statistics algorithms like the median (involving selection) or mode or distinct value count (involving hashing)(cannot, to my knowledge, benefit from SIMD)” then use collapse.\n\nExample: groupby state + min, max, mean\nD |&gt;\n  fgroup_by(state) |&gt; \n  fsummarise(min = fmin(measurement), \n             max = fmax(measurement), \n             mean = fmean(measurement)) |&gt;\n  fungroup()\n\n{r2c}: Fast grouped statistical computation; currently limited to a few functions, sometimes faster than {collapse}\n{data.table}: Enhanced data frame class with concise data manipulation framework offering powerful aggregation, extremely flexible split-apply-combine computing, reshaping, joins, rolling statistics, set operations on tables, fast csv read/write, and various utilities such as transposition of data.\n\nExample: groupby state + min, max, mean\nD[ ,.(mean = mean(measurement),\n      min = min(measurement),\n      max = max(measurement)),\n   by=state]\n\n# Supposedly faster\nrbindlist(lapply(unique(D$state), \n                 \\(x) data.table(state = x, \n                                 y[state == x, \n                                   .(mean(measurement), \n                                     min(measurement), \n                                     max(measurement))\n                                   ]\n                                 )))\n\n{rfast}: A collection of fast (utility) functions for data analysis. Column- and row- wise means, medians, variances, minimums, maximums, many t, F and G-square tests, many regressions (normal, logistic, Poisson), are some of the many fast functions\n\nThe vast majority of the functions accept matrices only, not data.frames.\nDo not have matrices or vectors with have missing data (i.e NAs). There are no checks and C++ internally transforms them into zeros (0), so you may get wrong results.\nExample: groupby state + min, max, mean\nlev_int &lt;- as.numeric(D$state)\nminmax &lt;- Rfast::group(D$measurement, lev_int, method = \"min.max\")\ndata.frame(\n    state = levels(D$state),\n    mean = Rfast::group(D$measurement, lev_int, method = \"mean\"),\n    min = minmax[1, ],\n    max = minmax[2, ]\n)\n\n{matrixStats}: Efficient row-and column-wise (weighted) statistics on matrices and vectors, including computations on subsets of rows and columns.\n{kit}: Fast vectorized and nested switches, some parallel (row-wise) statistics, and some utilities such as efficient partial sorting and unique values.\n{fst}: A compressed data file format that is very fast to read and write. Full random access in both rows and columns allows reading subsets from a ‘.fst’ file.",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-lgmem",
    "href": "qmd/big-data.html#sec-bgdat-lgmem",
    "title": "Big Data",
    "section": "Larger than Memory",
    "text": "Larger than Memory\n\nOnly work with a sample of the data\n\nRandom sample in CLI\n\nSee binder for code\n\nOnly read the first n lines\n\nset n_max arg in readr::read_*\n\n\ndatasette.io - App for exploring and publishing data. It helps people take data of any shape, analyze and explore it, and publish it as an interactive website and accompanying API.\n\nWell documented, many plugins\n\nRill - A tool for effortlessly transforming data sets into powerful, opinionated dashboards using SQL.\n\nDocs, Example Projects\nPowered by Sveltekit & DuckDB = conversation-fast, not wait-ten-seconds-for-result-set fast\nWorks with your local and remote datasets – imports and exports Parquet and CSV (s3, gcs, https, local)\nNo more data analysis “side-quests” – helps you build intuition about your dataset through automatic profiling\nNo “run query” button required – responds to each keystroke by re-profiling the resulting dataset\nRadically simple interactive dashboards – thoughtful, opinionated, interactive dashboard defaults to help you quickly derive insights from your data\nDashboards as code – each step from data to dashboard has versioning, Git sharing, and easy project rehydration\n\nOnline duckdb shell for parquet files (gist, https://shell.duckdb.org/)\nselect max(wind) \nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/gridwatch/gridwatch_2023-01-08.parquet';\n-- Takes 6 seconds on the first query, 200ms on subsequent similar queries\n\nselect * \nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/NSPL/NSPL.parquet' \nwhere pcd = 'SW1A1AA';\n-- Takes 13 seconds on the first query, 100ms on subsequent similar queries\nCSV Editors\n\nFor editing or reformatting cells\nPopular spreadsheet programs like googlesheets (100MB) and excel (25MB online) have file size limits and they’re slow to upload to. The following programs are free(-ish) local alternatives only limited by your RAM.\nSuggest for files over a few hundred MBs that you open as Read-Only\n\nOpening the files as “Editable” will probably balloon the memory cost to at least 5 times the file size. (e.g. 350MB csv \\(\\rightarrow\\) 2GB RAM)\n\nModern CSV - Nice modern interface, read-only mode that can open large csvs (100s of MBs) without making much of a dent in your RAM, fully featured (moreso if you pay a small-ish one time fee)\n\nDocs, Feature free/upgrade list\nStill has some functionality in read-only mode (e.g. search, sort)\n\nOpenRefine - Has read-only, Several add-ons, Completely open source.\n\nDocs, List of Extensions\nNo functionality when read-only (must create a project to do anything) — just reading\nStarts with a 1024 MB RAM usage limit which is proably fine for editing around a 100MB csv. Need to set the limit higher in a config file in order to edit larger files.\nOnce you create a project, I think it has some editing features that you’d have to pay for with Modern CV.\nOpens other file formats besides csv (e.g. xlsx, xml, json, etc)\n\n\ncsvkit - suite of command-line tools for converting to and working with CSV\n\nInstallation docs\n\nOne of the articles your terminal has to be a bash terminal but I dunno\n\nIf so, they recommend cmder or enabling the Linux subsystem with WSL2.\n\n\nNotes from\n\nArticle with additional examples and options\n\nFeatures\n\nPrint CSV files out nicely formatted\nCut out specific columns\nGet statistical information about columns\n\nConvert excel files to CSV files:\nin2csv excel_file.xlsx &gt; new_file.csv\n# +remove .xlsx file\nin2csv excel_file.xlsx &gt; new_file.csv && rm excel_file\nSearch within columns with regular expressions:\ncsvgrep -c county -m \"HOLT\" new_file.csv\n# subset of columns (might be faster) with pretty formatting\ncsvcut -c county,total_cost new_file.csv | csvgrep -c county -m \"HOLT\" | csvlook\n\nSearches for “HOLT” in the “county” column\n\nQuery with SQL\n\nsyntax csvsql --query \"ENTER YOUR SQL QUERY HERE\" FILE_NAME.csv\nExample\n\n\nView top lines: head new_file.csv\nView columns names: csvcut -n new_file.csv\nSelect specific columns: csvcut -c county,total_cost,ship_date new_file.csv\n\nWith pretty output: csvcut -c county,total_cost,ship_date new_file.csv | csvlook\nCan also use column indexes instead of names\n\nJoin 2 files: csvjoin -c cf data1.csv data2.csv &gt; joined.csv\n\n“cf” is the common column between the 2 files\n\nEDA-type stats:\ncsvstat new_file.csv\n# subset of columns\ncsvcut -c total_cost,ship_date new_file.csv | csvstat\n\nJSONata - a lightweight, open-source query and transformation language for JSON data, inspired by the ‘location path’ semantics of XPath 3.1.\n\nMisc\n\nNotes from: Hrbrmstr’s article\nJSONata also doesn’t throw errors for non-existing data in the input document. If during the navigation of the location path, a field is not found, then the expression returns nothing.\n\nThis can be beneficial in certain scenarios where the structure of the input JSON can vary and doesn’t always contain the same fields.\n\nTreats single values and arrays containing a single value as equivalent\nBoth JSONata and jq can work in the browser (JSONata embedding code, demo), but jq has a slight speed edge thanks to WASM. However, said edge comes at the cost of a slow-first-start\n\nFeatures\n\nDeclarative syntax that is pretty easy to read and write, which allows us to focus on the desired output rather than the procedural steps required to achieve it\nBuilt-in operators and functions for manipulating and combining data, making it easier to perform complex transformations without writing custom code in a traditional programming language like python or javascript\nUser-defined functions that let us extend JSONata’s capabilities and tailor it to our specific needs\nFlexible output structure that lets us format query results into pretty much any output type\n\n\njq + jsonlite - json files\njsoncrack.com - online editor/tool to visualize nested json (or regular json)\njj - cli tool for nested json. Full support for ndjson as well as setting/updating/deleting values. Plus it lets you perform similar pretty/ugly printing that jq does.\nsqlite3 - CLI utility allows the user to manually enter and execute SQL statements against an SQLite database or against a ZIP archive.\n\nalso directly against csv files (post)\n\ntextql - Execute SQL against structured text like CSV or TSV\n\nRequire Go language installed\nOnly for Macs or running a docker image\n\ncolumnq-cli - sql query json, csv, parquet, arrow, and more\nfread + CLI tools\n\nFor large csvs and fixing large csv with jacked-up formating see article, RBlogger version\n\n{arrow}\n\nconvert file into parquet files\n\npass the file path to open_dataset, use group_by to partition the Dataset into manageable chunks\nuse write_datasetto write each chunk to a separate Parquet file—all without needing to read the full CSV file into R\n\ndplyr support\n\nmultiplyr\n\nOption for data &gt; 10M rows and you only have access to one machine\nSpreads data over local cores\n\n{sparklyr}\n\nspin up a spark cluster\ndplyr support\nSet-up a cloud bucket and load data into it. Then, read into a local spark cluster. Process data.\n\n{h2o}\n\nh2o.import_file(path=path) holds data in the h2o cluster and not in memory\n\n{disk.frame}\n\nsupports many dplyr verbs\nsupports  future package to take advantage of multi-core CPUs but single machine focused\nstate-of-the-art data storage techniques such as fast data compression, and random access to rows and columns provided by the fst package to provide superior data manipulation speeds\n\nMatrix ops\n\nsee bkmks: mathematics &gt;&gt; packages\n\n{ff}\n\nsee bkmks: data &gt;&gt; loading/saving/memory\nThink it converts files to a ff file type, then you load them and use ffapply to perform row and column operations with base R functions and expressions\nmay not handle character and factor types but may work with {bit} pkg to solve this",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-viz",
    "href": "qmd/big-data.html#sec-bgdat-viz",
    "title": "Big Data",
    "section": "Viz",
    "text": "Viz\n\nScatter plots\n\n{scattermore}, {ggpointdensity}\n{ggrastr}\n\nRasterize only specific layers of a ggplot2 plot (for instance, large scatter plots with many points) while keeping all labels and text in vector format. This allows users to keep plots within a reasonable size limit without losing the vector properties of scale-sensitive information.\ngithub; tweet\n\n\nH2O\n\nh2o.aggregator Reduces data size to a representive sample, then you can visualize a clustering-based method for reducing a numerical/categorical dataset into a dataset with fewer rows A count column is added to show how many rows is represented by the exemplar row (I think)\n\nAggregator maintains outliers as outliers but lumps together dense clusters into exemplars with an attached count column showing the member points.\nFor cat vars:\n\nAccumulate the category frequencies.\nFor the top 1,000 or fewer categories (by frequency), generate dummy variables (called one-hot encoding by ML people, called dummy coding by statisticians).\nCalculate the first eigenvector of the covariance matrix of these dummy variables.\nReplace the row values on the categorical column with the value from the eigenvector corresponding to the dummy values.\n\ndocs; article\n\n\n{dbplot}\n\nplots data that are in databases\n\nAlso able to plot data within a spark cluster\n\ndocs\n\nObservableHQ\n\n{{{deepscatter}}}\n\nThread (using Arrow, duckdb)",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "title": "51  Census Data",
    "section": "",
    "text": "Notes from\n\nTidycensus Workshop 2024\n\nFIPS GEOID\n\npopular variable calculations from variables in ACS\nCensus Geocoder (link)\n\nEnter an address and codes for various geographies are returned\nBatch geocoding available for up to 10K records\n\nCodes for geographies returned in a .csv file\n\n\nTIGERweb (link)\n\nAllows you to get geography codes by searching for an area on a map\nOnce zoomed-in on your desired area, you turn on geography layers to find the geography code for your area.\n\nUS Census Regions",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Census Data</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "title": "51  Census Data",
    "section": "Geographies",
    "text": "Geographies\n\n\nMisc\n\n{tidycensus} docs on various geographies, function arguments, and which surveys (ACS, Census) they’re available in.\nACS Geography Boundaries by Year (link)\n\nTypes\n\nLegal/Administrative\n\nCensus gets boundaries from outside party (state, county, city, etc.)\ne.g. election areas, school districts, counties, county subdivisions\n\nStatistical\n\nCensus creates these boundaries\ne.g. regions, census tracts, ZCTAs, block groups, MSAs, urban areas\n\n\nNested Areas\n\n\nCensus Tracts\n\nAreas within a county\nAround 1200 to 8000 people\nSmall towns, rural areas, neighborhoods\n** Census tracts may cross city boundaries **\n\nBlock Groups\n\nAreas within a census tract\nAround 600 to 3000 people\n\nCensus Blocks\n\nAreas within a block group\nNot for ACS, only for the 10-yr census\n\n\nPlaces\n\nMisc\n\nOne place cannot overlap another place\nExpand and contract as population or commercial activity increases or decreases\nMust represent an organized settlement of people living in close proximity.\n\nIncorporated Places\n\ncities, towns, villages\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\nCensus Designated Places (CDPs)\n\nAreas that can’t become Incorporated Places because of state or city regulations\nConcentrations of population, housing, commericial structures\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\n\nCounty Subdivisions\n\nMinor Civil Divisions (MCDs)\n\nLegally defined by the state or county, stable entity. May have elected government\ne.g. townships, charter townships, or districts\n\nCensus County Divisions (CCDs)\n\nno population requirment\nSubcounty units with stable boundaries and recognizable names\n\n\nZip Code Tabulation Areas (ZCTAs)\n\n\nMisc\n\n{crosswalkZCTA} - Contains the US Census Bureau’s 2020 ZCTA to County Relationship File, as well as convenience functions to translate between States, Counties and ZIP Code Tabulation Areas (ZCTAs)\n\nApproximate USPS Code distribution for housing units\n\nThe most frequently occurring zip code within an census block is assigned to a census block\nThen blocks are aggregated into areas (ZCTAs)\n\nZCTAs do NOT nest within any other geographies\n\nI guess the aggregated ZCTA blocks can overlap block groups\n\n2010 ZCTAs exclude large bodies of water and unpopulated areas",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Census Data</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "title": "51  Census Data",
    "section": "American Community Survey (ACS)",
    "text": "American Community Survey (ACS)\n\nAbout\n\nYearly estimates based on samples of the population over a 5yr period\n\nTherefore a Margin of Error (MoE) is included with the estimates.\n\nDetailed social, economic, housing, and demographic characteristics\ncensus.gov/acs\n\nACS Release Schedule (releases)\n\nSeptember - 1-Year Estimates (from previous year’s collection)\n\nEstimates for areas with populations of &gt;65K\n\nOctober - 1-Year Supplemental Estimates\n\nEstimates for areas with populations between 20K-64999\n\nDecember - 5-Year Estimates\n\nEstimates for areas including census tract and block groups\n\n\nData Collected\n\nPopulation\n\nSocial\n\nAncestry, Citizenship, Citizen Voting Age  Population, Disability, Education Attainment, Fertility, Grandparents, Language, Marital Status, Migration, School Enrollment, Veterans\n\nDemographic\n\nAge, Hispanic Origin, Race, Relationship, Sex\n\nEconomic\n\nClass of worker, Commuting, Employment Status, Food Stamps (SNAP), Health Insurance, Hours/Week, Weeks/Year, Income, Industry & Occupation\n\n\nHousing\n\nComputer & Internet Use, Costs (Mortgage, Taxes, Insurance), Heating Fuel, Home Value, Occupancy, Plumbing/Kitchen Facilities, Structure, Tenure (Own/Rent), Utilities, Vehicles, Year Built/Year Movied In",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Census Data</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "title": "51  Census Data",
    "section": "Dicennial US Census",
    "text": "Dicennial US Census\n\nMisc\n\nA complete count and not based on samples like the ACS\nApplies differential privacy to preserve respondent confidentiality\n\nAdds noise to data. Greater effect at lower levels (i.e. block level)\nThe exception is that is no differetial privacy for household-level data.\n\n\n\n\nPL94-171\n\nPopulation data which the government needs for redistricting\nsumfile = “pl”\nState Populations\npop20 &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"P1_001N\",\n    year = 2020\n  )\n\nFor 2020, default is sumfile = “pl”\n\n\n\n\nDHC\n\nAge, Sex, Race, Ethnicity, and Housing Tenure (most popular dataset)\nsumfile = “dhc”\nCounty\ntx_population &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\nCensus Block (analogous to a city block)\nmatagorda_blocks &lt;- \n  get_decennial(\n    geography = \"block\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    county = \"Matagorda\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\n\n\n\nDemographic Profile\n\nPretabulated percentages from dhc\nsumfile = \"dp\"\nC suffix variables is counts while P suffix variables are percentages\n\n0.4 is 0.4% not 40%\n\nExample: Same-sex married and partnered in California by County\nca_samesex &lt;- \n  get_decennial(\n    geography = \"county\",\n    state = \"CA\",\n    variables = c(married = \"DP1_0116P\",\n                  partnered = \"DP1_0118P\"),\n    year = 2020,\n    sumfile = \"dp\",\n    output = \"wide\"\n  )\nTabulations for 118th Congress and Island Areas (i.e. congressional districts)\n\nsumfile = \"cd118\"\n\n\n\n\nDetailed DHC-A\n\nDetailed demographic data; Thousands of racial and ethnic groups; Tabulation by sex and age.\nDifferent groups are in different tables, so specific groups can be hard to locate.\nAdaptive design means the demographic group (i.e. variable) will only be available in certain areas. For privacy, data gets supressed where the area has low population.\n\nConsiderable sparsity especially when going down census tract\n\nArgs\n\nsumfile = “ddhca”\npop_group - Population group code (See get_pop_groups below)\n\n“all” for all groups\npop_group_label = TRUE - Adds group labels\n\n\nget_pop_groups(2020, \"ddhca\") - Gets group codes for ethnic groups\n\nFor various groups there could be at least two variable (e..g Somaili, Somali and any combination)\nFor time series analysis, analagous groups for 2000 is SF2/SF4 and for 2010 is SF2. (SF stands for Summary File)\n\ncheck_ddhca_groups - Checks which variables are available for a specific group\n\nExample: Somali\ncheck_ddhca_groups(\n  geography = \"county\", \n  pop_group = \"1325\", \n  state = \"MN\", \n  county = \"Hennepin\"\n)\n\nExample: Minnesota group populations\nload_variables(2020, \"ddhca\") %&gt;% \n  View()\nmn_population_groups &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"all\", # for all groups\n    pop_group_label = TRUE\n  )\n\nIncludes aggregate categories like European Alone, Other White Alone, etc., so you can’t just aggregate the value column to get the total population in Minnesota.\n\nSo, in order to calculate ethnic group ratios of the total state or county, etc. population, you need to those state/county totals from other tables (e.g. PL94-171)\n\n\nUse dot density and not chloropleths to visualize these sparse data\n\nExample: Somali populations by census tract in Minneapolis\n\nhennepin_somali &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    county = \"Hennepin\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"1325\", # somali\n    pop_group_label = TRUE,\n    geometry = TRUE\n  )\n\nsomali_dots &lt;- \n  as_dot_density(\n    hennepin_somali,\n    value = \"value\", # column name which is by default, \"value\"\n    values_per_dot = 25\n  )\n\nmapview(somali_dots, \n        cex = 0.01, \n        layer.name = \"Somali population&lt;br&gt;1 dot = 25 people\",\n        col.regions = \"navy\", \n        color = \"navy\")\n\nvalues_per_dot = 25 says make each dot worth 25 units (e.g. people or housing units)\n\n\n\n\n\nTime Series Analysis\n\n{tidycensus} only has 2010 and 2020\n\nSee https://nhgis.org for previous census values\n\nIssue: county names and boundaries change (e.g. Alaska redraws a lot)\n\nCensus gives a different GeoID to counties that rename even though they’re it’s the same county\nNA values showing up after you calculate how the value changes over time is a good indication of this problem: filter(county_change, is.na(value10))\n\nExample: Join 2010 and 2020\ncounty_pop_10 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P001001\", \n    year = 2010,\n    sumfile = \"sf1\"\n  )\n\ncounty_pop_10_clean &lt;- \n  county_pop_10 %&gt;%\n    select(GEOID, value10 = value) \n\ncounty_pop_20 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    year = 2020,\n    sumfile = \"dhc\"\n  ) %&gt;%\n    select(GEOID, NAME, value20 = value)\n\ncounty_joined &lt;- \n  county_pop_20 %&gt;%\n    left_join(county_pop_10_clean, by = \"GEOID\") \n\ncounty_joined\n\ncounty_change &lt;- \n  county_joined %&gt;%\n    mutate( \n      total_change = value20 - value10, \n      percent_change = 100 * (total_change / value10) \n    ) \nExample: Age distribution over time in Michigan\n\n\nCode available in the github repo or R/Workshops/tidycensus-umich-workshop-2024-main/census-2020/bonus-chart.R\nDistribution shape remains pretty much the same but decreasing for most age cohorts.\n\ne.g. The large hump representing the group of people in there mid-40s in 2000 steadily decreases over time.",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Census Data</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "title": "51  Census Data",
    "section": "tidycensus",
    "text": "tidycensus\n\nGet an API key\n\nRequest a key, then activate the key from the link in your email.(https://api.census.gov/data/key_signup.html)\nSet as an environment variable: census_api_key(\"&lt;api key&gt;\", install = TRUE)\n\nOr add this line to .Renviron file, CENSUS_API_KEY=‘&lt;api key’\n\n\nSearch Variables\n\nColumns\n\nName - ID of the variable (Use this in the survey functions)\nLabel - Detailed description of the variable\nContext - Subject of the table that the variable is located in.\n\nPrefixes (Variables can have combinations of prefixes)\n\nP: i.e. Person; Data available at the census block and larger\nCT: Data available at the census track and larger\nH: Data available at the Housing Unit level\n\nI think housing unit is an alternatve unit. So instead of the unit being a person, which I assume is the typical unit, it’s a housing unit (~family).\nNot affected by Differential Privacy (i.e. no noise added; true value)\nExample: Total Deleware housing units at census block level\ndp_households &lt;- \n      get_decennial(\n            geography = \"block\",\n            variables = \"H1_001N\",\n            state = \"DE\",\n            sumfile = \"dhc\",\n            year = 2020\n      )\n\n\nExample: DHC data in census for 2020\n\nvars &lt;- load_variables(2020, \"dhc\")\n\nView(vars)\n\nView table, click filter, and then search for parameters (e.g. Age, Median, etc.) with the Label, Context boxes, and overall search box\n\n\nsummary_var - Argument for supplying an additional variable that you need to calculate some kind of summary statistic\n\nExample: Race Percentage per Congressional District\n\nrace_vars &lt;- c(\n  Hispanic = \"P5_010N\", # all races identified as hispanic\n  White = \"P5_003N\", # white not hispanic\n  Black = \"P5_004N\", # black not hispanic\n  Native = \"P5_005N\", # native american not hispanic\n  Asian = \"P5_006N\", # asian not hispanic\n  HIPI = \"P5_007N\" # hawaiian, islander not hispanic\n)\n\ncd_race &lt;- \n  get_decennial(\n    geography = \"congressional district\",\n    variables = race_vars,\n    summary_var = \"P5_001N\", # total population for county\n    year = 2020,\n    sumfile = \"cd118\"\n)\n\ncd_race_percent &lt;- \n  cd_race %&gt;%\n    mutate(percent = 100 * (value / summary_value)) %&gt;% \n    select(NAME, variable, percent)\n\ngeometry = TRUE- Joins shapefile with data and returns a SF (Simple Features) dataframe for mapping\n\nMisc\n\nYou can create a discrete color palette with the at argument in the mapview function.\n\nExample\n# check min and max of your data to select range of bins\nmin(iowa_over_65, na.rm = TRUE) # 0\nmax(iowa_over_65, na.rm = TRUE) # 38.4\n\nm1 &lt;- \n  mapview(iowa_over_65, \n          zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1),\n          at = c(0, 10, 20, 30, 40))\n\nThis will result in a discrete palette with bins of 0-10, 10-20, etc. Looks like an overlap, so I’m sure which bin contains the endpoints.\n\n\n\nExample: Over 65 in Iowa by census tract\n\nlibrary(mapviw); library(viridisLite)\n\niowa_over_65 &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"DP1_0024P\",\n    state = \"IA\",\n    geometry = TRUE,\n    sumfile = \"dp\",\n    year = 2020\n  )\nm1 &lt;- \n  mapview(iowa_over_65, zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1))\nExport as an HTML file\nhtmlwidgets::saveWidget(m1@map, \"iowa_over_65.html\")\n\nCan embed it elsewhere (html report or website) by adding it as an asset",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Census Data</span>"
    ]
  }
]